<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Aug 2025 01:33:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Means of Random Variables in Lie Groups</title>
      <link>https://arxiv.org/abs/2508.12030</link>
      <description>arXiv:2508.12030v1 Announce Type: new 
Abstract: The concepts of mean (i.e., average) and covariance of a random variable are fundamental in statistics, and are used to solve real-world problems such as those that arise in robotics, computer vision, and medical imaging. On matrix Lie groups, multiple competing definitions of the mean arise, including the Euclidean, projected, distance-based (i.e., Fr\'echet and Karcher), group-theoretic, and parametric means. This article provides a comprehensive review of these definitions, investigates their relationships to each other, and determines the conditions under which the group-theoretic means minimize a least-squares type cost function. We also highlight the dependence of these definitions on the choice of inner product on the Lie algebra. The goal of this article is to guide practitioners in selecting an appropriate notion of the mean in applications involving matrix Lie groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12030v1</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiraz Khan, Jikai Ye, Gregory S. Chirikjian</dc:creator>
    </item>
    <item>
      <title>Identifying Network Hubs with the Partial Correlation Graphical LASSO</title>
      <link>https://arxiv.org/abs/2508.12258</link>
      <description>arXiv:2508.12258v1 Announce Type: new 
Abstract: The Partial Correlation Graphical LASSO (PCGLASSO) offers a scale-invariant alternative to the standard GLASSO. This paper provides the first comprehensive treatment of the PCGLASSO estimator.
  We introduce a novel and highly efficient algorithm. Our central theoretical contribution is the first scale-invariant irrepresentability criterion for PCGLASSO, which guarantees consistent model selection. We prove this condition is significantly weaker than its GLASSO counterpart, providing the first theoretical justification for PCGLASSO's superior empirical performance, especially in recovering networks with hub structures. Furthermore, we deliver the first analysis of the estimator's non-convex solution landscape, establishing new conditions for global uniqueness and guaranteeing the consistency of all minimizers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12258v1</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ma{\l}gorzata Bogdan, Adam Chojecki, Ivan Hejn\'y, Bartosz Ko{\l}odziejek, Jonas Wallin</dc:creator>
    </item>
    <item>
      <title>An optimal experimental design approach to sensor placement in continuous stochastic filtering</title>
      <link>https://arxiv.org/abs/2508.12288</link>
      <description>arXiv:2508.12288v1 Announce Type: new 
Abstract: Sequential filtering and spatial inverse problems assimilate data points distributed either temporally (in the case of filtering) or spatially (in the case of spatial inverse problems). Sometimes it is possible to choose the position of these data points (which we call sensors here) in advance, with the goal of maximising the expected information gain (or a different metric of performance) from future data, and this leads to an Optimal Experimental Design (OED) problem. Here we revisit an interpretation of optimising sensor placement as an integration with respect to a general probability measure $\xi$. This generalises the problem of discrete-time sensor placement (which corresponds to the special case where the probability measure is a mixture of Diracs) to an infinite-dimensional, but mathematically more well-behaved setting. We focus on the continuous-time stochastic filtering setting, whose solution is governed by the Zakai equation. We derive an expression for the Fr\'echet derivative of a general OED utility functional, the key to which is an adjoint (backwards in time) differential equation. This paves the way for utilising new gradient-based methods for solving the corresponding optimisation problem, as a potentially more efficient alternative to (semi-)discrete optimisation methods, e.g. based on greedy insertion and deletion of sensor placements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12288v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahani Pathiraja, Claudia Schillings, Philipp Wacker</dc:creator>
    </item>
    <item>
      <title>Moderate deviation principle for the chi-square statistics</title>
      <link>https://arxiv.org/abs/2508.12380</link>
      <description>arXiv:2508.12380v1 Announce Type: new 
Abstract: In the present paper, we consider the Pearson chi-square statistic defined on a finite alphabet which is assumed to dynamically vary as the sample size increases, and establish its moderate deviation principle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12380v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenhong Yu, Yu Miao</dc:creator>
    </item>
    <item>
      <title>Moderate deviation principle for plug-in estimators of diversity indices on countable alphabets</title>
      <link>https://arxiv.org/abs/2508.12382</link>
      <description>arXiv:2508.12382v1 Announce Type: new 
Abstract: In the present paper, we consider the moderate deviation principle for the plug-in estimators of a large class of diversity indices on countable alphabets, where the distribution may change with the sample size. Our results cover some of the most commonly used indices, including Tsallis entropy, Re\'{n}yi entropy and Hill diversity number.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12382v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenhong Yu, Yu Miao</dc:creator>
    </item>
    <item>
      <title>Asymptotic confidence bands for the histogram regression estimator</title>
      <link>https://arxiv.org/abs/2508.12391</link>
      <description>arXiv:2508.12391v1 Announce Type: new 
Abstract: In a multivariate nonparametric regression model with a H\"older continuous regression function and heteroscedastic noise asymptotic uniform confidence bands are constructed based on the histogram estimator. The radius of the confidence bands does not depend on an extreme value distribution, but instead can be numerically calculated for the chosen binning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12391v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Natalie Neumeyer, Jan Rabe, Mathias Trabs</dc:creator>
    </item>
    <item>
      <title>Asymptotic breakdown point analysis of the minimum density power divergence estimator under independent non-homogeneous setups</title>
      <link>https://arxiv.org/abs/2508.12426</link>
      <description>arXiv:2508.12426v1 Announce Type: new 
Abstract: The minimum density power divergence estimator (MDPDE) has gained significant attention in the literature of robust inference due to its strong robustness properties and high asymptotic efficiency; it is relatively easy to compute and can be interpreted as a generalization of the classical maximum likelihood estimator. It has been successfully applied in various setups, including the case of independent and non-homogeneous (INH) observations that cover both classification and regression-type problems with a fixed design. While the local robustness of this estimator has been theoretically validated through the bounded influence function, no general result is known about the global reliability or the breakdown behavior of this estimator under the INH setup, except for the specific case of location-type models. In this paper, we extend the notion of asymptotic breakdown point from the case of independent and identically distributed data to the INH setup and derive a theoretical lower bound for the asymptotic breakdown point of the MDPDE, under some easily verifiable assumptions. These results are further illustrated with applications to some fixed design regression models and corroborated through extensive simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12426v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suryasis Jana, Subhrajyoty Roy, Ayanendranath Basu, Abhik Ghosh</dc:creator>
    </item>
    <item>
      <title>Sparsity of the Main Effect Matrix Factor Model</title>
      <link>https://arxiv.org/abs/2508.12510</link>
      <description>arXiv:2508.12510v1 Announce Type: new 
Abstract: We introduce sparsity detection and estimation in main effect matrix factor models for matrix-valued time series. A carefully chosen set of identification conditions for the common component and the potentially nonstationary main effects is proposed to strengthen the interpretations of sparse main effects, while estimators of all model components are presented. Sparse estimation of the latent main effects is proposed using a doubly adaptive fused lasso estimation to allow for sparse sub-block detection, with theoretical guarantees and rates of convergence spelt out for the final estimators. Sparse block consistency for the main effects is also proved as a result. A realized Mallow's $C_p$ is developed for tuning parameter selection, with practical implementation described. Simulation experiments are performed under a variety of settings, showing our proposed estimators work well. A set of NYC taxi traffic data is analyzed, clearly showing the effects of Covid-19 lockdown, with prolonged sparse main effects detected.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12510v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zetai Cen, Kaixin Liu, Clifford Lam</dc:creator>
    </item>
    <item>
      <title>On the distance between mean and geometric median in high dimensions</title>
      <link>https://arxiv.org/abs/2508.12926</link>
      <description>arXiv:2508.12926v1 Announce Type: new 
Abstract: The geometric median, a notion of center for multivariate distributions, has gained recent attention in robust statistics and machine learning. Although conceptually distinct from the mean (i.e., expectation), we demonstrate that both are very close in high dimensions when the dependence between the distribution components is suitably controlled. Concretely, we find an upper bound on the distance that vanishes with the dimension asymptotically, and derive a rate-matching first order expansion of the geometric median components. Simulations illustrate and confirm our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12926v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Richard Schwank, Mathias Drton</dc:creator>
    </item>
    <item>
      <title>Likelihood confidence intervals for misspecified Cox models</title>
      <link>https://arxiv.org/abs/2508.11851</link>
      <description>arXiv:2508.11851v1 Announce Type: cross 
Abstract: The robust Wald confidence interval (CI) for the Cox model is commonly used when the model may be misspecified or when weights are applied. However it can perform poorly when there are few events in one or both treatment groups, as may occur when the event of interest is rare or when the experimental arm is highly efficacious. For instance, if we artificially remove events (assuming more events are unfavorable) from the experimental group, the resulting upper CI may increase. This is clearly counter-intuitive as a small number of events in the experimental arm represents stronger evidence for efficacy. It is well known that, when the sample size is small to moderate, likelihood CIs are better than Wald CIs in terms of actual coverage probabilities closely matching nominal levels. However, a robust version of the likelihood CI for the Cox model remains an open problem. For example, in the SAS procedure PHREG, the likelihood CI provided in the outputs is still the regular version, even when the robust option is specified. This is obviously undesirable as a user may mistakenly assume that the CI is the robust version. In this article we demonstrate that the likelihood ratio test statistic of the Cox model converges to a weighted chi-square distribution when the model is misspecified. The robust likelihood CI is then obtained by inverting the robust likelihood ratio test. The proposed CIs are evaluated through simulation studies and illustrated using real data from an HIV prevention trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11851v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yongwu Shao, Xu Guo</dc:creator>
    </item>
    <item>
      <title>Sub-Poisson distributions: Concentration inequalities, optimal variance proxies, and closure properties</title>
      <link>https://arxiv.org/abs/2508.12103</link>
      <description>arXiv:2508.12103v1 Announce Type: cross 
Abstract: We introduce a nonasymptotic framework for sub-Poisson distributions with moment generating function dominated by that of a Poisson distribution. At its core is a new notion of optimal sub-Poisson variance proxy, analogous to the variance parameter in the sub-Gaussian setting. This framework allows us to derive a Bennett-type concentration inequality without boundedness assumptions and to show that the sub-Poisson property is closed under key operations including independent sums and convex combinations, but not under all linear operations such as scalar multiplication. We derive bounds relating the sub-Poisson variance proxy to sub-Gaussian and sub-exponential Orlicz norms. Taken together, these results unify the treatment of Bernoulli and Poisson random variables and their signed versions in their natural tail regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12103v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lasse Leskel\"a, Ian V\"alimaa</dc:creator>
    </item>
    <item>
      <title>Navigating the Exploration-Exploitation Tradeoff in Inference-Time Scaling of Diffusion Models</title>
      <link>https://arxiv.org/abs/2508.12361</link>
      <description>arXiv:2508.12361v1 Announce Type: cross 
Abstract: Inference-time scaling has achieved remarkable success in language models, yet its adaptation to diffusion models remains underexplored. We observe that the efficacy of recent Sequential Monte Carlo (SMC)-based methods largely stems from globally fitting the The reward-tilted distribution, which inherently preserves diversity during multi-modal search. However, current applications of SMC to diffusion models face a fundamental dilemma: early-stage noise samples offer high potential for improvement but are difficult to evaluate accurately, whereas late-stage samples can be reliably assessed but are largely irreversible. To address this exploration-exploitation trade-off, we approach the problem from the perspective of the search algorithm and propose two strategies: Funnel Schedule and Adaptive Temperature. These simple yet effective methods are tailored to the unique generation dynamics and phase-transition behavior of diffusion models. By progressively reducing the number of maintained particles and down-weighting the influence of early-stage rewards, our methods significantly enhance sample quality without increasing the total number of Noise Function Evaluations. Experimental results on multiple benchmarks and state-of-the-art text-to-image diffusion models demonstrate that our approach outperforms previous baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12361v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xun Su, Jianming Huang, Yang Yusen, Zhongxi Fang, Hiroyuki Kasai</dc:creator>
    </item>
    <item>
      <title>Simultaneous estimation of connectivity and dimensionality in samples of networks</title>
      <link>https://arxiv.org/abs/2508.12483</link>
      <description>arXiv:2508.12483v1 Announce Type: cross 
Abstract: An overarching objective in contemporary statistical network analysis is extracting salient information from datasets consisting of multiple networks. To date, considerable attention has been devoted to node and network clustering, while comparatively less attention has been devoted to downstream connectivity estimation and parsimonious embedding dimension selection. Given a sample of potentially heterogeneous networks, this paper proposes a method to simultaneously estimate a latent matrix of connectivity probabilities and its embedding dimensionality or rank after first pre-estimating the number of communities and the node community memberships. The method is formulated as a convex optimization problem and solved using an alternating direction method of multipliers algorithm. We establish estimation error bounds under the Frobenius norm and nuclear norm for settings in which observable networks have blockmodel structure, even when node memberships are imperfectly recovered. When perfect membership recovery is possible and dimensionality is much smaller than the number of communities, the proposed method outperforms conventional averaging-based methods for estimating connectivity and dimensionality. Numerical studies empirically demonstrate the accuracy of our method across various scenarios. Additionally, analysis of a primate brain dataset demonstrates that posited connectivity is not necessarily full rank in practice, illustrating the need for flexible methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12483v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenlong Jiang, Chris McKennan, Jes\'us Arroyo, Joshua Cape</dc:creator>
    </item>
    <item>
      <title>Generalized quantum Chernoff bound</title>
      <link>https://arxiv.org/abs/2508.12889</link>
      <description>arXiv:2508.12889v1 Announce Type: cross 
Abstract: We establish a generalized quantum Chernoff bound for the discrimination of multiple sets of quantum states, thereby extending the classical and quantum Chernoff bounds to the general setting of composite and correlated quantum hypotheses. Specifically, we consider the task of distinguishing whether a quantum system is prepared in a state from one of several convex, compact sets of quantum states, each of which may exhibit arbitrary correlations. Assuming their stability under tensor product, we prove that the optimal error exponent for discrimination is precisely given by the regularized quantum Chernoff divergence between the sets. Furthermore, leveraging minimax theorems, we show that discriminating between sets of quantum states is no harder than discriminating between their worst-case elements in terms of error probability. This implies the existence of a universal optimal test that achieves the minimum error probability for all states in the sets, matching the performance of the optimal test for the most challenging states. We provide explicit characterizations of the universal optimal test in the binary composite case. Finally, we show that the maximum overlap between a pure state and a set of free states, a quantity that frequently arises in quantum resource theories, is equal to the quantum Chernoff divergence between the sets, thereby providing an operational interpretation of this quantity in the context of symmetric hypothesis testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12889v1</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Fang</dc:creator>
    </item>
    <item>
      <title>Error exponents of quantum state discrimination with composite correlated hypotheses</title>
      <link>https://arxiv.org/abs/2508.12901</link>
      <description>arXiv:2508.12901v1 Announce Type: cross 
Abstract: We study the error exponents in quantum hypothesis testing between two sets of quantum states, extending the analysis beyond the independent and identically distributed case to encompass composite and correlated hypotheses. We introduce and compare two natural extensions of the quantum Hoeffding divergence and anti-divergence to sets of quantum states, establishing their equivalence or quantitative relationships. Our main results generalize the quantum Hoeffding bound to stable sequences of convex, compact sets of quantum states, demonstrating that the optimal type-I error exponent, under an exponential constraint on the type-II error, is precisely characterized by the regularized quantum Hoeffding divergence between the sets. In the strong converse regime, we provide a lower bound on the exponent in terms of the regularized quantum Hoeffding anti-divergence. These findings refine the generalized quantum Stein's lemma and yield a detailed understanding of the trade-off between type-I and type-II errors in discrimination with composite correlated hypotheses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12901v1</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Fang</dc:creator>
    </item>
    <item>
      <title>Efficient estimation of stable Levy process with symmetric jumps</title>
      <link>https://arxiv.org/abs/1805.08926</link>
      <description>arXiv:1805.08926v3 Announce Type: replace 
Abstract: Efficient estimation of a non-Gaussian stable Levy process with drift and symmetric jumps observed at high frequency is considered. For this statistical experiment, the local asymptotic normality of the likelihood is proved with a non-singular Fisher information matrix through the use of a non-diagonal norming matrix. The asymptotic normality and efficiency of a sequence of roots of the associated likelihood equation are shown as well. Moreover, we show that a simple preliminary method of moments can be used as an initial estimator of a scoring procedure, thereby conveniently enabling us to bypass numerically demanding likelihood optimization. Our simulation results show that the one-step estimator can exhibit quite similar finite-sample performance as the maximum likelihood estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:1805.08926v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11203-018-9181-0</arxiv:DOI>
      <arxiv:journal_reference>Statistical Inference for Stochastic Processes, 2018</arxiv:journal_reference>
      <dc:creator>Alexandre Brouste, Hiroki Masuda</dc:creator>
    </item>
    <item>
      <title>A novel statistical approach to analyze image classification</title>
      <link>https://arxiv.org/abs/2206.02151</link>
      <description>arXiv:2206.02151v3 Announce Type: replace 
Abstract: The recent statistical theory of neural networks focuses on nonparametric denoising problems that treat randomness as additive noise. Variability in image classification datasets does, however, not originate from additive noise but from variation of the shape and other characteristics of the same object across different images. To address this problem, we introduce a tractable model for supervised image classification. While from the function estimation point of view, every pixel in an image is a variable, and large images lead to high-dimensional function recovery tasks suffering from the curse of dimensionality, increasing the number of pixels in the proposed image deformation model enhances the image resolution and makes the object classification problem easier. We introduce and theoretically analyze three approaches. Two methods combine image alignment with a one-nearest neighbor classifier. Under a separation condition, it is shown that perfect classification is possible. The third method fits a convolutional neural network (CNN) to the data. We derive a rate for the misclassification error that depends on the sample size and the complexity of the deformation class. An empirical study corroborates the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.02151v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juntong Chen, Sophie Langer, Johannes Schmidt-Hieber</dc:creator>
    </item>
    <item>
      <title>Kernel Ridge Regression Inference</title>
      <link>https://arxiv.org/abs/2302.06578</link>
      <description>arXiv:2302.06578v3 Announce Type: replace 
Abstract: We provide uniform confidence bands for kernel ridge regression (KRR), a widely used nonparametric regression estimator for nonstandard data such as preferences, sequences, and graphs. Despite the prevalence of these data--e.g., student preferences in school matching mechanisms--the inferential theory of KRR is not fully known. We construct valid and sharp confidence sets that shrink at nearly the minimax rate, allowing nonstandard regressors. Our bootstrap procedure uses anti-symmetric multipliers for computational efficiency and for validity under mis-specification. We use the procedure to develop a test for match effects, i.e. whether students benefit more from the schools they rank highly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.06578v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rahul Singh, Suhas Vijaykumar</dc:creator>
    </item>
    <item>
      <title>Non-parametric estimators of scaled cash flows</title>
      <link>https://arxiv.org/abs/2408.13176</link>
      <description>arXiv:2408.13176v2 Announce Type: replace 
Abstract: In multi-state life insurance, incidental policyholder behavior gives rise to expected cash flows that are not easily targeted by classic non-parametric estimators if data is subject to sampling effects. We introduce a scaled version of the classic Aalen--Johansen estimator that overcomes this challenge. Strong uniform consistency and asymptotic normality are established under entirely random right-censoring, subject to lax moment conditions on the multivariate counting process. In a simulation study, the estimator outperforms earlier proposals from the literature. Finally, we showcase the potential of the presented method to other areas of actuarial science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13176v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>T. Bathke, C. Furrer</dc:creator>
    </item>
    <item>
      <title>A projector--rank partition theorem for exact degrees of freedom in experimental design</title>
      <link>https://arxiv.org/abs/2506.01619</link>
      <description>arXiv:2506.01619v2 Announce Type: replace 
Abstract: Degrees of freedom ($\mathrm{df}$) allocation becomes opaque once blocking, nesting, fractionation, or unequal replication disturb the balanced one-stratum classical ANOVA. We resolve this ambiguity with a $\mathrm{df}$ partition theorem. For $N$ observations and a set $\mathcal{S}$ that indexes all randomization strata in the experiment, let $\mathbf{P}_s$ denote the idempotent projector that averages observations within randomization stratum $s \in \mathcal{S}$ and $\mathcal C_{\overline E}$ the factorial-contrast subspace attached to the possibly aliased effect $\overline{E}$. We prove the rank identity $N-1 = \sum_{s}\sum_{\overline E}\dim\left(\mathcal C_{\overline E} \cap \operatorname{Im}\mathbf{P}_s\right)$, which simultaneously diagonalizes all strata and all effects, yielding exact integer $\mathrm{df}$ for any fixed-random mixture, block structure, or replication pattern. It results in closed-form $\mathrm{df}$ tables for unbalanced designs previously reliant on numerical approximations. We introduce aliasing measures such as the $\mathrm{df}$-retention ratio $\rho(\overline E)$, deficiency $\delta(\overline{E})$ and the variance-inflation index $\alpha(\overline{E})$ that extend Box--Hunter's resolution concept to multi-stratum and incomplete designs, and supply immediate diagnostics of information loss. Classical results such as Cochran's identity when $\lvert \mathcal{S} \rvert =1$ and balanced fractions, Yates' $\mathrm{df}$ for balanced split-plots, and resolution-$R$ when $\rho(\overline{E}) = 0$ emerge as corollaries. The method yields calibrated error rates, boosts power by up to 60\%, and outpaces bootstrap approximations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01619v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nagananda K G</dc:creator>
    </item>
    <item>
      <title>A Note on Inferential Decisions, Errors and Path-Dependency</title>
      <link>https://arxiv.org/abs/2507.05634</link>
      <description>arXiv:2507.05634v3 Announce Type: replace 
Abstract: Consider the standard testing of a binary outcome. Depending on the underlying beliefs, the a posteriori belief process and its objectively true conditional-probability counterpart generally differ, but converge to the same target in well-defined tests. We show that unless the two are 'essentially identical', differing at most by an a priori factor, time-homogeneous continuous sequential decisions based on the former must be path-dependent with respect to state-variables based on the latter or any non-essentially-identical a posteriori beliefs. Total inferential errors decompose into two independent components with distinct properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05634v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kangda K. Wren</dc:creator>
    </item>
    <item>
      <title>Efficiently matching random inhomogeneous graphs via degree profiles</title>
      <link>https://arxiv.org/abs/2310.10441</link>
      <description>arXiv:2310.10441v2 Announce Type: replace-cross 
Abstract: In this paper, we study the problem of recovering the latent vertex correspondence between two correlated random graphs with vastly inhomogeneous and unknown edge probabilities between different pairs of vertices. Inspired by and extending the matching algorithm via degree profiles by Ding, Ma, Wu and Xu (2021), we obtain an efficient matching algorithm as long as the minimal average degree is at least $\Omega(\log^{2} n)$ and the minimal correlation is at least $1 - O(\log^{-2} n)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.10441v2</guid>
      <category>cs.DS</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jian Ding, Yumou Fei, Yuanzheng Wang</dc:creator>
    </item>
    <item>
      <title>On the Weak Convergence of the Function-Indexed Sequential Empirical Process and its Smoothed Analogue under Nonstationarity</title>
      <link>https://arxiv.org/abs/2412.01635</link>
      <description>arXiv:2412.01635v2 Announce Type: replace-cross 
Abstract: We study the sequential empirical process indexed by general function classes and its smoothed set-indexed analogue. Sufficient conditions for asymptotic equicontinuity are provided for nonstationary arrays of time series. This yields comprehensive general results that are applicable to various notions of dependence, which is exemplified in detail for nonstationary $\alpha$-mixing series. Especially, we obtain the weak convergence of the sequential process under essentially the same mild assumptions as known for the classical empirical process. Core ingredients of the proofs are a novel maximal inequality for nonmeasurable stochastic processes, uniform chaining arguments and, for the set-indexed smoothed process, uniform Lipschitz properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01635v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Alexander Scholze, Ansgar Steland</dc:creator>
    </item>
    <item>
      <title>Asymptotic Optimism of Random-Design Linear and Kernel Regression Models</title>
      <link>https://arxiv.org/abs/2502.12999</link>
      <description>arXiv:2502.12999v3 Announce Type: replace-cross 
Abstract: We derived the closed-form asymptotic optimism of linear regression models under random designs, and generalizes it to kernel ridge regression. Using scaled asymptotic optimism as a generic predictive model complexity measure, we studied the fundamental different behaviors of linear regression model, tangent kernel (NTK) regression model and three-layer fully connected neural networks (NN). Our contribution is two-fold: we provided theoretical ground for using scaled optimism as a model predictive complexity measure; and we show empirically that NN with ReLUs behaves differently from kernel models under this measure. With resampling techniques, we can also compute the optimism for regression models with real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12999v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hengrui Luo, Yunzhang Zhu</dc:creator>
    </item>
    <item>
      <title>Existence and non-existence of consistent estimators in supercritical controlled branching processes</title>
      <link>https://arxiv.org/abs/2504.03389</link>
      <description>arXiv:2504.03389v2 Announce Type: replace-cross 
Abstract: We consider the problem of estimating the parameters of a supercritical controlled branching process consistently from a single observed trajectory of population size counts. Our goal is to establish which parameters can and cannot be consistently estimated. When a parameter can be consistently estimated, we derive an explicit expression for the estimator. We address these questions in three scenarios: when the distribution of the control function distribution is known, when it is unknown, and when progenitor numbers are observed alongside population size counts. Our results offer a theoretical justification for the common practice in population ecology of estimating demographic and environmental stochasticity using separate observation schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03389v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Braunsteins, Sophie Hautphenne, James Kerlidis</dc:creator>
    </item>
    <item>
      <title>Learning from Samples: Inverse Problems over measures via Sharpened Fenchel-Young Losses</title>
      <link>https://arxiv.org/abs/2505.07124</link>
      <description>arXiv:2505.07124v2 Announce Type: replace-cross 
Abstract: Estimating parameters from samples of an optimal probability distribution is essential in applications ranging from socio-economic modeling to biological system analysis. In these settings, the probability distribution arises as the solution to an optimization problem that captures either static interactions among agents or the dynamic evolution of a system over time. We introduce a general methodology based on a new class of loss functions, called sharpened Fenchel-Young losses, which measure the sub-optimality gap of the optimization problem over the space of probability measures. We provide explicit stability guarantees for two relevant settings in the context of optimal transport: The first is inverse unbalanced optimal transport (iUOT) with entropic regularization, where the parameters to estimate are cost functions that govern transport computations; this method has applications such as link prediction in machine learning. The second is inverse gradient flow (iJKO), where the objective is to recover a potential function that drives the evolution of a probability distribution via the Jordan-Kinderlehrer-Otto (JKO) time-discretization scheme; this is particularly relevant for understanding cell population dynamics in single-cell genomics. We also establish source conditions to ensure stability of our method under mirror stratifiable regularizers (such as l1 or nuclear norm) that promote structure. Finally, we present optimization algorithms specifically tailored to efficiently solve iUOT and iJKO problems. We validate our approach through numerical experiments on Gaussian distributions, where closed-form solutions are available, to demonstrate the practical performance of our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07124v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francisco Andrade, Gabriel Peyr\'e, Clarice Poon</dc:creator>
    </item>
  </channel>
</rss>
