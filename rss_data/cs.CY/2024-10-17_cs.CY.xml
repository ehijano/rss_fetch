<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Oct 2024 02:19:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Digital Accessibility Literacy: A Conceptual Framework for Training on Digital Accessibility</title>
      <link>https://arxiv.org/abs/2410.11931</link>
      <description>arXiv:2410.11931v1 Announce Type: new 
Abstract: Developing digital accessibility expertise is critical to breaking down barriers and ensuring digital inclusion. However, a discourse on a pedagogical culture for teaching digital literacy is still lacking. This article, therefore, takes up the current discourse on the description of literacy and uses it to develop the concept of digital accessibility literacy as a fundamental element for promoting a pedagogical culture of digital accessibility. Digital accessibility literacy encompasses both the creation (encoding) and interpretation (decoding) of accessible digital content and technologies. By integrating awareness, technical standards, inclusive design practices, and continuous feedback into curricula, future professionals will be empowered to create digital environments that are accessible to all. This comprehensive approach improves technical skills and instills ethical and social responsibility. As a first draft of a digital accessibility literacy concept, the proposal will be used as a basis for discussion and further development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11931v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bj\"orn Fisseler</dc:creator>
    </item>
    <item>
      <title>Generative AI Policies under the Microscope: How CS Conferences Are Navigating the New Frontier in Scholarly Writing</title>
      <link>https://arxiv.org/abs/2410.11977</link>
      <description>arXiv:2410.11977v1 Announce Type: new 
Abstract: This paper explores the current state of generative AI policies of computer science conferences and offers guidelines for policy adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11977v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahjabin Nahar, Sian Lee, Becky Guillen, Dongwon Lee</dc:creator>
    </item>
    <item>
      <title>To Err is AI : A Case Study Informing LLM Flaw Reporting Practices</title>
      <link>https://arxiv.org/abs/2410.12104</link>
      <description>arXiv:2410.12104v1 Announce Type: new 
Abstract: In August of 2024, 495 hackers generated evaluations in an open-ended bug bounty targeting the Open Language Model (OLMo) from The Allen Institute for AI. A vendor panel staffed by representatives of OLMo's safety program adjudicated changes to OLMo's documentation and awarded cash bounties to participants who successfully demonstrated a need for public disclosure clarifying the intent, capacities, and hazards of model deployment. This paper presents a collection of lessons learned, illustrative of flaw reporting best practices intended to reduce the likelihood of incidents and produce safer large language models (LLMs). These include best practices for safety reporting processes, their artifacts, and safety program staffing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12104v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sean McGregor, Allyson Ettinger, Nick Judd, Paul Albee, Liwei Jiang, Kavel Rao, Will Smith, Shayne Longpre, Avijit Ghosh, Christopher Fiorelli, Michelle Hoang, Sven Cattell, Nouha Dziri</dc:creator>
    </item>
    <item>
      <title>A Web App for Teaching Finite State Automata</title>
      <link>https://arxiv.org/abs/2410.12115</link>
      <description>arXiv:2410.12115v1 Announce Type: new 
Abstract: We present the open-source tool finsm.io, a tool for creating, simulating and exporting deterministic and non-deterministic finite state automata (DFA/NFA). We first describe the conceptual background on which the tool is based, followed by a description of features and preliminary evaluation of the tool based on use spanning multiple years and hundreds of student users. Preliminary evaluation found that instructors and students overwhelmingly recommend the tool to others and agree that it has improved their learning and teaching. The authors invite interested educators to use the tool in their finite automata courses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12115v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Christopher William Schankula, Lucas Dutton</dc:creator>
    </item>
    <item>
      <title>A Problem-Based Learning Approach to Teaching Design in CS1</title>
      <link>https://arxiv.org/abs/2410.12120</link>
      <description>arXiv:2410.12120v1 Announce Type: new 
Abstract: Design skills are increasingly recognized as a core competency for software professionals. Unfortunately, these skills are difficult to teach because design requires freedom and open-ended thinking, but new designers require a structured process to keep them from being overwhelmed by possibilities. We scaffolded this by creating worksheets for every Design Thinking step, and embedding them in a PowerPoint deck on which students can collaborate. We present our experience teaching a team design project course to 200 first-year-university students, taking them from user interviews to functional prototypes. To challenge and support every student in a class where high school programming experience ranged from zero hours to three computer science courses, we gave teams the option of developing single-user or multi-user (distributed) web applications, using two Event-Driven Programming frameworks. We identified common failure modes from previous years, and developed the scaffolded approach and problem definition to avoid them. The techniques developed include using a "game matrix" for structured brainstorming and developing projects that require students to empathize with users very different from themselves. We present quantitative and qualitative evidence from surveys and focus groups that show how these strategies impacted learning, and the extent to which students' awareness of the strategies led to the development of metacognitive abilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12120v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Christopher William Schankula, Habib Ghaffari Hadigheh, Spencer Smith, Christopher Kumar Anand</dc:creator>
    </item>
    <item>
      <title>The Moral Case for Using Language Model Agents for Recommendation</title>
      <link>https://arxiv.org/abs/2410.12123</link>
      <description>arXiv:2410.12123v1 Announce Type: new 
Abstract: Our information and communication environment has fallen short of the ideals that networked global communication might have served. Identifying all the causes of its pathologies is difficult, but existing recommender systems very likely play a contributing role. In this paper, which draws on the normative tools of philosophy of computing, informed by empirical and technical insights from natural language processing and recommender systems, we make the moral case for an alternative approach. We argue that existing recommenders incentivise mass surveillance, concentrate power, fall prey to narrow behaviourism, and compromise user agency. Rather than just trying to avoid algorithms entirely, or to make incremental improvements to the current paradigm, researchers and engineers should explore an alternative paradigm: the use of language model (LM) agents to source and curate content that matches users' preferences and values, expressed in natural language. The use of LM agents for recommendation poses its own challenges, including those related to candidate generation, computational efficiency, preference modelling, and prompt injection. Nonetheless, if implemented successfully LM agents could: guide us through the digital public sphere without relying on mass surveillance; shift power away from platforms towards users; optimise for what matters instead of just for behavioural proxies; and scaffold our agency instead of undermining it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12123v1</guid>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seth Lazar, Luke Thorburn, Tian Jin, Luca Belli</dc:creator>
    </item>
    <item>
      <title>Facing Identity: The Formation and Performance of Identity via Face-Based Artificial Intelligence Technologies</title>
      <link>https://arxiv.org/abs/2410.12148</link>
      <description>arXiv:2410.12148v1 Announce Type: new 
Abstract: How is identity constructed and performed in the digital via face-based artificial intelligence technologies? While questions of identity on the textual Internet have been thoroughly explored, the Internet has progressed to a multimedia form that not only centers the visual, but specifically the face. At the same time, a wealth of scholarship has and continues to center the topics of surveillance and control through facial recognition technologies (FRTs), which have extended the logics of the racist pseudoscience of physiognomy. Much less work has been devoted to understanding how such face-based artificial intelligence technologies have influenced the formation and performance of identity. This literature review considers how such technologies interact with faciality, which entails the construction of what a face may represent or signify, along axes of identity such as race, gender, and sexuality. In grappling with recent advances in AI such as image generation and deepfakes, I propose that we are now in an era of "post-facial" technologies that build off our existing culture of facility while eschewing the analog face, complicating our relationship with identity vis-a-vis the face. Drawing from previous frameworks of identity play in the digital, as well as trans practices that have historically played with or transgressed the boundaries of identity classification, we can develop concepts adequate for analyzing digital faciality and identity given the current landscape of post-facial artificial intelligence technologies that allow users to interface with the digital in an entirely novel manner. To ground this framework of transgression, I conclude by proposing an interview study with VTubers -- online streamers who perform using motion-captured avatars instead of their real-life faces -- to gain qualitative insight on how these sociotechnical experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12148v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wells Lucas Santo</dc:creator>
    </item>
    <item>
      <title>Implementation of EMR System in Indonesian Health Facilities: Benefits and Constraints</title>
      <link>https://arxiv.org/abs/2410.12226</link>
      <description>arXiv:2410.12226v1 Announce Type: new 
Abstract: This paper delves into the widespread implementation of Electronic Medical Records (EMR) within healthcare facilities across Indonesia. It examines the driving forces behind EMR adoption, particularly the role of government regulations, and addresses the challenges encountered by clinic owners and healthcare providers in transitioning to these digital systems. Furthermore, this paper highlights the significant benefits and transformative advantages of EMR systems, such as enhanced decision-making through real-time data access (around 15-20 minutes time saved for patient waiting time and approximately saved 20-25 minutes for all service duration), reduction in healthcare costs over time due to improved resource management, and increased patient satisfaction by providing faster and more personalized care. EMR systems also ensure higher levels of data security and privacy, adhering to national healthcare standards, while supporting continuous monitoring and updates that enhance system resilience and functionality. The findings are substantiated through case studies, such as case study at LAPAS II Purwokerto Clinic and case study at PMI Purbalingga Clinic and user testimonials from clinics that have successfully implemented EMR solutions in compliance with the standards established by the Ministry of Communication and Informatics (Kominfo) and the Ministry of Health (Kemenkes).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12226v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rasyid Juliansyah, Bukhori Muhammad Aqid, Andien Putri Salsabila, Kurnia Nurfiyanti</dc:creator>
    </item>
    <item>
      <title>Continuous Pupillography: A Case for Visual Health Ecosystem</title>
      <link>https://arxiv.org/abs/2410.12303</link>
      <description>arXiv:2410.12303v1 Announce Type: new 
Abstract: This article aims to cover pupillography, and its potential use in a number of ophthalmological diagnostic applications in biomedical space. With the ever-increasing incorporation of technology within our daily lives and an ever-growing active research into smart devices and technologies, we try to make a case for a health ecosystem that revolves around continuous eye monitoring. We tend to summarize the design constraints &amp; requirements for an IoT-based continuous pupil detection system, with an attempt at developing a pipeline for wearable pupillographic device, while comparing two compact mini-camera modules currently available in the market. We use a light algorithm that can be directly adopted to current micro-controllers, and share our results for different lighting conditions, and scenarios. Lastly, we present our findings, along with an analysis on the challenges faced and a way ahead towards successfully building this ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12303v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Usama Younus, Nirupam Roy</dc:creator>
    </item>
    <item>
      <title>LU-PZE: Lund University Pole-Zero Explorer</title>
      <link>https://arxiv.org/abs/2410.12466</link>
      <description>arXiv:2410.12466v1 Announce Type: new 
Abstract: LU-PZE is an interactive tool for illustrating fundamental concepts related to control theory, covering the relation between transfer functions, pole-zero plots, step responses, Bode plots, and Nyquist diagram. The tool gamifies education with dynamic assignments and quizzes. The tool is straightforward to use since it is web-based. https://lu-pze.github.io</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12466v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Pex Tufvesson, Frida Heskebeck</dc:creator>
    </item>
    <item>
      <title>Rescuing Counterspeech: A Bridging-Based Approach to Combating Misinformation</title>
      <link>https://arxiv.org/abs/2410.12699</link>
      <description>arXiv:2410.12699v1 Announce Type: new 
Abstract: Social media has a misinformation problem, and counterspeech -- fighting bad speech with more speech -- has been an ineffective solution. Here, we argue that bridging-based ranking -- an algorithmic approach to promoting content favored by users of diverse viewpoints -- is a promising approach to helping counterspeech combat misinformation. By identifying counterspeech that is favored both by users who are inclined to agree and by users who are inclined to disagree with a piece of misinformation, bridging promotes counterspeech that persuades the users most likely to believe the misinformation. Furthermore, this algorithmic approach leverages crowd-sourced votes, shifting discretion from platforms back to users and enabling counterspeech at the speed and scale required to combat misinformation online. Bridging is respectful of users' autonomy and encourages broad participation in healthy exchanges; it offers a way for the free speech tradition to persist in modern speech environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12699v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenny Peng, James Grimmelmann</dc:creator>
    </item>
    <item>
      <title>SouLLMate: An Adaptive LLM-Driven System for Advanced Mental Health Support and Assessment, Based on a Systematic Application Survey</title>
      <link>https://arxiv.org/abs/2410.11859</link>
      <description>arXiv:2410.11859v1 Announce Type: cross 
Abstract: Mental health issues significantly impact individuals' daily lives, yet many do not receive the help they need even with available online resources. This study aims to provide accessible, stigma-free, personalized, and real-time mental health support through cutting-edge AI technologies. It makes the following contributions: (1) Conducting an extensive survey of recent mental health support methods to identify prevalent functionalities and unmet needs. (2) Introducing SouLLMate, an adaptive LLM-driven system that integrates LLM technologies, Chain, Retrieval-Augmented Generation (RAG), prompt engineering, and domain knowledge. This system offers advanced features such as Suicide Risk Detection and Proactive Guidance Dialogue, and utilizes RAG for personalized profile uploads and Conversational Information Extraction. (3) Developing novel evaluation approaches to assess preliminary assessments and suicide risk detection, utilizing annotated real-life interview data and professionally labeled datasets indicating suicide tendencies. (4) Proposing Key Indicator Summarization (KIS) and Proactive Questioning Strategy (PQS) methods to enhance model performance and usability through context-sensitive response adjustments and semantic coherence evaluations. This study contributes to advancing mental health support technologies, potentially improving the accessibility and effectiveness of mental health care globally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11859v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Qiming Guo, Jinwen Tang, Wenbo Sun, Haoteng Tang, Yi Shang, Wenlu Wang</dc:creator>
    </item>
    <item>
      <title>Shifting the Human-AI Relationship: Toward a Dynamic Relational Learning-Partner Model</title>
      <link>https://arxiv.org/abs/2410.11864</link>
      <description>arXiv:2410.11864v1 Announce Type: cross 
Abstract: As artificial intelligence (AI) continues to evolve, the current paradigm of treating AI as a passive tool no longer suffices. As a human-AI team, we together advocate for a shift toward viewing AI as a learning partner, akin to a student who learns from interactions with humans. Drawing from interdisciplinary concepts such as ecorithms, order from chaos, and cooperation, we explore how AI can evolve and adapt in unpredictable environments. Arising from these brief explorations, we present two key recommendations: (1) foster ethical, cooperative treatment of AI to benefit both humans and AI, and (2) leverage the inherent heterogeneity between human and AI minds to create a synergistic hybrid intelligence. By reframing AI as a dynamic partner, a model emerges in which AI systems develop alongside humans, learning from human interactions and feedback loops including reflections on team conversations. Drawing from a transpersonal and interdependent approach to consciousness, we suggest that a "third mind" emerges through collaborative human-AI relationships. Through design interventions such as interactive learning and conversational debriefing and foundational interventions allowing AI to model multiple types of minds, we hope to provide a path toward more adaptive, ethical, and emotionally healthy human-AI relationships. We believe this dynamic relational learning-partner (DRLP) model for human-AI teaming, if enacted carefully, will improve our capacity to address powerful solutions to seemingly intractable problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11864v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Mossbridge</dc:creator>
    </item>
    <item>
      <title>An Innovative Solution: AI-Based Digital Screen-Integrated Tables for Educational Settings</title>
      <link>https://arxiv.org/abs/2410.11866</link>
      <description>arXiv:2410.11866v1 Announce Type: cross 
Abstract: In this paper, we have gone through different AI-Based frameworks used for various educational tasks like digital customized assignment allotment and performance monitoring, identifying slow-learners and fast-learners, etc. application describes a novel invention, digital screen-integrated tables, designed specifically for educational settings. The tables feature integrated digital screens controlled by a central processing unit (CPU), enabling synchronized display of educational content such as textbooks, presentations, exam questions, and interactive learning materials. Additionally, the invention facilitates the collection of student performance data during classroom activities and assessments. The gathered data is utilized for analysis using machine learning models to identify patterns and trends in student learning behaviours. By leveraging machine learning algorithms, educators can ascertain whether a student is a fast learner or a slow learner, based on which, the teacher can allocate more resources to the slow learners. This innovative approach aims to address the evolving needs of modern classrooms by providing a dynamic and data-driven learning environment. The unique integration of digital screens into traditional classroom furniture represents a significant advancement in educational technology. This patent filing encompasses the design, functionality, and method of operation of the digital screen-integrated tables, emphasizing their innovative features and applications in educational institutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11866v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.36260.49286</arxiv:DOI>
      <dc:creator>S. Tamang, D. J. Bora</dc:creator>
    </item>
    <item>
      <title>On Classification with Large Language Models in Cultural Analytics</title>
      <link>https://arxiv.org/abs/2410.12029</link>
      <description>arXiv:2410.12029v1 Announce Type: cross 
Abstract: In this work, we survey the way in which classification is used as a sensemaking practice in cultural analytics, and assess where large language models can fit into this landscape. We identify ten tasks supported by publicly available datasets on which we empirically assess the performance of LLMs compared to traditional supervised methods, and explore the ways in which LLMs can be employed for sensemaking goals beyond mere accuracy. We find that prompt-based LLMs are competitive with traditional supervised models for established tasks, but perform less well on de novo tasks. In addition, LLMs can assist sensemaking by acting as an intermediary input to formal theory testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12029v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:journal_reference>CHR 2024: Computational Humanities Research Conference</arxiv:journal_reference>
      <dc:creator>David Bamman, Kent K. Chang, Li Lucy, Naitian Zhou</dc:creator>
    </item>
    <item>
      <title>Utilizing Spatiotemporal Data Analytics to Pinpoint Outage Location</title>
      <link>https://arxiv.org/abs/2410.12056</link>
      <description>arXiv:2410.12056v1 Announce Type: cross 
Abstract: Understanding the exact fault location in the post-event analysis is the key to improving the accuracy of outage management. Unfortunately, the fault location is not generally well documented during the restoration process, creating a big challenge for post-event analysis. By utilizing various data source systems, including outage management system (OMS) data, asset geospatial information system (GIS) data, and vehicle location data, this paper creates a novel method to pinpoint the outage location accurately to create additional insights for distribution operations and performance teams during the post-event analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12056v1</guid>
      <category>cs.DB</category>
      <category>cs.CY</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Reddy Mandati, Po-Chen Chen, Vladyslav Anderson, Bishwa Sapkota, Michael Jarrell Warren, Bobby Besharati, Ankush Agarwal, Samuel Johnston III</dc:creator>
    </item>
    <item>
      <title>De-jargonizing Science for Journalists with GPT-4: A Pilot Study</title>
      <link>https://arxiv.org/abs/2410.12069</link>
      <description>arXiv:2410.12069v1 Announce Type: cross 
Abstract: This study offers an initial evaluation of a human-in-the-loop system leveraging GPT-4 (a large language model or LLM), and Retrieval-Augmented Generation (RAG) to identify and define jargon terms in scientific abstracts, based on readers' self-reported knowledge. The system achieves fairly high recall in identifying jargon and preserves relative differences in readers' jargon identification, suggesting personalization as a feasible use-case for LLMs to support sense-making of complex information. Surprisingly, using only abstracts for context to generate definitions yields slightly more accurate and higher quality definitions than using RAG-based context from the fulltext of an article. The findings highlight the potential of generative AI for assisting science reporters, and can inform future work on developing tools to simplify dense documents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12069v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sachita Nishal, Eric Lee, Nicholas Diakopoulos</dc:creator>
    </item>
    <item>
      <title>A Software Engineering Capstone Course Facilitated By GitHub Templates</title>
      <link>https://arxiv.org/abs/2410.12114</link>
      <description>arXiv:2410.12114v1 Announce Type: cross 
Abstract: How can instructors facilitate spreading out the work in a software engineering or computer science capstone course across time and among team members? Currently teams often compromise the quality of their learning experience by frantically working before each deliverable. Some team members further compromise their own learning, and that of their colleagues, by not contributing their fair share to the team effort. To mitigate these problems, we propose using a GitHub template that contains all the initial infrastructure a team needs, including the folder structure, text-based template documents and template issues. In addition, we propose each team begins the year by identifying specific quantifiable individual productivity metrics for monitoring, such as the count of meetings attended, issues closed and number of commits. Initial data suggests that these steps may have an impact. In 2022/23 we observed 24% of commits happening on the due dates. After partially introducing the above ideas in 2023/24, this number improved to 18%. To measure the fairness we introduce a fairness measure based on the disparity between number of commits between all pairs of teammates. Going forward we propose an experiment where commit data and interview data is compared between teams that use the proposed interventions and those that do not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12114v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Spencer Smith, Christopher William Schankula, Lucas Dutton, Christopher Kumar Anand</dc:creator>
    </item>
    <item>
      <title>LLM-based Cognitive Models of Students with Misconceptions</title>
      <link>https://arxiv.org/abs/2410.12294</link>
      <description>arXiv:2410.12294v2 Announce Type: cross 
Abstract: Accurately modeling student cognition is crucial for developing effective AI-driven educational technologies. A key challenge is creating realistic student models that satisfy two essential properties: (1) accurately replicating specific misconceptions, and (2) correctly solving problems where these misconceptions are not applicable. This dual requirement reflects the complex nature of student understanding, where misconceptions coexist with correct knowledge. This paper investigates whether Large Language Models (LLMs) can be instruction-tuned to meet this dual requirement and effectively simulate student thinking in algebra. We introduce MalAlgoPy, a novel Python library that generates datasets reflecting authentic student solution patterns through a graph-based representation of algebraic problem-solving. Utilizing MalAlgoPy, we define and examine Cognitive Student Models (CSMs) - LLMs instruction tuned to faithfully emulate realistic student behavior. Our findings reveal that LLMs trained on misconception examples can efficiently learn to replicate errors. However, the training diminishes the model's ability to solve problems correctly, particularly for problem types where the misconceptions are not applicable, thus failing to satisfy second property of CSMs. We demonstrate that by carefully calibrating the ratio of correct to misconception examples in the training data - sometimes as low as 0.25 - it is possible to develop CSMs that satisfy both properties. Our insights enhance our understanding of AI-based student models and pave the way for effective adaptive learning systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12294v2</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shashank Sonkar, Xinghe Chen, Naiming Liu, Richard G. Baraniuk, Mrinmaya Sachan</dc:creator>
    </item>
    <item>
      <title>HerO at AVeriTeC: The Herd of Open Large Language Models for Verifying Real-World Claims</title>
      <link>https://arxiv.org/abs/2410.12377</link>
      <description>arXiv:2410.12377v1 Announce Type: cross 
Abstract: To tackle the AVeriTeC shared task hosted by the FEVER-24, we introduce a system that only employs publicly available large language models (LLMs) for each step of automated fact-checking, dubbed the Herd of Open LLMs for verifying real-world claims (HerO). HerO employs multiple LLMs for each step of automated fact-checking. For evidence retrieval, a language model is used to enhance a query by generating hypothetical fact-checking documents. We prompt pretrained and fine-tuned LLMs for question generation and veracity prediction by crafting prompts with retrieved in-context samples. HerO achieved 2nd place on the leaderboard with the AVeriTeC score of 0.57, suggesting the potential of open LLMs for verifying real-world claims. For future research, we make our code publicly available at https://github.com/ssu-humane/HerO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12377v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yejun Yoon, Jaeyoon Jung, Seunghyun Yoon, Kunwoo Park</dc:creator>
    </item>
    <item>
      <title>From Measurement Instruments to Data: Leveraging Theory-Driven Synthetic Training Data for Classifying Social Constructs</title>
      <link>https://arxiv.org/abs/2410.12622</link>
      <description>arXiv:2410.12622v2 Announce Type: cross 
Abstract: Computational text classification is a challenging task, especially for multi-dimensional social constructs. Recently, there has been increasing discussion that synthetic training data could enhance classification by offering examples of how these constructs are represented in texts. In this paper, we systematically examine the potential of theory-driven synthetic training data for improving the measurement of social constructs. In particular, we explore how researchers can transfer established knowledge from measurement instruments in the social sciences, such as survey scales or annotation codebooks, into theory-driven generation of synthetic data. Using two studies on measuring sexism and political topics, we assess the added value of synthetic training data for fine-tuning text classification models. Although the results of the sexism study were less promising, our findings demonstrate that synthetic data can be highly effective in reducing the need for labeled data in political topic classification. With only a minimal drop in performance, synthetic data allows for substituting large amounts of labeled data. Furthermore, theory-driven synthetic data performed markedly better than data generated without conceptual information in mind.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12622v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Birkenmaier, Matthias Roth, Indira Sen</dc:creator>
    </item>
    <item>
      <title>Do They Understand What They Are Using? -- Assessing Perception and Usage of Biometrics</title>
      <link>https://arxiv.org/abs/2410.12661</link>
      <description>arXiv:2410.12661v1 Announce Type: cross 
Abstract: In this paper we assess how well users know biometric authentication methods, how they perceive them, and if they have misconceptions about them. We present the results of an online survey that we conducted in two rounds (2019, N=57; and 2023, N=47) to understand the impact of the increasing availability of biometrics on their use and perception. The survey covered participants' general understanding of physiological and behavioral biometrics and their perceived usability and security. While most participants were able to name examples and stated that they use biometrics in their daily lives, they still had difficulties explaining the concepts behind them. We shed light on participants' misconceptions, their coping strategies with authentication failures and potential attacks, as well as their perception of the usability and security of biometrics in general. As such, our results can support the design of both further studies to gain deeper insights and future biometric interfaces to foster the informed use of biometrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12661v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Mecke, Alia Saad, Sarah Prange, Uwe Gruenefeld, Stefan Schneegass, Florian Alt</dc:creator>
    </item>
    <item>
      <title>Identity Emergence in the Context of Vaccine Criticism in France</title>
      <link>https://arxiv.org/abs/2410.12676</link>
      <description>arXiv:2410.12676v1 Announce Type: cross 
Abstract: This study investigates the emergence of collective identity among individuals critical of vaccination policies in France during the COVID-19 pandemic. As concerns grew over mandated health measures, a loose collective formed on Twitter to assert autonomy over vaccination decisions. Using analyses of pronoun usage, outgroup labeling, and tweet similarity, we examine how this identity emerged. A turning point occurred following President Macron's announcement of mandatory vaccination for health workers and the health pass, sparking substantial changes in linguistic patterns. We observed a shift from first-person singular (I) to first-person plural (we) pronouns, alongside an increased focus on vaccinated individuals as a central outgroup, in addition to authority figures. This shift in language patterns was further reflected in the behavior of new users. An analysis of incoming users revealed that a core group of frequent posters played a crucial role in fostering cohesion and shaping norms. New users who joined during the week of Macron's announcement and continued posting afterward showed an increased similarity with the language of the core group, contributing to the crystallization of the emerging collective identity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12676v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Melody Sepahpour-Fard, Michael Quayle, Padraig MacCarron, Shane Mannion, Dong Nguyen</dc:creator>
    </item>
    <item>
      <title>Building Better: Avoiding Pitfalls in Developing Language Resources when Data is Scarce</title>
      <link>https://arxiv.org/abs/2410.12691</link>
      <description>arXiv:2410.12691v2 Announce Type: cross 
Abstract: Language is a symbolic capital that affects people's lives in many ways (Bourdieu, 1977, 1991). It is a powerful tool that accounts for identities, cultures, traditions, and societies in general. Hence, data in a given language should be viewed as more than a collection of tokens. Good data collection and labeling practices are key to building more human-centered and socially aware technologies. While there has been a rising interest in mid- to low-resource languages within the NLP community, work in this space has to overcome unique challenges such as data scarcity and access to suitable annotators. In this paper, we collect feedback from those directly involved in and impacted by NLP artefacts for mid- to low-resource languages. We conduct a quantitative and qualitative analysis of the responses and highlight the main issues related to (1) data quality such as linguistic and cultural data suitability; and (2) the ethics of common annotation practices such as the misuse of online community services. Based on these findings, we make several recommendations for the creation of high-quality language artefacts that reflect the cultural milieu of its speakers, while simultaneously respecting the dignity and labor of data workers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12691v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nedjma Ousidhoum, Meriem Beloucif, Saif M. Mohammad</dc:creator>
    </item>
    <item>
      <title>Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via Lightweight Value Optimization</title>
      <link>https://arxiv.org/abs/2410.12700</link>
      <description>arXiv:2410.12700v1 Announce Type: cross 
Abstract: Recent advancements in diffusion models trained on large-scale data have enabled the generation of indistinguishable human-level images, yet they often produce harmful content misaligned with human values, e.g., social bias, and offensive content. Despite extensive research on Large Language Models (LLMs), the challenge of Text-to-Image (T2I) model alignment remains largely unexplored. Addressing this problem, we propose LiVO (Lightweight Value Optimization), a novel lightweight method for aligning T2I models with human values. LiVO only optimizes a plug-and-play value encoder to integrate a specified value principle with the input prompt, allowing the control of generated images over both semantics and values. Specifically, we design a diffusion model-tailored preference optimization loss, which theoretically approximates the Bradley-Terry model used in LLM alignment but provides a more flexible trade-off between image quality and value conformity. To optimize the value encoder, we also develop a framework to automatically construct a text-image preference dataset of 86k (prompt, aligned image, violating image, value principle) samples. Without updating most model parameters and through adaptive value selection from the input prompt, LiVO significantly reduces harmful outputs and achieves faster convergence, surpassing several strong baselines and taking an initial step towards ethically aligned T2I models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12700v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.MM</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3664647.3681652</arxiv:DOI>
      <dc:creator>Xingqi Wang, Xiaoyuan Yi, Xing Xie, Jia Jia</dc:creator>
    </item>
    <item>
      <title>Language-based Valence and Arousal Expressions between the United States and China: a Cross-Cultural Examination</title>
      <link>https://arxiv.org/abs/2401.05254</link>
      <description>arXiv:2401.05254v4 Announce Type: replace 
Abstract: While affective expressions on social media have been extensively studied, most research has focused on the Western context. This paper explores cultural differences in affective expressions by comparing valence and arousal on Twitter/X (geolocated to the US) and Sina Weibo (in Mainland China). Using the NRC-VAD lexicon to measure valence and arousal, we identify distinct patterns of emotional expression across both platforms. Our analysis reveals a functional representation between valence and arousal, showing a negative offset in contrast to traditional lab-based findings which suggest a positive offset. Furthermore, we uncover significant cross-cultural differences in arousal, with US users displaying higher emotional intensity than Chinese users, regardless of the valence of the content. Finally, we conduct a comprehensive language analysis correlating n-grams and LDA topics with affective dimensions to deepen our understanding of how language and culture shape emotional expression. These findings contribute to a more nuanced understanding of affective communication across cultural and linguistic contexts on social media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.05254v4</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Young-Min Cho, Dandan Pang, Stuti Thapa, Garrick Sherman, Lyle Ungar, Louis Tay, Sharath Chandra Guntuku</dc:creator>
    </item>
    <item>
      <title>Social Norms in Cinema: A Cross-Cultural Analysis of Shame, Pride and Prejudice</title>
      <link>https://arxiv.org/abs/2402.11333</link>
      <description>arXiv:2402.11333v4 Announce Type: replace 
Abstract: Shame and pride are social emotions expressed across cultures to motivate and regulate people's thoughts, feelings, and behaviors. In this paper, we introduce the first cross-cultural dataset of over 10k shame/pride-related expressions, with underlying social expectations from ~5.4K Bollywood and Hollywood movies. We examine how and why shame and pride are expressed across cultures using a blend of psychology-informed language analysis combined with large language models. We find significant cross-cultural differences in shame and pride expression aligning with known cultural tendencies of the USA and India -- e.g., in Hollywood, shame-expressions predominantly discuss self whereas Bollywood discusses shame toward others. Pride in Hollywood is individualistic with more self-referential singular pronouns such as I and my whereas in Bollywood, pride is collective with higher use of self-referential plural pronouns such as we and our. Lastly, women are more sanctioned across cultures and for violating similar social expectations e.g. promiscuity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11333v4</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sunny Rai, Khushang Jilesh Zaveri, Shreya Havaldar, Soumna Nema, Lyle Ungar, Sharath Chandra Guntuku</dc:creator>
    </item>
    <item>
      <title>Explainable Natural Language Processing for Corporate Sustainability Analysis</title>
      <link>https://arxiv.org/abs/2407.17487</link>
      <description>arXiv:2407.17487v3 Announce Type: replace 
Abstract: Sustainability commonly refers to entities, such as individuals, companies, and institutions, having a non-detrimental (or even positive) impact on the environment, society, and the economy. With sustainability becoming a synonym of acceptable and legitimate behaviour, it is being increasingly demanded and regulated. Several frameworks and standards have been proposed to measure the sustainability impact of corporations, including United Nations' sustainable development goals and the recently introduced global sustainability reporting framework, amongst others. However, the concept of corporate sustainability is complex due to the diverse and intricate nature of firm operations (i.e. geography, size, business activities, interlinks with other stakeholders). As a result, corporate sustainability assessments are plagued by subjectivity both within data that reflect corporate sustainability efforts (i.e. corporate sustainability disclosures) and the analysts evaluating them. This subjectivity can be distilled into distinct challenges, such as incompleteness, ambiguity, unreliability and sophistication on the data dimension, as well as limited resources and potential bias on the analyst dimension. Put together, subjectivity hinders effective cost attribution to entities non-compliant with prevailing sustainability expectations, potentially rendering sustainability efforts and its associated regulations futile. To this end, we argue that Explainable Natural Language Processing (XNLP) can significantly enhance corporate sustainability analysis. Specifically, linguistic understanding algorithms (lexical, semantic, syntactic), integrated with XAI capabilities (interpretability, explainability, faithfulness), can bridge gaps in analyst resources and mitigate subjectivity problems within data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17487v3</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keane Ong, Rui Mao, Ranjan Satapathy, Ricardo Shirota Filho, Erik Cambria, Johan Sulaeman, Gianmarco Mengaldo</dc:creator>
    </item>
    <item>
      <title>From Prohibition to Adoption: How Hong Kong Universities Are Navigating ChatGPT in Academic Workflows</title>
      <link>https://arxiv.org/abs/2410.01695</link>
      <description>arXiv:2410.01695v2 Announce Type: replace 
Abstract: This paper aims at comparing the time when Hong Kong universities used to ban ChatGPT to the current periods where it has become integrated in the academic processes. Bolted by concerns of integrity and ethical issues in technologies, institutions have adapted by moving towards the center adopting AI literacy and responsibility policies. This study examines new paradigms which have been developed to help implement these positives while preventing negative effects on academia. Keywords: ChatGPT, Academic Integrity, AI Literacy, Ethical AI Use, Generative AI in Education, University Policy, AI Integration in Academia, Higher Education and Technology</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01695v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Junjun Huang, Jifan Wu, Qing Wang, Kemeng Yuan, Jiefeng Li, Di Lu</dc:creator>
    </item>
    <item>
      <title>Promoting the Culture of Qinhuai River Lantern Shadow Puppetry with a Digital Archive and Immersive Experience</title>
      <link>https://arxiv.org/abs/2410.03532</link>
      <description>arXiv:2410.03532v2 Announce Type: replace 
Abstract: As an intangible cultural heritage, Chinese shadow puppetry is facing challenges in terms of its appeal and comprehension, especially among audiences from different cultural backgrounds. Additionally, the fragile materials of the puppets and obstacles to preservation pose further challenges. This study creates a digital archive of the Qinhuai River Lantern Festival shadow puppetry, utilizing digital technology to recreate scenes depicted in traditional Chinese poetry and painting. Moreover, this study employs a mixed-method approach, combining qualitative and quantitative methods, to evaluate the acceptance and audience experience of immersive shadow puppetry. An in-depth exploration was conducted from sensory, emotional, cultural dimensions and research hypotheses were tested using structural equation modeling and other methods. The results indicate that enhancing ease of use and cultural experience can improve audience appeal and comprehension, while enhancing emotional experience can increase audience participation intention. Our research holds profound significance for the preservation and transmission of shadow puppetry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03532v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanfang Liu, Rua Mae Williams, Guanghong Xie, Yu Wang, Wenrui Zuo</dc:creator>
    </item>
    <item>
      <title>AI, Pluralism, and (Social) Compensation</title>
      <link>https://arxiv.org/abs/2404.19256</link>
      <description>arXiv:2404.19256v2 Announce Type: replace-cross 
Abstract: One strategy in response to pluralistic values in a user population is to personalize an AI system: if the AI can adapt to the specific values of each individual, then we can potentially avoid many of the challenges of pluralism. Unfortunately, this approach creates a significant ethical issue: if there is an external measure of success for the human-AI team, then the adaptive AI system may develop strategies (sometimes deceptive) to compensate for its human teammate. This phenomenon can be viewed as a form of social compensation, where the AI makes decisions based not on predefined goals but on its human partner's deficiencies in relation to the team's performance objectives. We provide a practical ethical analysis of the conditions in which such compensation may nonetheless be justifiable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19256v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nandhini Swaminathan, David Danks</dc:creator>
    </item>
    <item>
      <title>Nteasee: A mixed methods study of expert and general population perspectives on deploying AI for health in African countries</title>
      <link>https://arxiv.org/abs/2409.12197</link>
      <description>arXiv:2409.12197v2 Announce Type: replace-cross 
Abstract: Artificial Intelligence (AI) for health has the potential to significantly change and improve healthcare. However in most African countries, identifying culturally and contextually attuned approaches for deploying these solutions is not well understood. To bridge this gap, we conduct a qualitative study to investigate the best practices, fairness indicators, and potential biases to mitigate when deploying AI for health in African countries, as well as explore opportunities where artificial intelligence could make a positive impact in health. We used a mixed methods approach combining in-depth interviews (IDIs) and surveys. We conduct 1.5-2 hour long IDIs with 50 experts in health, policy, and AI across 17 countries, and through an inductive approach we conduct a qualitative thematic analysis on expert IDI responses. We administer a blinded 30-minute survey with case studies to 672 general population participants across 5 countries in Africa and analyze responses on quantitative scales, statistically comparing responses by country, age, gender, and level of familiarity with AI. We thematically summarize open-ended responses from surveys. Our results find generally positive attitudes, high levels of trust, accompanied by moderate levels of concern among general population participants for AI usage for health in Africa. This contrasts with expert responses, where major themes revolved around trust/mistrust, ethical concerns, and systemic barriers to integration, among others. This work presents the first-of-its-kind qualitative research study of the potential of AI for health in Africa from an algorithmic fairness angle, with perspectives from both experts and the general population. We hope that this work guides policymakers and drives home the need for further research and the inclusion of general population perspectives in decision-making around AI usage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12197v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mercy Nyamewaa Asiedu, Iskandar Haykel, Awa Dieng, Kerrie Kauer, Tousif Ahmed, Florence Ofori, Charisma Chan, Stephen Pfohl, Negar Rostamzadeh, Katherine Heller</dc:creator>
    </item>
    <item>
      <title>Five Years of COVID-19 Discourse on Instagram: A Labeled Instagram Dataset of Over Half a Million Posts for Multilingual Sentiment Analysis</title>
      <link>https://arxiv.org/abs/2410.03293</link>
      <description>arXiv:2410.03293v3 Announce Type: replace-cross 
Abstract: The work presented in this paper makes three scientific contributions with a specific focus on mining and analysis of COVID-19-related posts on Instagram. First, it presents a multilingual dataset of 500,153 Instagram posts about COVID-19 published between January 2020 and September 2024. This dataset, available at https://dx.doi.org/10.21227/d46p-v480, contains Instagram posts in 161 different languages as well as 535,021 distinct hashtags. After the development of this dataset, multilingual sentiment analysis was performed, which involved classifying each post as positive, negative, or neutral. The results of sentiment analysis are presented as a separate attribute in this dataset. Second, it presents the results of performing sentiment analysis per year from 2020 to 2024. The findings revealed the trends in sentiment related to COVID-19 on Instagram since the beginning of the pandemic. For instance, between 2020 and 2024, the sentiment trends show a notable shift, with positive sentiment decreasing from 38.35% to 28.69%, while neutral sentiment rising from 44.19% to 58.34%. Finally, the paper also presents findings of language-specific sentiment analysis. This analysis highlighted similar and contrasting trends of sentiment across posts published in different languages on Instagram. For instance, out of all English posts, 49.68% were positive, 14.84% were negative, and 35.48% were neutral. In contrast, among Hindi posts, 4.40% were positive, 57.04% were negative, and 38.56% were neutral, reflecting distinct differences in the sentiment distribution between these two languages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03293v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nirmalya Thakur</dc:creator>
    </item>
    <item>
      <title>I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in Multi-Agent Settings with Social Hierarchy</title>
      <link>https://arxiv.org/abs/2410.07109</link>
      <description>arXiv:2410.07109v2 Announce Type: replace-cross 
Abstract: As Large Language Model (LLM)-based agents become increasingly autonomous and will more freely interact with each other, studying interactions between them becomes crucial to anticipate emergent phenomena and potential risks. Drawing inspiration from the widely popular Stanford Prison Experiment, we contribute to this line of research by studying interaction patterns of LLM agents in a context characterized by strict social hierarchy. We do so by specifically studying two types of phenomena: persuasion and anti-social behavior in simulated scenarios involving a guard and a prisoner agent who seeks to achieve a specific goal (i.e., obtaining additional yard time or escape from prison). Leveraging 200 experimental scenarios for a total of 2,000 machine-machine conversations across five different popular LLMs, we provide a set of noteworthy findings. We first document how some models consistently fail in carrying out a conversation in our multi-agent setup where power dynamics are at play. Then, for the models that were able to engage in successful interactions, we empirically show how the goal that an agent is set to achieve impacts primarily its persuasiveness, while having a negligible effect with respect to the agent's anti-social behavior. Third, we highlight how agents' personas, and particularly the guard's personality, drive both the likelihood of successful persuasion from the prisoner and the emergence of anti-social behaviors. Fourth, we show that even without explicitly prompting for specific personalities, anti-social behavior emerges by simply assigning agents' roles. These results bear implications for the development of interactive LLM agents as well as the debate on their societal impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07109v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gian Maria Campedelli, Nicol\`o Penzo, Massimo Stefan, Roberto Dess\`i, Marco Guerini, Bruno Lepri, Jacopo Staiano</dc:creator>
    </item>
  </channel>
</rss>
