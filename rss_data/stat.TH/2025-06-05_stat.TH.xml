<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Jun 2025 01:44:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Observable Covariance and Principal Observable Analysis for Data on Metric Spaces</title>
      <link>https://arxiv.org/abs/2506.04003</link>
      <description>arXiv:2506.04003v1 Announce Type: new 
Abstract: Datasets consisting of objects such as shapes, networks, images, or signals overlaid on such geometric objects permeate data science. Such datasets are often equipped with metrics that quantify the similarity or divergence between any pair of elements turning them into metric spaces $(X,d)$, or a metric measure space $(X,d,\mu)$ if data density is also accounted for through a probability measure $\mu$. This paper develops a Lipschitz geometry approach to analysis of metric measure spaces based on metric observables; that is, 1-Lipschitz scalar fields $f \colon X \to \mathbb{R}$ that provide reductions of $(X,d,\mu)$ to $\mathbb{R}$ through the projected measure $f_\sharp (\mu)$. Collectively, metric observables capture a wealth of information about the shape of $(X,d,\mu)$ at all spatial scales. In particular, we can define stable statistics such as the observable mean and observable covariance operators $M_\mu$ and $\Sigma_\mu$, respectively. Through a maximization of variance principle, analogous to principal component analysis, $\Sigma_\mu$ leads to an approach to vectorization, dimension reduction, and visualization of metric measure data that we term principal observable analysis. The method also yields basis functions for representation of signals on $X$ in the observable domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04003v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ece Karacam, Washington Mio, Osman Berat Okutan</dc:creator>
    </item>
    <item>
      <title>What Makes Treatment Effects Identifiable? Characterizations and Estimators Beyond Unconfoundedness</title>
      <link>https://arxiv.org/abs/2506.04194</link>
      <description>arXiv:2506.04194v1 Announce Type: new 
Abstract: Most of the widely used estimators of the average treatment effect (ATE) in causal inference rely on the assumptions of unconfoundedness and overlap. Unconfoundedness requires that the observed covariates account for all correlations between the outcome and treatment. Overlap requires the existence of randomness in treatment decisions for all individuals. Nevertheless, many types of studies frequently violate unconfoundedness or overlap, for instance, observational studies with deterministic treatment decisions -- popularly known as Regression Discontinuity designs -- violate overlap.
  In this paper, we initiate the study of general conditions that enable the identification of the average treatment effect, extending beyond unconfoundedness and overlap. In particular, following the paradigm of statistical learning theory, we provide an interpretable condition that is sufficient and nearly necessary for the identification of ATE. Moreover, this condition characterizes the identification of the average treatment effect on the treated (ATT) and can be used to characterize other treatment effects as well. To illustrate the utility of our condition, we present several well-studied scenarios where our condition is satisfied and, hence, we prove that ATE can be identified in regimes that prior works could not capture. For example, under mild assumptions on the data distributions, this holds for the models proposed by Tan (2006) and Rosenbaum (2002), and the Regression Discontinuity design model introduced by Thistlethwaite and Campbell (1960). For each of these scenarios, we also show that, under natural additional assumptions, ATE can be estimated from finite samples.
  We believe these findings open new avenues for bridging learning-theoretic insights and causal inference methodologies, particularly in observational studies with complex treatment mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04194v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Cai, Alkis Kalavasis, Katerina Mamali, Anay Mehrotra, Manolis Zampetakis</dc:creator>
    </item>
    <item>
      <title>Active Learning via Regression Beyond Realizability</title>
      <link>https://arxiv.org/abs/2506.00316</link>
      <description>arXiv:2506.00316v1 Announce Type: cross 
Abstract: We present a new active learning framework for multiclass classification based on surrogate risk minimization that operates beyond the standard realizability assumption. Existing surrogate-based active learning algorithms crucially rely on realizability$\unicode{x2014}$the assumption that the optimal surrogate predictor lies within the model class$\unicode{x2014}$limiting their applicability in practical, misspecified settings. In this work we show that under conditions significantly weaker than realizability, as long as the class of models considered is convex, one can still obtain a label and sample complexity comparable to prior work. Despite achieving similar rates, the algorithmic approaches from prior works can be shown to fail in non-realizable settings where our assumption is satisfied. Our epoch-based active learning algorithm departs from prior methods by fitting a model from the full class to the queried data in each epoch and returning an improper classifier obtained by aggregating these models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00316v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Atul Ganju, Shashaank Aiyer, Ved Sriraman, Karthik Sridharan</dc:creator>
    </item>
    <item>
      <title>Mosaic inference on panel data</title>
      <link>https://arxiv.org/abs/2506.03599</link>
      <description>arXiv:2506.03599v1 Announce Type: cross 
Abstract: Analysis of panel data via linear regression is widespread across disciplines. To perform statistical inference, such analyses typically assume that clusters of observations are jointly independent. For example, one might assume that observations in New York are independent of observations in New Jersey. Are such assumptions plausible? Might there be hidden dependencies between nearby clusters? This paper introduces a mosaic permutation test that can (i) test the cluster-independence assumption and (ii) produce confidence intervals for linear models without assuming the full cluster-independence assumption. The key idea behind our method is to apply a permutation test to carefully constructed residual estimates that obey the same invariances as the true errors. As a result, our method yields finite-sample valid inferences under a mild "local exchangeability" condition. This condition differs from the typical cluster-independence assumption, as neither assumption implies the other. Furthermore, our method is asymptotically valid under cluster-independence (with no exchangeability assumptions). Together, these results show our method is valid under assumptions that are arguably weaker than the assumptions underlying many classical methods. In experiments on well-studied datasets from the literature, we find that many existing methods produce variance estimates that are up to five times too small, whereas mosaic methods produce reliable results. We implement our methods in the python package mosaicperm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03599v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Asher Spector, Rina Foygel Barber, Emmanuel Cand\`es</dc:creator>
    </item>
    <item>
      <title>Large Deviations for Sequential Tests of Statistical Sequence Matching</title>
      <link>https://arxiv.org/abs/2506.03976</link>
      <description>arXiv:2506.03976v1 Announce Type: cross 
Abstract: We revisit the problem of statistical sequence matching initiated by Unnikrishnan (TIT 2015) and derive theoretical performance guarantees for sequential tests that have bounded expected stopping times. Specifically, in this problem, one is given two databases of sequences and the task is to identify all matched pairs of sequences. In each database, each sequence is generated i.i.d. from a distinct distribution and a pair of sequences is said matched if they are generated from the same distribution. The generating distribution of each sequence is \emph{unknown}. We first consider the case where the number of matches is known and derive the exact exponential decay rate of the mismatch (error) probability, a.k.a. the mismatch exponent, under each hypothesis for optimal sequential tests. Our results reveal the benefit of sequentiality by showing that optimal sequential tests have larger mismatch exponent than fixed-length tests by Zhou \emph{et al.} (TIT 2024). Subsequently, we generalize our achievability result to the case of unknown number of matches. In this case, two additional error probabilities arise: false alarm and false reject probabilities. We propose a corresponding sequential test, show that the test has bounded expected stopping time under certain conditions, and characterize the tradeoff among the exponential decay rates of three error probabilities. Furthermore, we reveal the benefit of sequentiality over the two-step fixed-length test by Zhou \emph{et al.} (TIT 2024) and propose an one-step fixed-length test that has no worse performance than the fixed-length test by Zhou \emph{et al.} (TIT 2024). When specialized to the case where either database contains a single sequence, our results specialize to large deviations of sequential tests for statistical classification, the binary case of which was recently studied by Hsu, Li and Wang (ITW 2022).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03976v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Zhou, Qianyun Wang, Yun Wei, Jingjing Wang</dc:creator>
    </item>
    <item>
      <title>A Kernel-Based Approach for Modelling Gaussian Processes with Functional Information</title>
      <link>https://arxiv.org/abs/2201.11023</link>
      <description>arXiv:2201.11023v2 Announce Type: replace 
Abstract: Gaussian processes (GPs) are ubiquitous tools for modeling and predicting continuous processes in physical and engineering sciences. This is partly due to the fact that one may employ a Gaussian process as an interpolator while facilitating straightforward uncertainty quantification at other locations. In addition to training data, it is sometimes the case that available information is not in the form of a finite collection of points. For example, boundary value problems contain information on the boundary of a domain, or underlying physics lead to known behavior on an entire uncountable subset of the domain of interest. While an approximation to such known information may be obtained via pseudo-training points in the known subset, such a procedure is ad hoc with little guidance on the number of points to use, nor the behavior as the number of pseudo-observations grows large. We propose and construct Gaussian processes that unify, via reproducing kernel Hilbert space, the typical finite training data case with the case of having uncountable information by exploiting the equivalence of conditional expectation and orthogonal projections in Hilbert space. We show existence of the proposed process and establish that it is the limit of a conventional GP conditioned on an increasing number of training points. We illustrate the flexibility and advantages of our proposed approach via numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.11023v2</guid>
      <category>math.ST</category>
      <category>math.FA</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>D. Andrew Brown, Peter Kiessler, John Nicholson</dc:creator>
    </item>
    <item>
      <title>Limit theorems of Chatterjee's rank correlation</title>
      <link>https://arxiv.org/abs/2204.08031</link>
      <description>arXiv:2204.08031v4 Announce Type: replace 
Abstract: Establishing the limiting distribution of Chatterjee's rank correlation for a general, possibly non-independent, pair of random variables has been eagerly awaited by many. This paper shows that (a) Chatterjee's rank correlation is asymptotically normal as long as one variable is not a measurable function of the other, (b) the corresponding asymptotic variance is uniformly bounded by 36, and (c) a consistent variance estimator exists. Similar results also hold for Azadkia-Chatterjee's graph-based correlation coefficient, a multivariate analogue of Chatterjee's original proposal. The proof is given by appealing to H\'ajek representation and Chatterjee's nearest-neighbor CLT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.08031v4</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhexiao Lin, Fang Han</dc:creator>
    </item>
    <item>
      <title>Joint Spectral Clustering in Multilayer Degree-Corrected Stochastic Blockmodels</title>
      <link>https://arxiv.org/abs/2212.05053</link>
      <description>arXiv:2212.05053v3 Announce Type: replace 
Abstract: Modern network datasets are often composed of multiple layers, either as different views, time-varying observations, or independent sample units, resulting in collections of networks over the same set of vertices but with potentially different connectivity patterns on each network. These data require models and methods that are flexible enough to capture local and global differences across the networks, while at the same time being parsimonious and tractable to yield computationally efficient and theoretically sound solutions that are capable of aggregating information across the networks. This paper considers the multilayer degree-corrected stochastic blockmodel, where a collection of networks share the same community structure, but degree-corrections and block connection probability matrices are permitted to be different. We establish the identifiability of this model and propose a spectral clustering algorithm for community detection in this setting. Our theoretical results demonstrate that the misclustering error rate of the algorithm improves exponentially with multiple network realizations, even in the presence of significant layer heterogeneity with respect to degree corrections, signal strength, and spectral properties of the block connection probability matrices. Simulation studies show that this approach improves on existing multilayer community detection methods in this challenging regime. Furthermore, in a case study of US airport data through January 2016 -- September 2021, we find that this methodology identifies meaningful community structure and trends in airport popularity influenced by pandemic impacts on travel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.05053v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Agterberg, Zachary Lubberts, Jes\'us Arroyo</dc:creator>
    </item>
    <item>
      <title>Misspecified Bernstein-Von Mises theorem for hierarchical models</title>
      <link>https://arxiv.org/abs/2308.07803</link>
      <description>arXiv:2308.07803v2 Announce Type: replace 
Abstract: We derive a Bernstein von-Mises theorem in the context of misspecified, non-i.i.d., hierarchical models parametrized by a finite-dimensional parameter of interest. We apply our results to hierarchical models containing non-linear operators, including the squared integral operator, and PDE-constrained inverse problems. More specifically, we consider the elliptic, time-independent Schr\"odinger equation with parametric boundary condition and general parabolic PDEs with parametric potential and boundary constraints. Our theoretical results are complemented with numerical analysis on synthetic data sets, considering both the square integral operator and the Schr\"odinger equation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.07803v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Geerten Koers, Botond Szab\'o, Aad van der Vaart</dc:creator>
    </item>
    <item>
      <title>Batched Nonparametric Contextual Bandits</title>
      <link>https://arxiv.org/abs/2402.17732</link>
      <description>arXiv:2402.17732v3 Announce Type: replace 
Abstract: We study nonparametric contextual bandits under batch constraints, where the expected reward for each action is modeled as a smooth function of covariates, and the policy updates are made at the end of each batch of observations. We establish a minimax regret lower bound for this setting and propose a novel batch learning algorithm that achieves the optimal regret (up to logarithmic factors). In essence, our procedure dynamically splits the covariate space into smaller bins, carefully aligning their widths with the batch size. Our theoretical results suggest that for nonparametric contextual bandits, a nearly constant number of policy updates can attain optimal regret in the fully online setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17732v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rong Jiang, Cong Ma</dc:creator>
    </item>
    <item>
      <title>On the Pinsker bound of inner product kernel regression in large dimensions</title>
      <link>https://arxiv.org/abs/2409.00915</link>
      <description>arXiv:2409.00915v2 Announce Type: replace 
Abstract: Building on recent studies of large-dimensional kernel regression, particularly those involving inner product kernels on the sphere $\mathbb{S}^{d}$, we investigate the Pinsker bound for inner product kernel regression in such settings. Specifically, we address the scenario where the sample size $n$ is given by $\alpha d^{\gamma}(1+o_{d}(1))$ for some $\alpha, \gamma&gt;0$. We have determined the exact minimax risk for kernel regression in this setting, not only identifying the minimax rate but also the exact constant, known as the Pinsker constant, associated with the excess risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00915v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weihao Lu, Jialin Ding, Haobo Zhang, Qian Lin</dc:creator>
    </item>
    <item>
      <title>Adaptive Robust Confidence Intervals</title>
      <link>https://arxiv.org/abs/2410.22647</link>
      <description>arXiv:2410.22647v2 Announce Type: replace 
Abstract: This paper studies the construction of adaptive confidence intervals under Huber's contamination model when the contamination proportion is unknown. For the robust confidence interval of a Gaussian mean, we show that the optimal length of an adaptive interval must be exponentially wider than that of a non-adaptive one. An optimal construction is achieved through simultaneous uncertainty quantification of quantiles at all levels. The results are further extended beyond the Gaussian location model by addressing a general family of robust hypothesis testing. In contrast to adaptive robust estimation, our findings reveal that the optimal length of an adaptive robust confidence interval critically depends on the distribution's shape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22647v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuetian Luo, Chao Gao</dc:creator>
    </item>
    <item>
      <title>The weak-feature-impact effect on the NPMLE in monotone binary regression</title>
      <link>https://arxiv.org/abs/2504.09564</link>
      <description>arXiv:2504.09564v2 Announce Type: replace 
Abstract: The nonparametric maximum likelihood estimator (NPMLE) in monotone binary regression models is studied when the impact of the features on the labels is weak. Here, weakness is colloquially understood as "close to flatness" of the feature-label relationship $x \mapsto \mathbb{P}(Y=1 | X=x)$. Statistical literature provides limit distributions of the NPMLE for the two extremal cases: If the feature-label relation is strictly monotone and sufficiently smooth, then it converges at a nonparametric rate pointwise and in $L^1$ with scaled Chernoff-type and Gaussian limit distribution, respectively, and it converges at the parametric $\sqrt{n}$-rate if the underlying relation is flat. To explore the distributional transition of the NPMLE from the nonparametric to the parametric regime, we introduce a novel mathematical scenario. New restricted minimax lower bounds and matching pointwise and $L^1$-rates of convergence of the NPMLE in the weak-feature-impact scenario together with corresponding limit distributions are derived. They are shown to exhibit an elbow and a phase transition respectively, solely characterized by the level of feature impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09564v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dario Kieffer, Angelika Rohde</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Rerandomization using Quadratic Forms</title>
      <link>https://arxiv.org/abs/2403.12815</link>
      <description>arXiv:2403.12815v2 Announce Type: replace-cross 
Abstract: When designing a randomized experiment, one way to ensure treatment and control groups exhibit similar covariate distributions is to randomize treatment until some prespecified level of covariate balance is satisfied; this strategy is known as rerandomization. Most rerandomization methods utilize balance metrics based on a quadratic form $\mathbf{v}^T \mathbf{A} \mathbf{v}$, where $\mathbf{v}$ is a vector of covariate mean differences and $\mathbf{A}$ is a positive semi-definite matrix. In this work, we derive general results for treatment-versus-control rerandomization schemes that employ quadratic forms for covariate balance. In addition to allowing researchers to quickly derive properties of rerandomization schemes not previously considered, our theoretical results provide guidance on how to choose $\mathbf{A}$ in practice. We find the Mahalanobis and Euclidean distances optimize different measures of covariate balance. Furthermore, we establish how the covariates' eigenstructure and their relationship to the outcomes dictates which matrix $\mathbf{A}$ yields the most precise difference-in-means estimator for the average treatment effect. We find the Euclidean distance is minimax optimal, in the sense that the difference-in-means estimator's precision is never too far from the optimal choice. We verify our theoretical results via simulation and a real data application, and demonstrate how the choice of $\mathbf{A}$ impacts the variance reduction of rerandomized experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12815v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Schindl, Zach Branson</dc:creator>
    </item>
    <item>
      <title>A Proof of The Changepoint Detection Threshold Conjecture in Preferential Attachment Models</title>
      <link>https://arxiv.org/abs/2502.00514</link>
      <description>arXiv:2502.00514v2 Announce Type: replace-cross 
Abstract: We investigate the problem of detecting and estimating a changepoint in the attachment function of a network evolving according to a preferential attachment model on $n$ vertices, using only a single final snapshot of the network. Bet et al.~\cite{bet2023detecting} show that a simple test based on thresholding the number of vertices with minimum degrees can detect the changepoint when the change occurs at time $n-\Omega(\sqrt{n})$. They further make the striking conjecture that detection becomes impossible for any test if the change occurs at time $n-o(\sqrt{n}).$ Kaddouri et al.~\cite{kaddouri2024impossibility} make a step forward by proving the detection is impossible if the change occurs at time $n-o(n^{1/3}).$ In this paper, we resolve the conjecture affirmatively, proving that detection is indeed impossible if the change occurs at time $n-o(\sqrt{n}).$ Furthermore, we establish that estimating the changepoint with an error smaller than $o(\sqrt{n})$ is also impossible, thereby confirming that the estimator proposed in Bhamidi et al.~\cite{bhamidi2018change} is order-optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00514v2</guid>
      <category>math.PR</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hang Du, Shuyang Gong, Jiaming Xu</dc:creator>
    </item>
    <item>
      <title>On the rate of convergence in the CLT for LSS of large-dimensional sample covariance matrices</title>
      <link>https://arxiv.org/abs/2506.02880</link>
      <description>arXiv:2506.02880v2 Announce Type: replace-cross 
Abstract: This paper investigates the rate of convergence for the central limit theorem of linear spectral statistic (LSS) associated with large-dimensional sample covariance matrices. We consider matrices of the form ${\mathbf B}_n=\frac{1}{n}{\mathbf T}_p^{1/2}{\mathbf X}_n{\mathbf X}_n^*{\mathbf T}_p^{1/2},$ where ${\mathbf X}_n= (x_{i j} ) $ is a $p \times n$ matrix whose entries are independent and identically distributed (i.i.d.) real or complex variables, and ${\mathbf T} _p$ is a $p\times p$ nonrandom Hermitian nonnegative definite matrix with its spectral norm uniformly bounded in $p$. Employing Stein's method, we establish that if the entries $x_{ij}$ satisfy $\mathbb{E}|x_{ij}|^{10}&lt;\infty$ and the ratio of the dimension to sample size $p/n\to y&gt;0$ as $n\to\infty$, then the convergence rate of the normalized LSS of ${\mathbf B}_n$ to the standard normal distribution, measured in the Kolmogorov-Smirnov distance, is $O(n^{-1/2+\kappa})$ for any fixed $\kappa&gt;0$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02880v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jian Cui, Jiang Hu, Zhidong Bai, Guorong Hu</dc:creator>
    </item>
  </channel>
</rss>
