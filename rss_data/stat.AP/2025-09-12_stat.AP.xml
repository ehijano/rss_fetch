<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Sep 2025 04:01:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Feature Representation and Clustering of Airport Congestion with Hurst Exponent and High Order Statistics</title>
      <link>https://arxiv.org/abs/2509.08952</link>
      <description>arXiv:2509.08952v1 Announce Type: new 
Abstract: Air traffic controllers benefit from referencing historical dates with similar complex air traffic conditions to identify potential management measures and their effects, which is critical for understanding air transportation system laws and optimizing decisions. This study conducted data mining using flight timetables. It first explored airport congestion mechanisms and quantified congestion as time series, then proposed a higher-order cumulants based time series feature extraction method. This method was fused with other features to build a high-dimensional airport congestion feature vector, and finally K-means clustering was applied to extract and analyze congestion patterns. The clustering method was empirically validated with 2023 flight data from Guangzhou Baiyun International Airport and it accurately classified airport operational states. To verify universality, the same framework was applied to 6 airports under the "one-city, two-airports" layout in Beijing, Shanghai and Chengdu. Results showed significant congestion pattern differences between existing and newly constructed airports. Conclusions confirm the proposed feature extraction and clustering framework is effective and universal, and it can accurately capture airport congestion dynamics. Under the "one-city, two-airports" layout, existing and newly constructed airports differ significantly in operational modes, and most single-airport city airports have operational modes highly consistent with existing airports. This study provides valuable decision-making references for airport managers and air traffic controllers. It helps them deepen understanding of air traffic dynamics and airport congestion patterns, thereby optimizing traffic management strategies and improving airport operational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08952v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Sun, Zi-Feng Yi, Zhi-Qiang Feng, Ji Ma, Ruo-shi Yang</dc:creator>
    </item>
    <item>
      <title>A Zero-Inflated Spatio-Temporal Model for Integrating Fishery-Dependent and Independent Data under Preferential Sampling</title>
      <link>https://arxiv.org/abs/2509.09336</link>
      <description>arXiv:2509.09336v1 Announce Type: new 
Abstract: Sustainable management of marine ecosystems is vital for maintaining healthy fishery resources, and benefits from advanced scientific tools to accurately assess species distribution patterns. In fisheries science, two primary data sources are used: fishery-independent data (FID), collected through systematic surveys, and fishery-dependent data (FDD), obtained from commercial fishing activities. While these sources provide complementary information, their distinct sampling schemes - systematic for FID and preferential for FDD - pose significant integration challenges. This study introduces a novel spatio-temporal model that integrates FID and FDD, addressing challenges associated with zero-inflation and preferential sampling (PS) common in ecological data. The model employs a six-layer structure to differentiate between presence-absence and biomass observations, offering a robust framework for ecological studies affected by PS biases. Simulation results demonstrate the model's accuracy in parameter estimation across diverse PS scenarios and its ability to detect preferential signals. Application to the study of the distribution patterns of the European sardine populations along the southern Portuguese continental shelf illustrates the model's effectiveness in integrating diverse data sources and incorporating environmental and vessel-specific covariates. The model reveals spatio-temporal variability in sardine presence and biomass, providing actionable insights for fisheries management. Beyond ecology, this framework offers broad applicability to data integration challenges in other disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09336v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Daniela Silva, Raquel Menezes, Gon\c{c}alo Ara\'ujo, Ana Machado, Renato Rosa, Ana Moreno, Alexandra Silva, Susana Garrido</dc:creator>
    </item>
    <item>
      <title>Measuring football fever through wearable technology: A case study on the German cup final</title>
      <link>https://arxiv.org/abs/2509.09569</link>
      <description>arXiv:2509.09569v1 Announce Type: new 
Abstract: Football is the world's most popular sport, evoking strong physiological and emotional responses among its fans. Yet, the specific dynamics of fan attachment to matches have received little attention in the literature. In this paper, we quantify these dynamics through a unique case study from professional football: the 2025 cup final of the German Football Association (DFB) between first-division club VfB Stuttgart and third-division club Arminia Bielefeld. We collected high-resolution smartwatch data, including heart rate and stress level, from 229 Arminia Bielefeld fans over approximately 12 weeks, complemented by survey responses on club attachment, match attendance, and personal characteristics from a subset of 37 participants. By combining physiological data with survey information, we analyse variations in emotional engagement across individuals and contexts, as well as physiological reactions to key match events. This approach provides rare, data-driven insights into the football fever that captivates fans during high-stakes competitions. Furthermore, we compare the vital parameters recorded on the day of the match with baseline levels on non-matchdays throughout the entire observation period. Our findings reveal pronounced physiological responses among fans, beginning hours before the match and peaking at kick-off.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09569v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timo Adam, Jonas Bauer, Christian Deutscher, Christiane Fuchs, Tamara Schamberger, David Winkelmann</dc:creator>
    </item>
    <item>
      <title>Documents Are People and Words Are Items: A Psychometric Approach to Textual Data with Contextual Embeddings</title>
      <link>https://arxiv.org/abs/2509.08920</link>
      <description>arXiv:2509.08920v1 Announce Type: cross 
Abstract: This research introduces a novel psychometric method for analyzing textual data using large language models. By leveraging contextual embeddings to create contextual scores, we transform textual data into response data suitable for psychometric analysis. Treating documents as individuals and words as items, this approach provides a natural psychometric interpretation under the assumption that certain keywords, whose contextual meanings vary significantly across documents, can effectively differentiate documents within a corpus. The modeling process comprises two stages: obtaining contextual scores and performing psychometric analysis. In the first stage, we utilize natural language processing techniques and encoder based transformer models to identify common keywords and generate contextual scores. In the second stage, we employ various types of factor analysis, including exploratory and bifactor models, to extract and define latent factors, determine factor correlations, and identify the most significant words associated with each factor. Applied to the Wiki STEM corpus, our experimental results demonstrate the method's potential to uncover latent knowledge dimensions and patterns within textual data. This approach not only enhances the psychometric analysis of textual data but also holds promise for applications in fields rich in textual information, such as education, psychology, and law.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08920v1</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jinsong Chen</dc:creator>
    </item>
    <item>
      <title>Scalable extensions to given-data Sobol' index estimators</title>
      <link>https://arxiv.org/abs/2509.09078</link>
      <description>arXiv:2509.09078v1 Announce Type: cross 
Abstract: Given-data methods for variance-based sensitivity analysis have significantly advanced the feasibility of Sobol' index computation for computationally expensive models and models with many inputs. However, the limitations of existing methods still preclude their application to models with an extremely large number of inputs. In this work, we present practical extensions to the existing given-data Sobol' index method, which allow variance-based sensitivity analysis to be efficiently performed on large models such as neural networks, which have $&gt;10^4$ parameterizable inputs. For models of this size, holding all input-output evaluations simultaneously in memory -- as required by existing methods -- can quickly become impractical. These extensions also support nonstandard input distributions with many repeated values, which are not amenable to equiprobable partitions employed by existing given-data methods.
  Our extensions include a general definition of the given-data Sobol' index estimator with arbitrary partition, a streaming algorithm to process input-output samples in batches, and a heuristic to filter out small indices that are indistinguishable from zero indices due to statistical noise. We show that the equiprobable partition employed in existing given-data methods can introduce significant bias into Sobol' index estimates even at large sample sizes and provide numerical analyses that demonstrate why this can occur. We also show that our streaming algorithm can achieve comparable accuracy and runtimes with lower memory requirements, relative to current methods which process all samples at once. We demonstrate our novel developments on two application problems in neural network modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09078v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Teresa Portone, Bert Debusschere, Samantha Yang, Emiliano Islas-Quinones, T. Patrick Xiao</dc:creator>
    </item>
    <item>
      <title>LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death Determination</title>
      <link>https://arxiv.org/abs/2509.09602</link>
      <description>arXiv:2509.09602v1 Announce Type: cross 
Abstract: Verbal autopsy (VA) is a critical tool for estimating causes of death in resource-limited settings where medical certification is unavailable. This study presents LA-VA, a proof-of-concept pipeline that combines Large Language Models (LLMs) with traditional algorithmic approaches and embedding-based classification for improved cause-of-death prediction. Using the Population Health Metrics Research Consortium (PHMRC) dataset across three age categories (Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches: GPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles. Our results demonstrate that GPT-5 achieves the highest individual performance with average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5% (Neonate), outperforming traditional statistical machine learning baselines by 5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches could substantially improve verbal autopsy accuracy, with important implications for global health surveillance in low-resource settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09602v1</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiqun T. Chen, Tyler H. McCormick, Li Liu, Abhirup Datta</dc:creator>
    </item>
    <item>
      <title>Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid Care Management</title>
      <link>https://arxiv.org/abs/2509.09655</link>
      <description>arXiv:2509.09655v1 Announce Type: cross 
Abstract: We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning (FG-FARL), an offline RL procedure that calibrates per-group safety thresholds to reduce harm while equalizing a chosen fairness target (coverage or harm) across protected subgroups. Using de-identified longitudinal trajectories from a Medicaid population health management program, we evaluate FG-FARL against behavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global conformal safety baseline). We report off-policy value estimates with bootstrap 95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL achieves comparable value to baselines while improving fairness metrics, demonstrating a practical path to safer and more equitable decision support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09655v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <category>stat.AP</category>
      <pubDate>Fri, 12 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanjay Basu, Sadiq Y. Patel, Parth Sheth, Bhairavi Muralidharan, Namrata Elamaran, Aakriti Kinra, Rajaie Batniji</dc:creator>
    </item>
  </channel>
</rss>
