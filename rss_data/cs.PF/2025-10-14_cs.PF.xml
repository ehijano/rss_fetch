<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 15 Oct 2025 01:47:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>CAPSim: A Fast CPU Performance Simulator Using Attention-based Predictor</title>
      <link>https://arxiv.org/abs/2510.10484</link>
      <description>arXiv:2510.10484v1 Announce Type: new 
Abstract: CPU simulators are vital for computer architecture research, primarily for estimating performance under different programs. This poses challenges for fast and accurate simulation of modern CPUs, especially in multi-core systems. Modern CPU peformance simulators such as GEM5 adopt the cycle-accurate and event-driven approach, which is timeconsuming to simulate the extensive microarchitectural behavior of a real benchmark running on out-of-order CPUs. Recently, machine leaning based approach has been proposed to improve simulation speed, but they are currently limited to estimating the cycles of basic blocks rather than the complete benchmark program. This paper introduces a novel ML-based CPU simulator named CAPSim, which uses an attention-based neural network performance predictor and instruction trace sampling method annotated with context. The attention mechanism effectively captures long-range influence within the instruction trace, emphasizing critical context information. This allows the model to improve performance prediction accuracy by focusing on important code instruction. CAPSim can predict the execution time of unseen benchmarks at a significantly fast speed compared with an accurate O3 simulator built with gem5. Our evaluation on a commercial Intel Xeon CPU demonstrates that CAPSim achieves a 2.2 - 8.3x speedup compared to using gem5 built simulator, which is superior to the cutting-edge deep learning approach</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10484v1</guid>
      <category>cs.PF</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Buqing Xu, Jianfeng Zhu, Yichi Zhang, Qinyi Cai, Guanhua Li, Shaojun Wei, Leibo Liu</dc:creator>
    </item>
    <item>
      <title>Towards Automated and Predictive Network-Level Energy Profiling in Reconfigurable IoT Systems</title>
      <link>https://arxiv.org/abs/2510.09842</link>
      <description>arXiv:2510.09842v1 Announce Type: cross 
Abstract: Energy efficiency has emerged as a defining constraint in the evolution of sustainable Internet of Things (IoT) networks. This work moves beyond simulation-based or device-centric studies to deliver measurement-driven, network-level smart energy analysis. The proposed system enables end-to-end visibility of energy flows across distributed IoT infrastructures, uniting Bluetooth Low Energy (BLE) and Visible Light Communication (VLC) modes with environmental sensing and E-ink display subsystems under a unified profiling and prediction platform. Through automated, time-synchronized instrumentation, the framework captures fine-grained energy dynamics across both node and gateway layers. We developed a suite of tools that generate energy datasets for IoT ecosystems, addressing the scarcity of such data and enabling AI-based predictive and adaptive energy optimization. Validated within a network-level IoT testbed, the approach demonstrates robust performance under real operating conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09842v1</guid>
      <category>cs.NI</category>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammud J. Bocus, Senhui Qiu, Robert J. Piechocki, Kerstin Eder</dc:creator>
    </item>
    <item>
      <title>THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware Resource Scheduling</title>
      <link>https://arxiv.org/abs/2510.09847</link>
      <description>arXiv:2510.09847v1 Announce Type: cross 
Abstract: The dynamic adaptation of resource levels enables the system to enhance energy efficiency while maintaining the necessary computational resources, particularly in scenarios where workloads fluctuate significantly over time. The proposed approach can play a crucial role in heterogeneous systems where workload characteristics are not uniformly distributed, such as non-pinning tasks. The deployed THEAS algorithm in this research work ensures a balance between performance and power consumption, making it suitable for a wide range of real-time applications. A comparative analysis of the proposed THEAS algorithm with well-known scheduling techniques such as Completely Fair Scheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling (HeteroSched), and Utility-Based Scheduling is presented in Table III. Each scheme is compared based on adaptability, core selection criteria, performance scaling, cache awareness, overhead, and real-time suitability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09847v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Said Muhammad, Lahlou Laaziz, Nadjia Kara, Phat Tan Nguyen, Timothy Murphy</dc:creator>
    </item>
    <item>
      <title>LOOPerSet: A Large-Scale Dataset for Data-Driven Polyhedral Compiler Optimization</title>
      <link>https://arxiv.org/abs/2510.10209</link>
      <description>arXiv:2510.10209v1 Announce Type: cross 
Abstract: The advancement of machine learning for compiler optimization, particularly within the polyhedral model, is constrained by the scarcity of large-scale, public performance datasets. This data bottleneck forces researchers to undertake costly data generation campaigns, slowing down innovation and hindering reproducible research learned code optimization. To address this gap, we introduce LOOPerSet, a new public dataset containing 28 million labeled data points derived from 220,000 unique, synthetically generated polyhedral programs. Each data point maps a program and a complex sequence of semantics-preserving transformations (such as fusion, skewing, tiling, and parallelism)to a ground truth performance measurement (execution time). The scale and diversity of LOOPerSet make it a valuable resource for training and evaluating learned cost models, benchmarking new model architectures, and exploring the frontiers of automated polyhedral scheduling. The dataset is released under a permissive license to foster reproducible research and lower the barrier to entry for data-driven compiler optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10209v1</guid>
      <category>cs.PL</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Massinissa Merouani, Afif Boudaoud, Riyadh Baghdadi</dc:creator>
    </item>
    <item>
      <title>CPU-Limits kill Performance: Time to rethink Resource Control</title>
      <link>https://arxiv.org/abs/2510.10747</link>
      <description>arXiv:2510.10747v1 Announce Type: cross 
Abstract: Research in compute resource management for cloud-native applications is dominated by the problem of setting optimal CPU limits -- a fundamental OS mechanism that strictly restricts a container's CPU usage to its specified CPU-limits . Rightsizing and autoscaling works have innovated on allocation/scaling policies assuming the ubiquity and necessity of CPU-limits . We question this. Practical experiences of cloud users indicate that CPU-limits harms application performance and costs more than it helps. These observations are in contradiction to the conventional wisdom presented in both academic research and industry best practices. We argue that this indiscriminate adoption of CPU-limits is driven by erroneous beliefs that CPU-limits is essential for operational and safety purposes. We provide empirical evidence making a case for eschewing CPU-limits completely from latency-sensitive applications. This prompts a fundamental rethinking of auto-scaling and billing paradigms and opens new research avenues. Finally, we highlight specific scenarios where CPU-limits can be beneficial if used in a well-reasoned way (e.g. background jobs).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10747v1</guid>
      <category>cs.DC</category>
      <category>cs.OS</category>
      <category>cs.PF</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chirag Shetty, Sarthak Chakraborty, Hubertus Franke, Larisa Shwartz, Chandra Narayanaswami, Indranil Gupta, Saurabh Jha</dc:creator>
    </item>
    <item>
      <title>A new $1/(1-\rho)$-scaling bound for multiserver queues via a leave-one-out technique</title>
      <link>https://arxiv.org/abs/2510.11015</link>
      <description>arXiv:2510.11015v1 Announce Type: cross 
Abstract: Bounding the queue length in a multiserver queue is a central challenge in queueing theory. Even for the classic $GI/GI/n$ queue with homogeneous servers, it is highly non-trivial to derive a simple and accurate bound for the steady-state queue length that holds across all scaling regimes. A recent breakthrough by Li and Goldberg (2025) establishes bounds that scale as $1/(1-\rho)$ for any load $\rho &lt; 1$ and number of servers $n$, which is the correct scaling in many well-known scaling regimes, including classic heavy-traffic, Halfin-Whitt and Nondegenerate-Slowdown. However, their bounds entail large constant factors and a highly intricate proof, suggesting room for further improvement.
  In this paper, we present a new $1/(1-\rho)$-scaling bound for the $GI/GI/n$ queue. Our bound, while restricted to the light-tailed case and the first moment of the queue length, has a more interpretable and often tighter leading constant. Our proof is relatively simple, utilizing a modified $GI/GI/n$ queue, the stationarity of a quadratic test function, and a novel leave-one-out coupling technique.
  Finally, we also extend our method to $GI/GI/n$ queues with fully heterogeneous service-time distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11015v1</guid>
      <category>math.PR</category>
      <category>cs.PF</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yige Hong</dc:creator>
    </item>
    <item>
      <title>Detection of Performance Changes in MooBench Results Using Nyrki\"o on GitHub Actions</title>
      <link>https://arxiv.org/abs/2510.11310</link>
      <description>arXiv:2510.11310v1 Announce Type: cross 
Abstract: In GitHub with its 518 million hosted projects, performance changes within these projects are highly relevant to the project's users. Although performance measurement is supported by GitHub CI/CD, performance change detection is a challenging topic.
  In this paper, we demonstrate how we incorporated Nyrki\"o to MooBench. Prior to this work, Moobench continuously ran on GitHub virtual machines, measuring overhead of tracing agents, but without change detection. By adding the upload of the measurements to the Nyrki\"o change detection service, we made it possible to detect performance changes. We identified one major performance regression and examined the performance change in depth. We report that (1) it is reproducible with GitHub actions, and (2) the performance regression is caused by a Linux Kernel version change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11310v1</guid>
      <category>cs.SE</category>
      <category>cs.OS</category>
      <category>cs.PF</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shinhyung Yang, David Georg Reichelt, Henrik Ingo, Wilhelm Hasselbring</dc:creator>
    </item>
    <item>
      <title>A protocol to reduce worst-case latency in deflection-based on-chip networks</title>
      <link>https://arxiv.org/abs/2510.11361</link>
      <description>arXiv:2510.11361v1 Announce Type: cross 
Abstract: We present a novel protocol that reduces worst-case packet latency in deflection-based on-chip interconnect networks. It enforces the deflection of the header of a packet but not its payload, resulting in a reduction in overall network traffic and, more importantly, worst-case packet latency due to decreased pre-injection latency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11361v1</guid>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leandro Soares Indrusiak</dc:creator>
    </item>
    <item>
      <title>Neuralink: Fast LLM Inference on Smartphones with Neuron Co-Activation Linking</title>
      <link>https://arxiv.org/abs/2410.19274</link>
      <description>arXiv:2410.19274v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have achieved remarkable success across various domains, yet deploying them on mobile devices remains an arduous challenge due to their extensive computational and memory demands. While lightweight LLMs have been developed to fit mobile environments, they suffer from degraded model accuracy. In contrast, sparsity-based techniques minimize DRAM usage by selectively transferring only relevant neurons to DRAM while retaining the full model in external storage, such as flash. However, such approaches are critically limited by numerous I/O operations, particularly on smartphones with severe IOPS constraints.
  In this paper, we propose Neuralink, a novel approach that accelerates LLM inference on smartphones by optimizing neuron placement in flash memory. Neuralink leverages the concept of Neuron Co-Activation, where neurons frequently activated together are linked to facilitate continuous read access and optimize I/O efficiency. Our approach incorporates a two-stage solution: an offline stage that reorganizes neuron placement based on co-activation patterns, and an online stage that employs tailored data access and caching strategies to align well with hardware characteristics. Evaluations conducted on a variety of smartphones and LLMs demonstrate that Neuralink achieves on average $1.49\times$ improvements in end-to-end latency compared to the state-of-the-art. As the first solution to optimize storage placement under sparsity, Neuralink explores a new optimization space at the intersection of sparsity-driven algorithm and storage-level system co-design for LLM inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19274v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.OS</category>
      <category>cs.PF</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3676642.3736114</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Vol. 3, Rotterdam, Netherlands, 2025, pp. 147-162</arxiv:journal_reference>
      <dc:creator>Tuowei Wang, Ruwen Fan, Minxing Huang, Zixu Hao, Kun Li, Ting Cao, Youyou Lu, Yaoxue Zhang, Ju Ren</dc:creator>
    </item>
    <item>
      <title>Surrogate Modeling for Scalable Evaluation of Distributed Computing Systems for HEP Applications</title>
      <link>https://arxiv.org/abs/2502.12741</link>
      <description>arXiv:2502.12741v2 Announce Type: replace-cross 
Abstract: The Worldwide LHC Computing Grid (WLCG) provides the robust computing infrastructure essential for the LHC experiments by integrating global computing resources into a cohesive entity. Simulations of different compute models present a feasible approach for evaluating future adaptations that are able to cope with future increased demands. However, running these simulations incurs a trade-off between accuracy and scalability. For example, while the simulator DCSim can provide accurate results, it falls short on scaling with the size of the simulated platform. Using Generative Machine Learning as a surrogate presents a candidate for overcoming this challenge.
  In this work, we evaluate the usage of three different Machine Learning models for the simulation of distributed computing systems and assess their ability to generalize to unseen situations. We show that those models can predict central observables derived from execution traces of compute jobs with approximate accuracy but with orders of magnitude faster execution times. Furthermore, we identify potentials for improving the predictions towards better accuracy and generalizability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12741v2</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <category>hep-ex</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1051/epjconf/202533701130</arxiv:DOI>
      <arxiv:journal_reference>EPJ Web Conf. Volume 337, 2025, Article Number 01130</arxiv:journal_reference>
      <dc:creator>Larissa Schmid, Maximilian Horzela, Valerii Zhyla, Manuel Giffels, G\"unter Quast, Anne Koziolek</dc:creator>
    </item>
    <item>
      <title>SysLLMatic: Large Language Models are Software System Optimizers</title>
      <link>https://arxiv.org/abs/2506.01249</link>
      <description>arXiv:2506.01249v2 Announce Type: replace-cross 
Abstract: Automatic software system optimization can improve software speed and save energy. Traditional approaches to optimization rely on manual tuning and compiler heuristics, limiting their ability to generalize across diverse codebases. Recent methods using LLMs introduce automation, but they do not scale effectively to the complexity and size of real-world software systems, leaving a gap in practical applicability. We present SysLLMatic, a system that integrates LLMs with performance diagnostics feedback and a curated catalog of 43 optimization patterns to automatically optimize software code. Our approach builds on recent advances in LLM-based code optimization and specifically targets the limitations of existing systems in handling real-world software applications. We evaluate it on three benchmark suites: HumanEval_CPP (competitive programming in C++), SciMark2 (scientific kernels in Java), and DaCapoBench (large-scale software systems in Java). Results show that SysLLMatic can improve software system performance, including latency, throughput, energy efficiency, memory usage, and CPU utilization. It consistently outperforms state-of-the-art LLM baselines on microbenchmarks. On large-scale application codes, to which prior LLM approaches have not scaled, it surpasses compiler optimizations, achieving average relative improvements of 1.5x in latency (vs. 1.01x for the compiler) and 1.76x in throughput (vs. 1.02x for the compiler). Our findings demonstrate that LLMs, guided by principled system thinking through the optimization pattern catalog and appropriate performance diagnostics, can serve as viable software system optimizers. We further identify limitations of our approach and the challenges involved in handling complex applications. This work provides a foundation for generating optimized code across various languages, benchmarks, and program sizes in a principled manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01249v2</guid>
      <category>cs.SE</category>
      <category>cs.PF</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huiyun Peng, Arjun Gupte, Ryan Hasler, Nicholas John Eliopoulos, Chien-Chou Ho, Rishi Mantri, Leo Deng, Konstantin L\"aufer, George K. Thiruvathukal, James C. Davis</dc:creator>
    </item>
  </channel>
</rss>
