<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Mar 2025 03:02:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Microbenchmark Framework for Performance Evaluation of OpenMP Target Offloading</title>
      <link>https://arxiv.org/abs/2503.00408</link>
      <description>arXiv:2503.00408v1 Announce Type: new 
Abstract: We present a framework based on Catch2 to evaluate performance of OpenMP's target offload model via micro-benchmarks. The compilers supporting OpenMP's target offload model for heterogeneous architectures are currently undergoing rapid development. These developments influence performance of various complex applications in different ways. This framework can be employed to track the impact of compiler upgrades and compare their performance with the native programming models. We use the framework to benchmark performance of a few commonly used operations on leadership class supercomputers such as Perlmutter at National Energy Research Scientific Computing (NERSC) Center and Frontier at Oak Ridge Leadership Computing Facility (OLCF). Such a framework will be useful for compiler developers to gain insights into the overall impact of many small changes, as well as for users to decide which compilers and versions are expected to yield best performance for their applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00408v1</guid>
      <category>cs.PF</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Atif, Tianle Wang, Zhihua Dong, Charles Leggett, Meifeng Lin</dc:creator>
    </item>
    <item>
      <title>Performance Optimization of 3D Stencil Computation on ARM Scalable Vector Extension</title>
      <link>https://arxiv.org/abs/2503.01348</link>
      <description>arXiv:2503.01348v1 Announce Type: new 
Abstract: Stencil computation is essential in high-performance computing, especially for large-scale tasks like liquid simulation and weather forecasting. Optimizing its performance can reduce both energy consumption and computation time, which is critical in disaster prediction. This paper explores optimization techniques for 7-point 3D stencil computation on ARM's Scalable Vector Extension (SVE), using the Roofline model and tools like Gem5 and cacti. We evaluate software optimizations such as vectorization and tiling, as well as hardware adjustments in ARM SVE vector lengths and cache configurations. The study also examines performance, power consumption, and chip area trade-offs to identify optimal configurations for ARM-based systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01348v1</guid>
      <category>cs.PF</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Hongguang Chen</dc:creator>
    </item>
    <item>
      <title>Can Tensor Cores Benefit Memory-Bound Kernels? (No!)</title>
      <link>https://arxiv.org/abs/2502.16851</link>
      <description>arXiv:2502.16851v2 Announce Type: cross 
Abstract: Tensor cores are specialized processing units within GPUs that have demonstrated significant efficiency gains in compute-bound applications such as Deep Learning Training by accelerating dense matrix operations. Given their success, researchers have attempted to extend tensor core capabilities beyond dense matrix computations to other computational patterns, including memory-bound kernels. Recent studies have reported that tensor cores can outperform traditional CUDA cores even on memory-bound kernels, where the primary performance bottleneck is not computation. In this research, we challenge these findings through both theoretical and empirical analysis. Our theoretical analysis reveals that tensor cores can achieve a maximum speedup of only 1.33x over CUDA cores for memory-bound kernels in double precision (for V100, A100, and H100 GPUs). We validate this theoretical limit through empirical analysis of three representative memory-bound kernels-STREAM Scale, SpMV, and stencil. We demonstrate that optimizing memory-bound kernels using tensor cores does not yield sound performance improvements over CUDA cores.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16851v2</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lingqi Zhang, Jiajun Huang, Sheng Di, Satoshi Matsuoka, Mohamed Wahib</dc:creator>
    </item>
    <item>
      <title>Breaking the Loop: Detecting and Mitigating Denial-of-Service Vulnerabilities in Large Language Models</title>
      <link>https://arxiv.org/abs/2503.00416</link>
      <description>arXiv:2503.00416v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have significantly advanced text understanding and generation, becoming integral to applications across education, software development, healthcare, entertainment, and legal services. Despite considerable progress in improving model reliability, latency remains under-explored, particularly through recurrent generation, where models repeatedly produce similar or identical outputs, causing increased latency and potential Denial-of-Service (DoS) vulnerabilities.
  We propose RecurrentGenerator, a black-box evolutionary algorithm that efficiently identifies recurrent generation scenarios in prominent LLMs like LLama-3 and GPT-4o. Additionally, we introduce RecurrentDetector, a lightweight real-time classifier trained on activation patterns, achieving 95.24% accuracy and an F1 score of 0.87 in detecting recurrent loops. Our methods provide practical solutions to mitigate latency-related vulnerabilities, and we publicly share our tools and data to support further research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00416v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junzhe Yu, Yi Liu, Huijia Sun, Ling Shi, Yuqi Chen</dc:creator>
    </item>
    <item>
      <title>m4: A Learned Flow-level Network Simulator</title>
      <link>https://arxiv.org/abs/2503.01770</link>
      <description>arXiv:2503.01770v1 Announce Type: cross 
Abstract: Flow-level simulation is widely used to model large-scale data center networks due to its scalability. Unlike packet-level simulators that model individual packets, flow-level simulators abstract traffic as continuous flows with dynamically assigned transmission rates. While this abstraction enables orders-of-magnitude speedup, it is inaccurate by omitting critical packet-level effects such as queuing, congestion control, and retransmissions.
  We present m4, an accurate and scalable flow-level simulator that uses machine learning to learn the dynamics of the network of interest. At the core of m4 lies a novel ML architecture that decomposes state transition computations into distinct spatial and temporal components, each represented by a suitable neural network. To efficiently learn the underlying flow-level dynamics, m4 adds dense supervision signals by predicting intermediate network metrics such as remaining flow size and queue length during training. m4 achieves a speedup of up to 104$\times$ over packet-level simulation. Relative to a traditional flow-level simulation, m4 reduces per-flow estimation errors by 45.3% (mean) and 53.0% (p90). For closed-loop applications, m4 accurately predicts network throughput under various congestion control schemes and workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01770v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenning Li, Anton A. Zabreyko, Arash Nasr-Esfahany, Kevin Zhao, Prateesh Goyal, Mohammad Alizadeh, Thomas Anderson</dc:creator>
    </item>
    <item>
      <title>Orthrus: Accelerating Multi-BFT Consensus through Concurrent Partial Ordering of Transactions (Extended Version)</title>
      <link>https://arxiv.org/abs/2501.14732</link>
      <description>arXiv:2501.14732v3 Announce Type: replace-cross 
Abstract: Multi-Byzantine Fault Tolerant (Multi-BFT) consensus allows multiple consensus instances to run in parallel, resolving the leader bottleneck problem inherent in classic BFT consensus. However, the global ordering of Multi-BFT consensus enforces a strict serialized sequence of transactions, imposing additional confirmation latency and also limiting concurrency. In this paper, we introduce Orthrus, a Multi-BFT protocol that accelerates transaction confirmation through partial ordering while reserving global ordering for transactions requiring stricter sequencing. To this end, Orthrus strategically partitions transactions to maximize concurrency and ensure consistency. Additionally, it incorporates an escrow mechanism to manage interactions between partially and globally ordered transactions. We evaluated Orthrus through extensive experiments in realistic settings, deploying 128 replicas in WAN and LAN environments. Our findings demonstrate latency reductions of up to 87% in WAN compared to existing Multi-BFT protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14732v3</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanzheng Lyu, Shaokang Xie, Jianyu Niu, Ivan Beschastnikh, Yinqian Zhang, Mohammad Sadoghi, Chen Feng</dc:creator>
    </item>
    <item>
      <title>Evaluating Fault Tolerance and Scalability in Distributed File Systems: A Case Study of GFS, HDFS, and MinIO</title>
      <link>https://arxiv.org/abs/2502.01981</link>
      <description>arXiv:2502.01981v2 Announce Type: replace-cross 
Abstract: Distributed File Systems (DFS) are essential for managing vast datasets across multiple servers, offering benefits in scalability, fault tolerance, and data accessibility. This paper presents a comprehensive evaluation of three prominent DFSs - Google File System (GFS), Hadoop Distributed File System (HDFS), and MinIO - focusing on their fault tolerance mechanisms and scalability under varying data loads and client demands. Through detailed analysis, how these systems handle data redundancy, server failures, and client access protocols, ensuring reliability in dynamic, large-scale environments is assessed. In addition, the impact of system design on performance, particularly in distributed cloud and computing architectures is assessed. By comparing the strengths and limitations of each DFS, the paper provides practical insights for selecting the most appropriate system for different enterprise needs, from high availability storage to big data analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01981v2</guid>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.PF</category>
      <category>cs.SE</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shubham Malhotra, Fnu Yashu, Muhammad Saqib, Dipkumar Mehta, Jagdish Jangid, Sachin Dixit</dc:creator>
    </item>
    <item>
      <title>Recurrent CircuitSAT Sampling for Sequential Circuits</title>
      <link>https://arxiv.org/abs/2502.21226</link>
      <description>arXiv:2502.21226v2 Announce Type: replace-cross 
Abstract: In this work, we introduce a novel GPU-accelerated circuit satisfiability (CircuitSAT) sampling technique for sequential circuits. This work is motivated by the requirement in constrained random verification (CRV) to generate input stimuli to validate the functionality of digital hardware circuits. A major challenge in CRV is generating inputs for sequential circuits, along with the appropriate number of clock cycles required to meet design constraints. Traditional approaches often use Boolean satisfiability (SAT) samplers to generate inputs by unrolling state transitions over a fixed number of clock cycles. However, these methods do not guarantee that a solution exists for the given number of cycles. Consequently, producing input stimuli together with the required clock cycles is essential for thorough testing and verification. Our approach converts the logical constraints and temporal behavior of sequential circuits into a recurrent CircuitSAT problem, optimized via gradient descent to efficiently explore a diverse set of valid solutions, including their associated number of clock cycles. By operating directly on the circuit structure, our method reinterprets the sampling process as a supervised multi-output regression task. This differentiable framework enables independent element-wise operations on each tensor element, facilitating parallel execution during learning. As a result, we achieve GPU-accelerated sampling with substantial runtime improvements (up to 105.1x) over state-of-the-art heuristic samplers. We demonstrate the effectiveness of our method through extensive evaluations on circuit problems from the ISCAS-89 and ITC'99 benchmark suites.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21226v2</guid>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Tue, 04 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arash Ardakani, Kevin He, John Wawrzynek</dc:creator>
    </item>
  </channel>
</rss>
