<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Jul 2025 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Structured linear factor models for tail dependence</title>
      <link>https://arxiv.org/abs/2507.16340</link>
      <description>arXiv:2507.16340v1 Announce Type: new 
Abstract: A common object to describe the extremal dependence of a $d$-variate random vector $X$ is the stable tail dependence function $L$. Various parametric models have emerged, with a popular subclass consisting of those stable tail dependence functions that arise for linear and max-linear factor models with heavy tailed factors. The stable tail dependence function is then parameterized by a $d \times K$ matrix $A$, where $K$ is the number of factors and where $A$ can be interpreted as a factor loading matrix. We study estimation of $L$ under an additional assumption on $A$ called the `pure variable assumption'. Both $K \in \{1, \dots, d\}$ and $A \in [0, \infty)^{d \times K}$ are treated as unknown, which constitutes an unconventional parameter space that does not fit into common estimation frameworks. We suggest two algorithms that allow to estimate $K$ and $A$, and provide finite sample guarantees for both algorithms. Remarkably, the guarantees allow for the case where the dimension $d$ is larger than the sample size $n$. The results are illustrated with numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16340v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexis Boulin, Axel B\"ucher</dc:creator>
    </item>
    <item>
      <title>Bayesian causal discovery: Posterior concentration and optimal detection</title>
      <link>https://arxiv.org/abs/2507.16529</link>
      <description>arXiv:2507.16529v1 Announce Type: new 
Abstract: We consider the problem of Bayesian causal discovery for the standard model of linear structural equations with equivariant Gaussian noise. A uniform prior is placed on the space of directed acyclic graphs (DAGs) over a fixed set of variables and, given the graph, independent Gaussian priors are placed on the associated linear coefficients of pairwise interactions. We show that the rate at which the posterior on model space concentrates on the true underlying DAG depends critically on its nature: If it is maximal, in the sense that adding any one new edge would violate acyclicity, then its posterior probability converges to 1 exponentially fast (almost surely) in the sample size $n$. Otherwise, it converges at a rate no faster than $1/\sqrt{n}$. This sharp dichotomy is an instance of the important general phenomenon that avoiding overfitting is significantly harder than identifying all of the structure that is present in the model. We also draw a new connection between the posterior distribution on model space and recent results on optimal hypothesis testing in the related problem of edge detection. Our theoretical findings are illustrated empirically through simulation experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16529v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valentinian Lungu, Joni Shaska, Ioannis Kontoyiannis, Urbashi Mitra</dc:creator>
    </item>
    <item>
      <title>Gaussian Sequence Model: Sample Complexities of Testing, Estimation and LFHT</title>
      <link>https://arxiv.org/abs/2507.16734</link>
      <description>arXiv:2507.16734v1 Announce Type: new 
Abstract: We study the Gaussian sequence model, i.e. $X \sim N(\mathbf{\theta}, I_\infty)$, where $\mathbf{\theta} \in \Gamma \subset \ell_2$ is assumed to be convex and compact. We show that goodness-of-fit testing sample complexity is lower bounded by the square-root of the estimation complexity, whenever $\Gamma$ is orthosymmetric. We show that the lower bound is tight when $\Gamma$ is also quadratically convex, thus significantly extending validity of the testing-estimation relationship from [GP24]. Using similar methods, we also completely characterize likelihood-free hypothesis testing (LFHT) complexity for $\ell_p$-bodies, discovering new types of tradeoff between the numbers of simulation and observation samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16734v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeyu Jia, Yury Polyanskiy</dc:creator>
    </item>
    <item>
      <title>Can we have it all? Non-asymptotically valid and asymptotically exact confidence intervals for expectations and linear regressions</title>
      <link>https://arxiv.org/abs/2507.16776</link>
      <description>arXiv:2507.16776v1 Announce Type: new 
Abstract: We contribute to bridging the gap between large- and finite-sample inference by studying confidence sets (CSs) that are both non-asymptotically valid and asymptotically exact uniformly (NAVAE) over semi-parametric statistical models. NAVAE CSs are not easily obtained; for instance, we show they do not exist over the set of Bernoulli distributions. We first derive a generic sufficient condition: NAVAE CSs are available as soon as uniform asymptotically exact CSs are. Second, building on that connection, we construct closed-form NAVAE confidence intervals (CIs) in two standard settings -- scalar expectations and linear combinations of OLS coefficients -- under moment conditions only. For expectations, our sole requirement is a bounded kurtosis. In the OLS case, our moment constraints accommodate heteroskedasticity and weak exogeneity of the regressors. Under those conditions, we enlarge the Central Limit Theorem-based CIs, which are asymptotically exact, to ensure non-asymptotic guarantees. Those modifications vanish asymptotically so that our CIs coincide with the classical ones in the limit. We illustrate the potential and limitations of our approach through a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16776v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexis Derumigny, Lucas Girard, Yannick Guyonvarch</dc:creator>
    </item>
    <item>
      <title>Canonical Representations of Markovian Structural Causal Models: A Framework for Counterfactual Reasoning</title>
      <link>https://arxiv.org/abs/2507.16370</link>
      <description>arXiv:2507.16370v1 Announce Type: cross 
Abstract: Counterfactual reasoning aims at answering contrary-to-fact questions like ''Would have Alice recovered had she taken aspirin?'' and corresponds to the most fine-grained layer of causation. Critically, while many counterfactual statements cannot be falsified -- even by randomized experiments -- they underpin fundamental concepts like individual-wise fairness. Therefore, providing models to formalize and implement counterfactual beliefs remains a fundamental scientific problem. In the Markovian setting of Pearl's causal framework, we propose an alternative approach to structural causal models to represent counterfactuals compatible with a given causal graphical model. More precisely, we introduce counterfactual models, also called canonical representations of structural causal models. They enable analysts to choose a counterfactual conception via random-process probability distributions with preassigned marginals and characterize the counterfactual equivalence class of structural causal models. Then, we present a normalization procedure to describe and implement various counterfactual conceptions. Compared to structural causal models, it allows to specify many counterfactual conceptions without altering the observational and interventional constraints. Moreover, the content of the model corresponding to the counterfactual layer does not need to be estimated; only to make a choice. Finally, we illustrate the specific role of counterfactuals in causality and the benefits of our approach on theoretical and numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16370v1</guid>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas de Lara (IECL)</dc:creator>
    </item>
    <item>
      <title>Structural Effect and Spectral Enhancement of High-Dimensional Regularized Linear Discriminant Analysis</title>
      <link>https://arxiv.org/abs/2507.16682</link>
      <description>arXiv:2507.16682v1 Announce Type: cross 
Abstract: Regularized linear discriminant analysis (RLDA) is a widely used tool for classification and dimensionality reduction, but its performance in high-dimensional scenarios is inconsistent. Existing theoretical analyses of RLDA often lack clear insight into how data structure affects classification performance. To address this issue, we derive a non-asymptotic approximation of the misclassification rate and thus analyze the structural effect and structural adjustment strategies of RLDA. Based on this, we propose the Spectral Enhanced Discriminant Analysis (SEDA) algorithm, which optimizes the data structure by adjusting the spiked eigenvalues of the population covariance matrix. By developing a new theoretical result on eigenvectors in random matrix theory, we derive an asymptotic approximation on the misclassification rate of SEDA. The bias correction algorithm and parameter selection strategy are then obtained. Experiments on synthetic and real datasets show that SEDA achieves higher classification accuracy and dimensionality reduction compared to existing LDA methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16682v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonghan Zhang, Zhangni Pu, Lu Yan, Jiang Hu</dc:creator>
    </item>
    <item>
      <title>Testing the variety hypothesis</title>
      <link>https://arxiv.org/abs/2507.16705</link>
      <description>arXiv:2507.16705v1 Announce Type: cross 
Abstract: Given a probability measure on the unit disk, we study the problem of deciding whether, for some threshold probability, this measure is supported near a real algebraic variety of given dimension and bounded degree. We call this "testing the variety hypothesis". We prove an upper bound on the so-called "sample complexity" of this problem and show how it can be reduced to a semialgebraic decision problem. This is done by studying in a quantitative way the Hausdorff geometry of the space of real algebraic varieties of a given dimension and degree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16705v1</guid>
      <category>math.AG</category>
      <category>math.MG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>A. Lerario, P. Roos Hoefgeest, M. Scolamiero, A. Tamai</dc:creator>
    </item>
    <item>
      <title>Identifying causal effects in maximally oriented partially directed acyclic graphs</title>
      <link>https://arxiv.org/abs/1910.02997</link>
      <description>arXiv:1910.02997v3 Announce Type: replace 
Abstract: We develop a necessary and sufficient causal identification criterion for maximally oriented partially directed acyclic graphs (MPDAGs). MPDAGs as a class of graphs include directed acyclic graphs (DAGs), completed partially directed acyclic graphs (CPDAGs), and CPDAGs with added background knowledge. As such, they represent the type of graph that can be learned from observational data and background knowledge under the assumption of no latent variables. Our identification criterion can be seen as a generalization of the g-formula of Robins (1986). We further obtain a generalization of the truncated factorization formula (Pearl, 2009) and compare our criterion to the generalized adjustment criterion of Perkovi\'c et al. (2017) which is sufficient, but not necessary for causal identification.</description>
      <guid isPermaLink="false">oai:arXiv.org:1910.02997v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of UAI 2020</arxiv:journal_reference>
      <dc:creator>Emilija Perkovi\'c</dc:creator>
    </item>
    <item>
      <title>Optimal tests of the composite null hypothesis arising in mediation analysis</title>
      <link>https://arxiv.org/abs/2107.07575</link>
      <description>arXiv:2107.07575v3 Announce Type: replace 
Abstract: The indirect effect of an exposure on an outcome through an intermediate variable can be identified by a product of two regression coefficients under certain causal and regression modeling assumptions. In this context, the null hypothesis of no indirect effect is a composite null hypothesis, as the null holds if either regression coefficient is zero. A consequence is that traditional hypothesis tests are severely underpowered near the origin (i.e., when both coefficients are small with respect to standard errors). We propose hypothesis tests that (i) preserve level alpha type~1 error, (ii) meaningfully improve power when both true underlying effects are small relative to sample size, and (iii) preserve power when at least one is not. One approach gives a closed-form test that is minimax optimal with respect to local power over the alternative parameter space. Another uses sparse linear programming to produce an approximately optimal test for a Bayes risk criterion. We discuss adaptations for performing large-scale hypothesis testing as well as modifications that yield improved interpretability. We provide an R package that implements our proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.07575v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caleb H. Miles, Antoine Chambaz</dc:creator>
    </item>
    <item>
      <title>On low frequency inference for diffusions without the hot spots conjecture</title>
      <link>https://arxiv.org/abs/2410.19393</link>
      <description>arXiv:2410.19393v2 Announce Type: replace 
Abstract: We remove the dependence on the `hot-spots' conjecture in two of the main theorems of the recent paper of Nickl (2024, Annals of Statistics). Specifically, we characterise the minimax convergence rates for estimation of the transition operator $P_{f}$ arising from the Neumann Laplacian with diffusion coefficient $f$ on arbitrary convex domains with smooth boundary, and further show that a general Lipschitz stability estimate holds for the inverse map $P_f\mapsto f$ from $H^2\to H^2$ to $L^1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19393v2</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giovanni S. Alberti, Douglas Barnes, Aditya Jambhale, Richard Nickl</dc:creator>
    </item>
    <item>
      <title>A computational transition for detecting correlated stochastic block models by low-degree polynomials</title>
      <link>https://arxiv.org/abs/2409.00966</link>
      <description>arXiv:2409.00966v2 Announce Type: replace-cross 
Abstract: Detection of correlation in a pair of random graphs is a fundamental statistical and computational problem that has been extensively studied in recent years. In this work, we consider a pair of correlated (sparse) stochastic block models $\mathcal{S}(n,\tfrac{\lambda}{n};k,\epsilon;s)$ that are subsampled from a common parent stochastic block model $\mathcal S(n,\tfrac{\lambda}{n};k,\epsilon)$ with $k=O(1)$ symmetric communities, average degree $\lambda=O(1)$, divergence parameter $\epsilon$, and subsampling probability $s$.
  For the detection problem of distinguishing this model from a pair of independent Erd\H{o}s-R\'enyi graphs with the same edge density $\mathcal{G}(n,\tfrac{\lambda s}{n})$, we focus on tests based on \emph{low-degree polynomials} of the entries of the adjacency matrices, and we determine the threshold that separates the easy and hard regimes. More precisely, we show that this class of tests can distinguish these two models if and only if $s&gt; \min \{ \sqrt{\alpha}, \frac{1}{\lambda \epsilon^2} \}$, where $\alpha\approx 0.338$ is the Otter's constant and $\frac{1}{\lambda \epsilon^2}$ is the Kesten-Stigum threshold. Combining a reduction argument in \cite{Li25+}, our hardness result also implies low-degree hardness for partial recovery and detection (to independent block models) when $s&lt; \min \{ \sqrt{\alpha}, \frac{1}{\lambda \epsilon^2} \}$. Finally, our proof of low-degree hardness is based on a conditional variant of the low-degree likelihood calculation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00966v2</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanyi Chen, Jian Ding, Shuyang Gong, Zhangsong Li</dc:creator>
    </item>
    <item>
      <title>Results related to the Gaussian product inequality conjecture for mixed-sign exponents in arbitrary dimension</title>
      <link>https://arxiv.org/abs/2505.09976</link>
      <description>arXiv:2505.09976v2 Announce Type: replace-cross 
Abstract: This note establishes that the opposite Gaussian product inequality (GPI) of the type proved by Russell &amp; Sun (2022a) in two dimensions, and partially extended to higher dimensions by Zhou et al. (2024), continues to hold for an arbitrary mix of positive and negative exponents. A general quantitative lower bound is also obtained conditionally on the GPI conjecture being true.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09976v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guolie Lan, Fr\'ed\'eric Ouimet, Wei Sun</dc:creator>
    </item>
    <item>
      <title>Loss Functions for Measuring the Accuracy of Nonnegative Cross-Sectional Predictions</title>
      <link>https://arxiv.org/abs/2505.18130</link>
      <description>arXiv:2505.18130v2 Announce Type: replace-cross 
Abstract: Measuring the accuracy of cross-sectional predictions is a subjective problem. Generally, this problem is avoided. In contrast, this paper confronts subjectivity up front by eliciting an impartial decision-maker's preferences. These preferences are embedded into an axiomatically-derived loss function, the simplest version of which is described. The parameters of the loss function can be estimated by linear regression. Specification tests for this function are described. This framework is extended to weighted averages of estimates to find the optimal weightings. Rescalings to account for changes in control data or base year data are considered. A special case occurs when the predictions represent resource allocations: the apportionment literature is used to construct the Webster-Saint Lague Rule, a particular parametrization of the loss function. These loss functions are compared to those existing in the literature. Finally, a bias measure is created that uses signed versions of these loss functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18130v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Charles D. Coleman</dc:creator>
    </item>
  </channel>
</rss>
