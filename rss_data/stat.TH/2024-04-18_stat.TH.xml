<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Apr 2024 04:05:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 19 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Estimation of subcritical Galton Watson processes with correlated immigration</title>
      <link>https://arxiv.org/abs/2404.12137</link>
      <description>arXiv:2404.12137v1 Announce Type: new 
Abstract: We consider an observed subcritical Galton Watson process $\{Y_n,\ n\in  \mathbb{Z} \}$ with correlated stationary immigration process $\{\epsilon_n,\ n\in  \mathbb{Z}  \}$. Two situations are presented. The first one is when $\mbox{Cov}(\epsilon_0,\epsilon_k)=0$ for $k$ larger than some $k_0$: a consistent estimator for the reproduction and mean immigration rates is given, and a central limit theorem is proved. The second one is when $\{\epsilon_n,\ n\in  \mathbb{Z}  \}$ has general correlation structure: under mixing assumptions, we exhibit an estimator for the the logarithm of the reproduction rate and we prove that it converges in quadratic mean with explicit speed. In addition, when the mixing coefficients decrease fast enough, we provide and prove a two terms expansion for the estimator. Numerical illustrations are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12137v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Landy Rabehasaina (UFC, LMB), Yacouba Boubacar Mainassara (UPHF, INSA Hauts-De-France, CERAMATHS)</dc:creator>
    </item>
    <item>
      <title>Estimation of the invariant measure of a multidimensional diffusion from noisy observations</title>
      <link>https://arxiv.org/abs/2404.12181</link>
      <description>arXiv:2404.12181v1 Announce Type: new 
Abstract: We introduce a new approach for estimating the invariant density of a multidimensional diffusion when dealing with high-frequency observations blurred by independent noises. We consider the intermediate regime, where observations occur at discrete time instances $k\Delta_n$ for $k=0,\dots,n$, under the conditions $\Delta_n\to 0$ and $n\Delta_n\to\infty$. Our methodology involves the construction of a kernel density estimator that uses a pre-averaging technique to proficiently remove noise from the data while preserving the analytical characteristics of the underlying signal and its asymptotic properties. The rate of convergence of our estimator depends on both the anisotropic regularity of the density and the intensity of the noise. We establish conditions on the intensity of the noise that ensure the recovery of convergence rates similar to those achievable without any noise. Furthermore, we prove a Bernstein concentration inequality for our estimator, from which we derive an adaptive procedure for the kernel bandwidth selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12181v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rapha\"el Maillet, Gr\'egoire Szymanski</dc:creator>
    </item>
    <item>
      <title>Asymmetric canonical correlation analysis of Riemannian and high-dimensional data</title>
      <link>https://arxiv.org/abs/2404.11781</link>
      <description>arXiv:2404.11781v1 Announce Type: cross 
Abstract: In this paper, we introduce a novel statistical model for the integrative analysis of Riemannian-valued functional data and high-dimensional data. We apply this model to explore the dependence structure between each subject's dynamic functional connectivity -- represented by a temporally indexed collection of positive definite covariance matrices -- and high-dimensional data representing lifestyle, demographic, and psychometric measures. Specifically, we employ a reformulation of canonical correlation analysis that enables efficient control of the complexity of the functional canonical directions using tangent space sieve approximations. Additionally, we enforce an interpretable group structure on the high-dimensional canonical directions via a sparsity-promoting penalty. The proposed method shows improved empirical performance over alternative approaches and comes with theoretical guarantees. Its application to data from the Human Connectome Project reveals a dominant mode of covariation between dynamic functional connectivity and lifestyle, demographic, and psychometric measures. This mode aligns with results from static connectivity studies but reveals a unique temporal non-stationary pattern that such studies fail to capture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11781v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Buenfil, Eardi Lila</dc:creator>
    </item>
    <item>
      <title>A nonparametric distribution-free test of independence among continuous random vectors based on \texorpdfstring{$L_1$}{}-norm</title>
      <link>https://arxiv.org/abs/2105.02164</link>
      <description>arXiv:2105.02164v3 Announce Type: replace 
Abstract: We propose a novel statistical test to assess the mutual independence of multidimensional random vectors. Our approach is based on the $L_1$-distance between the joint density function and the product of the marginal densities associated with the presumed independent vectors. Under the null hypothesis, we employ Poissonization techniques to establish the asymptotic normal approximation of the corresponding test statistic, without imposing any regularity assumptions on the underlying Lebesgue density function, denoted as $f(\cdot)$. Remarkably, we observe that the limiting distribution of the $L_1$-based statistics remains unaffected by the specific form of $f(\cdot)$. This unexpected outcome contributes to the robustness and versatility of our method. Moreover, our tests exhibit nontrivial local power against a subset of local alternatives, which converge to the null hypothesis at a rate of {${\tiny n^{\tiny -1/2}h_n^{\tiny -{d/4}}}$}, $d\geq 2$, where $n$ represents the sample size and $h_n$ denotes the bandwidth. Finally, the theory is supported by a comprehensive simulation study to investigate the finite-sample performance of our proposed test. The results demonstrate that our testing procedure generally outperforms existing approaches across various examined scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.02164v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nour-Eddine Berrahou, Salim Bouzebda, Lahcen Douge</dc:creator>
    </item>
    <item>
      <title>Precise Asymptotics for Spectral Methods in Mixed Generalized Linear Models</title>
      <link>https://arxiv.org/abs/2211.11368</link>
      <description>arXiv:2211.11368v4 Announce Type: replace 
Abstract: In a mixed generalized linear model, the objective is to learn multiple signals from unlabeled observations: each sample comes from exactly one signal, but it is not known which one. We consider the prototypical problem of estimating two statistically independent signals in a mixed generalized linear model with Gaussian covariates. Spectral methods are a popular class of estimators which output the top two eigenvectors of a suitable data-dependent matrix. However, despite the wide applicability, their design is still obtained via heuristic considerations, and the number of samples $n$ needed to guarantee recovery is super-linear in the signal dimension $d$. In this paper, we develop exact asymptotics on spectral methods in the challenging proportional regime in which $n, d$ grow large and their ratio converges to a finite constant. By doing so, we are able to optimize the design of the spectral method, and combine it with a simple linear estimator, in order to minimize the estimation error. Our characterization exploits a mix of tools from random matrices, free probability and the theory of approximate message passing algorithms. Numerical simulations for mixed linear regression and phase retrieval demonstrate the advantage enabled by our analysis over existing designs of spectral methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.11368v4</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yihan Zhang, Marco Mondelli, Ramji Venkataramanan</dc:creator>
    </item>
    <item>
      <title>Comparing Scale Parameter Estimators for Gaussian Process Interpolation with the Brownian Motion Prior: Leave-One-Out Cross Validation and Maximum Likelihood</title>
      <link>https://arxiv.org/abs/2307.07466</link>
      <description>arXiv:2307.07466v2 Announce Type: replace 
Abstract: Gaussian process (GP) regression is a Bayesian nonparametric method for regression and interpolation, offering a principled way of quantifying the uncertainties of predicted function values. For the quantified uncertainties to be well-calibrated, however, the kernel of the GP prior has to be carefully selected. In this paper, we theoretically compare two methods for choosing the kernel in GP regression: cross-validation and maximum likelihood estimation. Focusing on the scale-parameter estimation of a Brownian motion kernel in the noiseless setting, we prove that cross-validation can yield asymptotically well-calibrated credible intervals for a broader class of ground-truth functions than maximum likelihood estimation, suggesting an advantage of the former over the latter. Finally, motivated by the findings, we propose interior cross validation, a procedure that adapts to an even broader class of ground-truth functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.07466v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masha Naslidnyk, Motonobu Kanagawa, Toni Karvonen, Maren Mahsereci</dc:creator>
    </item>
    <item>
      <title>Distribution-uniform anytime-valid sequential inference</title>
      <link>https://arxiv.org/abs/2311.03343</link>
      <description>arXiv:2311.03343v2 Announce Type: replace 
Abstract: Are asymptotic confidence sequences and anytime $p$-values uniformly valid for a nontrivial class of distributions $\mathcal{P}$? We give a positive answer to this question by deriving distribution-uniform anytime-valid inference procedures. Historically, anytime-valid methods -- including confidence sequences, anytime $p$-values, and sequential hypothesis tests that enable inference at stopping times -- have been justified nonasymptotically. Nevertheless, asymptotic procedures such as those based on the central limit theorem occupy an important part of statistical toolbox due to their simplicity, universality, and weak assumptions. While recent work has derived asymptotic analogues of anytime-valid methods with the aforementioned benefits, these were not shown to be $\mathcal{P}$-uniform, meaning that their asymptotics are not uniformly valid in a class of distributions $\mathcal{P}$. Indeed, the anytime-valid inference literature currently has no central limit theory to draw from that is both uniform in $\mathcal{P}$ and in the sample size $n$. This paper fills that gap by deriving a novel $\mathcal{P}$-uniform strong Gaussian approximation theorem. We apply some of these results to obtain an anytime-valid test of conditional independence without the Model-X assumption, as well as a $\mathcal{P}$-uniform law of the iterated logarithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.03343v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ian Waudby-Smith, Edward H. Kennedy, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Double shrinkage priors for a normal mean matrix</title>
      <link>https://arxiv.org/abs/2311.13137</link>
      <description>arXiv:2311.13137v2 Announce Type: replace 
Abstract: We consider estimation of a normal mean matrix under the Frobenius loss. Motivated by the Efron--Morris estimator, a generalization of Stein's prior has been recently developed, which is superharmonic and shrinks the singular values towards zero. The generalized Bayes estimator with respect to this prior is minimax and dominates the maximum likelihood estimator. However, here we show that it is inadmissible by using Brown's condition. Then, we develop two types of priors that provide improved generalized Bayes estimators and examine their performance numerically. The proposed priors attain risk reduction by adding scalar shrinkage or column-wise shrinkage to singular value shrinkage. Parallel results for Bayesian predictive densities are also given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13137v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takeru Matsuda, Fumiyasu Komaki, William E. Strawderman</dc:creator>
    </item>
    <item>
      <title>Smoothness Estimation for Whittle-Mat\'ern Processes on Closed Riemannian Manifolds</title>
      <link>https://arxiv.org/abs/2401.00510</link>
      <description>arXiv:2401.00510v2 Announce Type: replace 
Abstract: The family of Mat\'ern kernels are often used in spatial statistics, function approximation and Gaussian process methods in machine learning. One reason for their popularity is the presence of a smoothness parameter that controls, for example, optimal error bounds for kriging and posterior contraction rates in Gaussian process regression. On closed Riemannian manifolds, we show that the smoothness parameter can be consistently estimated from the maximizer(s) of the Gaussian likelihood when the underlying data are from point evaluations of a Gaussian process and, perhaps surprisingly, even when the data comprise evaluations of a non-Gaussian process. The points at which the process is observed need not have any particular spatial structure beyond quasi-uniformity. Our methods are based on results from approximation theory for the Sobolev scale of Hilbert spaces. Moreover, we generalize a well-known equivalence of measures phenomenon related to Mat\'ern kernels to the non-Gaussian case by using Kakutani's theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00510v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moritz Korte-Stapff, Toni Karvonen, Eric Moulines</dc:creator>
    </item>
    <item>
      <title>Consistency of the bootstrap for asymptotically linear estimators based on machine learning</title>
      <link>https://arxiv.org/abs/2404.03064</link>
      <description>arXiv:2404.03064v2 Announce Type: replace 
Abstract: The bootstrap is a popular method of constructing confidence intervals due to its ease of use and broad applicability. Theoretical properties of bootstrap procedures have been established in a variety of settings. However, there is limited theoretical research on the use of the bootstrap in the context of estimation of a differentiable functional in a nonparametric or semiparametric model when nuisance functions are estimated using machine learning. In this article, we provide general conditions for consistency of the bootstrap in such scenarios. Our results cover a range of estimator constructions, nuisance estimation methods, bootstrap sampling distributions, and bootstrap confidence interval types. We provide refined results for the empirical bootstrap and smoothed bootstraps, and for one-step estimators, plug-in estimators, empirical mean plug-in estimators, and estimating equations-based estimators. We illustrate the use of our general results by demonstrating the asymptotic validity of bootstrap confidence intervals for the average density value and G-computed conditional mean parameters, and compare their performance in finite samples using numerical studies. Throughout, we emphasize whether and how the bootstrap can produce asymptotically valid confidence intervals when standard methods fail to do so.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03064v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhou Tang, Ted Westling</dc:creator>
    </item>
    <item>
      <title>High-probability Convergence Bounds for Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise</title>
      <link>https://arxiv.org/abs/2310.18784</link>
      <description>arXiv:2310.18784v4 Announce Type: replace-cross 
Abstract: We study high-probability convergence guarantees of learning on streaming data in the presence of heavy-tailed noise. In the proposed scenario, the model is updated in an online fashion, as new information is observed, without storing any additional data. To combat the heavy-tailed noise, we consider a general framework of nonlinear stochastic gradient descent (SGD), providing several strong results. First, for non-convex costs and component-wise nonlinearities, we establish a convergence rate arbitrarily close to $\mathcal{O}\left(t^{-\frac{1}{4}}\right)$, whose exponent is independent of noise and problem parameters. Second, for strongly convex costs and a broader class of nonlinearities, we establish convergence of the last iterate to the optimum, with a rate $\mathcal{O}\left(t^{-\zeta} \right)$, where $\zeta \in (0,1)$ depends on problem parameters, noise and nonlinearity. As we show analytically and numerically, $\zeta$ can be used to inform the preferred choice of nonlinearity for given problem settings. Compared to state-of-the-art, who only consider clipping, require bounded noise moments of order $\eta \in (1,2]$, and establish convergence rates whose exponents go to zero as $\eta \rightarrow 1$, we provide high-probability guarantees for a much broader class of nonlinearities and symmetric density noise, with convergence rates whose exponents are bounded away from zero, even when the noise has finite first moment only. Moreover, in the case of strongly convex functions, we demonstrate analytically and numerically that clipping is not always the optimal nonlinearity, further underlining the value of our general framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18784v4</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aleksandar Armacki, Pranay Sharma, Gauri Joshi, Dragana Bajovic, Dusan Jakovetic, Soummya Kar</dc:creator>
    </item>
    <item>
      <title>Functional weak convergence of stochastic integrals for moving averages and continuous-time random walks</title>
      <link>https://arxiv.org/abs/2401.13543</link>
      <description>arXiv:2401.13543v2 Announce Type: replace-cross 
Abstract: There is by now an extensive theory of weak convergence for moving averages and continuous-time random walks (CTRWs) with respect to Skorokhod's M1 and J1 topologies. Here we address the fundamental question of how this translates into functional limit theorems, in the M1 or J1 topology, for stochastic integrals driven by these processes. As an important application, we provide weak approximation results for general SDEs driven by time-changed L\'evy processes. Such SDEs and their associated fractional Fokker--Planck--Kolmogorov equations are central to models of anomalous diffusion in statistical physics. Our results yield a rigorous functional characterisation of these as continuum limits of the underlying models driven by CTRWs. In regard to strictly M1 convergent moving averages and correlated CTRWs, it turns out that the convergence of stochastic integrals can fail markedly. Nevertheless, we are able to identify natural classes of integrand processes for which M1 convergence holds. We show that these results are general enough to yield functional limit theorems, in the M1 topology, for certain stochastic delay differential equations driven by moving averages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13543v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas S{\o}jmark, Fabrice Wunderlich</dc:creator>
    </item>
  </channel>
</rss>
