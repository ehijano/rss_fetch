<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Feb 2026 02:46:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A computational account of dreaming: learning and memory consolidation</title>
      <link>https://arxiv.org/abs/2602.04095</link>
      <description>arXiv:2602.04095v1 Announce Type: new 
Abstract: A number of studies have concluded that dreaming is mostly caused by randomly arriving internal signals because "dream contents are random impulses", and argued that dream sleep is unlikely to play an important part in our intellectual capacity. On the contrary, numerous functional studies have revealed that dream sleep does play an important role in our learning and other intellectual functions. Specifically, recent studies have suggested the importance of dream sleep in memory consolidation, following the findings of neural replaying of recent waking patterns in the hippocampus. The randomness has been the hurdle that divides dream theories into either functional or functionless. This study presents a cognitive and computational model of dream process. This model is simulated to perform the functions of learning and memory consolidation, which are two most popular dream functions that have been proposed. The simulations demonstrate that random signals may result in learning and memory consolidation. Thus, dreaming is proposed as a continuation of brain's waking activities that processes signals activated spontaneously and randomly from the hippocampus. The characteristics of the model are discussed and found in agreement with many characteristics concluded from various empirical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04095v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cogsys.2008.06.002</arxiv:DOI>
      <arxiv:journal_reference>Cognitive System Research, 2009</arxiv:journal_reference>
      <dc:creator>Qi Zhang</dc:creator>
    </item>
    <item>
      <title>Discovering Mechanistic Models of Neural Activity: System Identification in an in Silico Zebrafish</title>
      <link>https://arxiv.org/abs/2602.04492</link>
      <description>arXiv:2602.04492v1 Announce Type: new 
Abstract: Constructing mechanistic models of neural circuits is a fundamental goal of neuroscience, yet verifying such models is limited by the lack of ground truth. To rigorously test model discovery, we establish an in silico testbed using neuromechanical simulations of a larval zebrafish as a transparent ground truth. We find that LLM-based tree search autonomously discovers predictive models that significantly outperform established forecasting baselines. Conditioning on sensory drive is necessary but not sufficient for faithful system identification, as models exploit statistical shortcuts. Structural priors prove essential for enabling robust out-of-distribution generalization and recovery of interpretable mechanistic models. Our insights provide guidance for modeling real-world neural recordings and offer a broader template for AI-driven scientific discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04492v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan-Matthis Lueckmann, Viren Jain, Micha{\l} Januszewski</dc:creator>
    </item>
    <item>
      <title>BrainVista: Modeling Naturalistic Brain Dynamics as Multimodal Next-Token Prediction</title>
      <link>https://arxiv.org/abs/2602.04512</link>
      <description>arXiv:2602.04512v1 Announce Type: new 
Abstract: Naturalistic fMRI characterizes the brain as a dynamic predictive engine driven by continuous sensory streams. However, modeling the causal forward evolution in realistic neural simulation is impeded by the timescale mismatch between multimodal inputs and the complex topology of cortical networks. To address these challenges, we introduce BrainVista, a multimodal autoregressive framework designed to model the causal evolution of brain states. BrainVista incorporates Network-wise Tokenizers to disentangle system-specific dynamics and a Spatial Mixer Head that captures inter-network information flow without compromising functional boundaries. Furthermore, we propose a novel Stimulus-to-Brain (S2B) masking mechanism to synchronize high-frequency sensory stimuli with hemodynamically filtered signals, enabling strict, history-only causal conditioning. We validate our framework on Algonauts 2025, CineBrain, and HAD, achieving state-of-the-art fMRI encoding performance. In long-horizon rollout settings, our model yields substantial improvements over baselines, increasing pattern correlation by 36.0\% and 33.3\% on relative to the strongest baseline Algonauts 2025 and CineBrain, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04512v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuanhua Yin, Runkai Zhao, Lina Yao, Weidong Cai</dc:creator>
    </item>
    <item>
      <title>A Hitchhiker's Guide to Poisson Gradient Estimation</title>
      <link>https://arxiv.org/abs/2602.03896</link>
      <description>arXiv:2602.03896v1 Announce Type: cross 
Abstract: Poisson-distributed latent variable models are widely used in computational neuroscience, but differentiating through discrete stochastic samples remains challenging. Two approaches address this: Exponential Arrival Time (EAT) simulation and Gumbel-SoftMax (GSM) relaxation. We provide the first systematic comparison of these methods, along with practical guidance for practitioners. Our main technical contribution is a modification to the EAT method that theoretically guarantees an unbiased first moment (exactly matching the firing rate), and reduces second-moment bias. We evaluate these methods on their distributional fidelity, gradient quality, and performance on two tasks: (1) variational autoencoders with Poisson latents, and (2) partially observable generalized linear models, where latent neural connectivity must be inferred from observed spike trains. Across all metrics, our modified EAT method exhibits better overall performance (often comparable to exact gradients), and substantially higher robustness to hyperparameter choices. Together, our results clarify the trade-offs between these methods and offer concrete recommendations for practitioners working with Poisson latent variable models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03896v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Ibrahim, Hanqi Zhao, Eli Sennesh, Zhi Li, Anqi Wu, Jacob L. Yates, Chengrui Li, Hadi Vafaii</dc:creator>
    </item>
    <item>
      <title>Multi-Integration of Labels across Categories for Component Identification (MILCCI)</title>
      <link>https://arxiv.org/abs/2602.04270</link>
      <description>arXiv:2602.04270v1 Announce Type: cross 
Abstract: Many fields collect large-scale temporal data through repeated measurements (trials), where each trial is labeled with a set of metadata variables spanning several categories. For example, a trial in a neuroscience study may be linked to a value from category (a): task difficulty, and category (b): animal choice. A critical challenge in time-series analysis is to understand how these labels are encoded within the multi-trial observations, and disentangle the distinct effect of each label entry across categories. Here, we present MILCCI, a novel data-driven method that i) identifies the interpretable components underlying the data, ii) captures cross-trial variability, and iii) integrates label information to understand each category's representation within the data. MILCCI extends a sparse per-trial decomposition that leverages label similarities within each category to enable subtle, label-driven cross-trial adjustments in component compositions and to distinguish the contribution of each category. MILCCI also learns each component's corresponding temporal trace, which evolves over time within each trial and varies flexibly across trials. We demonstrate MILCCI's performance through both synthetic and real-world examples, including voting patterns, online page view trends, and neuronal recordings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04270v1</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <category>stat.ML</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noga Mudrik, Yuxi Chen, Gal Mishne, Adam S. Charles</dc:creator>
    </item>
    <item>
      <title>Generative Modeling of Neural Dynamics via Latent Stochastic Differential Equations</title>
      <link>https://arxiv.org/abs/2412.12112</link>
      <description>arXiv:2412.12112v2 Announce Type: replace 
Abstract: We propose a probabilistic framework for developing computational models of biological neural systems. In this framework, physiological recordings are viewed as discrete-time partial observations of an underlying continuous-time stochastic dynamical system which implements computations through its state evolution. To model this dynamical system, we employ a system of coupled stochastic differential equations with differentiable drift and diffusion functions and use variational inference to infer its states and parameters. This formulation enables seamless integration of existing mathematical models in the literature, neural networks, or a hybrid of both to learn and compare different models. We demonstrate this in our framework by developing a generative model that combines coupled oscillators with neural networks to capture latent population dynamics from single-cell recordings. Evaluation across three neuroscience datasets spanning different species, brain regions, and behavioral tasks show that these hybrid models achieve competitive performance in predicting stimulus-evoked neural and behavioral responses compared to sophisticated black-box approaches while requiring an order of magnitude fewer parameters, providing uncertainty estimates, and offering a natural language for interpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12112v2</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahmed ElGazzar, Marcel van Gerven</dc:creator>
    </item>
    <item>
      <title>Attention Is Not Retention: The Orthogonality Constraint in Infinite-Context Architectures</title>
      <link>https://arxiv.org/abs/2601.15313</link>
      <description>arXiv:2601.15313v2 Announce Type: replace 
Abstract: Biological memory solves a problem that eludes current AI: storing specific episodic facts without corrupting general semantic knowledge. Complementary Learning Systems theory explains this through two subsystems - a fast hippocampal system using sparse, pattern-separated representations for episodes, and a slow neocortical system using distributed representations for statistical regularities. Current AI systems lack this separation, attempting to serve both functions through neural weights alone. We identify the Orthogonality Constraint: reliable memory requires orthogonal keys, but semantic embeddings cannot be orthogonal because training clusters similar concepts together. The result is Semantic Interference (connecting to what cognitive psychologists have long observed in human memory), where neural systems writing facts into shared continuous parameters collapse to near-random accuracy within tens of semantically related facts. Through semantic density (rho), the mean pairwise cosine similarity, we show collapse occurs at N=5 facts (rho &gt; 0.6) or N ~ 20-75 (moderate rho). We validate across modalities: 16,309 Wikipedia facts, scientific measurements (rho = 0.96, 0.02% accuracy at N=10,000), and image embeddings (rho = 0.82, 0.05% at N=2,000). This failure is geometric - no increase in model capacity can overcome interference when keys share semantic overlap. We propose Knowledge Objects (KOs): structured facts with hash-based identity, controlled vocabularies, and explicit version chains. On Wikipedia facts, KO retrieval achieves 45.7% where Modern Hopfield Networks collapse to near-zero; hash-based retrieval maintains 100%. Production systems (Claude Memory, ChatGPT Memory) store unstructured text, causing schema drift (40-70% consistency) and version ambiguity. Knowledge Objects provide the discrete hippocampal component that enables reliable bicameral memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15313v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oliver Zahn, Matt Beton, Simran Chana</dc:creator>
    </item>
    <item>
      <title>Mesoscale tissue properties and electric fields in brain stimulation: Bridging the macroscopic and microscopic scales using layer-specific cortical conductivity</title>
      <link>https://arxiv.org/abs/2511.16465</link>
      <description>arXiv:2511.16465v5 Announce Type: replace-cross 
Abstract: Accurate simulations of electric fields (E-fields) in neural stimulation depend on tissue conductivity representations that link underlying microscopic tissue structure with macroscopic assumptions. Mesoscale conductivity variations can produce meaningful changes in E-fields and neural activation thresholds but remain largely absent from standard macroscopic models. Conductivity variations within the cortex are expected given the differences in cell density and volume fraction across layers. We review recent efforts modeling microscopic and mesoscopic E-fields and outline approaches that bridge micro- and macroscales to derive consistent mesoscale conductivity distributions. Using simplified microscopic models, effective tissue conductivity was estimated as a function of volume fraction of extracellular space, and the conductivities of different cortical layers were interpolated based on experimental volume fraction. The effective tissue conductivities were monotonically decreasing convex functions of the cell volume fraction. With decreasing cell volume fraction, the conductivity of cortical layers increased with depth from layer 2 to 6. Although the variation of conductivity within the cortex was small when compared to the conductivity of extracellular fluid (9% to 15%), the conductivity difference was considerably larger when compared between layers, e.g., with layer 3 and 6 being 20% and 50% more conductive than layer 2, respectively. The review and analysis provide a foundation for accurate multiscale models of E-fields and neural stimulation. Using layer-specific conductivity values within the cortex could improve the accuracy of estimations of thresholds and distributions of neural activation in E-field models of brain stimulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16465v5</guid>
      <category>physics.bio-ph</category>
      <category>physics.app-ph</category>
      <category>physics.med-ph</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boshuo Wang, Torge H. Worbs, Minhaj A. Hussain, Aman S. Aberra, Axel Thielscher, Warren M. Grill, Angel V. Peterchev</dc:creator>
    </item>
  </channel>
</rss>
