<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 27 Aug 2025 01:33:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Optimal Differentially Private Randomized Response Designs to Collect Sensitive Binary Data</title>
      <link>https://arxiv.org/abs/2508.16709</link>
      <description>arXiv:2508.16709v1 Announce Type: new 
Abstract: Randomized response has long been used in statistical surveys to estimate the proportion of sensitive groups in a population while protecting the privacy of respondents. More recently, this technique has been adopted by organizations that generate synthetic data from real personal binary data, enabling data storage and sharing for research or commercial purposes without compromising individual privacy. While the main aim in statistical surveys is the accurate estimation of sensitive group proportions, synthetic data generation prioritizes privacy preservation. To achieve precise estimation, statisticians typically determine the required sample size based on a pre-specified power of hypothesis testing. However, we find that designing randomized response studies to achieve high statistical power can come at the expense of increased privacy risk. In this work, we analyze how various established randomized response designs perform with respect to both statistical power and differential privacy (DP), the latter quantifying the risk of privacy leakage. We demonstrate that commonly used design strategies may result in either insufficient power or excessive privacy loss. To address this issue, we propose optimal choices of design parameters across different randomized response models to simultaneously achieve the desired statistical power and maintain differential privacy within acceptable bounds. The practical advantages of these optimal designs are illustrated through comprehensive simulation studies and a real data application. Additionally, we present a user-friendly Shiny-App to assist researchers in designing randomized response studies and performing the associated data analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16709v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bittu Karmakar, Palash Ghosh</dc:creator>
    </item>
    <item>
      <title>From Partial Exchangeability to Predictive Probability: A Bayesian Perspective on Classification</title>
      <link>https://arxiv.org/abs/2508.16716</link>
      <description>arXiv:2508.16716v1 Announce Type: new 
Abstract: We propose a novel Bayesian nonparametric classification model that combines a Gaussian process prior for the latent function with a Dirichlet process prior for the link function, extending the interpretative framework of de Finetti representation theorem and the construction of random distribution functions made by Ferguson (1973). This approach allows for flexible uncertainty modeling in both the latent score and the mapping to probabilities. We demonstrate the method performance using simulated data where it outperforms standard logistic regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16716v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcio Alves Diniz</dc:creator>
    </item>
    <item>
      <title>CHIMA: a correlation-aware high-dimensional mediation analysis with its application to the living brain project study</title>
      <link>https://arxiv.org/abs/2508.16883</link>
      <description>arXiv:2508.16883v1 Announce Type: new 
Abstract: Mediation analysis examines the pathways through which mediators transmit the effect of an exposure to an outcome. In high-dimensional settings, the joint significance test is commonly applied using variable screening followed by statistical inference. However, when mediators are highly correlated, existing methods may experience reduced statistical power due to inaccurate screening and residual bias in asymptotic inference. To address these issues, we propose CHIMA (Correlation-aware High-dimensional Mediation Analysis), an extension of a recently developed high-dimensional mediation analysis framework that enhances performance under correlation by integrating two advances: (i) high-dimensional ordinary least squares projection for accurate screening under correlation; and (ii) approximate orthogonalization for bias reduction. Simulation studies demonstrate that CHIMA effectively identifies active mediators even in the presence of strong correlations and outperforms competing methods across various settings. We further apply CHIMA to ribonucleic acid sequencing (RNA-seq) from the Living Brain Project, identifying genes that mediate the effect of Parkinson's disease on brain cell composition, thereby revealing cell-type-specific mechanisms of disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16883v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Osarfo, Sangyoon Yi, Weijia Fu, Seungjun Ahn</dc:creator>
    </item>
    <item>
      <title>Efficient Semiparametric Inference for Distributed Data with Blockwise Missingness</title>
      <link>https://arxiv.org/abs/2508.16902</link>
      <description>arXiv:2508.16902v1 Announce Type: new 
Abstract: We consider statistical inference for a finite-dimensional parameter in a regular semiparametric model under a distributed setting with blockwise missingness, where entire blocks of variables are unavailable at certain sites and sharing individual-level data is not allowed. To improve efficiency of the internal study, we propose a class of augmented one-step estimators that incorporate information from external sites through ``transfer functions.'' The proposed approach has several advantages. First, it is communication-efficient, requiring only one-round communication of summary-level statistics. Second, it satisfies a do-no-harm property in the sense that the augmented estimator is no less efficient than the original one based solely on the internal data. Third, it is statistically optimal, achieving the semiparametric efficiency bound when the transfer function is appropriately estimated from data. Finally, it is scalable, remaining asymptotically normal even when the number of external sites and the data dimension grow exponentially with the internal sample size. Simulation studies confirm both the statistical efficiency and computational feasibility of our method in distributed settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16902v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyue Huang, Huiyuan Wang, Yuqing Lei, Yong Chen</dc:creator>
    </item>
    <item>
      <title>Challenges in Statistics: A Dozen Challenges in Causality and Causal Inference</title>
      <link>https://arxiv.org/abs/2508.17099</link>
      <description>arXiv:2508.17099v1 Announce Type: new 
Abstract: Causality and causal inference have emerged as core research areas at the interface of modern statistics and domains including biomedical sciences, social sciences, computer science, and beyond. The field's inherently interdisciplinary nature -- particularly the central role of incorporating domain knowledge -- creates a rich and varied set of statistical challenges. Much progress has been made, especially in the last three decades, but there remain many open questions. Our goal in this discussion is to outline research directions and open problems we view as particularly promising for future work. Throughout we emphasize that advancing causal research requires a wide range of contributions, from novel theory and methodological innovations to improved software tools and closer engagement with domain scientists and practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17099v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Cinelli, Avi Feller, Guido Imbens, Edward Kennedy, Sara Magliacane, Jose Zubizarreta</dc:creator>
    </item>
    <item>
      <title>Alternative statistical inference for the first normalized incomplete moment</title>
      <link>https://arxiv.org/abs/2508.17145</link>
      <description>arXiv:2508.17145v1 Announce Type: new 
Abstract: This paper re-examines the first normalized incomplete moment, a well-established measure of inequality with wide applications in economic and social sciences. Despite the popularity of the measure itself, existing statistical inference appears to lag behind the needs of modern-age analytics. To fill this gap, we propose an alternative solution that is intuitive, computationally efficient, mathematically equivalent to the existing solutions for "standard" cases, and easily adaptable to "non-standard" ones. The theoretical and practical advantages of the proposed methodology are demonstrated via both simulated and real-life examples. In particular, we discover that a common practice in industry can lead to highly non-trivial challenges for trustworthy statistical inference, or misleading decision making altogether.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17145v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiannan Lu, Peng Ding, Anqi Zhao</dc:creator>
    </item>
    <item>
      <title>Unit-Modified Weibull Distribution and Quantile Regression Model</title>
      <link>https://arxiv.org/abs/2508.17359</link>
      <description>arXiv:2508.17359v1 Announce Type: new 
Abstract: The Sustainable Development Goals (SDGs) of the United Nations consist of 17 general objectives, subdivided into 169 targets to be achieved by 2030. Several SDG indices and indicators require continuous analysis and evaluation, and most of these indices are supported in the unit interval (0,1). To incorporate the flexibility of the modified Weibull (MW) distribution in doubly constrained datasets, the first objective of this work is to propose a new unit probability distribution based on the MW distribution. For this, a transformation of the MW distribution is applied, through which the unit modified Weibull (UMW) distribution is obtained. The second objective of this work is to introduce a quantile regression model for random variables with UMW distribution, reparameterized in terms of the quantiles of the distribution. Maximum likelihood estimators (MLEs) are used to estimate the model parameters. Monte Carlo simulations are performed to evaluate the MLE properties of the model parameters in finite sample sizes. The introduced methods are used for modeling some sustainability indicators related to the SDGs, also considering the reading skills of dyslexic children, which are indirectly associated with SDG 4 (Quality Education) and SDG 3 (Health and Well-Being).</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17359v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jo\~ao In\'acio Scrimini, Cleber Bisognin, Renata Rojas Guerra, F\'abio M. Bayer</dc:creator>
    </item>
    <item>
      <title>Two-Sample Testing with Block-Wise Missingness in Multi-Source Data</title>
      <link>https://arxiv.org/abs/2508.17411</link>
      <description>arXiv:2508.17411v1 Announce Type: new 
Abstract: Multi-source and multi-modal datasets are increasingly common in scientific research, yet they often exhibit block-wise missingness, where entire data sources or modalities are systematically absent for subsets of subjects. This structured form of missingness presents significant challenges for statistical analysis, particularly for two-sample hypothesis testing. Standard approaches such as imputation or complete-case analysis can introduce bias or result in substantial information loss, especially when the missingness mechanism is not random. To address this methodological gap, we propose the Block-Pattern Enhanced Test (BPET), a general framework for two-sample testing that directly accounts for block-wise missingness without requiring imputation or deletion of observations. As a concrete instantiation, we develop the Block-wise Rank In Similarity graph Edge-count (BRISE) test, which extends rank-based similarity graph methods to settings with block-wise missing data. Under mild conditions, we establish that the null distribution of BRISE converges to a chi-squared distribution. Simulation studies show that BRISE consistently controls the type I error rate and achieves good statistical power under a wide range of alternatives. Applications to two real-world datasets with block-wise missingness further demonstrate the practical utility of our method in identifying meaningful distributional differences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17411v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kejian Zhang, Muxuan Liang, Robert Maile, Doudou Zhou</dc:creator>
    </item>
    <item>
      <title>Tracking Temporal Evolution of Topological Features in Image Data</title>
      <link>https://arxiv.org/abs/2508.17530</link>
      <description>arXiv:2508.17530v1 Announce Type: new 
Abstract: Topological Data Analysis (TDA) can be used to detect and characterize holes in an image, such as zero-dimensional holes (connected components) or one-dimensional holes (loops). However, there is currently no widely accepted statistical framework for modeling spatiotemporal dependence in the evolution of topological features, such as holes, within a time series of images. We propose a hypothesis testing framework to identify statistically significant topological features of images in space and time, simultaneously. This addition of time may induce higher-dimensional topological features which can be used to establish temporal connections between the lower-dimensional features at each point in time. The temporal evolution of these lower-dimensional features is then represented on a zigzag persistence diagram, as a topological summary statistic focused on time dynamics. We demonstrate that the method effectively captures the emergence and progression of topological features in a study of a series of images of a wounded cell as it repairs. The proposed method outperforms a current approach in a simulation study that includes features of the wound healing process. Since, the wounded cell images exhibit nonlinear, dynamic, spatial, and temporal structures during single-cell repair, they provide a good application for this method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17530v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Susan Glenn, Jessi Cisewski-Kehe, Jun Zhu, William M Bement</dc:creator>
    </item>
    <item>
      <title>Copas-Jackson-type bounds for publication bias over a general class of selection models</title>
      <link>https://arxiv.org/abs/2508.17716</link>
      <description>arXiv:2508.17716v1 Announce Type: new 
Abstract: Publication bias (PB) is one of the most vital threats to the accuracy of meta-analysis. Adjustment or sensitivity analysis based on selection models, which describe the probability of a study being published, provide a more objective evaluation of PB than widely-used simple graphical methods such as the trim-and-fill method. Most existing methods rely on parametric selection models. The Copas-Jackson bound (C-J bound) provides a worst-case bound of an analytical form over a nonparametric class of selection models, which would provide more robust conclusions than parametric sensitivity analysis. The nonparametric class of the selection models in the C-J bound is restrictive and only covers parametric selection models monotonic to the standard errors of outcomes. The novelty of this paper is to develop a method that constructs worst-case bounds over a general class of selection models weakening the assumption in the C-J bound. We propose an efficient numerical method to obtain an approximate worst-case bound via tractable nonlinear programming with linear constraints. We substantiate the effectiveness of the proposed bound with extensive simulation studies and show its applicability with two real-world meta-analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17716v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taojun Hu, Yi Zhou, Xiao-Hua Zhou, Satoshi Hattori</dc:creator>
    </item>
    <item>
      <title>Efficient Inference under Label Shift in Unsupervised Domain Adaptation</title>
      <link>https://arxiv.org/abs/2508.17780</link>
      <description>arXiv:2508.17780v1 Announce Type: new 
Abstract: In many real-world applications, researchers aim to deploy models trained in a source domain to a target domain, where obtaining labeled data is often expensive, time-consuming, or even infeasible. While most existing literature assumes that the labeled source data and the unlabeled target data follow the same distribution, distribution shifts are common in practice. This paper focuses on label shift and develops efficient inference procedures for general parameters characterizing the unlabeled target population. A central idea is to model the outcome density ratio between the labeled and unlabeled data. To this end, we propose a progressive estimation strategy that unfolds in three stages: an initial heuristic guess, a consistent estimation, and ultimately, an efficient estimation. This self-evolving process is novel in the statistical literature and of independent interest. We also highlight the connection between our approach and prediction-powered inference (PPI), which uses machine learning models to improve statistical inference in related settings. We rigorously establish the asymptotic properties of the proposed estimators and demonstrate their superior performance compared to existing methods. Through simulation studies and multiple real-world applications, we illustrate both the theoretical contributions and practical benefits of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17780v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seong-ho Lee, Yanyuan Ma, Jiwei Zhao</dc:creator>
    </item>
    <item>
      <title>Outlier-robust Bayesian Multivariate Analysis with Correlation-intact Sandwich Mixture</title>
      <link>https://arxiv.org/abs/2508.18004</link>
      <description>arXiv:2508.18004v1 Announce Type: new 
Abstract: Handling outliers is a fundamental challenge in multivariate data analysis, as outliers may distort structures of correlation or conditional independence. Although robust Bayesian inference has been extensively studied for univariate settings, theoretical results ensuring posterior robustness in multivariate models are scarce. We propose a novel scale mixture of multivariate normals called correlation-intact sandwich mixture, where the scale parameters are real-valued and follow the unfolded log-Pareto distribution. Our theoretical results on posterior robustness in multivariate settings emphasizes that the use of a symmetric, super heavy-tailed distribution for the scale parameters is essential in achieving posterior robustness against element-wise contamination. Posterior inference for the proposed model is feasible by an efficient Gibbs sampling algorithm we developed. The superiority of the proposed method is illustrated further in simulation and empirical studies using graphical models and multivariate regression in the presence of complex outlier structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18004v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasuyuki Hamura, Kaoru Irie, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Assessing the conditional calibration of interval forecasts using decompositions of the interval score</title>
      <link>https://arxiv.org/abs/2508.18034</link>
      <description>arXiv:2508.18034v1 Announce Type: new 
Abstract: Forecasts for uncertain future events should be probabilistic. Probabilistic forecasts are commonly issued as prediction intervals, which provide a measure of uncertainty in the unknown outcome whilst being easier to understand and communicate than full predictive distributions. The calibration of a $(1 - \alpha)$-level prediction interval can be assessed by checking whether the probability that the outcome falls within the interval is equal to $1 - \alpha$. However, such coverage checks are typically unconditional and therefore relatively weak. Although this is well known, there is a lack of methods to assess the conditional calibration of interval forecasts. In this work, we demonstrate how this can be achieved via decompositions of the well-known interval (or Winkler) score. We study notions of calibration for interval forecasts and then introduce a decomposition of the interval score based on isotonic distributional regression. This decomposition exhibits many desirable properties, both in theory and in practice, which allows users to accurately assess the conditional calibration of interval forecasts. This is illustrated on simulated data and in three applications to benchmark regression datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18034v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sam Allen, Julia Burnello, Johanna Ziegel</dc:creator>
    </item>
    <item>
      <title>Estimating the average treatment effect in cluster-randomized trials with misclassified outcomes and non-random validation subsets</title>
      <link>https://arxiv.org/abs/2508.18137</link>
      <description>arXiv:2508.18137v2 Announce Type: new 
Abstract: Randomized trials are viewed as the benchmark for assessing causal effects of treatments on outcomes of interest. Nonetheless, challenges such as measurement error can undermine the standard causal assumptions for randomized trials. In ASPIRE, a cluster-randomized trial, pediatric primary care clinics were assigned to one of two treatments aimed at promoting clinician delivery of a secure firearm program to parents during well-child visits. A key outcome of interest is thus parent receipt of the program at each visit. Clinicians documented program delivery in patients' electronic health records for all visits, but their reporting is a proxy measure for the parent receipt outcome. Parents were also surveyed to report directly on program receipt after their child's visit; however, only a small subset of them completed the survey. Here, we develop a causal inference framework for a binary outcome that is subject to misclassification through silver-standard measures (clinician reports), but gold-standard measures (parent reports) are only available for a non-random internal validation subset. We propose a method for identifying the average treatment effect (ATE) that addresses the risk of bias due to misclassification and non-random validation selection, even when the outcome (parent receipt) may directly impact selection propensity (survey responsiveness). We show that ATE estimation relies on specifying the relationship between the gold- and silver-standard outcome measures in the validation subset, which may depend on treatment and covariates. Additionally, the clustered design is reflected in our causal assumptions and in our cluster-robust approach to estimation of the ATE. Simulation studies demonstrate acceptable finite-sample operating characteristics of our ATE estimator, supporting its application to ASPIRE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18137v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dane Isenberg, Nandita Mitra, Steven C. Marcus, Rinad S. Beidas, Kristin A. Linn</dc:creator>
    </item>
    <item>
      <title>Matching-Based Nonparametric Estimation of Group Average Treatment Effects</title>
      <link>https://arxiv.org/abs/2508.18157</link>
      <description>arXiv:2508.18157v1 Announce Type: new 
Abstract: Heterogeneous treatment effects, which vary according to individual covariates, are crucial in fields such as personalized medicine and tailored treatment strategies. In many applications, rather than considering the heterogeneity induced by all covariates, practitioners focus on a few key covariates to develop tailored treatment decisions. Based on this, we aim to estimate the group average treatment effects (GATEs), which represent heterogeneous treatment effects across subpopulations defined by certain key covariates. Previous strategies for estimating GATEs, such as weighting-based and regression-based methods, suffer from instability or extrapolation bias, especially when several propensity scores are close to zero or one. To address these limitations, we propose two novel nonparametric estimation methods: a matching-based method and a bias-corrected matching method for estimating GATEs. The matching-based method imputes potential outcomes using a matching technique, followed by a nonparametric regression. This method avoids the instability caused by extreme propensity scores but may introduce non-negligible bias when the dimension of full covariates is high. To mitigate this, the bias-corrected matching estimator incorporates additional outcome regression models, enhancing robustness and reducing bias. We show the consistency, double robustness, and asymptotic normality of the bias-corrected matching estimator. We empirically demonstrate the advantages of the proposed methods with extensive simulation studies and a real-world application. An open-source R package, MatchGATE, is available to implement the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18157v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Wu, Pengtao Zeng, Zhaoqing Tian, Shaojie Wei</dc:creator>
    </item>
    <item>
      <title>Multidimensional Distributional Neural Network Output Demonstrated in Super-Resolution of Surface Wind Speed</title>
      <link>https://arxiv.org/abs/2508.16686</link>
      <description>arXiv:2508.16686v1 Announce Type: cross 
Abstract: Accurate quantification of uncertainty in neural network predictions remains a central challenge for scientific applications involving high-dimensional, correlated data. While existing methods capture either aleatoric or epistemic uncertainty, few offer closed-form, multidimensional distributions that preserve spatial correlation while remaining computationally tractable. In this work, we present a framework for training neural networks with a multidimensional Gaussian loss, generating closed-form predictive distributions over outputs with non-identically distributed and heteroscedastic structure. Our approach captures aleatoric uncertainty by iteratively estimating the means and covariance matrices, and is demonstrated on a super-resolution example. We leverage a Fourier representation of the covariance matrix to stabilize network training and preserve spatial correlation. We introduce a novel regularization strategy -- referred to as information sharing -- that interpolates between image-specific and global covariance estimates, enabling convergence of the super-resolution downscaling network trained on image-specific distributional loss functions. This framework allows for efficient sampling, explicit correlation modeling, and extensions to more complex distribution families all without disrupting prediction performance. We demonstrate the method on a surface wind speed downscaling task and discuss its broader applicability to uncertainty-aware prediction in scientific models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16686v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harrison J. Goldwyn, Mitchell Krock, Johann Rudi, Daniel Getter, Julie Bessac</dc:creator>
    </item>
    <item>
      <title>Factor Informed Double Deep Learning For Average Treatment Effect Estimation</title>
      <link>https://arxiv.org/abs/2508.17136</link>
      <description>arXiv:2508.17136v1 Announce Type: cross 
Abstract: We investigate the problem of estimating the average treatment effect (ATE) under a very general setup where the covariates can be high-dimensional, highly correlated, and can have sparse nonlinear effects on the propensity and outcome models. We present the use of a Double Deep Learning strategy for estimation, which involves combining recently developed factor-augmented deep learning-based estimators, FAST-NN, for both the response functions and propensity scores to achieve our goal. By using FAST-NN, our method can select variables that contribute to propensity and outcome models in a completely nonparametric and algorithmic manner and adaptively learn low-dimensional function structures through neural networks. Our proposed novel estimator, FIDDLE (Factor Informed Double Deep Learning Estimator), estimates ATE based on the framework of augmented inverse propensity weighting AIPW with the FAST-NN-based response and propensity estimates. FIDDLE consistently estimates ATE even under model misspecification and is flexible to also allow for low-dimensional covariates. Our method achieves semiparametric efficiency under a very flexible family of propensity and outcome models. We present extensive numerical studies on synthetic and real datasets to support our theoretical guarantees and establish the advantages of our methods over other traditional choices, especially when the data dimension is large.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17136v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianqing Fan, Soham Jana, Sanjeev Kulkarni, Qishuo Yin</dc:creator>
    </item>
    <item>
      <title>The Root Finding Problem Revisited: Beyond the Robbins-Monro procedure</title>
      <link>https://arxiv.org/abs/2508.17591</link>
      <description>arXiv:2508.17591v1 Announce Type: cross 
Abstract: We introduce Sequential Probability Ratio Bisection (SPRB), a novel stochastic approximation algorithm that adapts to the local behavior of the (regression) function of interest around its root. We establish theoretical guarantees for SPRB's asymptotic performance, showing that it achieves the optimal convergence rate and minimal asymptotic variance even when the target function's derivative at the root is small (at most half the step size), a regime where the classical Robbins-Monro procedure typically suffers reduced convergence rates. Further, we show that if the regression function is discontinuous at the root, Robbins-Monro converges at a rate of $1/n$ whilst SPRB attains exponential convergence. If the regression function has vanishing first-order derivative, SPRB attains a faster rate of convergence compared to stochastic approximation. As part of our analysis, we derive a nonasymptotic bound on the expected sample size and establish a generalized Central Limit Theorem under random stopping times. Remarkably, SPRB automatically provides nonasymptotic time-uniform confidence sequences that do not explicitly require knowledge of the convergence rate. We demonstrate the practical effectiveness of SPRB through simulation results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17591v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Yu, Moulinath Banerjee, Ya'acov Ritov</dc:creator>
    </item>
    <item>
      <title>Enhancing Differentially Private Linear Regression via Public Second-Moment</title>
      <link>https://arxiv.org/abs/2508.18037</link>
      <description>arXiv:2508.18037v1 Announce Type: cross 
Abstract: Leveraging information from public data has become increasingly crucial in enhancing the utility of differentially private (DP) methods. Traditional DP approaches often require adding noise based solely on private data, which can significantly degrade utility. In this paper, we address this limitation in the context of the ordinary least squares estimator (OLSE) of linear regression based on sufficient statistics perturbation (SSP) under the unbounded data assumption. We propose a novel method that involves transforming private data using the public second-moment matrix to compute a transformed SSP-OLSE, whose second-moment matrix yields a better condition number and improves the OLSE accuracy and robustness. We derive theoretical error bounds about our method and the standard SSP-OLSE to the non-DP OLSE, which reveal the improved robustness and accuracy achieved by our approach. Experiments on synthetic and real-world datasets demonstrate the utility and effectiveness of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18037v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zilong Cao (The School of Mathematics, Northwest University), Hai Zhang (The School of Mathematics, Northwest University)</dc:creator>
    </item>
    <item>
      <title>Testing Simultaneous Diagonalizability</title>
      <link>https://arxiv.org/abs/2101.07776</link>
      <description>arXiv:2101.07776v2 Announce Type: replace 
Abstract: This paper proposes novel methods to test for simultaneous diagonalization of possibly asymmetric matrices. Motivated by various applications, a two-sample test as well as a generalization for multiple matrices are proposed. A partial version of the test is also studied to check whether a partial set of eigenvectors is shared across samples. Additionally, a novel algorithm for the considered testing methods is introduced. Simulation studies demonstrate favorable performance for all designs. Finally, the theoretical results are utilized to decouple vector autoregression models into multiple univariate time series, and to test for the same stationary distribution in recurrent Markov chains. These applications are demonstrated using macroeconomic indices of 8 countries and streamflow data, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.07776v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2023.2202435</arxiv:DOI>
      <arxiv:journal_reference>Journal of the American Statistical Association 119 (2024) 1513-1525</arxiv:journal_reference>
      <dc:creator>Yuchen Xu, Marie-Christine D\"uker, David S. Matteson</dc:creator>
    </item>
    <item>
      <title>Causal Inference in Longitudinal Data under Unknown Interference</title>
      <link>https://arxiv.org/abs/2106.15074</link>
      <description>arXiv:2106.15074v5 Announce Type: replace 
Abstract: In longitudinal studies where units are embedded in space or a social network, interference may arise, meaning that a unit's outcome can depend on treatment histories of others. The presence of interference poses significant challenges for causal inference, particularly when the interference structure -- how a unit's outcome responds to others' influences -- is complex, heterogeneous, and unknown to researchers. This paper develops a general framework for identifying and estimating both direct and spillover effects of treatment histories under minimal assumptions about the interference structure. We introduce a class of causal estimands that capture the effects of treatment histories at any specified proximity level and show that they can be represented by a modified marginal structural model. Under sequential exchangeability, these estimands are identifiable and can be estimated using inverse probability weighting. We derive conditions for consistency and asymptotic normality of the estimators and provide procedures for constructing asymptotically conservative confidence intervals. The method's utility is demonstrated through applications in both social science and biomedical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.15074v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Wang, Michael Jetsupphasuk</dc:creator>
    </item>
    <item>
      <title>Two-step estimation of latent trait models</title>
      <link>https://arxiv.org/abs/2303.16101</link>
      <description>arXiv:2303.16101v3 Announce Type: replace 
Abstract: We consider likelihood-based two-step estimation of latent variable models, in which just the measurement model is estimated in the first step and the measurement parameters are then fixed at their estimated values in the second step where the structural model is estimated. We show how this approach can be implemented for latent trait models (item response theory models) where the latent variables are continuous and their measurement indicators are categorical variables. The properties of two-step estimators are examined using simulation studies and applied examples. They perform well, and have attractive practical and conceptual properties compared to the alternative one-step and three-step approaches. These results are in line with previous findings for other families of latent variable models. This provides strong evidence that two-step estimation is a flexible and useful general method of estimation for different types of latent variable models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.16101v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jouni Kuha, Zsuzsa Bakk</dc:creator>
    </item>
    <item>
      <title>Winner's Curse Free Robust Mendelian Randomization with Summary Data</title>
      <link>https://arxiv.org/abs/2309.04957</link>
      <description>arXiv:2309.04957v3 Announce Type: replace 
Abstract: In the past decade, the increased availability of genome-wide association studies summary data has popularized Mendelian Randomization (MR) for conducting causal inference. MR analyses, incorporating genetic variants as instrumental variables, are known for their robustness against reverse causation bias and unmeasured confounders. Nevertheless, classical MR analyses utilizing summary data may still produce biased causal effect estimates due to the winner's curse and pleiotropic issues. To address these two issues and establish valid causal conclusions, we propose a unified robust Mendelian Randomization framework with summary data, which systematically removes the winner's curse and screens out invalid genetic instruments with pleiotropic effects. Different from existing robust MR literature, our framework delivers valid statistical inference on the causal effect neither requiring the genetic pleiotropy effects to follow any parametric distribution nor relying on perfect instrument screening property. Under appropriate conditions, we show that our proposed estimator converges to a normal distribution and its variance can be well estimated. We demonstrate the performance of our proposed estimator through Monte Carlo simulations and two case studies. The codes implementing the procedures are available at https://github.com/ChongWuLab/CARE/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.04957v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongming Xie, Wanheng Zhang, Jingshen Wang, Chong Wu</dc:creator>
    </item>
    <item>
      <title>Utilizing Multiple Testing for Grouping in Singular Spectrum Analysis</title>
      <link>https://arxiv.org/abs/2401.01665</link>
      <description>arXiv:2401.01665v3 Announce Type: replace 
Abstract: A key step in separating signal from noise in time series by means of singular spectrum analysis (SSA) is grouping. We present a multiple testing method for the grouping step in SSA. As separability criterion, we utilize the weighted correlation between the signal and the noise component of the (reconstructed) time series, and we test whether this weighted correlation is equal to zero. This test has to be performed for several possible groupings, resulting in a multiple test problem. The null distributions of the corresponding test statistics are approximated by a wild bootstrap procedure. The performance of our proposed method is assessed in a simulation study, and we illustrate its practical application with an analysis of real world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01665v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maryam Movahedifar, Friederike Preusse, Anna Vesely, Thorsten Dickhaus</dc:creator>
    </item>
    <item>
      <title>Linear Discriminant Regularized Regression</title>
      <link>https://arxiv.org/abs/2402.14260</link>
      <description>arXiv:2402.14260v3 Announce Type: replace 
Abstract: Linear Discriminant Analysis (LDA) is an important classification approach. Its simple linear form makes it easy to interpret, and it is well-suited for handling multi-class responses. It is closely related to other classical multivariate statistical techniques, such as Fisher's discriminant analysis, canonical correlation analysis and linear regression. In this paper we strengthen its connection to multivariate response regression by characterizing the explicit relationship between discriminant directions and regression coefficients. This key characterization leads to a new regression-based multi-class classification procedure that is flexible enough to deploy any existing structured, regularized, and even non-parametric, regression methods. In contrast with the existing regression-based approaches, our new formulation is particularly amenable to establish provable guarantees: we establish a general strategy of analyzing the excess misclassification risk of the proposed classifier for all aforementioned regression techniques. As applications, we provide complete theoretical guarantees for using the widely used $\ell_1$-regularization as well as for using the reduced-rank regression, neither of which has yet been fully analyzed in the LDA context. Our theoretical findings are corroborated by extensive simulation studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14260v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Bing, Bingqing Li, Marten Wegkamp</dc:creator>
    </item>
    <item>
      <title>Practical limitations for real-life application of data fission and data thinning in post-clustering differential analysis</title>
      <link>https://arxiv.org/abs/2405.13591</link>
      <description>arXiv:2405.13591v3 Announce Type: replace 
Abstract: Post-clustering inference in single-cell RNA sequencing (scRNA-seq) analysis presents significant challenges in controlling Type I error during differential expression analysis. Data fission, a promising approach that aims to split data into two independent parts, relies on strong parametric assumptions of non-mixture distributions that are inherently violated in clustered data. To address this limitation, we introduce conditional data fission, an extension designed to decompose each mixture component into two independent parts. However, we demonstrate that applying such conditional data fission to mixture distributions requires prior knowledge of the clustering structure to ensure valid post-clustering inference. This arises from the need to accurately estimate component-specific scale parameters, which are critical for performing decomposition while maintaining independence. We theoretically quantify how biases in estimating these parameters lead to inflated Type I error rates due to deviations from independence. Given that mixture components are typically unknown in practice, our results underscore the fundamental difficulty of applying data fission in real-world settings, despite its prior proposal as a solution for post-clustering inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13591v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Hivert, Denis Agniel, Rodolphe Thi\'ebaut, Boris P. Hejblum</dc:creator>
    </item>
    <item>
      <title>Design-based Causal Inference for Incomplete Block Designs</title>
      <link>https://arxiv.org/abs/2405.19312</link>
      <description>arXiv:2405.19312v4 Announce Type: replace 
Abstract: Researchers often turn to block randomization to increase the precision of their inference or due to practical considerations, such as in multisite trials. However, if the number of treatments under consideration is large it might not be feasible or practical to assign all treatments within each block. We develop novel inference results under the finite-population design-based framework for natural alternatives to the complete block design that do not require reducing the number of treatment arms, the incomplete block design (IBD) and the balanced incomplete block design. This includes deriving the properties of two design-based estimators, developing a finite-population central limit theorem, and proposing conservative variance estimators. Comparisons of the design-based estimators are made to linear model-based estimators. Simulations and a data illustration further demonstrate performance of IBD estimators. This work highlights IBDs as practical and currently underutilized designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19312v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taehyeon Koo, Nicole E. Pashley</dc:creator>
    </item>
    <item>
      <title>Factorial Difference-in-Differences</title>
      <link>https://arxiv.org/abs/2407.11937</link>
      <description>arXiv:2407.11937v4 Announce Type: replace 
Abstract: We formulate factorial difference-in-differences (FDID) as a research design that extends the canonical difference-in-differences (DID) to settings without clean controls. Such situations often arise when researchers exploit cross-sectional variation in a baseline factor and temporal variation in an event affecting all units. In these applications, the exact estimand is often unspecified and justification for using the DID estimator is unclear. We formalize FDID by characterizing its data structure, target parameters, and identifying assumptions. Framing FDID as a factorial design with two factors -- the baseline factor G and the exposure level Z, we define effect modification and causal moderation as the associative and causal effects of G on the effect of Z. Under standard DID assumptions, including no anticipation and parallel trends, the DID estimator identifies effect modification but not causal moderation. To identify the latter, we propose an additional factorial parallel trends assumption. We also show that the canonical DID is a special case of FDID under an exclusion restriction. We extend the framework to conditionally valid assumptions and clarify regression-based implementations. We then discuss extensions to repeated cross-sectional data and continuous G. We illustrate the approach with an empirical example on the role of social capital in famine relief in China.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11937v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yiqing Xu, Anqi Zhao, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Causal Effect Identification and Inference with Endogenous Exposures and a Light-tailed Error</title>
      <link>https://arxiv.org/abs/2408.06211</link>
      <description>arXiv:2408.06211v3 Announce Type: replace 
Abstract: Endogeneity poses significant challenges in causal inference across various research domains. This paper proposes a novel approach to identify and estimate causal effects in the presence of endogeneity. We consider a structural equation with endogenous exposures and an additive error term. Assuming the light-tailedness of the error term, we show that the causal effect can be identified by contrasting extreme conditional quantiles of the outcome given the exposures. Unlike many existing results, our identification approach does not rely on additional parametric assumptions or auxiliary variables. Building on the identification result, we develop a new method that estimates the causal effect using extreme quantile regression. We establish the consistency of the proposed extreme-based estimator under a general additive structural equation and demonstrate its asymptotic normality in the linear model setting. Simulations and data analysis of an automobile sale dataset show the effectiveness of our method in handling endogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06211v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruoyu Wang, Wang Miao</dc:creator>
    </item>
    <item>
      <title>When Does Interference Matter? Decision-Making in Platform Experiments</title>
      <link>https://arxiv.org/abs/2410.06580</link>
      <description>arXiv:2410.06580v4 Announce Type: replace 
Abstract: This paper investigates decision-making in A/B experiments for online platforms and marketplaces. In such settings, due to constraints on inventory, A/B experiments typically lead to biased estimators because of *interference* between treatment and control groups; this phenomenon has been well studied in recent literature. By contrast, there has been relatively little discussion of the impact of interference on decision-making. In this paper, we analyze a benchmark Markovian model of an inventory-constrained platform, where arriving customers book listings that are limited in supply. We focus on the commonly used frequentist hypothesis testing approach for making launch decisions based on data from customer-randomized experiments, and we study the impact of interference on (1) false positive probability and (2) statistical power.
  We obtain three main findings. First, we show that for *sign-consistent* treatments -- i.e., those where the treatment changes booking probabilities in the same direction relative to control for all states of inventory availability -- the false positive probability of a test statistic using the standard difference-in-means estimator with a corresponding na\"ive variance estimator is correctly controlled. Second, we demonstrate that for sign-consistent treatments in realistic settings, the statistical power of this na\"ive approach is higher than that of any similar pipeline using a debiased estimator. Taken together, these two findings suggest that platforms may be better off *not* debiasing when treatments are sign-consistent. Third, using numerics, we investigate false positive probability and statistical power when treatments are sign-inconsistent, and we show that in principle, the performance of the na\"ive approach can be arbitrarily worse in such cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06580v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ramesh Johari, Hannah Li, Anushka Murthy, Gabriel Y. Weintraub</dc:creator>
    </item>
    <item>
      <title>Nonlocal Prior Mixture-Based Bayesian Wavelet Regression with Application to Noisy Imaging and Audio Data</title>
      <link>https://arxiv.org/abs/2501.18134</link>
      <description>arXiv:2501.18134v3 Announce Type: replace 
Abstract: We propose a novel Bayesian wavelet regression approach using a three-component spike-and-slab prior for wavelet coefficients, combining a point mass at zero, a moment (MOM) prior, and an inverse moment (IMOM) prior. This flexible prior supports small and large coefficients differently, offering advantages for highly dispersed data where wavelet coefficients span multiple scales. The IMOM prior's heavy tails capture large coefficients, while the MOM prior is better suited for smaller non-zero coefficients. Further, our method introduces innovative hyperparameter specifications for mixture probabilities and scale parameters, including generalized logit, hyperbolic secant, and generalized normal decay for probabilities, and double exponential decay for scaling. Hyperparameters are estimated via an empirical Bayes approach, enabling posterior inference tailored to the data. Extensive simulations demonstrate significant performance gains over two-component wavelet methods. Applications to electroencephalography and noisy audio data illustrate the method's utility in capturing complex signal characteristics. We implement our method in an R package, NLPwavelet (&gt;= 1.1).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18134v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/math13162642</arxiv:DOI>
      <arxiv:journal_reference>Sanyal, N. (2025). Nonlocal Prior Mixture-Based Bayesian Wavelet Regression with Application to Noisy Imaging and Audio Data. Mathematics, 13(16), 2642</arxiv:journal_reference>
      <dc:creator>Nilotpal Sanyal</dc:creator>
    </item>
    <item>
      <title>Correction for nonignorable nonresponse bias in the estimation of turnout using callback data</title>
      <link>https://arxiv.org/abs/2504.14169</link>
      <description>arXiv:2504.14169v2 Announce Type: replace 
Abstract: Overestimation of turnout has long been an issue in election surveys, with nonresponse bias or voter overrepresentation regarded as one of the major sources of bias. However, the adjustment for nonignorable nonresponse bias is substantially challenging. Based on the ANES Non-Response Follow-Up Study concerning the 2020 U.S. presidential election, we investigate the role of callback data in adjusting for nonresponse bias in the estimation of turnout. Callback data are the records of contact attempts in the survey course, available in many modern large-scale surveys. We propose a stableness of resistance assumption to account for the nonignorable missingness in the outcome, which states that the impact of the missing outcome on the response propensity is stable in the first two call attempts. Under this assumption and by leveraging covariates information from the census data, we establish the identifiability and develop estimation methods for turnout, including a doubly robust estimator. Our methods produce estimates very close to the official turnout and successfully capture the trend of declining willingness to vote as response reluctance increases. This work hints at the importance of adjusting for nonignorable nonresponse bias and exhibits the promise of callback data for political surveys.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14169v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Li, Naiwen Ying, Kendrick Qijun Li, Xu Shi, Wang Miao</dc:creator>
    </item>
    <item>
      <title>Tensor-product interactions in Markov-switching models</title>
      <link>https://arxiv.org/abs/2507.01555</link>
      <description>arXiv:2507.01555v3 Announce Type: replace 
Abstract: Markov-switching models are a powerful tool for modelling time series data that are driven by underlying latent states. As such, they are widely used in behavioural ecology, where discrete states can serve as proxies for behavioural modes and enable inference on latent behaviour driving e.g. observed movement. To understand drivers of behavioural changes, it is common to link model parameters to covariates. Over the last decade, nonparametric approaches have gained traction in this context to avoid unrealistic parametric assumptions. Nonetheless, existing methods are largely limited to univariate smooth functions of covariates, based on penalised splines, while real processes are typically complex requiring consideration of interaction effects. We address this gap by incorporating tensor-product interactions into Markov-switching models, enabling flexible modelling of multidimensional effects in a computationally efficient manner. Based on the extended Fellner-Schall method, we develop an efficient automatic smoothness selection procedure that is robust and scales well with the number of smooth functions in the model. The method builds on a random effects view of the spline coefficients and yields a recursive penalised likelihood procedure. As special cases, this general framework accommodates bivariate smoothing, function-valued random effects, and space-time interactions. We demonstrate its practical utility through three ecological case studies of an African elephant, common fruitflies, and Arctic muskoxen. The methodology is implemented in the LaMa R package, providing applied ecologists with an accessible and flexible tool for semiparametric inference in hidden-state models. The approach has the potential to drastically improve the level of detail in inference, allowing to fit HMMs with hundreds of parameters, 10-20 (potentially bivariate) smooths to thousands of observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01555v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan-Ole Koslik</dc:creator>
    </item>
    <item>
      <title>Bayesian Forecast Combination with Predictive Priors via Particle Filtering</title>
      <link>https://arxiv.org/abs/2508.07136</link>
      <description>arXiv:2508.07136v2 Announce Type: replace 
Abstract: We propose a Bayesian forecast combination framework that, for the first time, embeds forward-looking signals, formulated as predictive priors, directly into the time-varying weight-updating process. This approach enables weights to adapt using both historical forecast performance and anticipated future model behavior. We implement the framework with model diversity as the forward-looking signal, yielding the diversity-driven time-varying weights (DTVW) method. Compared with the standard time-varying weights (TVW) approach, DTVW embeds diversity-driven predictive priors that penalize redundancy and encourage informative contributions across constituent models. Simulation experiments, covering both a simple complete model set and a complex misspecified environment, show that DTVW improves forecast accuracy by dynamically focusing on well-performing models. Empirical applications to multi-step-ahead oil price forecasts and bivariate forecasts of U.S. inflation and GDP growth confirm its superiority over benchmarks including Equal weighting, Bayesian Model Averaging, and standard TVW. Beyond accuracy gains, diversity-based predictive priors provide diagnostic insights into model incompleteness and forecast uncertainty, making DTVW both more adaptive and more informative than existing Bayesian combination methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07136v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaorui Luo, Yanfei Kang, Xue Luo</dc:creator>
    </item>
    <item>
      <title>Finite Sample Bounds for Sequential Monte Carlo and Adaptive Path Selection Using the $L_2$ Norm</title>
      <link>https://arxiv.org/abs/1807.01346</link>
      <description>arXiv:1807.01346v3 Announce Type: replace-cross 
Abstract: We prove a bound on the finite sample error of sequential Monte Carlo (SMC) on static spaces using the $L_2$ distance between interpolating distributions and the mixing times of Markov kernels. This result is unique in that it is the first finite sample convergence result for SMC that does not require an upper bound on the importance weights. Using this bound we show that careful selection of the interpolating distributions can lead to substantial improvements in the computational complexity of the algorithm. This result also justifies the adaptive selection of SMC distributions using the relative effective sample size commonly used in the literature, and we establish conditions guaranteeing the approximation accuracy of the adaptive SMC approach. We show that the commonly used data tempering approach fails to satisfy these conditions, and introduce a modified data tempering algorithm under which our guarantees do hold. We then demonstrate empirically that this procedure provides nearly-optimal sequences of distributions in an automatic fashion for realistic examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:1807.01346v3</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joe Marion, Joseph Mathews, Scott C. Schmidler</dc:creator>
    </item>
    <item>
      <title>On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization</title>
      <link>https://arxiv.org/abs/2405.16455</link>
      <description>arXiv:2405.16455v2 Announce Type: replace-cross 
Abstract: Accurately aligning large language models (LLMs) with human preferences is crucial for informing fair, economically sound, and statistically efficient decision-making processes. However, we argue that the predominant approach for aligning LLMs with human preferences through a reward model -- reinforcement learning from human feedback (RLHF) -- suffers from an inherent algorithmic bias due to its Kullback--Leibler-based regularization in optimization. In extreme cases, this bias could lead to a phenomenon we term preference collapse, where minority preferences are virtually disregarded. To mitigate this algorithmic bias, we introduce preference matching (PM) RLHF, a novel approach that provably aligns LLMs with the preference distribution of the reward model under the Bradley--Terry--Luce/Plackett--Luce model. Central to our approach is a PM regularizer that takes the form of the negative logarithm of the LLM's policy probability distribution over responses, which helps the LLM balance response diversification and reward maximization. Notably, we obtain this regularizer by solving an ordinary differential equation that is necessary for the PM property. For practical implementation, we introduce a conditional variant of PM RLHF that is tailored to natural language generation. Finally, we empirically validate the effectiveness of conditional PM RLHF through experiments on the OPT and Llama-family models, demonstrating a 29% to 41% improvement in alignment with human preferences, as measured by a certain metric, compared to standard RLHF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16455v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiancong Xiao, Ziniu Li, Xingyu Xie, Emily Getzen, Cong Fang, Qi Long, Weijie J. Su</dc:creator>
    </item>
    <item>
      <title>A new measure of dependence: Integrated $R^2$</title>
      <link>https://arxiv.org/abs/2505.18146</link>
      <description>arXiv:2505.18146v3 Announce Type: replace-cross 
Abstract: We propose a new measure of dependence that quantifies the degree to which a random variable $Y$ depends on a random vector $X$. This measure is zero if and only if $Y$ and $X$ are independent, and equals one if and only if $Y$ is a measurable function of $X$. We introduce a simple and interpretable estimator that is comparable in ease of computation to classical correlation coefficients such as Pearson's, Spearman's, or Chatterjee's. Building on this coefficient, we develop a model-free variable selection algorithm, feature ordering by dependence (FORD), inspired by FOCI. FORD requires no tuning parameters and is provably consistent under suitable sparsity assumptions. We demonstrate its effectiveness and improvements over FOCI through experiments on both synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18146v3</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mona Azadkia, Pouya Roudaki</dc:creator>
    </item>
    <item>
      <title>The purpose of an estimator is what it does: Misspecification, estimands, and over-identification</title>
      <link>https://arxiv.org/abs/2508.13076</link>
      <description>arXiv:2508.13076v2 Announce Type: replace-cross 
Abstract: In over-identified models, misspecification -- the norm rather than exception -- fundamentally changes what estimators estimate. Different estimators imply different estimands rather than different efficiency for the same target. A review of recent applications of generalized method of moments in the American Economic Review suggests widespread acceptance of this fact: There is little formal specification testing and widespread use of estimators that would be inefficient were the model correct, including the use of "hand-selected" moments and weighting matrices. Motivated by these observations, we review and synthesize recent results on estimation under model misspecification, providing guidelines for transparent and robust empirical research. We also provide a new theoretical result, showing that Hansen's J-statistic measures, asymptotically, the range of estimates achievable at a given standard error. Given the widespread use of inefficient estimators and the resulting researcher degrees of freedom, we thus particularly recommend the broader reporting of J-statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13076v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaiah Andrew, Jiafeng Chen, Otavio Tecchio</dc:creator>
    </item>
  </channel>
</rss>
