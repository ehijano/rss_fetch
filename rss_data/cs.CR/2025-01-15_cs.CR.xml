<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CR</link>
    <description>cs.CR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 Jan 2025 02:27:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Review on the Security Vulnerabilities of the IoMT against Malware Attacks and DDoS</title>
      <link>https://arxiv.org/abs/2501.07703</link>
      <description>arXiv:2501.07703v1 Announce Type: new 
Abstract: The Internet of Medical Things (IoMT) has transformed the healthcare industry by connecting medical devices in monitoring treatment outcomes of patients. This increased connectivity has resulted to significant security vulnerabilities in the case of malware and Distributed Denial of Service (DDoS) attacks. This literature review examines the vulnerabilities of IoMT devices, focusing on critical threats and exploring mitigation strategies. We conducted a comprehensive search across leading databases such as ACM Digital Library, IEEE Xplore, and Elsevier to analyze peer-reviewed studies published within the last five years (from 2019 to 2024). The review shows that inadequate encryption protocols, weak authentication methods, and irregular firmware updates are the main causes of risks associated with IoMT devices. We have identified emerging solutions like machine learning algorithms, blockchain technology, and edge computing as promising approaches to enhance IoMT security. This review emphasizes the pressing need to develop lightweight security measures and standardized protocols to protect patient data and ensure the integrity of healthcare services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07703v1</guid>
      <category>cs.CR</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lily Dzamesi, Nelly Elsayed</dc:creator>
    </item>
    <item>
      <title>A Comparative Analysis of DNN-based White-Box Explainable AI Methods in Network Security</title>
      <link>https://arxiv.org/abs/2501.07801</link>
      <description>arXiv:2501.07801v1 Announce Type: new 
Abstract: New research focuses on creating artificial intelligence (AI) solutions for network intrusion detection systems (NIDS), drawing its inspiration from the ever-growing number of intrusions on networked systems, increasing its complexity and intelligibility. Hence, the use of explainable AI (XAI) techniques in real-world intrusion detection systems comes from the requirement to comprehend and elucidate black-box AI models to security analysts. In an effort to meet such requirements, this paper focuses on applying and evaluating White-Box XAI techniques (particularly LRP, IG, and DeepLift) for NIDS via an end-to-end framework for neural network models, using three widely used network intrusion datasets (NSL-KDD, CICIDS-2017, and RoEduNet-SIMARGL2021), assessing its global and local scopes, and examining six distinct assessment measures (descriptive accuracy, sparsity, stability, robustness, efficiency, and completeness). We also compare the performance of white-box XAI methods with black-box XAI methods. The results show that using White-box XAI techniques scores high in robustness and completeness, which are crucial metrics for IDS. Moreover, the source codes for the programs developed for our XAI evaluation framework are available to be improved and used by the research community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07801v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Osvaldo Arreche, Mustafa Abdallah</dc:creator>
    </item>
    <item>
      <title>How Far are App Secrets from Being Stolen? A Case Study on Android</title>
      <link>https://arxiv.org/abs/2501.07805</link>
      <description>arXiv:2501.07805v1 Announce Type: new 
Abstract: Android apps can hold secret strings of themselves such as cloud service credentials or encryption keys. Leakage of such secret strings can induce unprecedented consequences like monetary losses or leakage of user private information. In practice, various security issues were reported because many apps failed to protect their secrets. However, little is known about the types, usages, exploitability, and consequences of app secret leakage issues. While a large body of literature has been devoted to studying user private information leakage, there is no systematic study characterizing app secret leakage issues. How far are Android app secrets from being stolen?
  To bridge this gap, we conducted the first systematic study to characterize app secret leakage issues in Android apps based on 575 potential app secrets sampled from 14,665 popular Android apps on Google Play. We summarized the common categories of leaked app secrets, assessed their security impacts and disclosed app bad practices in storing app secrets. We devised a text mining strategy using regular expressions and demonstrated that numerous app secrets can be easily stolen, even from the highly popular Android apps on Google. In a follow-up study, we harvested 3,711 distinct exploitable app secrets through automatic analysis. Our findings highlight the prevalence of this problem and call for greater attention to app secret protection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07805v1</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lili Wei, Heqing Huang, Shing-Chi Cheung, Kevin Li</dc:creator>
    </item>
    <item>
      <title>PUFBind: PUF-Enabled Lightweight Program Binary Authentication for FPGA-based Embedded Systems</title>
      <link>https://arxiv.org/abs/2501.07868</link>
      <description>arXiv:2501.07868v1 Announce Type: new 
Abstract: Field Programmable Gate Array (FPGA)-based embedded systems have become mainstream in the last decade, often in security-sensitive applications. However, even with an authenticated hardware platform, compromised software can severely jeopardize the overall system security, making hardware protection insufficient if the software itself is malicious. In this paper, we propose a novel low-overhead hardware-software co-design solution that utilizes Physical Unclonable Functions (PUFs) to ensure the authenticity of program binaries for microprocessors/microcontrollers mapped on the FPGA. Our technique binds a program binary to a specific target FPGA through a PUF signature, performs runtime authentication for the program binary, and allows execution of the binary only after successful authentication. The proposed scheme is platform-agnostic and capable of operating in a "bare metal'' mode (no system software requirement) for maximum flexibility. Our scheme also does not require any modification of the original hardware design or program binary. We demonstrate a successful prototype implementation using the open-source PicoBlaze microcontroller on AMD/Xilinx FPGA, comparing its hardware resource footprint and performance with other existing solutions of a similar nature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07868v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sneha Swaroopa, Venkata Sreekanth Balijabudda, Rajat Subhra Chakraborty, Indrajit Chakrabarti</dc:creator>
    </item>
    <item>
      <title>Differentially Private Distance Query with Asymmetric Noise</title>
      <link>https://arxiv.org/abs/2501.07955</link>
      <description>arXiv:2501.07955v1 Announce Type: new 
Abstract: With the growth of online social services, social information graphs are becoming increasingly complex. Privacy issues related to analyzing or publishing on social graphs are also becoming increasingly serious. Since the shortest paths play an important role in graphs, privately publishing the shortest paths or distances has attracted the attention of researchers. Differential privacy (DP) is an excellent standard for preserving privacy. However, existing works to answer the distance query with the guarantee of DP were almost based on the weight private graph assumption, not on the paths themselves. In this paper, we consider edges as privacy and propose distance publishing mechanisms based on edge DP. To address the issue of utility damage caused by large global sensitivities, we revisit studies related to asymmetric neighborhoods in DP with the observation that the distance query is monotonic in asymmetric neighborhoods. We formally give the definition of asymmetric neighborhoods and propose Individual Asymmetric Differential Privacy with higher privacy guarantees in combination with smooth sensitivity. Then, we introduce two methods to efficiently compute the smooth sensitivity of distance queries in asymmetric neighborhoods. Finally, we validate our scheme using both real-world and synthetic datasets, which can reduce the error to $0.0862$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07955v1</guid>
      <category>cs.CR</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weihong Sheng, Jiajun Chen, Chunqiang Hu, Bin Cai, Meng Han, Jiguo Yu</dc:creator>
    </item>
    <item>
      <title>Entropy Mixing Networks: Enhancing Pseudo-Random Number Generators with Lightweight Dynamic Entropy Injection</title>
      <link>https://arxiv.org/abs/2501.08031</link>
      <description>arXiv:2501.08031v1 Announce Type: new 
Abstract: Random number generation plays a vital role in cryptographic systems and computational applications, where uniformity, unpredictability, and robustness are essential. This paper presents the Entropy Mixing Network (EMN), a novel hybrid random number generator designed to enhance randomness quality by combining deterministic pseudo-random generation with periodic entropy injection. To evaluate its effectiveness, we propose a comprehensive assessment framework that integrates statistical tests, advanced metrics, and visual analyses, providing a holistic view of randomness quality, predictability, and computational efficiency. The results demonstrate that EMN outperforms Python's SystemRandom and MersenneTwister in critical metrics, achieving the highest Chi-squared p-value (0.9430), entropy (7.9840), and lowest predictability (-0.0286). These improvements come with a trade-off in computational performance, as EMN incurs a higher generation time (0.2602 seconds). Despite this, its superior randomness quality makes it particularly suitable for cryptographic applications where security is prioritized over speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08031v1</guid>
      <category>cs.CR</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamed Aly Bouke, Omar Imhemmed Alramli, Azizol Abdullah, Nur Izura Udzir, Normalia Samian, Mohamed Othman, Zurina Mohd Hanapi</dc:creator>
    </item>
    <item>
      <title>Learning-based Detection of GPS Spoofing Attack for Quadrotors</title>
      <link>https://arxiv.org/abs/2501.07597</link>
      <description>arXiv:2501.07597v1 Announce Type: cross 
Abstract: Safety-critical cyber-physical systems (CPS), such as quadrotor UAVs, are particularly prone to cyber attacks, which can result in significant consequences if not detected promptly and accurately. During outdoor operations, the nonlinear dynamics of UAV systems, combined with non-Gaussian noise, pose challenges to the effectiveness of conventional statistical and machine learning methods. To overcome these limitations, we present QUADFormer, an advanced attack detection framework for quadrotor UAVs leveraging a transformer-based architecture. This framework features a residue generator that produces sequences sensitive to anomalies, which are then analyzed by the transformer to capture statistical patterns for detection and classification. Furthermore, an alert mechanism ensures UAVs can operate safely even when under attack. Extensive simulations and experimental evaluations highlight that QUADFormer outperforms existing state-of-the-art techniques in detection accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07597v1</guid>
      <category>cs.RO</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengyu Wang, Zhaohua Yang, Jialu Li, Ling Shi</dc:creator>
    </item>
    <item>
      <title>Masking Countermeasures Against Side-Channel Attacks on Quantum Computers</title>
      <link>https://arxiv.org/abs/2501.07695</link>
      <description>arXiv:2501.07695v1 Announce Type: cross 
Abstract: We propose a modification to the transpiler of a quantum computer to safeguard against side-channel attacks aimed at learning information about a quantum circuit. We demonstrate that if it is feasible to shield a specific subset of gates from side-channel attacks, then it is possible to conceal all information in a quantum circuit by transpiling it into a new circuit whose depth grows linearly, depending on the quantum computer's architecture. We provide concrete examples of implementing this protection on IBM's quantum computers, utilizing their virtual gates and editing their transpiler.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07695v1</guid>
      <category>quant-ph</category>
      <category>cs.CR</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/QCE60285.2024.00210</arxiv:DOI>
      <arxiv:journal_reference>2024 IEEE International Conference on Quantum Computing and Engineering (QCE), Montreal, QC, Canada, 2024, pp. 1809-1816</arxiv:journal_reference>
      <dc:creator>Jason T. LeGrow, Travis Morrison, Jamie Sikora, Nicolas Swanson</dc:creator>
    </item>
    <item>
      <title>Towards A Hybrid Quantum Differential Privacy</title>
      <link>https://arxiv.org/abs/2501.07844</link>
      <description>arXiv:2501.07844v2 Announce Type: cross 
Abstract: Quantum computing offers unparalleled processing power but raises significant data privacy challenges. Quantum Differential Privacy (QDP) leverages inherent quantum noise to safeguard privacy, surpassing traditional DP. This paper develops comprehensive noise profiles, identifies noise types beneficial for QDP, and highlights teh need for practical implementations beyond theoretical models. Existing QDP mechanisms, limited to single noise sources, fail to reflect teh multi-source noise reality of quantum systems. We propose a resilient hybrid QDP mechanism utilizing channel and measurement noise, optimizing privacy budgets to balance privacy and utility. Additionally, we introduce Lifted Quantum Differential Privacy, offering enhanced randomness for improved privacy audits and quantum algorithm evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07844v2</guid>
      <category>quant-ph</category>
      <category>cs.CR</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Baobao Song, Shiva Raj Pokhrel, Athanasios V. Vasilakos, Tianqing Zhu, Gang Li</dc:creator>
    </item>
    <item>
      <title>Unveiling Provider Bias in Large Language Models for Code Generation</title>
      <link>https://arxiv.org/abs/2501.07849</link>
      <description>arXiv:2501.07849v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have emerged as the new recommendation engines, outperforming traditional methods in both capability and scope, particularly in code generation applications. Our research reveals a novel provider bias in LLMs, namely without explicit input prompts, these models show systematic preferences for services from specific providers in their recommendations (e.g., favoring Google Cloud over Microsoft Azure). This bias holds significant implications for market dynamics and societal equilibrium, potentially promoting digital monopolies. It may also deceive users and violate their expectations, leading to various consequences. This paper presents the first comprehensive empirical study of provider bias in LLM code generation. We develop a systematic methodology encompassing an automated pipeline for dataset generation, incorporating 6 distinct coding task categories and 30 real-world application scenarios. Our analysis encompasses over 600,000 LLM-generated responses across seven state-of-the-art models, utilizing approximately 500 million tokens (equivalent to \$5,000+ in computational costs). The study evaluates both the generated code snippets and their embedded service provider selections to quantify provider bias. Additionally, we conduct a comparative analysis of seven debiasing prompting techniques to assess their efficacy in mitigating these biases. Our findings demonstrate that LLMs exhibit significant provider preferences, predominantly favoring services from Google and Amazon, and can autonomously modify input code to incorporate their preferred providers without users' requests. Notably, we observe discrepancies between providers recommended in conversational contexts versus those implemented in generated code. The complete dataset and analysis results are available in our repository.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07849v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoyu Zhang, Juan Zhai, Shiqing Ma, Qingshuang Bao, Weipeng Jiang, Chao Shen, Yang Liu</dc:creator>
    </item>
    <item>
      <title>Gandalf the Red: Adaptive Security for LLMs</title>
      <link>https://arxiv.org/abs/2501.07927</link>
      <description>arXiv:2501.07927v1 Announce Type: cross 
Abstract: Current evaluations of defenses against prompt attacks in large language model (LLM) applications often overlook two critical factors: the dynamic nature of adversarial behavior and the usability penalties imposed on legitimate users by restrictive defenses. We propose D-SEC (Dynamic Security Utility Threat Model), which explicitly separates attackers from legitimate users, models multi-step interactions, and rigorously expresses the security-utility in an optimizable form. We further address the shortcomings in existing evaluations by introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed to generate realistic, adaptive attack datasets. Using Gandalf, we collect and release a dataset of 279k prompt attacks. Complemented by benign user data, our analysis reveals the interplay between security and utility, showing that defenses integrated in the LLM (e.g., system prompts) can degrade usability even without blocking requests. We demonstrate that restricted application domains, defense-in-depth, and adaptive defenses are effective strategies for building secure and useful LLM applications. Code is available at \href{https://github.com/lakeraai/dsec-gandalf}{\texttt{https://github.com/lakeraai/dsec-gandalf}}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07927v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niklas Pfister, V\'aclav Volhejn, Manuel Knott, Santiago Arias, Julia Bazi\'nska, Mykhailo Bichurin, Alan Commike, Janet Darling, Peter Dienes, Matthew Fiedler, David Haber, Matthias Kraft, Marco Lancini, Max Mathys, Dami\'an Pascual-Ortiz, Jakub Podolak, Adri\`a Romero-L\'opez, Kyriacos Shiarlis, Andreas Signer, Zsolt Terek, Athanasios Theocharis, Daniel Timbrell, Samuel Trautwein, Samuel Watts, Natalie Wu, Mateo Rojas-Carulla</dc:creator>
    </item>
    <item>
      <title>Audio-visual Deepfake Detection With Local Temporal Inconsistencies</title>
      <link>https://arxiv.org/abs/2501.08137</link>
      <description>arXiv:2501.08137v1 Announce Type: cross 
Abstract: This paper proposes an audio-visual deepfake detection approach that aims to capture fine-grained temporal inconsistencies between audio and visual modalities. To achieve this, both architectural and data synthesis strategies are introduced. From an architectural perspective, a temporal distance map, coupled with an attention mechanism, is designed to capture these inconsistencies while minimizing the impact of irrelevant temporal subsequences. Moreover, we explore novel pseudo-fake generation techniques to synthesize local inconsistencies. Our approach is evaluated against state-of-the-art methods using the DFDC and FakeAVCeleb datasets, demonstrating its effectiveness in detecting audio-visual deepfakes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08137v1</guid>
      <category>cs.CV</category>
      <category>cs.CR</category>
      <category>cs.MM</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcella Astrid, Enjie Ghorbel, Djamila Aouada</dc:creator>
    </item>
    <item>
      <title>Verifying Device Drivers with Pancake</title>
      <link>https://arxiv.org/abs/2501.08249</link>
      <description>arXiv:2501.08249v1 Announce Type: cross 
Abstract: Device driver bugs are the leading cause of OS compromises, and their formal verification is therefore highly desirable. To the best of our knowledge, no realistic and performant driver has been verified for a non-trivial device. We propose Pancake, an imperative language for systems programming that features a well-defined and verification-friendly semantics. Leveraging the verified compiler backend of the CakeML functional language, we develop a compiler for Pancake that guarantees that the binary retains the semantics of the source code. Usng automatic translation of Pancake to the Viper SMT front-end, we verify a performant driver for an Ethernet NIC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08249v1</guid>
      <category>cs.PL</category>
      <category>cs.CR</category>
      <category>cs.OS</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Junming Zhao, Alessandro Legnani, Tiana Tsang Ung, H. Truong, Tsun Wang Sau, Miki Tanaka, Johannes {\AA}man Pohjola, Thomas Sewell, Rob Sison, Hira Syeda, Magnus Myreen, Michael Norrish, Gernot Heiser</dc:creator>
    </item>
    <item>
      <title>Towards an End-to-End (E2E) Adversarial Learning and Application in the Physical World</title>
      <link>https://arxiv.org/abs/2501.08258</link>
      <description>arXiv:2501.08258v1 Announce Type: cross 
Abstract: The traditional learning process of patch-based adversarial attacks, conducted in the digital domain and then applied in the physical domain (e.g., via printed stickers), may suffer from reduced performance due to adversarial patches' limited transferability from the digital domain to the physical domain. Given that previous studies have considered using projectors to apply adversarial attacks, we raise the following question: can adversarial learning (i.e., patch generation) be performed entirely in the physical domain with a projector? In this work, we propose the Physical-domain Adversarial Patch Learning Augmentation (PAPLA) framework, a novel end-to-end (E2E) framework that converts adversarial learning from the digital domain to the physical domain using a projector. We evaluate PAPLA across multiple scenarios, including controlled laboratory settings and realistic outdoor environments, demonstrating its ability to ensure attack success compared to conventional digital learning-physical application (DL-PA) methods. We also analyze the impact of environmental factors, such as projection surface color, projector strength, ambient light, distance, and angle of the target object relative to the camera, on the effectiveness of projected patches. Finally, we demonstrate the feasibility of the attack against a parked car and a stop sign in a real-world outdoor environment. Our results show that under specific conditions, E2E adversarial learning in the physical domain eliminates the transferability issue and ensures evasion by object detectors. Finally, we provide insights into the challenges and opportunities of applying adversarial learning in the physical domain and explain where such an approach is more effective than using a sticker.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08258v1</guid>
      <category>cs.CV</category>
      <category>cs.CR</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dudi Biton, Jacob Shams, Koda Satoru, Asaf Shabtai, Yuval Elovici, Ben Nassi</dc:creator>
    </item>
    <item>
      <title>SoK: Design, Vulnerabilities, and Security Measures of Cryptocurrency Wallets</title>
      <link>https://arxiv.org/abs/2307.12874</link>
      <description>arXiv:2307.12874v4 Announce Type: replace 
Abstract: With the advent of decentralised digital currencies powered by blockchain technology, a new era of peer-to-peer transactions has commenced. The rapid growth of the cryptocurrency economy has led to increased use of transaction-enabling wallets, making them a focal point for security risks. As the frequency of wallet-related incidents rises, there is a critical need for a systematic approach to measure and evaluate these attacks, drawing lessons from past incidents to enhance wallet security. In response, we introduce a multi-dimensional design taxonomy for existing and novel wallets with various design decisions. We classify existing industry wallets based on this taxonomy, identify previously occurring vulnerabilities and discuss the security implications of design decisions. We also systematise threats to the wallet mechanism and analyse the adversary's goals, capabilities and required knowledge. We present a multi-layered attack framework and investigate 84 incidents between 2012 and 2024, accounting for $5.4B. Following this, we classify defence implementations for these attacks on the precautionary and remedial axes. We map the mechanism and design decisions to vulnerabilities, attacks, and possible defence methods to discuss various insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.12874v4</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yimika Erinle, Yathin Kethepalli, Yebo Feng, Jiahua Xu</dc:creator>
    </item>
    <item>
      <title>A Hardware-Friendly Shuffling Countermeasure Against Side-Channel Attacks for Kyber</title>
      <link>https://arxiv.org/abs/2407.02452</link>
      <description>arXiv:2407.02452v2 Announce Type: replace 
Abstract: CRYSTALS-Kyber has been standardized as the only key-encapsulation mechanism (KEM) scheme by NIST to withstand attacks by large-scale quantum computers. However, the side-channel attacks (SCAs) on its implementation are still needed to be well considered for the upcoming migration. In this brief, we propose a secure and efficient hardware implementation for Kyber by incorporating a novel compact shuffling architecture. First of all, we modify the Fisher-Yates shuffle to make it more hardware-friendly. We then design an optimized shuffling architecture for the well-known open-source Kyber hardware implementation to enhance the security of all known and potential side-channel leakage points. Finally, we implement the modified Kyber design on FPGA and evaluate its security and performance. The security is verified by conducting correlation power analysis (CPA) and test vector leakage assessment (TVLA) on the hardware. Meanwhile, FPGA place-and-route results show that the proposed design reports only 8.7\% degradation on the hardware efficiency compared with the original unprotected version, much better than existing hardware hiding schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02452v2</guid>
      <category>cs.CR</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TCSII.2025.3528751</arxiv:DOI>
      <dc:creator>Dejun Xu, Kai Wang, Jing Tian</dc:creator>
    </item>
    <item>
      <title>An in-depth study of the power function $x^{q+2}$ over the finite field $\mathbb{F}_{q^2}$: the differential, boomerang, and Walsh spectra, with an application to coding theory</title>
      <link>https://arxiv.org/abs/2407.07710</link>
      <description>arXiv:2407.07710v3 Announce Type: replace 
Abstract: Let $q = p^m$, where $p$ is an odd prime number and $m$ is a positive integer. In this paper, we examine the finite field $\mathbb{F}_{q^2}$, which consists of $q^2$ elements. We first present an alternative method to determine the differential spectrum of the power function $f(x) = x^{q+2}$ on $\mathbb{F}_{q^2}$, incorporating several key simplifications. This methodology provides a new proof of the results established by Man, Xia, Li, and Helleseth in Finite Fields and Their Applications 84 (2022), 102100, which not only completely determine the differential spectrum of $f$ but also facilitate the analysis of its boomerang uniformity.
  Specifically, we determine the boomerang uniformity of $f$ for the cases where $q \equiv 1$ or $3$ (mod $6$), with the exception of the scenario where $p = 5$ and $m$ is even. Furthermore, for $p = 3$, we investigate the value distribution of the Walsh spectrum of $f$, demonstrating that it takes on only four distinct values. Using this result, we derive the weight distribution of a ternary cyclic code with four Hamming weights. The article integrates refined mathematical techniques from algebraic number theory and the theory of finite fields, employing several ingredients, such as exponential sums, to explore the cryptographic analysis of functions over finite fields. They can be used to explore the differential/boomerang uniformity across a wider range of functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07710v3</guid>
      <category>cs.CR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.NT</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sihem Mesnager, Huawei Wu</dc:creator>
    </item>
    <item>
      <title>Transformers and Large Language Models for Efficient Intrusion Detection Systems: A Comprehensive Survey</title>
      <link>https://arxiv.org/abs/2408.07583</link>
      <description>arXiv:2408.07583v2 Announce Type: replace 
Abstract: With significant advancements in Transformers LLMs, NLP has extended its reach into many research fields due to its enhanced capabilities in text generation and user interaction. One field benefiting greatly from these advancements is cybersecurity. In cybersecurity, many parameters that need to be protected and exchanged between senders and receivers are in the form of text and tabular data, making NLP a valuable tool in enhancing the security measures of communication protocols. This survey paper provides a comprehensive analysis of the utilization of Transformers and LLMs in cyber-threat detection systems. The methodology of paper selection and bibliometric analysis is outlined to establish a rigorous framework for evaluating existing research. The fundamentals of Transformers are discussed, including background information on various cyber-attacks and datasets commonly used in this field. The survey explores the application of Transformers in IDSs, focusing on different architectures such as Attention-based models, LLMs like BERT and GPT, CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others. Furthermore, it explores the diverse environments and applications where Transformers and LLMs-based IDS have been implemented, including computer networks, IoT devices, critical infrastructure protection, cloud computing, SDN, as well as in autonomous vehicles. The paper also addresses research challenges and future directions in this area, identifying key issues such as interpretability, scalability, and adaptability to evolving threats, and more. Finally, the conclusion summarizes the findings and highlights the significance of Transformers and LLMs in enhancing cyber-threat detection capabilities, while also outlining potential avenues for further research and development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07583v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>eess.AS</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hamza Kheddar</dc:creator>
    </item>
    <item>
      <title>Private Electronic Payments with Self-Custody and Zero-Knowledge Verified Reissuance</title>
      <link>https://arxiv.org/abs/2409.01958</link>
      <description>arXiv:2409.01958v3 Announce Type: replace 
Abstract: This article builds upon the protocol for digital transfers described by Goodell, Toliver, and Nakib, which combines privacy by design for consumers with strong compliance enforcement for recipients of payments and self-validating assets that carry their own verifiable provenance information. We extend the protocol to allow for the verification that reissued assets were created in accordance with rules prohibiting the creation of new assets by anyone but the issuer, without exposing information about the circumstances in which the assets were created that could be used to identify the payer. The modified protocol combines an audit log with zero-knowledge proofs, so that a consumer spending an asset can demonstrate that there exists a valid entry on the audit log that is associated with the asset, without specifying which entry it is. This property is important as a means to allow money to be reissued within the system without the involvement of system operators within the zone of control of the original issuer. Additionally, we identify a key property of privacy-respecting electronic payments, wherein the payer is not required to retain secrets arising from one transaction until the following transaction, and argue that this property is essential to framing security requirements for storage of digital assets and the risk of blackmail or coercion as a way to exfiltrate information about payment history. We claim that the design of our protocol strongly protects the anonymity of payers with respect to their payment transactions, while preventing the creation of assets by any party other than the original issuer without destroying assets of equal value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01958v3</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniele Friolo, Geoffrey Goodell, Dann Toliver, Hazem Danny Nakib</dc:creator>
    </item>
    <item>
      <title>Kite: How to Delegate Voting Power Privately</title>
      <link>https://arxiv.org/abs/2501.05626</link>
      <description>arXiv:2501.05626v2 Announce Type: replace 
Abstract: Ensuring the privacy of votes in an election is crucial for the integrity of a democratic process. Often, voting power is delegated to representatives (e.g., in congress) who subsequently vote on behalf of voters on specific issues. This delegation model is also widely used in Decentralized Autonomous Organizations (DAOs). Although several existing voting systems used in DAOs support private voting, they only offer public delegation. In this paper, we introduce Kite, a new protocol that enables $\textit{private}$ delegation of voting power for DAO members. Voters can freely delegate, revoke, and re-delegate their power without revealing any information about who they delegated to. Even the delegate does not learn who delegated to them. The only information that is recorded publicly is that the voter delegated or re-delegated their vote to someone. Kite accommodates both public and private voting for the delegates themselves. We analyze the security of our protocol within the Universal Composability (UC) framework. We implement Kite as an extension to the existing Governor Bravo smart contract on the Ethereum blockchain, that is widely used for DAO governance. Furthermore, we provide an evaluation of our implementation that demonstrates the practicality of the protocol. The most expensive operation is delegation due to the required zero-knowledge proofs. On a consumer-grade laptop, delegation takes between 7 and 167 seconds depending on the requested level of privacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05626v2</guid>
      <category>cs.CR</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kamilla Nazirkhanova, Vrushank Gunjur, X. Pilli Cruz-De Jesus, Dan Boneh</dc:creator>
    </item>
    <item>
      <title>Union: A Trust-minimized Bridge for Rootstock</title>
      <link>https://arxiv.org/abs/2501.07435</link>
      <description>arXiv:2501.07435v2 Announce Type: replace 
Abstract: We present Union, a trust-minimized bridge protocol that enables secure transfer of BTC between Bitcoin and a secondary blockchain. The growing ecosystem of blockchain systems built around Bitcoin has created a pressing need for secure and efficient bridges to transfer BTC between networks while preserving Bitcoin's security guarantees. Union employs a multi-party variant of BitVMX, an optimistic proving system on Bitcoin, to create a bridge that operates securely under the assumption that at least one participant remains honest. This 1-of-n honest approach is strikingly different from the conventional honest-majority assumption adopted by practically all federated systems. The protocol introduces several innovations: a packet-based architecture that allows security bonds to be reused for multiple bridge operations, improving capital efficiency; a system of enablers to manage functionaries participation and to enforce penalties; a flexible light client framework adaptable to various blockchain architectures; and an efficient stop watch mechanism to optimize time-lock management. Union is a practical and scalable solution for Bitcoin interoperability that maintains strong security guarantees and minimizes trust assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07435v2</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ramon Amela (RootstockLabs), Shreemoy Mishra (RootstockLabs), Sergio Demian Lerner (RootstockLabs, Fairgate Labs), Javier \'Alvarez Cid-Fuentes (RootstockLabs)</dc:creator>
    </item>
    <item>
      <title>Set-Based Training for Neural Network Verification</title>
      <link>https://arxiv.org/abs/2401.14961</link>
      <description>arXiv:2401.14961v3 Announce Type: replace-cross 
Abstract: Neural networks are vulnerable to adversarial attacks, i.e., small input perturbations can significantly affect the outputs of a neural network. Therefore, to ensure safety of safety-critical environments, the robustness of a neural network must be formally verified against input perturbations, e.g., from noisy sensors. To improve the robustness of neural networks and thus simplify the formal verification, we present a novel set-based training procedure in which we compute the set of possible outputs given the set of possible inputs and compute for the first time a gradient set, i.e., each possible output has a different gradient. Therefore, we can directly reduce the size of the output enclosure by choosing gradients toward its center. Small output enclosures increase the robustness of a neural network and, at the same time, simplify its formal verification. The latter benefit is due to the fact that a larger size of propagated sets increases the conservatism of most verification methods. Our extensive evaluation demonstrates that set-based training produces robust neural networks with competitive performance, which can be verified using fast (polynomial-time) verification algorithms due to the reduced output set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14961v3</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.LO</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Koller, Tobias Ladner, Matthias Althoff</dc:creator>
    </item>
    <item>
      <title>Poisoning Attacks on Federated Learning-based Wireless Traffic Prediction</title>
      <link>https://arxiv.org/abs/2404.14389</link>
      <description>arXiv:2404.14389v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) offers a distributed framework to train a global control model across multiple base stations without compromising the privacy of their local network data. This makes it ideal for applications like wireless traffic prediction (WTP), which plays a crucial role in optimizing network resources, enabling proactive traffic flow management, and enhancing the reliability of downstream communication-aided applications, such as IoT devices, autonomous vehicles, and industrial automation systems. Despite its promise, the security aspects of FL-based distributed wireless systems, particularly in regression-based WTP problems, remain inadequately investigated. In this paper, we introduce a novel fake traffic injection (FTI) attack, designed to undermine the FL-based WTP system by injecting fabricated traffic distributions with minimal knowledge. We further propose a defense mechanism, termed global-local inconsistency detection (GLID), which strategically removes abnormal model parameters that deviate beyond a specific percentile range estimated through statistical methods in each dimension. Extensive experimental evaluations, performed on real-world wireless traffic datasets, demonstrate that both our attack and defense strategies significantly outperform existing baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14389v2</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zifan Zhang, Minghong Fang, Jiayuan Huang, Yuchen Liu</dc:creator>
    </item>
    <item>
      <title>Securing Distributed Network Digital Twin Systems Against Model Poisoning Attacks</title>
      <link>https://arxiv.org/abs/2407.01917</link>
      <description>arXiv:2407.01917v2 Announce Type: replace-cross 
Abstract: In the era of 5G and beyond, the increasing complexity of wireless networks necessitates innovative frameworks for efficient management and deployment. Digital twins (DTs), embodying real-time monitoring, predictive configurations, and enhanced decision-making capabilities, stand out as a promising solution in this context. Within a time-series data-driven framework that effectively maps wireless networks into digital counterparts, encapsulated by integrated vertical and horizontal twinning phases, this study investigates the security challenges in distributed network DT systems, which potentially undermine the reliability of subsequent network applications such as wireless traffic forecasting. Specifically, we consider a minimal-knowledge scenario for all attackers, in that they do not have access to network data and other specialized knowledge, yet can interact with previous iterations of server-level models. In this context, we spotlight a novel fake traffic injection attack designed to compromise a distributed network DT system for wireless traffic prediction. In response, we then propose a defense mechanism, termed global-local inconsistency detection (GLID), to counteract various model poisoning threats. GLID strategically removes abnormal model parameters that deviate beyond a particular percentile range, thereby fortifying the security of network twinning process. Through extensive experiments on real-world wireless traffic datasets, our experimental evaluations show that both our attack and defense strategies significantly outperform existing baselines, highlighting the importance of security measures in the design and implementation of DTs for 5G and beyond network systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01917v2</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zifan Zhang, Minghong Fang, Mingzhe Chen, Gaolei Li, Xi Lin, Yuchen Liu</dc:creator>
    </item>
    <item>
      <title>Differential privacy and Sublinear time are incompatible sometimes</title>
      <link>https://arxiv.org/abs/2407.07262</link>
      <description>arXiv:2407.07262v2 Announce Type: replace-cross 
Abstract: Differential privacy and sublinear algorithms are both rapidly emerging algorithmic themes in times of big data analysis. Although recent works have shown the existence of differentially private sublinear algorithms for many problems including graph parameter estimation and clustering, little is known regarding hardness results on these algorithms. In this paper, we initiate the study of lower bounds for problems that aim for both differentially-private and sublinear-time algorithms. Our main result is the incompatibility of both the desiderata in the general case. In particular, we prove that a simple problem based on one-way marginals yields both a differentially-private algorithm, as well as a sublinear-time algorithm, but does not admit a ``strictly'' sublinear-time algorithm that is also differentially private.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07262v2</guid>
      <category>cs.DS</category>
      <category>cs.CR</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremiah Blocki, Hendrik Fichtenberger, Elena Grigorescu, Tamalika Mukherjee</dc:creator>
    </item>
    <item>
      <title>Private Collaborative Edge Inference via Over-the-Air Computation</title>
      <link>https://arxiv.org/abs/2407.21151</link>
      <description>arXiv:2407.21151v2 Announce Type: replace-cross 
Abstract: We consider collaborative inference at the wireless edge, where each client's model is trained independently on its local dataset. Clients are queried in parallel to make an accurate decision collaboratively. In addition to maximizing the inference accuracy, we also want to ensure the privacy of local models. To this end, we leverage the superposition property of the multiple access channel to implement bandwidth-efficient multi-user inference methods. We propose different methods for ensemble and multi-view classification that exploit over-the-air computation (OAC). We show that these schemes perform better than their orthogonal counterparts with statistically significant differences while using fewer resources and providing privacy guarantees. We also provide experimental results verifying the benefits of the proposed OAC approach to multi-user inference, and perform an ablation study to demonstrate the effectiveness of our design choices. We share the source code of the framework publicly on Github to facilitate further research and reproducibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21151v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TMLCN.2025.3526551</arxiv:DOI>
      <dc:creator>Selim F. Yilmaz, Burak Hasircioglu, Li Qiao, Deniz Gunduz</dc:creator>
    </item>
  </channel>
</rss>
