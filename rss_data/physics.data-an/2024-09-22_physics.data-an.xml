<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Sep 2024 04:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Signal model parameter scan using Normalizing Flow</title>
      <link>https://arxiv.org/abs/2409.13201</link>
      <description>arXiv:2409.13201v1 Announce Type: new 
Abstract: This paper presents a parameter scan technique for BSM signal models based on normalizing flow. Normalizing flow is a type of deep learning model that transforms a simple probability distribution into a complex probability distribution as an invertible function. By learning an invertible transformation between a complex multidimensional distribution, such as experimental data observed in collider experiments, and a multidimensional normal distribution, the normalizing flow model gains the ability to sample (or generate) pseudo experimental data from random numbers and to evaluate a log-likelihood value from multidimensional observed events. The normalizing flow model can also be extended to take multidimensional conditional variables as arguments. Thus, the normalizing flow model can be used as a generator and evaluator of pseudo experimental data conditioned by the BSM model parameters. The log-likelihood value, the output of the normalizing flow model, is a function of the conditional variables. Therefore, the model can quickly calculate gradients of the log-likelihood to the conditional variables. Following this property, it is expected that the most likely set of conditional variables that reproduce the experimental data, i.e. the optimal set of parameters for the BSM model, can be efficiently searched. This paper demonstrates this on a simple dataset and discusses its limitations and future extensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13201v1</guid>
      <category>physics.data-an</category>
      <category>hep-ph</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masahiko Saito, Masahiro Morinaga, Tomoe Kishimoto, Junichi Tanaka</dc:creator>
    </item>
    <item>
      <title>Model selection for extremal dependence structures using deep learning: Application to environmental data</title>
      <link>https://arxiv.org/abs/2409.13276</link>
      <description>arXiv:2409.13276v1 Announce Type: new 
Abstract: This paper introduces a new methodology for extreme spatial dependence structure selection. It is based on deep learning techniques, specifically Convolutional Neural Networks -CNNs. Two schemes are considered: in the first scheme, the matching probability is evaluated through a single CNN while in the second scheme, a hierarchical procedure is proposed: a first CNN is used to select a max-stable model, then another network allows to select the most adapted covariance function, according to the selected max-stable model. This model selection approach demonstrates performs very well on simulations. In contrast, the Composite Likelihood Information Criterion CLIC faces issues in selecting the correct model. Both schemes are applied to a dataset of 2m air temperature over Iraq land, CNNs are trained on dependence structures summarized by the Concurrence probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13276v1</guid>
      <category>physics.data-an</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manaf Ahmed (ICJ), V\'eronique Maume-Deschamps (ICJ,PSPM), Pierre Ribereau (PSPM,ICJ)</dc:creator>
    </item>
    <item>
      <title>Partial information decomposition for mixed discrete and continuous random variables</title>
      <link>https://arxiv.org/abs/2409.13506</link>
      <description>arXiv:2409.13506v1 Announce Type: new 
Abstract: The framework of Partial Information Decomposition (PID) unveils complex nonlinear interactions in network systems by dissecting the mutual information (MI) between a target variable and several source variables. While PID measures have been formulated mostly for discrete variables, with only recent extensions to continuous systems, the case of mixed variables where the target is discrete and the sources are continuous is not yet covered properly. Here, we introduce a PID scheme whereby the MI between a specific state of the discrete target and (subsets of) the continuous sources is expressed as a Kullback-Leibler divergence and is estimated through a data-efficient nearest-neighbor strategy. The effectiveness of this PID is demonstrated in simulated systems of mixed variables and showcased in a physiological application. Our approach is relevant to many scientific problems, including sensory coding in neuroscience and feature selection in machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13506v1</guid>
      <category>physics.data-an</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Bar\`a, Yuri Antonacci, Marta Iovino, Ivan Lazic, Luca Faes</dc:creator>
    </item>
    <item>
      <title>Efficient Training of Deep Neural Operator Networks via Randomized Sampling</title>
      <link>https://arxiv.org/abs/2409.13280</link>
      <description>arXiv:2409.13280v1 Announce Type: cross 
Abstract: Neural operators (NOs) employ deep neural networks to learn mappings between infinite-dimensional function spaces. Deep operator network (DeepONet), a popular NO architecture, has demonstrated success in the real-time prediction of complex dynamics across various scientific and engineering applications. In this work, we introduce a random sampling technique to be adopted during the training of DeepONet, aimed at improving the generalization ability of the model, while significantly reducing the computational time. The proposed approach targets the trunk network of the DeepONet model that outputs the basis functions corresponding to the spatiotemporal locations of the bounded domain on which the physical system is defined. Traditionally, while constructing the loss function, DeepONet training considers a uniform grid of spatiotemporal points at which all the output functions are evaluated for each iteration. This approach leads to a larger batch size, resulting in poor generalization and increased memory demands, due to the limitations of the stochastic gradient descent (SGD) optimizer. The proposed random sampling over the inputs of the trunk net mitigates these challenges, improving generalization and reducing memory requirements during training, resulting in significant computational gains. We validate our hypothesis through three benchmark examples, demonstrating substantial reductions in training time while achieving comparable or lower overall test errors relative to the traditional training approach. Our results indicate that incorporating randomization in the trunk network inputs during training enhances the efficiency and robustness of DeepONet, offering a promising avenue for improving the framework's performance in modeling complex physical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13280v1</guid>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sharmila Karumuri, Lori Graham-Brady, Somdatta Goswami</dc:creator>
    </item>
    <item>
      <title>An Accessible Instrument for Measuring Soft Material Mechanical Properties</title>
      <link>https://arxiv.org/abs/2404.15036</link>
      <description>arXiv:2404.15036v2 Announce Type: replace-cross 
Abstract: Soft material research has seen significant growth in recent years, with emerging applications in robotics, electronics, and healthcare diagnostics where understanding material mechanical response is crucial for precision design. Traditional methods for measuring nonlinear mechanical properties of soft materials require specially sized samples that are extracted from their natural environment to be mounted on the testing instrument. This has been shown to compromise data accuracy and precision in various soft and biological materials. To overcome this, the Volume Controlled Cavity Expansion (VCCE) method was developed. This technique tests soft materials by controlling the formation rate of a liquid cavity inside the materials at the tip of an injection needle, and simultaneously measuring the resisting pressure which describes the material response. Despite VCCE's early successes, expansion of its application beyond academia has been hindered by cost, size, and expertise. In response to this, the first portable, bench-top instrument utilizing VCCE is presented here. This device, built with affordable, readily available components and open-source software, streamlines VCCE experimentation without sacrificing performance or precision. It is especially suitable for space-limited settings and designed for use by non-experts, promoting widespread adoption. The instrument's efficacy was demonstrated through testing Polydimethylsiloxane (PDMS) samples of varying stiffness. This study not only validates instrument performance, but also sets the stage for further advancements and broader applications in soft material testing. All data, along with acquisition, control, and post-processing scripts, are made available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15036v2</guid>
      <category>physics.ins-det</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>B. M. Unikewicz, A. M. Pincot, T. Cohen</dc:creator>
    </item>
  </channel>
</rss>
