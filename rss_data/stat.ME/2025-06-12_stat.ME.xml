<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Jun 2025 14:32:58 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Longitudinal weighted and trimmed treatment effects with flip interventions</title>
      <link>https://arxiv.org/abs/2506.09188</link>
      <description>arXiv:2506.09188v1 Announce Type: new 
Abstract: Weighting and trimming are popular methods for addressing positivity violations in causal inference. While well-studied with single-timepoint data, standard methods do not easily generalize to address non-baseline positivity violations in longitudinal data, and remain vulnerable to such violations. In this paper, we extend weighting and trimming to longitudinal data via stochastic ``flip'' interventions, which maintain the treatment status of subjects who would have received the target treatment, and flip others' treatment to the target with probability equal to their weight (e.g., overlap weight, trimming indicator). We first show, in single-timepoint data, that flip interventions yield a large class of weighted average treatment effects, ascribing a novel policy interpretation to these popular weighted estimands. With longitudinal data, we then show that flip interventions provide interpretable weighting or trimming on non-baseline covariates and, crucially, yield effects that are identifiable under arbitrary positivity violations. Moreover, we demonstrate that flip interventions are policy-relevant since they could be implemented in practice. By contrast, we show that alternative approaches for weighting on non-baseline covariates fail to achieve this property. We derive flexible and efficient estimators based on efficient influence functions when the weight is a smooth function of the propensity score. Namely, we construct multiply robust-style and sequentially doubly robust-style estimators that achieve root-n consistency and asymptotic normality under nonparametric conditions. Finally, we demonstrate our methods through an analysis of the effect of union membership on earnings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09188v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alec McClean, Alexander W. Levis, Nicholas Williams, Ivan Diaz</dc:creator>
    </item>
    <item>
      <title>A Spectral Confounder Adjustment for Spatial Regression with Multiple Exposures and Outcomes</title>
      <link>https://arxiv.org/abs/2506.09325</link>
      <description>arXiv:2506.09325v1 Announce Type: new 
Abstract: Unmeasured spatial confounding complicates exposure effect estimation in environmental health studies. This problem is exacerbated in studies with multiple health outcomes and environmental exposure variables, as the source and magnitude of confounding bias may differ across exposure/outcome pairs. We propose to mitigate the effects of spatial confounding in multivariate studies by projecting to the spectral domain to separate relationships by the spatial scale and assuming that the confounding bias dissipates at more local scales. Under this assumption and some reasonable conditions, the random effect is uncorrelated with the exposures in local scales, ensuring causal interpretation of the regression coefficients. Our model for the exposure effects is a three-way tensor over exposure, outcome, and spatial scale. We use a canonical polyadic decomposition and shrinkage priors to encourage sparsity and borrow strength across the dimensions of the tensor. We demonstrate the performance of our method in an extensive simulation study and data analysis to understand the relationship between disaster resilience and the incidence of chronic diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09325v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shih-Ni Prim, Yawen Guan, Shu Yang, Ana G Rappold, K. Lloyd Hill, Wei-Lun Tsai, Corinna Keeler, Brian J Reich</dc:creator>
    </item>
    <item>
      <title>Functional Tensor Regression</title>
      <link>https://arxiv.org/abs/2506.09358</link>
      <description>arXiv:2506.09358v1 Announce Type: new 
Abstract: Tensor regression has attracted significant attention in statistical research. This study tackles the challenge of handling covariates with smooth varying structures. We introduce a novel framework, termed functional tensor regression, which incorporates both the tensor and functional aspects of the covariate. To address the high dimensionality and functional continuity of the regression coefficient, we employ a low Tucker rank decomposition along with smooth regularization for the functional mode. We develop a functional Riemannian Gauss--Newton algorithm that demonstrates a provable quadratic convergence rate, while the estimation error bound is based on the tensor covariate dimension. Simulations and a neuroimaging analysis illustrate the finite sample performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09358v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tongyu Li, Fang Yao, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Sufficient digits and density estimation: A Bayesian nonparametric approach using generalized finite P\'olya trees</title>
      <link>https://arxiv.org/abs/2506.09437</link>
      <description>arXiv:2506.09437v1 Announce Type: new 
Abstract: This paper proposes a novel approach for statistical modelling of a continuous random variable $X$ on $[0, 1)$, based on its digit representation $X=.X_1X_2\ldots$. In general, $X$ can be coupled with a random variable $N$ so that if a prior of $N$ is imposed, $(X_1,\ldots,X_N)$ becomes a sufficient statistics and $.X_{N+1}X_{N+2}\ldots$ is uniformly distributed. In line with this fact, and focusing on binary digits for simplicity, we propose a family of generalized finite P{\'o}lya trees that induces a random density for a sample, which becomes a flexible tool for density estimation. Here, the digit system may be random and learned from the data. We provide a detailed Bayesian analysis, including closed form expression for the posterior distribution which sidesteps the need of MCMC methods for posterior inference. We analyse the frequentist properties as the sample size increases, and provide sufficient conditions for consistency of the posterior distributions of the random density and $N$. We consider an extension to data spanning multiple orders of magnitude, and propose a prior distribution that encodes the so-called extended Newcomb-Benford law. Such a model shows promising results for density estimation of human-activity data. Our methodology is illustrated on several synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09437v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Beraha, Jesper M{\o}ller</dc:creator>
    </item>
    <item>
      <title>A new hierarchical distribution on arbitrary sparse precision matrices</title>
      <link>https://arxiv.org/abs/2506.09607</link>
      <description>arXiv:2506.09607v1 Announce Type: new 
Abstract: We introduce a general strategy for defining distributions over the space of sparse symmetric positive definite matrices. Our method utilizes the Cholesky factorization of the precision matrix, imposing sparsity through constraints on its elements while preserving their independence and avoiding the numerical evaluation of normalization constants. In particular, we develop the S-Bartlett as a modified Bartlett decomposition, recovering the standard Wishart as a particular case. By incorporating a Spike-and-Slab prior to model graph sparsity, our approach facilitates Bayesian estimation through a tailored MCMC routine based on a Dual Averaging Hamiltonian Monte Carlo update. This framework extends naturally to the Generalized Linear Model setting, enabling applications to non-Gaussian outcomes via latent Gaussian variables. We test and compare the proposed S-Bartelett prior with the G-Wishart both on simulated and real data. Results highlight that the S-Bartlett prior offers a flexible alternative for estimating sparse precision matrices, with potential applications across diverse fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09607v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gianluca Mastrantonio, Pierfrancesco Alaimo Di Loro, Marco Mingione</dc:creator>
    </item>
    <item>
      <title>Causal effects on non-terminal event time with application to antibiotic usage and future resistance</title>
      <link>https://arxiv.org/abs/2506.09624</link>
      <description>arXiv:2506.09624v1 Announce Type: new 
Abstract: Comparing future antibiotic resistance levels resulting from different antibiotic treatments is challenging because some patients may survive only under one of the antibiotic treatments. We embed this problem within a semi-competing risks approach to study the causal effect on resistant infection, treated as a non-terminal event time. We argue that existing principal stratification estimands for such problems exclude patients for whom a causal effect is well-defined and is of clinical interest. Therefore, we present a new principal stratum, the infected-or-survivors (ios). The ios is the subpopulation of patients who would have survived or been infected under both antibiotic treatments. This subpopulation is more inclusive than previously defined subpopulations. We target the causal effect among these patients, which we term the feasible-infection causal effect (FICE). We develop large-sample bounds under novel assumptions, and discuss the plausibility of these assumptions in our application. As an alternative, we derive FICE identification using two illness-death models with a bivariate frailty random variable. These two models are connected by a cross-world correlation parameter. Estimation is performed by an expectation-maximization algorithm followed by a Monte Carlo procedure. We apply our methods to detailed clinical data obtained from a hospital setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09624v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tamir Zehavi, Uri Obolski, Michal Chowers, Daniel Nevo</dc:creator>
    </item>
    <item>
      <title>Knockoffs Inference under Privacy Constraints</title>
      <link>https://arxiv.org/abs/2506.09690</link>
      <description>arXiv:2506.09690v1 Announce Type: new 
Abstract: Model-X knockoff framework offers a model-free variable selection method that ensures finite sample false discovery rate (FDR) control. However, the complexity of generating knockoff variables, coupled with the model-free assumption, presents significant challenges for protecting data privacy in this context. In this paper, we propose a comprehensive framework for knockoff inference within the differential privacy paradigm. Our proposed method guarantees robust privacy protection while preserving the exact FDR control entailed by the original model-X knockoff procedure. We further conduct power analysis and establish sufficient conditions under which the noise added for privacy preservation does not asymptotically compromise power. Through various applications, we demonstrate that the differential privacy knockoff (DP-knockoff) method can be effectively utilized to safeguard privacy during variable selection with FDR control in both low and high dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09690v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhanrui Cai, Yingying Fan, Lan Gao</dc:creator>
    </item>
    <item>
      <title>Fully Bayesian Sequential Design for Mean Response Surface Prediction of Heteroscedastic Stochastic Simulations</title>
      <link>https://arxiv.org/abs/2506.09722</link>
      <description>arXiv:2506.09722v1 Announce Type: new 
Abstract: We present a fully Bayesian sequential strategy for predicting the mean response surface of heteroscedastic stochastic simulation functions. Leveraging dual Gaussian processes as the surrogate model and a criterion based on empirical expected integrated mean-square prediction error, our approach sequentially selects informative design points while fully accounting for parameter uncertainty. Sequential importance sampling is employed to efficiently update the posterior distribution of the parameters. Our strategy is tailored for expensive simulation functions, where achieving robust predictive accuracy under a limited budget is critical. We illustrate its potential advantages compared to existing approaches through synthetic examples. We then implement the proposed strategy on a real motivating application in seismic design of wood-frame podium buildings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09722v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuying Huang, Samuel W. K. Wong</dc:creator>
    </item>
    <item>
      <title>Lower-dimensional posterior density and cluster summaries for overparameterized Bayesian models</title>
      <link>https://arxiv.org/abs/2506.09850</link>
      <description>arXiv:2506.09850v1 Announce Type: new 
Abstract: The usefulness of Bayesian models for density and cluster estimation is well established across multiple literatures. However, there is still a known tension between the use of simpler, more interpretable models and more flexible, complex ones. In this paper, we propose a novel method that integrates these two approaches by projecting the fit of a flexible, over-parameterized model onto a lower-dimensional parametric summary, which serves as a surrogate. This process increases interpretability while preserving most of the fit of the original model. Our approach involves three main steps. First, we fit the data using nonparametric or over-parameterized models. Second, we project the posterior predictive distribution of the original model onto a sequence of parametric summary estimates using a decision-theoretic approach. Finally, given the lower parametric dimension of the summary estimate that best approximates the original model learned in the second step, we construct uncertainty quantification for the summary by projecting the original full posterior distribution. We demonstrate the effectiveness of our method in summarizing a variety of nonparametric and overparameterized models, providing uncertainty quantification for both density and cluster summaries on synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09850v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Henrique Bolfarine, Hedibert F. Lopes, Carlos M. Carvalho</dc:creator>
    </item>
    <item>
      <title>Optimal Adjustment Sets for Nonparametric Estimation of Weighted Controlled Direct Effect</title>
      <link>https://arxiv.org/abs/2506.09871</link>
      <description>arXiv:2506.09871v1 Announce Type: new 
Abstract: The weighted controlled direct effect (WCDE) generalizes the standard controlled direct effect (CDE) by averaging over the mediator distribution, providing a robust estimate when treatment effects vary across mediator levels. This makes the WCDE especially relevant in fairness analysis, where it isolates the direct effect of an exposure on an outcome, independent of mediating pathways. This work establishes three fundamental advances for WCDE in observational studies: First, we establish necessary and sufficient conditions for the unique identifiability of the WCDE, clarifying when it diverges from the CDE. Next, we consider nonparametric estimation of the WCDE and derive its influence function, focusing on the class of regular and asymptotically linear estimators. Lastly, we characterize the optimal covariate adjustment set that minimizes the asymptotic variance, demonstrating how mediator-confounder interactions introduce distinct requirements compared to average treatment effect estimation. Our results offer a principled framework for efficient estimation of direct effects in complex causal systems, with practical applications in fairness and mediation analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09871v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ruiyang Lin (University of Science,Technology of China), Yongyi Guo (University of Wisconsin-Madison), Kyra Gan (Cornell Tech)</dc:creator>
    </item>
    <item>
      <title>Constrained Denoising, Empirical Bayes, and Optimal Transport</title>
      <link>https://arxiv.org/abs/2506.09986</link>
      <description>arXiv:2506.09986v1 Announce Type: new 
Abstract: In the statistical problem of denoising, Bayes and empirical Bayes methods can "overshrink" their output relative to the latent variables of interest. This work is focused on constrained denoising problems which mitigate such phenomena. At the oracle level, i.e., when the latent variable distribution is assumed known, we apply tools from the theory of optimal transport to characterize the solution to (i) variance-constrained, (ii) distribution-constrained, and (iii) general-constrained denoising problems. At the empirical level, i.e., when the latent variable distribution is not known, we use empirical Bayes methodology to estimate these oracle denoisers. Our approach is modular, and transforms any suitable (unconstrained) empirical Bayes denoiser into a constrained empirical Bayes denoiser. We prove explicit rates of convergence for our proposed methodologies, which both extend and sharpen existing asymptotic results that have previously considered only variance constraints. We apply our methodology in two applications: one in astronomy concerning the relative chemical abundances in a large catalog of red-clump stars, and one in baseball concerning minor- and major league batting skill for rookie players.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09986v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Quinn Jaffe, Nikolaos Ignatiadis, Bodhisattva Sen</dc:creator>
    </item>
    <item>
      <title>Practically significant differences between conditional distribution functions</title>
      <link>https://arxiv.org/abs/2506.06545</link>
      <description>arXiv:2506.06545v1 Announce Type: cross 
Abstract: In the framework of semiparametric distribution regression, we consider the problem of comparing the conditional distribution functions corresponding to two samples. In contrast to testing for exact equality, we are interested in the (null) hypothesis that the $L^2$ distance between the conditional distribution functions does not exceed a certain threshold in absolute value. The consideration of these hypotheses is motivated by the observation that in applications, it is rare, and perhaps impossible, that a null hypothesis of exact equality is satisfied and that the real question of interest is to detect a practically significant deviation between the two conditional distribution functions.
  The consideration of a composite null hypothesis makes the testing problem challenging, and in this paper we develop a pivotal test for such hypotheses. Our approach is based on self-normalization and therefore requires neither the estimation of (complicated) variances nor bootstrap approximations. We derive the asymptotic limit distribution of the (appropriately normalized) test statistic and show consistency under local alternatives. A simulation study and an application to German SOEP data reveal the usefulness of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06545v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Holger Dette, Kathrin M\"ollenhoff, Dominik Wied</dc:creator>
    </item>
    <item>
      <title>Integrated Analysis for Electronic Health Records with Structured and Sporadic Missingness</title>
      <link>https://arxiv.org/abs/2506.09208</link>
      <description>arXiv:2506.09208v1 Announce Type: cross 
Abstract: Objectives: We propose a novel imputation method tailored for Electronic Health Records (EHRs) with structured and sporadic missingness. Such missingness frequently arises in the integration of heterogeneous EHR datasets for downstream clinical applications. By addressing these gaps, our method provides a practical solution for integrated analysis, enhancing data utility and advancing the understanding of population health.
  Materials and Methods: We begin by demonstrating structured and sporadic missing mechanisms in the integrated analysis of EHR data. Following this, we introduce a novel imputation framework, Macomss, specifically designed to handle structurally and heterogeneously occurring missing data. We establish theoretical guarantees for Macomss, ensuring its robustness in preserving the integrity and reliability of integrated analyses. To assess its empirical performance, we conduct extensive simulation studies that replicate the complex missingness patterns observed in real-world EHR systems, complemented by validation using EHR datasets from the Duke University Health System (DUHS).
  Results: Simulation studies show that our approach consistently outperforms existing imputation methods. Using datasets from three hospitals within DUHS, Macomss achieves the lowest imputation errors for missing data in most cases and provides superior or comparable downstream prediction performance compared to benchmark methods.
  Conclusions: We provide a theoretically guaranteed and practically meaningful method for imputing structured and sporadic missing data, enabling accurate and reliable integrated analysis across multiple EHR datasets. The proposed approach holds significant potential for advancing research in population health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09208v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jianbin Tan, Yan Zhang, Chuan Hong, T. Tony Cai, Tianxi Cai, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Causal Graph Recovery in Neuroimaging through Answer Set Programming</title>
      <link>https://arxiv.org/abs/2506.09286</link>
      <description>arXiv:2506.09286v1 Announce Type: cross 
Abstract: Learning graphical causal structures from time series data presents significant challenges, especially when the measurement frequency does not match the causal timescale of the system. This often leads to a set of equally possible underlying causal graphs due to information loss from sub-sampling (i.e., not observing all possible states of the system throughout time). Our research addresses this challenge by incorporating the effects of sub-sampling in the derivation of causal graphs, resulting in more accurate and intuitive outcomes. We use a constraint optimization approach, specifically answer set programming (ASP), to find the optimal set of answers. ASP not only identifies the most probable underlying graph, but also provides an equivalence class of possible graphs for expert selection. In addition, using ASP allows us to leverage graph theory to further prune the set of possible solutions, yielding a smaller, more accurate answer set significantly faster than traditional approaches. We validate our approach on both simulated data and empirical structural brain connectivity, and demonstrate its superiority over established methods in these experiments. We further show how our method can be used as a meta-approach on top of established methods to obtain, on average, 12% improvement in F1 score. In addition, we achieved state of the art results in terms of precision and recall of reconstructing causal graph from sub-sampled time series data. Finally, our method shows robustness to varying degrees of sub-sampling on realistic simulations, whereas other methods perform worse for higher rates of sub-sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09286v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammadsajad Abavisani, Kseniya Solovyeva, David Danks, Vince Calhoun, Sergey Plis</dc:creator>
    </item>
    <item>
      <title>LLM-Powered CPI Prediction Inference with Online Text Time Series</title>
      <link>https://arxiv.org/abs/2506.09516</link>
      <description>arXiv:2506.09516v1 Announce Type: cross 
Abstract: Forecasting the Consumer Price Index (CPI) is an important yet challenging task in economics, where most existing approaches rely on low-frequency, survey-based data. With the recent advances of large language models (LLMs), there is growing potential to leverage high-frequency online text data for improved CPI prediction, an area still largely unexplored. This paper proposes LLM-CPI, an LLM-based approach for CPI prediction inference incorporating online text time series. We collect a large set of high-frequency online texts from a popularly used Chinese social network site and employ LLMs such as ChatGPT and the trained BERT models to construct continuous inflation labels for posts that are related to inflation. Online text embeddings are extracted via LDA and BERT. We develop a joint time series framework that combines monthly CPI data with LLM-generated daily CPI surrogates. The monthly model employs an ARX structure combining observed CPI data with text embeddings and macroeconomic variables, while the daily model uses a VARX structure built on LLM-generated CPI surrogates and text embeddings. We establish the asymptotic properties of the method and provide two forms of constructed prediction intervals. The finite-sample performance and practical advantages of LLM-CPI are demonstrated through both simulation and real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09516v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingying Fan, Jinchi Lv, Ao Sun, Yurou Wang</dc:creator>
    </item>
    <item>
      <title>Parallel computations for Metropolis Markov chains with Picard maps</title>
      <link>https://arxiv.org/abs/2506.09762</link>
      <description>arXiv:2506.09762v1 Announce Type: cross 
Abstract: We develop parallel algorithms for simulating zeroth-order (aka gradient-free) Metropolis Markov chains based on the Picard map. For Random Walk Metropolis Markov chains targeting log-concave distributions $\pi$ on $\mathbb{R}^d$, our algorithm generates samples close to $\pi$ in $\mathcal{O}(\sqrt{d})$ parallel iterations with $\mathcal{O}(\sqrt{d})$ processors, therefore speeding up the convergence of the corresponding sequential implementation by a factor $\sqrt{d}$. Furthermore, a modification of our algorithm generates samples from an approximate measure $ \pi_\epsilon$ in $\mathcal{O}(1)$ parallel iterations and $\mathcal{O}(d)$ processors. We empirically assess the performance of the proposed algorithms in high-dimensional regression problems and an epidemic model where the gradient is unavailable. Our algorithms are straightforward to implement and may constitute a useful tool for practitioners seeking to sample from a prescribed distribution $\pi$ using only point-wise evaluations of $\log\pi$ and parallel computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09762v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastiano Grazzi, Giacomo Zanella</dc:creator>
    </item>
    <item>
      <title>Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning</title>
      <link>https://arxiv.org/abs/2506.09853</link>
      <description>arXiv:2506.09853v1 Announce Type: cross 
Abstract: Chain-of-Thought (CoT) prompting plays an indispensable role in endowing large language models (LLMs) with complex reasoning capabilities. However, CoT currently faces two fundamental challenges: (1) Sufficiency, which ensures that the generated intermediate inference steps comprehensively cover and substantiate the final conclusion; and (2) Necessity, which identifies the inference steps that are truly indispensable for the soundness of the resulting answer. We propose a causal framework that characterizes CoT reasoning through the dual lenses of sufficiency and necessity. Incorporating causal Probability of Sufficiency and Necessity allows us not only to determine which steps are logically sufficient or necessary to the prediction outcome, but also to quantify their actual influence on the final reasoning outcome under different intervention scenarios, thereby enabling the automated addition of missing steps and the pruning of redundant ones. Extensive experimental results on various mathematical and commonsense reasoning benchmarks confirm substantial improvements in reasoning efficiency and reduced token usage without sacrificing accuracy. Our work provides a promising direction for improving LLM reasoning performance and cost-effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09853v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiangning Yu, Zhuohan Wang, Linyi Yang, Haoxuan Li, Anjie Liu, Xiao Xue, Jun Wang, Mengyue Yang</dc:creator>
    </item>
    <item>
      <title>Nonstationary Spatial Process Models with Spatially Varying Covariance Kernels</title>
      <link>https://arxiv.org/abs/2203.11873</link>
      <description>arXiv:2203.11873v3 Announce Type: replace 
Abstract: Building spatial process models that capture nonstationary behavior while delivering computationally efficient inference is challenging. Nonstationary spatially varying kernels (see, e.g., Paciorek, 2003) offer flexibility and richness, but computation is impeded by high-dimensional parameter spaces resulting from spatially varying process parameters. Matters are exacerbated if the number of locations recording measurements is massive. With limited theoretical tractability, obviating computational bottlenecks requires synergy between model construction and algorithm development. We build a class of scalable nonstationary spatial process models using spatially varying covariance kernels. We implement a Bayesian modeling framework using Hybrid Monte Carlo with nested interweaving. We conduct experiments on synthetic data sets to explore model selection and parameter identifiability, and assess inferential improvements accrued from nonstationary modeling. We illustrate strengths and pitfalls with a data set on remote sensed normalized difference vegetation index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.11873v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S\'ebastien Coube-Sisqueille, Sudipto Banerjee, Beno\^it Liquet</dc:creator>
    </item>
    <item>
      <title>Adaptive Projected Two-Sample Comparisons for Single-Cell Gene Expression Data</title>
      <link>https://arxiv.org/abs/2403.05679</link>
      <description>arXiv:2403.05679v2 Announce Type: replace 
Abstract: We study high-dimensional two-sample mean comparison and address the curse of dimensionality through data-adaptive projections. Leveraging the low-dimensional and localized signal structures commonly seen in single-cell genomics data, our first proposed method identifies a sparse, informative low-dimensional subspace and then performs statistical inference restricted to this subspace. To address the double-dipping issue -- arising from using the same data for projection and inference -- we develop a debiased projected estimator using the semiparametric double-machine learning framework. The resulting inference not only has the usual frequentist validity but also provides useful information on the potential location of the signal due to the sparsity of the projection. Our second method uses a more flexible projection scheme to improve the power against the global null hypothesis and avoid the degeneracy issue commonly faced by existing methods. It is particularly useful when debiasing is practically challenging or when the informative signal is not well-captured by the subspace. Experiments on synthetic data and real datasets demonstrate the theoretical promise and interpretability of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05679v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianyu Zhang, Jing Lei, Kathryn Roeder</dc:creator>
    </item>
    <item>
      <title>Fisher's Randomization Test for Causality with General Types of Treatments</title>
      <link>https://arxiv.org/abs/2501.06864</link>
      <description>arXiv:2501.06864v2 Announce Type: replace 
Abstract: We extend Fisher's randomization test (FRT) to test conditional independence between observed outcomes and treatments given covariates in both randomized experiments and observational studies, with no restriction on the variable type of treatments. Under a generalized unconfoundedness assumption, we provide causal identification for this hypothesis. Our approach requires neither the no-interference nor the positive overlap assumption, making it a widely applicable tool for detecting causal effects. A unique advantage of FRT lies in the separated roles of assignment and outcome models. The former, whether known from randomized experiments or estimated in observational studies, guarantees valid Type I error control at least asymptotically. The latter, even if misspecified, is used to construct optimal test statistics derived from Bayes factors. The synthesis of two classes of models through FRT yields a calibrated Bayesian procedure with desired frequentist properties. Recognizing that the generalized unconfoundedness assumption is untestable in observational studies, we develop a novel sensitivity analysis to assess the robustness of causal conclusions to unobserved confounding. Through a re-analysis of a panel dataset, we show how our methods can be integrated into a pipeline for observational causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06864v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhen Zhong</dc:creator>
    </item>
    <item>
      <title>Association and Independence Test for Random Objects</title>
      <link>https://arxiv.org/abs/2505.01983</link>
      <description>arXiv:2505.01983v2 Announce Type: replace 
Abstract: We develop a unified framework for testing independence and quantifying association between random objects that are located in general metric spaces. Special cases include functional and high-dimensional data as well as networks, covariance matrices and data on Riemannian manifolds, among other metric space-valued data. A key concept is the profile association, a measure based on distance profiles that intrinsically characterize the distributions of random objects in metric spaces. We rigorously establish a connection between the Hoeffding D statistic and the profile association and derive a permutation test with theoretical guarantees for consistency and power under alternatives to the null hypothesis of independence/no association. We extend this framework to the conditional setting, where the independence between random objects given a Euclidean predictor is of interest. In simulations across various metric spaces, the proposed profile independence test is found to outperform existing approaches. The practical utility of this framework is demonstrated with applications to brain connectivity networks derived from magnetic resonance imaging and age-at-death distributions for males and females obtained from human mortality data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01983v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hang Zhou, Hans-Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>Change-Points Detection and Support Recovery for Spatially Indexed Functional Data</title>
      <link>https://arxiv.org/abs/2506.07206</link>
      <description>arXiv:2506.07206v2 Announce Type: replace 
Abstract: Large volumes of spatiotemporal data, characterized by high spatial and temporal variability, may experience structural changes over time. Unlike traditional change-point problems, each sequence in this context consists of function-valued curves observed at multiple spatial locations, with typically only a small subset of locations affected. This paper addresses two key issues: detecting the global change-point and identifying the spatial support set, within a unified framework tailored to spatially indexed functional data. By leveraging a weakly separable cross-covariance structure -- an extension beyond the restrictive assumption of space-time separability -- we incorporate functional principal component analysis into the change-detection methodology, while preserving common temporal features across locations. A kernel-based test statistic is further developed to integrate spatial clustering pattern into the detection process, and its local variant, combined with the estimated change-point, is employed to identify the subset of locations contributing to the mean shifts. To control the false discovery rate in multiple testing, we introduce a functional symmetrized data aggregation approach that does not rely on pointwise p-values and effectively pools spatial information. We establish the asymptotic validity of the proposed change detection and support recovery method under mild regularity conditions. The efficacy of our approach is demonstrated through simulations, with its practical usefulness illustrated in an application to China's precipitation data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07206v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fengyi Song, Decai Liang, Changliang Zou</dc:creator>
    </item>
    <item>
      <title>Guidelines for LASSO and derivatives use under different dependence and scale structures</title>
      <link>https://arxiv.org/abs/2506.08582</link>
      <description>arXiv:2506.08582v2 Announce Type: replace 
Abstract: In a multivariate linear regression model with $p&gt;1$ covariates, implementation of penalization techniques often implies a preliminary univariate standardization step. Although this prevents scale effects on the covariates selection procedure, possible dependence structures can be disrupted, leading to wrong results. This is particularly challenging in high-dimensional settings where $p \geq n$. In this paper, we analyze the standardization effect on the LASSO for different dependence-scales contexts by means of an extensive simulation study. Two distinct objectives are pursued: adequate covariate selection and proper predictive capability. Additionally, its behavior is compared with the one of some well-known or innovative competitors. This comparison is also extended to three real datasets facing different dependence-scales patterns. Eventually, we conclude with discussion and guidelines on the most suitable methodology for each case in terms of covariates selection or prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08582v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laura Freijeiro-Gonz\'alez, Manuel Febrero-Bande, Wenceslao Gonz\'alez-Manteiga</dc:creator>
    </item>
    <item>
      <title>Simulation-based Inference for High-dimensional Data using Surjective Sequential Neural Likelihood Estimation</title>
      <link>https://arxiv.org/abs/2308.01054</link>
      <description>arXiv:2308.01054v3 Announce Type: replace-cross 
Abstract: Neural likelihood estimation methods for simulation-based inference can suffer from performance degradation when the modeled data is very high-dimensional or lies along a lower-dimensional manifold, which is due to the inability of the density estimator to accurately estimate a density function. We present Surjective Sequential Neural Likelihood (SSNL) estimation, a novel member in the family of methods for simulation-based inference (SBI). SSNL fits a dimensionality-reducing surjective normalizing flow model and uses it as a surrogate likelihood function, which allows for computational inference via Markov chain Monte Carlo or variational Bayes methods. Among other benefits, SSNL avoids the requirement to manually craft summary statistics for inference of high-dimensional data sets, since the lower-dimensional representation is computed simultaneously with learning the likelihood and without additional computational overhead. We evaluate SSNL on a wide variety of experiments, including two challenging real-world examples from the astrophysics and neuroscience literatures, and show that it either outperforms or is on par with state-of-the-art methods, making it an excellent off-the-shelf estimator for SBI for high-dimensional data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.01054v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Dirmeier, Carlo Albert, Fernando Perez-Cruz</dc:creator>
    </item>
    <item>
      <title>Temperature Optimization for Bayesian Deep Learning</title>
      <link>https://arxiv.org/abs/2410.05757</link>
      <description>arXiv:2410.05757v2 Announce Type: replace-cross 
Abstract: The Cold Posterior Effect (CPE) is a phenomenon in Bayesian Deep Learning (BDL), where tempering the posterior to a cold temperature often improves the predictive performance of the posterior predictive distribution (PPD). Although the term `CPE' suggests colder temperatures are inherently better, the BDL community increasingly recognizes that this is not always the case. Despite this, there remains no systematic method for finding the optimal temperature beyond grid search. In this work, we propose a data-driven approach to select the temperature that maximizes test log-predictive density, treating the temperature as a model parameter and estimating it directly from the data. We empirically demonstrate that our method performs comparably to grid search, at a fraction of the cost, across both regression and classification tasks. Finally, we highlight the differing perspectives on CPE between the BDL and Generalized Bayes communities: while the former primarily emphasizes the predictive performance of the PPD, the latter prioritizes the utility of the posterior under model misspecification; these distinct objectives lead to different temperature preferences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05757v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenyon Ng, Chris van der Heide, Liam Hodgkinson, Susan Wei</dc:creator>
    </item>
    <item>
      <title>On the optimality of coin-betting for mean estimation</title>
      <link>https://arxiv.org/abs/2412.02640</link>
      <description>arXiv:2412.02640v2 Announce Type: replace-cross 
Abstract: Confidence sequences are sequences of confidence sets that adapt to incoming data while maintaining validity. Recent advances have introduced an algorithmic formulation for constructing some of the tightest confidence sequences for the mean of bounded real random variables. These approaches use a coin-betting framework, where a player sequentially bets on differences between potential mean values and observed data. This work discusses the optimality of such coin-betting formulation among algorithmic frameworks building on e-variables methods to test and estimate the mean of bounded random variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02640v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eugenio Clerico</dc:creator>
    </item>
    <item>
      <title>Difference-in-Differences Designs: A Practitioner's Guide</title>
      <link>https://arxiv.org/abs/2503.13323</link>
      <description>arXiv:2503.13323v2 Announce Type: replace-cross 
Abstract: Difference-in-differences (DiD) is arguably the most popular quasi-experimental research design. Its canonical form, with two groups and two periods, is well-understood. However, empirical practices can be ad hoc when researchers go beyond that simple case. This article provides an organizing framework for discussing different types of DiD designs and their associated DiD estimators. It discusses covariates, weights, handling multiple periods, and staggered treatments. The organizational framework, however, applies to other extensions of DiD methods as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13323v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrew Baker, Brantly Callaway, Scott Cunningham, Andrew Goodman-Bacon, Pedro H. C. Sant'Anna</dc:creator>
    </item>
  </channel>
</rss>
