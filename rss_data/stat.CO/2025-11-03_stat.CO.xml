<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 03 Nov 2025 05:01:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Robust fuzzy clustering for high-dimensional multivariate time series with outlier detection</title>
      <link>https://arxiv.org/abs/2510.26982</link>
      <description>arXiv:2510.26982v1 Announce Type: new 
Abstract: Fuzzy clustering provides a natural framework for modeling partial memberships, particularly important in multivariate time series (MTS) where state boundaries are often ambiguous. For example, in EEG monitoring of driver alertness, neural activity evolves along a continuum (from unconscious to fully alert, with many intermediate levels of drowsiness) so crisp labels are unrealistic and partial memberships are essential. However, most existing algorithms are developed for static, low-dimensional data and struggle with temporal dependence, unequal sequence lengths, high dimensionality, and contamination by noise or artifacts. To address these challenges, we introduce RFCPCA, a robust fuzzy subspace-clustering method explicitly tailored to MTS that, to the best of our knowledge, is the first of its kind to simultaneously: (i) learn membership-informed subspaces, (ii) accommodate unequal lengths and moderately high dimensions, (iii) achieve robustness through trimming, exponential reweighting, and a dedicated noise cluster, and (iv) automatically select all required hyperparameters. These components enable RFCPCA to capture latent temporal structure, provide calibrated membership uncertainty, and flag series-level outliers while remaining stable under contamination. On driver drowsiness EEG, RFCPCA improves clustering accuracy over related methods and yields a more reliable characterization of uncertainty and outlier structure in MTS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26982v1</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziling Ma, \'Angel L\'opez-Oriona, Hernando Ombao, Ying Sun</dc:creator>
    </item>
    <item>
      <title>Differential Set Selection via Confidence-Guided Entropy Minimization</title>
      <link>https://arxiv.org/abs/2510.27479</link>
      <description>arXiv:2510.27479v1 Announce Type: new 
Abstract: This paper addresses the challenge of identifying a minimal subset of discrete, independent variables that best predicts a binary class. We propose an efficient iterative method that sequentially selects variables based on which one provides the most statistically significant reduction in conditional entropy, using confidence bounds to account for finite-sample uncertainty. Tests on simulated data demonstrate the method's ability to correctly identify influential variables while minimizing spurious selections, even with small sample sizes, offering a computationally tractable solution to this NP-complete problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27479v1</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mar\'ia del Carmen Romero, Mariana del Fresno, Alejandro Clausse</dc:creator>
    </item>
    <item>
      <title>The Interplay between Bayesian Inference and Conformal Prediction</title>
      <link>https://arxiv.org/abs/2510.26930</link>
      <description>arXiv:2510.26930v1 Announce Type: cross 
Abstract: Conformal prediction has emerged as a cutting-edge methodology in statistics and machine learning, providing prediction intervals with finite-sample frequentist coverage guarantees. Yet, its interplay with Bayesian statistics, often criticised for lacking frequentist guarantees, remains underexplored. Recent work has suggested that conformal prediction can serve to "calibrate" Bayesian credible sets, thereby imparting frequentist validity and motivating deeper investigation into frequentist-Bayesian hybrids. We further argue that Bayesian procedures have the potential to enhance conformal prediction, not only in terms of more informative intervals, but also for achieving nearly optimal solutions under a decision-theoretic framework. Thus, the two paradigms can be jointly used for a principled balance between validity and efficiency. This work provides a basis for bridging this gap. After surveying existing ideas, we formalise the Bayesian conformal inference framework, covering challenging aspects such as statistical efficiency and computational complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26930v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nina Deliu, Brunero Liseo</dc:creator>
    </item>
    <item>
      <title>AI-boosted rare event sampling to characterize extreme weather</title>
      <link>https://arxiv.org/abs/2510.27066</link>
      <description>arXiv:2510.27066v1 Announce Type: cross 
Abstract: Assessing the frequency and intensity of extreme weather events, and understanding how climate change affects them, is crucial for developing effective adaptation and mitigation strategies. However, observational datasets are too short and physics-based global climate models (GCMs) are too computationally expensive to obtain robust statistics for the rarest, yet most impactful, extreme events. AI-based emulators have shown promise for predictions at weather and even climate timescales, but they struggle on extreme events with few or no examples in their training dataset. Rare event sampling (RES) algorithms have previously demonstrated success for some extreme events, but their performance depends critically on a hard-to-identify "score function", which guides efficient sampling by a GCM. Here, we develop a novel algorithm, AI+RES, which uses ensemble forecasts of an AI weather emulator as the score function to guide highly efficient resampling of the GCM and generate robust (physics-based) extreme weather statistics and associated dynamics at 30-300x lower cost. We demonstrate AI+RES on mid-latitude heatwaves, a challenging test case requiring a score function with predictive skill many days in advance. AI+RES, which synergistically integrates AI, RES, and GCMs, offers a powerful, scalable tool for studying extreme events in climate science, as well as other disciplines in science and engineering where rare events and AI emulators are active areas of research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27066v1</guid>
      <category>physics.ao-ph</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amaury Lancelin, Alex Wikner, Laurent Dubus, Cl\'ement Le Priol, Dorian S. Abbot, Freddy Bouchet, Pedram Hassanzadeh, Jonathan Weare</dc:creator>
    </item>
    <item>
      <title>Bayesian Optimization on Networks</title>
      <link>https://arxiv.org/abs/2510.27643</link>
      <description>arXiv:2510.27643v1 Announce Type: cross 
Abstract: This paper studies optimization on networks modeled as metric graphs. Motivated by applications where the objective function is expensive to evaluate or only available as a black box, we develop Bayesian optimization algorithms that sequentially update a Gaussian process surrogate model of the objective to guide the acquisition of query points. To ensure that the surrogates are tailored to the network's geometry, we adopt Whittle-Mat\'ern Gaussian process prior models defined via stochastic partial differential equations on metric graphs. In addition to establishing regret bounds for optimizing sufficiently smooth objective functions, we analyze the practical case in which the smoothness of the objective is unknown and the Whittle-Mat\'ern prior is represented using finite elements. Numerical results demonstrate the effectiveness of our algorithms for optimizing benchmark objective functions on a synthetic metric graph and for Bayesian inversion via maximum a posteriori estimation on a telecommunication network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27643v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenwen Li, Daniel Sanz-Alonso, Ruiyi Yang</dc:creator>
    </item>
    <item>
      <title>Estimating MCMC convergence rates using common random number simulation</title>
      <link>https://arxiv.org/abs/2309.15735</link>
      <description>arXiv:2309.15735v4 Announce Type: replace 
Abstract: This paper presents how to use common random number (CRN) simulation to evaluate Markov chain Monte Carlo (MCMC) convergence to stationarity. We provide an upper bound on the Wasserstein distance of a Markov chain to its stationary distribution after $N$ steps in terms of averages over CRN simulations. We apply our bound to Gibbs samplers on a model related to James-Stein estimators, a variance component model, and a Bayesian linear regression model. For the first two examples, we show that the CRN simulated bound converges to zero significantly more quickly compared to available drift and minorization bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15735v4</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sabrina Sixta, Jeffrey S. Rosenthal, Austin Brown</dc:creator>
    </item>
    <item>
      <title>Amortising over hyperparameters in Generalised Bayesian Inference</title>
      <link>https://arxiv.org/abs/2412.16419</link>
      <description>arXiv:2412.16419v2 Announce Type: replace 
Abstract: In Bayesian inference prior hyperparameters are chosen subjectively or estimated using empirical Bayes methods. Generalised Bayesian Inference (GBI) also has a learning rate hyperparameter. This is compounded in Semi-Modular Inference (SMI), a GBI framework for multiple datasets (multi-modular problems). As part of any GBI workflow it is necessary to check sensitivity to the choice of hyperparameters, but running MCMC or fitting a variational approximation at each of the hyperparameter values of interest is impractical. Simulation-based Inference has been used by previous authors to amortise over data and hyperparameters, fitting a posterior approximation targeting the forward-KL divergence. However, for GBI and SMI posteriors, it is not possible to amortise over data, as there is no generative model. Working with a variational family parameterised by a conditional normalising flow, we give a direct variational approximation for GBI and SMI posteriors, targeting the reverse-KL divergence, and amortised over prior and loss hyperparameters at fixed data. This can be sampled efficiently at different hyperparameter values without refitting, and supports efficient robustness checks and hyperparameter selection. We show that there exist amortised conditional normalising-flow architectures which are universal approximators. We illustrate our methods with an epidemiological example well known in SMI work and then give the motivating application, a spatial location-prediction task for linguistic-profile data. SMI gives improved prediction with hyperparameters chosen using our amortised framework. The code is available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16419v2</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Battaglia, Chris U. Carmona, Ross A. Haines, Max Anderson Loake, Michael Benskin, Geoff K. Nicholls</dc:creator>
    </item>
    <item>
      <title>Robust Principal Components by Casewise and Cellwise Weighting</title>
      <link>https://arxiv.org/abs/2408.13596</link>
      <description>arXiv:2408.13596v2 Announce Type: replace-cross 
Abstract: Principal component analysis (PCA) is a fundamental tool for analyzing multivariate data. Here the focus is on dimension reduction to the principal subspace, characterized by its projection matrix. The classical principal subspace can be strongly affected by the presence of outliers. Traditional robust approaches consider casewise outliers, that is, cases generated by an unspecified outlier distribution that differs from that of the clean cases. But there may also be cellwise outliers, which are suspicious entries that can occur anywhere in the data matrix. Another common issue is that some cells may be missing. This paper proposes a new robust PCA method, called cellPCA, that can simultaneously deal with casewise outliers, cellwise outliers, and missing cells. Its single objective function combines two robust loss functions, that together mitigate the effect of casewise and cellwise outliers. The objective function is minimized by an iteratively reweighted least squares (IRLS) algorithm. Residual cellmaps and enhanced outlier maps are proposed for outlier detection. The casewise and cellwise influence functions of the principal subspace are derived, and its asymptotic distribution is obtained. Extensive simulations and two real data examples illustrate the performance of cellPCA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13596v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabio Centofanti, Mia Hubert, Peter J. Rousseeuw</dc:creator>
    </item>
    <item>
      <title>Generative Adversarial Networks for High-Dimensional Item Factor Analysis: A Deep Adversarial Learning Algorithm</title>
      <link>https://arxiv.org/abs/2502.10650</link>
      <description>arXiv:2502.10650v3 Announce Type: replace-cross 
Abstract: Advances in deep learning and representation learning have transformed item factor analysis (IFA) in the item response theory (IRT) literature by enabling more efficient and accurate parameter estimation. Variational Autoencoders (VAEs) have been one of the most impactful techniques in modeling high-dimensional latent variables in this context. However, the limited expressiveness of the inference model based on traditional VAEs can still hinder the estimation performance. We introduce Adversarial Variational Bayes (AVB) algorithms as an improvement to VAEs for IFA with improved flexibility and accuracy. By bridging the strengths of VAEs and Generative Adversarial Networks (GANs), AVB incorporates an auxiliary discriminator network to reframe the estimation process as a two-player adversarial game and removes the restrictive assumption of standard normal distributions in the inference model. Theoretically, AVB can achieve similar or higher likelihood compared to VAEs. A further enhanced algorithm, Importance-weighted Adversarial Variational Bayes (IWAVB) is proposed and compared with Importance-weighted Autoencoders (IWAE). In an exploratory analysis of empirical data, IWAVB demonstrated superior expressiveness by achieving a higher likelihood compared to IWAE. In confirmatory analysis with simulated data, IWAVB achieved similar mean-square error results to IWAE while consistently achieving higher likelihoods. When latent variables followed a multimodal distribution, IWAVB outperformed IWAE. With its innovative use of GANs, IWAVB is shown to have the potential to extend IFA to handle large-scale data, facilitating the potential integration of psychometrics and multimodal data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10650v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nanyu Luo, Feng Ji</dc:creator>
    </item>
    <item>
      <title>SLIM: Stochastic Learning and Inference in Overidentified Models</title>
      <link>https://arxiv.org/abs/2510.20996</link>
      <description>arXiv:2510.20996v2 Announce Type: replace-cross 
Abstract: We propose SLIM (Stochastic Learning and Inference in overidentified Models), a scalable stochastic approximation framework for nonlinear GMM. SLIM forms iterative updates from independent mini-batches of moments and their derivatives, producing unbiased directions that ensure almost-sure convergence. It requires neither a consistent initial estimator nor global convexity and accommodates both fixed-sample and random-sampling asymptotics. We further develop an optional second-order refinement achieving full-sample GMM efficiency and inference procedures based on random scaling and plug-in methods, including plug-in, debiased plug-in, and online versions of the Sargan--Hansen $J$-test tailored to stochastic learning. In Monte Carlo experiments based on a nonlinear demand system with 576 moment conditions, 380 parameters, and $n = 10^5$, SLIM solves the model in under 1.4 hours, whereas full-sample GMM in Stata on a powerful laptop converges only after 18 hours. The debiased plug-in $J$-test delivers satisfactory finite-sample inference, and SLIM scales smoothly to $n = 10^6$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20996v2</guid>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiaohong Chen, Min Seong Kim, Sokbae Lee, Myung Hwan Seo, Myunghyun Song</dc:creator>
    </item>
  </channel>
</rss>
