<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 26 Apr 2024 04:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 26 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Pilot Study to Discover Candidate Biomarkers for Autism based on Perception and Production of Facial Expressions</title>
      <link>https://arxiv.org/abs/2404.16040</link>
      <description>arXiv:2404.16040v1 Announce Type: new 
Abstract: Purpose: Facial expression production and perception in autism spectrum disorder (ASD) suggest potential presence of behavioral biomarkers that may stratify individuals on the spectrum into prognostic or treatment subgroups. Construct validity and group discriminability have been recommended as criteria for identification of candidate stratification biomarkers.
  Methods: In an online pilot study of 11 children and young adults diagnosed with ASD and 11 age- and gender-matched neurotypical (NT) individuals, participants recognize and mimic static and dynamic facial expressions of 3D avatars. Webcam-based eye-tracking (ET) and facial video tracking (VT), including activation and asymmetry of action units (AUs) from the Facial Action Coding System (FACS) are collected. We assess validity of constructs for each dependent variable (DV) based on the expected response in the NT group. Then, the Boruta statistical method identifies DVs that are significant to group discriminability (ASD or NT).
  Results: We identify one candidate ET biomarker (percentage gaze duration to the face while mimicking static 'disgust' expression) and 14 additional DVs of interest for future study, including 4 ET DVs, 5 DVs related to VT AU activation, and 4 DVs related to AU asymmetry in VT. Based on a power analysis, we provide sample size recommendations for future studies.
  Conclusion: This pilot study provides a framework for ASD stratification biomarker discovery based on perception and production of facial expressions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16040v1</guid>
      <category>q-bio.NC</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Megan A. Witherow, Norou Diawara, Janice Keener, John W. Harrington, Khan M. Iftekharuddin</dc:creator>
    </item>
    <item>
      <title>Reverse engineering the brain input: Network control theory to identify cognitive task-related control nodes</title>
      <link>https://arxiv.org/abs/2404.16357</link>
      <description>arXiv:2404.16357v1 Announce Type: new 
Abstract: The human brain receives complex inputs when performing cognitive tasks, which range from external inputs via the senses to internal inputs from other brain regions. However, the explicit inputs to the brain during a cognitive task remain unclear. Here, we present an input identification framework for reverse engineering the control nodes and the corresponding inputs to the brain. The framework is verified with synthetic data generated by a predefined linear system, indicating it can robustly reconstruct data and recover the inputs. Then we apply the framework to the real motor-task fMRI data from 200 human subjects. Our results show that the model with sparse inputs can reconstruct neural dynamics in motor tasks ($EV=0.779$) and the identified 28 control nodes largely overlap with the motor system. Underpinned by network control theory, our framework offers a general tool for understanding brain inputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16357v1</guid>
      <category>q-bio.NC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhichao Liang, Yinuo Zhang, Jushen Wu, Quanying Liu</dc:creator>
    </item>
    <item>
      <title>CoCoG: Controllable Visual Stimuli Generation based on Human Concept Representations</title>
      <link>https://arxiv.org/abs/2404.16482</link>
      <description>arXiv:2404.16482v1 Announce Type: new 
Abstract: A central question for cognitive science is to understand how humans process visual objects, i.e, to uncover human low-dimensional concept representation space from high-dimensional visual stimuli. Generating visual stimuli with controlling concepts is the key. However, there are currently no generative models in AI to solve this problem. Here, we present the Concept based Controllable Generation (CoCoG) framework. CoCoG consists of two components, a simple yet efficient AI agent for extracting interpretable concept and predicting human decision-making in visual similarity judgment tasks, and a conditional generation model for generating visual stimuli given the concepts. We quantify the performance of CoCoG from two aspects, the human behavior prediction accuracy and the controllable generation ability. The experiments with CoCoG indicate that 1) the reliable concept embeddings in CoCoG allows to predict human behavior with 64.07\% accuracy in the THINGS-similarity dataset; 2) CoCoG can generate diverse objects through the control of concepts; 3) CoCoG can manipulate human similarity judgment behavior by intervening key concepts. CoCoG offers visual objects with controlling concepts to advance our understanding of causality in human cognition. The code of CoCoG is available at \url{https://github.com/ncclab-sustech/CoCoG}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16482v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chen Wei, Jiachen Zou, Dietmar Heinke, Quanying Liu</dc:creator>
    </item>
    <item>
      <title>Directional intermodular coupling enriches functional complexity in biological neuronal networks</title>
      <link>https://arxiv.org/abs/2404.16582</link>
      <description>arXiv:2404.16582v1 Announce Type: new 
Abstract: Hierarchically modular organization is a canonical network topology that is evolutionarily conserved in the nervous systems of animals. Within the network, neurons form directional connections defined by the growth of their axonal terminals. However, this topology is dissimilar to the network formed by dissociated neurons in culture because they form randomly connected networks on homogeneous substrates. In this study, we fabricated microfluidic devices to reconstitute hierarchically modular neuronal networks in culture (in vitro) and investigated how non-random structures, such as directional connectivity between modules, affect global network dynamics. Embedding directional connections in a pseudo-feedforward manner suppressed excessive synchrony in cultured neuronal networks and enhanced the integration-segregation balance. Modeling the behavior of biological neuronal networks using spiking neural networks (SNNs) further revealed that modularity and directionality cooperate to shape such network dynamics. Finally, we demonstrate that for a given network topology, the statistics of network dynamics, such as global network activation, correlation coefficient, and functional complexity, can be analytically predicted based on eigendecomposition of the transition matrix in the state-transition model. Hence, the integration of bioengineering and cell culture technologies enables us not only to reconstitute complex network circuitry in the nervous system but also to understand the structure-function relationships in biological neuronal networks by bridging theoretical modeling with in vitro experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16582v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nobuaki Monma, Hideaki Yamamoto, Naoya Fujiwara, Hakuba Murota, Satoshi Moriya, Ayumi Hirano-Iwata, Shigeo Sato</dc:creator>
    </item>
    <item>
      <title>Lu.i -- A low-cost electronic neuron for education and outreach</title>
      <link>https://arxiv.org/abs/2404.16664</link>
      <description>arXiv:2404.16664v1 Announce Type: new 
Abstract: With an increasing presence of science throughout all parts of society, there is a rising expectation for researchers to effectively communicate their work and, equally, for teachers to discuss contemporary findings in their classrooms. While the community can resort to an established set of teaching aids for the fundamental concepts of most natural sciences, there is a need for similarly illustrative experiments and demonstrators in neuroscience. We therefore introduce Lu.i: a parametrizable electronic implementation of the leaky-integrate-and-fire neuron model in an engaging form factor. These palm-sized neurons can be used to visualize and experience the dynamics of individual cells and small spiking neural networks. When stimulated with real or simulated sensory input, Lu.i demonstrates brain-inspired information processing in the hands of a student. As such, it is actively used at workshops, in classrooms, and for science communication. As a versatile tool for teaching and outreach, Lu.i nurtures the comprehension of neuroscience research and neuromorphic engineering among future generations of scientists and in the general public.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16664v1</guid>
      <category>q-bio.NC</category>
      <category>cs.NE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yannik Stradmann, Julian G\"oltz, Mihai A. Petrovici, Johannes Schemmel, Sebastian Billaudelle</dc:creator>
    </item>
    <item>
      <title>Report on Candidate Computational Indicators for Conscious Valenced Experience</title>
      <link>https://arxiv.org/abs/2404.16696</link>
      <description>arXiv:2404.16696v1 Announce Type: new 
Abstract: This report enlists 13 functional conditions cashed out in computational terms that have been argued to be constituent of conscious valenced experience. These are extracted from existing empirical and theoretical literature on, among others, animal sentience, medical disorders, anaesthetics, philosophy, evolution, neuroscience, and artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16696v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andres Campero</dc:creator>
    </item>
    <item>
      <title>Brain-inspired computing with fluidic iontronic nanochannels</title>
      <link>https://arxiv.org/abs/2309.11438</link>
      <description>arXiv:2309.11438v2 Announce Type: replace-cross 
Abstract: The brain's remarkable and efficient information processing capability is driving research into brain-inspired (neuromorphic) computing paradigms. Artificial aqueous ion channels are emerging as an exciting platform for neuromorphic computing, representing a departure from conventional solid-state devices by directly mimicking the brain's fluidic ion transport. Supported by a quantitative theoretical model, we present easy to fabricate tapered microchannels that embed a conducting network of fluidic nanochannels between a colloidal structure. Due to transient salt concentration polarisation our devices are volatile memristors (memory resistors) that are remarkably stable. The voltage-driven net salt flux and accumulation, that underpin the concentration polarisation, surprisingly combine into a diffusionlike quadratic dependence of the memory retention time on the channel length, allowing channel design for a specific timescale. We implement our device as a synaptic element for neuromorphic reservoir computing. Individual channels distinguish various time series, that together represent (handwritten) numbers, for subsequent in-silico classification with a simple readout function. Our results represent a significant step towards realising the promise of fluidic ion channels as a platform to emulate the rich aqueous dynamics of the brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.11438v2</guid>
      <category>cond-mat.soft</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1073/pnas.2320242121</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the National Academy of Sciences (2024), Vol 121, Issue 18</arxiv:journal_reference>
      <dc:creator>T. M. Kamsma, J. Kim, K. Kim, W. Q. Boon, C. Spitoni, J. Park, R. van Roij</dc:creator>
    </item>
  </channel>
</rss>
