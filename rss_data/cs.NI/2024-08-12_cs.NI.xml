<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 12 Aug 2024 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 12 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Redefining Accountability: Navigating Legal Challenges of Participant Liability in Decentralized Autonomous Organizations</title>
      <link>https://arxiv.org/abs/2408.04717</link>
      <description>arXiv:2408.04717v1 Announce Type: new 
Abstract: In the digital era, where innovative technologies like blockchain are revolutionizing traditional organizational paradigms, Decentralized Autonomous Organizations (DAOs) emerge as avant-garde models of collective governance. However, their unique structure challenges existing legal frameworks, especially concerning the liability of participants. This study focuses on analyzing the legal implications of the decentralized nature of DAOs, with a particular emphasis on the aspects of participant liability. Such considerations are essential for understanding how current legal systems might be adapted or reformed to effectively address these novel challenges.
  The paper examines the specificity of DAOs, highlighting their decentralized governance structure and reliance on smart contracts, which introduce unique issues related to the blurring of liability boundaries. It underscores how the anonymity of DAO participants and the automatic execution of smart contracts complicate the traditional concept of legal liability, both within the DAO context and in interactions with external parties.
  The analysis also includes a comparison between DAOs and traditional organizational forms, such as corporations and associations, to identify potential analogies and differences in participant liability. It explores how existing regulations on partner liability might be insufficient or inapplicable in the DAO context, prompting the search for new, innovative legal solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04717v1</guid>
      <category>cs.NI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aneta Napieralska, Przemys{\l}aw K\k{e}pczy\'nski</dc:creator>
    </item>
    <item>
      <title>Next-Generation Wi-Fi Networks with Generative AI: Design and Insights</title>
      <link>https://arxiv.org/abs/2408.04835</link>
      <description>arXiv:2408.04835v1 Announce Type: new 
Abstract: Generative artificial intelligence (GAI), known for its powerful capabilities in image and text processing, also holds significant promise for the design and performance enhancement of future wireless networks. In this article, we explore the transformative potential of GAI in next-generation Wi-Fi networks, exploiting its advanced capabilities to address key challenges and improve overall network performance. We begin by reviewing the development of major Wi-Fi generations and illustrating the challenges that future Wi-Fi networks may encounter. We then introduce typical GAI models and detail their potential capabilities in Wi-Fi network optimization, performance enhancement, and other applications. Furthermore, we present a case study wherein we propose a retrieval-augmented LLM (RA-LLM)-enabled Wi-Fi design framework that aids in problem formulation, which is subsequently solved using a generative diffusion model (GDM)-based deep reinforcement learning (DRL) framework to optimize various network parameters. Numerical results demonstrate the effectiveness of our proposed algorithm in high-density deployment scenarios. Finally, we provide some potential future research directions for GAI-assisted Wi-Fi networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04835v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyu Wang, Xuming Fang, Dusit Niyato, Tie Liu</dc:creator>
    </item>
    <item>
      <title>Energy performance of LR-FHSS: analysis and evaluation</title>
      <link>https://arxiv.org/abs/2408.04908</link>
      <description>arXiv:2408.04908v1 Announce Type: new 
Abstract: Long Range-Frequency Hopping Spread Spectrum (LR-FHSS) is a pivotal advancement in the LoRaWAN protocol, designed to enhance the network's capacity and robustness, particularly in densely populated environments. Although energy consumption is paramount in LoRaWAN-based end-devices, there are currently no studies in the literature, to our knowledge, that model the impact of this novel mechanism on energy consumption. In this article, we provide a comprehensive energy consumption analytical model of LR-FHSS, focusing on three critical metrics: average current consumption, battery lifetime, and energy efficiency of data transmission. The model is based on measurements performed on real hardware in a fully operational LR-FHSS network. While in our evaluation, LR-FHSS can show worse consumption figures than LoRa, we found that with optimal configuration, the battery lifetime of LR-FHSS end-devices can reach 2.5 years for a 50-minute notification period. For the most energy-efficient payload size, this lifespan can be extended to a theoretical maximum of up to 16 years with a one-day notification interval using a cell-coin battery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04908v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roger Sanchez-Vital, Llu\'is Casals, Bartomeu Heer-Salva, Rafael Vidal, Carles Gomez, Eduard Garcia-Villegas</dc:creator>
    </item>
    <item>
      <title>Large Models for Aerial Edges: An Edge-Cloud Model Evolution and Communication Paradigm</title>
      <link>https://arxiv.org/abs/2408.04927</link>
      <description>arXiv:2408.04927v1 Announce Type: new 
Abstract: The future sixth-generation (6G) of wireless networks is expected to surpass its predecessors by offering ubiquitous coverage through integrated air-ground facility deployments in both communication and computing domains. In this network, aerial facilities, such as unmanned aerial vehicles (UAVs), conduct artificial intelligence (AI) computations based on multi-modal data to support diverse applications including surveillance and environment construction. However, these multi-domain inference and content generation tasks require large AI models, demanding powerful computing capabilities, thus posing significant challenges for UAVs. To tackle this problem, we propose an integrated edge-cloud model evolution framework, where UAVs serve as edge nodes for data collection and edge model computation. Through wireless channels, UAVs collaborate with ground cloud servers, providing cloud model computation and model updating for edge UAVs. With limited wireless communication bandwidth, the proposed framework faces the challenge of information exchange scheduling between the edge UAVs and the cloud server. To tackle this, we present joint task allocation, transmission resource allocation, transmission data quantization design, and edge model update design to enhance the inference accuracy of the integrated air-ground edge-cloud model evolution framework by mean average precision (mAP) maximization. A closed-form lower bound on the mAP of the proposed framework is derived, and the solution to the mAP maximization problem is optimized accordingly. Simulations, based on results from vision-based classification experiments, consistently demonstrate that the mAP of the proposed framework outperforms both a centralized cloud model framework and a distributed edge model framework across various communication bandwidths and data sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04927v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Shuhang Zhang, Qingyu Liu, Ke Chen, Boya Di, Hongliang Zhang, Wenhan Yang, Dusit Niyato, Zhu Han, H. Vincent Poor</dc:creator>
    </item>
    <item>
      <title>Overlay-based Decentralized Federated Learning in Bandwidth-limited Networks</title>
      <link>https://arxiv.org/abs/2408.04705</link>
      <description>arXiv:2408.04705v1 Announce Type: cross 
Abstract: The emerging machine learning paradigm of decentralized federated learning (DFL) has the promise of greatly boosting the deployment of artificial intelligence (AI) by directly learning across distributed agents without centralized coordination. Despite significant efforts on improving the communication efficiency of DFL, most existing solutions were based on the simplistic assumption that neighboring agents are physically adjacent in the underlying communication network, which fails to correctly capture the communication cost when learning over a general bandwidth-limited network, as encountered in many edge networks. In this work, we address this gap by leveraging recent advances in network tomography to jointly design the communication demands and the communication schedule for overlay-based DFL in bandwidth-limited networks without requiring explicit cooperation from the underlying network. By carefully analyzing the structure of our problem, we decompose it into a series of optimization problems that can each be solved efficiently, to collectively minimize the total training time. Extensive data-driven simulations show that our solution can significantly accelerate DFL in comparison with state-of-the-art designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04705v1</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yudi Huang, Tingyang Sun, Ting He</dc:creator>
    </item>
    <item>
      <title>On the Ground and in the Sky: A Tutorial on Radio Localization in Ground-Air-Space Networks</title>
      <link>https://arxiv.org/abs/2312.05704</link>
      <description>arXiv:2312.05704v3 Announce Type: replace 
Abstract: The inherent limitations in scaling up ground infrastructure for future wireless networks, combined with decreasing operational costs of aerial and space networks, are driving considerable research interest in multisegment ground-air-space (GAS) networks. In GAS networks, where ground and aerial users share network resources, ubiquitous and accurate user localization becomes indispensable, not only as an end-user service but also as an enabler for location-aware communications. This breaks the convention of having localization as a byproduct in networks primarily designed for communications. To address these imperative localization needs, the design and utilization of ground, aerial, and space anchors require thorough investigation. In this tutorial, we provide an in-depth systemic analysis of the radio localization problem in GAS networks, considering ground and aerial users as targets to be localized. Starting from a survey of the most relevant works, we then define the key characteristics of anchors and targets in GAS networks. Subsequently, we detail localization fundamentals in GAS networks, considering 3D positions, orientations, and velocities. Afterward, we thoroughly analyze radio localization systems in GAS networks, detailing the system model, design aspects, and considerations for each of the three GAS anchors. Preliminary results are presented to provide a quantifiable perspective on key design aspects in GAS-based localization scenarios. We then identify the vital roles 6G enablers are expected to play in radio localization in GAS networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05704v3</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/COMST.2024.3417336</arxiv:DOI>
      <dc:creator>Hazem Sallouha, Sharief Saleh, Sibren De Bast, Zhuangzhuang Cui, Sofie Pollin, Henk Wymeersch</dc:creator>
    </item>
    <item>
      <title>Congestion or No Congestion: Packet Loss Identification and Prediction Using Machine Learning</title>
      <link>https://arxiv.org/abs/2408.03007</link>
      <description>arXiv:2408.03007v3 Announce Type: replace 
Abstract: Packet losses in the network significantly impact network performance. Most TCP variants reduce the transmission rate when detecting packet losses, assuming network congestion, resulting in lower throughput and affecting bandwidth-intensive applications like immersive applications. However, not all packet losses are due to congestion; some occur due to wireless link issues, which we refer to as non-congestive packet losses. In today's hybrid Internet, packets of a single flow may traverse wired and wireless segments of a network to reach their destination. TCP should not react to non-congestive packet losses the same way as it does to congestive losses. However, TCP currently can not differentiate between these types of packet losses and lowers its transmission rate irrespective of packet loss type, resulting in lower throughput for wireless clients. To address this challenge, we use machine learning techniques to distinguish between these types of packet losses at end hosts, utilizing easily available features at the host. Our results demonstrate that Random Forest and K-Nearest Neighbor classifiers perform better in predicting the type of packet loss, offering a promising solution to enhance network performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03007v3</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Inayat Ali, Seungwoo Hong, Taesik Cheung</dc:creator>
    </item>
  </channel>
</rss>
