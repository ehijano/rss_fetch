<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 19 May 2025 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Decentralized Min-Max Optimization with Gradient Tracking</title>
      <link>https://arxiv.org/abs/2505.10631</link>
      <description>arXiv:2505.10631v1 Announce Type: new 
Abstract: This paper presents a novel distributed formulation of the min-max optimization problem. Such a formulation enables enhanced flexibility among agents when optimizing their maximization variables. To address the problem, we propose two distributed gradient methods over networks, termed Distributed Gradient Tracking Ascent (DGTA) and Distributed Stochastic Gradient Tracking Ascent (DSGTA). We demonstrate that DGTA achieves an iteration complexity of $\mathcal{O}(\kappa^2\varepsilon^{-2})$, and DSGTA attains a sample complexity of $\mathcal{O}(\kappa^3\varepsilon^{-4})$ for nonconvex strongly concave (NC-SC) objective functions. Both results match those of their centralized counterparts up to constant factors related to the communication network. Numerical experiments further demonstrate the superior empirical performance of the proposed algorithms compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10631v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Runze You, Kun Huang, Shi Pu</dc:creator>
    </item>
    <item>
      <title>Barrier relaxations of the classical and quantum optimal transport problems</title>
      <link>https://arxiv.org/abs/2505.10667</link>
      <description>arXiv:2505.10667v1 Announce Type: new 
Abstract: In the last fifteen years a significant progress was achieved by considering an entropic relaxation of the classical multi-partite optimal transport problem (MPOTP). The entropic relaxation gives rise to the rescaling problem of a given tensor. This rescaling can be achieved fast with the Sinkhorn type algorithms. Recently, it was shown that a similar approach works for the quantum MPOTP. However, the analog of the rescaling Sinkhorn algorithm is much more complicated than in the classical MPOTP. In this paper we show that the interior point method (IPM) for the primary and dual problems of classical and quantum MPOTP problems can be considered as barrier relaxations of the optimal transport problems (OTP). It is well known that the dual of the OTP are advantageous as it has much less variables than the primary problem. The IPM for the dual problem of the classical MPOTP are not as fast as the Sinkhorn type algorithm. However, IPM method for the dual of the quantum MPOTP seems to work quite efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10667v1</guid>
      <category>math.OC</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shmuel Friedland</dc:creator>
    </item>
    <item>
      <title>Variational structure of Fokker-Planck equations with variable mobility</title>
      <link>https://arxiv.org/abs/2505.10676</link>
      <description>arXiv:2505.10676v1 Announce Type: new 
Abstract: We study Fokker--Planck equations with symmetric, positive definite mobility matrices capturing diffusion in heterogeneous environments. A weighted Wasserstein metric is introduced for which these equations are gradient flows. This metric is shown to emerge from an optimal control problem in the space of probability densities for a class of variable mobility matrices, with the cost function capturing the work dissipated via friction. Using the Nash-Kuiper isometric embedding theorem for Riemannian manifolds, we demonstrate the existence of optimal transport maps. Additionally, we construct a time-discrete variational scheme, establish key properties for the associated minimizing problem, and prove convergence to weak solutions of the associated Fokker-Planck equation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10676v1</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hailiang Liu, Athanasios E. Tzavaras</dc:creator>
    </item>
    <item>
      <title>Consensus of A Class of Nonlinear Systems with Varying Topology: A Hilbert Metric Approach</title>
      <link>https://arxiv.org/abs/2505.10795</link>
      <description>arXiv:2505.10795v1 Announce Type: new 
Abstract: In this technical note, we introduce a novel approach to studying consensus of continuous-time nonlinear systems with varying topology based on Hilbert metric. We demonstrate that this metric offers significant flexibility in analyzing consensus properties, while effectively handling nonlinearities and time dependencies. Notably, our approach relaxes key technical assumptions from some standard results while yielding stronger conclusions with shorter proofs. This framework provides new insights into nonlinear consensus under varying topology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10795v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TAC.2025.3570225</arxiv:DOI>
      <dc:creator>Dongjun Wu</dc:creator>
    </item>
    <item>
      <title>Contractive difference-of-convex algorithms</title>
      <link>https://arxiv.org/abs/2505.10800</link>
      <description>arXiv:2505.10800v1 Announce Type: new 
Abstract: The difference-of-convex algorithm (DCA) and its variants are the most popular methods to solve the difference-of-convex optimization problem. Each iteration of them is reduced to a convex optimization problem, which generally needs to be solved by iterative methods such as proximal gradient algorithm. However, these algorithms essentially belong to some iterative methods of fixed point problems of averaged mappings, and their convergence speed is generally slow. Furthermore, there is seldom research on the termination rule of these iterative algorithms solving the subproblem of DCA. To overcome these defects, we ffrstly show that the subproblem of the linearized proximal method (LPM) in each iteration is equal to the ffxed point problem of a contraction. Secondly, by using Picard iteration to approximately solve the subproblem of LPM in each iteration, we propose a contractive difference-ofconvex algorithm (cDCA) where an adaptive termination rule is presented. Both global subsequential convergence and global convergence of the whole sequence of cDCA are established. Finally, preliminary results from numerical experiments are promising.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10800v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Songnian He, Qiao-Li Dong, Michael Th. Rassias</dc:creator>
    </item>
    <item>
      <title>Convergence analysis of the Halpern iteration with adaptive anchoring parameters</title>
      <link>https://arxiv.org/abs/2505.10807</link>
      <description>arXiv:2505.10807v1 Announce Type: new 
Abstract: We propose an adaptive way to choose the anchoring parameters for the Halpern iteration to find a fixed point of a nonexpansive mapping in a real Hilbert space. We prove strong convergence of this adaptive Halpern iteration and obtain the rate of asymptotic regularity at least O(1/k), where k is the number of iterations. Numerical experiments are also provided to show advantages and outperformance of our adaptive Halpern algorithm over the standard Halpern algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10807v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Songnian He, Hong-Kun Xu, Qiao-Li Dong, Na Mei</dc:creator>
    </item>
    <item>
      <title>Optimal $\mathbb{H}_2$ Control with Passivity-Constrained Feedback: Convex Approach</title>
      <link>https://arxiv.org/abs/2505.10811</link>
      <description>arXiv:2505.10811v1 Announce Type: new 
Abstract: We consider the $\mathbb{H}_2$-optimal feedback control problem, for the case in which the plant is passive with bounded $\mathbb{L}_2$ gain, and the feedback law is constrained to be output-strictly passive. In this circumstance, we show that this problem distills to a convex optimal control problem, in which the optimization domain is the associated Youla parameter for the closed-loop system. This enables the globally-optimal controller to be solved as an infinite-dimensional but convex optimization. Near-optimal solutions may be found through the finite-dimensional convex truncation of this infinite-dimensional domain. The idea is demonstrated on a simple vibration suppression example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10811v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. T. Scruggs</dc:creator>
    </item>
    <item>
      <title>A Discretization Approach for Bilevel Optimization with Low-Dimensional and Non-Convex Lower-Level</title>
      <link>https://arxiv.org/abs/2505.10830</link>
      <description>arXiv:2505.10830v1 Announce Type: new 
Abstract: Bilevel optimization (BLO) problem, where two optimization problems (referred to as upper- and lower-level problems) are coupled hierarchically, has wide applications in areas such as machine learning and operations research. Recently, many first-order algorithms have been developed for solving bilevel problems with strongly convex and/or unconstrained lower-level problems; this special structure of the lower-level problem is needed to ensure the tractability of gradient computation (among other reasons). In this work, we deal with a class of more challenging BLO problems where the lower-level problem is non-convex and constrained. We propose a novel approach that approximates the value function of the lower-level problem by first sampling a set of feasible solutions and then constructing an equivalent convex optimization problem. This convexified value function is then used to construct a penalty function for the original BLO problem. We analyze the properties of the original BLO problem and the newly constructed penalized problem by characterizing the relation between their KKT points, as well as the local and global minima of the two problems. We then develop a gradient descent-based algorithm to solve the reformulated problem, and establish its finite-time convergence guarantees. Finally, we conduct numerical experiments to corroborate the theoretical performance of the proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10830v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaotian Jiang, Ioannis Tsaknakis, Prashant Khanduri, Mingyi Hong</dc:creator>
    </item>
    <item>
      <title>Convergence Analysis of the Last Iterate in Distributed Stochastic Gradient Descent with Momentum</title>
      <link>https://arxiv.org/abs/2505.10889</link>
      <description>arXiv:2505.10889v1 Announce Type: new 
Abstract: Distributed stochastic gradient methods are widely used to preserve data privacy and ensure scalability in large-scale learning tasks. While existing theory on distributed momentum Stochastic Gradient Descent (mSGD) mainly focuses on time-averaged convergence, the more practical last-iterate convergence remains underexplored. In this work, we analyze the last-iterate convergence behavior of distributed mSGD in non-convex settings under the classical Robbins-Monro step-size schedule. We prove both almost sure convergence and $L_2$ convergence of the last iterate, and derive convergence rates. We further show that momentum can accelerate early-stage convergence, and provide experiments to support our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10889v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Difei Cheng, Ruinan Jin, Hong Qiao, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>A Scalable Procedure for $\mathcal{H}_{\infty}-$Control Design</title>
      <link>https://arxiv.org/abs/2505.10979</link>
      <description>arXiv:2505.10979v1 Announce Type: new 
Abstract: This paper proposes a novel gradient based scalable procedure for $\mathcal{H}_{\infty}-$control design. We compute the gradient using algebraic Riccati equation and then couple it with a novel Armijo rule inspired step-size selection procedure. We perform numerical experiments of the proposed solution procedure on an exhaustive list of benchmark engineering systems to show its convergence properties. Finally we compare our proposed solution procedure with available semi-definite programming based gradient-descent algorithm to demonstrate its scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10979v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amit Kumar (Department of Electronics and Communication Engineering, Indraprastha Institute of Information Technology, New Delhi, India), Prasad Vilas Chanekar (Department of Electronics and Communication Engineering, Indraprastha Institute of Information Technology, New Delhi, India)</dc:creator>
    </item>
    <item>
      <title>A Superlinearly Convergent Evolution Strategy</title>
      <link>https://arxiv.org/abs/2505.10987</link>
      <description>arXiv:2505.10987v1 Announce Type: new 
Abstract: We present a hybrid algorithm between an evolution strategy and a quasi Newton method. The design is based on the Hessian Estimation Evolution Strategy, which iteratively estimates the inverse square root of the Hessian matrix of the problem. This is akin to a quasi-Newton method and corresponding derivative-free trust-region algorithms like NEWUOA. The proposed method therefore replaces the global recombination step commonly found in non-elitist evolution strategies with a quasi-Newton step. Numerical results show superlinear convergence, resulting in improved performance in particular on smooth convex problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10987v1</guid>
      <category>math.OC</category>
      <category>cs.NE</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tobias Glasmachers</dc:creator>
    </item>
    <item>
      <title>Multilevel Optimization: Geometric Coarse Models and Convergence Analysis</title>
      <link>https://arxiv.org/abs/2505.11104</link>
      <description>arXiv:2505.11104v1 Announce Type: new 
Abstract: We study multilevel techniques, commonly used in PDE multigrid literature, to solve structured optimization problems. For a given hierarchy of levels, we formulate a coarse model that approximates the problem at each level and provides a descent direction for the fine-grid objective using fewer variables. Unlike common algebraic approaches, we assume the objective function and its gradient can be evaluated at each level. Under the assumptions of strong convexity and gradient L-smoothness, we analyze convergence and extend the method to box-constrained optimization. Large-scale numerical experiments on a discrete tomography problem show that the multilevel approach converges rapidly when far from the solution and performs competitively with state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11104v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ferdinand Vanmaele, Yara Elshiaty, Stefania Petra</dc:creator>
    </item>
    <item>
      <title>Linear Convergence of the Frank-Wolfe Algorithm over Product Polytopes</title>
      <link>https://arxiv.org/abs/2505.11259</link>
      <description>arXiv:2505.11259v1 Announce Type: new 
Abstract: We study the linear convergence of Frank-Wolfe algorithms over product polytopes. We analyze two condition numbers for the product polytope, namely the \emph{pyramidal width} and the \emph{vertex-facet distance}, based on the condition numbers of individual polytope components. As a result, for convex objectives that are $\mu$-Polyak-{\L}ojasiewicz, we show linear convergence rates quantified in terms of the resulting condition numbers. We apply our results to the problem of approximately finding a feasible point in a polytope intersection in high-dimensions, and demonstrate the practical efficiency of our algorithms through empirical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11259v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriele Iommazzo, David Mart\'inez-Rubio, Francisco Criado, Elias Wirth, Sebastian Pokutta</dc:creator>
    </item>
    <item>
      <title>Bilevel Transmission Expansion Planning with Joint Chance-Constrained Dispatch</title>
      <link>https://arxiv.org/abs/2505.11273</link>
      <description>arXiv:2505.11273v1 Announce Type: new 
Abstract: In transmission expansion planning (TEP), network planners make long-term investment decisions while anticipating market clearing outcomes that are increasingly affected by renewable generation uncertainty. Additionally, market participants' sensitivity to network charges and the requirement for cost recovery by the network planner introduce further complexity. Since the day-ahead market clears before uncertainty realizes, explicitly modelling these uncertainties at the lower-level market clearing becomes important in bilevel TEP problems. In this paper, we introduce a novel bilevel TEP framework with lower-level joint chance-constrained market clearing that manages line flow constraints under wind uncertainty and accounts for the effect of network tariffs on participants' actual marginal costs and utility. To solve this complex problem, we propose a Strengthened Linear Approximation (SLA) technique for handling Wasserstein distributionally robust joint chance constraints with right-hand-side uncertainties (RHS-WDRJCC). The proposed method offers more efficient approximations without additional conservativeness and avoids the numerical issues encountered in existing approaches by introducing valid inequalities. The case study demonstrates that the proposed model achieves the desired out-of-sample constraint satisfaction probability. Moreover, the numerical results highlight the significant computational advantage of SLA, achieving up to a 26x speedup compared to existing methods such as worst-case conditional value-at-risk, while maintaining high solution quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11273v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuxin Xia, Yihong Zhou, Iacopo Savelli, Thomas Morstyn</dc:creator>
    </item>
    <item>
      <title>Revisiting Stochastic Approximation and Stochastic Gradient Descent</title>
      <link>https://arxiv.org/abs/2505.11343</link>
      <description>arXiv:2505.11343v1 Announce Type: new 
Abstract: In this paper, we take a fresh look at stochastic approximation (SA) and Stochastic Gradient Descent (SGD). We derive new sufficient conditions for the convergence of SA. In particular, the "noise" or measurement error need not have a finite second moment, and under suitable conditions, not even a finite mean. By adapting this method of proof, we also derive sufficient conditions for the convergence of zero-order SGD, wherein the stochastic gradient is computed using only two function evaluations, and no gradient computations. The sufficient conditions derived here are the weakest to date, thus leading to a considerable expansion of the applicability of SA and SGD theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11343v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajeeva Laxman Karandikar, Bhamidi Visweswara Rao, Mathukumalli Vidyasagar</dc:creator>
    </item>
    <item>
      <title>Long-Term Average Impulse Control with Mean Field Interactions</title>
      <link>https://arxiv.org/abs/2505.11345</link>
      <description>arXiv:2505.11345v1 Announce Type: new 
Abstract: This paper analyzes and provides explicit solutions for a long-term average impulse control problem with a specific mean-field interaction. The underlying process is a general one-dimensional diffusion with appropriate boundary behavior. The model is motivated by applications such the optimal long-term management of renewable natural resources and financial portfolio management. Each individual agent seeks to maximize the long-term average reward, which consists of a running reward and incomes from discrete impulses, where the unit intervention price depends on the market through a stationary supply rate. In a competitive market setting, we establish the existence of and explicitly characterize an equilibrium strategy within a large class of policies under mild conditions. Additionally, we formulate and solve the mean field control problem, in which agents cooperate with each other, aiming to realize a common maximal long-term average profit. To illustrate the theoretical results, we examine a stochastic logistic growth model and a population growth model in a stochastic environment with impulse control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11345v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>K. L. Helmes, R. H. Stockbridge, C. Zhu</dc:creator>
    </item>
    <item>
      <title>Decoupling Collision Avoidance in and for Optimal Control using Least-Squares Support Vector Machines</title>
      <link>https://arxiv.org/abs/2505.11376</link>
      <description>arXiv:2505.11376v1 Announce Type: new 
Abstract: This paper details an approach to linearise differentiable but non-convex collision avoidance constraints tailored to convex shapes. It revisits introducing differential collision avoidance constraints for convex objects into an optimal control problem (OCP) using the separating hyperplane theorem. By framing this theorem as a classification problem, the hyperplanes are eliminated as optimisation variables from the OCP. This effectively transforms non-convex constraints into linear constraints. A bi-level algorithm computes the hyperplanes between the iterations of an optimisation solver and subsequently embeds them as parameters into the OCP. Experiments demonstrate the approach's favourable scalability towards cluttered environments and its applicability to various motion planning approaches. It decreases trajectory computation times between 50\% and 90\% compared to a state-of-the-art approach that directly includes the hyperplanes as variables in the optimal control problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11376v1</guid>
      <category>math.OC</category>
      <category>cs.RO</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dries Dirckx, Wilm Decr\'e, Jan Swevers</dc:creator>
    </item>
    <item>
      <title>Controlling the Flow: Stability and Convergence for Stochastic Gradient Descent with Decaying Regularization</title>
      <link>https://arxiv.org/abs/2505.11434</link>
      <description>arXiv:2505.11434v1 Announce Type: new 
Abstract: The present article studies the minimization of convex, L-smooth functions defined on a separable real Hilbert space. We analyze regularized stochastic gradient descent (reg-SGD), a variant of stochastic gradient descent that uses a Tikhonov regularization with time-dependent, vanishing regularization parameter. We prove strong convergence of reg-SGD to the minimum-norm solution of the original problem without additional boundedness assumptions. Moreover, we quantify the rate of convergence and optimize the interplay between step-sizes and regularization decay. Our analysis reveals how vanishing Tikhonov regularization controls the flow of SGD and yields stable learning dynamics, offering new insights into the design of iterative algorithms for convex problems, including those that arise in ill-posed inverse problems. We validate our theoretical findings through numerical experiments on image reconstruction and ODE-based inverse problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11434v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Kassing, Simon Weissmann, Leif D\"oring</dc:creator>
    </item>
    <item>
      <title>Quantum thermodynamics and semi-definite optimization</title>
      <link>https://arxiv.org/abs/2505.04514</link>
      <description>arXiv:2505.04514v2 Announce Type: cross 
Abstract: In quantum thermodynamics, a system is described by a Hamiltonian and a list of non-commuting charges representing conserved quantities like particle number or electric charge, and an important goal is to determine the system's minimum energy in the presence of these conserved charges. In optimization theory, a semi-definite program (SDP) involves a linear objective function optimized over the cone of positive semi-definite operators intersected with an affine space. These problems arise from differing motivations in the physics and optimization communities and are phrased using very different terminology, yet they are essentially identical mathematically. By adopting Jaynes' mindset motivated by quantum thermodynamics, we observe that minimizing free energy in the aforementioned thermodynamics problem, instead of energy, leads to an elegant solution in terms of a dual chemical potential maximization problem that is concave in the chemical potential parameters. As such, one can employ standard (stochastic) gradient ascent methods to find the optimal values of these parameters, and these methods are guaranteed to converge quickly. At low temperature, the minimum free energy provides an excellent approximation for the minimum energy. We then show how this Jaynes-inspired gradient-ascent approach can be used in both first- and second-order classical and hybrid quantum-classical algorithms for minimizing energy, and equivalently, how it can be used for solving SDPs, with guarantees on the runtimes of the algorithms. The approach discussed here is well grounded in quantum thermodynamics and, as such, provides physical motivation underpinning why algorithms published fifty years after Jaynes' seminal work, including the matrix multiplicative weights update method, the matrix exponentiated gradient update method, and their quantum algorithmic generalizations, perform well at solving SDPs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04514v2</guid>
      <category>quant-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nana Liu, Michele Minervini, Dhrumil Patel, Mark M. Wilde</dc:creator>
    </item>
    <item>
      <title>Prefix-bounded matrices</title>
      <link>https://arxiv.org/abs/2505.10739</link>
      <description>arXiv:2505.10739v1 Announce Type: cross 
Abstract: By unifying various earlier extensions of alternating sign matrices (ASMs), we introduce the notion of prefix-bounded matrices (PBMs). It is shown that the convex hull of these matrices forms the intersection of two special (called laminar) g-polymatroids. This implies (in a more general form) that the linear inequality system given by Behrend and Knight and by Striker for describing the polytope of alternating sign matrices is TDI, confirming a recent conjecture of Edmonds. By relying on the polymatroidal approach, we derive a characterization for the existence of prefix-bounded matrices meeting upper and lower bound requirements on their entries.
  Furthermore, we point out that the constraint matrix of the linear system describing the convex hull of PBMs (in particular, ASMs) is a network matrix. This implies that (a) standard network flow techniques can be used to manage algorithmically optimization and structural results on PBMs obtained via g-polymatroids, (b) the linear system is actually box-TDI, and (c) the convex hull of PBMs has (a sharpened form of) the integer Carath\'eodory property, in particular, the integer decomposition property. This latter feature makes it possible to confirm (in an extended form) an elegant conjecture of Brualdi and Dahl on the decomposability of a so-called $k$-regular alternating sign matrix as the sum of $k$ pattern-disjoint ASMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10739v1</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>N\'ora A. Borsik, Andr\'as Frank, P\'eter Madarasi, Tam\'as Tak\'acs</dc:creator>
    </item>
    <item>
      <title>Asymptotics of Constrained Quantization for Compactly Supported Measures</title>
      <link>https://arxiv.org/abs/2505.10801</link>
      <description>arXiv:2505.10801v1 Announce Type: cross 
Abstract: We investigated the asymptotics of high-rate constrained quantization errors for a compactly supported probability measure P on Euclidean spaces whose quantizers are confined to a closed set S. The key tool is the metric projection of K onto S that assigns each source point to its nearest neighbor in S, allowing the errors to be transferred to the projection, where K = supp P. For the upper estimate, we establish a projection pull-back inequality that bounds the errors by the classical covering radius of the projection. For the lower estimate, a weighted distance function enables us to perturb any quantizer element lying on the projection slightly into the complement in S without enlarging the error, provided the projection is nowhere dense (automatically true when S and K are disjoint). Under mild conditions on the pushforward measure of P by T, obtained via a measurable selector T, we derive a uniform lower bound. If this set is Ahlfors regular of dimension d, the error decays like the reciprocal of the d-th root of n and every constrained quantization dimension equals d. The two estimates coincide, giving the first complete dimension comparison formula for constrained quantization and closing the gap left by earlier self-similar examples by Pandey-Roychowdhury while extending classical unconstrained theory to closed constraints under mild geometric assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10801v1</guid>
      <category>math.MG</category>
      <category>math.CA</category>
      <category>math.FA</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenxing Qian</dc:creator>
    </item>
    <item>
      <title>Comparative Analysis of Black-Box Optimization Methods for Weather Intervention Design</title>
      <link>https://arxiv.org/abs/2505.10843</link>
      <description>arXiv:2505.10843v1 Announce Type: cross 
Abstract: As climate change increases the threat of weather-related disasters, research on weather control is gaining importance. The objective of weather control is to mitigate disaster risks by administering interventions with optimal timing, location, and intensity. However, the optimization process is highly challenging due to the vast scale and complexity of weather phenomena, which introduces two major challenges. First, obtaining accurate gradient information for optimization is difficult. In addition, numerical weather prediction (NWP) models demand enormous computational resources, necessitating parameter optimization with minimal function evaluations. To address these challenges, this study proposes a method for designing weather interventions based on black-box optimization, which enables efficient exploration without requiring gradient information. The proposed method is evaluated in two distinct control scenarios: one-shot initial value intervention and sequential intervention based on model predictive control. Furthermore, a comparative analysis is conducted among four representative black-box optimization methods in terms of total rainfall reduction. Experimental results show that Bayesian optimization achieves higher control effectiveness than the others, particularly in high-dimensional search spaces. These findings suggest that Bayesian optimization is a highly effective approach for weather intervention computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10843v1</guid>
      <category>physics.ao-ph</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuta Higuchi, Rikuto Nagai, Atsushi Okazaki, Masaki Ogura, Naoki Wakamiya</dc:creator>
    </item>
    <item>
      <title>Certifying Stability of Reinforcement Learning Policies using Generalized Lyapunov Functions</title>
      <link>https://arxiv.org/abs/2505.10947</link>
      <description>arXiv:2505.10947v1 Announce Type: cross 
Abstract: We study the problem of certifying the stability of closed-loop systems under control policies derived from optimal control or reinforcement learning (RL). Classical Lyapunov methods require a strict step-wise decrease in the Lyapunov function but such a certificate is difficult to construct for a learned control policy. The value function associated with an RL policy is a natural Lyapunov function candidate but it is not clear how it should be modified. To gain intuition, we first study the linear quadratic regulator (LQR) problem and make two key observations. First, a Lyapunov function can be obtained from the value function of an LQR policy by augmenting it with a residual term related to the system dynamics and stage cost. Second, the classical Lyapunov decrease requirement can be relaxed to a generalized Lyapunov condition requiring only decrease on average over multiple time steps. Using this intuition, we consider the nonlinear setting and formulate an approach to learn generalized Lyapunov functions by augmenting RL value functions with neural network residual terms. Our approach successfully certifies the stability of RL policies trained on Gymnasium and DeepMind Control benchmarks. We also extend our method to jointly train neural controllers and stability certificates using a multi-step Lyapunov loss, resulting in larger certified inner approximations of the region of attraction compared to the classical Lyapunov approach. Overall, our formulation enables stability certification for a broad class of systems with learned policies by making certificates easier to construct, thereby bridging classical control theory and modern learning-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10947v1</guid>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kehan Long, Jorge Cort\'es, Nikolay Atanasov</dc:creator>
    </item>
    <item>
      <title>Beyond KL-divergence: Risk Aware Control Through Cross Entropy and Adversarial Entropy Regularization</title>
      <link>https://arxiv.org/abs/2505.11068</link>
      <description>arXiv:2505.11068v1 Announce Type: cross 
Abstract: While the idea of robust dynamic programming (DP) is compelling for systems affected by uncertainty, addressing worst-case disturbances generally results in excessive conservatism. This paper introduces a method for constructing control policies robust to adversarial disturbance distributions that relate to a provided empirical distribution. The character of the adversary is shaped by a regularization term comprising a weighted sum of (i) the cross-entropy between the empirical and the adversarial distributions, and (ii) the entropy of the adversarial distribution itself. The regularization weights are interpreted as the likelihood factor and the temperature respectively. The proposed framework leads to an efficient DP-like algorithm -- referred to as the minsoftmax algorithm -- to obtain the optimal control policy, where the disturbances follow an analytical softmax distribution in terms of the empirical distribution, temperature, and likelihood factor. It admits a number of control-theoretic interpretations and can thus be understood as a flexible tool for integrating complementary features of related control frameworks. In particular, in the linear model quadratic cost setting, with a Gaussian empirical distribution, we draw connections to the well-known $\mathcal{H}_{\infty}$-control. We illustrate our results through a numerical example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11068v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Menno van Zutphen, Domagoj Herceg, Duarte J. Antunes</dc:creator>
    </item>
    <item>
      <title>Inexact Column Generation for Bayesian Network Structure Learning via Difference-of-Submodular Optimization</title>
      <link>https://arxiv.org/abs/2505.11089</link>
      <description>arXiv:2505.11089v1 Announce Type: cross 
Abstract: In this paper, we consider a score-based Integer Programming (IP) approach for solving the Bayesian Network Structure Learning (BNSL) problem. State-of-the-art BNSL IP formulations suffer from the exponentially large number of variables and constraints. A standard approach in IP to address such challenges is to employ row and column generation techniques, which dynamically generate rows and columns, while the complex pricing problem remains a computational bottleneck for BNSL. For the general class of $\ell_0$-penalized likelihood scores, we show how the pricing problem can be reformulated as a difference of submodular optimization problem, and how the Difference of Convex Algorithm (DCA) can be applied as an inexact method to efficiently solve the pricing problems. Empirically, we show that, for continuous Gaussian data, our row and column generation approach yields solutions with higher quality than state-of-the-art score-based approaches, especially when the graph density increases, and achieves comparable performance against benchmark constraint-based and hybrid approaches, even when the graph size increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11089v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiran Yang, Rui Chen</dc:creator>
    </item>
    <item>
      <title>Modeling Cell Dynamics and Interactions with Unbalanced Mean Field Schr\"odinger Bridge</title>
      <link>https://arxiv.org/abs/2505.11197</link>
      <description>arXiv:2505.11197v1 Announce Type: cross 
Abstract: Modeling the dynamics from sparsely time-resolved snapshot data is crucial for understanding complex cellular processes and behavior. Existing methods leverage optimal transport, Schr\"odinger bridge theory, or their variants to simultaneously infer stochastic, unbalanced dynamics from snapshot data. However, these approaches remain limited in their ability to account for cell-cell interactions. This integration is essential in real-world scenarios since intercellular communications are fundamental life processes and can influence cell state-transition dynamics. To address this challenge, we formulate the Unbalanced Mean-Field Schr\"odinger Bridge (UMFSB) framework to model unbalanced stochastic interaction dynamics from snapshot data. Inspired by this framework, we further propose CytoBridge, a deep learning algorithm designed to approximate the UMFSB problem. By explicitly modeling cellular transitions, proliferation, and interactions through neural networks, CytoBridge offers the flexibility to learn these processes directly from data. The effectiveness of our method has been extensively validated using both synthetic gene regulatory data and real scRNA-seq datasets. Compared to existing methods, CytoBridge identifies growth, transition, and interaction patterns, eliminates false transitions, and reconstructs the developmental landscape with greater accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11197v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhenyi Zhang, Zihan Wang, Yuhao Sun, Tiejun Li, Peijie Zhou</dc:creator>
    </item>
    <item>
      <title>Strictly abnormal geodesics with a degeneracy point in the interior of their domain</title>
      <link>https://arxiv.org/abs/2505.11284</link>
      <description>arXiv:2505.11284v1 Announce Type: cross 
Abstract: In this article, we study abnormal curves in a family of sub-Riemannian manifolds of rank 2. We focus on abnormal curves whose lifts to the cotangent bundle annihilate, at an interior point of the domain, all Lie brackets of length up to three of vector fields tangent to the distribution. We present a method to prove that such curves are length-minimizing. Finally, we prove that strictly abnormal geodesics may cease to be locally length-minimizing after a change of the metric.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11284v1</guid>
      <category>math.DG</category>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicola Paddeu, Alessandro Socionovo</dc:creator>
    </item>
    <item>
      <title>Sobolev Training of End-to-End Optimization Proxies</title>
      <link>https://arxiv.org/abs/2505.11342</link>
      <description>arXiv:2505.11342v1 Announce Type: cross 
Abstract: Optimization proxies - machine learning models trained to approximate the solution mapping of parametric optimization problems in a single forward pass - offer dramatic reductions in inference time compared to traditional iterative solvers. This work investigates the integration of solver sensitivities into such end to end proxies via a Sobolev training paradigm and does so in two distinct settings: (i) fully supervised proxies, where exact solver outputs and sensitivities are available, and (ii) self supervised proxies that rely only on the objective and constraint structure of the underlying optimization problem. By augmenting the standard training loss with directional derivative information extracted from the solver, the proxy aligns both its predicted solutions and local derivatives with those of the optimizer. Under Lipschitz continuity assumptions on the true solution mapping, matching first order sensitivities is shown to yield uniform approximation error proportional to the training set covering radius. Empirically, different impacts are observed in each studied setting. On three large Alternating Current Optimal Power Flow benchmarks, supervised Sobolev training cuts mean squared error by up to 56 percent and the median worst case constraint violation by up to 400 percent while keeping the optimality gap below 0.22 percent. For a mean variance portfolio task trained without labeled solutions, self supervised Sobolev training halves the average optimality gap in the medium risk region (standard deviation above 10 percent of budget) and matches the baseline elsewhere. Together, these results highlight Sobolev training whether supervised or self supervised as a path to fast reliable surrogates for safety critical large scale optimization workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11342v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andrew W. Rosemberg, Joaquim Dias Garcia, Russell Bent, Pascal Van Hentenryck</dc:creator>
    </item>
    <item>
      <title>How a Small Amount of Data Sharing Benefits Distributed Optimization and Learning : The Upside of Data Heterogeneity</title>
      <link>https://arxiv.org/abs/2208.09735</link>
      <description>arXiv:2208.09735v5 Announce Type: replace 
Abstract: Distributed optimization algorithms are widely used in machine learning. This paper investigates how a small amount of data sharing can improve their performance. Focusing on general linear models, we analyze the effects of data sharing on both primal and primal-dual optimization methods. Our contributions are threefold. First, from a theoretical perspective, we show that minimal data sharing improves algorithmic performance by shifting data from less favorable to more favorable structures. Contrary to the common belief that data heterogeneity is always harmful, we prove that while heterogeneity generally slows convergence in primal methods such as FedAvg and distributed PCG, it can accelerate convergence in primal-dual consensus algorithms like distributed ADMM, Fed-ADMM, and EXTRA by enriching dual dynamics. This reveals a form of duality in how heterogeneity affects different algorithm families. Second, building on this insight, we design a meta-algorithm for minimal data sharing, adaptable to both primal and primal-dual methods. We show that with as little as 1 percent shared data, convergence can be significantly accelerated across machine learning tasks. Finally, we argue from a broader perspective that even limited collaboration can yield large synergies, an idea that transcends the optimization context. Our findings provide both theoretical and practical guidance for improving distributed learning through minimal cooperation and motivate further exploration of cross-agent collaboration in solving complex global learning problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.09735v5</guid>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingxi Zhu, Yinyu Ye</dc:creator>
    </item>
    <item>
      <title>Duality of Hoffman constants</title>
      <link>https://arxiv.org/abs/2312.09858</link>
      <description>arXiv:2312.09858v2 Announce Type: replace 
Abstract: Suppose $A\in \mathbb{R}^{m\times n}$, and $R\subseteq \mathbb{R}^n$ and $S\subseteq \mathbb{R}^m$ are {\em reference} polyhedral cones with dual cones $R^*\subseteq \mathbb{R}^n, \; S^*\subseteq \mathbb{R}^m$. We show that a suitable Slater condition implies a {\em duality inequality} between the Hoffman constants of the feasibility problems $$ \begin{array}{r} Ax-b \in S\\ x \in R \end{array} \qquad\text{ and }\qquad \begin{array}{r} c-A\transp y \in R^*\\ y \in S^*. \end{array} $$ As an interesting application, we show a striking identity between the Hoffman constants of {\em box-constrained} feasibility problems with a similar primal-dual format, but where one of the reference sets is a box and the other is a linear subspace. We also establish a surprising identity between Hoffman constants of box-constrained feasibility problems and the chi condition measures for weighted least-squares problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.09858v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Javier F. Pena, Juan C. Vera, Luis F. Zuluaga</dc:creator>
    </item>
    <item>
      <title>Chordal-NMF with Riemannian Multiplicative Update</title>
      <link>https://arxiv.org/abs/2405.12823</link>
      <description>arXiv:2405.12823v3 Announce Type: replace 
Abstract: Nonnegative Matrix Factorization (NMF) is the problem of approximating a given nonnegative matrix M through the product of two nonnegative low-rank matrices W and H. Traditionally NMF is tackled by optimizing a specific objective function evaluating the quality of the approximation. This assessment is often done based on the Frobenius norm (F-norm). In this work, we argue that the F-norm, as the "point-to-point" distance, may not always be appropriate. Viewing from the perspective of cone, NMF may not naturally align with F-norm. So, a ray-to-ray chordal distance is proposed as an alternative way of measuring the quality of the approximation. As this measure corresponds to the Euclidean distance on the sphere, it motivates the use of manifold optimization techniques. We apply Riemannian optimization technique to solve chordal-NMF by casting it on a manifold. Unlike works on Riemannian optimization that require the manifold to be smooth, the nonnegativity in chordal-NMF defines a non-differentiable manifold. We propose a Riemannian Multiplicative Update (RMU), and showcase the effectiveness of the chordal-NMF on synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12823v3</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Flavia Esposito, Andersen Ang</dc:creator>
    </item>
    <item>
      <title>Best-Worst Disaggregation: An approach to the preference disaggregation problem</title>
      <link>https://arxiv.org/abs/2410.12678</link>
      <description>arXiv:2410.12678v2 Announce Type: replace 
Abstract: Preference disaggregation methods in Multi-Criteria Decision-Making (MCDM) often encounter challenges related to inconsistency and cognitive biases when deriving a value function from experts' holistic preferences. This paper introduces the Best-Worst Disaggregation (BWD) method, a novel approach that integrates the principles of the Best-Worst Method (BWM) into the disaggregation framework to enhance the consistency and reliability of derived preference models. BWD employs the "consider-the-opposite" strategy from BWM, allowing experts to provide two opposite pairwise comparison vectors of alternatives. This approach reduces cognitive load and mitigates anchoring bias, possibly leading to more reliable criteria weights and attribute value functions. An optimization model is formulated to determine the most suitable additive value function to the preferences expressed by an expert. The method also incorporates a consistency analysis to quantify and improve the reliability of the judgments. Additionally, BWD is extended to handle interval-valued preferences, enhancing its applicability in situations with uncertainty or imprecise information. We also developed an approach to identify a reference set, which is used for pairwise comparisons to elicit the value functions and weights. A case study in logistics performance evaluation demonstrates the practicality and effectiveness of BWD, showing that it produces reliable rankings aligned closely with experts' preferences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12678v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Brunelli, Fuqi Liang, Jafar Rezaei</dc:creator>
    </item>
    <item>
      <title>Global optimality conditions for sensor placement, with extensions to binary low-rank A-optimal designs</title>
      <link>https://arxiv.org/abs/2410.16590</link>
      <description>arXiv:2410.16590v5 Announce Type: replace 
Abstract: The \emph{sensor placement problem} for stochastic linear inverse problems consists of determining the optimal manner in which sensors can be employed to collect data. Specifically, one wishes to place a limited number of sensors over a large number of candidate locations, quantifying and optimising over the effect this data collection strategy has on the solution of the inverse problem. In this article, we provide a global optimality condition for the sensor placement problem via a subgradient argument, obtaining sufficient and necessary conditions for optimality\revix{, and marking certain sensors as \emph{dominant} or \emph{redundant}, i.e.~always on or always off}. We demonstrate how to take advantage of this optimality criterion to find approximately optimal binary designs, i.e.~designs where no fractions of sensors are placed. Leveraging our optimality criteria, we derive a powerful low-rank formulation of the A-optimal design objective for finite element-discretised function space settings, demonstrating its high computational efficiency, particularly in terms of derivatives, and study globally optimal designs for a Helmholtz-type source problem and extensions towards optimal binary designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16590v5</guid>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Aarset</dc:creator>
    </item>
    <item>
      <title>Reducing Contextual Stochastic Bilevel Optimization via Structured Function Approximation</title>
      <link>https://arxiv.org/abs/2503.19991</link>
      <description>arXiv:2503.19991v2 Announce Type: replace 
Abstract: Contextual Stochastic Bilevel Optimization (CSBO) extends standard stochastic bilevel optimization (SBO) by incorporating context-dependent lower-level problems, as in hyperparameter tuning and inverse optimization. This added structure introduces significant computational challenges: solving CSBO requires solving an infinite number of lower-level problems - one for each context realization - and existing approaches either suffer from high sample complexity or rely on impractical conditional sampling oracles. We propose a reduction framework that approximates the lower-level solutions using expressive basis functions, thereby decoupling the lower-level dependence on context and transforming CSBO into a standard SBO problem solvable using only joint samples from the context and noise distribution. Under mild assumptions, we show this reduction preserves hypergradient accuracy and yields an $\epsilon$-stationary solution to CSBO. We relate the sample complexity of the reduced problem to simple metrics of the basis and show that using Chebyshev polynomials leads to a near-optimal complexity of $\tilde{\mathcal{O}}(\epsilon^{-3})$, matching the best-known rates for standard SBO. Empirical results on hyperparameter and inverse optimization tasks demonstrate that our approach outperforms CSBO baselines in convergence, sample efficiency, and memory usage, especially in settings without conditional sampling access.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19991v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Maxime Bouscary, Jiawei Zhang, Saurabh Amin</dc:creator>
    </item>
    <item>
      <title>Model-Targeted Data Poisoning Attacks against ITS Applications with Provable Convergence</title>
      <link>https://arxiv.org/abs/2505.03966</link>
      <description>arXiv:2505.03966v2 Announce Type: replace 
Abstract: The growing reliance of intelligent systems on data makes the systems vulnerable to data poisoning attacks. Such attacks could compromise machine learning or deep learning models by disrupting the input data. Previous studies on data poisoning attacks are subject to specific assumptions, and limited attention is given to learning models with general (equality and inequality) constraints or lacking differentiability. Such learning models are common in practice, especially in Intelligent Transportation Systems (ITS) that involve physical or domain knowledge as specific model constraints. Motivated by ITS applications, this paper formulates a model-target data poisoning attack as a bi-level optimization problem with a constrained lower-level problem, aiming to induce the model solution toward a target solution specified by the adversary by modifying the training data incrementally. As the gradient-based methods fail to solve this optimization problem, we propose to study the Lipschitz continuity property of the model solution, enabling us to calculate the semi-derivative, a one-sided directional derivative, of the solution over data. We leverage semi-derivative descent to solve the bi-level optimization problem, and establish the convergence conditions of the method to any attainable target model. The model and solution method are illustrated with a simulation of a poisoning attack on the lane change detection using SVM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03966v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xin Wang (Jeff), Feilong Wang (Jeff), Yuan Hong (Jeff), R. Tyrrell Rockafellar (Jeff),  Xuegang (Jeff),  Ban</dc:creator>
    </item>
    <item>
      <title>The Essential Best and Average Rate of Convergence of the Exact Line Search Gradient Descent Method</title>
      <link>https://arxiv.org/abs/2305.09140</link>
      <description>arXiv:2305.09140v4 Announce Type: replace-cross 
Abstract: It is very well known that when the exact line search gradient descent method is applied to a convex quadratic objective, the worst-case rate of convergence (ROC), among all seed vectors, deteriorates as the condition number of the Hessian of the objective grows. By an elegant analysis due to H. Akaike, it is generally believed -- but not proved -- that in the ill-conditioned regime the ROC for almost all initial vectors, and hence also the average ROC, is close to the worst case ROC. We complete Akaike's analysis by determining the \emph{essential best case ROC} (defined in a measure-theoretic way) by using a dynamical system approach, facilitated by the theorem of center and stable manifolds. Our analysis also makes apparent the effect of an intermediate eigenvalue in the Hessian by establishing the following amusing result: In the absence of an intermediate eigenvalue, the average ROC gets arbitrarily \emph{fast} -- not slow -- as the Hessian gets increasingly ill-conditioned.
  We discuss in passing some contemporary applications of exact line search GD to well-conditioned polynomial optimization problems arising from imaging and data sciences. In particular, we observe that a tailored exact line search GD algorithm for a POP arising from the phase retrieval problem is only 50\% more expensive per iteration than its constant step size counterpart, while promising a ROC only matched by the optimally tuned (constant) step size which can rarely be achieved in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.09140v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Yu</dc:creator>
    </item>
    <item>
      <title>A Stability Principle for Learning under Non-Stationarity</title>
      <link>https://arxiv.org/abs/2310.18304</link>
      <description>arXiv:2310.18304v5 Announce Type: replace-cross 
Abstract: We develop a versatile framework for statistical learning in non-stationary environments. In each time period, our approach applies a stability principle to select a look-back window that maximizes the utilization of historical data while keeping the cumulative bias within an acceptable range relative to the stochastic error. Our theory showcases the adaptivity of this approach to unknown non-stationarity. We prove regret bounds that are minimax optimal up to logarithmic factors when the population losses are strongly convex, or Lipschitz only. At the heart of our analysis lie two novel components: a measure of similarity between functions and a segmentation technique for dividing the non-stationary data sequence into quasi-stationary pieces. We evaluate the practical performance of our approach through real-data experiments on electricity demand prediction and hospital nurse staffing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18304v5</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengpiao Huang, Kaizheng Wang</dc:creator>
    </item>
    <item>
      <title>Changing the Kernel During Training Leads to Double Descent in Kernel Regression</title>
      <link>https://arxiv.org/abs/2311.01762</link>
      <description>arXiv:2311.01762v3 Announce Type: replace-cross 
Abstract: We investigate changing the bandwidth of a translational-invariant kernel during training when solving kernel regression with gradient descent. We present a theoretical bound on the out-of-sample generalization error that advocates for decreasing the bandwidth (and thus increasing the model complexity) during training. We further use the bound to show that kernel regression exhibits a double descent behavior when the model complexity is expressed as the minimum allowed bandwidth during training. Decreasing the bandwidth all the way to zero results in benign overfitting, and also circumvents the need for model selection. We demonstrate the double descent behavior on real and synthetic data and also demonstrate that kernel regression with a decreasing bandwidth outperforms that of a constant bandwidth, selected by cross-validation or marginal likelihood maximization. We finally apply our findings to neural networks, demonstrating that by modifying the neural tangent kernel (NTK) during training, making the NTK behave as if its bandwidth were decreasing to zero, we can make the network overfit more benignly, and converge in fewer iterations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01762v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oskar Allerbo</dc:creator>
    </item>
    <item>
      <title>CONGO: Compressive Online Gradient Optimization</title>
      <link>https://arxiv.org/abs/2407.06325</link>
      <description>arXiv:2407.06325v4 Announce Type: replace-cross 
Abstract: We address the challenge of zeroth-order online convex optimization where the objective function's gradient exhibits sparsity, indicating that only a small number of dimensions possess non-zero gradients. Our aim is to leverage this sparsity to obtain useful estimates of the objective function's gradient even when the only information available is a limited number of function samples. Our motivation stems from the optimization of large-scale queueing networks that process time-sensitive jobs. Here, a job must be processed by potentially many queues in sequence to produce an output, and the service time at any queue is a function of the resources allocated to that queue. Since resources are costly, the end-to-end latency for jobs must be balanced with the overall cost of the resources used. While the number of queues is substantial, the latency function primarily reacts to resource changes in only a few, rendering the gradient sparse. We tackle this problem by introducing the Compressive Online Gradient Optimization framework which allows compressive sensing methods previously applied to stochastic optimization to achieve regret bounds with an optimal dependence on the time horizon without the full problem dimension appearing in the bound. For specific algorithms, we reduce the samples required per gradient estimate to scale with the gradient's sparsity factor rather than its full dimensionality. Numerical simulations and real-world microservices benchmarks demonstrate CONGO's superiority over gradient descent approaches that do not account for sparsity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06325v4</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeremy Carleton, Prathik Vijaykumar, Divyanshu Saxena, Dheeraj Narasimha, Srinivas Shakkottai, Aditya Akella</dc:creator>
    </item>
    <item>
      <title>Multilevel quadrature formulae for the optimal control of random PDEs</title>
      <link>https://arxiv.org/abs/2407.06678</link>
      <description>arXiv:2407.06678v2 Announce Type: replace-cross 
Abstract: This manuscript presents a framework for using multilevel quadrature formulae to compute the solution of optimal control problems constrained by random partial differential equations. Our approach consists in solving a sequence of optimal control problems discretized with different levels of accuracy of the physical and probability discretizations. The final approximation of the control is then obtained in a postprocessing step, by suitably combining the adjoint variables computed on the different levels. We present a general convergence and complexity analysis for an unconstrained linear quadratic problem under abstract assumptions on the spatial discretization and on the quadrature formulae. We detail our framework for the specific case of a MultiLevel Monte Carlo (MLMC) quadrature formula, and numerical experiments confirm the better computational complexity of our MLMC approach compared to a standard Monte Carlo sample average approximation, even beyond the theoretical assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06678v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabio Nobile, Tommaso Vanzan</dc:creator>
    </item>
    <item>
      <title>IID Prophet Inequality with Random Horizon: Going Beyond Increasing Hazard Rates</title>
      <link>https://arxiv.org/abs/2407.11752</link>
      <description>arXiv:2407.11752v3 Announce Type: replace-cross 
Abstract: Prophet inequalities are a central object of study in optimal stopping theory. In the iid model, a gambler sees values in an online fashion, sampled independently from a given distribution. Upon observing each value, the gambler either accepts it as a reward or irrevocably rejects it and proceeds to observe the next value. The goal of the gambler, who cannot see the future, is maximising the expected value of the reward while competing against the expectation of a prophet (the offline maximum). In other words, one seeks to maximise the gambler-to-prophet ratio of the expectations.
  This model has been studied with infinite, finite and unknown number of values. When the gambler faces a random number of values, the model is said to have random horizon. We consider the model in which the gambler is given a priori knowledge of the horizon's distribution. Alijani et al. (2020) designed a single-threshold algorithm achieving a ratio of $1/2$ when the random horizon has an increasing hazard rate and is independent of the values. We prove that with a single threshold, a ratio of $1/2$ is actually achievable for several larger classes of horizon distributions, with the largest being known as the $\mathcal{G}$ class in reliability theory. Moreover, we show that this does not extend to its dual, the $\overline{\mathcal{G}}$ class (which includes the decreasing hazard rate class), while it can be extended to low-variance horizons. Finally, we construct the first example of a family of horizons, for which multiple thresholds are necessary to achieve a nonzero ratio. We establish that the Secretary Problem optimal stopping rule provides one such algorithm, paving the way towards the study of the model beyond single-threshold algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11752v3</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giordano Giambartolomei, Frederik Mallmann-Trenn, Raimundo Saona</dc:creator>
    </item>
    <item>
      <title>EXAdam: The Power of Adaptive Cross-Moments</title>
      <link>https://arxiv.org/abs/2412.20302</link>
      <description>arXiv:2412.20302v2 Announce Type: replace-cross 
Abstract: This paper introduces EXAdam ($\textbf{EX}$tended $\textbf{Adam}$), a novel optimization algorithm that builds upon the widely-used Adam optimizer. EXAdam incorporates two key enhancements: (1) new debiasing terms for improved moment estimation and (2) a gradient-based acceleration mechanism for increased responsiveness to the current loss landscape. These innovations work synergistically to address limitations of the original Adam algorithm, potentially offering improved convergence properties, enhanced ability to escape saddle points, and potentially greater robustness to hyperparameter choices, though this requires further investigation. We provide a theoretical analysis of EXAdam's components and their interactions, highlighting the algorithm's potential advantages in navigating complex optimization landscapes. Empirical evaluations demonstrate EXAdam's superiority over Adam, achieving 38.46% faster convergence and yielding improvements of 1.96%, 2.17%, and 1.17% in training, validation, and testing accuracies, respectively, when applied to a CNN trained on the CIFAR-10 dataset. While these results are promising, further empirical validation across diverse tasks is essential to fully gauge EXAdam's efficacy. Nevertheless, EXAdam represents a significant advancement in adaptive optimization techniques, with promising implications for a wide range of machine learning applications. This work aims to contribute to the ongoing development of more efficient, adaptive, and universally applicable optimization methods in the field of machine learning and artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20302v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmed M. Adly</dc:creator>
    </item>
    <item>
      <title>Towards Understanding Gradient Flow Dynamics of Homogeneous Neural Networks Beyond the Origin</title>
      <link>https://arxiv.org/abs/2502.15952</link>
      <description>arXiv:2502.15952v2 Announce Type: replace-cross 
Abstract: Recent works exploring the training dynamics of homogeneous neural network weights under gradient flow with small initialization have established that in the early stages of training, the weights remain small and near the origin, but converge in direction. Building on this, the current paper studies the gradient flow dynamics of homogeneous neural networks with locally Lipschitz gradients, after they escape the origin. Insights gained from this analysis are used to characterize the first saddle point encountered by gradient flow after escaping the origin. Also, it is shown that for homogeneous feed-forward neural networks, under certain conditions, the sparsity structure emerging among the weights before the escape is preserved after escaping the origin and until reaching the next saddle point.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15952v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akshay Kumar, Jarvis Haupt</dc:creator>
    </item>
  </channel>
</rss>
