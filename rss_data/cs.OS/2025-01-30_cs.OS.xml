<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.OS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.OS</link>
    <description>cs.OS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.OS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 Jan 2025 02:31:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Ownership-based Virtual Memory for Intermittently-Powered Embedded Systems</title>
      <link>https://arxiv.org/abs/2501.17707</link>
      <description>arXiv:2501.17707v1 Announce Type: new 
Abstract: The Battery-Free Internet of Things might revolutionize our understanding of connected devices, which harvest their operational energy from the environment (e.g., using solar cells). These systems come with the major system-software challenge that the intermittently-powered IoT devices have to checkpoint their state in non-volatile memory to later resume operation with this state when sufficient energy is available. The scarce energy resources demand that only modified data is persisted to non-volatile memory before a power failure, which requires precise modification-tracking.
  In this paper, we present the first virtually Non-Volatile Heap (vNV-Heap) abstraction for intermittently-powered systems with guaranteed power-failure resilience and non-volatile memory safety (analogous to memory-safety for RAM). The heap exploits ownership systems, a zero-cost (i.e., compile-time) abstraction for example implemented by Rust, to track modifications and virtualize object-persistence. To achieve power-failure resilience, our heap is designed and implemented to guarantee bounded operations by static program code analysis: As an example, the heap allows for determining a worst-case energy consumption for the operation of persisting modified and currently volatile objects. Our evaluations with our open-source implementation on an embedded hardware platform (i.e., ESP32-C3) shows that using our heap abstraction is more energy-efficient than existing approaches, while also providing runtime guarantees by static worst-case analysis bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17707v1</guid>
      <category>cs.OS</category>
      <category>cs.ET</category>
      <category>cs.PL</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Markus Elias Gerber, Luis Gerhorst, Ishwar Mudraje, Kai Vogelgesang, Thorsten Herfet, Peter W\"agemann</dc:creator>
    </item>
    <item>
      <title>vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention</title>
      <link>https://arxiv.org/abs/2405.04437</link>
      <description>arXiv:2405.04437v3 Announce Type: replace-cross 
Abstract: PagedAttention is a popular approach for dynamic memory allocation in LLM serving systems. It enables on-demand allocation of GPU memory to mitigate KV cache fragmentation -- a phenomenon that crippled the batch size (and consequently throughput) in prior systems. However, in trying to allocate physical memory at runtime, PagedAttention ends up changing the virtual memory layout of the KV cache from contiguous to non-contiguous. Such a design leads to non-trivial programming and performance overheads.
  We present vAttention -- an approach that mitigates fragmentation in physical memory while retaining the contiguity of KV cache in virtual memory. We achieve this by decoupling the allocation of virtual and physical memory using CUDA virtual memory management APIs. We also introduce various LLM-specific optimizations to address the limitations of CUDA virtual memory support. Overall, vAttention is a simpler, portable, and performant alternative to PagedAttention: it supports various attention kernels out-of-the-box and improves LLM serving throughput by up to 1.23x compared to the use of PagedAttention-based kernels of FlashAttention and FlashInfer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04437v3</guid>
      <category>cs.LG</category>
      <category>cs.OS</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ramya Prabhu, Ajay Nayak, Jayashree Mohan, Ramachandran Ramjee, Ashish Panwar</dc:creator>
    </item>
    <item>
      <title>Optimizing SSD Caches for Cloud Block Storage Systems Using Machine Learning Approaches</title>
      <link>https://arxiv.org/abs/2501.14770</link>
      <description>arXiv:2501.14770v2 Announce Type: replace-cross 
Abstract: The growing demand for efficient cloud storage solutions has led to the widespread adoption of Solid-State Drives (SSDs) for caching in cloud block storage systems. The management of data writes to SSD caches plays a crucial role in improving overall system performance, reducing latency, and extending the lifespan of storage devices. A critical challenge arises from the large volume of write-only data, which significantly impacts the performance of SSD caches when handled inefficiently. Specifically, writes that have not been read for a certain period may introduce unnecessary write traffic to the SSD cache without offering substantial benefits for cache performance. This paper proposes a novel approach to mitigate this issue by leveraging machine learning techniques to dynamically optimize the write policy in cloud-based storage systems. The proposed method identifies write-only data and selectively filters it out in real-time, thereby minimizing the number of unnecessary write operations and improving the overall performance of the cache system. Experimental results demonstrate that the proposed machine learning-based policy significantly outperforms traditional approaches by reducing the number of harmful writes and optimizing cache utilization. This solution is particularly suitable for cloud environments with varying and unpredictable workloads, where traditional cache management strategies often fall short.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14770v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.OS</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chiyu Cheng, Chang Zhou, Yang Zhao, Jin Cao</dc:creator>
    </item>
    <item>
      <title>Dynamic Adaptation in Data Storage: Real-Time Machine Learning for Enhanced Prefetching</title>
      <link>https://arxiv.org/abs/2501.14771</link>
      <description>arXiv:2501.14771v2 Announce Type: replace-cross 
Abstract: The exponential growth of data storage demands has necessitated the evolution of hierarchical storage management strategies [1]. This study explores the application of streaming machine learning [3] to revolutionize data prefetching within multi-tiered storage systems. Unlike traditional batch-trained models, streaming machine learning [5] offers adaptability, real-time insights, and computational efficiency, responding dynamically to workload variations. This work designs and validates an innovative framework that integrates streaming classification models for predicting file access patterns, specifically the next file offset. Leveraging comprehensive feature engineering and real-time evaluation over extensive production traces, the proposed methodology achieves substantial improvements in prediction accuracy, memory efficiency, and system adaptability. The results underscore the potential of streaming models in real-time storage management, setting a precedent for advanced caching and tiering strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14771v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.OS</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chiyu Cheng, Chang Zhou, Yang Zhao, Jin Cao</dc:creator>
    </item>
  </channel>
</rss>
