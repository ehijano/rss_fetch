<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Jul 2025 01:25:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Bayesian Compressed Mixed-Effects Models</title>
      <link>https://arxiv.org/abs/2507.16961</link>
      <description>arXiv:2507.16961v1 Announce Type: cross 
Abstract: Penalized likelihood and quasi-likelihood methods dominate inference in high-dimensional linear mixed-effects models. Sampling-based Bayesian inference is less explored due to the computational bottlenecks introduced by the random effects covariance matrix. To address this gap, we propose the compressed mixed-effects (CME) model, which defines a quasi-likelihood using low-dimensional covariance parameters obtained via random projections of the random effects covariance. This dimension reduction, combined with a global-local shrinkage prior on the fixed effects, yields an efficient collapsed Gibbs sampler for prediction and fixed effects selection. Theoretically, when the compression dimension grows slowly relative to the number of fixed effects and observations, the Bayes risk for prediction is asymptotically negligible, ensuring accurate prediction using the CME model. Empirically, the CME model outperforms existing approaches in terms of predictive accuracy, interval coverage, and fixed-effects selection across varied simulation settings and a real-world dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16961v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sreya Sarkar, Kshitij Khare, Sanvesh Srivastava</dc:creator>
    </item>
    <item>
      <title>Efficient Bayesian Inference for Spatial Point Patterns Using the Palm Likelihood</title>
      <link>https://arxiv.org/abs/2507.17065</link>
      <description>arXiv:2507.17065v1 Announce Type: cross 
Abstract: Bayesian inference for spatial point patterns is often hindered computationally by intractable likelihoods. In the frequentist literature, estimating equations utilizing pseudolikelihoods have long been used for simulation-free parameter estimation. One such pseudolikelihood based on the process of differences is known as the Palm likelihood. Utilizing notions of Bayesian composite likelihoods and generalized Bayesian inference, we develop a framework for the use of Palm likelihoods in a Bayesian context. Naive implementation of the Palm likelihood results in posterior undercoverage of model parameters. We propose two approaches to remedy this issue and calibrate the resulting posterior. Numerical simulations illustrate both the efficacy of the method in terms of statistical properties and the superiority in terms of computational efficiency when compared to classical Markov chain Monte Carlo. The method is then applied to the popular \textit{Beilschmiedia pendula Lauraceae} dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17065v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kevin M. Collins, Erin M. Schliep</dc:creator>
    </item>
    <item>
      <title>MCMC Importance Sampling via Moreau-Yosida Envelopes</title>
      <link>https://arxiv.org/abs/2501.02228</link>
      <description>arXiv:2501.02228v2 Announce Type: replace 
Abstract: Non-differentiable priors are standard in modern parsimonious Bayesian models. Lack of differentiability, however, precludes gradient-based Markov chain Monte Carlo (MCMC) for posterior sampling. Recently proposed proximal MCMC approaches can partially remedy this limitation by using a differentiable approximation, constructed via Moreau-Yosida (MY) envelopes, to make proposals. In this work, we build an importance sampling paradigm by using the MY envelope as an importance distribution. Leveraging properties of the envelope, we establish asymptotic normality of the importance sampling estimator with an explicit expression for the asymptotic covariance matrix. Since the MY envelope density is smooth, it is amenable to gradient-based samplers. We provide sufficient conditions for geometric ergodicity of Metropolis-adjusted Langevin and Hamiltonian Monte Carlo algorithms, sampling from this importance distribution. Our numerical studies show that the proposed scheme can yield lower variance estimators compared to existing proximal MCMC alternatives, and is effective in low and high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02228v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Apratim Shukla, Dootika Vats, Eric C. Chi</dc:creator>
    </item>
    <item>
      <title>Polynomial time guarantees for sampling based posterior inference in high-dimensional generalised linear models</title>
      <link>https://arxiv.org/abs/2208.13296</link>
      <description>arXiv:2208.13296v3 Announce Type: replace-cross 
Abstract: The problem of computing posterior functionals in general high-dimensional statistical models with possibly non-log-concave likelihood functions is considered. Based on the proof strategy of Nickl and Wang (2022), but using only local likelihood conditions and without relying on M-estimation theory, nonasymptotic statistical and computational guarantees are provided for a gradient based MCMC algorithm. Given a suitable initialiser, these guarantees scale polynomially in key algorithmic quantities. The abstract results are applied to several concrete statistical models, including density estimation, nonparametric regression with generalised linear models and a canonical statistical non-linear inverse problem from PDEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.13296v3</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Randolf Altmeyer</dc:creator>
    </item>
    <item>
      <title>Fast post-process Bayesian inference with Variational Sparse Bayesian Quadrature</title>
      <link>https://arxiv.org/abs/2303.05263</link>
      <description>arXiv:2303.05263v4 Announce Type: replace-cross 
Abstract: In applied Bayesian inference scenarios, users may have access to a large number of pre-existing model evaluations, for example from maximum-a-posteriori (MAP) optimization runs. However, traditional approximate inference techniques make little to no use of this available information. We propose the framework of post-process Bayesian inference as a means to obtain a quick posterior approximation from existing target density evaluations, with no further model calls. Within this framework, we introduce Variational Sparse Bayesian Quadrature (VSBQ), a method for post-process approximate inference for models with black-box and potentially noisy likelihoods. VSBQ reuses existing target density evaluations to build a sparse Gaussian process (GP) surrogate model of the log posterior density function. Subsequently, we leverage sparse-GP Bayesian quadrature combined with variational inference to achieve fast approximate posterior inference over the surrogate. We validate our method on challenging synthetic scenarios and real-world applications from computational neuroscience. The experiments show that VSBQ builds high-quality posterior approximations by post-processing existing optimization traces, with no further model evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.05263v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengkun Li, Gr\'egoire Clart\'e, Martin J{\o}rgensen, Luigi Acerbi</dc:creator>
    </item>
    <item>
      <title>Bayesian Generalized Nonlinear Models Offer Basis Free SINDy With Model Uncertainty</title>
      <link>https://arxiv.org/abs/2507.06776</link>
      <description>arXiv:2507.06776v2 Announce Type: replace-cross 
Abstract: Sparse Identification of Nonlinear Dynamics (SINDy) has become a standard methodology for inferring governing equations of dynamical systems from observed data using statistical modeling. However, classical SINDy approaches rely on predefined libraries of candidate functions to model nonlinearities, which limits flexibility and excludes robust uncertainty quantification. This paper proposes Bayesian Generalized Nonlinear Models (BGNLMs) as a principled alternative for more flexible statistical modeling. BGNLMs employ spike-and-slab priors combined with binary inclusion indicators to automatically discover relevant nonlinearities without predefined basis functions. Moreover, BGNLMs quantify uncertainty in selected bases and final model predictions, enabling robust exploration of the model space. In this paper, the BGNLM framework is applied to several three-dimensional (3D) SINDy problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06776v2</guid>
      <category>stat.ME</category>
      <category>math.DS</category>
      <category>stat.CO</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>https://iwsm2025.ie/wp-content/uploads/2025/07/IWSM2025_Limerick_Proceedings.pdf</arxiv:journal_reference>
      <dc:creator>Aliaksandr Hubin</dc:creator>
    </item>
  </channel>
</rss>
