<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 26 Aug 2024 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Research on Improved U-net Based Remote Sensing Image Segmentation Algorithm</title>
      <link>https://arxiv.org/abs/2408.12672</link>
      <description>arXiv:2408.12672v1 Announce Type: new 
Abstract: In recent years, although U-Net network has made significant progress in the field of image segmentation, it still faces performance bottlenecks in remote sensing image segmentation. In this paper, we innovatively propose to introduce SimAM and CBAM attention mechanism in U-Net, and the experimental results show that after adding SimAM and CBAM modules alone, the model improves 17.41% and 12.23% in MIoU, and the Mpa and Accuracy are also significantly improved. And after fusing the two,the model performance jumps up to 19.11% in MIoU, and the Mpa and Accuracy are also improved by 16.38% and 14.8% respectively, showing excellent segmentation accuracy and visual effect with strong generalization ability and robustness. This study opens up a new path for remote sensing image segmentation technology and has important reference value for algorithm selection and improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12672v1</guid>
      <category>cs.DC</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiming Yang, Zixin Wang, Shinan Liu, Zizheng Li</dc:creator>
    </item>
    <item>
      <title>NanoFlow: Towards Optimal Large Language Model Serving Throughput</title>
      <link>https://arxiv.org/abs/2408.12757</link>
      <description>arXiv:2408.12757v1 Announce Type: new 
Abstract: The increasing usage of Large Language Models (LLMs) has resulted in a surging demand for planet-scale serving systems, where tens of thousands of GPUs continuously serve hundreds of millions of users. Consequently, throughput (under reasonable latency constraints) has emerged as a key metric that determines serving systems' performance. To boost throughput, various methods of inter-device parallelism (e.g., data, tensor, pipeline) have been explored. However, existing methods do not consider overlapping the utilization of different resources within a single device, leading to underutilization and sub-optimal performance.
  We propose NanoFlow, a novel serving framework that exploits intra-device parallelism, which overlaps the usage of resources including compute, memory, and network within a single device through operation co-scheduling. To exploit intra-device parallelism, NanoFlow introduces two key innovations: First, NanoFlow splits requests into nano-batches at the granularity of operations, which breaks the dependency of sequential operations in LLM inference and enables overlapping; then, to get benefit from overlapping, NanoFlow uses an operation-level pipeline with execution unit scheduling, which partitions the device's functional units and simultaneously executes different operations in each unit. NanoFlow automates the pipeline setup using a parameter search algorithm, which enables easily porting NanoFlow to different models. We implement NanoFlow on NVIDIA GPUs and evaluate end-to-end serving throughput on several popular models such as LLaMA-2-70B, Mixtral 8x7B, LLaMA-3-8B, etc.. With practical workloads, NanoFlow provides 1.91x throughput boost compared to state-of-the-art serving systems achieving 59% to 72% of optimal throughput across ported models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12757v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kan Zhu, Yilong Zhao, Liangyu Zhao, Gefei Zuo, Yile Gu, Dedong Xie, Yufei Gao, Qinyu Xu, Tian Tang, Zihao Ye, Keisuke Kamahori, Chien-Yu Lin, Stephanie Wang, Arvind Krishnamurthy, Baris Kasikci</dc:creator>
    </item>
    <item>
      <title>Granular Synchrony</title>
      <link>https://arxiv.org/abs/2408.12853</link>
      <description>arXiv:2408.12853v1 Announce Type: new 
Abstract: Today's mainstream network timing models for distributed computing are synchrony, partial synchrony, and asynchrony. These models are coarse-grained and often make either too strong or too weak assumptions about the network. This paper introduces a new timing model called granular synchrony that models the network as a mixture of synchronous, partially synchronous, and asynchronous communication links. The new model is not only theoretically interesting but also more representative of real-world networks. It also serves as a unifying framework where current mainstream models are its special cases. We present necessary and sufficient conditions for solving crash and Byzantine fault-tolerant consensus in granular synchrony. Interestingly, consensus among $n$ parties can be achieved against $f \geq n/2$ crash faults or $f \geq n/3$ Byzantine faults without resorting to full synchrony.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12853v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neil Giridharan, Ittai Abraham, Natacha Crooks, Kartik Nayak, Ling Ren</dc:creator>
    </item>
    <item>
      <title>Efficient Training Approaches for Performance Anomaly Detection Models in Edge Computing Environments</title>
      <link>https://arxiv.org/abs/2408.12855</link>
      <description>arXiv:2408.12855v1 Announce Type: new 
Abstract: Microservice architectures are increasingly used to modularize IoT applications and deploy them in distributed and heterogeneous edge computing environments. Over time, these microservice-based IoT applications are susceptible to performance anomalies caused by resource hogging (e.g., CPU or memory), resource contention, etc., which can negatively impact their Quality of Service and violate their Service Level Agreements. Existing research on performance anomaly detection for edge computing environments focuses on model training approaches that either achieve high accuracy at the expense of a time-consuming and resource-intensive training process or prioritize training efficiency at the cost of lower accuracy. To address this gap, while considering the resource constraints and the large number of devices in modern edge platforms, we propose two clustering-based model training approaches : (1) intra-cluster parameter transfer learning-based model training (ICPTL) and (2) cluster-level model training (CM). These approaches aim to find a trade-off between the training efficiency of anomaly detection models and their accuracy. We compared the models trained under ICPTL and CM to models trained for specific devices (most accurate, least efficient) and a single general model trained for all devices (least accurate, most efficient). Our findings show that the model accuracy of ICPTL is comparable to that of the model per device approach while requiring only 40% of the training time. In addition, CM further improves training efficiency by requiring 23% less training time and reducing the number of trained models by approximately 66% compared to ICPTL, yet achieving a higher accuracy than a single general model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12855v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Duneesha Fernando, Maria A. Rodriguez, Patricia Arroba, Leila Ismail, Rajkumar Buyya</dc:creator>
    </item>
    <item>
      <title>Complete Graph Identification in Population Protocols</title>
      <link>https://arxiv.org/abs/2408.12862</link>
      <description>arXiv:2408.12862v1 Announce Type: new 
Abstract: We consider the population protocol model where indistinguishable state machines, referred to as agents, communicate in pairs. The communication graph specifies potential interactions (\ie communication) between agent pairs. This paper addresses the complete graph identification problem, requiring agents to determine if their communication graph is a clique or not. We evaluate various settings based on: (i) the fairness preserved by the adversarial scheduler -- either global fairness or weak fairness, and (ii) the knowledge provided to agents beforehand -- either the exact population size $n$, a common upper bound $P$ on $n$, or no prior information. Positively, we show that $O(n^2)$ states per agent suffice to solve the complete graph identification problem under global fairness without prior knowledge. With prior knowledge of $n$, agents can solve the problem using only $O(n)$ states under weak fairness. Negatively, we prove that complete graph identification remains unsolvable under weak fairness when only a common upper bound $P$ on the population size $n$ is known.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12862v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haruki Kanaya, Yuichi Sudo</dc:creator>
    </item>
    <item>
      <title>Tight Bounds for Constant-Round Domination on Graphs of High Girth and Low Expansion</title>
      <link>https://arxiv.org/abs/2408.12998</link>
      <description>arXiv:2408.12998v1 Announce Type: new 
Abstract: A long-standing open question is which graph class is the most general one permitting constant-time constant-factor approximations for dominating sets. The approximation ratio has been bounded by increasingly general parameters such as genus, arboricity, or expansion of the input graph. Amiri and Wiederhake considered $k$-hop domination in graphs of bounded $k$-hop expansion and girth at least $4k+3$; the $k$-hop expansion $f(k)$ of a graph family denotes the maximum ratio of edges to nodes that can be achieved by contracting disjoint subgraphs of radius $k$ and deleting nodes. In this setting, these authors to obtain a simple $O(k)$-round algorithm achieving approximation ratio $\Theta(kf(k))$.
  In this work, we study the same setting but derive tight bounds:
  - A $\Theta(kf(k))$-approximation is possible in $k$, but not $k-1$ rounds.
  - In $3k$ rounds an $O(k+f(k)^{k/(k+1)})$-approximation can be achieved.
  - No constant-round deterministic algorithm can achieve approximation ratio $o(k+f(k)^{k/(k+1)})$.
  Our upper bounds hold in the port numbering model with small messages, while the lower bounds apply to local algorithms, i.e., with arbitrary message size and unique identifiers. This means that the constant-time approximation ratio can be \emph{sublinear} in the edge density of the graph, in a graph class which does not allow a constant approximation. This begs the question whether this is an artefact of the restriction to high girth or can be extended to all graphs of $k$-hop expansion $f(k)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12998v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christoph Lenzen, Sophie Wenning</dc:creator>
    </item>
    <item>
      <title>Simopt -- Simulation pass for Speculative Optimisation of FPGA-CAD flow</title>
      <link>https://arxiv.org/abs/2408.12676</link>
      <description>arXiv:2408.12676v1 Announce Type: cross 
Abstract: Behavioural simulation is deployed in CAD flow to verify the functional correctness of a Register Transfer Level (RTL) design. Metadata extracted from behavioural simulation could be used to optimise and/or speed up subsequent steps in the hardware design flow. In this paper, we propose Simopt, a tool flow that extracts simulation metadata to improve the timing performance of the design by introducing latency awareness during the placement phase and subsequently improving the routing time of the post-placed netlist using vendor tools. For our experiments, we adapt the open-source Yosys flow to perform Simopt-aware placement. Our results show that using the Simopt-pass in the design implementation flow results in up to 38.2% reduction in timing performance (latency) of the design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12676v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eashan Wadhwa, Shanker Shreejith</dc:creator>
    </item>
    <item>
      <title>General-Purpose Multicore Architectures</title>
      <link>https://arxiv.org/abs/2408.12999</link>
      <description>arXiv:2408.12999v1 Announce Type: cross 
Abstract: The first years of the 2000s led to an inflection point in computer architectures: while the number of available transistors on a chip continued to grow, crucial transistor scaling properties started to break down and result in increasing power consumption, while aggressive single-core performance optimizations were resulting in diminishing returns due to inherent limits in instruction-level parallelism. This led to the rise of multicore CPU architectures, which are now commonplace in modern computers at all scales. In this chapter, we discuss the evolution of multicore CPUs since their introduction. Starting with a historic overview of multiprocessing, we explore the basic microarchitecture of a multicore CPU, key challenges resulting from shared memory resources, operating system modifications to optimize multicore CPU support, popular metrics for multicore evaluation, and recent trends in multicore CPU design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12999v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saugata Ghose</dc:creator>
    </item>
    <item>
      <title>Self-stabilizing Graph Exploration by a Single Agent</title>
      <link>https://arxiv.org/abs/2010.08929</link>
      <description>arXiv:2010.08929v3 Announce Type: replace 
Abstract: In this paper, we present two self-stabilizing algorithms that enable a single (mobile) agent to explore graphs. The agent visits all nodes starting from any configuration, \ie regardless of the initial state of the agent, the initial states of all nodes, and the initial location of the agent. We evaluate the algorithms using two metrics: cover time, which is the number of moves required to visit all nodes, and memory usage, which includes the storage needed for the state of the agent and the state of each node. The first algorithm is randomized. Given an integer $c = \Omega(n)$, the cover time of this algorithm is optimal, \ie $O(m)$ in expectation, and the memory requirements for the agent and each node $v$ are $O(\log c)$ and $O(\log (c+\delta_v))$ bits, respectively, where $n$ and $m$ are the numbers of nodes and edges, respectively, and $\delta_v$ is the degree of $v$. The second algorithm is deterministic. It requires an input integer $k \ge \max(D,d_{\mathrm{max}})$, where $D$ and $d_{\mathrm{max}}$ are the diameter and the maximum degree of the graph, respectively. The cover time of this algorithm is $O(m + nD)$, and it uses $O(\log k)$ bits both for agent memory and each node.</description>
      <guid isPermaLink="false">oai:arXiv.org:2010.08929v3</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuichi Sudo, Fukuhito Ooshita, Sayaka Kamei</dc:creator>
    </item>
    <item>
      <title>Reconfigurable Heterogeneous Quorum Systems</title>
      <link>https://arxiv.org/abs/2304.02156</link>
      <description>arXiv:2304.02156v2 Announce Type: replace 
Abstract: In contrast to proof-of-work replication, Byzantine quorum systems maintain consistency across replicas with higher throughput modest energy consumption, and deterministic liveness guarantees. If complemented with heterogeneous trust and open membership, they have the potential to serve as blockchains backbone. This paper presents a general model of heterogeneous quorum systems where each participant can declare its own quorums, and captures the consistency, availability and inclusion properties of these systems. In order to support open membership, it then presents reconfiguration protocols for heterogeneous quorum systems including joining and leaving of a process, and adding and removing of a quorum, and further, proves their correctness in the face of Byzantine attacks. The design of the protocols is informed by the trade-offs that the paper proves for the properties that reconfigurations can preserve. The paper further presents a graph characterization of heterogeneous quorum systems, and its application for reconfiguration optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.02156v2</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiao Li, Mohsen Lesani</dc:creator>
    </item>
    <item>
      <title>KaMPIng: Flexible and (Near) Zero-Overhead C++ Bindings for MPI</title>
      <link>https://arxiv.org/abs/2404.05610</link>
      <description>arXiv:2404.05610v2 Announce Type: replace 
Abstract: The Message-Passing Interface (MPI) and C++ form the backbone of high-performance computing, but MPI only provides C and Fortran bindings. While this offers great language interoperability, high-level programming languages like C++ make software development quicker and less error-prone.
  We propose novel C++ language bindings that cover all abstraction levels from low-level MPI calls to convenient STL-style bindings, where most parameters are inferred from a small subset of parameters, by bringing named parameters to C++. This enables rapid prototyping and fine-tuning runtime behavior and memory management. A flexible type system and additional safety guarantees help to prevent programming errors.
  By exploiting C++'s template metaprogramming capabilities, this has (near) zero overhead, as only required code paths are generated at compile time.
  We demonstrate that our library is a strong foundation for a future distributed standard library using multiple application benchmarks, ranging from text-book sorting algorithms to phylogenetic interference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05610v2</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim Niklas Uhl, Matthias Schimek, Lukas H\"ubner, Demian Hespe, Florian Kurpicz, Christoph Stelz, Peter Sanders</dc:creator>
    </item>
    <item>
      <title>Large-Scale Metric Computation in Online Controlled Experiment Platform</title>
      <link>https://arxiv.org/abs/2405.08411</link>
      <description>arXiv:2405.08411v2 Announce Type: replace 
Abstract: Online controlled experiment (also called A/B test or experiment) is the most important tool for decision-making at a wide range of data-driven companies like Microsoft, Google, Meta, etc. Metric computation is the core procedure for reaching a conclusion during an experiment. With the growth of experiments and metrics in an experiment platform, computing metrics efficiently at scale becomes a non-trivial challenge. This work shows how metric computation in WeChat experiment platform can be done efficiently using bit-sliced index (BSI) arithmetic. This approach has been implemented in a real world system and the performance results are presented, showing that the BSI arithmetic approach is very suitable for large-scale metric computation scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08411v2</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.14778/3685800.3685823</arxiv:DOI>
      <dc:creator>Tao Xiong, Yong Wang</dc:creator>
    </item>
    <item>
      <title>Impacts of floating-point non-associativity on reproducibility for HPC and deep learning applications</title>
      <link>https://arxiv.org/abs/2408.05148</link>
      <description>arXiv:2408.05148v2 Announce Type: replace 
Abstract: Run-by-run variability in parallel programs caused by floating-point non-associativity (FPNA) has been known to significantly affect reproducibility in iterative algorithms, due to accumulating errors. Non-reproducibility negatively affects efficiency and effectiveness of correctness testing for stochastic programs. Recently, the sensitivity of deep learning (DL) training and inference pipelines to FPNA have been found to be extreme, and can prevent certification for commercial applications, accurate assessment of robustness and sensitivity, and bug detection. New approaches in scientific computing applications have coupled DL models with high-performance computing (HPC) simulations, leading to an aggravation of debugging and testing challenges. Here we perform an investigation of the statistical properties of FPNA within modern parallel programming models, analyze performance and productivity impacts of replacing atomic operations with deterministic alternatives on GPUs, and examine the recently-added deterministic options within the PyTorch framework within the context of GPU deployment, uncovering and quantifying the impacts of input parameters triggering run-by-run variability and reporting on the reliability and completeness of the documentation. Finally, we evaluate the strategy of exploiting automatic determinism provided by deterministic hardware, using the Groq LPU$^{TM}$ accelerator for inference portions of the DL pipeline. We demonstrate the benefits that this strategy can provide within reproducibility and correctness efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05148v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjif Shanmugavelu, Mathieu Taillefumier, Christopher Culver, Oscar Hernandez, Mark Coletti, Ada Sedova</dc:creator>
    </item>
    <item>
      <title>etuner: A Redundancy-Aware Framework for Efficient Continual Learning Application on Edge Devices</title>
      <link>https://arxiv.org/abs/2401.16694</link>
      <description>arXiv:2401.16694v5 Announce Type: replace-cross 
Abstract: Many emerging applications, such as robot-assisted eldercare and object recognition, generally employ deep learning neural networks (DNNs) and require the deployment of DNN models on edge devices. These applications naturally require i) handling streaming-in inference requests and ii) fine-tuning the deployed models to adapt to possible deployment scenario changes. Continual learning (CL) is widely adopted to satisfy these needs. CL is a popular deep learning paradigm that handles both continuous model fine-tuning and overtime inference requests. However, an inappropriate model fine-tuning scheme could involve significant redundancy and consume considerable time and energy, making it challenging to apply CL on edge devices. In this paper, we propose ETuner, an efficient edge continual learning framework that optimizes inference accuracy, fine-tuning execution time, and energy efficiency through both inter-tuning and intra-tuning optimizations. Experimental results show that, on average, ETuner reduces overall fine-tuning execution time by 64%, energy consumption by 56%, and improves average inference accuracy by 1.75% over the immediate model fine-tuning approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16694v5</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sheng Li, Geng Yuan, Yawen Wu, Yue Dai, Tianyu Wang, Chao Wu, Alex K. Jones, Jingtong Hu, Yanzhi Wang, Xulong Tang</dc:creator>
    </item>
    <item>
      <title>Addressing Skewed Heterogeneity via Federated Prototype Rectification with Personalization</title>
      <link>https://arxiv.org/abs/2408.07966</link>
      <description>arXiv:2408.07966v2 Announce Type: replace-cross 
Abstract: Federated learning is an efficient framework designed to facilitate collaborative model training across multiple distributed devices while preserving user data privacy. A significant challenge of federated learning is data-level heterogeneity, i.e., skewed or long-tailed distribution of private data. Although various methods have been proposed to address this challenge, most of them assume that the underlying global data is uniformly distributed across all clients. This paper investigates data-level heterogeneity federated learning with a brief review and redefines a more practical and challenging setting called Skewed Heterogeneous Federated Learning (SHFL). Accordingly, we propose a novel Federated Prototype Rectification with Personalization which consists of two parts: Federated Personalization and Federated Prototype Rectification. The former aims to construct balanced decision boundaries between dominant and minority classes based on private data, while the latter exploits both inter-class discrimination and intra-class consistency to rectify empirical prototypes. Experiments on three popular benchmarks show that the proposed approach outperforms current state-of-the-art methods and achieves balanced performance in both personalization and generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07966v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shunxin Guo, Hongsong Wang, Shuxia Lin, Zhiqiang Kou, Xin Geng</dc:creator>
    </item>
  </channel>
</rss>
