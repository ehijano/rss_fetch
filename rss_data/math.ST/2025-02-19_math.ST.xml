<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Feb 2025 05:01:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Stability Bounds for Smooth Optimal Transport Maps and their Statistical Implications</title>
      <link>https://arxiv.org/abs/2502.12326</link>
      <description>arXiv:2502.12326v1 Announce Type: new 
Abstract: We study estimators of the optimal transport (OT) map between two probability distributions. We focus on plugin estimators derived from the OT map between estimates of the underlying distributions. We develop novel stability bounds for OT maps which generalize those in past work, and allow us to reduce the problem of optimally estimating the transport map to that of optimally estimating densities in the Wasserstein distance. In contrast, past work provided a partial connection between these problems and relied on regularity theory for the Monge-Ampere equation to bridge the gap, a step which required unnatural assumptions to obtain sharp guarantees. We also provide some new insights into the connections between stability bounds which arise in the analysis of plugin estimators and growth bounds for the semi-dual functional which arise in the analysis of Brenier potential-based estimators of the transport map. We illustrate the applicability of our new stability bounds by revisiting the smooth setting studied by Manole et al., analyzing two of their estimators under more general conditions. Critically, our bounds do not require smoothness or boundedness assumptions on the underlying measures. As an illustrative application, we develop and analyze a novel tuning parameter-free estimator for the OT map between two strongly log-concave distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12326v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sivaraman Balakrishnan, Tudor Manole</dc:creator>
    </item>
    <item>
      <title>Unsupervised optimal deep transfer learning for classification under general conditional shift</title>
      <link>https://arxiv.org/abs/2502.12729</link>
      <description>arXiv:2502.12729v1 Announce Type: new 
Abstract: Classifiers trained solely on labeled source data may yield misleading results when applied to unlabeled target data drawn from a different distribution. Transfer learning can rectify this by transferring knowledge from source to target data, but its effectiveness frequently relies on stringent assumptions, such as label shift. In this paper, we introduce a novel General Conditional Shift (GCS) assumption, which encompasses label shift as a special scenario. Under GCS, we demonstrate that both the target distribution and the shift function are identifiable. To estimate the conditional probabilities ${\bm\eta}_P$ for source data, we propose leveraging deep neural networks (DNNs). Subsequent to transferring the DNN estimator, we estimate the target label distribution ${\bm\pi}_Q$ utilizing a pseudo-maximum likelihood approach. Ultimately, by incorporating these estimates and circumventing the need to estimate the shift function, we construct our proposed Bayes classifier. We establish concentration bounds for our estimators of both ${\bm\eta}_P$ and ${\bm\pi}_Q$ in terms of the intrinsic dimension of ${\bm\eta}_P$ . Notably, our DNN-based classifier achieves the optimal minimax rate, up to a logarithmic factor. A key advantage of our method is its capacity to effectively combat the curse of dimensionality when ${\bm\eta}_P$ exhibits a low-dimensional structure. Numerical simulations, along with an analysis of an Alzheimer's disease dataset, underscore its exceptional performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12729v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junjun Lang, Yukun Liu</dc:creator>
    </item>
    <item>
      <title>Existence of Direct Density Ratio Estimators</title>
      <link>https://arxiv.org/abs/2502.12738</link>
      <description>arXiv:2502.12738v1 Announce Type: new 
Abstract: Many two-sample problems call for a comparison of two distributions from an exponential family. Density ratio estimation methods provide ways to solve such problems through direct estimation of the differences in natural parameters. The term direct indicates that one avoids estimating both marginal distributions. In this context, we consider the Kullback--Leibler Importance Estimation Procedure (KLIEP), which has been the subject of recent work on differential networks. Our main result shows that the existence of the KLIEP estimator is characterized by whether the average sufficient statistic for one sample belongs to the convex hull of the set of all sufficient statistics for data points in the second sample. For high-dimensional problems it is customary to regularize the KLIEP loss by adding the product of a tuning parameter and a norm of the vector of parameter differences. We show that the existence of the regularized KLIEP estimator requires the tuning parameter to be no less than the dual norm-based distance between the average sufficient statistic and the convex hull. The implications of these existence issues are explored in applications to differential network analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12738v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erika Banzato, Mathias Drton, Kian Saraf-Poor, Hongjian Shi</dc:creator>
    </item>
    <item>
      <title>Simpson's Paradox with Any Given Number of Factors</title>
      <link>https://arxiv.org/abs/2502.12864</link>
      <description>arXiv:2502.12864v1 Announce Type: new 
Abstract: Simpson's Paradox is a well-known phenomenon in statistical science, where the relationship between the response variable $X$ and a certain explanatory factor of interest $A$ reverses when an additional factor $B_1$ is considered. This paper explores the extension of Simpson's Paradox to any given number $n$ of factors, referred to as the $n$-factor Simpson's Paradox. We first provide a rigorous definition of the $n$-factor Simpson's Paradox, then demonstrate the existence of a probability distribution through a geometric construction. Specifically, we show that for any positive integer $n$, it is possible to construct a probability distribution in which the conclusion about the effect of $A$ on $X$ reverses each time an additional factor $B_i$ is introduced for $i=1,...,n$. A detailed example for $n = 3$ illustrates the construction. Our results highlight that, contrary to the intuition that more data leads to more accurate inferences, the inclusion of additional factors can repeatedly reverse conclusions, emphasizing the complexity of statistical inference in the presence of multiple confounding variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12864v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guisheng Dai, Weizhen Wang</dc:creator>
    </item>
    <item>
      <title>An Iterative Block Matrix Inversion (IBMI) Algorithm for Symmetric Positive Definite Matrices with Applications to Covariance Matrices</title>
      <link>https://arxiv.org/abs/2502.06377</link>
      <description>arXiv:2502.06377v1 Announce Type: cross 
Abstract: Obtaining the inverse of a large symmetric positive definite matrix $\mathcal{A}\in\mathbb{R}^{p\times p}$ is a continual challenge across many mathematical disciplines. The computational complexity associated with direct methods can be prohibitively expensive, making it infeasible to compute the inverse. In this paper, we present a novel iterative algorithm (IBMI), which is designed to approximate the inverse of a large, dense, symmetric positive definite matrix. The matrix is first partitioned into blocks, and an iterative process using block matrix inversion is repeated until the matrix approximation reaches a satisfactory level of accuracy. We demonstrate that the two-block, non-overlapping approach converges for any positive definite matrix, while numerical results provide strong evidence that the multi-block, overlapping approach also converges for such matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06377v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ann Paterson, Jennifer Pestana, Victorita Dolean Maini</dc:creator>
    </item>
    <item>
      <title>Hallucinations are inevitable but statistically negligible</title>
      <link>https://arxiv.org/abs/2502.12187</link>
      <description>arXiv:2502.12187v1 Announce Type: cross 
Abstract: Hallucinations, a phenomenon where a language model (LM) generates nonfactual content, pose a significant challenge to the practical deployment of LMs. While many empirical methods have been proposed to mitigate hallucinations, a recent study established a computability-theoretic result showing that any LM will inevitably generate hallucinations on an infinite set of inputs, regardless of the quality and quantity of training datasets and the choice of the language model architecture and training and inference algorithms. Although the computability-theoretic result may seem pessimistic, its significance in practical viewpoints has remained unclear. In contrast, we present a positive theoretical result from a probabilistic perspective. Specifically, we prove that hallucinations can be made statistically negligible, provided that the quality and quantity of the training data are sufficient. Interestingly, our positive result coexists with the computability-theoretic result, implying that while hallucinations on an infinite set of inputs cannot be entirely eliminated, their probability can always be reduced by improving algorithms and training data. By evaluating the two seemingly contradictory results through the lens of information theory, we argue that our probability-theoretic positive result better reflects practical considerations than the computability-theoretic negative result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12187v1</guid>
      <category>cs.CL</category>
      <category>cs.FL</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Atsushi Suzuki, Yulan He, Feng Tian, Zhongyuan Wang</dc:creator>
    </item>
    <item>
      <title>A Simplified and Numerically Stable Approach to the BG/NBD Churn Prediction model</title>
      <link>https://arxiv.org/abs/2502.12912</link>
      <description>arXiv:2502.12912v1 Announce Type: cross 
Abstract: This study extends the BG/NBD churn probability model, addressing its limitations in industries where customer behaviour is often influenced by seasonal events and possibly high purchase counts. We propose a modified definition of churn, considering a customer to have churned if they make no purchases within M days. Our contribution is twofold: First, we simplify the general equation for the specific case of zero purchases within M days. Second, we derive an alternative expression using numerical techniques to mitigate numerical overflow or underflow issues. This approach provides a more practical and robust method for predicting customer churn in industries with irregular purchase patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12912v1</guid>
      <category>stat.OT</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dylan Zammit, Christopher Zerafa</dc:creator>
    </item>
    <item>
      <title>Asymptotic Optimism of Random-Design Linear and Kernel Regression Models</title>
      <link>https://arxiv.org/abs/2502.12999</link>
      <description>arXiv:2502.12999v1 Announce Type: cross 
Abstract: We derived the closed-form asymptotic optimism of linear regression models under random designs, and generalizes it to kernel ridge regression. Using scaled asymptotic optimism as a generic predictive model complexity measure, we studied the fundamental different behaviors of linear regression model, tangent kernel (NTK) regression model and three-layer fully connected neural networks (NN). Our contribution is two-fold: we provided theoretical ground for using scaled optimism as a model predictive complexity measure; and we show empirically that NN with ReLUs behaves differently from kernel models under this measure. With resampling techniques, we can also compute the optimism for regression models with real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12999v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hengrui Luo, Yunzhang Zhu</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Private Learning in Linear Contextual Bandits</title>
      <link>https://arxiv.org/abs/2502.13115</link>
      <description>arXiv:2502.13115v1 Announce Type: cross 
Abstract: We analyze the problem of private learning in generalized linear contextual bandits. Our approach is based on a novel method of re-weighted regression, yielding an efficient algorithm with regret of order $\sqrt{T}+\frac{1}{\alpha}$ and $\sqrt{T}/\alpha$ in the joint and local model of $\alpha$-privacy, respectively. Further, we provide near-optimal private procedures that achieve dimension-independent rates in private linear models and linear contextual bandits. In particular, our results imply that joint privacy is almost "for free" in all the settings we consider, partially addressing the open problem posed by Azize and Basu (2024).</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13115v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fan Chen, Jiachun Li, Alexander Rakhlin, David Simchi-Levi</dc:creator>
    </item>
    <item>
      <title>Inference on testing the number of spikes in a high-dimensional generalized spiked Fisher matrix</title>
      <link>https://arxiv.org/abs/2401.03622</link>
      <description>arXiv:2401.03622v2 Announce Type: replace 
Abstract: The spiked Fisher matrix is a significant topic for two-sample problems in multivariate statistical inference. This paper is dedicated to testing the number of spikes in a high-dimensional generalized spiked Fisher matrix that relaxes the Gaussian population assumption and the diagonal constraints on the population covariance matrices. First, we propose a general test statistic predicated on partial linear spectral statistics to test the number of spikes, then establish the central limit theorem (CLT) for this statistic under the null hypothesis. Second, we apply the CLT to address two statistical problems: variable selection in high-dimensional linear regression and change point detection. For each test problem, we construct new statistics and derive their asymptotic distributions under the null hypothesis. Finally, simulations and empirical analysis are conducted to demonstrate the remarkable effectiveness and generality of our proposed methods across various scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03622v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Wang, Dandan Jiang</dc:creator>
    </item>
    <item>
      <title>Distribution of singular values in large sample cross-covariance matrices</title>
      <link>https://arxiv.org/abs/2502.05254</link>
      <description>arXiv:2502.05254v2 Announce Type: replace 
Abstract: For two large matrices ${\mathbf X}$ and ${\mathbf Y}$ with Gaussian i.i.d.\ entries and dimensions $T\times N_X$ and $T\times N_Y$, respectively, we derive the probability distribution of the singular values of $\mathbf{X}^T \mathbf{Y}$ in different parameter regimes. This extends the Marchenko-Pastur result for the distribution of eigenvalues of empirical sample covariance matrices to singular values of empirical cross-covariances. Our results will help to establish statistical significance of cross-correlations in many data-science applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05254v2</guid>
      <category>math.ST</category>
      <category>cond-mat.dis-nn</category>
      <category>physics.data-an</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arabind Swain, Sean Alexander Ridout, Ilya Nemenman</dc:creator>
    </item>
    <item>
      <title>Invariant Subspace Decomposition</title>
      <link>https://arxiv.org/abs/2404.09962</link>
      <description>arXiv:2404.09962v2 Announce Type: replace-cross 
Abstract: We consider the task of predicting a response Y from a set of covariates X in settings where the conditional distribution of Y given X changes over time. For this to be feasible, assumptions on how the conditional distribution changes over time are required. Existing approaches assume, for example, that changes occur smoothly over time so that short-term prediction using only the recent past becomes feasible. To additionally exploit observations further in the past, we propose a novel invariance-based framework for linear conditionals, called Invariant Subspace Decomposition (ISD), that splits the conditional distribution into a time-invariant and a residual time-dependent component. As we show, this decomposition can be utilized both for zero-shot and time-adaptation prediction tasks, that is, settings where either no or a small amount of training data is available at the time points we want to predict Y at, respectively. We propose a practical estimation procedure, which automatically infers the decomposition using tools from approximate joint matrix diagonalization. Furthermore, we provide finite sample guarantees for the proposed estimator and demonstrate empirically that it indeed improves on approaches that do not use the additional invariant structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09962v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Margherita Lazzaretto, Jonas Peters, Niklas Pfister</dc:creator>
    </item>
    <item>
      <title>Tropical combinatorics of max-linear Bayesian networks</title>
      <link>https://arxiv.org/abs/2411.10394</link>
      <description>arXiv:2411.10394v3 Announce Type: replace-cross 
Abstract: A polytrope is a tropical polyhedron that is also classically convex. We study the tropical combinatorial types of polytropes associated to weighted directed acyclic graphs (DAGs). This family of polytropes arises in algebraic statistics when describing the model class of max-linear Bayesian networks. We show how the edge weights of a network directly relate to the facet structure of the corresponding polytrope. We also give a classification of polytropes from weighted DAGs at different levels of equivalence. These results give insight on the statistical problem of identifiability for a max-linear Bayesian network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10394v3</guid>
      <category>math.CO</category>
      <category>math.AG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlos Am\'endola, Kamillo Ferry</dc:creator>
    </item>
    <item>
      <title>Gradient Equilibrium in Online Learning: Theory and Applications</title>
      <link>https://arxiv.org/abs/2501.08330</link>
      <description>arXiv:2501.08330v3 Announce Type: replace-cross 
Abstract: We present a new perspective on online learning that we refer to as gradient equilibrium: a sequence of iterates achieves gradient equilibrium if the average of gradients of losses along the sequence converges to zero. In general, this condition is not implied by, nor implies, sublinear regret. It turns out that gradient equilibrium is achievable by standard online learning methods such as gradient descent and mirror descent with constant step sizes (rather than decaying step sizes, as is usually required for no regret). Further, as we show through examples, gradient equilibrium translates into an interpretable and meaningful property in online prediction problems spanning regression, classification, quantile estimation, and others. Notably, we show that the gradient equilibrium framework can be used to develop a debiasing scheme for black-box predictions under arbitrary distribution shift, based on simple post hoc online descent updates. We also show that post hoc gradient updates can be used to calibrate predicted quantiles under distribution shift, and that the framework leads to unbiased Elo scores for pairwise preference prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08330v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasios N. Angelopoulos, Michael I. Jordan, Ryan J. Tibshirani</dc:creator>
    </item>
    <item>
      <title>False Discovery Rate Control via Frequentist-assisted Horseshoe</title>
      <link>https://arxiv.org/abs/2502.05460</link>
      <description>arXiv:2502.05460v2 Announce Type: replace-cross 
Abstract: The horseshoe prior, a widely used handy alternative to the spike-and-slab prior, has proven to be an exceptional default global-local shrinkage prior in Bayesian inference and machine learning. However, designing tests with frequentist false discovery rate (FDR) control using the horseshoe prior or the general class of global-local shrinkage priors remains an open problem. In this paper, we propose a frequentist-assisted horseshoe procedure that not only resolves this long-standing FDR control issue for the high dimensional normal means testing problem but also exhibits satisfactory finite-sample FDR control under any desired nominal level for both large-scale multiple independent and correlated tests. We carry out the frequentist-assisted horseshoe procedure in an easy and intuitive way by using the minimax estimator of the global parameter of the horseshoe prior while maintaining the remaining full Bayes vanilla horseshoe structure. The results of both intensive simulations under different sparsity levels, and real-world data demonstrate that the frequentist-assisted horseshoe procedure consistently achieves robust finite-sample FDR control. Existing frequentist or Bayesian FDR control procedures can lose finite-sample FDR control in a variety of common sparse cases. Based on the intimate relationship between the minimax estimation and the level of FDR control discovered in this work, we point out potential generalizations to achieve FDR control for both more complicated models and the general global-local shrinkage prior family.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05460v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiaoyu Liang, Zihan Zhu, Ziang Fu, Michael Evans</dc:creator>
    </item>
  </channel>
</rss>
