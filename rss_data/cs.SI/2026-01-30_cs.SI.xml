<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SI</link>
    <description>cs.SI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 Jan 2026 05:00:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>SMART: A Social Movement Analysis &amp; Reasoning Tool with Case Studies on #MeToo and #BlackLivesMatter</title>
      <link>https://arxiv.org/abs/2601.20986</link>
      <description>arXiv:2601.20986v1 Announce Type: new 
Abstract: Social movements supporting the UN's Sustainable Development Goals (SDGs) play a vital role in improving human lives. If journalists were aware of the relationship between social movements and external events, they could provide more precise, time-sensitive reporting about movement issues and SDGs. Our SMART system achieves this goal by collecting data from multiple sources, extracting emotions on various themes, and then using a transformer-based forecasting engine (DEEP) to predict quantity and intensity of emotions in future posts. This paper demonstrates SMART's Retrospective capabilities required by journalists via case studies analyzing social media discussions of the #MeToo and #BlackLivesMatter before and after the 2024 U.S. election. We create a novel 1-year dataset which we will release upon publication. It contains over 2.7M Reddit posts and over 1M news articles. We show that SMART enables early detection of discourse shifts around key political events, providing journalists with actionable insights to inform editorial planning. SMART was developed through multiple interactions with a panel of over 20 journalists from a variety of news organizations over a 2-year period, including an author of this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20986v1</guid>
      <category>cs.SI</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Valerio La Gatta, Marco Postiglione, Jeremy Gilbert, Daniel W. Linna Jr., Morgan Manella Greenfield, Aaron Shaw, V. S. Subrahmanian</dc:creator>
    </item>
    <item>
      <title>Opinion Consensus Formation Among Networked Large Language Models</title>
      <link>https://arxiv.org/abs/2601.21540</link>
      <description>arXiv:2601.21540v1 Announce Type: new 
Abstract: Can classical consensus models predict the group behavior of large language models (LLMs)? We examine multi-round interactions among LLM agents through the DeGroot framework, where agents exchange text-based messages over diverse communication graphs. To track opinion evolution, we map each message to an opinion score via sentiment analysis. We find that agents typically reach consensus and the disagreement between the agents decays exponentially. However, the limiting opinion departs from DeGroot's network-centrality-weighted forecast. The consensus between LLM agents turns out to be largely insensitive to initial conditions and instead depends strongly on the discussion subject and inherent biases. Nevertheless, transient dynamics align with classical graph theory and the convergence rate of opinions is closely related to the second-largest eigenvalue of the graph's combination matrix. Together, these findings can be useful for LLM-driven social-network simulations and the design of resource-efficient multi-agent LLM applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21540v1</guid>
      <category>cs.SI</category>
      <category>cs.MA</category>
      <category>eess.SP</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iris Yazici, Mert Kayaalp, Stefan Taga, Ali H. Sayed</dc:creator>
    </item>
    <item>
      <title>Community detection in network using Szegedy quantum walk</title>
      <link>https://arxiv.org/abs/2601.21152</link>
      <description>arXiv:2601.21152v1 Announce Type: cross 
Abstract: In a network, the vertices with similar characteristics construct communities. The vertices in a community are well-connected. Detecting the communities in a network is a challenging and important problem in the theory of complex networks. One approach to solve this problem uses the classical random walks on the graphs. In quantum computing, quantum walks are the quantum mechanical counterparts of classical random walks. In this article, we employ a variant of Szegedy's quantum walk to develop a procedure for discovering the communities in networks. The limiting probability distribution of quantum walks assists us in determining the inclusion of a vertex in a community. We apply our procedure of community detection on a number of graphs and social networks, such as the relaxed caveman graph, $l$-partition graph, Karate club graph, dolphin's social network, etc.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21152v1</guid>
      <category>quant-ph</category>
      <category>cs.SI</category>
      <category>math.CO</category>
      <category>physics.soc-ph</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Samsur Rahaman, Supriyo Dutta</dc:creator>
    </item>
    <item>
      <title>From Vulnerable to Resilient: Examining Parent and Teen Perceptions on How to Respond to Unwanted Cybergrooming Advances</title>
      <link>https://arxiv.org/abs/2601.21518</link>
      <description>arXiv:2601.21518v1 Announce Type: cross 
Abstract: Cybergrooming is a form of online abuse that threatens teens' mental health and physical safety. Yet, most prior work has focused on detecting perpetrators' behaviors, leaving a limited understanding of how teens might respond to such unwanted advances. To address this gap, we conducted an online survey with 74 participants -- 51 parents and 23 teens -- who responded to simulated cybergrooming scenarios in two ways: responses that they think would make teens more vulnerable or resilient to unwanted sexual advances. Through a mixed-methods analysis, we identified four types of vulnerable responses (encouraging escalation, accepting an advance, displaying vulnerability, and negating risk concern) and four types of protective strategies (setting boundaries, directly declining, signaling risk awareness, and leveraging avoidance techniques). As the cybergrooming risk escalated, both vulnerable responses and protective strategies showed a corresponding progression. This study contributes a teen-centered understanding of cybergrooming, a labeled dataset, and a stage-based taxonomy of perceived protective strategies, while offering implications for educational programs and sociotechnical interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21518v1</guid>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3790949</arxiv:DOI>
      <dc:creator>Xinyi Zhang, Mamtaj Akter, Heajun An, Minqian Liu, Qi Zhang, Lifu Huang, Jin-Hee Cho, Pamela J. Wisniewski, Sang Won Lee</dc:creator>
    </item>
    <item>
      <title>Toward Culturally Aligned LLMs through Ontology-Guided Multi-Agent Reasoning</title>
      <link>https://arxiv.org/abs/2601.21700</link>
      <description>arXiv:2601.21700v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) increasingly support culturally sensitive decision making, yet often exhibit misalignment due to skewed pretraining data and the absence of structured value representations. Existing methods can steer outputs, but often lack demographic grounding and treat values as independent, unstructured signals, reducing consistency and interpretability. We propose OG-MAR, an Ontology-Guided Multi-Agent Reasoning framework. OG-MAR summarizes respondent-specific values from the World Values Survey (WVS) and constructs a global cultural ontology by eliciting relations over a fixed taxonomy via competency questions. At inference time, it retrieves ontology-consistent relations and demographically similar profiles to instantiate multiple value-persona agents, whose outputs are synthesized by a judgment agent that enforces ontology consistency and demographic proximity. Experiments on regional social-survey benchmarks across four LLM backbones show that OG-MAR improves cultural alignment and robustness over competitive baselines, while producing more transparent reasoning traces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21700v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <category>cs.MA</category>
      <category>cs.SI</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Wonduk Seo, Wonseok Choi, Junseo Koh, Juhyeon Lee, Hyunjin An, Minhyeong Yu, Jian Park, Qingshan Zhou, Seunghyun Lee, Yi Bu</dc:creator>
    </item>
    <item>
      <title>Moral Outrage Shapes Commitments Beyond Attention: Multimodal Moral Emotions on YouTube in Korea and the US</title>
      <link>https://arxiv.org/abs/2601.21815</link>
      <description>arXiv:2601.21815v1 Announce Type: cross 
Abstract: Understanding how media rhetoric shapes audience engagement is crucial in the attention economy. This study examines how moral emotional framing by mainstream news channels on YouTube influences user behavior across Korea and the United States. To capture the platform's multimodal nature, combining thumbnail images and video titles, we develop a multimodal moral emotion classifier by fine tuning a vision language model. The model is trained on human annotated multimodal datasets in both languages and applied to approximately 400,000 videos from major news outlets. We analyze engagement levels including views, likes, and comments, representing increasing degrees of commitment. The results show that other condemning rhetoric expressions of moral outrage that criticize others morally consistently increase all forms of engagement across cultures, with effects ranging from passive viewing to active commenting. These findings suggest that moral outrage is a particularly effective emotional strategy, attracting not only attention but also active participation. We discuss concerns about the potential misuse of other condemning rhetoric, as such practices may deepen polarization by reinforcing in group and out group divisions. To facilitate future research and ensure reproducibility, we publicly release our Korean and English multimodal moral emotion classifiers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21815v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.SI</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seongchan Park, Jaehong Kim, Hyeonseung Kim, Heejin Bin, Sue Moon, Wonjae Lee</dc:creator>
    </item>
    <item>
      <title>Industrialized Deception: The Collateral Effects of LLM-Generated Misinformation on Digital Ecosystems</title>
      <link>https://arxiv.org/abs/2601.21963</link>
      <description>arXiv:2601.21963v1 Announce Type: cross 
Abstract: Generative AI and misinformation research has evolved since our 2024 survey. This paper presents an updated perspective, transitioning from literature review to practical countermeasures. We report on changes in the threat landscape, including improved AI-generated content through Large Language Models (LLMs) and multimodal systems. Central to this work are our practical contributions: JudgeGPT, a platform for evaluating human perception of AI-generated news, and RogueGPT, a controlled stimulus generation engine for research. Together, these tools form an experimental pipeline for studying how humans perceive and detect AI-generated misinformation. Our findings show that detection capabilities have improved, but the competition between generation and detection continues. We discuss mitigation strategies including LLM-based detection, inoculation approaches, and the dual-use nature of generative AI. This work contributes to research addressing the adverse impacts of AI on information quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.21963v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.SI</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Loth, Martin Kappes, Marc-Oliver Pahl</dc:creator>
    </item>
    <item>
      <title>HetGCoT: Heterogeneous Graph-Enhanced Chain-of-Thought LLM Reasoning for Academic Question Answering</title>
      <link>https://arxiv.org/abs/2501.01203</link>
      <description>arXiv:2501.01203v3 Announce Type: replace 
Abstract: Academic question answering (QA) in heterogeneous scholarly networks presents unique challenges requiring both structural understanding and interpretable reasoning. While graph neural networks (GNNs) capture structured graph information and large language models (LLMs) demonstrate strong capabilities in semantic comprehension, current approaches lack integration at the reasoning level. We propose HetGCoT, a framework enabling LLMs to effectively leverage and learn information from graphs to reason interpretable academic QA results. Our framework introduces three technical contributions: (1) a framework that transforms heterogeneous graph structural information into LLM-processable reasoning chains, (2) an adaptive metapath selection mechanism identifying relevant subgraphs for specific queries, and (3) a multi-step reasoning strategy systematically incorporating graph contexts into the reasoning process. Experiments on OpenAlex and DBLP datasets show our approach outperforms all sota baselines. The framework demonstrates adaptability across different LLM architectures and applicability to various scholarly question answering tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01203v3</guid>
      <category>cs.SI</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.18653/v1/2025.findings-emnlp.864</arxiv:DOI>
      <dc:creator>Runsong Jia, Mengjia Wu, Ying Ding, Jie Lu, Yi Zhang</dc:creator>
    </item>
    <item>
      <title>Leader-driven or Leaderless: How Participation Structure Sustains Engagement and Shapes Narratives in Online Hate Communities</title>
      <link>https://arxiv.org/abs/2512.12441</link>
      <description>arXiv:2512.12441v4 Announce Type: replace 
Abstract: Extremist communities increasingly rely on social media to sustain and amplify divisive discourse. However, the relationship between their internal participation structures, audience engagement, and narrative expression remains underexplored. This study analyzes ten years of Facebook activity by hate groups related to the Israel-Palestine conflict, focusing on anti-Semitic and Islamophobic ideologies. Consistent with prior work, we find that higher participation centralization in online hate groups is associated with greater user engagement across hate ideologies, suggesting the role of key actors in sustaining group activity over time. Meanwhile, our narrative frame detection models--based on an eight-frame extremist taxonomy (e.g., dehumanization, violence justification)--reveal a clear contrast across hate ideologies: centralized Islamophobic groups employ more uniform messaging, while centralized anti-Semitic groups demonstrate greater framing diversity and topical breadth, potentially reflecting distinct historical trajectories and leader coordination patterns. Analysis of the inter-group network indicates that, although centralization and homophily are not clearly linked, ideological distinctions emerge: Islamophobic groups cluster tightly, whereas anti-Semitic groups remain more evenly connected. Overall, these findings clarify how participation structure may shape the dissemination pattern and resonance of extremist narratives online and provide a foundation for tailored strategies to disrupt or mitigate such discourse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12441v4</guid>
      <category>cs.SI</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rr. Nefriana, Muheng Yan, Rebecca Hwa, Yu-Ru Lin</dc:creator>
    </item>
    <item>
      <title>Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups</title>
      <link>https://arxiv.org/abs/2504.06160</link>
      <description>arXiv:2504.06160v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have been shown to demonstrate imbalanced biases against certain groups. However, the study of unprovoked targeted attacks by LLMs towards at-risk populations remains underexplored. Our paper presents three novel contributions: (1) the explicit evaluation of LLM-generated attacks on highly vulnerable mental health groups; (2) a network-based framework to study the propagation of relative biases; and (3) an assessment of the relative degree of stigmatization that emerges from these attacks. Our analysis of a recently released large-scale bias audit dataset reveals that mental health entities occupy central positions within attack narrative networks, as revealed by a significantly higher mean centrality of closeness (p-value = 4.06e-10) and dense clustering (Gini coefficient = 0.7). Drawing from an established stigmatization framework, our analysis indicates increased labeling components for mental health disorder-related targets relative to initial targets in generation chains. Taken together, these insights shed light on the structural predilections of large language models to heighten harmful discourse and highlight the need for suitable approaches for mitigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06160v4</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Second Conference on Language Modeling (COLM 2025)</arxiv:journal_reference>
      <dc:creator>Rijul Magu, Arka Dutta, Sean Kim, Ashiqur R. KhudaBukhsh, Munmun De Choudhury</dc:creator>
    </item>
    <item>
      <title>The Price of Uncertainty for Social Consensus</title>
      <link>https://arxiv.org/abs/2508.17557</link>
      <description>arXiv:2508.17557v3 Announce Type: replace-cross 
Abstract: How hard is it to achieve consensus in a social network under uncertainty? In this paper we model this problem as a social graph of agents where each vertex is initially colored red or blue. The goal of the agents is to achieve consensus, which is when the colors of all agents align. Agents attempt to do this locally through steps in which an agent changes their color to the color of the majority of their neighbors. In real life, agents may not know exactly how many of their neighbors are red or blue, which introduces uncertainty into this process. Modeling uncertainty as perturbations of relative magnitude $1+\varepsilon$ to these color neighbor counts, we show that even small values of $\varepsilon$ greatly hinder the ability to achieve consensus in a social network. We prove theoretically tight upper and lower bounds on the \emph{price of uncertainty}, a metric defined in previous work by Balcan et al. to quantify the effect of uncertainty in network games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17557v3</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>cs.SI</category>
      <pubDate>Fri, 30 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunzhe Bai, Alec Sun</dc:creator>
    </item>
  </channel>
</rss>
