<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 Oct 2025 01:45:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On Logit Weibull Manifold</title>
      <link>https://arxiv.org/abs/2510.03299</link>
      <description>arXiv:2510.03299v1 Announce Type: new 
Abstract: In this work, it is shown that there is no potential function on the Weilbull statistical manifold. However, from the two-parameter Weibull model we can extract a model with a potential function called the logit model. On this logit model, there is a completely integrable Hamiltonian gradient system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03299v1</guid>
      <category>math.ST</category>
      <category>math.DG</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Prosper Rosaire Mama Assandje, Joseph Dongho, Thomas Bouetou Bouetou</dc:creator>
    </item>
    <item>
      <title>Identification in source apportionment using geometry</title>
      <link>https://arxiv.org/abs/2510.03616</link>
      <description>arXiv:2510.03616v2 Announce Type: new 
Abstract: Source apportionment analysis, which aims to quantify the attribution of observed concentrations of multiple air pollutants to specific sources, can be formulated as a non-negative matrix factorization (NMF) problem. However, NMF is non-unique and typically relies on unverifiable assumptions such as sparsity and uninterpretable scalings. In this manuscript, we establish identifiability of the source attribution percentage matrix under much weaker and more realistic conditions. We introduce the population-level estimand for this matrix, and show that it is scale-invariant and identifiable even when the NMF factors are not. Viewing the data as a point cloud in a conical hull, we show that a geometric estimator of the source attribution percentage matrix is consistent without any sparsity or parametric distributional assumptions, and while accommodating spatio-temporal dependence. Numerical experiments corroborate the theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03616v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bora Jin, Abhirup Datta</dc:creator>
    </item>
    <item>
      <title>Mean and quantile regression in the copula setting: properties, sharp bounds and a note on estimation</title>
      <link>https://arxiv.org/abs/2510.03804</link>
      <description>arXiv:2510.03804v1 Announce Type: new 
Abstract: Driven by the interest on how uniformity of marginal distributions propa\-gates to properties of regression functions, in this contribution we tackle the following questions: Given a $(d-1)$-dimensional random vector $\textbf{X}$ and a random variable $Y$ such that all univariate marginals of $(\textbf{X},Y)$ are uniformly distributed on $[0,1]$, how large can the average absolute deviation of the mean and the quantile regression function of $Y$ given $\textbf{X}$ from the value $\frac{1}{2}$ be, and how much mass may sets with large deviation have? We answer these questions by deriving sharp inequalities, both in the mean as well as in the quantile setting, and sketch some cautionary consequences to nowadays quite popular pair copula constructions involving the so-called simplifying assumption. Rounding off our results, working with the so-called empirical checkerboard estimator in the bivariate setting, we show strong consistency for both regression types and illustrate the speed of convergence in terms of a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03804v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henrik Kaiser, Wolfgang Trutschnig</dc:creator>
    </item>
    <item>
      <title>Optimality and computational barriers in variable selection under dependence</title>
      <link>https://arxiv.org/abs/2510.03990</link>
      <description>arXiv:2510.03990v1 Announce Type: new 
Abstract: We study the optimal sample complexity of variable selection in linear regression under general design covariance, and show that subset selection is optimal while under standard complexity assumptions, efficient algorithms for this problem do not exist. Specifically, we analyze the variable selection problem and provide the optimal sample complexity with exact dependence on the problem parameters for both known and unknown sparsity settings. Moreover, we establish a sample complexity lower bound for any efficient estimator, highlighting a gap between the statistical efficiency achievable by combinatorial algorithms (such as subset selection) compared to efficient algorithms (such as those based on convex programming). The proofs rely on a finite-sample analysis of an information criterion estimator, which may be of independent interest. Our results emphasize the optimal position of subset selection, the critical role played by restricted eigenvalues, and characterize the statistical-computational trade-off in high-dimensional variable selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03990v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ming Gao, Bryon Aragam</dc:creator>
    </item>
    <item>
      <title>Optimal estimation of a factorizable density using diffusion models with ReLU neural networks</title>
      <link>https://arxiv.org/abs/2510.03994</link>
      <description>arXiv:2510.03994v1 Announce Type: new 
Abstract: This paper investigates the score-based diffusion models for density estimation when the target density admits a factorizable low-dimensional nonparametric structure. To be specific, we show that when the log density admits a $d^*$-way interaction model with $\beta$-smooth components, the vanilla diffusion model, which uses a fully connected ReLU neural network for score matching, can attain optimal $n^{-\beta/(2\beta+d^*)}$ statistical rate of convergence in total variation distance. This is, to the best of our knowledge, the first in the literature showing that diffusion models with standard configurations can adapt to the low-dimensional factorizable structures. The main challenge is that the low-dimensional factorizable structure no longer holds for most of the diffused timesteps, and it is very challenging to show that these diffused score functions can be well approximated without a significant increase in the number of network parameters. Our key insight is to demonstrate that the diffused score functions can be decomposed into a composition of either super-smooth or low-dimensional components, leading to a new approximation error analysis of ReLU neural networks with respect to the diffused score function. The rate of convergence under the 1-Wasserstein distance is also derived with a slight modification of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03994v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianqing Fan, Yihong Gu, Ximing Li</dc:creator>
    </item>
    <item>
      <title>Asymptotic distributions of four linear hypotheses test statistics under generalized spiked model</title>
      <link>https://arxiv.org/abs/2510.04185</link>
      <description>arXiv:2510.04185v1 Announce Type: new 
Abstract: In this paper, we establish the Central Limit Theorem (CLT) for linear spectral statistics (LSSs) of large-dimensional generalized spiked sample covariance matrices, where the spiked eigenvalues may be either bounded or diverge to infinity. Building upon this theorem, we derive the asymptotic distributions of linear hypothesis test statistics under the generalized spiked model, including Wilks' likelihood ratio test statistic U, the Lawley-Hotelling trace test statistic W, and the Bartlett-Nanda-Pillai trace test statistic V. Due to the complexity of the test functions, explicit solutions for the contour integrals in our calculations are generally intractable. To address this, we employ Taylor series expansions to approximate the theoretical results in the asymptotic regime. We also derive asymptotic power functions for three test criteria above, and make comparisons with Roy's largest root test under specific scenarios. Finally, numerical simulations are conducted to validate the accuracy of our asymptotic approximations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04185v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhijun Liu, Jiang Hu, Zhidong Bai, Zhihui Lv</dc:creator>
    </item>
    <item>
      <title>Statistical inference using debiased group graphical lasso for multiple sparse precision matrices</title>
      <link>https://arxiv.org/abs/2510.04683</link>
      <description>arXiv:2510.04683v1 Announce Type: new 
Abstract: Debiasing group graphical lasso estimates enables statistical inference when multiple Gaussian graphical models share a common sparsity pattern. We analyze the estimation properties of group graphical lasso, establishing convergence rates and model selection consistency under irrepresentability conditions. Based on these results, we construct debiased estimators that are asymptotically Gaussian, allowing hypothesis testing for linear combinations of precision matrix entries across populations. We also investigate regimes where irrepresentibility conditions does not hold, showing that consistency can still be attained in moderately high-dimensional settings. Simulation studies confirm the theoretical results, and applications to real datasets demonstrate the practical utility of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04683v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sayan Ranjan Bhowal, Debashis Paul, Gopal K Basak, Samarjit Das</dc:creator>
    </item>
    <item>
      <title>Deconvolution of Arbitrary Distribution Functions and Densities</title>
      <link>https://arxiv.org/abs/2510.04742</link>
      <description>arXiv:2510.04742v1 Announce Type: new 
Abstract: In this article we recover the distribution function (and possible density) of an arbitrary random variable that is subject to an additive measurement error. This problem is also known as deconvolution and has a long tradition in mathematics. We show that the model under consideration always can be transformed to a model with a symmetric error variable, whose characteristic function has its values in the unit interval. As a consequence, the characteristic function of the target variable turns out as the limit of a geometric series. By truncation of this series, an approximation for the associated distribution function (and density) is established. The convergence properties of these approximations are examined in detail across diverse setups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04742v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henrik Kaiser</dc:creator>
    </item>
    <item>
      <title>Pivotal CLTs for Pseudolikelihood via Conditional Centering in Dependent Random Fields</title>
      <link>https://arxiv.org/abs/2510.04972</link>
      <description>arXiv:2510.04972v1 Announce Type: new 
Abstract: In this paper, we study fluctuations of conditionally centered statistics of the form $$N^{-1/2}\sum_{i=1}^N c_i(g(\sigma_i)-\mathbb{E}_N[g(\sigma_i)|\sigma_j,j\neq i])$$ where $(\sigma_1,\ldots ,\sigma_N)$ are sampled from a dependent random field, and $g$ is some bounded function. Our first main result shows that under weak smoothness assumptions on the conditional means (which cover both sparse and dense interactions), the above statistic converges to a Gaussian \emph{scale mixture} with a random scale determined by a \emph{quadratic variance} and an \emph{interaction component}. We also show that under appropriate studentization, the limit becomes a pivotal Gaussian. We leverage this theory to develop a general asymptotic framework for maximum pseudolikelihood (MPLE) inference in dependent random fields. We apply our results to Ising models with pairwise as well as higher-order interactions and exponential random graph models (ERGMs). In particular, we obtain a joint central limit theorem for the inverse temperature and magnetization parameters via the joint MPLE (to our knowledge, the first such result in dense, irregular regimes), and we derive conditionally centered edge CLTs and marginal MPLE CLTs for ERGMs without restricting to the ``sub-critical" region. Our proof is based on a method of moments approach via combinatorial decision-tree pruning, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04972v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nabarun Deb</dc:creator>
    </item>
    <item>
      <title>Structural Identifiability of Graphical Continuous Lyapunov Models</title>
      <link>https://arxiv.org/abs/2510.04985</link>
      <description>arXiv:2510.04985v1 Announce Type: new 
Abstract: We prove two characterizations of model equivalence of acyclic graphical continuous Lyapunov models (GCLMs) with uncorrelated noise. The first result shows that two graphs are model equivalent if and only if they have the same skeleton and equivalent induced 4-node subgraphs. We also give a transformational characterization via structured edge reversals. The two theorems are Lyapunov analogues of celebrated results for Bayesian networks by Verma and Pearl, and Chickering, respectively. Our results have broad consequences for the theory of causal inference of GCLMs. First, we find that model equivalence classes of acyclic GCLMs refine the corresponding classes of Bayesian networks. Furthermore, we obtain polynomial-time algorithms to test model equivalence and structural identifiability of given directed acyclic graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04985v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlos Am\'endola, Tobias Boege, Benjamin Hollering, Pratik Misra</dc:creator>
    </item>
    <item>
      <title>Quantile-Scaled Bayesian Optimization Using Rank-Only Feedback</title>
      <link>https://arxiv.org/abs/2510.03277</link>
      <description>arXiv:2510.03277v1 Announce Type: cross 
Abstract: Bayesian Optimization (BO) is widely used for optimizing expensive black-box functions, particularly in hyperparameter tuning. However, standard BO assumes access to precise objective values, which may be unavailable, noisy, or unreliable in real-world settings where only relative or rank-based feedback can be obtained. In this study, we propose Quantile-Scaled Bayesian Optimization (QS-BO), a principled rank-based optimization framework. QS-BO converts ranks into heteroscedastic Gaussian targets through a quantile-scaling pipeline, enabling the use of Gaussian process surrogates and standard acquisition functions without requiring explicit metric scores. We evaluate QS-BO on synthetic benchmark functions, including one- and two-dimensional nonlinear functions and the Branin function, and compare its performance against Random Search. Results demonstrate that QS-BO consistently achieves lower objective values and exhibits greater stability across runs. Statistical tests further confirm that QS-BO significantly outperforms Random Search at the 1\% significance level. These findings establish QS-BO as a practical and effective extension of Bayesian Optimization for rank-only feedback, with promising applications in preference learning, recommendation, and human-in-the-loop optimization where absolute metric values are unavailable or unreliable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03277v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tunde Fahd Egunjobi</dc:creator>
    </item>
    <item>
      <title>Robust and efficient estimation for the Generalized Extreme-Value distribution with application to flood frequency analysis in the UK</title>
      <link>https://arxiv.org/abs/2510.03338</link>
      <description>arXiv:2510.03338v1 Announce Type: cross 
Abstract: A common approach for modeling extremes, such as peak flow or high temperatures, is the three-parameter Generalized Extreme-Value distribution. This is typically fit to extreme observations, here defined as maxima over disjoint blocks. This results in limited sample sizes and consequently, the use of classic estimators, such as the maximum likelihood estimator, may be inappropriate, as they are highly sensitive to outliers. To address these limitations, we propose a novel robust estimator based on the minimization of the density power divergence, controlled by a tuning parameter $\alpha$ that balances robustness and efficiency. When $\alpha = 0$, our estimator coincides with the maximum likelihood estimator; when $\alpha = 1$, it corresponds to the $L^2$ estimator, known for its robustness. We establish convenient theoretical properties of the proposed estimator, including its asymptotic normality and the boundedness of its influence function for $\alpha &gt; 0$. The practical efficiency of the method is demonstrated through empirical comparisons with the maximum likelihood estimator and other robust alternatives. Finally, we illustrate its relevance in a case study on flood frequency analysis in the UK and provide some general conclusions in Section 6.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03338v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Huet, Ilaria Prosdocimi</dc:creator>
    </item>
    <item>
      <title>Optimal Regularization Under Uncertainty: Distributional Robustness and Convexity Constraints</title>
      <link>https://arxiv.org/abs/2510.03464</link>
      <description>arXiv:2510.03464v1 Announce Type: cross 
Abstract: Regularization is a central tool for addressing ill-posedness in inverse problems and statistical estimation, with the choice of a suitable penalty often determining the reliability and interpretability of downstream solutions. While recent work has characterized optimal regularizers for well-specified data distributions, practical deployments are often complicated by distributional uncertainty and the need to enforce structural constraints such as convexity. In this paper, we introduce a framework for distributionally robust optimal regularization, which identifies regularizers that remain effective under perturbations of the data distribution. Our approach leverages convex duality to reformulate the underlying distributionally robust optimization problem, eliminating the inner maximization and yielding formulations that are amenable to numerical computation. We show how the resulting robust regularizers interpolate between memorization of the training distribution and uniform priors, providing insights into their behavior as robustness parameters vary. For example, we show how certain ambiguity sets, such as those based on the Wasserstein-1 distance, naturally induce regularity in the optimal regularizer by promoting regularizers with smaller Lipschitz constants. We further investigate the setting where regularizers are required to be convex, formulating a convex program for their computation and illustrating their stability with respect to distributional shifts. Taken together, our results provide both theoretical and computational foundations for designing regularizers that are reliable under model uncertainty and structurally constrained for robust deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03464v1</guid>
      <category>math.OC</category>
      <category>math.MG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oscar Leong, Eliza O'Reilly, Yong Sheng Soh</dc:creator>
    </item>
    <item>
      <title>Restrictions of PCBNs for integration-free computations</title>
      <link>https://arxiv.org/abs/2510.03518</link>
      <description>arXiv:2510.03518v1 Announce Type: cross 
Abstract: The pair-copula Bayesian Networks (PCBN) are graphical models composed of a directed acyclic graph (DAG) that represents (conditional) independence in a joint distribution. The nodes of the DAG are associated with marginal densities, and arcs are assigned with bivariate (conditional) copulas following a prescribed collection of parental orders. The choice of marginal densities and copulas is unconstrained. However, the simulation and inference of a PCBN model may necessitate possibly high-dimensional integration.
  We present the full characterization of DAGs that do not require any integration for density evaluation or simulations. Furthermore, we propose an algorithm that can find all possible parental orders that do not lead to (expensive) integration. Finally, we show the asymptotic normality of estimators of PCBN models using stepwise estimating equations. Such estimators can be computed effectively if the PCBN does not require integration. A simulation study shows the good finite-sample properties of our estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03518v1</guid>
      <category>stat.ME</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexis Derumigny, Niels Horsman, Dorota Kurowicka</dc:creator>
    </item>
    <item>
      <title>Transformed $\ell_1$ Regularizations for Robust Principal Component Analysis: Toward a Fine-Grained Understanding</title>
      <link>https://arxiv.org/abs/2510.03624</link>
      <description>arXiv:2510.03624v1 Announce Type: cross 
Abstract: Robust Principal Component Analysis (RPCA) aims to recover a low-rank structure from noisy, partially observed data that is also corrupted by sparse, potentially large-magnitude outliers. Traditional RPCA models rely on convex relaxations, such as nuclear norm and $\ell_1$ norm, to approximate the rank of a matrix and the $\ell_0$ functional (the number of non-zero elements) of another. In this work, we advocate a nonconvex regularization method, referred to as transformed $\ell_1$ (TL1), to improve both approximations. The rationale is that by varying the internal parameter of TL1, its behavior asymptotically approaches either $\ell_0$ or $\ell_1$. Since the rank is equal to the number of non-zero singular values and the nuclear norm is defined as their sum, applying TL1 to the singular values can approximate either the rank or the nuclear norm, depending on its internal parameter. We conduct a fine-grained theoretical analysis of statistical convergence rates, measured in the Frobenius norm, for both the low-rank and sparse components under general sampling schemes. These rates are comparable to those of the classical RPCA model based on the nuclear norm and $\ell_1$ norm. Moreover, we establish constant-order upper bounds on the estimated rank of the low-rank component and the cardinality of the sparse component in the regime where TL1 behaves like $\ell_0$, assuming that the respective matrices are exactly low-rank and exactly sparse. Extensive numerical experiments on synthetic data and real-world applications demonstrate that the proposed approach achieves higher accuracy than the classic convex model, especially under non-uniform sampling schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03624v1</guid>
      <category>stat.ML</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Zhao, Haoke Zhang, Jiayi Wang, Yifei Lou</dc:creator>
    </item>
    <item>
      <title>Can Linear Probes Measure LLM Uncertainty?</title>
      <link>https://arxiv.org/abs/2510.04108</link>
      <description>arXiv:2510.04108v1 Announce Type: cross 
Abstract: Effective Uncertainty Quantification (UQ) represents a key aspect for reliable deployment of Large Language Models (LLMs) in automated decision-making and beyond. Yet, for LLM generation with multiple choice structure, the state-of-the-art in UQ is still dominated by the naive baseline given by the maximum softmax score. To address this shortcoming, we demonstrate that taking a principled approach via Bayesian statistics leads to improved performance despite leveraging the simplest possible model, namely linear regression. More precisely, we propose to train multiple Bayesian linear models, each predicting the output of a layer given the output of the previous one. Based on the obtained layer-level posterior distributions, we infer the global uncertainty level of the LLM by identifying a sparse combination of distributional features, leading to an efficient UQ scheme. Numerical experiments on various LLMs show consistent improvement over state-of-the-art baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04108v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ramzi Dakhmouche, Adrien Letellier, Hossein Gorji</dc:creator>
    </item>
    <item>
      <title>Don't Pass$\mathtt{@}k$: A Bayesian Framework for Large Language Model Evaluation</title>
      <link>https://arxiv.org/abs/2510.04265</link>
      <description>arXiv:2510.04265v1 Announce Type: cross 
Abstract: Pass$@k$ is widely used to report performance for LLM reasoning, but it often yields unstable, misleading rankings, especially when the number of trials (samples) is limited and compute is constrained. We present a principled Bayesian evaluation framework that replaces Pass$@k$ and average accuracy over $N$ trials (avg$@N$) with posterior estimates of a model's underlying success probability and credible intervals, yielding stable rankings and a transparent decision rule for differences. Evaluation outcomes are modeled as categorical (not just 0/1) with a Dirichlet prior, giving closed-form expressions for the posterior mean and uncertainty of any weighted rubric and enabling the use of prior evidence when appropriate. Theoretically, under a uniform prior, the Bayesian posterior mean is order-equivalent to average accuracy (Pass$@1$), explaining its empirical robustness while adding principled uncertainty. Empirically, in simulations with known ground-truth success rates and on AIME'24/'25, HMMT'25, and BrUMO'25, the Bayesian/avg procedure achieves faster convergence and greater rank stability than Pass$@k$ and recent variants, enabling reliable comparisons at far smaller sample counts. The framework clarifies when observed gaps are statistically meaningful (non-overlapping credible intervals) versus noise, and it naturally extends to graded, rubric-based evaluations. Together, these results recommend replacing Pass$@k$ for LLM evaluation and ranking with a posterior-based, compute-efficient protocol that unifies binary and non-binary evaluation while making uncertainty explicit. Code is available at https://mohsenhariri.github.io/bayes-kit</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04265v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Hariri, Amirhossein Samandar, Michael Hinczewski, Vipin Chaudhary</dc:creator>
    </item>
    <item>
      <title>Learning Survival Models with Right-Censored Reporting Delays</title>
      <link>https://arxiv.org/abs/2510.04421</link>
      <description>arXiv:2510.04421v1 Announce Type: cross 
Abstract: Survival analysis is a statistical technique used to estimate the time until an event occurs. Although it is applied across a wide range of fields, adjusting for reporting delays under practical constraints remains a significant challenge in the insurance industry. Such delays render event occurrences unobservable when their reports are subject to right censoring. This issue becomes particularly critical when estimating hazard rates for newly enrolled cohorts with limited follow-up due to administrative censoring. Our study addresses this challenge by jointly modeling the parametric hazard functions of event occurrences and report timings. The joint probability distribution is marginalized over the latent event occurrence status. We construct an estimator for the proposed survival model and establish its asymptotic consistency. Furthermore, we develop an expectation-maximization algorithm to compute its estimates. Using these findings, we propose a two-stage estimation procedure based on a parametric proportional hazards model to evaluate observations subject to administrative censoring. Experimental results demonstrate that our method effectively improves the timeliness of risk evaluation for newly enrolled cohorts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04421v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuta Shikuri, Hironori Fujisawa</dc:creator>
    </item>
    <item>
      <title>Inverse Mixed-Integer Programming: Learning Constraints then Objective Functions</title>
      <link>https://arxiv.org/abs/2510.04455</link>
      <description>arXiv:2510.04455v1 Announce Type: cross 
Abstract: In mixed-integer linear programming, data-driven inverse optimization that learns the objective function and the constraints from observed data plays an important role in constructing appropriate mathematical models for various fields, including power systems and scheduling. However, to the best of our knowledge, there is no known method for learning both the objective functions and the constraints. In this paper, we propose a two-stage method for a class of problems where the objective function is expressed as a linear combination of functions and the constraints are represented by functions and thresholds. Specifically, our method first learns the constraints and then learns the objective function. On the theoretical side, we show the proposed method can solve inverse optimization problems in finite dataset, develop statistical learning theory in pseudometric spaces and sub-Gaussian distributions, and construct a statistical learning for inverse optimization. On the experimental side, we demonstrate that our method is practically applicable for scheduling problems formulated as integer linear programmings with up to 100 decision variables, which are typical in real-world settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04455v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Akira Kitaoka</dc:creator>
    </item>
    <item>
      <title>Two new approaches to multiple canonical correlation analysis for repeated measures data</title>
      <link>https://arxiv.org/abs/2510.04457</link>
      <description>arXiv:2510.04457v1 Announce Type: cross 
Abstract: In classical canonical correlation analysis (CCA), the goal is to determine the linear transformations of two random vectors into two new random variables that are most strongly correlated. Canonical variables are pairs of these new random variables, while canonical correlations are correlations between these pairs. In this paper, we propose and study two generalizations of this classical method:
  (1) Instead of two random vectors we study more complex data structures that appear in important applications. In these structures, there are $L$ features, each described by $p_l$ scalars, $1 \le l \le L$. We observe $n$ such objects over $T$ time points. We derive a suitable analog of the CCA for such data. Our approach relies on embeddings into Reproducing Kernel Hilbert Spaces, and covers several related data structures as well.
  (2) We develop an analogous approach for multidimensional random processes. In this case, the experimental units are multivariate continuous, square-integrable functions over a given interval. These functions are modeled as elements of a Hilbert space, so in this case, we define the multiple functional canonical correlation analysis, MFCCA.
  We justify our approaches by their application to two data sets and suitable large sample theory. We derive consistency rates for the related transformation and correlation estimators, and show that it is possible to relax two common assumptions on the compactness of the underlying cross-covariance operators and the independence of the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04457v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomasz G\'orecki, Miros{\l}aw Krzy\'sko, Felix Gnettner, Piotr Kokoszka</dc:creator>
    </item>
    <item>
      <title>Perspectives on Stochastic Localization</title>
      <link>https://arxiv.org/abs/2510.04460</link>
      <description>arXiv:2510.04460v1 Announce Type: cross 
Abstract: We survey different perspectives on the stochastic localization process of [Eld13], a powerful construction that has had many exciting recent applications in high-dimensional probability and algorithm design. Unlike prior surveys on this topic, our focus is on giving a self-contained presentation of all known alternative constructions of Eldan's stochastic localization, with an emphasis on connections between different constructions. Our hope is that by collecting these perspectives, some of which had primarily arisen within a particular community (e.g., probability theory, theoretical computer science, information theory, or machine learning), we can broaden the accessibility of stochastic localization, and ease its future use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04460v1</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bobby Shi, Kevin Tian, Matthew S. Zhang</dc:creator>
    </item>
    <item>
      <title>Gini-based Model Monitoring: A General Framework with an Application to Non-life Insurance Pricing</title>
      <link>https://arxiv.org/abs/2510.04556</link>
      <description>arXiv:2510.04556v1 Announce Type: cross 
Abstract: In a dynamic landscape where portfolios and environments evolve, maintaining the accuracy of pricing models is critical. To the best of our knowledge, this is the first study to systematically examine concept drift in non-life insurance pricing. We (i) provide an overview of the relevant literature and commonly used methodologies, clarify the distinction between virtual drift and concept drift, and explain their implications for long-run model performance; (ii) review and formalize common performance measures, including the Gini index and deviance loss, and articulate their interpretation; (iii) derive the asymptotic distribution of the Gini index, enabling valid inference and hypothesis testing; and (iv) present a standardized monitoring procedure that indicates when refitting is warranted. We illustrate the framework using a modified real-world portfolio with induced concept drift and discuss practical considerations and pitfalls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04556v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alexej Brauer, Paul Menzel</dc:creator>
    </item>
    <item>
      <title>When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set Updates</title>
      <link>https://arxiv.org/abs/2510.04769</link>
      <description>arXiv:2510.04769v1 Announce Type: cross 
Abstract: Many machine learning algorithms rely on iterative updates of uncertainty representations, ranging from variational inference and expectation-maximization, to reinforcement learning, continual learning, and multi-agent learning. In the presence of imprecision and ambiguity, credal sets -- closed, convex sets of probability distributions -- have emerged as a popular framework for representing imprecise probabilistic beliefs. Under such imprecision, many learning problems in imprecise probabilistic machine learning (IPML) may be viewed as processes involving successive applications of update rules on credal sets. This naturally raises the question of whether this iterative process converges to stable fixed points -- or, more generally, under what conditions on the updating mechanism such fixed points exist, and whether they can be attained. We provide the first analysis of this problem and illustrate our findings using Credal Bayesian Deep Learning as a concrete example. Our work demonstrates that incorporating imprecision into the learning process not only enriches the representation of uncertainty, but also reveals structural conditions under which stability emerges, thereby offering new insights into the dynamics of iterative learning under imprecision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04769v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michele Caprio, Siu Lun Chau, Krikamol Muandet</dc:creator>
    </item>
    <item>
      <title>ResCP: Reservoir Conformal Prediction for Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2510.05060</link>
      <description>arXiv:2510.05060v1 Announce Type: cross 
Abstract: Conformal prediction offers a powerful framework for building distribution-free prediction intervals for exchangeable data. Existing methods that extend conformal prediction to sequential data rely on fitting a relatively complex model to capture temporal dependencies. However, these methods can fail if the sample size is small and often require expensive retraining when the underlying data distribution changes. To overcome these limitations, we propose Reservoir Conformal Prediction (ResCP), a novel training-free conformal prediction method for time series. Our approach leverages the efficiency and representation learning capabilities of reservoir computing to dynamically reweight conformity scores. In particular, we compute similarity scores among reservoir states and use them to adaptively reweight the observed residuals at each step. With this approach, ResCP enables us to account for local temporal dynamics when modeling the error distribution without compromising computational scalability. We prove that, under reasonable assumptions, ResCP achieves asymptotic conditional coverage, and we empirically demonstrate its effectiveness across diverse forecasting tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05060v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberto Neglia, Andrea Cini, Michael M. Bronstein, Filippo Maria Bianchi</dc:creator>
    </item>
    <item>
      <title>Algorithm for direct sampling from conditional distributions of toric models</title>
      <link>https://arxiv.org/abs/2110.14992</link>
      <description>arXiv:2110.14992v3 Announce Type: replace 
Abstract: We show that contiguity relations of hypergeometric functions of several variables give a direct sampling algorithm from the conditional distribution of toric models in statistics. The algorithm is based on a Markov chain on a lattice generated by a matrix $A$. A correspondence between decomposable graphical models and $A$-hypergeometric systems is discussed. We give a sum formula of special values of $A$-hypergeometric polynomials. Some examples with implementations are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.14992v3</guid>
      <category>math.ST</category>
      <category>math.CA</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuhei Mano, Nobuki Takayama</dc:creator>
    </item>
    <item>
      <title>KL-BSS: Rethinking optimality for neighbourhood selection in structural equation models</title>
      <link>https://arxiv.org/abs/2306.02244</link>
      <description>arXiv:2306.02244v3 Announce Type: replace 
Abstract: We introduce a new method for neighbourhood selection in linear structural equation models that improves over classical methods such as best subset selection (BSS) and the Lasso. Our method, called KL-BSS, takes advantage of the existence of underlying structure in SEM -- even when this structure is unknown -- and is easily implemented using existing solvers. Under weaker eigenvalue conditions compared to BSS and the Lasso, KL-BSS can provably recover the support of linear models with fewer samples. We establish both the pointwise and minimax sample complexity for recovery, which KL-BSS obtains. Extensive experiments on both real and simulated data confirm the improvements offered by KL-BSS. While it is well-known that the Lasso encounters difficulties under structured dependencies, it is less well-known that even BSS runs into trouble as well, and can be substantially improved. These results have implications for structure learning in graphical models, which often relies on neighbourhood selection as a subroutine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.02244v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming Gao, Wai Ming Tai, Bryon Aragam</dc:creator>
    </item>
    <item>
      <title>Optimal testing in a class of nonregular models</title>
      <link>https://arxiv.org/abs/2403.16413</link>
      <description>arXiv:2403.16413v2 Announce Type: replace 
Abstract: This paper studies optimal hypothesis testing for nonregular econometric models with parameter-dependent support. We consider both one-sided and two-sided hypothesis testing and develop asymptotically uniformly most powerful tests based on a limit experiment. Our two-sided test becomes asymptotically uniformly most powerful without imposing further restrictions such as unbiasedness, and can be inverted to construct a confidence set for the nonregular parameter. Simulation results illustrate desirable finite sample properties of the proposed tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16413v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuya Shimizu, Taisuke Otsu</dc:creator>
    </item>
    <item>
      <title>Maximum mean discrepancies of Farey sequences</title>
      <link>https://arxiv.org/abs/2407.10214</link>
      <description>arXiv:2407.10214v3 Announce Type: replace 
Abstract: We identify a large class of positive-semidefinite kernels for which a certain polynomial rate of convergence of maximum mean discrepancies of Farey sequences is equivalent to the Riemann hypothesis. This class includes all Mat\'ern kernels of order at least one-half.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10214v3</guid>
      <category>math.ST</category>
      <category>math.NT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toni Karvonen, Anatoly Zhigljavsky</dc:creator>
    </item>
    <item>
      <title>Testing Random Effects for Binomial Data</title>
      <link>https://arxiv.org/abs/2504.13977</link>
      <description>arXiv:2504.13977v2 Announce Type: replace 
Abstract: In modern scientific research, small-scale studies with limited participants are increasingly common. However, interpreting individual outcomes can be challenging, making it standard practice to combine data across studies using random effects to draw broader scientific conclusions. In this work, we introduce an optimal methodology for assessing the goodness of fit of a reference distribution for the random effects arising from binomial counts. For meta-analyses, we also derive optimal tests to evaluate whether multiple studies are in agreement before pooling the data. In all cases, we prove that the proposed tests optimally distinguish null and alternative hypotheses separated in the 1-Wasserstein distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13977v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Kania, Larry Wasserman, Sivaraman Balakrishnan</dc:creator>
    </item>
    <item>
      <title>M-estimation for Gaussian processes with time-inhomogeneous drifts from high-frequency data</title>
      <link>https://arxiv.org/abs/2508.01164</link>
      <description>arXiv:2508.01164v3 Announce Type: replace 
Abstract: We propose a contrast-based estimation method for Gaussian processes with time-inhomogeneous drifts, observed under high-frequency sampling. The process is modeled as the sum of a deterministic drift function and a stationary Gaussian component with a parametric kernel. Our method constructs a local contrast function from adjacent increments, which avoids inversion of large covariance matrices and allows for efficient computation. We prove consistency and asymptotic normality of the resulting estimators under general ergodicity conditions. A distinctive feature of our approach is that the drift estimator attains a nonstandard convergence rate, stemming from the direct Riemann integrability of the drift density. This highlights a fundamental difference from standard estimation regimes. Furthermore, when the local contrast fails to identify all parameters in the covariance kernel, moment-based corrections can be incorporated to recover identifiability. The proposed framework is simple, flexible, and particularly well suited for high-frequency inference with time-inhomogeneous structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01164v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yasutaka Shimizu</dc:creator>
    </item>
    <item>
      <title>Nonparametric hazard rate estimation with associated kernels and minimax bandwidth choice</title>
      <link>https://arxiv.org/abs/2509.24535</link>
      <description>arXiv:2509.24535v3 Announce Type: replace 
Abstract: In this paper, we introduce a general theoretical framework for nonparametric hazard rate estimation using associated kernels, whose shapes depend on the point of estimation. Within this framework, we establish rigorous asymptotic results, including a second-order expansion of the MISE, and a central limit theorem for the proposed estimator. We also prove a new oracle-type inequality for both local and global minimax bandwidth selection, extending the Goldenshluger-Lepski method to the context of associated kernels. Our results propose a systematic way to construct and analyze new associated kernels. Finally, we show that the general framework applies to the Gamma kernel, and we provide several examples of applications on simulated data and experimental data for the study of aging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24535v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luce Breuil (MERGE), Sarah Kaaka\"i (LAGA)</dc:creator>
    </item>
    <item>
      <title>Diffusion Approximations for Thompson Sampling in the Small Gap Regime</title>
      <link>https://arxiv.org/abs/2105.09232</link>
      <description>arXiv:2105.09232v5 Announce Type: replace-cross 
Abstract: We study the process-level dynamics of Thompson sampling in the ``small gap'' regime. The small gap regime is one in which the gaps between the arm means are of order $\sqrt{\gamma}$ or smaller and the time horizon is of order $1/\gamma$, where $\gamma$ is small. As $\gamma \downarrow 0$, we show that the process-level dynamics of Thompson sampling converge weakly to the solutions to certain stochastic differential equations and stochastic ordinary differential equations. Our weak convergence theory is developed from first principles using the Continuous Mapping Theorem, can handle stationary, weakly dependent reward processes, and can also be adapted to analyze a variety of sampling-based bandit algorithms. Indeed, we show that the process-level dynamics of many sampling-based bandit algorithms -- including Thompson sampling designed for any single-parameter exponential family of rewards, as well as non-parametric bandit algorithms based on bootstrap re-sampling -- satisfy an invariance principle. Namely, their weak limits coincide with that of Gaussian parametric Thompson sampling with Gaussian priors. Moreover, in the small gap regime, the regret performance of these algorithms is generally insensitive to model mis-specification, changing continuously with increasing degrees of mis-specification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.09232v5</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Fan, Peter W. Glynn</dc:creator>
    </item>
    <item>
      <title>Pearson Chi-squared Conditional Randomization Test</title>
      <link>https://arxiv.org/abs/2111.00027</link>
      <description>arXiv:2111.00027v2 Announce Type: replace-cross 
Abstract: Conditional independence (CI) testing arises naturally in many scientific problems and applications domains. The goal of this problem is to investigate the conditional independence between a response variable $Y$ and another variable $X$, while controlling for the effect of a high-dimensional confounding variable $Z$. In this paper, we introduce a novel test, called `Pearson Chi-squared Conditional Randomization' (PCR) test, which uses the distributional information on covariates $X,Z$ and constructs randomizations to test conditional independence. PCR leverages the i.i.d-ness property of the observations to obtain high-resolution p-values with a very small number of conditional randomizations. We also provide a power analysis of the PCR test, which captures the effect of various parameters of the test, the sample size and the distance of the alternative from the set of null distributions, measured in terms of a notion called `conditional relative density'. In addition, we propose two extensions of the PCR test, with important practical implications: $(i)$ parameter-free PCR, which uses Bonferroni's correction to decide on a tuning parameter in the test; $(ii)$ robust PCR, which avoids inflations in the size of the test when there is slight error in estimating the conditional law $P_{X|Z}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.00027v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adel Javanmard, Mohammad Mehrabi</dc:creator>
    </item>
    <item>
      <title>The Fisher metric as a metric on the cotangent bundle</title>
      <link>https://arxiv.org/abs/2310.13237</link>
      <description>arXiv:2310.13237v2 Announce Type: replace-cross 
Abstract: The Fisher metric on a manifold of probability distributions is usually treated as a metric on the tangent bundle. In this paper, we focus on the metric on the cotangent bundle induced from the Fisher metric with calling it the Fisher co-metric. We show that the Fisher co-metric can be defined directly without going through the Fisher metric by establishing a natural correspondence between cotangent vectors and random variables. This definition clarifies a close relation between the Fisher co-metric and the variance/covariance of random variables, whereby the Cram\'{e}r-Rao inequality is trivialized. We also discuss the monotonicity and the invariance of the Fisher co-metric with respect to Markov maps, and present a theorem characterizing the co-metric by the invariance, which can be regarded as a cotangent version of \v{C}encov's characterization theorem for the Fisher metric. The obtained theorem can also viewed as giving a characterization of the variance/covariance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13237v2</guid>
      <category>cs.IT</category>
      <category>math.DG</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s41884-023-00126-9</arxiv:DOI>
      <arxiv:journal_reference>Information Geometry (2024) 7:S651-S677</arxiv:journal_reference>
      <dc:creator>Hiroshi Nagaoka</dc:creator>
    </item>
    <item>
      <title>Generalization of LiNGAM that allows confounding</title>
      <link>https://arxiv.org/abs/2401.16661</link>
      <description>arXiv:2401.16661v4 Announce Type: replace-cross 
Abstract: LiNGAM determines the variable order from cause to effect using additive noise models, but it faces challenges with confounding. Previous methods maintained LiNGAM's fundamental structure while trying to identify and address variables affected by confounding. As a result, these methods required significant computational resources regardless of the presence of confounding, and they did not ensure the detection of all confounding types. In contrast, this paper enhances LiNGAM by introducing LiNGAM-MMI, a method that quantifies the magnitude of confounding using KL divergence and arranges the variables to minimize its impact. This method efficiently achieves a globally optimal variable order through the shortest path problem formulation. LiNGAM-MMI processes data as efficiently as traditional LiNGAM in scenarios without confounding while effectively addressing confounding situations. Our experimental results suggest that LiNGAM-MMI more accurately determines the correct variable order, both in the presence and absence of confounding. The code is in the supplementary file in this link: https://github.com/SkyJoyTianle/ISIT2024.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16661v4</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ISIT57864.2024.10619691</arxiv:DOI>
      <dc:creator>Joe Suzuki, Tian-Le Yang</dc:creator>
    </item>
    <item>
      <title>Reproducing kernel methods for machine learning, PDEs, and statistics</title>
      <link>https://arxiv.org/abs/2402.07084</link>
      <description>arXiv:2402.07084v3 Announce Type: replace-cross 
Abstract: This monograph develops a unified, application-driven framework for kernel methods grounded in reproducing kernel Hilbert spaces (RKHS) and optimal transport (OT). Part I lays the theoretical and numerical foundations on positive-definite kernels; discrete and continuous RKHS; kernel engineering and scaling maps; error assessment via kernel discrepancy/maximum mean discrepancy (MMD); and a systematic operator view of kernels. In this viewpoint, projection, gradient, divergence, and Laplace-Beltrami operators are built directly from kernels, enabling discrete analogues of differential operators and variational tools that connect learning with PDE-style modeling. Part II turns to practice across four domains. In machine learning, we treat supervised and unsupervised tasks, then develop RKHS-based generative modeling, contrasting density and projection approaches and enhancing them with OT and scalable, combinatorial assignments. We introduce clustering strategies that reduce computational burden and support large-scale regression and transport. In physics-informed modeling, we present mesh-free kernel discretizations for elliptic and time-dependent PDEs, discuss automatic differentiation, and propose high-order discrete approximations. In reinforcement learning, we formulate kernel Q-learning and non-parametric HJB methods, and show how kernel operators yield sample-efficient baselines on continuous-state, discrete-action tasks. In mathematical finance, we build nonparametric time-series models and market generators, study benchmarking and extrapolation for pricing, and apply the framework to stress testing and portfolio methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07084v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philippe G. LeFloch, Jean-Marc Mercier, Shohruh Miryusupov</dc:creator>
    </item>
    <item>
      <title>Conditional normality and finite-state dimensions revisited</title>
      <link>https://arxiv.org/abs/2403.01534</link>
      <description>arXiv:2403.01534v2 Announce Type: replace-cross 
Abstract: The notion of a normal bit sequence was introduced by Borel in 1909; it was the first definition of an individual random object. Normality is a weak notion of randomness requiring only that all $2^n$ factors (substrings) of arbitrary length~$n$ appear with the same limit frequency $2^{-n}$. Later many stronger definitions of randomness were introduced, and in this context normality found its place as ``randomness against a finite-memory adversary''. A quantitative measure of finite-state compressibility was also introduced (the finite-state dimension) and normality means that the finite state dimension is maximal (equals~$1$).
  Recently Nandakumar, Pulari and S (2023) introduced the notion of relative finite-state dimension for a binary sequence with respect to some other binary sequence (treated as an oracle), and the corresponding notion of conditional (relative) normality. (Different notions of conditional randomness were considered before, but not for the finite memory case.) They establish equivalence between the block frequency and the gambling approaches to conditional normality and finite-state dimensions.
  In this note we revisit their definitions and explain how this equivalence can be obtained easily by generalizing known characterizations of (unconditional) normality and dimension in terms of compressibility (finite-state complexity), superadditive complexity measures and gambling (finite-state gales), thus also answering some questions left open in the above-mentioned paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01534v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Shen</dc:creator>
    </item>
    <item>
      <title>Characteristic Learning for Provable One Step Generation</title>
      <link>https://arxiv.org/abs/2405.05512</link>
      <description>arXiv:2405.05512v5 Announce Type: replace-cross 
Abstract: We propose the characteristic generator, a novel one-step generative model that combines the efficiency of sampling in Generative Adversarial Networks (GANs) with the stable performance of flow-based models. Our model is driven by characteristics, along which the probability density transport can be described by ordinary differential equations (ODEs). Specifically, we first estimate the underlying velocity field and use the Euler method to solve the probability flow ODE, generating discrete approximations of the characteristics. A deep neural network is then trained to fit these characteristics, creating a one-step map that pushes a simple Gaussian distribution to the target distribution. In the theoretical aspect, we provide a comprehensive analysis of the errors arising from velocity matching, Euler discretization, and characteristic fitting to establish a non-asymptotic convergence rate in the 2-Wasserstein distance under mild data assumptions. Crucially, we demonstrate that under a standard manifold assumption, this convergence rate depends only on the intrinsic dimension of data rather than the much larger ambient dimension, proving our model's ability to mitigate the curse of dimensionality. To our knowledge, this is the first rigorous convergence analysis for a flow-based one-step generative model. Experiments on both synthetic and real-world datasets demonstrate that the characteristic generator achieves high-quality and high-resolution sample generation with the efficiency of just a single neural network evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05512v5</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhao Ding, Chenguang Duan, Yuling Jiao, Ruoxuan Li, Jerry Zhijian Yang, Pingwen Zhang</dc:creator>
    </item>
    <item>
      <title>Sharp Generalization for Nonparametric Regression in Interpolation Space by Over-Parameterized Neural Networks Trained with Preconditioned Gradient Descent and Early Stopping</title>
      <link>https://arxiv.org/abs/2407.11353</link>
      <description>arXiv:2407.11353v4 Announce Type: replace-cross 
Abstract: We study nonparametric regression using an over-parameterized two-layer neural networks trained with algorithmic guarantees in this paper. We consider the setting where the training features are drawn uniformly from the unit sphere in $\RR^d$, and the target function lies in an interpolation space commonly studied in statistical learning theory. We demonstrate that training the neural network with a novel Preconditioned Gradient Descent (PGD) algorithm, equipped with early stopping, achieves a sharp regression rate of $\cO(n^{-\frac{2\alpha s'}{2\alpha s'+1}})$ when the target function is in the interpolation space $\bth{\cH_K}^{s'}$ with $s' \ge 3$. This rate is even sharper than the currently known nearly-optimal rate of $\cO(n^{-\frac{2\alpha s'}{2\alpha s'+1}})\log^2(1/\delta)$~\citep{Li2024-edr-general-domain}, where $n$ is the size of the training data and $\delta \in (0,1)$ is a small probability. This rate is also sharper than the standard kernel regression rate of $\cO(n^{-\frac{2\alpha}{2\alpha+1}})$ obtained under the regular Neural Tangent Kernel (NTK) regime when training the neural network with the vanilla gradient descent (GD), where $2\alpha = d/(d-1)$. Our analysis is based on two key technical contributions. First, we present a principled decomposition of the network output at each PGD step into a function in the reproducing kernel Hilbert space (RKHS) of a newly induced integral kernel, and a residual function with small $L^{\infty}$-norm. Second, leveraging this decomposition, we apply local Rademacher complexity theory to tightly control the complexity of the function class comprising all the neural network functions obtained in the PGD iterates. Our results further suggest that PGD enables the neural network to escape the linear NTK regime and achieve improved generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11353v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingzhen Yang, Ping Li</dc:creator>
    </item>
    <item>
      <title>Universality of Kernel Random Matrices and Kernel Regression in the Quadratic Regime</title>
      <link>https://arxiv.org/abs/2408.01062</link>
      <description>arXiv:2408.01062v2 Announce Type: replace-cross 
Abstract: Kernel ridge regression (KRR) is a popular class of machine learning models that has become an important tool for understanding deep learning. Much of the focus thus far has been on studying the proportional asymptotic regime, $n \asymp d$, where $n$ is the number of training samples and $d$ is the dimension of the dataset. In the proportional regime, under certain conditions on the data distribution, the kernel random matrix involved in KRR exhibits behavior akin to that of a linear kernel. In this work, we extend the study of kernel regression to the quadratic asymptotic regime, where $n \asymp d^2$. In this regime, we demonstrate that a broad class of inner-product kernels exhibits behavior similar to a quadratic kernel. Specifically, we establish an operator norm approximation bound for the difference between the original kernel random matrix and a quadratic kernel random matrix with additional correction terms compared to the Taylor expansion of the kernel functions. The approximation works for general data distributions under a Gaussian-moment-matching assumption with a covariance structure. This new approximation is utilized to obtain a limiting spectral distribution of the original kernel matrix and characterize the precise asymptotic training and test errors for KRR in the quadratic regime when $n/d^2$ converges to a non-zero constant. The generalization errors are obtained for (i) a random teacher model, (ii) a deterministic teacher model where the weights are perfectly aligned with the covariance of the data. Under the random teacher model setting, we also verify that the generalized cross-validation (GCV) estimator can consistently estimate the generalization error in the quadratic regime for anisotropic data. Our proof techniques combine moment methods, Wick's formula, orthogonal polynomials, and resolvent analysis of random matrices with correlated entries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01062v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parthe Pandit, Zhichao Wang, Yizhe Zhu</dc:creator>
    </item>
    <item>
      <title>A Statistical Hypothesis Testing Framework for Data Misappropriation Detection in Large Language Models</title>
      <link>https://arxiv.org/abs/2501.02441</link>
      <description>arXiv:2501.02441v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are rapidly gaining enormous popularity in recent years. However, the training of LLMs has raised significant privacy and legal concerns, particularly regarding the distillation and inclusion of copyrighted materials in their training data without proper attribution or licensing, an issue that falls under the broader concern of data misappropriation. In this article, we focus on a specific problem of data misappropriation detection, namely, to determine whether a given LLM has incorporated the data generated by another LLM. We propose embedding watermarks into the copyrighted training data and formulating the detection of data misappropriation as a hypothesis testing problem. We develop a general statistical testing framework, construct test statistics, determine optimal rejection thresholds, and explicitly control type I and type II errors. Furthermore, we establish the asymptotic optimality properties of the proposed tests, and demonstrate the empirical effectiveness through intensive numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02441v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinpeng Cai, Lexin Li, Linjun Zhang</dc:creator>
    </item>
    <item>
      <title>Graph Alignment via Birkhoff Relaxation</title>
      <link>https://arxiv.org/abs/2503.05323</link>
      <description>arXiv:2503.05323v2 Announce Type: replace-cross 
Abstract: We consider the graph alignment problem, wherein the objective is to find a vertex correspondence between two graphs that maximizes the edge overlap. The graph alignment problem is an instance of the quadratic assignment problem (QAP), known to be NP-hard in the worst case even to approximately solve. In this paper, we analyze Birkhoff relaxation, a tight convex relaxation of QAP, and present theoretical guarantees on its performance when the inputs follow the Gaussian Wigner Model. More specifically, the weighted adjacency matrices are correlated Gaussian Orthogonal Ensemble with correlation $1/\sqrt{1+\sigma^2}$. Denote the optimal solutions of the QAP and Birkhoff relaxation by $\Pi^\star$ and $X^\star$ respectively. We show that $\|X^\star-\Pi^\star\|_F^2 = o(n)$ when $\sigma = o(n^{-1.25})$ and $\|X^\star-\Pi^\star\|_F^2 = \Omega(n)$ when $\sigma = \Omega(n^{-0.5})$. Thus, the optimal solution $X^\star$ transitions from a small perturbation of $\Pi^\star$ for small $\sigma$ to being well separated from $\Pi^\star$ as $\sigma$ becomes larger than $n^{-0.5}$. This result allows us to guarantee that simple rounding procedures on $X^\star$ align $1-o(1)$ fraction of vertices correctly whenever $\sigma = o(n^{-1.25})$. This condition on $\sigma$ to ensure the success of the Birkhoff relaxation is state-of-the-art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05323v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sushil Mahavir Varma, Ir\`ene Waldspurger, Laurent Massouli\'e</dc:creator>
    </item>
    <item>
      <title>Stochastic Subspace via Probabilistic Principal Component Analysis for Characterizing Model Error</title>
      <link>https://arxiv.org/abs/2504.19963</link>
      <description>arXiv:2504.19963v3 Announce Type: replace-cross 
Abstract: This paper proposes a probabilistic model of subspaces based on the probabilistic principal component analysis (PCA). Given a sample of vectors in the embedding space -- commonly known as a snapshot matrix -- this method uses quantities derived from the probabilistic PCA to construct distributions of the sample matrix, as well as the principal subspaces. It is applicable to projection-based reduced-order modeling methods, such as proper orthogonal decomposition and related model reduction methods. The stochastic subspace thus constructed can be used, for example, to characterize model-form uncertainty in computational mechanics. The proposed method has multiple desirable properties: (1) it is naturally justified by the probabilistic PCA and has analytic forms for the induced random matrix models; (2) it satisfies linear constraints, such as boundary conditions of all kinds, by default; (3) it has only one hyperparameter, which significantly simplifies training; and (4) its algorithm is very easy to implement. We demonstrate the performance of the proposed method via several numerical examples in computational mechanics and structural dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19963v3</guid>
      <category>cs.CE</category>
      <category>math.ST</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s00466-025-02701-6</arxiv:DOI>
      <dc:creator>Akash Yadav, Ruda Zhang</dc:creator>
    </item>
    <item>
      <title>Directional Convergence, Benign Overfitting of Gradient Descent in leaky ReLU two-layer Neural Networks</title>
      <link>https://arxiv.org/abs/2505.16204</link>
      <description>arXiv:2505.16204v2 Announce Type: replace-cross 
Abstract: In this paper, we study benign overfitting of fixed width leaky ReLU two-layer neural network classifiers trained on mixture data via gradient descent. We provide both, upper and lower classification error bounds, and discover a phase transition in the bound as a function of signal strength. The lower bound leads to a characterization of cases when benign overfitting provably fails even if directional convergence occurs. Our analysis allows us to considerably relax the distributional assumptions that are made in existing work on benign overfitting of leaky ReLU two-layer neural network classifiers. We can allow for non-sub-Gaussian data and do not require near orthogonality. Our results are derived by establishing directional convergence of the network parameters and studying classification error bounds for the convergent direction. Previously, directional convergence in (leaky) ReLU neural networks was established only for gradient flow. By first establishing directional convergence, we are able to study benign overfitting of fixed width leaky ReLU two-layer neural network classifiers in a much wider range of scenarios than was done before.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16204v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ichiro Hashimoto</dc:creator>
    </item>
    <item>
      <title>Gradient-flow SDEs have unique transient population dynamics</title>
      <link>https://arxiv.org/abs/2505.21770</link>
      <description>arXiv:2505.21770v2 Announce Type: replace-cross 
Abstract: Identifying the drift and diffusion of an SDE from its population dynamics is a notoriously challenging task. Researchers in machine learning and single cell biology have only been able to prove a partial identifiability result: for potential-driven SDEs, the gradient-flow drift can be identified from temporal marginals if the Brownian diffusivity is already known. Existing methods therefore assume that the diffusivity is known a priori, despite it being unknown in practice. We dispel the need for this assumption by providing a complete characterization of identifiability: the gradient-flow drift and Brownian diffusivity are jointly identifiable from temporal marginals if and only if the process is observed outside of equilibrium. Given this fundamental result, we propose nn-APPEX, the first Schr\"odinger Bridge-based inference method that can simultaneously learn the drift and diffusion of gradient-flow SDEs solely from observed marginals. Extensive numerical experiments show that nn-APPEX's ability to adjust its diffusion estimate enables accurate inference, while previous Schr\"odinger Bridge methods obtain biased drift estimates due to their assumed, and likely incorrect, diffusion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21770v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Guan, Joseph Janssen, Nicolas Lanzetti, Antonio Terpin, Geoffrey Schiebinger, Elina Robeva</dc:creator>
    </item>
    <item>
      <title>Variational Gaussian Approximation in Replica Analysis of Parametric Models</title>
      <link>https://arxiv.org/abs/2509.11780</link>
      <description>arXiv:2509.11780v2 Announce Type: replace-cross 
Abstract: We revisit the replica method for analyzing inference and learning in parametric models, considering situations where the data-generating distribution is unknown or analytically intractable. Instead of assuming idealized distributions to carry out quenched averages analytically, we use a variational Gaussian approximation for the replicated system in grand canonical formalism in which the data average can be deferred and replaced by empirical averages, leading to stationarity conditions that adaptively determine the parameters of the trial Hamiltonian for each dataset. This approach clarifies how fluctuations affect information extraction and connects directly with the results of mathematical statistics or learning theory such as information criteria. As a concrete application, we analyze linear regression and derive learning curves. This includes cases with real-world datasets, where exact replica calculations are not feasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11780v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takashi Takahashi</dc:creator>
    </item>
    <item>
      <title>Analyzing Uncertainty Quantification in Statistical and Deep Learning Models for Probabilistic Electricity Price Forecasting</title>
      <link>https://arxiv.org/abs/2509.19417</link>
      <description>arXiv:2509.19417v2 Announce Type: replace-cross 
Abstract: Precise probabilistic forecasts are fundamental for energy risk management, and there is a wide range of both statistical and machine learning models for this purpose. Inherent to these probabilistic models is some form of uncertainty quantification. However, most models do not capture the full extent of uncertainty, which arises not only from the data itself but also from model and distributional choices. In this study, we examine uncertainty quantification in state-of-the-art statistical and deep learning probabilistic forecasting models for electricity price forecasting in the German market. In particular, we consider deep distributional neural networks (DDNNs) and augment them with an ensemble approach, Monte Carlo (MC) dropout, and conformal prediction to account for model uncertainty. Additionally, we consider the LASSO-estimated autoregressive (LEAR) approach combined with quantile regression averaging (QRA), generalized autoregressive conditional heteroskedasticity (GARCH), and conformal prediction. Across a range of performance metrics, we find that the LEAR-based models perform well in terms of probabilistic forecasting, irrespective of the uncertainty quantification method. Furthermore, we find that DDNNs benefit from incorporating both data and model uncertainty, improving both point and probabilistic forecasting. Uncertainty itself appears to be best captured by the models using conformal prediction. Overall, our extensive study shows that all models under consideration perform competitively. However, their relative performance depends on the choice of metrics for point and probabilistic forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19417v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Lebedev, Abhinav Das, Sven Pappert, Stephan Schl\"uter</dc:creator>
    </item>
  </channel>
</rss>
