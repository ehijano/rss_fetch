<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.hist-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.hist-ph</link>
    <description>physics.hist-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.hist-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Sep 2025 01:33:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Origins of the Ising model</title>
      <link>https://arxiv.org/abs/2509.00632</link>
      <description>arXiv:2509.00632v1 Announce Type: new 
Abstract: In 1925, Ernest Ising published a paper analyzing a model proposed in 1920 by Wilhelm Lenz for ferromagnetism. The model is composed of constituent units that take only two states and interact only when they are neighbors. Ising showed that in a linear chain the model does not present an ordered ferromagnetic state, a frustrating but correct result. However, Rudolf Peierls demonstrated in 1936 that the model does in fact present an ordered state in two dimensions, and therefore in three dimensions. This result reveals that short-range interaction and only two states for each constituent unit are sufficient for ordering to occur over long distances. These two elements are the key to understanding the success of the model and its variants even a hundred years after its appearance. Here we analyze the emergence of the model in the period up to 1936.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00632v1</guid>
      <category>physics.hist-ph</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M\'ario J. de Oliveira</dc:creator>
    </item>
    <item>
      <title>Un avenir commun au sein de la soci\'et\'e num\'erique</title>
      <link>https://arxiv.org/abs/2509.01014</link>
      <description>arXiv:2509.01014v1 Announce Type: new 
Abstract: Today, data and information have become overabundant resources within a global network of machines that exchange signals at speeds approaching that of light. In this highly saturated environment, communication has emerged as the most central form of interaction, supported by a rapidly evolving technical infrastructure. These new communication tools have created an overwhelming surplus of information so much so that no human could process it all. In response, platforms like Facebook, YouTube, and Google use algorithms to filter and suggest content. These algorithms act as sorting mechanisms, reducing the informational noise and presenting users with content tailored to their habits and preferences. However, by placing themselves between users and data, these platforms gain control over what users see and, in doing so, shape their preferences and behaviors. In physical terms, we might say they function like selective filters or control gates in an information system directing flows and creating feedback loops. Over time, this can lead to a kind of informational inertia, where users become increasingly shaped by algorithmic influence and lose the ability to form independent judgments. This process reflects a broader trend that Bernard Stiegler describes as a new kind of proletarianization where individuals lose the knowledge and skills that are absorbed and automated by digital systems. Borrowing from physics, we study this as a shift toward higher entropy. However, platforms like Wikipedia and arXiv demonstrate how digital tools can support collective knowledge without leading to cognitive degradation. Inspired by Elinor Ostrom work on commons, we propose a model for a digital commons economy where information is shared and governed collaboratively, helping to restore a balance between entropy and organization in our digital environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01014v1</guid>
      <category>physics.hist-ph</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miguel Vanvlasselaer</dc:creator>
    </item>
    <item>
      <title>Einstein's Hidden Scaffolding, with a Glance at Poincar\'e</title>
      <link>https://arxiv.org/abs/2509.02456</link>
      <description>arXiv:2509.02456v1 Announce Type: new 
Abstract: This paper reconstructs the derivations underlying the kinematical part of Einstein's 1905 special relativity paper, emphasizing their operational clarity and minimalist use of mathematics. Einstein employed modest tools-algebraic manipulations, Taylor expansions, partial differentials, and functional arguments-yet his method was guided by principles of linearity, symmetry, and invariance rather than the elaborate frameworks of electron theory. The published text in "Annalen der Physik" concealed much of the algebraic scaffolding, presenting instead a streamlined sequence of essential equations. Far from reflecting a lack of sophistication, this economy of means was a deliberate rhetorical and philosophical choice: to demonstrate that relativity arises from two simple postulates and basic operational definitions, not from the complexities of electron theory. The reconstruction highlights how Einstein's strategy subordinated mathematics to principle, advancing a new mode of reasoning in which physical insight, rather than computational elaboration, held decisive authority. In this respect, I show that Einstein's presentation diverges sharply from Poincar\'e's.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02456v1</guid>
      <category>physics.hist-ph</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Galina Weinstein</dc:creator>
    </item>
    <item>
      <title>Remembering Yvonne Choquet-Bruhat</title>
      <link>https://arxiv.org/abs/2509.00597</link>
      <description>arXiv:2509.00597v1 Announce Type: cross 
Abstract: I describe the impact of some of the mathematical results of Yvonne Choquet-Bruhat on gravitational physics, as well as the evolution of my interactions with her over the years.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00597v1</guid>
      <category>gr-qc</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>physics.hist-ph</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thibault Damour</dc:creator>
    </item>
    <item>
      <title>Algorithmic Randomness and Probabilistic Laws</title>
      <link>https://arxiv.org/abs/2303.01411</link>
      <description>arXiv:2303.01411v2 Announce Type: replace 
Abstract: We apply recent ideas about complexity and randomness to the philosophy of laws and chances. We develop two ways to use algorithmic randomness to characterize probabilistic laws of nature. The first, a generative chance* law, employs a nonstandard notion of chance. The second, a probabilistic* constraining law, impose relative frequency and randomness constraints that every physically possible world must satisfy. The constraining notion removes a major obstacle to a unified governing account of non-Humean laws, on which laws govern by constraining physical possibilities; it also provides independently motivated solutions to familiar problems for the Humean best-system account (the Big Bad Bug and the zero-fit problem). On either approach, probabilistic laws are tied more tightly to corresponding sets of possible worlds: some histories permitted by traditional probabilistic laws are now ruled out as physically impossible. Consequently, the framework avoids one variety of empirical underdetermination while bringing to light others that are typically overlooked.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.01411v2</guid>
      <category>physics.hist-ph</category>
      <category>cs.CC</category>
      <category>math.PR</category>
      <category>quant-ph</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jeffrey A. Barrett, Eddy Keming Chen</dc:creator>
    </item>
  </channel>
</rss>
