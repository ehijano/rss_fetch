<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Dec 2025 05:09:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Comparing Two Proxy Methods for Causal Identification</title>
      <link>https://arxiv.org/abs/2512.00175</link>
      <description>arXiv:2512.00175v1 Announce Type: new 
Abstract: Identifying causal effects in the presence of unmeasured variables is a fundamental challenge in causal inference, for which proxy variable methods have emerged as a powerful solution. We contrast two major approaches in this framework: (1) bridge equation methods, which leverage solutions to integral equations to recover causal targets, and (2) array decomposition methods, which recover latent factors composing counterfactual quantities by exploiting unique determination of eigenspaces. We compare the model restrictions underlying these two approaches and provide insight into implications of the underlying assumptions, clarifying the scope of applicability for each method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00175v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Helen Guo, Elizabeth L. Ogburn, Ilya Shpitser</dc:creator>
    </item>
    <item>
      <title>Penalized spatial function-on-function regression</title>
      <link>https://arxiv.org/abs/2512.00237</link>
      <description>arXiv:2512.00237v1 Announce Type: new 
Abstract: The function-on-function regression model is fundamental for analyzing relationships between functional covariates and responses. However, most existing function-on-function regression methodologies assume independence between observations, which is often unrealistic for spatially structured functional data. We propose a novel penalized spatial function-on-function regression model to address this limitation. Our approach extends the generalized spatial two-stage least-squares estimator to functional data, while incorporating a roughness penalty on the regression coefficient function using a tensor product of B-splines. This penalization ensures optimal smoothness, mitigating overfitting, and improving interpretability. The proposed penalized spatial two-stage least-squares estimator effectively accounts for spatial dependencies, significantly improving estimation accuracy and predictive performance. We establish the asymptotic properties of our estimator, proving its $\sqrt{n}$-consistency and asymptotic normality under mild regularity conditions. Extensive Monte Carlo simulations demonstrate the superiority of our method over existing non-penalized estimators, particularly under moderate to strong spatial dependence. In addition, an application to North Dakota weather data illustrates the practical utility of our approach in modeling spatially correlated meteorological variables. Our findings highlight the critical role of penalization in enhancing robustness and efficiency in spatial function-on-function regression models. To implement our method we used the \texttt{robflreg} package on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00237v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ufuk Beyaztas, Han Lin Shang, Gizel Bakicierler Sezer</dc:creator>
    </item>
    <item>
      <title>Difference-in-differences with stochastic policy shifts of continuous treatments</title>
      <link>https://arxiv.org/abs/2512.00296</link>
      <description>arXiv:2512.00296v1 Announce Type: new 
Abstract: Treatment effects under stochastic policy shifts quantify differences in outcomes across counterfactual scenarios with varying treatment distributions. Stochastic policy shifts generalize common notions of treatment effects since they include deterministic interventions (e.g., all individuals treated versus none treated) as a special case. While stochastic policy effects have been examined under causal exchangeability, they have not been integrated into the difference-in-differences (DiD) framework, which relies on parallel trends rather than exchangeability. In this paper, nonparametric efficient estimators of stochastic intervention effects are developed under a DiD setup with continuous treatments. The proposed causal estimand is the average stochastic dose effect among the treated, where the stochastic dose effect is the contrast between potential outcomes under a counterfactual dose distribution and no treatment. Several possible stochastic interventions are discussed, including those that do and do not depend on the observed data distribution. For generic stochastic interventions, the causal estimand is identified under standard conditions and estimators are proposed. Then, we focus on a specific stochastic policy shift, the exponential tilt, that increments the conditional density function of the continuous dose. For the exponential tilt intervention, a nonparametric estimator is proposed that allows for data-adaptive, machine learning nuisance function estimation. Under mild convergence rate conditions, the estimator is shown to be root-$n$ consistent and asymptotically normal with variance attaining the nonparametric efficiency bound. The proposed method is used to study the effect of hydraulic fracturing activity on employment and income.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00296v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Jetsupphasuk, Chenwei Fang, Didong Li, Michael G. Hudgens</dc:creator>
    </item>
    <item>
      <title>Finite-Sample Valid Rank Confidence Sets for a Broad Class of Statistical and Machine Learning Models</title>
      <link>https://arxiv.org/abs/2512.00316</link>
      <description>arXiv:2512.00316v1 Announce Type: new 
Abstract: Ranking populations such as institutions based on certain characteristics is often of interest, and these ranks are typically estimated using samples drawn from the populations. Due to sample randomness, it is important to quantify the uncertainty associated with the estimated ranks. This becomes crucial when latent characteristics are poorly separated and where many rank estimates may be incorrectly ordered. Understanding uncertainty can help quantify and mitigate these issues and provide a fuller picture. However, this task is especially challenging because the rank parameters are discrete and the central limit theorem does not apply to the rank estimates. In this article, we propose a Repro Samples Method to address this nontrivial inference problem by developing a confidence set for the true, unobserved population ranks. This method provides finite-sample coverage guarantees and is broadly applicable to ranking problems. The effectiveness of the method is illustrated and compared with several published large sample ranking approaches using simulation studies and real data examples involving samples both from traditional statistical models and modern data science algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00316v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Onrina Chandra, Min-ge Xie</dc:creator>
    </item>
    <item>
      <title>On Statistical Inference for High-Dimensional Binary Time Series</title>
      <link>https://arxiv.org/abs/2512.00338</link>
      <description>arXiv:2512.00338v1 Announce Type: new 
Abstract: The analysis of non-real-valued data, such as binary time series, has attracted great interest in recent years. This manuscript proposes a post-selection estimator for estimating the coefficient matrices of a high-dimensional generalized binary vector autoregressive process and establishes a Gaussian approximation theorem for the proposed estimator. Furthermore, it introduces a second-order wild bootstrap algorithm to enable statistical inference on the coefficient matrices. Numerical studies and empirical applications demonstrate the good finite-sample performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00338v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dehao Dai, Yunyi Zhang</dc:creator>
    </item>
    <item>
      <title>Evaluating Surrogates in Individual Treatment Regimes</title>
      <link>https://arxiv.org/abs/2512.00405</link>
      <description>arXiv:2512.00405v1 Announce Type: new 
Abstract: In many decision-making problems, the primary outcome of interest is costly or time-consuming to observe, prompting the use of surrogate variables to learn individualized treatment rules (ITRs). However, even when a surrogate is strongly correlated with the outcome or satisfies conventional surrogate validity conditions, surrogate-based ITRs may diverge from outcome-optimal decisions - particularly under realistic budget constraints. To address this gap, we develop a framework for evaluating the decision-making value of surrogate endpoints. We introduce three ITR-oriented performance metrics: surrogate regret, which measures the utility loss from using surrogate-based ITRs instead of outcome-optimal ITRs; surrogate gain, which quantifies the benefit of surrogate-based treatment decisions relative to no treatment; and surrogate efficiency, which evaluates improvement over random treatment assignment. We further extend these metrics to budget-constrained settings and propose doubly robust estimators. We establish their asymptotic properties and provide valid statistical inference. Simulation and real data studies demonstrate the effectiveness of the proposed approach. Overall, this work offers the first principled framework to rigorously evaluate surrogates, providing a promising pathway toward more efficient decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00405v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeyu Xu, Xiaojie Mao, Hao Mei, Yue Liu</dc:creator>
    </item>
    <item>
      <title>High-dimensional Autoregressive Modeling for Time Series with Hierarchical Structures</title>
      <link>https://arxiv.org/abs/2512.00508</link>
      <description>arXiv:2512.00508v1 Announce Type: new 
Abstract: High-dimensional time series often exhibit hierarchical structures represented by tensors, while statistical methodologies that can effectively exploit the structural information remain limited. We propose a supervised factor modeling framework that accommodates general hierarchical structures by extracting low-dimensional features sequentially in the mode orders that respect the hierarchical structure. Our method can select a small collection of such orders to allow for impurities in the hierarchical structures, yielding interpretable loading matrices that preserve the hierarchical relationships. A practical estimation procedure is proposed, with a hyperparameter selection scheme that identifies a parsimonious set of action orders and interim ranks, thereby revealing the possibly latent hierarchical structures. Theoretically, non-asymptotic error bounds are derived for the proposed estimators in both regression and autoregressive settings. An application to the IPIP-NEO-120 personality panel illustrates superior forecasting performance and clearer structural interpretation compared with existing methods based on tensor decompositions and hierarchical factor analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00508v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lan Li, Shibo Yu, Yingzhou Wang, Guodong Li</dc:creator>
    </item>
    <item>
      <title>Testing similarity of competing risks models by comparing transition probabilities</title>
      <link>https://arxiv.org/abs/2512.00583</link>
      <description>arXiv:2512.00583v1 Announce Type: new 
Abstract: Assessing whether two patient populations exhibit comparable event dynamics is essential for evaluating treatment equivalence, pooling data across cohorts, or comparing clinical pathways across hospitals or strategies. We introduce a statistical framework for formally testing the similarity of competing risks models based on transition probabilities, which represent the cumulative risk of each event over time. Our method defines a maximum-type distance between the transition probability matrices of two multistate processes and employs a novel constrained parametric bootstrap test to evaluate similarity under both administrative and random right censoring. We theoretically establish the asymptotic validity and consistency of the bootstrap test. Through extensive simulation studies, we show that our method reliably controls the type I error and achieves higher statistical power than existing intensity-based approaches. Applying the framework to routine clinical data of prostate cancer patients treated with radical prostatectomy, we identify the smallest similarity threshold at which patients with and without prior in-house fusion biopsy exhibit comparable readmission dynamics. The proposed method provides a robust and interpretable tool for quantifying similarity in event history models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00583v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zoe Kristin Lange, Maryam Farhadizadeh, Holger Dette, Nadine Binder</dc:creator>
    </item>
    <item>
      <title>R\'enyi's $\alpha$-divergence variational Bayes for spike-and-slab high-dimensional linear regression</title>
      <link>https://arxiv.org/abs/2512.00627</link>
      <description>arXiv:2512.00627v1 Announce Type: new 
Abstract: Sparse high-dimensional linear regression is a central problem in statistics, where the goal is often variable selection and/or coefficient estimation. We propose a mean-field variational Bayes approximation for sparse regression with spike-and-slab Laplace priors that replaces the standard Kullback-Leibler (KL) divergence objective with the R\'enyi's $\alpha$ divergence: a one-parameter generalization of KL divergence indexed by $\alpha \in (0, \infty) \setminus \{1\}$ that allows flexibility between zero-forcing and mass-covering behavior. We derive coordinate ascent variational inference (CAVI) updates via a second-order delta method and develop a stochastic variational inference algorithm based on a Monte Carlo surrogate R\'enyi lower bound. In simulations, our two methods perform comparably to state-of-the-art Bayesian variable selection procedures across a range of sparsity configurations and $\alpha$ values for both variable selection and estimation, and our numerical results illustrate how different choices of $\alpha$ can be advantageous in different sparsity configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00627v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chadi Bsila, Yiqi Tang, Kaiwen Wang, Laurie Heyer</dc:creator>
    </item>
    <item>
      <title>Maximum Likelihood Estimation of the Vector AutoRegressive To Anything (VARTA) model</title>
      <link>https://arxiv.org/abs/2512.00631</link>
      <description>arXiv:2512.00631v1 Announce Type: new 
Abstract: The literature on multivariate time series is, largely, limited to either models based on the multivariate Gaussian distribution or models specifically developed for a given application. In this paper we develop a general approach which is based on an underlying, unobserved, Gaussian Vector Autoregressive (VAR) model. Using a transformation, we can capture the time dynamics as well as the distributional properties of a multivariate time series. The model is called the Vector AutoRegressive To Anyting (VARTA) model and was originally presented by Biller and Nelson (2003) who used it for the purpose of simulation. In this paper we derive a maximum likelihood estimator for the model and investigate its performance. We also provide diagnostic analysis and how to compute the predictive distribution. The proposed approach can provide better estimates about the forecasting distributions which can be of every kind not necessarily Gaussian distributions as for the standard VAR models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00631v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Andersson, Dimitris Karlis</dc:creator>
    </item>
    <item>
      <title>NOVA: Coordinated Test Selection and Bayes-Optimized Constrained Randomization for Accelerated Coverage Closure</title>
      <link>https://arxiv.org/abs/2512.00688</link>
      <description>arXiv:2512.00688v1 Announce Type: new 
Abstract: Functional verification relies on large simulation-based regressions. Traditional test selection relies on static test features and overlooks actual coverage behavior, wasting substantial simulation time, while constrained random stimuli generation depends on manually crafted distributions that are difficult to design and often ineffective. We present NOVA, a framework that coordinates coverage-aware test selection with Bayes-optimized constrained randomization. NOVA extracts fine-grained coverage features to filter redundant tests and modifies the constraint solver to expose parameterized decision strategies whose settings are tuned via Bayesian optimization to maximize coverage growth. Across multiple RTL designs, NOVA achieves up to a 2.82$\times$ coverage convergence speedup without requiring human-crafted heuristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00688v1</guid>
      <category>stat.ME</category>
      <category>cs.AR</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weijie Peng (Eric), Nanbing Li (Eric), Jin Luo (Eric), Shuai Wang (Eric), Yihui Li (Eric), Jun Fang (Eric),  Yun (Eric),  Liang</dc:creator>
    </item>
    <item>
      <title>A Scalable Variational Bayes Approach for Fitting Non-Conjugate Spatial Generalized Linear Mixed Models via Basis Expansions</title>
      <link>https://arxiv.org/abs/2512.00895</link>
      <description>arXiv:2512.00895v1 Announce Type: new 
Abstract: Large spatial datasets with non-Gaussian responses are increasingly common in environmental monitoring, ecology, and remote sensing, yet scalable Bayesian inference for such data remains challenging. Markov chain Monte Carlo (MCMC) methods are often prohibitive for large datasets, and existing variational Bayes methods rely on conjugacy or strong approximations that limit their applicability and can underestimate posterior variances. We propose a scalable variational framework that incorporates semi-implicit variational inference (SIVI) with basis representations of spatial generalized linear mixed models (SGLMMs), which may not have conjugacy. Our approach accommodates gamma, negative binomial, Poisson, Bernoulli, and Gaussian responses on continuous spatial domains. Across 20 simulation scenarios with 50,000 locations, SIVI achieves predictive accuracy and posterior distributions comparable to Metropolis-Hastings and Hamiltonian Monte Carlo while providing notable computational speedups. Applications to MODIS land surface temperature and Blue Jay abundance further demonstrate the utility of the approach for large non-Gaussian spatial datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00895v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin Hyung Lee, Ben Seiyon Lee</dc:creator>
    </item>
    <item>
      <title>Grouped Competition Test with Unified False Discovery Rate Control</title>
      <link>https://arxiv.org/abs/2512.00901</link>
      <description>arXiv:2512.00901v1 Announce Type: new 
Abstract: This paper discusses several p-value-free multiple hypothesis testing methods proposed in recent years and organizes them by introducing a unified framework termed competition test. Although existing competition tests are effective in controlling the False Discovery Rate (FDR), they struggle with handling data with strong heterogeneity or dependency structures. Based on this framework, the paper proposes a novel approach that applies a corrected competition procedure to group data with certain structure, and then integrates the results from each group. Using the favorable properties of competition test, the paper proposes a theorem demonstrating that this approach controls the global FDR. We further show that although the correction parameters may lead to a slight loss in power, such loss is typically minimal. Through simulation experiments and mass spectrometry data analysis, we illustrate the flexibility and efficacy of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00901v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingzhou Deng, Yan Fu</dc:creator>
    </item>
    <item>
      <title>An Imbalance-Robust Evaluation Framework for Extreme Risk Forecasts</title>
      <link>https://arxiv.org/abs/2512.00916</link>
      <description>arXiv:2512.00916v1 Announce Type: new 
Abstract: Evaluating rare-event forecasts is challenging because standard metrics collapse as event prevalence declines. Measures such as F1-score, AUPRC, MCC, and accuracy induce degenerate thresholds -- converging to zero or one -- and their values become dominated by class imbalance rather than tail discrimination. We develop a family of rare-event-stable (RES) metrics whose optimal thresholds remain strictly interior as the event probability approaches zero, ensuring coherent decision rules under extreme rarity. Simulations spanning event probabilities from 0.01 down to one in a million show that RES metrics maintain stable thresholds, consistent model rankings, and near-complete prevalence invariance, whereas traditional metrics exhibit statistically significant threshold drift and structural collapse. A credit-default application confirms these results: RES metrics yield interpretable probability-of-default cutoffs (4-9%) and remain robust under subsampling, while classical metrics fail operationally. The RES framework provides a principled, prevalence-invariant basis for evaluating extreme-risk forecasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00916v1</guid>
      <category>stat.ME</category>
      <category>q-fin.RM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sotirios D. Nikolopoulos</dc:creator>
    </item>
    <item>
      <title>The Dual Wavelet Spectra: An Alternative Perspective on Hurst Exponent Estimation with Application to Mammogram Classification</title>
      <link>https://arxiv.org/abs/2512.00996</link>
      <description>arXiv:2512.00996v1 Announce Type: new 
Abstract: The wavelet spectra is a common starting point for estimating the Hurst exponent of a self-similar signal using wavelet-based techniques. The decay of the $\log_2$ average energy of the detail wavelet coefficients as a function of the level of signal decomposition can be used to construct estimators for this parameter. In this paper, we expand on previous work which introduced the ``dual" wavelet spectra, where decomposition levels are instead treated as a function of energy values, and propose a relationship between its slope and the Hurst exponent by inverting the standard wavelet spectra, thereby creating a new estimator. The effectiveness of this estimator and its sensitivity to several settings are demonstrated through a simulation study. Finally, we show how the technique performs as a feature extraction method by applying it to the task of detecting the presence of breast cancer in mammogram images. Dual spectra wavelet features had a statistically significant effect on the log-odds of Cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00996v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Raymond J. Hinton, Jr., Pepa Ram\'irez Cobo, Brani Vidakovic</dc:creator>
    </item>
    <item>
      <title>Correlated Confounding Variables Are Not Easily Controlled for in Large Survey Research</title>
      <link>https://arxiv.org/abs/2512.01003</link>
      <description>arXiv:2512.01003v1 Announce Type: new 
Abstract: Results in epidemiology and social science often require the removal of confounding effects from measurements of the pairwise correlation of variables in survey data. This is typically accomplished by some variant of linear regression (e.g., ``logistic" or ``Cox proportional"). But, knowing whether all possible confounders have been identified, or are even visible (not latent), is in general impossible. Here, we exhibit two examples that frame the issue. The first example proposes a highly unlikely hypothesis on drug use, draws data from a large, respected survey, and succeeds in ``proving" the implausible hypothesis, despite regressing out more than 20 confounding variables. The second constructs a ``metamodel" in which a single (by hypothesis unmeasurable) latent variable affects many mutually correlated confounders. From simulations, we derive formulas for the magnitude of spurious association that persists even as increasing numbers of confounders are regressed out. The intent of these examples is for them to serve as cautionary tales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01003v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William H. Press</dc:creator>
    </item>
    <item>
      <title>Weight a Minute: Understanding Variability in PATE Estimates Across Target Populations</title>
      <link>https://arxiv.org/abs/2512.01157</link>
      <description>arXiv:2512.01157v1 Announce Type: new 
Abstract: Clinical study populations often differ meaningfully from the broader populations to which results are intended to generalize. Weighting methods such as inverse probability of sampling weights (IPSW) reweight study participants to resemble a target population, but the accuracy of these estimates depends heavily on how well the chosen population represents the population of substantive interest. We conduct a simulation study grounded in empirical covariate distributions from several real-world data sources spanning a continuum from highly selective to broadly inclusive populations. Using treatment effect scenarios with varying levels of effect modification, we evaluate IPSW estimators of the population average treatment effect (PATE) across multiple candidate target populations. We quantify the bias that arises when the dataset used to operationalize the target population differs from the intended inference population, even when IPSW is correctly specified. Our results show that bias increases systematically as target populations diverge from a well-representative population, and that weighting to a poorly aligned target can introduce more bias than not weighting at all. These findings highlight that selecting an appropriate target population dataset is a critical design choice for valid generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01157v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William Stewart (Duke University School of Medicine), Carly L. Brantner (Duke University School of Medicine), Elizabeth A. Stuart (Johns Hopkins Bloomberg School of Public Health), Laine Thomas (Duke University School of Medicine)</dc:creator>
    </item>
    <item>
      <title>Gaussian Process State-Space Modeling and Particle Filtering for Time Series Decomposition and Nonlinear Signal Extraction</title>
      <link>https://arxiv.org/abs/2512.01162</link>
      <description>arXiv:2512.01162v1 Announce Type: new 
Abstract: Gaussian-process state-space models (GP-SSMs) provide a flexible nonparametric alternative for modeling time-series dynamics that are nonlinear or difficult to specify parametrically. While the Kalman filter is effective for linear-Gaussian trend and seasonal components, many real-world systems require more expressive representations. GP-SSMs address this need by learning transition functions directly from data, while particle filtering enables Bayesian state estimation even when posterior distributions deviate from Gaussianity. This paper develops a particle-filtering framework for GP-SSM inference and compares its performance with the Kalman filter in trend extraction and seasonal adjustment. We further evaluate nonlinear signal-extraction tasks, demonstrating that GP-SSMs can recover latent states under sharp or asymmetric dynamics. The results highlight the utility of combining GP modeling with sequential Monte Carlo methods for complex time-series analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01162v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Genshiro Kitagawa (Tokyo University of Marine Science,Technology,The Institute of Statistical Mathematics)</dc:creator>
    </item>
    <item>
      <title>The Dynamical Model Representation of Convolution-Generated Spatio-Temporal Gaussian Processes and Its Applications</title>
      <link>https://arxiv.org/abs/2512.01279</link>
      <description>arXiv:2512.01279v1 Announce Type: new 
Abstract: Convolution-generated space-time models yield an important class of non-separable stationary Gaussian Processes (GP) through a sequence of convolution operations, in both space and time, on spatially correlated Brownian motion with a Gaussian convolution kernel. Because of its solid connection to stochastic partial differential equations, such a modeling approach offers strong physical interpretations when it is applied to scientific and engineering processes. In this paper, we obtain a new dynamical model representation for convolution-generated spatio-temporal GP. In particular, an infinite-dimensional linear state-space representation is firstly obtained where the state transition is governed by a stochastic differential equation (SDE) whose solution has the same space-time covariance as the original convolution-generated process. Then, using the Galerkin's method, a finite-dimension approximation to the infinite-dimensional SDE is obtained, yielding a dynamical model with finite states that facilitates the computation and parameter estimation. The space-time covariance of the approximated dynamical model is obtained, and the error between the approximate and exact covariance matrices is quantified. We investigate the performance of the proposed model through a simulation-based study, and apply the approach to a real case study utilizing the remote-sensing aerosol data during the recent 2025 Los Angeles wildfire. The modeling capability of the proposed approach has been well demonstrated, and the proposed approach is found particularly effective in monitoring the first-order time derivative of the underlying space-time process, making it a good candidate for process modeling, monitoring and anomaly detection problems. Computer code and data have been made publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01279v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yutong Zhang, Xiao Liu</dc:creator>
    </item>
    <item>
      <title>Inferring Dynamic Hidden Graph Structure in Heterogeneous Correlated Time Series</title>
      <link>https://arxiv.org/abs/2512.01301</link>
      <description>arXiv:2512.01301v1 Announce Type: new 
Abstract: Modeling heterogeneous correlated time series requires the ability to learn hidden dynamic relationships between component time series with possibly varying periodicities and generative processes. To address this challenge, we formulate and evaluate a windowed variance-correlation metric (WVC) designed to quantify time-varying correlations between signals. This method directly recovers hidden relationships in an specified time interval as a weighted adjacency matrix, consequently inferring hidden dynamic graph structure. On simulated data, our method captures correlations that other methods miss. The proposed method expands the ability to learn dynamic graph structure between significantly different signals within a single cohesive dynamical graph model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01301v1</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jeshwanth Mohan, Bharath Ramsundar, Sandya Subramanian</dc:creator>
    </item>
    <item>
      <title>Convolution-smoothing based locally sparse estimation for functional quantile regression</title>
      <link>https://arxiv.org/abs/2512.01341</link>
      <description>arXiv:2512.01341v1 Announce Type: new 
Abstract: Motivated by an application to study the impact of temperature, precipitation and irrigation on soybean yield, this article proposes a sparse semi-parametric functional quantile model. The model is called ``sparse'' because the functional coefficients are only nonzero in the local time region where the functional covariates have significant effects on the response under different quantile levels. To tackle the computational and theoretical challenges in optimizing the quantile loss function added with a concave penalty, we develop a novel Convolution-smoothing based Locally Sparse Estimation (CLoSE) method, to do three tasks in one step, including selecting significant functional covariates, identifying the nonzero region of functional coefficients to enhance the interpretability of the model and estimating the functional coefficients. We establish the functional oracle properties and simultaneous confidence bands for the estimated functional coefficients, along with the asymptotic normality for the estimated parameters. In addition, because it is difficult to estimate the conditional density function given the scalar and functional covariates, we propose the split wild bootstrap method to construct the confidence interval of the estimated parameters and simultaneous confidence band for the functional coefficients. We also establish the consistency of the split wild bootstrap method. The finite sample performance of the proposed CLoSE method is assessed with simulation studies. The proposed model and estimation procedure are also illustrated by identifying the active time regions when the daily temperature influences the soybean yield.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01341v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hua Liu, Boyi Hu, Jinhong You, Jiguo Cao</dc:creator>
    </item>
    <item>
      <title>Active Hypothesis Testing under Computational Budgets with Applications to GWAS and LLM</title>
      <link>https://arxiv.org/abs/2512.01423</link>
      <description>arXiv:2512.01423v1 Announce Type: new 
Abstract: In large-scale hypothesis testing, computing exact $p$-values or $e$-values is often resource-intensive, creating a need for budget-aware inferential methods. We propose a general framework for active hypothesis testing that leverages inexpensive auxiliary statistics to allocate a global computational budget. For each hypothesis, our data-adaptive procedure probabilistically decides whether to compute the exact test statistic or a transformed proxy, guaranteeing a valid $p$-value or $e$-value while satisfying the budget constraint in expectation. Theoretical guarantees are established for our constructions, showing that the procedure achieves optimality for $e$-values and for $p$-values under independence, and admissibility for $p$-values under general dependence. Empirical results from simulations and two real-world applications, including a large-scale genome-wide association study (GWAS) and a clinical prediction task leveraging large language models (LLM), demonstrate that our framework improves statistical efficiency under fixed resource limits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01423v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qi Kuang, Bowen Gang, Yin Xia</dc:creator>
    </item>
    <item>
      <title>Model-Based Clustering of Functional Data Via Random Projection Ensembles</title>
      <link>https://arxiv.org/abs/2512.01450</link>
      <description>arXiv:2512.01450v1 Announce Type: new 
Abstract: Clustering functional data is a challenging task due to intrinsic infinite-dimensionality and the need for stable, data-adaptive partitioning. In this work, we propose a clustering framework based on Random Projections, which simultaneously performs dimensionality reduction and generates multiple stochastic representations of the original functions. Each projection is clustered independently, and the resulting partitions are then aggregated through an ensemble consensus procedure, enhancing robustness and mitigating the influence of any single projection. To focus on the most informative representations, projections are ranked according to clustering quality criteria, and only a selected subset is retained. In particular, we adopt Gaussian Mixture Models as base clusterers and employ the Kullback-Leibler divergence to order the random projections; these choices enable fast computation and eliminate the need to specify the number of clusters a priori. The performance of the proposed methodology is assessed through an extensive simulation study and two real-data applications, one from spectroscopy data for food authentication and one from log-periodograms of speech recording; the obtained results suggest that the proposal represents an effective tool for the clustering of functional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01450v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Mori, Laura Anderlucci</dc:creator>
    </item>
    <item>
      <title>A mixture of distributed lag non-linear models to account for spatially heterogeneous exposure-lag-response associations</title>
      <link>https://arxiv.org/abs/2512.01508</link>
      <description>arXiv:2512.01508v1 Announce Type: new 
Abstract: Environmental exposures, such as air pollution and extreme temperatures, have complex effects on human health. These effects are often characterized by non-linear exposure-lag-response relationships and delayed impacts over time. Accurately capturing these dynamics is crucial for informing public health interventions. The Distributed Lag Non-Linear Model (DLNM) is a flexible statistical framework for estimating such effects in epidemiological research. However, standard DLNM implementations typically assume a homogeneous exposure-lag-response association across the study region, overlooking potential spatial heterogeneity, which can lead to biased risk estimates. To address this limitation, we introduce DLNM-Clust: a novel mixture of DLNMs that extends the traditional DLNM. Within a Bayesian framework, DLNM-Clust probabilistically assigns each geographic unit to one of $C$ latent spatial clusters, each of which is defined by a distinct DLNM specification. This approach allows capturing both common patterns and singular deviations in the exposure-lag-response surface. We demonstrate the method using municipality-level time-series data on the relationship between air pollution and the incidence of COVID-19 in Belgium. Our results emphasize the importance of spatially aware modeling strategies in environmental epidemiology, facilitating region-specific risk assessment and supporting the development of targeted public health initiatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01508v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\'Alvaro Briz-Red\'on, Ana Corber\'an-Vallet, Adina Iftimi, Carmen \'I\~niguez</dc:creator>
    </item>
    <item>
      <title>Dynamic functional brain connectivity results depend on modeling assumptions: comparing frequentist and Bayesian hypothesis tests</title>
      <link>https://arxiv.org/abs/2512.01513</link>
      <description>arXiv:2512.01513v1 Announce Type: new 
Abstract: Understanding the temporal dynamics of functional brain connectivity is important for addressing various questions in network neuroscience, such as how connectivity affects cognition and changes with disease. A fundamental challenge is to evaluate whether connectivity truly exhibits dynamics, or simply is static. The most common frequentist approach uses sliding-window methods to model functional connectivity over time, but this requires defining appropriate sampling distributions and hyperparameters, such as window length, which imposes specific assumptions on the dynamics. Here, we explore how these assumptions influence the detection of dynamic connectivity, and introduce an alternative approach based on Bayesian hypothesis testing with Wishart processes. This framework encodes assumptions through prior distributions, allowing prior knowledge on the time-dependent structure of connectivity to be incorporated into the model. Moreover, this framework provides evidence for both dynamic and static connectivity, offering additional information. Using simulations, we compare the frequentist and Bayesian approaches and demonstrate how different assumptions affect the detection of dynamic connectivity. Finally, by applying both approaches to an fMRI working-memory task, we find that conclusions at the individual level vary with modeling choices, while group-level results are more robust. Our work highlights the importance of carefully considering modeling assumptions when evaluating dynamic connectivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01513v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hester Huijsdens, Linda Geerligs, Max Hinne</dc:creator>
    </item>
    <item>
      <title>Detecting Model Misspecification in Bayesian Inverse Problems via Variational Gradient Descent</title>
      <link>https://arxiv.org/abs/2512.01667</link>
      <description>arXiv:2512.01667v1 Announce Type: new 
Abstract: Bayesian inference is optimal when the statistical model is well-specified, while outside this setting Bayesian inference can catastrophically fail; accordingly a wealth of post-Bayesian methodologies have been proposed. Predictively oriented (PrO) approaches lift the statistical model $P_\theta$ to an (infinite) mixture model $\int P_\theta \; \mathrm{d}Q(\theta)$ and fit this predictive distribution via minimising an entropy-regularised objective functional. In the well-specified setting one expects the mixing distribution $Q$ to concentrate around the true data-generating parameter in the large data limit, while such singular concentration will typically not be observed if the model is misspecified. Our contribution is to demonstrate that one can empirically detect model misspecification by comparing the standard Bayesian posterior to the PrO `posterior' $Q$. To operationalise this, we present an efficient numerical algorithm based on variational gradient descent. A simulation study, and a more detailed case study involving a Bayesian inverse problem in seismology, confirm that model misspecification can be automatically detected using this framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01667v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qingyang Liu, Matthew A. Fisher, Zheyang Shen, Katy Tant, Xuebin Zhao, Andrew Curtis, Chris. J. Oates</dc:creator>
    </item>
    <item>
      <title>The partial K function</title>
      <link>https://arxiv.org/abs/2512.01823</link>
      <description>arXiv:2512.01823v1 Announce Type: new 
Abstract: The K function and its related statistics have been an enduring tool in the analysis of spatial point processes, providing an easy to compute and interpret summary statistic for characterising the interactions between points of one type, or between two different types of points. In this paper, introduce a partial K function, enabling us to account for some of the effects of the other point types when analysing point-point interactions. The partial K function we introduce reduces to the usual K function when the other points are independent of the points of interest and has a similar interpretation. Using examples, we demonstrate how the partial K function can unpick dependence between point types that would otherwise be hidden in the usual K function. We also discuss important bias correction steps and hyperparameter selection. In addition, we discuss an extension to account for other spatial covariates, and demonstrate the methodology on the Lansing Woods dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01823v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jake P. Grainger, Tuomas A. Rajala, David J. Murrell, Sofia C. Olhede</dc:creator>
    </item>
    <item>
      <title>A discomfort-informed adaptive Gibbs sampler for finite mixture models</title>
      <link>https://arxiv.org/abs/2512.01847</link>
      <description>arXiv:2512.01847v1 Announce Type: new 
Abstract: Finite mixture models are frequently used to uncover latent structures in high-dimensional datasets (e.g.\ identifying clusters of patients in electronic health records). The inference of such structures can be performed in a Bayesian framework, and involves the use of sampling algorithms such as Gibbs samplers aimed at deriving posterior distribution of the probabilities of observations to belong to specific clusters. Unfortunately, traditional implementations of Gibbs samplers in this context often face critical challenges, such as inefficient use of computational resources and unnecessary updates for observations that are highly likely to remain in their current cluster. This paper introduces a new adaptive Gibbs sampler that improves the convergence efficiency over existing methods. In particular, our sampler is guided by a function that, at each iteration, uses the past of the chain to focus the updating on observations potentially misclassified in the current clustering, i.e.\ those with a low probability of belonging to their current component. Through simulation studies and two real data analyses, we empirically demonstrate that, in terms of convergence time, our method tends to perform more efficiently compared to state-of-the-art approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01847v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davide Fabbrico, Andi Q. Wang, Sebastiano Grazzi, Alice Corbella, Gareth O. Roberts, Sylvia Richardson, Filippo Pagani, Paul D. W. Kirk</dc:creator>
    </item>
    <item>
      <title>Statistical Inference under Adaptive Sampling with LinUCB</title>
      <link>https://arxiv.org/abs/2512.00222</link>
      <description>arXiv:2512.00222v1 Announce Type: cross 
Abstract: Adaptively collected data has become ubiquitous within modern practice. However, even seemingly benign adaptive sampling schemes can introduce severe biases, rendering traditional statistical inference tools inapplicable. This can be mitigated by a property called stability, which states that if the rate at which an algorithm takes actions converges to a deterministic limit, one can expect that certain parameters are asymptotically normal. Building on a recent line of work for the multi-armed bandit setting, we show that the linear upper confidence bound (LinUCB) algorithm for linear bandits satisfies this property. In doing so, we painstakingly characterize the behavior of the eigenvalues and eigenvectors of the random design feature covariance matrix in the setting where the action set is the unit ball, showing that it decomposes into a rank-one direction that locks onto the true parameter and an almost-isotropic bulk that grows at a predictable $\sqrt{T}$ rate. This allows us to establish a central limit theorem for the LinUCB algorithm, establishing asymptotic normality for the limiting distribution of the estimation error where the convergence occurs at a $T^{-1/4}$ rate. The resulting Wald-type confidence sets and hypothesis tests do not depend on the feature covariance matrix and are asymptotically tighter than existing nonasymptotic confidence sets. Numerical simulations corroborate our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00222v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Fan, Kevin Tan, Yuting Wei</dc:creator>
    </item>
    <item>
      <title>Improved Inference for Nonparametric Regression</title>
      <link>https://arxiv.org/abs/2512.00566</link>
      <description>arXiv:2512.00566v1 Announce Type: cross 
Abstract: Nonparametric regression estimators, including those employed in regression-discontinuity designs (RDD), are central to the economist's toolbox. Their application, however, is complicated by the presence of asymptotic bias, which undermines coverage accuracy of conventional confidence intervals. Extant solutions to the problem include debiasing methods, such as the widely applied robust bias-corrected (RBC) confidence interval of Calonico et al. (2014, 2018). We show that this interval is equivalent to a prepivoted interval based on an invalid residual-based bootstrap method. Specifically, prepivoting performs an implicit bias correction while adjusting the nonparametric regression estimator's standard error to account for the additional uncertainty introduced by debiasing. This idea can also be applied to other bootstrap schemes, leading to new implicit bias corrections and corresponding standard error adjustments. We propose a prepivoted interval based on a bootstrap that generates observations from nonparametric regression estimates at each regressor value and show how it can be implemented as an RBC-type interval without the need for resampling. Importantly, we show that the new interval is shorter than the existing RBC interval. For example, with the Epanechnikov kernel, the length is reduced by 17%, while maintaining accurate coverage probability. This result holds irrespectively of: (a) the evaluation point being in the interior or on the boundary; (b) the use of a 'small' or 'large' bandwidths; (c) the distribution of the regressor and the error term.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00566v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Cavaliere, S\'ilvia Gon\c{c}alves, Morten {\O}rregaard Nielsen, Edoardo Zanelli</dc:creator>
    </item>
    <item>
      <title>ARCADIA: Scalable Causal Discovery for Corporate Bankruptcy Analysis Using Agentic AI</title>
      <link>https://arxiv.org/abs/2512.00839</link>
      <description>arXiv:2512.00839v1 Announce Type: cross 
Abstract: This paper introduces ARCADIA, an agentic AI framework for causal discovery that integrates large-language-model reasoning with statistical diagnostics to construct valid, temporally coherent causal structures. Unlike traditional algorithms, ARCADIA iteratively refines candidate DAGs through constraint-guided prompting and causal-validity feedback, leading to stable and interpretable models for real-world high-stakes domains. Experiments on corporate bankruptcy data show that ARCADIA produces more reliable causal graphs than NOTEARS, GOLEM, and DirectLiNGAM while offering a fully explainable, intervention-ready pipeline. The framework advances AI by demonstrating how agentic LLMs can participate in autonomous scientific modeling and structured causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00839v1</guid>
      <category>cs.AI</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabrizio Maturo, Donato Riccio, Andrea Mazzitelli, Giuseppe Bifulco, Francesco Paolone, Iulia Brezeanu</dc:creator>
    </item>
    <item>
      <title>A Clinical Instrument to Measure Patient Anecdotes in Clinical Trials</title>
      <link>https://arxiv.org/abs/2512.01041</link>
      <description>arXiv:2512.01041v1 Announce Type: cross 
Abstract: Clinical trials assessing neurological treatment are challenging due to the diversity of brain function, and the difficulty in quantifying it. Traditional treatment studies in epilepsy use seizure frequency as the primary outcome measure, which may overlooking meaningful improvements in patients' quality of life. This paper introduces the Clinical Instrument for Measuring Patient Anecdotes in Clinical Trials (Clinical IMPACT), a novel tool designed to capture qualitative non-seizure improvement across neurological domains.
  The Clinical IMPACT incorporates open-ended inquiries that allow participants or caregivers to identify and select anecdotal evidence of their most significant treatment benefits. A blinded panel of experts ranks these anecdotes, facilitating a rigorous statistical analysis using the Wilcoxon Rank-Sum Test to detect treatment efficacy. The approach is resistant to type 1 error, yet comprehensive in its ability to capture real-world effects on quality of life.
  The potential of the Clinical IMPACT tool to enhance sensitivity while also providing qualitative insights that can inform patients, healthcare providers, and regulatory bodies about treatment effects makes it important to consider in any neurological trial. We describe how it can be used in epilepsy, and advocate for its inclusion as a key secondary endpoint to provide a perspective on non-seizure outcomes, which have previously been challenging to measure, let alone to interpret, even when the clinical trial is positive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01041v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ian Miller, Ann Hyslop, Colin Decker</dc:creator>
    </item>
    <item>
      <title>Differential Geometry of the Fixed-Rank Core Covariance Manifold</title>
      <link>https://arxiv.org/abs/2512.01070</link>
      <description>arXiv:2512.01070v1 Announce Type: cross 
Abstract: We study the differential geometry of the fixed-rank core covariance manifold. According to Hoff, McCormack, and Zhang [J. R. Stat. Soc., B: Stat., 85 (2023), pp. 1659--1679], every covariance matrix $\Sigma$ of $p_1\times p_2$ matrix-variate data uniquely decomposes into a separable component $K$ and a core component $C$. Such a decomposition may exist for rank-$r$ $\Sigma$ if $p_1/p_2+p_2/p_1&lt;r$, with $C$ sharing the same rank. They posed an open question on whether a partial-isotropy structure can be imposed on $C$ for high-dimensional covariance estimation. We address this question by showing that a partial-isotropy rank-$r$ core is a non-trivial convex combination of a rank-$r$ core and $I_p$ for $p:=p_1p_2$, motivating the study of rank-$r$ cores. For fixed $r&gt;p_1/p_2+p_2/p_1$, we prove that the set of rank-$r$ cores, $\mathcal{C}_{p_1,p_2,r}^+$, is a compact, smooth, embedded submanifold of the set of rank-$r$ positive semi-definite matrices, except for a measure-zero subset associated with canonical decomposability. When $r=p$, the set of full-rank cores $\mathcal{C}_{p_1,p_2}^{++}$ is itself a smooth manifold. Moreover, the positive definite cone $\mathcal{S}_p^{++}$ is diffeomorphic to the product of the Kronecker and core covariance manifolds, providing new geometric insight into $\mathcal{S}_p^{++}$ via separability. Differential geometric quantities, such as the differential of the diffeomorphism, as well as the Riemannian gradient and Hessian operator on $\mathcal{C}_{p_1,p_2}^{++}$ and the manifolds used in constructing $\mathcal{C}_{p_1,p_2,r}^+$, are also derived. Lastly, we propose a partial-isotropy core shrinkage estimator for matrix-variate data, supported by numerical illustrations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01070v1</guid>
      <category>math.DG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bongjung Sung</dc:creator>
    </item>
    <item>
      <title>Discriminative classification with generative features: bridging Naive Bayes and logistic regression</title>
      <link>https://arxiv.org/abs/2512.01097</link>
      <description>arXiv:2512.01097v1 Announce Type: cross 
Abstract: We introduce Smart Bayes, a new classification framework that bridges generative and discriminative modeling by integrating likelihood-ratio-based generative features into a logistic-regression-style discriminative classifier. From the generative perspective, Smart Bayes relaxes the fixed unit weights of Naive Bayes by allowing data-driven coefficients on density-ratio features. From a discriminative perspective, it constructs transformed inputs as marginal log-density ratios that explicitly quantify how much more likely each feature value is under one class than another, thereby providing predictors with stronger class separation than the raw covariates. To support this framework, we develop a spline-based estimator for univariate log-density ratios that is flexible, robust, and computationally efficient. Through extensive simulations and real-data studies, Smart Bayes often outperforms both logistic regression and Naive Bayes. Our results highlight the potential of hybrid approaches that exploit generative structure to enhance discriminative performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01097v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zachary Terner, Alexander Petersen, Yuedong Wang</dc:creator>
    </item>
    <item>
      <title>A novel sequential method for building upper and lower bounds of moments of distributions</title>
      <link>https://arxiv.org/abs/2512.01761</link>
      <description>arXiv:2512.01761v1 Announce Type: cross 
Abstract: Approximating integrals is a fundamental task in probability theory and statistical inference, and their applied fields of signal processing, and Bayesian learning, as soon as expectations over probability distributions must be computed efficiently and accurately. When these integrals lack closed-form expressions, numerical methods must be used, from the Newton-Cotes formulas and Gaussian quadrature, to Monte Carlo and variational approximation techniques. Despite these numerous tools, few are guaranteed to preserve majoration/minoration inequalities, while this feature is fundamental in certain applications in statistics. In this paper, we focus on the integration problem arising in the estimation of moments of scalar, unnormalized, distributions. We introduce a sequential method for constructing upper and lower bounds on the sought integral. Our approach leverages the majorization-minimization framework to iteratively refine these bounds, in an enveloped principle. The method has proven convergence, and controlled accuracy, under mild conditions. We demonstrate its effectiveness through a detailed numerical example of the estimation of a Monte-Carlo sampler variance in a Bayesian inference problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01761v1</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Solal Martin, Emilie Chouzenoux, Victor Elvira</dc:creator>
    </item>
    <item>
      <title>Jenss-Bayley Latent Change Score Model with Individual Ratio of Growth Acceleration in the Framework of Individual Measurement Occasions</title>
      <link>https://arxiv.org/abs/2103.00290</link>
      <description>arXiv:2103.00290v4 Announce Type: replace 
Abstract: Longitudinal data analysis has been widely employed to examine between-individual differences in within-individual changes. One challenge of such analyses is that the rate-of-change is only available indirectly when change patterns are nonlinear with respect to time. Latent change score models (LCSMs), which can be employed to investigate the change in rate-of-change at the individual level, have been developed to address this challenge. We extend an existing LCSM with the Jenss-Bayley growth curve \cite[Chapter~18]{Grimm2016growth} and propose a novel expression for change scores that allows for (1) unequally-spaced study waves and (2) individual measurement occasions around each wave. We also extend the existing model to estimate the individual ratio of the growth acceleration (that largely determines the trajectory shape and is viewed as the most important parameter in the Jenss-Bayley model). We present the proposed model by a simulation study and a real-world data analysis. Our simulation study demonstrates that the proposed model can estimate the parameters unbiasedly and precisely and exhibit target confidence interval coverage. The simulation study also shows that the proposed model with the novel expression for the change scores outperforms the existing model. An empirical example using longitudinal reading scores shows that the model can estimate the individual ratio of the growth acceleration and generate individual rate-of-change in practice. We also provide the corresponding code for the proposed model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2103.00290v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin Liu</dc:creator>
    </item>
    <item>
      <title>Extending Latent Basis Growth Model to Explore Joint Development in the Framework of Individual Measurement Occasions</title>
      <link>https://arxiv.org/abs/2107.01773</link>
      <description>arXiv:2107.01773v5 Announce Type: replace 
Abstract: Longitudinal processes often pose nonlinear change patterns. Latent basis growth models (LBGMs) provide a versatile solution without requiring specific functional forms. Building on the LBGM specification for unequally-spaced waves and individual occasions proposed by Liu and Perera (2023), we extend LBGMs to multivariate longitudinal outcomes. This provides a unified approach to nonlinear, interconnected trajectories. Simulation studies demonstrate that the proposed model can provide unbiased and accurate estimates with target coverage probabilities for the parameters of interest. Real-world analyses of reading and mathematics scores demonstrates its effectiveness in analyzing joint developmental processes that vary in temporal patterns. Computational code is included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.01773v5</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin Liu</dc:creator>
    </item>
    <item>
      <title>Multiple Randomization Designs: Estimation and Inference with Interference</title>
      <link>https://arxiv.org/abs/2112.13495</link>
      <description>arXiv:2112.13495v4 Announce Type: replace 
Abstract: Completely randomized experiments, originally developed by Fisher and Neyman in the 1930s, are still widely used in practice, even in online experimentation. However, such designs are of limited value for answering standard questions in marketplaces, where multiple populations of agents interact strategically, leading to complex patterns of spillover effects. In this paper, we derive the finite-sample properties of tractable estimators for "Simple Multiple Randomization Designs" (SMRDs), a new class of experimental designs which account for complex spillover effects in randomized experiments. Our derivations are obtained under a natural and general form of cross-unit interference, which we call "local interference". We discuss the estimation of main effects, direct effects, and spillovers, and present associated central limit theorems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.13495v4</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssb/qkaf073</arxiv:DOI>
      <dc:creator>Lorenzo Masoero, Suhas Vijaykumar, Thomas Richardson, James McQueen, Ido Rosen, Brian Burdick, Pat Bajari, Guido Imbens</dc:creator>
    </item>
    <item>
      <title>Decomposing Impact on Longitudinal Outcome of Time-varying Covariate into Baseline Effect and Temporal Effect</title>
      <link>https://arxiv.org/abs/2210.16916</link>
      <description>arXiv:2210.16916v3 Announce Type: replace 
Abstract: Longitudinal processes are often associated with each other over time; therefore, it is important to investigate the associations among developmental processes and understand their joint development. The latent growth curve model (LGCM) with a time-varying covariate (TVC) provides a method to estimate the TVC's effect on a longitudinal outcome while simultaneously modeling the outcome's change. However, it does not allow the TVC to predict variations in the random growth coefficients. We propose decomposing the TVC's effect into initial trait and temporal states using three methods to address this limitation. In each method, the baseline of the TVC is viewed as an initial trait, and the corresponding effects are obtained by regressing random intercepts and slopes on the baseline value. Temporal states are characterized as (1) interval-specific slopes, (2) interval-specific changes, or (3) changes from the baseline at each measurement occasion, depending on the method. We demonstrate our methods through simulations and real-world data analyses, assuming a linear-linear functional form for the longitudinal outcome. The results demonstrate that LGCMs with a decomposed TVC can provide unbiased and precise estimates with target confidence intervals. We also provide OpenMx and Mplus 8 code for these methods with commonly used linear and nonlinear functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.16916v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin Liu</dc:creator>
    </item>
    <item>
      <title>Further Exploration of the Effects of Time-varying Covariate in Growth Mixture Models with Nonlinear Trajectories</title>
      <link>https://arxiv.org/abs/2301.06014</link>
      <description>arXiv:2301.06014v3 Announce Type: replace 
Abstract: Growth mixture modeling (GMM) is an analytical tool for identifying multiple unobserved sub-populations of longitudinal processes. In particular, it describes change patterns within each latent sub-population and examines between-individual differences in within-individual change for each sub-group. One research interest in utilizing GMMs is to explore how covariates affect such heterogeneity in change patterns. Liu and Perera (2022c) extended mixture-of-experts (MoE) models, which mainly focus on time-invariant covariates, for allowing the covariates to account for within-group and between-group differences simultaneously and examining the heterogeneity in nonlinear trajectories. The present study further extends Liu and Perera (2022c) and examines the effects on trajectory heterogeneity of time-varying covariates (TVCs). Specifically, we propose methods to decompose a TVC into a trait feature (e.g., the baseline value of the TVC) and a set of state features (e.g., interval-specific slopes or changes). The trait features are allowed to account for within-group differences in growth factors of trajectories (i.e., trait effect), and the state features are allowed to impact observed values of a longitudinal process (i.e., state effect). We examine the proposed models using a simulation study and a real-world data analysis. The simulation study demonstrated that the proposed models are capable of separating trajectories into several clusters and generally generating unbiased and accurate estimates with target coverage probabilities. With the proposed models, we showed the heterogeneity in the trait and state features of reading ability across latent classes of students' mathematics performance. Meanwhile, the trait and state effects on mathematics development of reading ability are also heterogeneous across the clusters of students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.06014v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin Liu</dc:creator>
    </item>
    <item>
      <title>Computational Approaches for Exponential-Family Factor Analysis</title>
      <link>https://arxiv.org/abs/2403.14925</link>
      <description>arXiv:2403.14925v3 Announce Type: replace 
Abstract: We study a general factor analysis framework where the $n$-by-$p$ data matrix is assumed to follow a general exponential family distribution entry-wise. While this model framework has been proposed before, we here further relax its distributional assumption by using a quasi-likelihood setup. By parameterizing the mean-variance relationship on data entries, we additionally introduce a dispersion parameter and entry-wise weights to model large variations and missing values. The resulting model is thus not only robust to distribution misspecification but also more flexible and able to capture mean-dependent covariance structures of the data matrix. Our main focus is on efficient computational approaches to perform the factor analysis. Previous modeling frameworks rely on simulated maximum likelihood (SML) to find the factorization solution, but this method was shown to lead to asymptotic bias when the simulated sample size grows slower than the square root of the sample size $n$, eliminating its practical application for data matrices with large $n$. Borrowing from expectation-maximization (EM) and stochastic gradient descent (SGD), we investigate three estimation procedures based on iterative factorization updates. Our proposed solution does not show asymptotic biases, and scales even better for large matrix factorizations with error $O(1/p)$. To support our findings, we conduct simulation experiments and discuss its application in four case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14925v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Liang Wang, Luis Carvalho</dc:creator>
    </item>
    <item>
      <title>Blessing of dimension in Bayesian inference on covariance matrices</title>
      <link>https://arxiv.org/abs/2404.03805</link>
      <description>arXiv:2404.03805v3 Announce Type: replace 
Abstract: Bayesian factor analysis is routinely used for dimensionality reduction in modeling of high-dimensional covariance matrices. Factor analytic decompositions express the covariance as a sum of a low rank and diagonal matrix. In practice, Gibbs sampling algorithms are typically used for posterior computation, alternating between updating the latent factors, loadings, and residual variances. In this article, we exploit a blessing of dimensionality to develop a provably accurate posterior approximation for the covariance matrix that bypasses the need for Gibbs or other variants of Markov chain Monte Carlo sampling. Our proposed Factor Analysis with BLEssing of dimensionality (FABLE) approach relies on a first-stage singular value decomposition (SVD) to estimate the latent factors, and then defines a jointly conjugate prior for the loadings and residual variances. The accuracy of the resulting posterior approximation for the covariance improves with increasing samples as well as increasing dimensionality. We show that FABLE has excellent performance in high-dimensional covariance matrix estimation, including producing well-calibrated credible intervals, both theoretically and through simulation experiments. We also demonstrate the strength of our approach in terms of accurate inference and computational efficiency by applying it to a gene expression dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03805v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shounak Chattopadhyay, Anru R. Zhang, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Robust Estimation of Polychoric Correlation</title>
      <link>https://arxiv.org/abs/2407.18835</link>
      <description>arXiv:2407.18835v5 Announce Type: replace 
Abstract: Polychoric correlation is often an important building block in the analysis of rating data, particularly for structural equation models. However, the commonly employed maximum likelihood (ML) estimator is highly susceptible to misspecification of the polychoric correlation model, for instance through violations of latent normality assumptions. We propose a novel estimator that is designed to be robust against partial misspecification of the polychoric model, that is, when the model is misspecified for an unknown fraction of observations, such as careless respondents. To this end, the estimator minimizes a robust loss function based on the divergence between observed frequencies and theoretical frequencies implied by the polychoric model. In contrast to existing literature, our estimator makes no assumption on the type or degree of model misspecification. It furthermore generalizes ML estimation, is consistent as well as asymptotically normally distributed, and comes at no additional computational cost. We demonstrate the robustness and practical usefulness of our estimator in simulation studies and an empirical application on a Big Five administration. In the latter, the polychoric correlation estimates of our estimator and ML differ substantially, which, after further inspection, is likely due to the presence of careless respondents that the estimator helps identify.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18835v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1017/psy.2025.10066</arxiv:DOI>
      <arxiv:journal_reference>Psychometrika (2025+), forthcoming</arxiv:journal_reference>
      <dc:creator>Max Welz, Patrick Mair, Andreas Alfons</dc:creator>
    </item>
    <item>
      <title>Differential Test Functioning via Robust Scaling</title>
      <link>https://arxiv.org/abs/2409.03502</link>
      <description>arXiv:2409.03502v4 Announce Type: replace 
Abstract: In the item response theory (IRT) literature, differential test functioning (DTF) has been conceptualized in terms of how the test response function differs over groups of respondents. This paper presents an alternative approach to DTF that focusses on how the distribution of the latent trait differs over groups, which is referred to as impact. We propose to compare two estimates of impact, one that naively aggregates over all test items and one that down-weights items that exhibit differential item functioning (DIF). Taking this approach, we make the following three contributions to the literature on DTF. First it is shown that the difference between the two estimates provides a convenient effect size for quantifying the extent to which DIF affects conclusions about impact (as opposed to test scores). Second, we provide a relatively general purpose Wald test of the difference between two estimates of impact. Third, we extend the recent literature on robust scaling to propose a procedure for down-weighting items that is shown to produce consistent estimates of impact whenever fewer than 1/2 of items exhibit DIF. Using simulations and an empirical example from physics education, we show how the proposed effect size and test statistic perform using the proposed robust estimator of impact, as well as estimators that arise from conventional item-by-item tests of DIF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03502v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Peter F. Halpin</dc:creator>
    </item>
    <item>
      <title>Localized Conformal Multi-Quantile Regression</title>
      <link>https://arxiv.org/abs/2411.19523</link>
      <description>arXiv:2411.19523v3 Announce Type: replace 
Abstract: Standard conformal prediction methods guarantee marginal coverage but often produce inefficient intervals that fail to adapt to local heteroscedasticity, while recent localized approaches often struggle to maintain validity across distinct subpopulations with varying noise profiles. To address these challenges, we introduce Localized Conformal Multi-Quantile Regression (LCMQR), a novel framework that synergizes multi-quantile information with kernel-based localization to construct efficient and adaptive prediction intervals. Theoretically, we resolve an inconsistency in Conformalized Composite Quantile Regression (CCQR) by proving that our consistent Average-then-Max scoring mechanism systematically yields tighter intervals than the Max-then-Average approach used in prior work. For heterogeneous environments, we extend this framework to Group-Calibrated LCMQR (GC-LCMQR) via a stratified calibration step that guarantees finite-sample validity within distinct subgroups. Experiments on benchmark datasets and an Individual Treatment Effect (ITE) task demonstrate that LCMQR achieves superior efficiency on standard benchmarks, while GC-LCMQR uniquely achieves group-level coverage for target subgroups in mixture populations where baselines fail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19523v3</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Lu</dc:creator>
    </item>
    <item>
      <title>Using a Two-Parameter Sensitivity Analysis Framework to Efficiently Combine Randomized and Non-randomized Studies</title>
      <link>https://arxiv.org/abs/2412.03731</link>
      <description>arXiv:2412.03731v2 Announce Type: replace 
Abstract: Causal inference is vital for informed decision-making across fields such as biomedical research and social sciences. Randomized controlled trials (RCTs) are considered the gold standard for internal validity of inferences, whereas observational studies (OSs) often provide the opportunity for greater external validity. However, both data sources have inherent limitations preventing their use for broadly valid statistical inferences: RCTs may lack generalizability due to their selective eligibility criterion, and OSs are vulnerable to unobserved confounding. This paper proposes an innovative approach to integrate RCT and OS that borrows the other study's strengths to remedy each study's limitations. The method uses a novel triplet matching algorithm to align RCT and OS samples and a new two-parameter sensitivity analysis framework to quantify internal and external validity biases. This combined approach yields causal estimates that are more robust to hidden biases than OSs alone and provides reliable inferences about the treatment effect in the general population. We apply this method to investigate the effects of lactation on maternal health using a small RCT and a long-term observational health records dataset from the California National Primate Research Center. This application demonstrates the practical utility of our approach in generating scientifically sound and actionable causal estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03731v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruoqi Yu, Bikram Karmakar, Jessica Vandeleest, Eleanor Bimla Schwarz</dc:creator>
    </item>
    <item>
      <title>How Many Human Survey Respondents is a Large Language Model Worth? An Uncertainty Quantification Perspective</title>
      <link>https://arxiv.org/abs/2502.17773</link>
      <description>arXiv:2502.17773v4 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly used to simulate survey responses, but synthetic data can be misaligned with the human population, leading to unreliable inference. We develop a general framework that converts LLM-simulated responses into reliable confidence sets for population parameters of human responses, addressing the distribution shift between the simulated and real populations. The key design choice is the number of simulated responses: too many produce overly narrow sets with poor coverage, while too few yield excessively loose estimates. We propose a data-driven approach that adaptively selects the simulation sample size to achieve nominal average-case coverage, regardless of the LLM's simulation fidelity or the confidence set construction procedure. The selected sample size is further shown to reflect the effective human population size that the LLM can represent, providing a quantitative measure of its simulation fidelity. Experiments on real survey datasets reveal heterogeneous fidelity gaps across different LLMs and domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17773v4</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengpiao Huang, Yuhang Wu, Kaizheng Wang</dc:creator>
    </item>
    <item>
      <title>Bayesian Synthetic Control with a Soft Simplex Constraint</title>
      <link>https://arxiv.org/abs/2503.06454</link>
      <description>arXiv:2503.06454v2 Announce Type: replace 
Abstract: The challenges posed by high-dimensional data and use of the simplex constraint are two major concerns in the empirical application of the synthetic control method (SCM) in econometric studies. To address both issues simultaneously, we propose a Bayesian SCM that integrates a soft simplex constraint within spike-and-slab variable selection. The hierarchical prior structure captures the extent to which the data supports the simplex constraint, allowing for more efficient and data-adaptive counterfactual estimation. The intractable marginal likelihood induced by the soft simplex constraint presents a major computational challenge, which we resolve by developing a novel Metropolis-within-Gibbs algorithm that updates the regression coefficients of two predictors simultaneously. Our main theoretical contribution is a high-dimensional selection consistency result for the spike-and-slab variable selection under the simplex constraint, which significantly extends the current theory for high-dimensional Bayesian variable selection. Simulation studies demonstrate that our method performs well across diverse settings. To illustrate its practical values, we apply it to two empirical examples for estimating the effect of economic policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06454v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihong Xu, Quan Zhou</dc:creator>
    </item>
    <item>
      <title>An interpretable family of projected normal distributions and a related copula model for Bayesian analysis of hypertoroidal data</title>
      <link>https://arxiv.org/abs/2508.16432</link>
      <description>arXiv:2508.16432v2 Announce Type: replace 
Abstract: This paper introduces two families of probability distributions for Bayesian analysis of hypertoroidal data. The first family consists of symmetric distributions derived from the projection of multivariate normal distributions under specific parameter constraints. This family is closed under marginalization and hence any marginal distribution belongs to a lower-dimensional case of the same family. In particular the univariate marginal of the family is the unimodal case of the projected normal distribution on the circle. The second family is a flexible extension of the copula case of the first family, which can accommodate any univariate marginal distributions. Unlike existing models derived via projection, both families have the common advantage that their parameters possess a clear and intuitive interpretation. The use of latent variables simplifies Bayesian estimation using Markov chain Monte Carlo algorithms. The usefulness of the proposed families is demonstrated through the analysis of a meteorological dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16432v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shogo Kato, Gianluca Mastrantonio, Masayuki Ishikawa</dc:creator>
    </item>
    <item>
      <title>hdMTD: An R Package for High-Dimensional Mixture Transition Distribution Models</title>
      <link>https://arxiv.org/abs/2509.01808</link>
      <description>arXiv:2509.01808v2 Announce Type: replace 
Abstract: Several natural phenomena exhibit long-range conditional dependencies. High-order mixture transition distribution (MTD) are parsimonious non-parametric models to study these phenomena. An MTD is a Markov chain in which the transition probabilities are expressed as a convex combination of lower-order conditional distributions. Despite their generality, inference for MTD models has traditionally been limited by the need to estimate high-dimensional joint distributions. In particular, for a sample of size n, the feasible order d of the MTD is typically restricted to d approximately O(log n). To overcome this limitation, Ost and Takahashi (2023) recently introduced a computationally efficient non-parametric inference method that identifies the relevant lags in high-order MTD models, even when d is approximately O(n), provided that the set of relevant lags is sparse. In this article, we introduce hdMTD, an R package allowing us to estimate parameters of such high-dimensional Markovian models. Given a sample from an MTD chain, hdMTD can retrieve the relevant past set using the BIC algorithm or the forward stepwise and cut algorithm described in Ost and Takahashi (2023). The package also computes the maximum likelihood estimate for transition probabilities and estimates high-order MTD parameters through the expectation-maximization algorithm. Additionally, hdMTD also allows for simulating an MTD chain from its stationary invariant distribution using the perfect (exact) sampling algorithm, enabling Monte Carlo simulation of the model. We illustrate the package's capabilities through simulated data and a real-world application involving temperature records from Brazil.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01808v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maiara Gripp, Giulio Iacobelli, Guilherme Ost, Daniel Y. Takahashi</dc:creator>
    </item>
    <item>
      <title>Beyond Linearity and Time-Homogeneity: Relational Hyper Event Models with Time-Varying Non-Linear Effects</title>
      <link>https://arxiv.org/abs/2509.05289</link>
      <description>arXiv:2509.05289v3 Announce Type: replace 
Abstract: Recent technological advances have made it easier to collect large and complex networks of time-stamped relational events connecting two or more entities. Relational hyper-event models (RHEMs) aim to explain the dynamics of these events by modeling the event rate as a function of statistics based on past history and external information.
  However, despite the complexity of the data, most current RHEM approaches still rely on a linearity assumption to model this relationship. In this work, we address this limitation by introducing a more flexible model that allows the effects of statistics to vary non-linearly and over time. While time-varying and non-linear effects have been used in relational event modeling, we take this further by modeling joint time-varying and non-linear effects using tensor product smooths.
  We validate our methodology on both synthetic and empirical data. In particular, we use RHEMs to study how patterns of scientific collaboration and impact evolve over time. Our approach provides deeper insights into the dynamic factors driving relational hyper-events, allowing us to evaluate potential non-monotonic patterns that cannot be identified using linear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05289v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martina Boschi, J\"urgen Lerner, Ernst C. Wit</dc:creator>
    </item>
    <item>
      <title>A Weighted Regression Approach to Break-Point Detection in Panel Data</title>
      <link>https://arxiv.org/abs/2510.00598</link>
      <description>arXiv:2510.00598v2 Announce Type: replace 
Abstract: New procedures for detecting a change in the cross-sectional mean of panel data are proposed. The procedures rely on estimating nuisance parameters using certain cross-sectional means across panels using a weighted least squares regression. In the case of weak cross-sectional dependence between panels, we show how test statistics can be constructed to have a limit null distribution not depending on any choice of bandwidths typically needed to estimate the long-run variances of the panel errors. The theoretical assertions are derived for general choices of the regression weights, and it is shown that consistent test procedures can be obtained from the proposed process. The theoretical results are extended to the case where strong cross-sectional dependence exist between panels. The paper concludes with a numerical study illustrating the behavior of several special cases of the test procedure in finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00598v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Charl Pretorius, Heinrich Roodt</dc:creator>
    </item>
    <item>
      <title>Robust Estimation for Dependent Binary Network Data</title>
      <link>https://arxiv.org/abs/2510.22177</link>
      <description>arXiv:2510.22177v2 Announce Type: replace 
Abstract: We consider the problem of learning the interaction strength between the nodes of a network based on dependent binary observations residing on these nodes, generated from a Markov Random Field (MRF). Since these observations can possibly be corrupted/noisy in larger networks in practice, it is important to robustly estimate the parameters of the underlying true MRF to account for such inherent contamination in observed data. However, it is well-known that classical likelihood and pseudolikelihood based approaches are highly sensitive to even a small amount of data contamination. So, in this paper, we propose a density power divergence (DPD) based robust generalization of the computationally efficient maximum pseudolikelihood (MPL) estimator of the interaction strength parameter, and derive its rate of consistency under the pure model. Along the way, we establish consistency and asymptotics for a class of general $Z$-estimators, covering our proposed DPD based estimators, under flexible assumptions that hold for a substantial class of standard models. To the best of our knowledge, these are the first central limit theorems for the class of general $Z$-estimators in such settings. Moreover, we show that the gross error sensitivities of the proposed DPD based estimators are significantly smaller than that of the MPL estimator, thereby theoretically justifying the greater (local) robustness of the former under contaminated settings. Finally, we demonstrate the superior (finite sample) performance of the DPD based variants over the traditional MPL estimator in a number of synthetically generated contaminated network datasets, and apply them to learn the network interaction strength in several real datasets from diverse domains of social science, neurobiology and genomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22177v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyu Liu, Somabha Mukherjee, Abhik Ghosh</dc:creator>
    </item>
    <item>
      <title>Scalable approximation of the transformation-free linear simplicial-simplicial regression via constrained iterative reweighted least squares</title>
      <link>https://arxiv.org/abs/2511.13296</link>
      <description>arXiv:2511.13296v3 Announce Type: replace 
Abstract: Simplicial-simplicial regression refers to the regression setting where both the responses and predictor variables lie within the simplex space, i.e. they are compositional. \cite{fiksel2022} proposed a transformation-free linear regression model, that minimizes the Kullback-Leibler divergence from the observed to the fitted compositions, where the EM algorithm is used to estimate the regression coefficients. We formulate the model as a constrained logistic regression, in the spirit of \cite{tsagris2025}, and we estimate the regression coefficients using constrained iteratively reweighted least squares. The simulation studies depict that this algorithm makes the estimation procedure significantly faster, and approximates accurately enough the solution of the EM algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13296v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michail Tsagris</dc:creator>
    </item>
    <item>
      <title>CAP: A General Algorithm for Online Selective Conformal Prediction with FCR Control</title>
      <link>https://arxiv.org/abs/2403.07728</link>
      <description>arXiv:2403.07728v5 Announce Type: replace-cross 
Abstract: We study the problem of post-selection predictive inference in an online fashion. To avoid devoting resources to unimportant units, a preliminary selection of the current individual before reporting its prediction interval is common and meaningful in online predictive tasks. Since the online selection causes a temporal multiplicity in the selected prediction intervals, it is important to control the real-time false coverage-statement rate (FCR) which measures the overall miscoverage level. We develop a general framework named CAP (Calibration after Adaptive Pick) that performs an adaptive pick rule on historical data to construct a calibration set if the current individual is selected and then outputs a conformal prediction interval for the unobserved label. We provide tractable procedures for constructing the calibration set for popular online selection rules. We proved that CAP can achieve an exact selection-conditional coverage guarantee in the finite-sample and distribution-free regimes. To account for the distribution shift in online data, we also embed CAP into some recent dynamic conformal prediction algorithms and show that the proposed method can deliver long-run FCR control. Numerical results on both synthetic and real data corroborate that CAP can effectively control FCR around the target level and yield more narrowed prediction intervals over existing baselines across various settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07728v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yajie Bao, Yuyang Huo, Haojie Ren, Changliang Zou</dc:creator>
    </item>
    <item>
      <title>Autoregressive Networks with Dependent Edges</title>
      <link>https://arxiv.org/abs/2404.15654</link>
      <description>arXiv:2404.15654v2 Announce Type: replace-cross 
Abstract: We propose an autoregressive framework for modelling dynamic networks with dependent edges. It encompasses models that accommodate, for example, transitivity, degree heterogenenity, and other stylized features often observed in real network data. By assuming the edges of networks at each time are independent conditionally on their lagged values, the models, which exhibit a close connection with temporal ERGMs, facilitate both simulation and the maximum likelihood estimation in a straightforward manner. Due to the possibly large number of parameters in the models, the natural MLEs may suffer from slow convergence rates. An improved estimator for each component parameter is proposed based on an iteration employing projection, which mitigates the impact of the other parameters (Chang et al., 2021; Chang et al., 2023). Leveraging a martingale difference structure, the asymptotic distribution of the improved estimator is derived without the assumption of stationarity. The limiting distribution is not normal in general, although it reduces to normal when the underlying process satisfies some mixing conditions. Illustration with a transitivity model was carried out in both simulation and a real network data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15654v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Qin Fang, Eric D. Kolaczyk, Peter W. MacDonald, Qiwei Yao</dc:creator>
    </item>
    <item>
      <title>Stabilizing black-box model selection with the inflated argmax</title>
      <link>https://arxiv.org/abs/2410.18268</link>
      <description>arXiv:2410.18268v3 Announce Type: replace-cross 
Abstract: Model selection is the process of choosing from a class of candidate models given data. For instance, methods such as the LASSO and sparse identification of nonlinear dynamics (SINDy) formulate model selection as finding a sparse solution to a linear system of equations determined by training data. However, absent strong assumptions, such methods are highly unstable: if a single data point is removed from the training set, a different model may be selected. In this paper, we present a new approach to stabilizing model selection with theoretical stability guarantees that leverages a combination of bagging and an ''inflated'' argmax operation. Our method selects a small collection of models that all fit the data, and it is stable in that, with high probability, the removal of any training point will result in a collection of selected models that overlaps with the original collection. We illustrate this method in (a) a simulation in which strongly correlated covariates make standard LASSO model selection highly unstable, (b) a Lotka-Volterra model selection problem focused on identifying how competition in an ecosystem influences species' abundances, (c) a graph subset selection problem using cell-signaling data from proteomics, and (d) unsupervised $\kappa$-means clustering. In these settings, the proposed method yields stable, compact, and accurate collections of selected models, outperforming a variety of benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18268v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Melissa Adrian, Jake A. Soloff, Rebecca Willett</dc:creator>
    </item>
    <item>
      <title>Geometric Calibration and Neutral Zones for Uncertainty-Aware Multi-Class Classification</title>
      <link>https://arxiv.org/abs/2511.20960</link>
      <description>arXiv:2511.20960v2 Announce Type: replace-cross 
Abstract: Modern artificial intelligence systems make critical decisions yet often fail silently when uncertain -- even well-calibrated models provide no mechanism to identify \textit{which specific predictions} are unreliable. We develop a geometric framework addressing both calibration and instance-level uncertainty quantification for neural network probability outputs. Treating probability vectors as points on the $(c-1)$-dimensional probability simplex equipped with the Fisher--Rao metric, we construct: (i) Additive Log-Ratio (ALR) calibration maps that reduce exactly to Platt scaling for binary problems while extending naturally to multi-class settings, and (ii) geometric reliability scores that translate calibrated probabilities into actionable uncertainty measures, enabling principled deferral of ambiguous predictions to human review.
  Theoretical contributions include: consistency of the calibration estimator at rate $O_p(n^{-1/2})$ via M-estimation theory (Theorem~1), and tight concentration bounds for reliability scores with explicit sub-Gaussian parameters enabling sample size calculations for validation set design (Theorem~2). We conjecture Neyman--Pearson optimality of our neutral zone construction based on connections to Bhattacharyya coefficients. Empirical validation on Adeno-Associated Virus classification demonstrates that the two-stage framework captures 72.5\% of errors while deferring 34.5\% of samples, reducing automated decision error rates from 16.8\% to 6.9\%. Notably, calibration alone yields marginal accuracy gains; the operational benefit arises primarily from the reliability scoring mechanism, which applies to any well-calibrated probability output. This work bridges information geometry and statistical learning, offering formal guarantees for uncertainty-aware classification in applications requiring rigorous validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20960v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soumojit Das, Nairanjana Dasgupta, Prashanta Dutta</dc:creator>
    </item>
  </channel>
</rss>
