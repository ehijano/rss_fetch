<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 26 Aug 2024 04:01:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Rates of strong uniform consistency for the $k$-nearest neighbors kernel estimators of density and regression function</title>
      <link>https://arxiv.org/abs/2408.12741</link>
      <description>arXiv:2408.12741v1 Announce Type: new 
Abstract: We adress the problem of consistency of the $k$-nearest neighbors kernel estimators of the density and the regression function in the multivariate case. We get the rates of strong uniform consistency on the whole space $\mathbb{R}^p$ for these estimators under specified assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12741v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luran Bengono Mintogo, Emmanuel de Dieu Nkou, Guy Martial Nkiet</dc:creator>
    </item>
    <item>
      <title>On the relation between likelihood ratios and p-values for testing success probabilities</title>
      <link>https://arxiv.org/abs/2408.12905</link>
      <description>arXiv:2408.12905v1 Announce Type: new 
Abstract: We investigate the asymptotic relation between likelihood ratios and p-values. We do that in a setting in which exact computations are possible: a coin-tossing context where the hypotheses of interest address the success probability of the coin. We obtain exact asymptotic results and conclude that the p-value scales very differently than the likelihood ratio. We also investigate the p-value of the likelihood ratio, that is, the probability of finding a more extreme likelihood ratio under the various hypotheses. Here we also find explicit asymptotic relations, with similar conclusions. Finally, we study the expected value of the likelihood ratio in an optional stopping context. Our results imply, for instance, that in a coin-tossing context, a p-value of 0.05 cannot correspond to an actual likelihood ratio larger than 6.8.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12905v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wouter Kager, Ronald Meester</dc:creator>
    </item>
    <item>
      <title>Real Log Canonical Thresholds at Non-singular Points</title>
      <link>https://arxiv.org/abs/2408.13030</link>
      <description>arXiv:2408.13030v1 Announce Type: new 
Abstract: Recent advances have clarified theoretical learning accuracy in Bayesian inference, revealing that the asymptotic behavior of metrics such as generalization loss and free energy, assessing predictive accuracy, is dictated by a rational number unique to each statistical model, termed the learning coefficient (real log canonical threshold). For models meeting regularity conditions, their learning coefficients are known. However, for singular models not meeting these conditions, exact values of learning coefficients are provided for specific models like reduced-rank regression, but a broadly applicable calculation method for these learning coefficients in singular models remains elusive.
  This paper extends the application range of the previous work and provides an approach that can be applied to many points within the set of realizable parameters. Specifically, it provides a formula for calculating the real log canonical threshold at many non-singular points within the set of realizable parameters. If this calculation can be performed, it is possible to obtain an upper bound for the learning coefficient of the statistical model. Thus, this approach can also be used to easily obtain an upper bound for the learning coefficients of statistical models. As an application example, it provides an upper bound for the learning coefficient of a mixed binomial model, and calculates the learning coefficient for a specific case of reduced-rank regression, confirming that the results are consistent with previous research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13030v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuki Kurumadani</dc:creator>
    </item>
    <item>
      <title>Trimmed Mean for Partially Observed Functional Data</title>
      <link>https://arxiv.org/abs/2408.13062</link>
      <description>arXiv:2408.13062v1 Announce Type: new 
Abstract: In practice, as opposed to a large set of finite-dimensional vectors approximated from discrete data, we often prefer to utilize functional data. In recent years, partially observable function data have frequently appeared in practical applications and are the objectofan increasing interest by the literature. In this thesis, we learn the concept of data integration depth of partially observable functions proposed by Elias et al. 2023, which can be used to measure the degree of data centralization. At the same time, we also studied the trimmed-mean estimator method and consistency proof proposed by Fraiman and Muniz 2001 for completely observable functions. This method refers to the process of removing some of the smallest and largest values before calculating the mean to enhance the robustness of the estimate. In this thesis, we introduce the concept of trimmed-mean estimator for partially observable functions. We discuss several theoretical and practical issues, including the proof that the proposed trimmed-mean estimator converges almost surely and provide a simulation study. The results show that our estimator performs better in terms of efficiency and robustness compared to the ordinary mean under partially observable functional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13062v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixiao Wang</dc:creator>
    </item>
    <item>
      <title>Spectral Recovery in the Labeled SBM</title>
      <link>https://arxiv.org/abs/2408.13075</link>
      <description>arXiv:2408.13075v1 Announce Type: new 
Abstract: We consider the problem of exact community recovery in the Labeled Stochastic Block Model (LSBM) with $k$ communities, where each pair of vertices is associated with a label from the set $\{0,1, \dots, L\}$. A pair of vertices from communities $i,j$ is given label $\ell$ with probability $p_{ij}^{(\ell)}$, and the goal is to recover the community partition. We propose a simple spectral algorithm for exact community recovery, and show that it achieves the information-theoretic threshold in the logarithmic-degree regime, under the assumption that the eigenvalues of certain parameter matrices are distinct and nonzero. Our results generalize recent work of Dhara, Gaudio, Mossel, and Sandon (2023), who showed that a spectral algorithm achieves the information-theoretic threshold in the Censored SBM, which is equivalent to the LSBM with $L = 2$. Interestingly, their algorithm uses eigenvectors from two matrix representations of the graph, while our algorithm uses eigenvectors from $L$ matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13075v1</guid>
      <category>math.ST</category>
      <category>cs.SI</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Gaudio, Heming Liu</dc:creator>
    </item>
    <item>
      <title>Non-parametric estimators of scaled cash flows</title>
      <link>https://arxiv.org/abs/2408.13176</link>
      <description>arXiv:2408.13176v1 Announce Type: new 
Abstract: In multi-state life insurance, incidental policyholder behavior gives rise to expected cash flows that are not easily targeted by classic non-parametric estimators if data is subject to sampling effects. We introduce a scaled version of the classic Aalen--Johansen estimator that overcomes this challenge. Strong uniform consistency and asymptotic normality are established under entirely random right-censoring, subject to lax moment conditions on the multivariate counting process. In a simulation study, the estimator outperforms earlier proposals from the literature. Finally, we showcase the potential of the presented method to other areas of actuarial science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13176v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>T. Bathke, C. Furrer</dc:creator>
    </item>
    <item>
      <title>Augmented Functional Random Forests: Classifier Construction and Unbiased Functional Principal Components Importance through Ad-Hoc Conditional Permutations</title>
      <link>https://arxiv.org/abs/2408.13179</link>
      <description>arXiv:2408.13179v1 Announce Type: cross 
Abstract: This paper introduces a novel supervised classification strategy that integrates functional data analysis (FDA) with tree-based methods, addressing the challenges of high-dimensional data and enhancing the classification performance of existing functional classifiers. Specifically, we propose augmented versions of functional classification trees and functional random forests, incorporating a new tool for assessing the importance of functional principal components. This tool provides an ad-hoc method for determining unbiased permutation feature importance in functional data, particularly when dealing with correlated features derived from successive derivatives. Our study demonstrates that these additional features can significantly enhance the predictive power of functional classifiers. Experimental evaluations on both real-world and simulated datasets showcase the effectiveness of the proposed methodology, yielding promising results compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13179v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabrizio Maturo, Annamaria Porreca</dc:creator>
    </item>
    <item>
      <title>Double Descent: Understanding Linear Model Estimation of Nonidentifiable Parameters and a Model for Overfitting</title>
      <link>https://arxiv.org/abs/2408.13235</link>
      <description>arXiv:2408.13235v1 Announce Type: cross 
Abstract: We consider ordinary least squares estimation and variations on least squares estimation such as penalized (regularized) least squares and spectral shrinkage estimates for problems with p &gt; n and associated problems with prediction of new observations. After the introduction of Section 1, Section 2 examines a number of commonly used estimators for p &gt; n. Section 3 introduces prediction with p &gt; n. Section 4 introduces notational changes to facilitate discussion of overfitting and Section 5 illustrates the phenomenon of double descent. We conclude with some final comments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13235v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ronald Christensen</dc:creator>
    </item>
    <item>
      <title>Some improved Gaussian correlation inequalities for symmetrical n-rectangles extended to some multivariate gamma distributions and some further probability inequalities</title>
      <link>https://arxiv.org/abs/2006.00769</link>
      <description>arXiv:2006.00769v4 Announce Type: replace 
Abstract: The Gaussian correlation inequality (GCI) for symmetrical n-rectangles is improved if the absolute components have a joint cumulative distribution (cdf) which is MTP2 (multivariate totally positive of order 2). Inequalities of the here given type hold at least for all MTP2-cdfs on R^n or (0,infinity)^n with everywhere positive smooth densities. In particular, at least some infinitely divisible multivariate chi-square distribution functions (gamma distributions in the sense of Krishnamoorthy and Parthasarathy) with any positive real "degree of freedom" are shown to be MTP2. Moreover, further numerically calculable probability inequalities for a broad class of multivariate gamma distributions are derived and a different improvement for inequalities of the GCI-type - and of a similar type with three instead of two groups of components - with more special correlation structures. The main idea behind these inequalities is to find for a given correlation matrix with positive correlations a further correlation matrix with smaller correlations whose inverse is an M-matrix and where the corresponding multivariate gamma distribution function is numerically available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2006.00769v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Royen</dc:creator>
    </item>
    <item>
      <title>Split Conformal Prediction and Non-Exchangeable Data</title>
      <link>https://arxiv.org/abs/2203.15885</link>
      <description>arXiv:2203.15885v2 Announce Type: replace 
Abstract: Split conformal prediction (CP) is arguably the most popular CP method for uncertainty quantification, enjoying both academic interest and widespread deployment. However, the original theoretical analysis of split CP makes the crucial assumption of data exchangeability, which hinders many real-world applications. In this paper, we present a novel theoretical framework based on concentration inequalities and decoupling properties of the data, proving that split CP remains valid for many non-exchangeable processes by adding a small coverage penalty. Through experiments with both real and synthetic data, we show that our theoretical results translate to good empirical performance under non-exchangeability, e.g., for time series and spatiotemporal data. Compared to recent conformal algorithms designed to counter specific exchangeability violations, we show that split CP is competitive in terms of coverage and interval size, with the benefit of being extremely simple and orders of magnitude faster than alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.15885v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of Machine Learning Research, 25(225), 2024</arxiv:journal_reference>
      <dc:creator>Roberto I. Oliveira, Paulo Orenstein, Thiago Ramos, Jo\~ao Vitor Romano</dc:creator>
    </item>
    <item>
      <title>Posterior Sampling in High Dimension via Diffusion Processes</title>
      <link>https://arxiv.org/abs/2304.11449</link>
      <description>arXiv:2304.11449v2 Announce Type: replace 
Abstract: Sampling from the posterior is a key technical problem in Bayesian statistics. Rigorous guarantees are difficult to obtain for Markov Chain Monte Carlo algorithms of common use. In this paper, we study an alternative class of algorithms based on diffusion processes and variational methods. The diffusion is constructed in such a way that, at its final time, it approximates the target posterior distribution. The drift of this diffusion is given by the posterior expectation of the unknown parameter vector ${\boldsymbol \theta}$ given the data and the additional noisy observations.
  In order to construct an efficient sampling algorithm, we use a simple Euler discretization of the diffusion process, and leverage message passing algorithms and variational inference techniques to approximate the posterior expectation oracle.
  We apply this method to posterior sampling in two canonical problems in high-dimensional statistics: sparse regression and low-rank matrix estimation within the spiked model. In both cases we develop the first algorithms with accuracy guarantees in the regime of constant signal-to-noise ratios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.11449v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Montanari, Yuchen Wu</dc:creator>
    </item>
    <item>
      <title>Monge-Kantorovich superquantiles and expected shortfalls with applications to multivariate risk measurements</title>
      <link>https://arxiv.org/abs/2307.01584</link>
      <description>arXiv:2307.01584v3 Announce Type: replace 
Abstract: We propose center-outward superquantile and expected shortfall functions, with applications to multivariate risk measurements, extending the standard notion of value at risk and conditional value at risk from the real line to $\mathbb{R}^d$. Our new concepts are built upon the recent definition of Monge-Kantorovich quantiles based on the theory of optimal transport, and they provide a natural way to characterize multivariate tail probabilities and central areas of point clouds. They preserve the univariate interpretation of a typical observation that lies beyond or ahead a quantile, but in a meaningful multivariate way. We show that they characterize random vectors and their convergence in distribution, which underlines their importance. Our new concepts are illustrated on both simulated and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.01584v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bernard Bercu, Jeremie Bigot, Gauthier Thurin</dc:creator>
    </item>
    <item>
      <title>Generalized Estimation and Information</title>
      <link>https://arxiv.org/abs/2407.07036</link>
      <description>arXiv:2407.07036v2 Announce Type: replace 
Abstract: This paper extends the idea of a generalized estimator for a scalar parameter (Vos, 2022) to multi-dimensional parameters both with and without nuisance parameters. The title reflects the fact that generalized estimators provide more than simply another method to find point estimators, and that the methods to assess generalized estimators differ from those for point estimators. By generalized estimation we mean the use of generalized estimators together with an extended definition of information to assess their inferential properties. We show that Fisher information provides an upper bound for the information utilized by an estimator and that the score attains this bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07036v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paul Vos, Qiang Wu</dc:creator>
    </item>
    <item>
      <title>Uniform Inference for Subsampled Moment Regression</title>
      <link>https://arxiv.org/abs/2405.07860</link>
      <description>arXiv:2405.07860v2 Announce Type: replace-cross 
Abstract: We propose a method for constructing simultaneous confidence intervals for solutions to conditional moment equations. The intervals are built around a class of nonparametric regression algorithms based on subsampled kernels. This class encompasses various forms of subsampled random forest regression, including Generalized Random Forests (Athey et al., 2019). Although simultaneous validity is often necessary in leading empirical applications--for example, in fine-grained characterization of heterogeneous treatment effects--only confidence intervals that confer pointwise guarantees were previously available. Our work closes this gap. As a by-product, we obtain several new order-explicit results on the concentration and normal approximation of high-dimensional U-statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07860v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David M. Ritzwoller, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>On the Robustness of Kernel Goodness-of-Fit Tests</title>
      <link>https://arxiv.org/abs/2408.05854</link>
      <description>arXiv:2408.05854v2 Announce Type: replace-cross 
Abstract: Goodness-of-fit testing is often criticized for its lack of practical relevance; since ``all models are wrong'', the null hypothesis that the data conform to our model is ultimately always rejected when the sample size is large enough. Despite this, probabilistic models are still used extensively, raising the more pertinent question of whether the model is good enough for a specific task. This question can be formalized as a robust goodness-of-fit testing problem by asking whether the data were generated by a distribution corresponding to our model up to some mild perturbation. In this paper, we show that existing kernel goodness-of-fit tests are not robust according to common notions of robustness including qualitative and quantitative robustness. We also show that robust techniques based on tilted kernels from the parameter estimation literature are not sufficient for ensuring both types of robustness in the context of goodness-of-fit testing. We therefore propose the first robust kernel goodness-of-fit test which resolves this open problem using kernel Stein discrepancy balls, which encompass perturbation models such as Huber contamination models and density uncertainty bands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05854v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xing Liu, Fran\c{c}ois-Xavier Briol</dc:creator>
    </item>
  </channel>
</rss>
