<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Jun 2024 01:49:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On Approximation of Robust Max-Cut and Related Problems using Randomized Rounding Algorithms</title>
      <link>https://arxiv.org/abs/2406.01856</link>
      <description>arXiv:2406.01856v1 Announce Type: new 
Abstract: Goemans and Williamson proposed a randomized rounding algorithm for the MAX-CUT problem with a 0.878 approximation bound in expectation. The 0.878 approximation bound remains the best-known approximation bound for this APX-hard problem. Their approach was subsequently applied to other related problems such as Max-DiCut, MAX-SAT, and Max-2SAT, etc. We show that the randomized rounding algorithm can also be used to achieve a 0.878 approximation bound for the robust and distributionally robust counterparts of the max-cut problem. We also show that the approximation bounds for the other problems are maintained for their robust and distributionally robust counterparts if the randomization projection framework is used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01856v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Haoyan Shi, Sanjay Mehrotra</dc:creator>
    </item>
    <item>
      <title>Convergence Properties of the Asynchronous Maximum Model</title>
      <link>https://arxiv.org/abs/2406.01910</link>
      <description>arXiv:2406.01910v1 Announce Type: new 
Abstract: Let $G = (V,E)$ be a connected directed graph on $n$ vertices. Assign values from the set $\{1,2,\dots,n\}$ to the vertices of $G$ and update the values according to the following rule: uniformly at random choose a vertex and update its value to the maximum of the values in its neighbourhood. The value at this vertex can potentially decrease. This random process is called the asynchronous maximum model. Repeating this process we show that for a strongly connected directed graph eventually all vertices have the same value and the model is said to have \textit{converged}. In the undirected case the expected convergence time is shown to be asymptotically (as $n\to \infty$) in $\Omega(n\log n)$ and $O(n^2)$ and these bounds are tight. We further characterise the convergence time in $O(\frac{n}{\phi}\log n)$ where $\phi$ is the vertex expansion of $G$. This provides a better upper bound for a large class of graphs. Further, we show the number of rounds until convergence is in $O((\frac{n}{\phi}\log n)g(n))$ with high probability, where $g(n)$ satisfies $\frac{1}{g^2(n)} \to 0$ as $n \to \infty$. For a strongly connected directed graph the convergence time is shown to be in $O(nb^2 + \frac{n}{\phi'}\log n)$ where $b$ is a parameter measuring directed cycle length and $\phi'$ is a parameter measuring vertex expansion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01910v1</guid>
      <category>cs.DS</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John Larkin</dc:creator>
    </item>
    <item>
      <title>Random Abstract Cell Complexes</title>
      <link>https://arxiv.org/abs/2406.01999</link>
      <description>arXiv:2406.01999v1 Announce Type: new 
Abstract: We define a model for random (abstract) cell complexes (CCs), similiar to the well-known Erd\H{o}s-R\'enyi model for graphs and its extensions for simplicial complexes. To build a random cell complex, we first draw from an Erd\H{o}s-R\'enyi graph, and consecutively augment the graph with cells for each dimension with a specified probability. As the number of possible cells increases combinatorially -- e.g., 2-cells can be represented as cycles, or permutations -- we derive an approximate sampling algorithm for this model limited to two-dimensional abstract cell complexes. Since there is a large variance in the number of simple cycles on graphs drawn from the same configuration of ER, we also provide an efficient method to approximate that number, which is of independent interest. Moreover, it enables us to specify the expected number of 2-cells of each boundary length we want to sample. We provide some initial analysis into the properties of random CCs drawn from this model. We further showcase practical applications for our random CCs as null models, and in the context of (random) liftings of graphs to cell complexes. Both the sampling and cycle count estimation algorithms are available in the package `py-raccoon` on the Python Packaging Index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01999v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.SI</category>
      <category>math.AT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Josef Hoppe, Michael T. Schaub</dc:creator>
    </item>
    <item>
      <title>On the Computation of 2-Dimensional Recurrence Equations</title>
      <link>https://arxiv.org/abs/2406.02082</link>
      <description>arXiv:2406.02082v1 Announce Type: new 
Abstract: The paper demonstrates how a 2-dimensional recurrence problem can be reduced to a mono-dimensional recurrence problem where the Kogge and Stone algorithm is applicable, with the computation time - excluding the reduction step - becoming proportional to $log_2(2n-1)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02082v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Natale</dc:creator>
    </item>
    <item>
      <title>Reweighted Solutions for Weighted Low Rank Approximation</title>
      <link>https://arxiv.org/abs/2406.02431</link>
      <description>arXiv:2406.02431v1 Announce Type: new 
Abstract: Weighted low rank approximation (WLRA) is an important yet computationally challenging primitive with applications ranging from statistical analysis, model compression, and signal processing. To cope with the NP-hardness of this problem, prior work considers heuristics, bicriteria, or fixed parameter tractable algorithms to solve this problem. In this work, we introduce a new relaxed solution to WLRA which outputs a matrix that is not necessarily low rank, but can be stored using very few parameters and gives provable approximation guarantees when the weight matrix has low rank. Our central idea is to use the weight matrix itself to reweight a low rank solution, which gives an extremely simple algorithm with remarkable empirical performance in applications to model compression and on synthetic datasets. Our algorithm also gives nearly optimal communication complexity bounds for a natural distributed problem associated with this problem, for which we show matching communication lower bounds. Together, our communication complexity bounds show that the rank of the weight matrix provably parameterizes the communication complexity of WLRA. We also obtain the first relative error guarantees for feature selection with a weighted objective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02431v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David P. Woodruff, Taisuke Yasuda</dc:creator>
    </item>
    <item>
      <title>Coresets for Multiple $\ell_p$ Regression</title>
      <link>https://arxiv.org/abs/2406.02432</link>
      <description>arXiv:2406.02432v1 Announce Type: new 
Abstract: A coreset of a dataset with $n$ examples and $d$ features is a weighted subset of examples that is sufficient for solving downstream data analytic tasks. Nearly optimal constructions of coresets for least squares and $\ell_p$ linear regression with a single response are known in prior work. However, for multiple $\ell_p$ regression where there can be $m$ responses, there are no known constructions with size sublinear in $m$. In this work, we construct coresets of size $\tilde O(\varepsilon^{-2}d)$ for $p&lt;2$ and $\tilde O(\varepsilon^{-p}d^{p/2})$ for $p&gt;2$ independently of $m$ (i.e., dimension-free) that approximate the multiple $\ell_p$ regression objective at every point in the domain up to $(1\pm\varepsilon)$ relative error. If we only need to preserve the minimizer subject to a subspace constraint, we improve these bounds by an $\varepsilon$ factor for all $p&gt;1$. All of our bounds are nearly tight.
  We give two application of our results. First, we settle the number of uniform samples needed to approximate $\ell_p$ Euclidean power means up to a $(1+\varepsilon)$ factor, showing that $\tilde\Theta(\varepsilon^{-2})$ samples for $p = 1$, $\tilde\Theta(\varepsilon^{-1})$ samples for $1 &lt; p &lt; 2$, and $\tilde\Theta(\varepsilon^{1-p})$ samples for $p&gt;2$ is tight, answering a question of Cohen-Addad, Saulpic, and Schwiegelshohn. Second, we show that for $1&lt;p&lt;2$, every matrix has a subset of $\tilde O(\varepsilon^{-1}k)$ rows which spans a $(1+\varepsilon)$-approximately optimal $k$-dimensional subspace for $\ell_p$ subspace approximation, which is also nearly optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02432v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David P. Woodruff, Taisuke Yasuda</dc:creator>
    </item>
    <item>
      <title>A Surprisingly Simple Method for Distributed Euclidean-Minimum Spanning Tree / Single Linkage Dendrogram Construction from High Dimensional Embeddings via Distance Decomposition</title>
      <link>https://arxiv.org/abs/2406.01739</link>
      <description>arXiv:2406.01739v1 Announce Type: cross 
Abstract: We introduce a decomposition method for the distributed calculation of exact Euclidean Minimum Spanning Trees in high dimensions (where sub-quadratic algorithms are not effective), or more generalized geometric-minimum spanning trees of complete graphs, where for each vertex $v\in V$ in the graph $G=(V,E)$ is represented by a vector in $\vec{v}\in \mathbb{R}^n$, and each for any edge, the the weight of the edge in the graph is given by a symmetric binary `distance' function between the representative vectors $w(\{x,y\}) = d(\vec{x},\vec{y})$. This is motivated by the task of clustering high dimensional embeddings produced by neural networks, where low-dimensional algorithms are ineffective; such geometric-minimum spanning trees find applications as a subroutine in the construction of single linkage dendrograms, as the two structures can be converted between each other efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01739v1</guid>
      <category>cs.DC</category>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Richard Lettich</dc:creator>
    </item>
    <item>
      <title>Influence Maximization in Hypergraphs by Stratified Sampling for Efficient Generation of Reverse Reachable Sets</title>
      <link>https://arxiv.org/abs/2406.01911</link>
      <description>arXiv:2406.01911v1 Announce Type: cross 
Abstract: Given a hypergraph, influence maximization (IM) is to discover a seed set containing $k$ vertices that have the maximal influence. Although the existing vertex-based IM algorithms perform better than the hyperedge-based algorithms by generating random reverse researchable (RR) sets, they are inefficient because (i) they ignore important structural information associated with hyperedges and thus obtain inferior results, (ii) the frequently-used sampling methods for generating RR sets have low efficiency because of a large number of required samplings along with high sampling variances, and (iii) the vertex-based IM algorithms have large overheads in terms of running time and memory costs. To overcome these shortcomings, this paper proposes a novel approach, called \emph{HyperIM}. The key idea behind \emph{HyperIM} is to differentiate structural information of vertices for developing stratified sampling combined with highly-efficient strategies to generate the RR sets. With theoretical guarantees, \emph{HyperIM} is able to accelerate the influence spread, improve the sampling efficiency, and cut down the expected running time. To further reduce the running time and memory costs, we optimize \emph{HyperIM} by inferring the bound of the required number of RR sets in conjunction with stratified sampling. Experimental results on real-world hypergraphs show that \emph{HyperIM} is able to reduce the number of required RR sets and running time by orders of magnitude while increasing the influence spread by up to $2.73X$ on average, compared to the state-of-the-art IM algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01911v1</guid>
      <category>cs.SI</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lingling Zhang, Hong Jiang, Ye Yuan, Guoren Wang</dc:creator>
    </item>
    <item>
      <title>Almost linear time differentially private release of synthetic graphs</title>
      <link>https://arxiv.org/abs/2406.02156</link>
      <description>arXiv:2406.02156v1 Announce Type: cross 
Abstract: In this paper, we give an almost linear time and space algorithms to sample from an exponential mechanism with an $\ell_1$-score function defined over an exponentially large non-convex set. As a direct result, on input an $n$ vertex $m$ edges graph $G$, we present the \textit{first} $\widetilde{O}(m)$ time and $O(m)$ space algorithms for differentially privately outputting an $n$ vertex $O(m)$ edges synthetic graph that approximates all the cuts and the spectrum of $G$. These are the \emph{first} private algorithms for releasing synthetic graphs that nearly match this task's time and space complexity in the non-private setting while achieving the same (or better) utility as the previous works in the more practical sparse regime. Additionally, our algorithms can be extended to private graph analysis under continual observation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02156v1</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingcheng Liu, Jalaj Upadhyay, Zongrui Zou</dc:creator>
    </item>
    <item>
      <title>The complexity of approximate (coarse) correlated equilibrium for incomplete information games</title>
      <link>https://arxiv.org/abs/2406.02357</link>
      <description>arXiv:2406.02357v1 Announce Type: cross 
Abstract: We study the iteration complexity of decentralized learning of approximate correlated equilibria in incomplete information games.
  On the negative side, we prove that in $\mathit{extensive}$-$\mathit{form}$ $\mathit{games}$, assuming $\mathsf{PPAD} \not\subset \mathsf{TIME}(n^{\mathsf{polylog}(n)})$, any polynomial-time learning algorithms must take at least $2^{\log_2^{1-o(1)}(|\mathcal{I}|)}$ iterations to converge to the set of $\epsilon$-approximate correlated equilibrium, where $|\mathcal{I}|$ is the number of nodes in the game and $\epsilon &gt; 0$ is an absolute constant. This nearly matches, up to the $o(1)$ term, the algorithms of [PR'24, DDFG'24] for learning $\epsilon$-approximate correlated equilibrium, and resolves an open question of Anagnostides, Kalavasis, Sandholm, and Zampetakis [AKSZ'24]. Our lower bound holds even for the easier solution concept of $\epsilon$-approximate $\mathit{coarse}$ correlated equilibrium
  On the positive side, we give uncoupled dynamics that reach $\epsilon$-approximate correlated equilibria of a $\mathit{Bayesian}$ $\mathit{game}$ in polylogarithmic iterations, without any dependence of the number of types. This demonstrates a separation between Bayesian games and extensive-form games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02357v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Binghui Peng, Aviad Rubinstein</dc:creator>
    </item>
    <item>
      <title>Singular Subspace Perturbation Bounds via Rectangular Random Matrix Diffusions</title>
      <link>https://arxiv.org/abs/2406.02502</link>
      <description>arXiv:2406.02502v1 Announce Type: cross 
Abstract: Given a matrix $A \in \mathbb{R}^{m\times d}$ with singular values $\sigma_1\geq \cdots \geq \sigma_d$, and a random matrix $G \in \mathbb{R}^{m\times d}$ with iid $N(0,T)$ entries for some $T&gt;0$, we derive new bounds on the Frobenius distance between subspaces spanned by the top-$k$ (right) singular vectors of $A$ and $A+G$. This problem arises in numerous applications in statistics where a data matrix may be corrupted by Gaussian noise, and in the analysis of the Gaussian mechanism in differential privacy, where Gaussian noise is added to data to preserve private information. We show that, for matrices $A$ where the gaps in the top-$k$ singular values are roughly $\Omega(\sigma_k-\sigma_{k+1})$ the expected Frobenius distance between the subspaces is $\tilde{O}(\frac{\sqrt{d}}{\sigma_k-\sigma_{k+1}} \times \sqrt{T})$, improving on previous bounds by a factor of $\frac{\sqrt{m}}{\sqrt{d}} \sqrt{k}$. To obtain our bounds we view the perturbation to the singular vectors as a diffusion process -- the Dyson-Bessel process -- and use tools from stochastic calculus to track the evolution of the subspace spanned by the top-$k$ singular vectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02502v1</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peiyao Lai, Oren Mangoubi</dc:creator>
    </item>
    <item>
      <title>TreePIR: Efficient Private Retrieval of Merkle Proofs via Tree Colorings with Fast Indexing and Zero Storage Overhead</title>
      <link>https://arxiv.org/abs/2205.05211</link>
      <description>arXiv:2205.05211v5 Announce Type: replace 
Abstract: A Batch Private Information Retrieval (batch-PIR) scheme allows a client to retrieve multiple data items from a database without revealing them to the storage server(s). Most existing approaches for batch-PIR are based on batch codes, in particular, probabilistic batch codes (PBC) (Angel et al. S&amp;P'18), which incur large storage overheads. In this work, we show that \textit{zero} storage overhead is achievable for tree-shaped databases. In particular, we develop TreePIR, a novel approach tailored made for private retrieval of the set of nodes along an arbitrary root-to-leaf path in a Merkle tree with no storage redundancy. This type of trees has been widely implemented in many real-world systems such as Amazon DynamoDB, Google's Certificate Transparency, and blockchains. Tree nodes along a root-to-leaf path forms the well-known Merkle proof. TreePIR, which employs a novel tree coloring, outperforms PBC, a fundamental component in state-of-the-art batch-PIR schemes (Angel et al. S&amp;P'18, Mughees-Ren S&amp;P'23, Liu et al. S&amp;P'24), in all metrics, achieving $3\times$ lower total storage and $1.5$-$2\times$ lower computation and communication costs. Most notably, TreePIR has $8$-$160\times$ lower setup time and its polylog-complexity indexing algorithm is $19$-$160\times$ faster than PBC for trees of $2^{10}$-$2^{24}$ leaves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.05211v5</guid>
      <category>cs.DS</category>
      <category>cs.CR</category>
      <category>math.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Son Hoang Dau, Quang Cao, Rinaldo Gagiano, Duy Huynh, Xun Yi, Phuc Lu Le, Quang-Hung Luu, Emanuele Viterbo, Yu-Chih Huang, Jingge Zhu, Mohammad M. Jalalzai, Chen Feng</dc:creator>
    </item>
    <item>
      <title>Prophet Inequalities: Separating Random Order from Order Selection</title>
      <link>https://arxiv.org/abs/2304.04024</link>
      <description>arXiv:2304.04024v4 Announce Type: replace 
Abstract: Prophet inequalities are a central object of study in optimal stopping theory. A gambler is sent values in an online fashion, sampled from an instance of independent distributions, in an adversarial, random or selected order, depending on the model. When observing each value, the gambler either accepts it as a reward or irrevocably rejects it and proceeds to observe the next value. The goal of the gambler, who cannot see the future, is maximising the expected value of the reward while competing against the expectation of a prophet (the offline maximum). In other words, one seeks to maximise the gambler-to-prophet ratio of the expectations.
  The model, in which the gambler selects the arrival order first, and then observes the values, is known as Order Selection. In this model a ratio of $0.7251$ has been proved to be attainable for any instance. In very recent work, this has been improved up to $0.7258$. If the gambler chooses the arrival order (uniformly) at random, we obtain the Random Order model. The worst case ratio over all possible instances has been extensively studied for at least $40$ years. In the recent work aforementioned, through simulations, this ratio has been shown to be at most $0.7254$ for the Random Order model, thus establishing for the first time that carefully choosing the order, instead of simply taking it at random, benefits the gambler. We give an alternative, non-simulation-assisted proof of this fact, by showing mathematically that in the Random Order model, no algorithm can achieve a ratio larger than $0.7235$. This sets a new state-of-the-art hardness for this model, and establishes more formally that there is a real benefit in choosing the order.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.04024v4</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giordano Giambartolomei, Frederik Mallmann-Trenn, Raimundo Saona</dc:creator>
    </item>
    <item>
      <title>Approximate Distance Sensitivity Oracles in Subquadratic Space</title>
      <link>https://arxiv.org/abs/2305.11580</link>
      <description>arXiv:2305.11580v4 Announce Type: replace 
Abstract: An $f$-edge fault-tolerant distance sensitive oracle ($f$-DSO) with stretch $\sigma \ge 1$ is a data structure that preprocesses a given undirected, unweighted graph $G$ with $n$ vertices and $m$ edges, and a positive integer $f$. When queried with a pair of vertices $s, t$ and a set $F$ of at most $f$ edges, it returns a $\sigma$-approximation of the $s$-$t$-distance in $G-F$.
  We study $f$-DSOs that take subquadratic space. Thorup and Zwick [JACM 2005] showed that this is only possible for $\sigma \ge 3$. We present, for any constant $f \ge 1$ and $\alpha \in (0, \frac{1}{2})$, and any $\varepsilon &gt; 0$, a randomized $f$-DSO with stretch $ 3 + \varepsilon$ that w.h.p. takes $\widetilde{O}(n^{2-\frac{\alpha}{f+1}}) \cdot O(\log n/\varepsilon)^{f+2}$ space and has an $O(n^\alpha/\varepsilon^2)$ query time. The time to build the oracle is $\widetilde{O}(mn^{2-\frac{\alpha}{f+1}}) \cdot O(\log n/\varepsilon)^{f+1}$. We also give an improved construction for graphs with diameter at most $D$. For any positive integer $k$, we devise an $f$-DSO with stretch $2k-1$ that w.h.p. takes $O(D^{f+o(1)} n^{1+1/k})$ space and has $\widetilde{O}(D^{o(1)})$ query time, with a preprocessing time of $O(D^{f+o(1)} mn^{1/k})$.
  Chechik, Cohen, Fiat, and Kaplan [SODA 2017] devised an $f$-DSO with stretch $1{+}\varepsilon$ and preprocessing time $O(n^{5+o(1)}/\varepsilon^f)$, albeit with a super-quadratic space requirement. We show how to reduce their preprocessing time to $O(mn^{2+o(1)}/\varepsilon^f)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.11580v4</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.46298/theoretics.24.15</arxiv:DOI>
      <arxiv:journal_reference>TheoretiCS, Volume 3 (2024), Article 15, 1-47</arxiv:journal_reference>
      <dc:creator>Davide Bil\`o, Shiri Chechik, Keerti Choudhary, Sarel Cohen, Tobias Friedrich, Simon Krogmann, Martin Schirneck</dc:creator>
    </item>
    <item>
      <title>Approximate Nearest Neighbor Search with Window Filters</title>
      <link>https://arxiv.org/abs/2402.00943</link>
      <description>arXiv:2402.00943v2 Announce Type: replace 
Abstract: We define and investigate the problem of $\textit{c-approximate window search}$: approximate nearest neighbor search where each point in the dataset has a numeric label, and the goal is to find nearest neighbors to queries within arbitrary label ranges. Many semantic search problems, such as image and document search with timestamp filters, or product search with cost filters, are natural examples of this problem. We propose and theoretically analyze a modular tree-based framework for transforming an index that solves the traditional c-approximate nearest neighbor problem into a data structure that solves window search. On standard nearest neighbor benchmark datasets equipped with random label values, adversarially constructed embeddings, and image search embeddings with real timestamps, we obtain up to a $75\times$ speedup over existing solutions at the same level of recall.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00943v2</guid>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua Engels, Benjamin Landrum, Shangdi Yu, Laxman Dhulipala, Julian Shun</dc:creator>
    </item>
    <item>
      <title>Improved FPT Approximation Scheme and Approximate Kernel for Biclique-Free Max k-Weight SAT: Greedy Strikes Back</title>
      <link>https://arxiv.org/abs/2403.06335</link>
      <description>arXiv:2403.06335v2 Announce Type: replace 
Abstract: In the Max $k$-Weight SAT (aka Max SAT with Cardinality Constraint) problem, we are given a CNF formula with $n$ variables and $m$ clauses together with a positive integer $k$. The goal is to find an assignment where at most $k$ variables are set to one that satisfies as many constraints as possible. Recently, Jain et al. [SODA'23] gave an FPT approximation scheme (FPT-AS) with running time $2^{O\left(\left(dk/\epsilon\right)^d\right)} \cdot (n + m)^{O(1)}$ for Max $k$-Weight SAT when the incidence graph is $K_{d,d}$-free. They asked whether a polynomial-size approximate kernel exists. In this work, we answer this question positively by giving an $(1 - \epsilon)$-approximate kernel with $\left(\frac{d k}{\epsilon}\right)^{O(d)}$ variables. This also implies an improved FPT-AS with running time $(dk/\epsilon)^{O(dk)} \cdot (n + m)^{O(1)}$. Our approximate kernel is based mainly on a couple of greedy strategies together with a sunflower lemma-style reduction rule.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06335v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pasin Manurangsi</dc:creator>
    </item>
    <item>
      <title>Tree Proof-of-Position Algorithms</title>
      <link>https://arxiv.org/abs/2405.06761</link>
      <description>arXiv:2405.06761v2 Announce Type: replace 
Abstract: We present a novel class of proof-of-position algorithms: Tree-Proof-of-Position (T-PoP). This algorithm is decentralised, collaborative and can be computed in a privacy preserving manner, such that agents do not need to reveal their position publicly. We make no assumptions of honest behaviour in the system, and consider varying ways in which agents may misbehave. Our algorithm is therefore resilient to highly adversarial scenarios. This makes it suitable for a wide class of applications, namely those in which trust in a centralised infrastructure may not be assumed, or high security risk scenarios. Our algorithm has a worst case quadratic runtime, making it suitable for hardware constrained IoT applications. We also provide a mathematical model that summarises T-PoP's performance for varying operating conditions. We then simulate T-PoP's behaviour with a large number of agent-based simulations, which are in complete agreement with our mathematical model, thus demonstrating its validity. T-PoP can achieve high levels of reliability and security by tuning its operating conditions, both in high and low density environments. Finally, we also present a mathematical model to probabilistically detect platooning attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06761v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aida Manzano Kharman, Pietro Ferraro, Homayoun Hamedmoghadam, Robert Shorten</dc:creator>
    </item>
    <item>
      <title>Private Edge Density Estimation for Random Graphs: Optimal, Efficient and Robust</title>
      <link>https://arxiv.org/abs/2405.16663</link>
      <description>arXiv:2405.16663v2 Announce Type: replace 
Abstract: We give the first polynomial-time, differentially node-private, and robust algorithm for estimating the edge density of Erd\H{o}s-R\'enyi random graphs and their generalization, inhomogeneous random graphs. We further prove information-theoretical lower bounds, showing that the error rate of our algorithm is optimal up to logarithmic factors. Previous algorithms incur either exponential running time or suboptimal error rates.
  Two key ingredients of our algorithm are (1) a new sum-of-squares algorithm for robust edge density estimation, and (2) the reduction from privacy to robustness based on sum-of-squares exponential mechanisms due to Hopkins et al. (STOC 2023).</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16663v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongjie Chen, Jingqiu Ding, Yiding Hua, David Steurer</dc:creator>
    </item>
    <item>
      <title>Correcting Underrepresentation and Intersectional Bias for Classification</title>
      <link>https://arxiv.org/abs/2306.11112</link>
      <description>arXiv:2306.11112v4 Announce Type: replace-cross 
Abstract: We consider the problem of learning from data corrupted by underrepresentation bias, where positive examples are filtered from the data at different, unknown rates for a fixed number of sensitive groups. We show that with a small amount of unbiased data, we can efficiently estimate the group-wise drop-out rates, even in settings where intersectional group membership makes learning each intersectional rate computationally infeasible. Using these estimates, we construct a reweighting scheme that allows us to approximate the loss of any hypothesis on the true distribution, even if we only observe the empirical error on a biased sample. From this, we present an algorithm encapsulating this learning and reweighting process along with a thorough empirical investigation. Finally, we define a bespoke notion of PAC learnability for the underrepresentation and intersectional bias setting and show that our algorithm permits efficient learning for model classes of finite VC dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11112v4</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emily Diana, Alexander Williams Tolbert</dc:creator>
    </item>
    <item>
      <title>Adversarial Attacks on Combinatorial Multi-Armed Bandits</title>
      <link>https://arxiv.org/abs/2310.05308</link>
      <description>arXiv:2310.05308v2 Announce Type: replace-cross 
Abstract: We study reward poisoning attacks on Combinatorial Multi-armed Bandits (CMAB). We first provide a sufficient and necessary condition for the attackability of CMAB, a notion to capture the vulnerability and robustness of CMAB. The attackability condition depends on the intrinsic properties of the corresponding CMAB instance such as the reward distributions of super arms and outcome distributions of base arms. Additionally, we devise an attack algorithm for attackable CMAB instances. Contrary to prior understanding of multi-armed bandits, our work reveals a surprising fact that the attackability of a specific CMAB instance also depends on whether the bandit instance is known or unknown to the adversary. This finding indicates that adversarial attacks on CMAB are difficult in practice and a general attack strategy for any CMAB instance does not exist since the environment is mostly unknown to the adversary. We validate our theoretical findings via extensive experiments on real-world CMAB applications including probabilistic maximum covering problem, online minimum spanning tree, cascading bandits for online ranking, and online shortest path.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.05308v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rishab Balasubramanian, Jiawei Li, Prasad Tadepalli, Huazheng Wang, Qingyun Wu, Haoyu Zhao</dc:creator>
    </item>
    <item>
      <title>Online Algorithms with Uncertainty-Quantified Predictions</title>
      <link>https://arxiv.org/abs/2310.11558</link>
      <description>arXiv:2310.11558v2 Announce Type: replace-cross 
Abstract: The burgeoning field of algorithms with predictions studies the problem of using possibly imperfect machine learning predictions to improve online algorithm performance. While nearly all existing algorithms in this framework make no assumptions on prediction quality, a number of methods providing uncertainty quantification (UQ) on machine learning models have been developed in recent years, which could enable additional information about prediction quality at decision time. In this work, we investigate the problem of optimally utilizing uncertainty-quantified predictions in the design of online algorithms. In particular, we study two classic online problems, ski rental and online search, where the decision-maker is provided predictions augmented with UQ describing the likelihood of the ground truth falling within a particular range of values. We demonstrate that non-trivial modifications to algorithm design are needed to fully leverage the UQ predictions. Moreover, we consider how to utilize more general forms of UQ, proposing an online learning framework that learns to exploit UQ to make decisions in multi-instance settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.11558v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bo Sun, Jerry Huang, Nicolas Christianson, Mohammad Hajiesmaili, Adam Wierman, Raouf Boutaba</dc:creator>
    </item>
    <item>
      <title>Transductive Sample Complexities Are Compact</title>
      <link>https://arxiv.org/abs/2402.10360</link>
      <description>arXiv:2402.10360v2 Announce Type: replace-cross 
Abstract: We demonstrate a compactness result holding broadly across supervised learning with a general class of loss functions: Any hypothesis class $H$ is learnable with transductive sample complexity $m$ precisely when all of its finite projections are learnable with sample complexity $m$. We prove that this exact form of compactness holds for realizable and agnostic learning with respect to any proper metric loss function (e.g., any norm on $\mathbb{R}^d$) and any continuous loss on a compact space (e.g., cross-entropy, squared loss). For realizable learning with improper metric losses, we show that exact compactness of sample complexity can fail, and provide matching upper and lower bounds of a factor of 2 on the extent to which such sample complexities can differ. We conjecture that larger gaps are possible for the agnostic case. Furthermore, invoking the equivalence between sample complexities in the PAC and transductive models (up to lower order factors, in the realizable case) permits us to directly port our results to the PAC model, revealing an almost-exact form of compactness holding broadly in PAC learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10360v2</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Julian Asilis, Siddartha Devic, Shaddin Dughmi, Vatsal Sharan, Shang-Hua Teng</dc:creator>
    </item>
  </channel>
</rss>
