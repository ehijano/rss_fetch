<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Dec 2025 02:34:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>HeteroJIVE: Joint Subspace Estimation for Heterogeneous Multi-View Data</title>
      <link>https://arxiv.org/abs/2512.02866</link>
      <description>arXiv:2512.02866v1 Announce Type: new 
Abstract: Many modern datasets consist of multiple related matrices measured on a common set of units, where the goal is to recover the shared low-dimensional subspace. While the Angle-based Joint and Individual Variation Explained (AJIVE) framework provides a solution, it relies on equal-weight aggregation, which can be strictly suboptimal when views exhibit significant statistical heterogeneity (arising from varying SNR and dimensions) and structural heterogeneity (arising from individual components). In this paper, we propose HeteroJIVE, a weighted two-stage spectral algorithm tailored to such heterogeneity. Theoretically, we first revisit the ``non-diminishing" error barrier with respect to the number of views $K$ identified in recent literature for the equal-weight case. We demonstrate that this barrier is not universal: under generic geometric conditions, the bias term vanishes and our estimator achieves the $O(K^{-1/2})$ rate without the need for iterative refinement. Extending this to the general-weight case, we establish error bounds that explicitly disentangle the two layers of heterogeneity. Based on this, we derive an oracle-optimal weighting scheme implemented via a data-driven procedure. Extensive simulations corroborate our theoretical findings, and an application to TCGA-BRCA multi-omics data validates the superiority of HeteroJIVE in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02866v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyang Li, Zhongyuan Lyu</dc:creator>
    </item>
    <item>
      <title>Discrete Sequential Barycenter Arrays: Representation, Approximation, and Modeling of Probability Measures</title>
      <link>https://arxiv.org/abs/2512.02249</link>
      <description>arXiv:2512.02249v1 Announce Type: cross 
Abstract: Constructing flexible probability models that respect constraints on key functionals -- such as the mean -- is a fundamental problem in nonparametric statistics. Existing approaches lack systematic tools for enforcing such constraints while retaining full modeling flexibility. This paper introduces a new representation for univariate probability measures based on discrete sequential barycenter arrays (SBA). We study structural properties of SBA representations and establish new approximation results. In particular, we show that for any target distribution, its SBA-based discrete approximations converge in both the weak topology and in Wasserstein distances, and that the representation is exact for all distributions with finite discrete support. We further characterize a broad class of measures whose SBA partitions exhibit regularity and induce increasingly fine meshes, and we prove that this class is dense in standard probabilistic topologies. These theoretical results enable the construction of probability models that preserve prescribed values -- or full distributions -- of the mean while maintaining large support. As an application, we derive a mixture model for density estimation whose induced mixing distribution has a fixed or user-specified mean. The resulting framework provides a principled mechanism for incorporating mean constraints in nonparametric modeling while preserving strong approximation properties. The approach is illustrated using both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02249v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alejandro Jara, Carlos Sing-Long</dc:creator>
    </item>
    <item>
      <title>Mean First Passage Time of the Symmetric Noisy Voter Model with Arbitrary Initial and Boundary Conditions</title>
      <link>https://arxiv.org/abs/2512.02519</link>
      <description>arXiv:2512.02519v1 Announce Type: cross 
Abstract: Models of imitation and herding behavior often underestimate the role of individualistic actions and assume symmetric boundary conditions. However, real-world systems (e.g., electoral processes) frequently involve asymmetric boundaries. In this study, we explore how arbitrarily placed boundary conditions influence the mean first passage time in the symmetric noisy voter model, and how individualistic behavior amplifies this asymmetry. We derive exact analytical expressions for mean first passage time that accommodate any initial condition and two types of boundary configurations: (i) both boundaries absorbing, and (ii) one absorbing and one reflective. In both scenarios, mean first passage time exhibits a clear asymmetry with respect to the initial condition, shaped by the boundary placement and the rate of independent transitions. Symmetry in mean first passage time emerges only when absorbing boundaries are equidistant from the midpoint. Additionally, we show that Kramers' law holds in both configurations when the rate of independent transitions is large. Our analytical results are in excellent agreement with numerical simulations, reinforcing the robustness of our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02519v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>physics.soc-ph</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.chaos.2025.117649</arxiv:DOI>
      <arxiv:journal_reference>Chaos, Solitons &amp; Fractals 203: 117649 (2026)</arxiv:journal_reference>
      <dc:creator>Rytis Kazakevi\v{c}ius, Aleksejus Kononovicius</dc:creator>
    </item>
    <item>
      <title>Revisiting Theory of Contrastive Learning for Domain Generalization</title>
      <link>https://arxiv.org/abs/2512.02831</link>
      <description>arXiv:2512.02831v1 Announce Type: cross 
Abstract: Contrastive learning is among the most popular and powerful approaches for self-supervised representation learning, where the goal is to map semantically similar samples close together while separating dissimilar ones in the latent space. Existing theoretical methods assume that downstream task classes are drawn from the same latent class distribution used during the pretraining phase. However, in real-world settings, downstream tasks may not only exhibit distributional shifts within the same label space but also introduce new or broader label spaces, leading to domain generalization challenges. In this work, we introduce novel generalization bounds that explicitly account for both types of mismatch: domain shift and domain generalization. Specifically, we analyze scenarios where downstream tasks either (i) draw classes from the same latent class space but with shifted distributions, or (ii) involve new label spaces beyond those seen during pretraining. Our analysis reveals how the performance of contrastively learned representations depends on the statistical discrepancy between pretraining and downstream distributions. This extended perspective allows us to derive provable guarantees on the performance of learned representations on average classification tasks involving class distributions outside the pretraining latent class set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02831v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ali Alvandi, Mina Rezaei</dc:creator>
    </item>
    <item>
      <title>Hypothesis Testing for Generalized Thurstone Models</title>
      <link>https://arxiv.org/abs/2512.02912</link>
      <description>arXiv:2512.02912v1 Announce Type: cross 
Abstract: In this work, we develop a hypothesis testing framework to determine whether pairwise comparison data is generated by an underlying \emph{generalized Thurstone model} $\mathcal{T}_F$ for a given choice function $F$. While prior work has predominantly focused on parameter estimation and uncertainty quantification for such models, we address the fundamental problem of minimax hypothesis testing for $\mathcal{T}_F$ models. We formulate this testing problem by introducing a notion of separation distance between general pairwise comparison models and the class of $\mathcal{T}_F$ models. We then derive upper and lower bounds on the critical threshold for testing that depend on the topology of the observation graph. For the special case of complete observation graphs, this threshold scales as $\Theta((nk)^{-1/2})$, where $n$ is the number of agents and $k$ is the number of comparisons per pair. Furthermore, we propose a hypothesis test based on our separation distance, construct confidence intervals, establish time-uniform bounds on the probabilities of type I and II errors using reverse martingale techniques, and derive minimax lower bounds using information-theoretic methods. Finally, we validate our results through experiments on synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02912v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>42nd International Conference on Machine Learning (ICML 2025)</arxiv:journal_reference>
      <dc:creator>Anuran Makur, Japneet Singh</dc:creator>
    </item>
    <item>
      <title>Asymptotics for additive functionals of particle systems via Stein's method</title>
      <link>https://arxiv.org/abs/2512.02922</link>
      <description>arXiv:2512.02922v1 Announce Type: cross 
Abstract: We consider additive functionals of systems of random measures whose initial configuration is given by a Poisson point process, and whose individual components evolve according to arbitrary Markovian or non-Markovian measure valued dynamics, with no structural assumptions beyond basic moment bounds. In this setting and under adequate conditions, we establish a general third moment theorem for the normalized functionals. Building on this result, we obtain the first quantitative bounds in the Wasserstein distance for a variety of moving-measure models initialized by Poisson-driven clouds of points, turning qualitative central limit theorems into explicit rates of convergence. The scope of the approach is then demonstrated through several examples, including systems driven by fractional Brownian motion, $\alpha$-stable processes, uniformly elliptic diffusions, and spectral empirical measures arising from Dyson Brownian motion, all under broad assumptions on the control measure of the initial Poisson configuration. The analysis relies on a combination of Stein's method with Mecke's formula, in the spirit of the Poisson Malliavin-Stein methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02922v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arturo Jaramillo, Antonio Murillo-Salas</dc:creator>
    </item>
    <item>
      <title>Autoregressive networks with dependent edges</title>
      <link>https://arxiv.org/abs/2404.15654</link>
      <description>arXiv:2404.15654v3 Announce Type: replace 
Abstract: We propose an autoregressive framework for modelling dynamic networks with dependent edges. It encompasses models that accommodate, for example, transitivity, degree heterogenenity, and other stylized features often observed in real network data. By assuming the edges of networks at each time are independent conditionally on their lagged values, the models, which exhibit a close connection with temporal ERGMs, facilitate both simulation and the maximum likelihood estimation in a straightforward manner. Due to the possibly large number of parameters in the models, the natural MLEs may suffer from slow convergence rates. An improved estimator for each component parameter is proposed based on an iteration employing projection, which mitigates the impact of the other parameters (Chang et al., 2021; Chang et al., 2023). Leveraging a martingale difference structure, the asymptotic distribution of the improved estimator is derived without the assumption of stationarity. The limiting distribution is not normal in general, although it reduces to normal when the underlying process satisfies some mixing conditions. Illustration with a transitivity model was carried out in both simulation and a real network data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15654v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Qin Fang, Eric D. Kolaczyk, Peter W. MacDonald, Qiwei Yao</dc:creator>
    </item>
    <item>
      <title>Generalized Linear Spectral Statistics of High-dimensional Sample Covariance Matrices and Its Applications</title>
      <link>https://arxiv.org/abs/2406.05811</link>
      <description>arXiv:2406.05811v2 Announce Type: replace 
Abstract: In this paper, we introduce the \textbf{G}eneralized \textbf{L}inear \textbf{S}pectral \textbf{S}tatistics (GLSS) of a high-dimensional sample covariance matrix $\bm{S}_n$, denoted as $\operatorname{tr}f(\bm{S}_n)\bm{B}_n$, which effectively captures distinct spectral properties of $\bm{S}_n$ by incorporating an ancillary matrix $\bm{B}_n$ and a test function $f$. The joint asymptotic normality of GLSS associated with different test functions is established under mild assumptions on $\bm{B}_n$ and the underlying distribution, when the dimension $n$ and sample size $N$ are comparable. The convergence rate of GLSS is determined by $\sqrt{{N}/{\operatorname{rank}(\bm{B}_n)}}$. Subsequently, we propose a novel functional projection approach based on GLSS for hypothesis testing on eigenspaces of ``population-spiked'' covariance matrices, showcasing a universality phenomenon in the magnitude of the spikes. The theoretical accuracy of our results established for GLSS and the advantages of the newly suggested testing procedure are demonstrated through various numerical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05811v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanlin Hu, Qing Yang, Xiao Han</dc:creator>
    </item>
    <item>
      <title>The Harmonic Entropy Estimator: Minimax Optimality and Semiparametric Efficiency for Infinite Alphabets</title>
      <link>https://arxiv.org/abs/2505.20153</link>
      <description>arXiv:2505.20153v4 Announce Type: replace 
Abstract: This paper considers the estimation of Shannon entropy for discrete distributions with countably infinite support. While minimax rates for finite-support distributions are established, infinite-support distributions present distinct challenges regarding bias control as probabilities vanish. We address this by introducing the \textit{harmonic entropy estimator}, a statistic derived from an exact algebraic identity relating the expectation of harmonic-transformed binomial counts to the logarithm of underlying success probabilities. We establish two main results characterizing the statistical limits of this problem. First, for the class of distributions with at least quadratically decaying tails ($p_j\lesssim j^{-2}$), we prove that the estimator achieves the parametric $L_2$-minimax convergence rate of order $1/n$. Second, under the stronger condition $p_j =o(j^{-2})$, we demonstrate that the estimator is semiparametrically efficient, converging to a normal distribution with variance matching the asymptotic efficiency bound $\textrm{Var}[\log p(X)]$. These results unify entropy estimation theory for finite-variance distributions, and provide a simple, one-step estimator with sharp theoretical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20153v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Octavio C\'esar Mesner</dc:creator>
    </item>
    <item>
      <title>Empirical Likelihood Based Inference for a Divergence Measure Based on Survival Extropy</title>
      <link>https://arxiv.org/abs/2507.15810</link>
      <description>arXiv:2507.15810v2 Announce Type: replace 
Abstract: Survival extropy, which quantifies the uncertainty associated with the remaining lifetime distribution, provides an information-theoretic perspective on survival behavior. We consider a divergence measure based on survival extropy and derive its nonparametric estimators based on U-statistics, empirical distribution functions, and kernel density. Further, we construct confidence intervals for the divergence measure using the jackknife empirical likelihood (JEL) method and the normal approximation method with a jackknife pseudo-value-based variance estimator. A comprehensive Monte Carlo simulation study is conducted to compare the performance of the measure with existing divergence measures. Additionally, we evaluate the finite-sample performance of various estimators for the proposed measure. The findings highlight the effectiveness of the divergence measure and its estimators in practical applications. Finally, we show how the proposed divergence measure is used to detect the small differences between images in image datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15810v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Naresh Garg, Isha Dewan, Sudheesh Kumar Kattumannil</dc:creator>
    </item>
    <item>
      <title>The Algorithmic Phase Transition in Correlated Spiked Models</title>
      <link>https://arxiv.org/abs/2511.06040</link>
      <description>arXiv:2511.06040v4 Announce Type: replace 
Abstract: We study the computational task of detecting and estimating correlated signals in a pair of spiked matrices $$ X=\tfrac{\lambda}{\sqrt{n}} xu^{\top}+W, \quad Y=\tfrac{\mu}{\sqrt{n}} yv^{\top}+Z $$ where the spikes $x,y$ have correlation $\rho$. Specifically, we consider two fundamental models: (1) Correlated spiked Wigner model with signal-to-noise ratio $\lambda,\mu$; (2) Correlated spiked $n*N$ Wishart (covariance) model with signal-to-noise ratio $\sqrt\lambda,\sqrt\mu$.
  We propose an efficient detection and estimation algorithm based on counting a specific family of edge-decorated cycles. The algorithm's performance is governed by the function $$ F(\lambda,\mu,\rho,\gamma)=\max\Big\{ \frac{ \lambda^2 }{ \gamma }, \frac{ \mu^2 }{ \gamma }, \frac{ \lambda^2 \rho^2 }{ \gamma-\lambda^2+\lambda^2 \rho^2 } + \frac{ \mu^2 \rho^2 }{ \gamma-\mu^2+\mu^2 \rho^2 } \Big\} \,. $$ We prove our algorithm succeeds for the correlated spiked Wigner model whenever $F(\lambda,\mu,\rho,1)&gt;1$, and succeeds for the correlated spiked Wishart model whenever $F(\lambda,\mu,\rho,\tfrac{n}{N})&gt;1$. Our result shows that an algorithm can leverage the correlation between the spikes to detect and estimate the signals even in regimes where efficiently recovering either $x$ from ${X}$ alone or $y$ from ${Y}$ alone is believed to be computationally infeasible.
  We complement our algorithmic results with evidence for a matching computational lower bound. In particular, we prove that when $F(\lambda,\mu,\rho,1)&lt;1$ for the correlated spiked Wigner model and when $F(\lambda,\mu,\rho,\tfrac{n}{N})&lt;1$ for the spiked Wishart model, all algorithms based on low-degree polynomials fails to distinguish $({X},{Y})$ with two independent noise matrices. This strongly suggests that $F=1$ is the precise computation threshold for our models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06040v4</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhangsong Li</dc:creator>
    </item>
    <item>
      <title>Sharp Self-Normalized Concentration Inequalities of Marginal Mean with Sample Variance Only</title>
      <link>https://arxiv.org/abs/2512.01817</link>
      <description>arXiv:2512.01817v3 Announce Type: replace 
Abstract: (This is the first version of a working paper. A more detailed follow-up with applications is in preparation.) We develop a family of self-normalized concentration inequalities for marginal mean under martingale-difference structure and $\phi/\tilde{\phi}$-mixing conditions, where the latter includes many processes that are not strongly mixing. The variance term is fully data-observable: naive sample variance in the martingale case and an empirical block long-run variance under mixing conditions. Thus, no predictable variance proxy is required. No specific assumption on the decay of the mixing coefficients (e.g. summability) is needed for the validity. The constants are explicit and the bounds are ready to use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01817v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihao Yuan</dc:creator>
    </item>
    <item>
      <title>Gaussian and Non-Gaussian Universality of Data Augmentation</title>
      <link>https://arxiv.org/abs/2202.09134</link>
      <description>arXiv:2202.09134v5 Announce Type: replace-cross 
Abstract: We provide universality results that quantify how data augmentation affects the variance and limiting distribution of estimates through simple surrogates, and analyze several specific models in detail. The results confirm some observations made in machine learning practice, but also lead to unexpected findings: Data augmentation may increase rather than decrease the uncertainty of estimates, such as the empirical prediction risk. It can act as a regularizer, but fails to do so in certain high-dimensional problems, and it may shift the double-descent peak of an empirical risk. Overall, the analysis shows that several properties data augmentation has been attributed with are not either true or false, but rather depend on a combination of factors -- notably the data distribution, the properties of the estimator, and the interplay of sample size, number of augmentations, and dimension. As our main theoretical tool, we develop an adaptation of Lindeberg's technique for block dependence. The resulting universality regime may be Gaussian or non-Gaussian.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.09134v5</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Han Huang, Peter Orbanz, Morgane Austern</dc:creator>
    </item>
    <item>
      <title>Gaussian universality for approximately polynomial functions of high-dimensional data</title>
      <link>https://arxiv.org/abs/2403.10711</link>
      <description>arXiv:2403.10711v4 Announce Type: replace-cross 
Abstract: Gaussian universality results assert that the properties of many estimators remain unchanged when the input data are replaced by Gaussians. Such results have gained popularity in high-dimensional statistics and machine learning, as Gaussianity often substantially simplifies downstream analyses. Yet, an open question remains on when universality may cease to hold. To address this, we establish nearly optimal upper and lower bounds for Gaussian universality approximation, measured in Kolmogorov distance, over the class of approximately polynomial functions of high-dimensional random vectors. The upper bounds adapt the invariance principle of Mossel, O'Donnell and Oleszkiewicz (2010) for high-dimensional vectors and functions beyond multilinear forms. As applications, we obtain a delta method for high-dimensional data with non-Gaussian limits, a necessary and sufficient condition for asymptotic normality, and simple estimators that are asymptotically normal but for which bootstrap fails to be consistent. We also extend recent results on the high-dimensional degeneracy of non-degenerate U-statistics, phase transition of MMD in two-sample tests with imbalanced data, and confidence spheres for high-dimensional averages. Our lower bound is constructive and shows that, for polynomials of even degree $m$, universality holds up to $m=o(\log n)$. As a corollary, the Gaussian polynomial approximation error of $\Omega(n^{-1/6m})$ is not improvable for even-degree U-statistics and V-statistics. Our results also explain how universality results for U-statistics and V-statistics differ significantly in their dependence on dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10711v4</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Han Huang, Morgane Austern, Peter Orbanz</dc:creator>
    </item>
    <item>
      <title>Jackknife Empirical Likelihood Method for U Statistics Based on Multivariate Samples and its Applications</title>
      <link>https://arxiv.org/abs/2408.14038</link>
      <description>arXiv:2408.14038v2 Announce Type: replace-cross 
Abstract: We develop a jackknife empirical likelihood (JEL) framework for inference on parameters defined through multivariate three-sample U-statistic. From three independent multivariate samples, we construct JEL ratio statistic based on suitable jackknife pseudo-values and, under mild regularity conditions, establish a Wilks-type result showing that the log JEL ratio converges in distribution to a chi-square limit. This provides asymptotically valid confidence intervals for the parameter of interest without explicit variance estimation or heavy resampling. To illustrate the usefulness of the proposed method, we construct confidence intervals for differences in volume under the surface (VUS) measures, which are widely used in classification problems. Through Monte Carlo simulations, we compare the performance of JEL-based confidence intervals with those obtained from normal approximation of U-statistic and kernel-based methods. The findings indicate that the proposed JEL approach outperforms existing methods in terms of coverage probability and computational efficiency. Finally, we apply our methods to a recent real dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14038v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Naresh Garg, Litty Mathew, Isha Dewan, Sudheesh Kumar Kattumannil</dc:creator>
    </item>
    <item>
      <title>Minimax Hypothesis Testing for the Bradley-Terry-Luce Model</title>
      <link>https://arxiv.org/abs/2410.08360</link>
      <description>arXiv:2410.08360v2 Announce Type: replace-cross 
Abstract: The Bradley-Terry-Luce (BTL) model is one of the most widely used models for ranking a collection of items or agents based on pairwise comparisons among them. Given $n$ agents, the BTL model endows each agent $i$ with a latent skill score $\alpha_i &gt; 0$ and posits that the probability that agent $i$ is preferred over agent $j$ is $\alpha_i/(\alpha_i + \alpha_j)$. In this work, our objective is to formulate a hypothesis test that determines whether a given pairwise comparison dataset, with $k$ comparisons per pair of agents, originates from an underlying BTL model. We formalize this testing problem in the minimax sense and define the critical threshold of the problem. We then establish upper bounds on the critical threshold for general induced observation graphs (satisfying mild assumptions) and develop lower bounds for complete induced graphs. Our bounds demonstrate that for complete induced graphs, the critical threshold scales as $\Theta((nk)^{-1/2})$ in a minimax sense. In particular, our test statistic for the upper bounds is based on a new approximation we derive for the separation distance between general pairwise comparison models and the class of BTL models. To further assess the performance of our statistical test, we prove upper bounds on the type I and type II probabilities of error. Much of our analysis is conducted within the context of a fixed observation graph structure, where the graph possesses certain ``nice'' properties, such as expansion and bounded principal ratio. Additionally, we derive several auxiliary results, such as bounds on principal ratios of graphs, $\ell^2$-bounds on BTL parameter estimation under model mismatch, stability of rankings under the BTL model, etc. We validate our theoretical results through experiments on synthetic and real-world datasets and propose a data-driven permutation testing approach to determine test thresholds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08360v2</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TIT.2025.3605846</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Information Theory, vol. 71, no. 12, December 2025</arxiv:journal_reference>
      <dc:creator>Anuran Makur, Japneet Singh</dc:creator>
    </item>
    <item>
      <title>kNNSampler: Stochastic Imputations for Recovering Missing Value Distributions</title>
      <link>https://arxiv.org/abs/2509.08366</link>
      <description>arXiv:2509.08366v2 Announce Type: replace-cross 
Abstract: We study a missing-value imputation method, termed kNNSampler, that imputes a given unit's missing response by randomly sampling from the observed responses of the $k$ most similar units to the given unit in terms of the observed covariates. This method can sample unknown missing values from their distributions, quantify the uncertainties of missing values, and be readily used for multiple imputation. Unlike popular kNNImputer, which estimates the conditional mean of a missing response given an observed covariate, kNNSampler is theoretically shown to estimate the conditional distribution of a missing response given an observed covariate. Experiments illustrate the performance of kNNSampler. The code for kNNSampler is made publicly available (https://github.com/SAP/knn-sampler).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08366v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Parastoo Pashmchi, J\'er\^ome Benoit, Motonobu Kanagawa</dc:creator>
    </item>
    <item>
      <title>Coherent estimation of risk measures</title>
      <link>https://arxiv.org/abs/2510.05809</link>
      <description>arXiv:2510.05809v3 Announce Type: replace-cross 
Abstract: We develop a statistical framework for risk estimation, inspired by the axiomatic theory of risk measures. Coherent risk estimators -- functionals of P&amp;L samples inheriting the economic properties of risk measures -- are defined and characterized through robust representations linked to $L$-estimators. The framework provides a canonical methodology for constructing estimators with sound financial and statistical properties, unifying risk measure theory, principles for capital adequacy, and practical statistical challenges in market risk. A numerical study illustrates the approach, focusing on expected shortfall estimation under both i.i.d. and overlapping samples relevant for regulatory FRTB model applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05809v3</guid>
      <category>q-fin.RM</category>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Aichele, Igor Cialenco, Damian Jelito, Marcin Pitera</dc:creator>
    </item>
    <item>
      <title>On Statistical Inference for High-Dimensional Binary Time Series</title>
      <link>https://arxiv.org/abs/2512.00338</link>
      <description>arXiv:2512.00338v2 Announce Type: replace-cross 
Abstract: The analysis of non-real-valued data, such as binary time series, has attracted great interest in recent years. This manuscript proposes a post-selection estimator for estimating the coefficient matrices of a high-dimensional generalized binary vector autoregressive process and establishes a Gaussian approximation theorem for the proposed estimator. Furthermore, it introduces a second-order wild bootstrap algorithm to enable statistical inference on the coefficient matrices. Numerical studies and empirical applications demonstrate the good finite-sample performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00338v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dehao Dai, Yunyi Zhang</dc:creator>
    </item>
  </channel>
</rss>
