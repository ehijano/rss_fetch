<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 May 2025 04:02:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Chatting with Papers: A Hybrid Approach Using LLMs and Knowledge Graphs</title>
      <link>https://arxiv.org/abs/2505.11633</link>
      <description>arXiv:2505.11633v1 Announce Type: new 
Abstract: This demo paper reports on a new workflow \textit{GhostWriter} that combines the use of Large Language Models and Knowledge Graphs (semantic artifacts) to support navigation through collections. Situated in the research area of Retrieval Augmented Generation, this specific workflow details the creation of local and adaptable chatbots. Based on the tool-suite \textit{EverythingData} at the backend, \textit{GhostWriter} provides an interface that enables querying and ``chatting'' with a collection. Applied iteratively, the workflow supports the information needs of researchers when interacting with a collection of papers, whether it be to gain an overview, to learn more about a specific concept and its context, and helps the researcher ultimately to refine their research question in a controlled way. We demonstrate the workflow for a collection of articles from the \textit{method data analysis} journal published by GESIS -- Leibniz-Institute for the Social Sciences. We also point to further application areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11633v1</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vyacheslav Tykhonov, Han Yang, Philipp Mayr, Jetze Touber, Andrea Scharnhorst</dc:creator>
    </item>
    <item>
      <title>The effect of APC discounts on Ukraine's participation in gold open access journals</title>
      <link>https://arxiv.org/abs/2505.12134</link>
      <description>arXiv:2505.12134v1 Announce Type: new 
Abstract: This study investigates the effect of article processing charge (APC) waivers on the participation of Ukrainian researchers in fully Gold Open Access journals published by the five largest academic publishers during the period 2019-2024. In response to the full-scale war launched against Ukraine in 2022, many publishers implemented extraordinary APC waiver policies to support affected authors. Using bibliometric data from the Web of Science Core Collection, this study examines trends in Ukrainian-authored publications in fully Gold OA journals before and after 2022, comparing them with those in neighbouring countries (Poland, Czech Republic, Hungary, and Romania). The results reveal a substantial post-2022 increase in Ukraine's Gold OA output, particularly in journals by Springer Nature and Elsevier. While this growth appears to correlate with the introduction of APC-waivers, additional factors, such as international collaborations, emergency grant support, and individual publishing strategies, also contributed. Disciplinary differences and publisher-specific patterns are observed, with notable increases in medical and applied sciences. The study highlights the potential of targeted support initiatives during crises but also points to the limitations of APC-based models in achieving equitable scholarly communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12134v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Serhii Nazarovets</dc:creator>
    </item>
    <item>
      <title>CHAD-KG: A Knowledge Graph for Representing Cultural Heritage Objects and Digitisation Paradata</title>
      <link>https://arxiv.org/abs/2505.13276</link>
      <description>arXiv:2505.13276v1 Announce Type: new 
Abstract: This paper presents CHAD-KG, a knowledge graph designed to describe bibliographic metadata and digitisation paradata of cultural heritage objects in exhibitions, museums, and collections. It also documents the related data model and materialisation engine. Originally based on two tabular datasets, the data was converted into RDF according to CHAD-AP, an OWL application profile built on standards like CIDOC-CRM, LRMoo, CRMdig, and Getty AAT. A reproducible pipeline, developed with a Morph-KGC extension, was used to generate the graph. CHAD-KG now serves as the main metadata source for the Digital Twin of the temporary exhibition titled \emph{The Other Renaissance - Ulisse Aldrovandi and The Wonders Of The World}, and other collections related to the digitisation work under development in a nationwide funded project, i.e. Project CHANGES (https://fondazionechanges.org). To ensure accessibility and reuse, it offers a SPARQL endpoint, a user interface, open documentation, and is published on Zenodo under a CC0 license. The project improves the semantic interoperability of cultural heritage data, with future work aiming to extend the data model and materialisation pipeline to better capture the complexities of acquisition and digitisation, further enrich the dataset and broaden its relevance to similar initiatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13276v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Barzaghi, Arianna Moretti, Ivan Heibi, Silvio Peroni</dc:creator>
    </item>
    <item>
      <title>The Impact and Influence of Academic Genealogies</title>
      <link>https://arxiv.org/abs/2505.11503</link>
      <description>arXiv:2505.11503v1 Announce Type: cross 
Abstract: We introduce the concept of an academic genealogy, or AG, and illustrate how AG charts may be constructed and then demonstrate how this methodology can be used by applying it to create the partial or full AG charts to two scientists, Paul A. Samuelson and Ronald E. Mickens.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11503v1</guid>
      <category>physics.hist-ph</category>
      <category>cs.DL</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bryan Briones, Ronald E. Mickens, Charmayne Patterson</dc:creator>
    </item>
    <item>
      <title>Let's have a chat with the EU AI Act</title>
      <link>https://arxiv.org/abs/2505.11946</link>
      <description>arXiv:2505.11946v1 Announce Type: cross 
Abstract: As artificial intelligence (AI) regulations evolve and the regulatory landscape develops and becomes more complex, ensuring compliance with ethical guidelines and legal frameworks remains a challenge for AI developers. This paper introduces an AI-driven self-assessment chatbot designed to assist users in navigating the European Union AI Act and related standards. Leveraging a Retrieval-Augmented Generation (RAG) framework, the chatbot enables real-time, context-aware compliance verification by retrieving relevant regulatory texts and providing tailored guidance. By integrating both public and proprietary standards, it streamlines regulatory adherence, reduces complexity, and fosters responsible AI development. The paper explores the chatbot's architecture, comparing naive and graph-based RAG models, and discusses its potential impact on AI governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11946v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.LG</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Kovari, Yasin Ghafourian, Csaba Hegedus, Belal Abu Naim, Kitti Mezei, Pal Varga, Markus Tauber</dc:creator>
    </item>
    <item>
      <title>Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment</title>
      <link>https://arxiv.org/abs/2505.12452</link>
      <description>arXiv:2505.12452v1 Announce Type: cross 
Abstract: Large language models (LLMs) increasingly demonstrate signs of conceptual understanding, yet much of their internal knowledge remains latent, loosely structured, and difficult to access or evaluate. We propose self-questioning as a lightweight and scalable strategy to improve LLMs' understanding, particularly in domains where success depends on fine-grained semantic distinctions. To evaluate this approach, we introduce a challenging new benchmark of 1.3 million post-2015 computer science patent pairs, characterized by dense technical jargon and strategically complex writing. The benchmark centers on a pairwise differentiation task: can a model distinguish between closely related but substantively different inventions? We show that prompting LLMs to generate and answer their own questions - targeting the background knowledge required for the task - significantly improves performance. These self-generated questions and answers activate otherwise underutilized internal knowledge. Allowing LLMs to retrieve answers from external scientific texts further enhances performance, suggesting that model knowledge is compressed and lacks the full richness of the training data. We also find that chain-of-thought prompting and self-questioning converge, though self-questioning remains more effective for improving understanding of technical concepts. Notably, we uncover an asymmetry in prompting: smaller models often generate more fundamental, more open-ended, better-aligned questions for mid-sized models than large models with better understanding do, revealing a new strategy for cross-model collaboration. Altogether, our findings establish self-questioning as both a practical mechanism for automatically improving LLM comprehension, especially in domains with sparse and underrepresented knowledge, and a diagnostic probe of how internal and external knowledge are organized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12452v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.IR</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyang Wu, Honglin Bao, Nadav Kunievsky, James A. Evans</dc:creator>
    </item>
    <item>
      <title>Detecting LLM-Generated Peer Reviews</title>
      <link>https://arxiv.org/abs/2503.15772</link>
      <description>arXiv:2503.15772v2 Announce Type: replace 
Abstract: The integrity of peer review is fundamental to scientific progress, but the rise of large language models (LLMs) has introduced concerns that some reviewers may rely on these tools to generate reviews rather than writing them independently. Although some venues have banned LLM-assisted reviewing, enforcement remains difficult as existing detection tools cannot reliably distinguish between fully generated reviews and those merely polished with AI assistance. In this work, we address the challenge of detecting LLM-generated reviews. We consider the approach of performing indirect prompt injection via the paper's PDF, prompting the LLM to embed a covert watermark in the generated review, and subsequently testing for presence of the watermark in the review. We identify and address several pitfalls in na\"ive implementations of this approach. Our primary contribution is a rigorous watermarking and detection framework that offers strong statistical guarantees. Specifically, we introduce watermarking schemes and hypothesis tests that control the family-wise error rate across multiple reviews, achieving higher statistical power than standard corrections such as Bonferroni, while making no assumptions about the nature of human-written reviews. We explore multiple indirect prompt injection strategies--including font-based embedding and obfuscated prompts--and evaluate their effectiveness under various reviewer defense scenarios. Our experiments find high success rates in watermark embedding across various LLMs. We also empirically find that our approach is resilient to common reviewer defenses, and that the bounds on error rates in our statistical tests hold in practice. In contrast, we find that Bonferroni-style corrections are too conservative to be useful in this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15772v2</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vishisht Rao, Aounon Kumar, Himabindu Lakkaraju, Nihar B. Shah</dc:creator>
    </item>
    <item>
      <title>Gaming the Metrics? Bibliometric Anomalies and the Integrity Crisis in Global University Rankings</title>
      <link>https://arxiv.org/abs/2505.06448</link>
      <description>arXiv:2505.06448v2 Announce Type: replace 
Abstract: Global university rankings have reshaped how academic success is defined, incentivizing metrics such as publication counts and citation rates at the expense of scholarly integrity. This study examines 18 universities in India, Lebanon, Saudi Arabia, and the United Arab Emirates, selected from among the world's 1,000 most-publishing institutions for their extraordinary research growth and sharp declines in first and corresponding authorship. These institutions exhibit bibliometric patterns consistent with strategic metric optimization, including publication surges of up to 965%, a proliferation of hyper-prolific authors, dense reciprocal co-authorship and citation networks, elevated shares of output in delisted journals, and rising retraction rates. These patterns are analyzed in light of Goodhart's Law and institutional isomorphism, illustrating how performance pressures can reshape academic behavior. To systematically assess and monitor such risks, the study introduces the Research Integrity Risk Index (RI2), a composite indicator based on retraction rates and reliance on delisted journals. RI2 effectively identifies institutions with bibliometric profiles that diverge from global norms and may warrant closer examination. The findings highlight the urgent need for integrity-sensitive reforms in how rankings, funders, and institutions assess scholarly performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06448v2</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lokman I. Meho</dc:creator>
    </item>
    <item>
      <title>The ICML 2023 Ranking Experiment: Examining Author Self-Assessment in ML/AI Peer Review</title>
      <link>https://arxiv.org/abs/2408.13430</link>
      <description>arXiv:2408.13430v2 Announce Type: replace-cross 
Abstract: We conducted an experiment during the review process of the 2023 International Conference on Machine Learning (ICML), asking authors with multiple submissions to rank their papers based on perceived quality. In total, we received 1,342 rankings, each from a different author, covering 2,592 submissions. In this paper, we present an empirical analysis of how author-provided rankings could be leveraged to improve peer review processes at machine learning conferences. We focus on the Isotonic Mechanism, which calibrates raw review scores using the author-provided rankings. Our analysis shows that these ranking-calibrated scores outperform the raw review scores in estimating the ground truth ``expected review scores'' in terms of both squared and absolute error metrics. Furthermore, we propose several cautious, low-risk applications of the Isotonic Mechanism and author-provided rankings in peer review, including supporting senior area chairs in overseeing area chairs' recommendations, assisting in the selection of paper awards, and guiding the recruitment of emergency reviewers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13430v2</guid>
      <category>stat.AP</category>
      <category>cs.DL</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Buxin Su, Jiayao Zhang, Natalie Collina, Yuling Yan, Didong Li, Kyunghyun Cho, Jianqing Fan, Aaron Roth, Weijie Su</dc:creator>
    </item>
  </channel>
</rss>
