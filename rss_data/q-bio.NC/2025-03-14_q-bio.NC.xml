<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Mar 2025 04:00:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Backward Stochastic Differential Equations-guided Generative Model for Structural-to-functional Neuroimage Translator</title>
      <link>https://arxiv.org/abs/2503.09606</link>
      <description>arXiv:2503.09606v1 Announce Type: new 
Abstract: A Method for structural-to-functional neuroimage translator</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09606v1</guid>
      <category>q-bio.NC</category>
      <category>math.PR</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zengjing Chen, Lu Wang, Yongkang Lin, Jie Peng, Zhiping Liu, Jie Luo, Bao Wang, Yingchao Liu, Nazim Haouchine, Xu Qiao</dc:creator>
    </item>
    <item>
      <title>Mechanoreceptive A$\beta$ primary afferents discriminate naturalistic social touch inputs at a functionally relevant time scale</title>
      <link>https://arxiv.org/abs/2503.09672</link>
      <description>arXiv:2503.09672v1 Announce Type: new 
Abstract: Interpersonal touch is an important channel of social emotional interaction. How these physical skin-to-skin touch expressions are processed in the peripheral nervous system is not well understood. From microneurography recordings in humans, we evaluated the capacity of six subtypes of cutaneous mechanoreceptive afferents to differentiate human-delivered social touch expressions. Leveraging statistical and classification analyses, we found that single units of multiple mechanoreceptive A$\beta$ subtypes, especially slowly adapting type II (SA-II) and fast adapting hair follicle afferents (HFA), can reliably differentiate social touch expressions at accuracies similar to human recognition. We then identified the most informative firing patterns of SA-II and HFA afferents, which indicate that average durations of 3-4 s of firing provide sufficient discriminative information. Those two subtypes also exhibit robust tolerance to spike-timing shifts of up to 10-20 ms, varying with touch expressions due to their specific firing properties. Greater shifts in spike-timing, however, can change a firing pattern's envelope to resemble that of another expression and drastically compromise an afferent's discrimination capacity. Altogether, the findings indicate that SA-II and HFA afferents differentiate the skin contact of social touch at time scales relevant for such interactions, which are 1-2 orders of magnitude longer than those for non-social touch.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09672v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TAFFC.2024.3435060</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Affective Computing, 2025, 16(1), 346-359</arxiv:journal_reference>
      <dc:creator>Shan Xu, Steven C. Hauser, Saad S. Nagi, James A. Jablonski, Merat Rezaei, Ewa Jarocka, Andrew G. Marshall, H{\aa}kan Olausson, Sarah McIntyre, Gregory J. Gerling</dc:creator>
    </item>
    <item>
      <title>Increased GM-WM in a prefrontal network and decreased GM in the insula and the precuneus are associated with reappraisal usage: A data fusion approach</title>
      <link>https://arxiv.org/abs/2503.09984</link>
      <description>arXiv:2503.09984v1 Announce Type: new 
Abstract: Emotion regulation plays a crucial role in mental health, and difficulties in regulating emotions can contribute to psychological disorders. While reappraisal and suppression are well-studied strategies, the combined contributions of gray matter (GM) and white matter (WM) to these strategies remain unclear due to methodological limitations in previous studies. To address this, we applied a data fusion approach using Parallel Independent Component Analysis (Parallel ICA) to GM and WM MRI images from 165 individuals. Parallel ICA identified two networks associated with reappraisal usage. Network 1 included a large lateral and medial prefrontal cortical network, overlapping with the default mode network (DMN) and adjacent WM regions. Higher reappraisal frequency was associated with greater GM-WM density within this network, and this network was negatively correlated with perceived stress. Network 2 included the insula, precuneus, sub-gyral, and lingual gyri in its GM portion, showing a negative association with reappraisal usage. The WM portion, adjacent to regions of the central executive network (CEN), was positively associated with reappraisal usage. Regarding suppression, no significant network was associated with this strategy. This study provides new insights into individual differences in reappraisal use, showing a positive association between reappraisal frequency and increased gray and white matter concentration in a large frontal network, including regions of the frontal DMN and the CEN. Conversely, subcortical areas exhibited reduced gray and white concentration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09984v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Grecucci, Parisa Ahmadi Ghomroudi, Carmen Morawetz, Valerie Lesk, Irene Messina</dc:creator>
    </item>
    <item>
      <title>Thermodynamic Bound on Energy and Negentropy Costs of Inference in Deep Neural Networks</title>
      <link>https://arxiv.org/abs/2503.09980</link>
      <description>arXiv:2503.09980v1 Announce Type: cross 
Abstract: The fundamental thermodynamic bound is derived for the energy cost of inference in Deep Neural Networks (DNNs). By applying Landauer's principle, we demonstrate that the linear operations in DNNs can, in principle, be performed reversibly, whereas the non-linear activation functions impose an unavoidable energy cost. The resulting theoretical lower bound on the inference energy is determined by the average number of neurons undergoing state transition for each inference. We also restate the thermodynamic bound in terms of negentropy, a metric which is more universal than energy for assessing thermodynamic cost of information processing. Concept of negentropy is further elaborated in the context of information processing in biological and engineered system as well as human intelligence. Our analysis provides insight into the physical limits of DNN efficiency and suggests potential directions for developing energy-efficient AI architectures that leverage reversible analog computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09980v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexei V. Tkachenko</dc:creator>
    </item>
    <item>
      <title>ST-FlowNet: An Efficient Spiking Neural Network for Event-Based Optical Flow Estimation</title>
      <link>https://arxiv.org/abs/2503.10195</link>
      <description>arXiv:2503.10195v1 Announce Type: cross 
Abstract: Spiking Neural Networks (SNNs) have emerged as a promising tool for event-based optical flow estimation tasks due to their ability to leverage spatio-temporal information and low-power capabilities. However, the performance of SNN models is often constrained, limiting their application in real-world scenarios. In this work, we address this gap by proposing a novel neural network architecture, ST-FlowNet, specifically tailored for optical flow estimation from event-based data. The ST-FlowNet architecture integrates ConvGRU modules to facilitate cross-modal feature augmentation and temporal alignment of the predicted optical flow, improving the network's ability to capture complex motion dynamics. Additionally, to overcome the challenges associated with training SNNs, we introduce a novel approach to derive SNN models from pre-trained artificial neural networks (ANNs) through ANN-to-SNN conversion or our proposed BISNN method. Notably, the BISNN method alleviates the complexities involved in biological parameter selection, further enhancing the robustness of SNNs in optical flow estimation tasks. Extensive evaluations on three benchmark event-based datasets demonstrate that the SNN-based ST-FlowNet model outperforms state-of-the-art methods, delivering superior performance in accurate optical flow estimation across a diverse range of dynamic visual scenes. Furthermore, the inherent energy efficiency of SNN models is highlighted, establishing a compelling advantage for their practical deployment. Overall, our work presents a novel framework for optical flow estimation using SNNs and event-based data, contributing to the advancement of neuromorphic vision applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10195v1</guid>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongze Sun, Jun Wang, Wuque Cai, Duo Chen, Qianqian Liao, Jiayi He, Yan Cui, Dezhong Yao, Daqing Guo</dc:creator>
    </item>
    <item>
      <title>BioSerenity-E1: a self-supervised EEG model for medical applications</title>
      <link>https://arxiv.org/abs/2503.10362</link>
      <description>arXiv:2503.10362v1 Announce Type: cross 
Abstract: Electroencephalography (EEG) serves as an essential diagnostic tool in neurology; however, its accurate manual interpretation is a time-intensive process that demands highly specialized expertise, which remains relatively scarce and not consistently accessible. To address these limitations, the implementation of automated pre-screening and analysis systems for EEG data holds considerable promise. Advances in self-supervised learning made it possible to pre-train complex deep learning architectures on large volumes of unlabeled EEG data to learn generalizable representations, that can later be used to enhance performance on multiple tasks while needing less downstream data. In the present paper, we introduce BioSerenity-E1, the first of a family of self-supervised foundation models for clinical EEG applications that combines spectral tokenization with masked prediction to achieve state-of-the-art performance across relevant diagnostic tasks. The two-phase self-supervised pretraining framework initially acquires compressed EEG representations via a transformer-based VQ-VAE architecture designed to reconstruct log-multitaper spectral projections, then implements extensive (70% block) masked token prediction to force the model to learn complex spatiotemporal dependencies in EEG signals. BioSerenity-E1 achieves strong performance across three clinical tasks, either in line or above state-of-the-art methods: seizure detection (AUROC = 0.926, Sensitivity = 0.909), normal/abnormal classification (AUPRC = 0.970 on proprietary data; 0.910 on TUH-Abnormal), and multiclass pathology differentiation on unbalanced data (Weighted F1 = 0.730). The utility of BioSerenity-E1 is further confirmed in low-data regimes scenarios, showing clear improvements in AUPRC (from +2% to 17%) when trained on less than 10% of the available data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10362v1</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ruggero G. Bettinardi, Mohamed Rahmouni, Ulysse Gimenez</dc:creator>
    </item>
    <item>
      <title>Why the Brain Cannot Be a Digital Computer: History-Dependence and the Computational Limits of Consciousness</title>
      <link>https://arxiv.org/abs/2503.10518</link>
      <description>arXiv:2503.10518v1 Announce Type: cross 
Abstract: This paper presents a novel information-theoretic proof demonstrating that the human brain as currently understood cannot function as a classical digital computer. Through systematic quantification of distinguishable conscious states and their historical dependencies, we establish that the minimum information required to specify a conscious state exceeds the physical information capacity of the human brain by a significant factor. Our analysis calculates the bit-length requirements for representing consciously distinguishable sensory "stimulus frames" and demonstrates that consciousness exhibits mandatory temporal-historical dependencies that multiply these requirements beyond the brain's storage capabilities. This mathematical approach offers new insights into the fundamental limitations of computational models of consciousness and suggests that non-classical information processing mechanisms may be necessary to account for conscious experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10518v1</guid>
      <category>physics.hist-ph</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Knight</dc:creator>
    </item>
    <item>
      <title>Short-term AI literacy intervention does not reduce over-reliance on incorrect ChatGPT recommendations</title>
      <link>https://arxiv.org/abs/2503.10556</link>
      <description>arXiv:2503.10556v1 Announce Type: cross 
Abstract: In this study, we examined whether a short-form AI literacy intervention could reduce the adoption of incorrect recommendations from large language models. High school seniors were randomly assigned to either a control or an intervention group, which received an educational text explaining ChatGPT's working mechanism, limitations, and proper use. Participants solved math puzzles with the help of ChatGPT's recommendations, which were incorrect in half of the cases. Results showed that students adopted incorrect suggestions 52.1% of the time, indicating widespread over-reliance. The educational intervention did not significantly reduce over-reliance. Instead, it led to an increase in ignoring ChatGPT's correct recommendations. We conclude that the usage of ChatGPT is associated with over-reliance and it is not trivial to increase AI literacy to counter over-reliance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10556v1</guid>
      <category>cs.CY</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Brett Puppart, Jaan Aru</dc:creator>
    </item>
    <item>
      <title>Deciphering Functions of Neurons in Vision-Language Models</title>
      <link>https://arxiv.org/abs/2502.18485</link>
      <description>arXiv:2502.18485v3 Announce Type: replace 
Abstract: The burgeoning growth of open-sourced vision-language models (VLMs) has catalyzed a plethora of applications across diverse domains. Ensuring the transparency and interpretability of these models is critical for fostering trustworthy and responsible AI systems. In this study, our objective is to delve into the internals of VLMs to interpret the functions of individual neurons. We observe the activations of neurons with respects to the input visual tokens and text tokens, and reveal some interesting findings. Particularly, we found that there are neurons responsible for only visual or text information, or both, respectively, which we refer to them as visual neurons, text neurons, and multi-modal neurons, respectively. We build a framework that automates the explanation of neurons with the assistant of GPT-4o. Meanwhile, for visual neurons, we propose an activation simulator to assess the reliability of the explanations for visual neurons. System statistical analyses on top of one representative VLM of LLaVA, uncover the behaviors/characteristics of different categories of neurons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18485v3</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaqi Xu, Cuiling Lan, Xuejin Chen, Yan Lu</dc:creator>
    </item>
    <item>
      <title>Three tiers of computation in transformers and in brain architectures</title>
      <link>https://arxiv.org/abs/2503.04848</link>
      <description>arXiv:2503.04848v2 Announce Type: replace-cross 
Abstract: Human language and logic abilities are computationally quantified within the well-studied grammar-automata hierarchy. We identify three hierarchical tiers and two corresponding transitions and show their correspondence to specific abilities in transformer-based language models (LMs). These emergent abilities have often been described in terms of scaling; we show that it is the transition between tiers, rather than scaled size itself, that determines a system's capabilities. Specifically, humans effortlessly process language yet require critical training to perform arithmetic or logical reasoning tasks; and LMs possess language abilities absent from predecessor systems, yet still struggle with logical processing. We submit a novel benchmark of computational power, provide empirical evaluations of humans and fifteen LMs, and, most significantly, provide a theoretically grounded framework to promote careful thinking about these crucial topics. The resulting principled analyses provide explanatory accounts of the abilities and shortfalls of LMs, and suggest actionable insights into the expansion of their logic abilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04848v2</guid>
      <category>cs.CL</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 14 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>E Graham, R Granger</dc:creator>
    </item>
  </channel>
</rss>
