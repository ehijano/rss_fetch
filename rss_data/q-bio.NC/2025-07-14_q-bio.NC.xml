<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 15 Jul 2025 02:16:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>SPINT: Spatial Permutation-Invariant Neural Transformer for Consistent Intracortical Motor Decoding</title>
      <link>https://arxiv.org/abs/2507.08402</link>
      <description>arXiv:2507.08402v1 Announce Type: new 
Abstract: Intracortical Brain-Computer Interfaces (iBCI) aim to decode behavior from neural population activity, enabling individuals with motor impairments to regain motor functions and communication abilities. A key challenge in long-term iBCI is the nonstationarity of neural recordings, where the composition and tuning profiles of the recorded populations are unstable across recording sessions. Existing methods attempt to address this issue by explicit alignment techniques; however, they rely on fixed neural identities and require test-time labels or parameter updates, limiting their generalization across sessions and imposing additional computational burden during deployment. In this work, we introduce SPINT - a Spatial Permutation-Invariant Neural Transformer framework for behavioral decoding that operates directly on unordered sets of neural units. Central to our approach is a novel context-dependent positional embedding scheme that dynamically infers unit-specific identities, enabling flexible generalization across recording sessions. SPINT supports inference on variable-size populations and allows few-shot, gradient-free adaptation using a small amount of unlabeled data from the test session. To further promote model robustness to population variability, we introduce dynamic channel dropout, a regularization method for iBCI that simulates shifts in population composition during training. We evaluate SPINT on three multi-session datasets from the FALCON Benchmark, covering continuous motor decoding tasks in human and non-human primates. SPINT demonstrates robust cross-session generalization, outperforming existing zero-shot and few-shot unsupervised baselines while eliminating the need for test-time alignment and fine-tuning. Our work contributes an initial step toward a robust and scalable neural decoding framework for long-term iBCI applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08402v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Trung Le, Hao Fang, Jingyuan Li, Tung Nguyen, Lu Mi, Amy Orsborn, Uygar S\"umb\"ul, Eli Shlizerman</dc:creator>
    </item>
    <item>
      <title>Transcranial Focused Ultrasound for Identifying the Neural Substrate of Conscious Perception</title>
      <link>https://arxiv.org/abs/2507.08517</link>
      <description>arXiv:2507.08517v1 Announce Type: new 
Abstract: Identifying what aspects of brain activity are responsible for conscious perception remains one of the most challenging problems in science. While progress has been made through psychophysical studies employing EEG and fMRI, research would greatly benefit from improved methods for stimulating the brain in healthy human subjects. Traditional techniques for neural stimulation through the skull, including electrical or magnetic stimulation, suffer from coarse spatial resolution and have limited ability to target deep brain structures with high spatial selectivity. Over the past decade, a new tool has emerged known as transcranial focused ultrasound (tFUS), which enables the human brain to be stimulated safely and non-invasively through the skull with millimeter-scale spatial resolution, including cortical as well as deep brain structures. This tool offers an exciting opportunity for breakthroughs in consciousness research. Given the extensive preparation and regulatory approvals associated with tFUS testing, careful experimental planning is essential. Therefore, our goal here is to provide a roadmap for using tFUS in humans for exploring the neural substrate of conscious perception.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08517v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel K. Freeman, Brian Odegaard, Seung-Schik Yoo, Matthias Michel</dc:creator>
    </item>
    <item>
      <title>Critical dynamics governs deep learning</title>
      <link>https://arxiv.org/abs/2507.08527</link>
      <description>arXiv:2507.08527v1 Announce Type: new 
Abstract: Artificial intelligence has advanced rapidly through larger and deeper neural networks, yet fundamental questions remain about how to optimize network dynamics for performance and adaptability. This study shows that deep neural networks (DNNs), like biological brains, perform optimally when operating near a critical phase transition - poised between active and inactive dynamics. Drawing from physics and neuroscience, we demonstrate that criticality provides a unifying principle linking structure, dynamics, and function in DNNs. Analyzing more than 80 state-of-the-art models, we first report that improvements in accuracy over the past decade coincided with an implicit evolution toward more critical dynamics. Architectural and training innovations unknowingly guided networks toward this optimal regime. Second, building on these insights, we develop a training method that explicitly drives networks to criticality, improving robustness and performance. Third, we show that fundamental problems in AI, including loss of performance in deep continual learning, are caused by loss of criticality and that maintaining criticality rescues performance. This work introduces criticality as a fundamental framework for AI development by emphasizing dynamic optimization alongside scale. It bridges artificial intelligence with physics and biological cortical network function inspiring novel self-tuning strategies in DNNs. The findings offer a theoretically grounded path forward in designing efficient, adaptable, and high-performing artificial intelligence systems drawing inspiration from principles observed in biological neural systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08527v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Vock, Christian Meisel</dc:creator>
    </item>
    <item>
      <title>Distinct neurodynamics of functional brain networks in Alzheimer's disease and frontotemporal dementia as revealed by EEG</title>
      <link>https://arxiv.org/abs/2507.08728</link>
      <description>arXiv:2507.08728v1 Announce Type: new 
Abstract: Objective While Alzheimer's disease (AD) and frontotemporal dementia (FTD) show some common memory deficits, these two disorders show partially overlapping complex spatiotemporal patterns of neural dynamics. The objective of this study is to characterize these patterns to better understand the general principles of neurodynamics in these conditions.
  Methods A comprehensive array of methods to study brain rhythms and functional brain networks are used in the study, from spectral power measures to Lyapunov exponent, phase synchronization, temporal synchrony patterns, and measures of the functional brain connectivity. Furthermore, machine learning techniques for classification are used to augment the methodology.
  Results Multiple measures (spectral, synchrony, functional network organization) indicate an array of differences between neurodynamics between AD and FTD, and control subjects across different frequency bands.
  Conclusions These differences taken together in an integrative way suggest that AD neural activity may be less coordinated and less connected across areas, and more random, while FTD shows more coordinated neural activity (except slow frontal activity).
  Significance AD and FTD may represent opposite changes from normal brain function in terms of the spatiotemporal coordination of neural activity. Deviations from normal in both directions may lead to neurological deficits, which are specific to each of the disorders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08728v1</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.clinph.2025.2110931</arxiv:DOI>
      <arxiv:journal_reference>Clinical Neurophysiology (2025)</arxiv:journal_reference>
      <dc:creator>Sungwoo Ahn, Evie A. Malaia, Leonid L Rubchinsky</dc:creator>
    </item>
    <item>
      <title>Geometric representations of brain networks can predict the surgery outcome in temporal lobe epilepsy</title>
      <link>https://arxiv.org/abs/2412.17820</link>
      <description>arXiv:2412.17820v5 Announce Type: replace 
Abstract: Epilepsy surgery, particularly for temporal lobe epilepsy (TLE), remains a vital treatment option for patients with drug-resistant seizures. However, accurately predicting surgical outcomes remains a significant challenge. This study introduces a novel biomarker derived from brain connectivity, analyzed using non-Euclidean network geometry, to predict the surgery outcome in TLE. Using structural and diffusion magnetic resonance imaging (MRI) data from 51 patients, we examined differences in structural connectivity networks associated to surgical outcomes. Our approach uniquely utilized hyperbolic embeddings of pre- and post-surgery brain networks, successfully distinguishing patients with favorable outcomes from those with poor outcomes. Notably, the method identified regions in the contralateral hemisphere relative to the epileptogenic zone, whose connectivity patterns emerged as a potential biomarker for favorable surgical outcomes. The prediction model achieves an area under the curve (AUC) of 0.87 and a balanced accuracy of 0.81. These results underscore the predictive capability of our model and its effectiveness in individual outcome forecasting based on structural network changes. Our findings highlight the value of non-Euclidean representation of brain networks in gaining deeper insights into connectivity alterations in epilepsy, and advancing personalized prediction of surgical outcomes in TLE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17820v5</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Guillemaud, Alice Longhena, Louis Cousyn, Valerio Frazzini, Bertrand Mathon, Vincent Navarro, Mario Chavez</dc:creator>
    </item>
    <item>
      <title>A Turing Test for Artificial Nets devoted to model Human Vision</title>
      <link>https://arxiv.org/abs/2502.00721</link>
      <description>arXiv:2502.00721v3 Announce Type: replace 
Abstract: In our invited talk at the AI Evaluation Workshop of the University of Bristol back in June 2022 we argued that, despite claims about successful modeling of the visual brain using ANNs, the problem is far from being solved (even for low-level vision). Open issues include: where should we read from ANNs to reproduce human behavior?, this ad-hoc read-out is part of the brain model or not?, should we use artificial psychophysics or artificial physiology?, artificial experiments should literally match the experiments in humans?. There is a clear need of rigorous procedures for experimental tests for ANNs models of the visual brain, and more generally, to understand ANNs devoted to generic vision tasks. Following our experience in using low-level facts from Visual Neuroscience in Image Processing, we presented the idea of developing a low-level dataset compiling the basic spatio-temporal and chromatic facts that are known to happen in the retina-V1 pathway, and they are not currently available in existing databases such as BrainScore. In our results we checked the behavior of three recently proposed models with similar architecture: (1) A parametric model tuned via Maximum Differentiation [Malo &amp; Simoncelli SPIE 15, Martinez et al. PLOS 18, Martinez et al. Front. Neurosci. 19], (2) A non-parametric model, the PerceptNet, tuned to maximize the correlation with human opinion on subjective distortions [Hepburn et al. IEEE ICIP 20], and (3) A model with the same encoder as PerceptNet, but tuned for segmentation (published later as Hernandez-Camara et al. Patt.Recogn.Lett. 23, Hernandez-Camara et al. Neurocomp. 25). Results on 10 compelling psycho/physio visual facts show that the first model is the one with closer behavior to the humans in terms of receptive fields, but more interestingly, on the nonlinear behavior for spatio-chromatic patterns of a range of luminances and contrasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00721v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jorge Vila-Tom\'as, Pablo Hern\'andez-C\'amara, Qiang Li, Valero Laparra, Jes\'us Malo</dc:creator>
    </item>
    <item>
      <title>The role of gain neuromodulation in layer-5 pyramidal neurons</title>
      <link>https://arxiv.org/abs/2507.03222</link>
      <description>arXiv:2507.03222v2 Announce Type: replace 
Abstract: Biological and artificial learning systems alike confront the plasticity-stability dilemma. In the brain, neuromodulators such as acetylcholine and noradrenaline relieve this tension by tuning neuronal gain and inhibitory gating, balancing segregation and integration of circuits. Fed by dense cholinergic and noradrenergic projections from the ascending arousal system, layer-5 pyramidal neurons in the cerebral cortex offer a relevant substrate for understanding these dynamics. When distal dendritic signals coincide with back-propagating action potentials, calcium plateaus turn a single somatic spike into a high-gain burst, and interneuron inhibition sculpts the output. These properties make layer-5 cells gain-tunable amplifiers that translate neuromodulatory cues into flexible cortical activity. To capture this mechanism we developed a two-compartment Izhikevich model for pyramidal neurons and single-compartment somatostatin (SOM) and parvalbumin (PV) interneurons, linked by Gaussian connectivity and spike-timing-dependent plasticity (STDP). The soma and apical dendrite are so coupled that somatic spikes back-propagate, while dendritic plateaus can switch the soma from regular firing to bursting by shifting reset and adaptation variables. We show that stronger dendritic drive or tighter coupling raise gain by increasing the likelihood of calcium-triggered somatic bursts. In contrast, dendritic-targeted inhibition suppresses gain, while somatic-targeted inhibition raises the firing threshold of neighboring neurons, thus gating neurons output. Notably, bursting accelerates STDP, supporting rapid synaptic reconfiguration and flexibility. This suggests that brief gain pulses driven by neuromodulators could serve as an adaptive two-timescale optimization mechanism, effectively modulating the synaptic weight updates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03222v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Rodriguez-Garcia, Christopher J. Whyte, Brandon R. Munn, Jie Mei, James M. Shine, Srikanth Ramaswamy</dc:creator>
    </item>
    <item>
      <title>Communities in the Kuramoto Model: Dynamics and Detection via Path Signatures</title>
      <link>https://arxiv.org/abs/2503.17546</link>
      <description>arXiv:2503.17546v3 Announce Type: replace-cross 
Abstract: The behavior of multivariate dynamical processes is often governed by underlying structural connections that relate the components of the system. For example, brain activity, which is often measured via time series is determined by an underlying structural graph, where nodes represent neurons or brain regions and edges cortical connectivity. Existing methods for inferring structural connections from observed dynamics, such as correlation-based or spectral techniques, may fail to fully capture complex relationships in high-dimensional time series in an interpretable way. Here, we propose the use of path signatures, a mathematical framework that encodes geometric and temporal properties of continuous paths, to address this problem. Path signatures provide a reparametrization-invariant characterization of dynamical data and can be used to compute the lead matrix, which reveals lead-lag phenomena. We showcase our approach on time series from coupled oscillators in the Kuramoto model defined on a stochastic block model graph, termed the Kuramoto Stochastic Block Model (KSBM). Using mean-field theory and Gaussian approximations, we analytically derive reduced models of KSBM dynamics in different temporal regimes and theoretically characterize the lead matrix in these settings. Leveraging these insights, we propose a novel signature-based community detection algorithm, achieving exact recovery of structural communities from observed time series in multiple KSBM instances. We also explored the performance of our community detection on a stochastic variant of the KSBM as well as on real neuropixels of cortical recordings to demonstrate applicability on real-world data. Our results demonstrate that path signatures provide a novel perspective on analyzing complex neural data and other high-dimensional systems, explicitly exploiting temporal functional relationships to infer underlying structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17546v3</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>nlin.AO</category>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>T\^am Johan Nguy\^en, Darrick Lee, Bernadette Jana Stolz</dc:creator>
    </item>
    <item>
      <title>Feature Learning beyond the Lazy-Rich Dichotomy: Insights from Representational Geometry</title>
      <link>https://arxiv.org/abs/2503.18114</link>
      <description>arXiv:2503.18114v2 Announce Type: replace-cross 
Abstract: Integrating task-relevant information into neural representations is a fundamental ability of both biological and artificial intelligence systems. Recent theories have categorized learning into two regimes: the rich regime, where neural networks actively learn task-relevant features, and the lazy regime, where networks behave like random feature models. Yet this simple lazy-rich dichotomy overlooks a diverse underlying taxonomy of feature learning, shaped by differences in learning algorithms, network architectures, and data properties. To address this gap, we introduce an analysis framework to study feature learning via the geometry of neural representations. Rather than inspecting individual learned features, we characterize how task-relevant representational manifolds evolve throughout the learning process. We show, in both theoretical and empirical settings, that as networks learn features, task-relevant manifolds untangle, with changes in manifold geometry revealing distinct learning stages and strategies beyond the lazy-rich dichotomy. This framework provides novel insights into feature learning across neuroscience and machine learning, shedding light on structural inductive biases in neural circuits and the mechanisms underlying out-of-distribution generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18114v2</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chi-Ning Chou, Hang Le, Yichen Wang, SueYeon Chung</dc:creator>
    </item>
    <item>
      <title>Hyperchaos and complex dynamical regimes in $N$-dimensional neuron lattices</title>
      <link>https://arxiv.org/abs/2505.03051</link>
      <description>arXiv:2505.03051v2 Announce Type: replace-cross 
Abstract: We study the dynamics of $N$-dimensional lattices of nonchaotic Rulkov neurons coupled with a flow of electrical current. We consider both nearest-neighbor and next-nearest-neighbor couplings, homogeneous and heterogeneous neurons, and small and large lattices over a wide range of electrical coupling strengths. As the coupling strength is varied, the neurons exhibit a number of complex dynamical regimes, including unsynchronized chaotic spiking, local quasi-bursting, synchronized chaotic bursting, and synchronized hyperchaos. For lattices in higher spatial dimensions, we discover dynamical effects arising from the "destructive interference" of many connected neurons and miniature "phase transitions" from coordinated spiking threshold crossings. In large two- and three-dimensional neuron lattices, we observe emergent dynamics such as local synchronization, quasi-synchronization, and lag synchronization. These results illustrate the rich dynamics that emerge from coupled neurons in multiple spatial dimensions, highlighting how dimensionality, connectivity, and heterogeneity critically shape the collective behavior of neuronal systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03051v2</guid>
      <category>nlin.CD</category>
      <category>cond-mat.dis-nn</category>
      <category>math.DS</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1140/epjs/s11734-025-01786-7</arxiv:DOI>
      <arxiv:journal_reference>Eur. Phys. J. Spec. Top. (2025)</arxiv:journal_reference>
      <dc:creator>Brandon B. Le, Dima Watkins</dc:creator>
    </item>
    <item>
      <title>Scientific Machine Learning of Chaotic Systems Discovers Governing Equations for Neural Populations</title>
      <link>https://arxiv.org/abs/2507.03631</link>
      <description>arXiv:2507.03631v2 Announce Type: replace-cross 
Abstract: Discovering governing equations that describe complex chaotic systems remains a fundamental challenge in physics and neuroscience. Here, we introduce the PEM-UDE method, which combines the prediction-error method with universal differential equations to extract interpretable mathematical expressions from chaotic dynamical systems, even with limited or noisy observations. This approach succeeds where traditional techniques fail by smoothing optimization landscapes and removing the chaotic properties during the fitting process without distorting optimal parameters. We demonstrate its efficacy by recovering hidden states in the Rossler system and reconstructing dynamics from noise-corrupted electrical circuit data, where the correct functional form of the dynamics is recovered even when one of the observed time series is corrupted by noise 5x the magnitude of the true signal. We demonstrate that this method is capable of recovering the correct dynamics, whereas direct symbolic regression methods, such as SINDy, fail to do so with the given amount of data and noise. Importantly, when applied to neural populations, our method derives novel governing equations that respect biological constraints such as network sparsity - a constraint necessary for cortical information processing yet not captured in next-generation neural mass models - while preserving microscale neuronal parameters. These equations predict an emergent relationship between connection density and both oscillation frequency and synchrony in neural circuits. We validate these predictions using three intracranial electrode recording datasets from the medial entorhinal cortex, prefrontal cortex, and orbitofrontal cortex. Our work provides a pathway to develop mechanistic, multi-scale brain models that generalize across diverse neural architectures, bridging the gap between single-neuron dynamics and macroscale brain activity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03631v2</guid>
      <category>cs.LG</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>nlin.CD</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anthony G. Chesebro, David Hofmann, Vaibhav Dixit, Earl K. Miller, Richard H. Granger, Alan Edelman, Christopher V. Rackauckas, Lilianne R. Mujica-Parodi, Helmut H. Strey</dc:creator>
    </item>
  </channel>
</rss>
