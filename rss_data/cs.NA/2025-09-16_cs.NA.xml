<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Sep 2025 04:04:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Invariant subspace perturbations related to defective eigenvalues of $\Delta$-Hermitian and Hamiltonian matrices</title>
      <link>https://arxiv.org/abs/2509.10643</link>
      <description>arXiv:2509.10643v1 Announce Type: new 
Abstract: Structured perturbation results for invariant subspaces of $\Delta$-Hermitian and Hamiltonian matrices are provided. The invariant subspaces under consideration are associated with the eigenvalues perturbed from a single defective eigenvalue. The results show how the original eigenvectors and generalized eigenvectors are involved in composing such perturbed invariant subspaces and eigenvectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10643v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongguo Xu</dc:creator>
    </item>
    <item>
      <title>Combined perturbation bounds for eigenstructure of Hermitian matrices and singular structure of general matrices</title>
      <link>https://arxiv.org/abs/2509.10688</link>
      <description>arXiv:2509.10688v1 Announce Type: new 
Abstract: Combined perturbation bounds are presented for eigenvalues and eigenspaces of Hermitian matrices or singular values and singular subspaces of general matrices. The bounds are derived based on the smooth decompositions and elementary calculus techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10688v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiao Shan Chen, Hongguo Xu</dc:creator>
    </item>
    <item>
      <title>Mixed regularity and sparse grid approximations of $N$-body Schr\"odinger evolution equation</title>
      <link>https://arxiv.org/abs/2509.10805</link>
      <description>arXiv:2509.10805v1 Announce Type: new 
Abstract: In this paper, we present a mathematical analysis of time-dependent $N$-body electronic systems and establish mixed regularity for the corresponding wavefunctions. Based on this, we develop sparse grid approximations to reduce computational complexity, including a sparse grid Gaussian-type orbital (GTO) scheme. We validate the approach on the Helium atom (${\rm He}$) and Hydrogen molecule (${\rm H}_2$), showing that sparse grid GTOs offer an efficient alternative to full grid discretizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10805v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.AP</category>
      <category>math.MP</category>
      <category>physics.atom-ph</category>
      <category>physics.comp-ph</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Long Meng, Dexuan Zhou</dc:creator>
    </item>
    <item>
      <title>The coupling of mixed and primal finite element methods for the coupled body-plate problem</title>
      <link>https://arxiv.org/abs/2509.10827</link>
      <description>arXiv:2509.10827v1 Announce Type: new 
Abstract: This paper considers the coupled problem of a three-dimensional elastic body and a two-dimensional plate, which are rigidly connected at their interface. The plate consists of a plane elasticity model along the longitudinal direction and a plate bending model with Kirchhoff assumptions along the transverse direction. The Hellinger-Reissner formulation is adopted for the body by introducing the stress as an auxiliary variable, while the primal formulation is employed for the plate. The well-posedness of the weak formulation is established. This approach enables direct stress approximations and allows for non-matching meshes at the interface since the continuity condition of displacements acts as a natural boundary condition for the body. Under certain assumptions, discrete stability and error estimates are derived for both conforming and nonconforming finite element methods. Two specific pairs of conforming and nonconforming finite elements are shown to satisfy the required assumptions, respectively. Furthermore, the problem is reduced to an interface problem based on the domain decomposition, which can be solved effectively by a conjugate gradient iteration. Numerical experiments are conducted to validate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10827v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jun Hu, Zhen Liu, Rui Ma</dc:creator>
    </item>
    <item>
      <title>Design and accuracy trade-offs in Computational Statistics</title>
      <link>https://arxiv.org/abs/2509.10934</link>
      <description>arXiv:2509.10934v1 Announce Type: new 
Abstract: Statistical computations are becoming increasingly important. These computations often need to be performed in log-space because probabilities become extremely small due to repeated multiplications. While using logarithms effectively prevents numerical underflow, this paper shows that its cost is high in performance, resource utilization, and, notably, numerical accuracy. This paper then argues that using posit, a recently proposed floating-point format, is a better strategy for statistical computations operating on extremely small numbers because of its unique encoding mechanism. To that end, this paper performs a comprehensive analysis comparing posit, binary64, and logarithm representations, examining both individual arithmetic operations, statistical bioinformatics applications, and their accelerators. FPGA implementation results highlight that posit-based accelerators can achieve up to two orders of magnitude higher accuracy, up to 60\% lower resource utilization, and up to $1.3\times$ speedup, compared to log-space accelerators. Such improvement translates to $2\times$ performance per unit resource on the FPGA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10934v1</guid>
      <category>math.NA</category>
      <category>cs.AR</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tiancheng Xu, Alan L. Cox, Scott Rixner</dc:creator>
    </item>
    <item>
      <title>Development and Analysis of Chien-Physics-Informed Neural Networks for Singular Perturbation Problems</title>
      <link>https://arxiv.org/abs/2509.10945</link>
      <description>arXiv:2509.10945v1 Announce Type: new 
Abstract: In this article, we employ Chien-Physics Informed Neural Networks (C-PINNs) to obtain solutions for singularly perturbed convection-diffusion equations, reaction-diffusion equations, and their coupled forms in both one and two-dimensional settings. While PINNs have emerged as a powerful tool for solving various types of differential equations, their application to singular perturbation problems (SPPs) presents significant challenges. These challenges arise because a small perturbation parameter multiplies the highest-order derivatives, leading to sharp gradient changes near the boundary layer. To overcome these difficulties, we apply C-PINNs, a modified version of the standard PINNs framework, which is specifically designed to address singular perturbation problems. Our study shows that C-PINNs provide a more accurate solution for SPPs, demonstrating better performance than conventional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10945v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gautam Singh, Sofia Haider</dc:creator>
    </item>
    <item>
      <title>Vectorized 3D mesh refinement and implementation of primal hybrid FEM in MATLAB</title>
      <link>https://arxiv.org/abs/2509.11133</link>
      <description>arXiv:2509.11133v1 Announce Type: new 
Abstract: In this article, we introduce a Face-to-Tetrahedron connectivity in MATLAB together with a vectorized 3D uniform mesh refinement technique. We introduce a MATLAB vectorized assembly of 3D lowest-order primal hybrid finite element matrices for a second-order elliptic problem. We introduce a parallel solver and a vectorized Schur complement solver to solve the associated linear problem. The numerical results illustrate the software's runtime performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11133v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Harish Nagula Mallesham, Sharat Gaddam, Jan Valdman, Sanjib Kumar Acharya</dc:creator>
    </item>
    <item>
      <title>A time-splitting Fourier pseudospectral method for the Wigner(-Poisson)-Fokker-Planck equations</title>
      <link>https://arxiv.org/abs/2509.11153</link>
      <description>arXiv:2509.11153v1 Announce Type: new 
Abstract: In this article, we propose an efficient time-splitting Fourier pseudospectral method for the Wigner(-Poisson)-Fokker-Planck equations. The method achieves second-order accuracy in time and spectral accuracy in phase space, both of which are rigorously verified by numerical experiments. The validated scheme is then employed to study the long-time dynamics of these systems. We investigate the existence of steady states for both the Wigner-Fokker-Planck and Wigner-Poisson-Fokker-Planck equations. Notably, for the Wigner-Fokker-Planck system, our results provide numerical evidence for the existence of a steady state even when the external potential is far from harmonic. This is an important discovery, since this phenomenon has not been thoroughly established in theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11153v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Qian Yi, Limin Xu</dc:creator>
    </item>
    <item>
      <title>Improvements on uncertainty quantification with variational autoencoders</title>
      <link>https://arxiv.org/abs/2509.11174</link>
      <description>arXiv:2509.11174v1 Announce Type: new 
Abstract: Inverse problems aim to determine model parameters of a mathematical problem from given observational data. Neural networks can provide an efficient tool to solve these problems. In the context of Bayesian inverse problems, Uncertainty Quantification Variational AutoEncoders (UQ-VAE), a class of neural networks, approximate the posterior distribution mean and covariance of model parameters. This allows for both the estimation of the parameters and their uncertainty in relation to the observational data. In this work, we propose a novel loss function for training UQ-VAEs, which includes, among other modifications, the removal of a sample mean term from an already existing one. This modification improves the accuracy of UQ-VAEs, as the original theoretical result relies on the convergence of the sample mean to the expected value (a condition that, in high dimensional parameter spaces, requires a prohibitively large number of samples due to the curse of dimensionality). Avoiding the computation of the sample mean significantly reduces the training time in high dimensional parameter spaces compared to previous literature results. Under this new formulation, we establish a new theoretical result for the approximation of the posterior mean and covariance for general mathematical problems. We validate the effectiveness of UQ-VAEs through three benchmark numerical tests: a Poisson inverse problem, a non affine inverse problem and a 0D cardiocirculatory model, under the two clinical scenarios of systemic hypertension and ventricular septal defect. For the latter case, we perform forward uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11174v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andrea Tonini, Tan Bui-Thanh, Francesco Regazzoni, Luca Dede', Alfio Quarteroni</dc:creator>
    </item>
    <item>
      <title>Mechanical Proving the Symplecticity of Partitioned Runge--Kutta Methods for Determinate and Stochastic Hamiltonian Systems</title>
      <link>https://arxiv.org/abs/2509.11188</link>
      <description>arXiv:2509.11188v1 Announce Type: new 
Abstract: We propose a new method to prove the partitioned Runge--Kutta methods with symplectic conditions for determinate and stochastic Hamiltonian systems are symplectic. We utilize Gr\"obner basis technology which is the one of symbolic computation method based on computer algebra theory and geometrical mechanical proving theory. In this approach, from determinate Hamilton's equations, we get the relations of partial differentials which are regarded as polynomials of plenty variables marked indeterminates. Then, we compute the Gr\"obner basis of above polynomials, and the normal form of symplectic expression, which is as the middle expression, with respect to the Gr\"obner basis. Then, we compute the Gr\"obner basis of symplectic conditions and the normal form of the middle expression with respect to above Gr\"obner basis, and get that the normal form is zero, which complete the proof. We also develop this procedure to the stochastic Hamiltonian systems case and get similar result. In this paper, the new try provide us a new idea to prove the structure-preservation laws of another numerical methods, including the energy conservation law, the momentum conservation law and so on.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11188v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaojing Zhang</dc:creator>
    </item>
    <item>
      <title>Dynamical Low-Rank Approximations for Kalman Filtering</title>
      <link>https://arxiv.org/abs/2509.11210</link>
      <description>arXiv:2509.11210v1 Announce Type: new 
Abstract: We propose a dynamical low rank approximation of the Kalman-Bucy process (DLR-KBP), which evolves the filtering distribution of a partially continuously observed linear SDE on a small time-varying subspace at reduced computational cost. This reduction is valid in presence of small noise and when the filtering distribution concentrates around a low dimensional subspace. We further extend this approach to a DLR-ENKF process, where particles are evolved in a low dimensional time-varying subspace at reduced cost. This allows for a significantly larger ensemble size compared to standard EnKF at equivalent cost, thereby lowering the Monte Carlo error and improving filter accuracy. Theoretical properties of the DLR-KBP and DLR-ENKF are investigated, including a propagation of chaos property. Numerical experiments demonstrate the effectiveness of the technique.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11210v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabio Nobile, Thomas Trigo Trindade</dc:creator>
    </item>
    <item>
      <title>GP-CMRH: An inner product free iterative method for block two-by-two nonsymmetric linear systems</title>
      <link>https://arxiv.org/abs/2509.11272</link>
      <description>arXiv:2509.11272v1 Announce Type: new 
Abstract: We propose an inner product free iterative method called GP-CMRH for solving block two-by-two nonsymmetric linear systems. GP-CMRH relies on a new simultaneous Hessenberg process that reduces two rectangular matrices to upper Hessenberg form simultaneously, without employing inner products. Compared with GPMR [SIAM J. Matrix Anal. Appl., 44 (2023), pp. 293--311], GP-CMRH requires less computational cost per iteration and may be more suitable for high performance computing and low or mixed precision arithmetic due to its inner product free property. Our numerical experiments demonstrate that GP-CMRH and GPMR exhibit comparable convergence behavior (with GP-CMRH requiring slightly more iterations), yet GP-CMRH consumes less computational time in most cases. GP-CMRH significantly outperforms GMRES and CMRH in terms of convergence rate and runtime efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11272v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kui Du, Jia-Jun Fan</dc:creator>
    </item>
    <item>
      <title>Derivative-informed Graph Convolutional Autoencoder with Phase Classification for the Lifshitz-Petrich Model</title>
      <link>https://arxiv.org/abs/2509.11293</link>
      <description>arXiv:2509.11293v1 Announce Type: new 
Abstract: The Lifshitz-Petrich (LP) model is a classical model for describing complex spatial patterns such as quasicrystals and multiphase structures. Solving and classifying the solutions of the LP model is challenging due to the presence of high-order gradient terms and the long-range orientational order characteristic of the quasicrystals. To address these challenges, we propose a Derivative-informed Graph Convolutional Autoencoder (DiGCA) to classify the multi-component multi-state solutions of the LP model. The classifier consists of two stages. In the offline stage, the DiGCA phase classifier innovatively incorporates both solutions and their derivatives for training a graph convolutional autoencoder which effectively captures intricate spatial dependencies while significantly reducing the dimensionality of the solution space. In the online phase, the framework employs a neural network classifier to efficiently categorize encoded solutions into distinct phase diagrams. The numerical results demonstrate that the DiGCA phase classifier accurately solves the LP model, classifies its solutions, and rapidly generates detailed phase diagrams in a robust manner, offering significant improvements in both efficiency and accuracy over traditional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11293v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanlai Chen, Yajie Ji, Zhenli Xu</dc:creator>
    </item>
    <item>
      <title>IGA-LBM: Isogeometric lattice Boltzmann method</title>
      <link>https://arxiv.org/abs/2509.11427</link>
      <description>arXiv:2509.11427v1 Announce Type: new 
Abstract: The lattice Boltzmann method has become a widely adopted approach in computational fluid dynamics, offering unique advantages in mesoscopic kinetic modeling, intrinsic parallelism, and simple treatment of boundary conditions. However, its conventional reliance on Cartesian grids fundamentally limits geometric fidelity in flows involving curved boundaries, introducing stair-step artifacts that propagate as spurious forces and boundary-layer inaccuracies.
  To address these challenges, we propose the isogeometric lattice Boltzmann method, which seamlessly integrates Isogeometric Analysis with LBM, leveraging the geometric precision of non-uniform rational B-Splines to construct body-fitted computational grids. Unlike conventional Cartesian-based LBM, the proposed approach eliminates stair-step boundary artifacts by providing sub-element geometric accuracy while maintaining the efficiency of LBM. Furthermore, the higher-order continuity of NURBS improves gradient resolution, reducing numerical diffusion in high-Reynold's-number flows. The parametric grid adaptation of IGA enables $h$-, $p$-, and $k$-refinement strategies, allowing for localized resolution enhancement in boundary layers and regions with high solution gradients. Additionally, the diffeomorphic mapping properties of IGA ensure intrinsic conservation, preserving advection invariants and suppressing numerical oscillations, leading to enhanced stability.
  Benchmark simulations on flows with curved and complex geometries demonstrate that IGA-LBM delivers significantly more accurate boundary-layer predictions and pressure/force estimates than standard Cartesian LBM, while preserving its computational efficiency and scalability. By combining geometric exactness with the algorithmic simplicity of LBM, IGA-LBM offers a practical route to high-fidelity simulations in engineering and scientific applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11427v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.flu-dyn</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ye Ji, Monica Lacatus, Matthias M\"oller</dc:creator>
    </item>
    <item>
      <title>Unified analysis of saddle point problems via auxiliary space theory</title>
      <link>https://arxiv.org/abs/2509.11434</link>
      <description>arXiv:2509.11434v1 Announce Type: new 
Abstract: We present sharp estimates for the extremal eigenvalues of the Schur complements arising in saddle point problems. These estimates are derived using the auxiliary space theory, in which a given iterative method is interpreted as an equivalent but more elementary iterative method on an auxiliary space, enabling us to obtain sharp convergence estimates. The proposed framework improves or refines several existing results, which can be recovered as corollaries of our results. To demonstrate the versatility of the framework, we present various applications from scientific computing: the augmented Lagrangian method, mixed finite element methods, and nonoverlapping domain decomposition methods. In all these applications, the condition numbers of the corresponding Schur complements can be estimated in a straightforward manner using the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11434v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jongho Park</dc:creator>
    </item>
    <item>
      <title>Convergence of a Second-Order Projection Method to Leray-Hopf Solutions of the Incompressible Navier-Stokes Equations</title>
      <link>https://arxiv.org/abs/2509.11483</link>
      <description>arXiv:2509.11483v1 Announce Type: new 
Abstract: We analyze a second-order projection method for the incompressible Navier-Stokes equations on bounded Lipschitz domains. The scheme employs a Backward Differentiation Formula of order two (BDF2) for the time discretization, combined with conforming finite elements in space. Projection methods are widely used to enforce incompressibility, yet rigorous convergence results for possibly non-smooth solutions have so far been restricted to first-order schemes. We establish, for the first time, convergence (up to subsequence) of a second-order projection method to Leray-Hopf weak solutions under minimal assumptions on the data, namely $u_0 \in L^2_{\text{div}}(\Omega)$ and $f \in L^2(0,T;L^2_{\text{div}}(\Omega))$. Our analysis relies on two ingredients: A discrete energy inequality providing uniform $L^{\infty}(0,T;L^2(\Omega))$ and $L^2(0,T;H^1_0(\Omega))$ bounds for suitable interpolants of the discrete velocities, and a compactness argument combining Simon's theorem with refined time-continuity estimates. These tools overcome the difficulty that only the projected velocity satisfies an approximate divergence-free condition, while the intermediate velocity is controlled in space. We conclude that a subsequence of the approximations converges to a Leray-Hopf weak solution. This result provides the first rigorous convergence proof for a higher-order projection method under no additional assumptions on the solution beyond those following from the standard a priori energy estimate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11483v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Franziska Weber</dc:creator>
    </item>
    <item>
      <title>Neural solver for sixth-order ordinary differential equations</title>
      <link>https://arxiv.org/abs/2509.11541</link>
      <description>arXiv:2509.11541v1 Announce Type: new 
Abstract: A method for approximating sixth-order ordinary differential equations is proposed, which utilizes a deep learning feedforward artificial neural network, referred to as a neural solver. The efficacy of this unsupervised machine learning method is demonstrated through the solution of two distinct boundary value problems (BVPs), with the method being extended to include the solution of a sixth-order ordinary differential equation (ODE). The proposed mean squared loss function is comprised of two terms: the differential equation is satisfied by the first term, while the initial or boundary conditions are satisfied by the second. The total loss function is minimized using a quasi-Newton optimization method to obtain the desired network output. The approximation capability of the proposed method is verified for sixth-order ODEs. Point-wise comparisons of the approximations show strong agreement with available exact solutions. The proposed algorithm minimizes the overall learnable network hyperparameters in a given BVP. Simple minimization of the total loss function yields highly accurate results even with a low number of epochs. Therefore, the proposed framework offers an attractive setting for the computational mathematics community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11541v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Janavi Bhalala (Faculty Advsior), B. Veena S. N. Rao (Faculty Advsior)</dc:creator>
    </item>
    <item>
      <title>Learning Singularity-Encoded Green's Functions with Application to Iterative Methods</title>
      <link>https://arxiv.org/abs/2509.11580</link>
      <description>arXiv:2509.11580v1 Announce Type: new 
Abstract: Green's function provides an inherent connection between theoretical analysis and numerical methods for elliptic partial differential equations, and general absence of its closed-form expression necessitates surrogate modeling to guide the design of effective solvers. Unfortunately, numerical computation of Green's function remains challenging due to its doubled dimensionality and intrinsic singularity. In this paper, we present a novel singularity-encoded learning approach to resolve these problems in an unsupervised fashion. Our method embeds the Green's function within a one-order higher-dimensional space by encoding its prior estimate as an augmented variable, followed by a neural network parametrization to manage the increased dimensionality. By projecting the trained neural network solution back onto the original domain, our deep surrogate model exploits its spectral bias to accelerate conventional iterative schemes, serving either as a preconditioner or as part of a hybrid solver. The effectiveness of our proposed method is empirically verified through numerical experiments with two and four dimensional Green's functions, achieving satisfactory resolution of singularities and acceleration of iterative solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11580v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Qi Sun, Shengyan Li, Bowen Zheng, Lili Ju, Xuejun Xu</dc:creator>
    </item>
    <item>
      <title>Strong convergence rates of stochastic theta methods for index 1 stochastic differential algebraic equations under non-globally Lipschitz conditions</title>
      <link>https://arxiv.org/abs/2509.11618</link>
      <description>arXiv:2509.11618v1 Announce Type: new 
Abstract: This work investigates numerical approximations of index 1 stochastic differential algebraic equations (SDAEs) with non-constant singular matrices under non-global Lipschitz conditions. Analyzing the strong convergence rates of numerical solutions in this setting is highly nontrivial, due to both the singularity of the constraint matrix and the superlinear growth of the coefficients. To address these challenges, we develop an approach for establishing mean square convergence rates of numerical methods for SDAEs under global monotonicity conditions. Specifically, we prove that each stochastic theta method with $\theta \in [\frac{1}{2},1]$ achieves a mean square convergence rate of order $\frac{1}{2}$. Theoretical findings are further validated through a series of numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11618v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Chen, Ziheng Chen, Jing Zhao</dc:creator>
    </item>
    <item>
      <title>Linear and Nonlinear Boundary Conditions: What's the difference?</title>
      <link>https://arxiv.org/abs/2509.11651</link>
      <description>arXiv:2509.11651v1 Announce Type: new 
Abstract: In previous work, we derived new energy and entropy stable open boundary conditions and implementation procedures for linear and nonlinear initial boundary value problems. These boundary procedures results in estimates bounded by external data only. Interestingly, these new boundary conditions generalize the well known classical characteristic boundary conditions for linear problems to the nonlinear setting. We discuss the similarities and differences between these two boundary procedures and point out the advantages with the new procedures. In particular we show that the new boundary conditions bound both linear and nonlinear initial boundary value problems and can be implemented both strongly and weakly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11651v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Nordstr\"om</dc:creator>
    </item>
    <item>
      <title>Numerical Approximation of the logarithmic Laplacian via sinc-basis</title>
      <link>https://arxiv.org/abs/2509.11693</link>
      <description>arXiv:2509.11693v1 Announce Type: new 
Abstract: In recent works, the authors of this chapter have shown with co-authors how a basis consisting of dilated and shifted $\text{sinc}$-functions can be used to solve fractional partial differential equations. As a model problem, the fractional Dirichlet problem with homogeneous exterior value conditions was solved. In this work, we briefly recap the algorithms developed there and that -- from a computational point of view -- they can be used to solve nonlocal equations given through different operators as well. As an example, we numerically solve the Dirichlet problem for the logarithmic Laplacian $\log(-\Delta)$ which has the Fourier symbol $\log(\left|\omega\right|^2)$ and compute its Eigenvalues on disks with different radii in $\mathbb R^2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11693v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Dondl, Ludwig Striet</dc:creator>
    </item>
    <item>
      <title>A Chebyshev--Ritz Spectral Framework for Nonlinear Vibration of CNT-Reinforced Composite Beams</title>
      <link>https://arxiv.org/abs/2509.11946</link>
      <description>arXiv:2509.11946v1 Announce Type: new 
Abstract: This study develops a spectral Ritz formulation for the nonlinear free vibration analysis of carbon nanotube-reinforced composite (CNTRC) beams. Boundary-adapted Chebyshev basis functions are constructed to exactly satisfy clamped and simply supported boundary conditions. The governing equations incorporate von~K\'{a}rm\'{a}n geometric nonlinearity, while the effective material properties for both uniform and functionally graded (FG) CNT distributions are evaluated using a modified rule of mixtures. Discretization via the Chebyshev-Ritz approach produces a reduced-order model exhibiting exponential convergence; for basis sizes $N \geq 12$, the fundamental frequency error remains below $0.1\%$ relative to published benchmarks.
  Computational results demonstrate substantial efficiency gains, with the spectral approach requiring significantly less time than high-fidelity finite element discretizations of comparable accuracy. Parametric studies reveal that the fundamental frequency increases with CNT volume fraction and is sensitive to the interfacial load-transfer efficiency parameter $\eta_E$. Selected FG patterns are shown to enhance stiffness relative to uniformly distributed CNTs.
  Validation against established numerical benchmarks yields relative differences of only a few percent. The current limitation of the method is its reliance on the Euler-Bernoulli beam assumption, which neglects transverse shear deformation and damping; addressing these effects is proposed for future work. All numerical data and scripts are provided as supplementary material to ensure reproducibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11946v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maryam Jalili, Samad Noeiaghdam</dc:creator>
    </item>
    <item>
      <title>X-ray imaging from nonlinear waves: numerical reconstruction of a cubic nonlinearity</title>
      <link>https://arxiv.org/abs/2509.11951</link>
      <description>arXiv:2509.11951v1 Announce Type: new 
Abstract: We study an inverse boundary value problem for the nonlinear wave equation in $2 + 1$ dimensions. The objective is to recover an unknown potential $q(x, t)$ from the associated Dirichlet-to-Neumann map using real-valued waves. We propose a direct numerical reconstruction method for the Radon transform of $q$, which can then be inverted using standard X-ray tomography techniques to determine $q$. Our implementation introduces a spectral regularization procedure to stabilize the numerical differentiation step required in the reconstruction, improving robustness with respect to noise in the boundary data. We also give rigorous justification and stability estimates for the regularized spectral differentiation of noisy measurements. A direct pointwise reconstruction method for $q$ is also implemented for comparison. Numerical experiments demonstrate the feasibility of recovering potentials from boundary measurements of nonlinear waves and illustrate the advantages of the Radon-based reconstruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11951v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Markus Harju, Suvi Takalahti, Teemu Tyni</dc:creator>
    </item>
    <item>
      <title>Adaptive least-squares space-time finite element methods for convection-diffusion problems</title>
      <link>https://arxiv.org/abs/2509.11955</link>
      <description>arXiv:2509.11955v1 Announce Type: new 
Abstract: In this paper we formulate and analyse adaptive (space-time) least-squares finite element methods for the solution of convection-diffusion equations. The convective derivative $\mathbf{v} \cdot \nabla u$ is considered as part of the total time derivative $\frac{d}{dt}u = \partial_t u + \mathbf{v} \cdot \nabla u$, and therefore we can use a rather standard stability and error analysis for related space-time finite element methods. For stationary problems we restrict the ansatz space $H^1_0(\Omega)$ such that the convective derivative is considered as an element of the dual $H^{-1}(\Omega)$ of the test space $H^1_0(\Omega)$, which also allows unbounded velocities $\mathbf{v}$. While the discrete finite element schemes are always unique solvable, the numerical solutions may suffer from a bad approximation property of the finite element space when considering convection dominated problems, i.e., small diffusion coefficients. Instead of adding suitable stabilization terms, we aim to resolve the solutions by using adaptive (space-time) finite element methods. For this we introduce a least-squares approach where the discrete adjoint defines local a posteriori error indicators to drive an adaptive scheme. Numerical examples illustrate the theoretical considerations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11955v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian K\"othe, Olaf Steinbach</dc:creator>
    </item>
    <item>
      <title>RJD-BASE: Multi-Modal Spectral Clustering via Randomized Joint Diagonalization</title>
      <link>https://arxiv.org/abs/2509.11981</link>
      <description>arXiv:2509.11981v1 Announce Type: new 
Abstract: We revisit the problem of spectral clustering in multimodal settings, where each data modality is encoded as a graph Laplacian. While classical approaches--including joint diagonalization, spectral co-regularization, and multiview clustering--attempt to align embeddings across modalities, they often rely on costly iterative refinement and may fail to directly target the spectral subspace relevant for clustering. In this work, we introduce two key innovations. First, we bring the power of randomization to this setting by sampling random convex combinations of Laplacians as a simple and scalable alternative to explicit eigenspace alignment. Second, we propose a principled selection rule based on Bottom-$k$ Aggregated Spectral Energy (BASE)--a $k$-dimensional extension of the directional smoothness objective from recent minimax formulations--which we uniquely apply as a selection mechanism rather than an optimization target. The result is Randomized Joint Diagonalization with BASE Selection (RJD-BASE), a method that is easily implementable, computationally efficient, aligned with the clustering objective, and grounded in decades of progress in standard eigensolvers. Through experiments on synthetic and real-world datasets, we show that RJD-BASE reliably selects high-quality embeddings, outperforming classical multimodal clustering methods at low computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11981v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>eess.SP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoze He, Artemis Pados, Daniel Kressner</dc:creator>
    </item>
    <item>
      <title>Designing MacPherson Suspension Architectures using Bayesian Optimization</title>
      <link>https://arxiv.org/abs/2206.09022</link>
      <description>arXiv:2206.09022v1 Announce Type: cross 
Abstract: Engineering design is traditionally performed by hand: an expert makes design proposals based on past experience, and these proposals are then tested for compliance with certain target specifications. Testing for compliance is performed first by computer simulation using what is called a discipline model. Such a model can be implemented by a finite element analysis, multibody systems approach, etc. Designs passing this simulation are then considered for physical prototyping. The overall process may take months, and is a significant cost in practice. We have developed a Bayesian optimization system for partially automating this process by directly optimizing compliance with the target specification with respect to the design parameters. The proposed method is a general framework for computing a generalized inverse of a high-dimensional non-linear function that does not require e.g. gradient information, which is often unavailable from discipline models. We furthermore develop a two-tier convergence criterion based on (i) convergence to a solution optimally satisfying all specified design criteria, or (ii) convergence to a minimum-norm solution. We demonstrate the proposed approach on a vehicle chassis design problem motivated by an industry setting using a state-of-the-art commercial discipline model. We show that the proposed approach is general, scalable, and efficient, and that the novel convergence criteria can be implemented straightforwardly based on existing concepts and subroutines in popular Bayesian optimization software packages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.09022v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sinnu Susan Thomas, Jacopo Palandri, Mohsen Lakehal-ayat, Punarjay Chakravarty, Friedrich Wolf-Monheim, Matthew B. Blaschko</dc:creator>
    </item>
    <item>
      <title>Kernel-based Stochastic Approximation Framework for Nonlinear Operator Learning</title>
      <link>https://arxiv.org/abs/2509.11070</link>
      <description>arXiv:2509.11070v1 Announce Type: cross 
Abstract: We develop a stochastic approximation framework for learning nonlinear operators between infinite-dimensional spaces utilizing general Mercer operator-valued kernels. Our framework encompasses two key classes: (i) compact kernels, which admit discrete spectral decompositions, and (ii) diagonal kernels of the form $K(x,x')=k(x,x')T$, where $k$ is a scalar-valued kernel and $T$ is a positive operator on the output space. This broad setting induces expressive vector-valued reproducing kernel Hilbert spaces (RKHSs) that generalize the classical $K=kI$ paradigm, thereby enabling rich structural modeling with rigorous theoretical guarantees. To address target operators lying outside the RKHS, we introduce vector-valued interpolation spaces to precisely quantify misspecification error. Within this framework, we establish dimension-free polynomial convergence rates, demonstrating that nonlinear operator learning can overcome the curse of dimensionality. The use of general operator-valued kernels further allows us to derive rates for intrinsically nonlinear operator learning, going beyond the linear-type behavior inherent in diagonal constructions of $K=kI$. Importantly, this framework accommodates a wide range of operator learning tasks, ranging from integral operators such as Fredholm operators to architectures based on encoder-decoder representations. Moreover, we validate its effectiveness through numerical experiments on the two-dimensional Navier-Stokes equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11070v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jia-Qi Yang, Lei Shi</dc:creator>
    </item>
    <item>
      <title>Large language model-empowered next-generation computer-aided engineering</title>
      <link>https://arxiv.org/abs/2509.11447</link>
      <description>arXiv:2509.11447v1 Announce Type: cross 
Abstract: Software development has entered a new era where large language models (LLMs) now serve as general-purpose reasoning engines, enabling natural language interaction and transformative applications across diverse domains. This paradigm is now extending into computer-aided engineering (CAE). Recent applications of LLMs in CAE have successfully automated routine tasks, including CAD model generation and FEM simulations. Nevertheless, these contributions, which primarily serve to reduce manual labor, are often insufficient for addressing the significant computational challenges posed by large-scale, high-dimensional systems. To this aim, we first introduce the concept of LLM-empowered CAE agent, where LLMs act as autonomous collaborators that plan, execute, and adapt CAE workflows. Then, we propose an LLM-empowered CAE agent for data-free model order reduction (MOR), a powerful yet underused approach for ultra-fast large-scale parametric analysis due to the intrusive nature and labor-intensive redevelopment of solvers. LLMs can alleviate this barrier by automating derivations, code restructuring, and implementation, making intrusive MOR both practical and broadly accessible. To demonstrate feasibility, we present an LLM-empowered CAE agent for solving ultra-large-scale space-parameter-time (S-P-T) physical problems using Tensor-decomposition-based A Priori Surrogates (TAPS). Our results show that natural language prompts describing parametric partial differential equations (PDEs) can be translated into efficient solver implementations, substantially reducing human effort while producing high-fidelity reduced-order models. Moreover, LLMs can synthesize novel MOR solvers for unseen cases such as nonlinear and high-dimensional parametric problems based on their internal knowledge base. This highlights the potential of LLMs to establish the foundation for next-generation CAE systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11447v1</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiachen Guo, Chanwook Park, Dong Qian, Thomas J. R. Hughes, Wing Kam Liu</dc:creator>
    </item>
    <item>
      <title>Numerical analysis of fluid estimation for source terms in neutral particles simulation</title>
      <link>https://arxiv.org/abs/2509.11883</link>
      <description>arXiv:2509.11883v1 Announce Type: cross 
Abstract: In plasma edge simulations, kinetic Monte Carlo (MC) is often used to simulate neutral particles and estimate source terms. For large-sized reactors, like ITER and DEMO, high particle collision rates lead to a substantial computational cost for such schemes. To address this challenge, an asymptotic-preserving kinetic-diffusion Monte Carlo (KDMC) simulation method and a corresponding fluid estimation technique have been proposed in the literature. In this work, we perform numerical analysis on the convergence of KDMC with the fluid estimation. To do so, we compare the accuracy of the analyzed algorithm with the accuracy of an approximate fluid method using the kinetic MC method as a reference. In a one-dimensional test case, KDMC with the fluid estimation achieves at least one order of magnitude lower errors than the fluid method for both high- and low-collisional regimes. Moreover, KDMC with the fluid estimation outperforms the kinetic MC method with a clear speed-up. Overall, our analysis confirms the effectiveness of the discussed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11883v1</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhirui Tang, Emil L{\o}vbak, Julian Koellermeier, Giovanni Samaey</dc:creator>
    </item>
    <item>
      <title>Contractive kinetic Langevin samplers beyond global Lipschitz continuity</title>
      <link>https://arxiv.org/abs/2509.12031</link>
      <description>arXiv:2509.12031v1 Announce Type: cross 
Abstract: In this paper, we examine the problem of sampling from log-concave distributions with (possibly) superlinear gradient growth under kinetic (underdamped) Langevin algorithms. Using a carefully tailored taming scheme, we propose two novel discretizations of the kinetic Langevin SDE, and we show that they are both contractive and satisfy a log-Sobolev inequality. Building on this, we establish a series of non-asymptotic bounds in $2$-Wasserstein distance between the law reached by each algorithm and the underlying target measure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12031v1</guid>
      <category>math.PR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iosif Lytras, Panagiotis Mertikopoulos</dc:creator>
    </item>
    <item>
      <title>Superfast Low Rank Approximation</title>
      <link>https://arxiv.org/abs/1812.11406</link>
      <description>arXiv:1812.11406v3 Announce Type: replace 
Abstract: Low rank approximation of a matrix (LRA) is a highly important area of Numerical Linear and Multilinear Algebra and Data Mining and Analysis. One can operate with an LRA superfast -- by using much fewer memory cells and flops than an input matrix has entries. Can we, however, compute an LRA of a matrix superfast? YES and NO. For worst case inputs, any LRA algorithm fails miserably unless it involves all input entries, but in computational practice worst case inputs seem to appear rarely, and accurate LRA are routinely computed superfast for large and important classes of matrices, in particular in the memory efficient form of CUR, widely used in data analysis. We advance formal study of this YES and NO coexistence by proving novel universal upper bounds on the spectral output error norms of all CUR LRA algorithms and, under a fixed probabilistic structure in the space of input matrices, on both spectral and Frobenius error norms of nearly all sketching LRA algorithms. These bounds imply that superfast LRA algorithms of the two kinds fail miserably only for a very narrow input class. Furthermore, in our numerical tests such superfast algorithms were consistently much more accurate than our upper estimates ensure and usually were reasonably close to optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:1812.11406v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soo Go, Qi Luan, Victor Y. Pan, John Svadlenka, Liang Zhao</dc:creator>
    </item>
    <item>
      <title>An iterative Jacobi-like algorithm to compute a few sparse approximate eigenvectors</title>
      <link>https://arxiv.org/abs/2105.14642</link>
      <description>arXiv:2105.14642v3 Announce Type: replace 
Abstract: In this paper, we describe a new algorithm that approximates the extreme eigenvalue/eigenvector pairs of a symmetric matrix. The proposed algorithm can be viewed as an extension of the Jacobi eigenvalue method for symmetric matrices diagonalization to the case where we want to approximate just a few extreme eigenvalues/eigenvectors. The method is also particularly well-suited for the computation of sparse approximations of the eigenvectors. In fact, we show that in general, our method provides a trade-off between the sparsity of the computed approximate eigenspaces and their accuracy. We provide theoretical results that show the linear convergence of the proposed method. Finally, we show experimental numerical results for sparse low-rank approximations of random symmetric matrices and show applications to graph Fourier transforms, and the sparse principal component analysis in image classification experiments. These applications are chosen because, in these cases, there is no need to perform the eigenvalue decomposition to high precision to achieve good numerical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.14642v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>eess.SP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cristian Rusu</dc:creator>
    </item>
    <item>
      <title>Operator learning for hyperbolic partial differential equations</title>
      <link>https://arxiv.org/abs/2312.17489</link>
      <description>arXiv:2312.17489v2 Announce Type: replace 
Abstract: We construct the first rigorously justified probabilistic algorithm for recovering the solution operator of a hyperbolic partial differential equation (PDE) in two variables from input-output training pairs. The primary challenge of recovering the solution operator of hyperbolic PDEs is the presence of characteristics, along which the associated Green's function is discontinuous. Therefore, a central component of our algorithm is a rank detection scheme that identifies the approximate location of the characteristics. By combining the randomized singular value decomposition with an adaptive hierarchical partition of the domain, we construct an approximant to the solution operator using $O(\Psi_\epsilon^{-1}\epsilon^{-7}\log(\Xi_\epsilon^{-1}\epsilon^{-1}))$ input-output pairs with relative error $O(\Xi_\epsilon^{-1}\epsilon)$ in the operator norm as $\epsilon\to0$, with high probability. Here, $\Psi_\epsilon$ represents the existence of degenerate singular values of the solution operator, and $\Xi_\epsilon$ measures the quality of the training data. Our assumptions on the regularity of the coefficients of the hyperbolic PDE are relatively weak given that hyperbolic PDEs do not have the ``instantaneous smoothing effect'' of elliptic and parabolic PDEs, and our recovery rate improves as the regularity of the coefficients increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17489v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher Wang, Alex Townsend</dc:creator>
    </item>
    <item>
      <title>Development of discontinuous Galerkin methods for hyperbolic systems that preserve a curl or a divergence constraint: the case of linear systems</title>
      <link>https://arxiv.org/abs/2405.04347</link>
      <description>arXiv:2405.04347v3 Announce Type: replace 
Abstract: Some hyperbolic systems are known to include implicit preservation of differential constraints: these are for example the time conservation of the curl or the divergence of a vector that appear as an implicit constraint. In this article, we show that this kind of constraint can be easily conserved at the discrete level with the classical discontinuous Galerkin method, provided the right approximation space is used for the vectorial space, and under some mild assumption on the numerical flux. For this, we recall a discrete de-Rham framework in which discontinuous approximation spaces for vectors fits. The discrete adjoint divergence and curl are proven to be exactly preserved by the discontinuous Galerkin method under a small assumption on the numerical flux. Numerical tests are performed on the wave system, the two dimensional Maxwell system and the induction equation, and confirm that the differential constraints are preserved at machine precision while keeping the high order of accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04347v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Perrier (CAGIRE, LMAP)</dc:creator>
    </item>
    <item>
      <title>Analysis of the SQP Method for Hyperbolic PDE-Constrained Optimization in Acoustic Full Waveform Inversion</title>
      <link>https://arxiv.org/abs/2405.05158</link>
      <description>arXiv:2405.05158v3 Announce Type: replace 
Abstract: In this paper, the SQP method applied to a hyperbolic PDE-constrained optimization problem is considered. The model arises from the acoustic full waveform inversion in the time domain. The analysis is mainly challenging due to the involved hyperbolicity and second-order bilinear structure. This notorious character leads to an undesired effect of loss of regularity in the SQP method, calling for a substantial extension of developed parabolic techniques. We propose and analyze a novel strategy for the well-posedness and convergence analysis based on the use of a smooth-in-time initial condition, a tailored self-mapping operator, and a two-step estimation process along with Stampacchia's method for second-order wave equations. Our final theoretical result is the R-superlinear convergence of the SQP method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05158v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luis Ammann, Irwin Yousept</dc:creator>
    </item>
    <item>
      <title>Randomized LU-Householder CholeskyQR</title>
      <link>https://arxiv.org/abs/2412.06551</link>
      <description>arXiv:2412.06551v4 Announce Type: replace 
Abstract: In this work, we develop randomized LU-Householder CholeskyQR (rLHC) for QR factorization of the tall-skinny matrices, consisting of SLHC3 with single-sketching and SSLHC3 with multi-sketching. Similar to LU-CholeskyQR2 (LUC2), they do not require a condition of $\kappa_{2}(X)$ for the input matrix $X \in \mathbb{R}^{m\times n}$, which ensures the applicability of the algorithms and is distinguished from many CholeskyQR-type algorithms. To address the issue of numerical breakdown of LUC2 when the $L$-factor from LU factorization is ill-conditioned, we employ HouseholderQR to generate the upper-triangular factor alternatively together the latest matrix sketching to guarantee the efficiency. We provide rounding error analysis of our new algorithms and show their numerical stability. Numerical experiments demonstrate the better applicability of SLHC3 and SSLHC3 compared to the existing algorithms while maintaining good accuracy and robustness. Regarding efficiency, SLHC3 and SSLHC3 are at the similar level as that of LUC2. When $m=\mathcal{O}(n^{2})$ for $X$, SSLHC3 with multi-sketching is more efficient than SLHC3 with single-sketching. SLHC3 and SSLHC3 balance the applicability, efficiency, accuracy and robustness, becoming outstanding algorithms for QR factorization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06551v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoran Guan, Yuwei Fan</dc:creator>
    </item>
    <item>
      <title>MCMC-Net: Accelerating Markov Chain Monte Carlo with Neural Networks for Inverse Problems</title>
      <link>https://arxiv.org/abs/2412.16883</link>
      <description>arXiv:2412.16883v2 Announce Type: replace 
Abstract: In many computational problems, using the Markov Chain Monte Carlo (MCMC) can be prohibitively time-consuming. We propose MCMC-Net, a simple yet efficient way to accelerate MCMC via neural networks. The key idea of our approach is to substitute the true likelihood function of the MCMC method with a neural operator based surrogate. We extensively evaluate the accuracy and speedup of our method on three different PDE-based inverse problems where likelihood computations are computationally expensive, namely electrical impedance tomography, diffuse optical tomography, and quantitative photoacoustic tomography.
  MCMC-Net performs similar to the classical likelihood counterpart but with a significant speedup. We conjecture that the method can be applied to any problem with a sufficiently expensive likelihood function. We also analyze MCMC-Net in a theoretical setting for the different use cases. We prove a universal approximation theorem-type result to show that the proposed network can approximate the mapping resulting from forward model evaluations to a desired accuracy. Furthermore, we establish convergence of the surrogate posterior to the true posterior under Hellinger distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16883v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1088/1361-6420/ae05c2</arxiv:DOI>
      <dc:creator>Sudeb Majee, Anuj Abhishek, Thilo Strauss, Taufiquar Khan</dc:creator>
    </item>
    <item>
      <title>A fast Fourier spectral method for the linearized Boltzmann collision operator</title>
      <link>https://arxiv.org/abs/2503.09580</link>
      <description>arXiv:2503.09580v2 Announce Type: replace 
Abstract: We introduce a fast Fourier spectral method to compute linearized collision operators of the Boltzmann equation for variable hard-sphere gases. While the state-of-the-art method provides a computational cost O(MN^4 log N), with N being the number of modes in each direction and M being the number of quadrature points on a hemisphere, our method reduces the cost to O(N^4 log N), removing the factor M, which could be large in our numerical tests. The method is applied in a numerical solver for the steady-state Boltzmann equation with quadratic collision operators. Numerical experiments for both spatially homogeneous and inhomogeneous Boltzmann equations have been carried out to test the accuracy and efficiency of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09580v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianai Yin, Zhenning Cai, Yanli Wang</dc:creator>
    </item>
    <item>
      <title>A Stiff Order Condition Theory for Runge-Kutta Methods Applied to Semilinear ODEs</title>
      <link>https://arxiv.org/abs/2505.15099</link>
      <description>arXiv:2505.15099v2 Announce Type: replace 
Abstract: Classical convergence theory of Runge-Kutta methods assumes that the time step is small relative to the Lipschitz constant of the ordinary differential equation (ODE). For stiff problems, that assumption is often violated, and a problematic degradation in accuracy, known as order reduction, can arise. Methods with high stage order, e.g., Gauss-Legendre and Radau, are known to avoid order reduction, but they must be fully implicit. For the broad class of semilinear ODEs, which consist of a stiff linear term and non-stiff nonlinear term, we show that weaker conditions suffice. Our new semilinear order conditions are formulated in terms of orthogonality relations and can be enumerated by rooted trees. Finally, we prove global error bounds that hold uniformly with respect to stiffness of the linear term.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15099v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Steven B. Roberts, David Shirokoff, Abhijit Biswas, Benjamin Seibold</dc:creator>
    </item>
    <item>
      <title>Improved error bounds for Koopman operator and reconstructed trajectories approximations with kernel-based methods</title>
      <link>https://arxiv.org/abs/2506.09266</link>
      <description>arXiv:2506.09266v3 Announce Type: replace 
Abstract: In this article, we propose a new error bound for Koopman operator approximation using Kernel Extended Dynamic Mode Decomposition. The new estimate is $O(N^{-1/2})$, with a constant related to the probability of success of the bound, given by Hoeffding's inequality, similar to other methodologies, such as Philipp et al. Furthermore, we propose a \textit{lifting back} operator to obtain trajectories generated by embedding the initial state and iterating a linear system in a higher dimension. This naturally yields an $O(N^{-1/2})$ error bound for mean trajectories. Finally, we show numerical results including an example of nonlinear system, exhibiting successful approximation with exponential decay faster than $-1/2$, as suggested by the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09266v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diego Olgu\'in, Axel Osses, H\'ector Ram\'irez</dc:creator>
    </item>
    <item>
      <title>Robust and fast iterative method for the elliptic Monge-Amp\`ere equation</title>
      <link>https://arxiv.org/abs/2509.00794</link>
      <description>arXiv:2509.00794v2 Announce Type: replace 
Abstract: This paper introduces a fast and robust iterative scheme for the elliptic Monge-Amp\`ere equation with Dirichlet boundary conditions. The Monge-Amp\`ere equation is a nonlinear and degenerate equation, with applications in optimal transport, geometric optics, and differential geometry. The proposed method linearises the equation and uses a fixed-point iteration (L-scheme), solving a Poisson problem in each step with a weighted residual as the right-hand side. This algorithm is robust against discretisation, nonlinearities, and degeneracies. For a weight greater than the largest eigenvalue of the Hessian, contraction in $H^2$ and $L^\infty$ is proven for both classical and generalised solutions, respectively. The method's performance can be enhanced by using preconditioners or Green's functions. Test cases demonstrate that the scheme outperforms Newton's method in speed and stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00794v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R. N. K\"ohle, K. T. W. Menting, K. Mitra, J. H. M. ten Thije Boonkkamp</dc:creator>
    </item>
    <item>
      <title>On the Lebesgue Constant of Extended-Domain Spectral Methods for Elliptic PDEs</title>
      <link>https://arxiv.org/abs/2509.08745</link>
      <description>arXiv:2509.08745v2 Announce Type: replace 
Abstract: The extended-domain method is a simple and effective strategy for applying spectral methods to complex geometries. Its stability, however, is complicated by the use of a Fourier extension basis, which is known to be exponentially ill-conditioned. Numerical experiments reveal a discrepancy: the method appears stable for the non-self-adjoint convection-diffusion equation but unstable for the self-adjoint Poisson equation. This paper provides a rigorous analysis that resolves this issue. We first prove that the method is asymptotically unstable for both operators; the Lebesgue constant is shown to grow exponentially as a direct consequence of the frame redundancy of the basis. We then explain the observed discrepancy as a pre-asymptotic phenomenon. We prove a structural dichotomy in the discrete Green's functions (the inverses of the physical-space operators): the inverse of the convection-diffusion operator is numerically quasi-sparse, with entries that decay exponentially off-diagonal, in stark contrast to the numerically dense inverse of the Poisson operator. This framework provides a rigorous theoretical foundation that reconciles the method's practical utility in certain regimes with its ultimate asymptotic instability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08745v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Po-Yi Wu</dc:creator>
    </item>
    <item>
      <title>Expressive Power of Deep Networks on Manifolds: Simultaneous Approximation</title>
      <link>https://arxiv.org/abs/2509.09362</link>
      <description>arXiv:2509.09362v2 Announce Type: replace 
Abstract: A key challenge in scientific machine learning is solving partial differential equations (PDEs) on complex domains, where the curved geometry complicates the approximation of functions and their derivatives required by differential operators. This paper establishes the first simultaneous approximation theory for deep neural networks on manifolds. We prove that a constant-depth $\mathrm{ReLU}^{k-1}$ network with bounded weights--a property that plays a crucial role in controlling generalization error--can approximate any function in the Sobolev space $\mathcal{W}_p^{k}(\mathcal{M}^d)$ to an error of $\varepsilon$ in the $\mathcal{W}_p^{s}(\mathcal{M}^d)$ norm, for $k\geq 3$ and $s&lt;k$, using $\mathcal{O}(\varepsilon^{-d/(k-s)})$ nonzero parameters, a rate that overcomes the curse of dimensionality by depending only on the intrinsic dimension $d$. These results readily extend to functions in H\"older-Zygmund spaces. We complement this result with a matching lower bound, proving our construction is nearly optimal by showing the required number of parameters matches up to a logarithmic factor. Our proof of the lower bound introduces novel estimates for the Vapnik-Chervonenkis dimension and pseudo-dimension of the network's high-order derivative classes. These complexity bounds provide a theoretical cornerstone for learning PDEs on manifolds involving derivatives. Our analysis reveals that the network architecture leverages a sparse structure to efficiently exploit the manifold's low-dimensional geometry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09362v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>stat.ML</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanfei Zhou, Lei Shi</dc:creator>
    </item>
    <item>
      <title>Structure-Preserving High-Order Methods for the Compressible Euler Equations in Potential Temperature Formulation for Atmospheric Flows</title>
      <link>https://arxiv.org/abs/2509.10311</link>
      <description>arXiv:2509.10311v2 Announce Type: replace 
Abstract: We develop structure-preserving numerical methods for the compressible Euler equations, employing potential temperature as a prognostic variable. We construct three numerical fluxes designed to ensure the conservation of entropy and total energy within the discontinuous Galerkin framework on general curvilinear meshes. Furthermore, we introduce a generalization for the kinetic energy preservation property and total energy conservation in the presence of a gravitational potential term. To this end, we adopt a flux-differencing approach for the discretization of the source term, treated as non-conservative product. We present well-balanced schemes for different constant background states for both formulations (total energy and potential temperature) on curvilinear meshes. Finally, we validate the methods by comparing the potential temperature formulation with the traditional Euler equations formulation across a range of classical atmospheric scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10311v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.ao-ph</category>
      <category>physics.comp-ph</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Artiano, Oswald Knoth, Peter Spichtinger, Hendrik Ranocha</dc:creator>
    </item>
    <item>
      <title>Continuum Attention for Neural Operators</title>
      <link>https://arxiv.org/abs/2406.06486</link>
      <description>arXiv:2406.06486v3 Announce Type: replace-cross 
Abstract: Transformers, and the attention mechanism in particular, have become ubiquitous in machine learning. Their success in modeling nonlocal, long-range correlations has led to their widespread adoption in natural language processing, computer vision, and time series problems. Neural operators, which map spaces of functions into spaces of functions, are necessarily both nonlinear and nonlocal if they are universal; it is thus natural to ask whether the attention mechanism can be used in the design of neural operators. Motivated by this, we study transformers in the function space setting. We formulate attention as a map between infinite dimensional function spaces and prove that the attention mechanism as implemented in practice is a Monte Carlo or finite difference approximation of this operator. The function space formulation allows for the design of transformer neural operators, a class of architectures designed to learn mappings between function spaces. In this paper, we state and prove the first universal approximation result for transformer neural operators, using only a slight modification of the architecture implemented in practice. The prohibitive cost of applying the attention operator to functions defined on multi-dimensional domains leads to the need for more efficient attention-based architectures. For this reason we also introduce a function space generalization of the patching strategy from computer vision, and introduce a class of associated neural operators. Numerical results, on an array of operator learning problems, demonstrate the promise of our approaches to function space formulations of attention and their use in neural operators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06486v3</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edoardo Calvello, Nikola B. Kovachki, Matthew E. Levine, Andrew M. Stuart</dc:creator>
    </item>
    <item>
      <title>Raking mortality rates across cause, population group and geography with uncertainty quantification</title>
      <link>https://arxiv.org/abs/2407.20520</link>
      <description>arXiv:2407.20520v4 Announce Type: replace-cross 
Abstract: The Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) is the single largest and most detailed scientific effort ever conducted to quantify levels and trends in health. This global health model to estimate mortality rates and other health metrics is run at different scales, leading to large data sets of results for a global region and its different sub-regions, or for a cause of death and different sub-causes for example. These models do not necessarily lead to consistent data tables where, for instance, the sum of the number of deaths for each of the sub-regions is equal to the number of deaths for the global region. Raking is widely used in survey inference and global health models to adjust the observations in contingency tables to given marginals, in the latter case reconciling estimates between models with different granularities. The results of global health models usually associate to the point estimates an uncertainty, such as standard deviations or confidence intervals. In this paper, we propose an uncertainty propagation approach that obtains, at the cost of a single solve, nearly the same uncertainty estimates as computationally intensive Monte Carlo techniques that pass thousands of observed and marginal samples through the entire raking process. We introduce a convex optimization approach that provides a unified framework to raking extensions such as uncertainty propagation, raking with differential weights, raking with different loss functions in order to ensure that bounds on estimates are respected, verifying the feasibility of the constraints, raking to margins either as hard constraints or as aggregate observations, and handling missing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20520v4</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ariane Ducellier (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Alexander Hsu (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Parkes Kendrick (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Bill Gustafson (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Laura Dwyer-Lindgren (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Christopher Murray (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Peng Zheng (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Aleksandr Aravkin (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA)</dc:creator>
    </item>
    <item>
      <title>Intrinsic Dimension Estimating Autoencoder (IDEA) Using CancelOut Layer and a Projected Loss</title>
      <link>https://arxiv.org/abs/2509.10011</link>
      <description>arXiv:2509.10011v2 Announce Type: replace-cross 
Abstract: This paper introduces the Intrinsic Dimension Estimating Autoencoder (IDEA), which identifies the underlying intrinsic dimension of a wide range of datasets whose samples lie on either linear or nonlinear manifolds. Beyond estimating the intrinsic dimension, IDEA is also able to reconstruct the original dataset after projecting it onto the corresponding latent space, which is structured using re-weighted double CancelOut layers. Our key contribution is the introduction of the projected reconstruction loss term, guiding the training of the model by continuously assessing the reconstruction quality under the removal of an additional latent dimension. We first assess the performance of IDEA on a series of theoretical benchmarks to validate its robustness. These experiments allow us to test its reconstruction ability and compare its performance with state-of-the-art intrinsic dimension estimators. The benchmarks show good accuracy and high versatility of our approach. Subsequently, we apply our model to data generated from the numerical solution of a vertically resolved one-dimensional free-surface flow, following a pointwise discretization of the vertical velocity profile in the horizontal direction, vertical direction, and time. IDEA succeeds in estimating the dataset's intrinsic dimension and then reconstructs the original solution by working directly within the projection space identified by the network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10011v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine Oriou, Philipp Krah, Julian Koellermeier</dc:creator>
    </item>
  </channel>
</rss>
