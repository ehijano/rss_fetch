<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SC</link>
    <description>cs.SC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Oct 2025 11:57:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>HYPERDOA: Robust and Efficient DoA Estimation using Hyperdimensional Computing</title>
      <link>https://arxiv.org/abs/2510.10718</link>
      <description>arXiv:2510.10718v1 Announce Type: cross 
Abstract: Direction of Arrival (DoA) estimation techniques face a critical trade-off, as classical methods often lack accuracy in challenging, low signal-to-noise ratio (SNR) conditions, while modern deep learning approaches are too energy-intensive and opaque for resource-constrained, safety-critical systems. We introduce HYPERDOA, a novel estimator leveraging Hyperdimensional Computing (HDC). The framework introduces two distinct feature extraction strategies -- Mean Spatial-Lag Autocorrelation and Spatial Smoothing -- for its HDC pipeline, and then reframes DoA estimation as a pattern recognition problem. This approach leverages HDC's inherent robustness to noise and its transparent algebraic operations to bypass the expensive matrix decompositions and ``black-box'' nature of classical and deep learning methods, respectively. Our evaluation demonstrates that HYPERDOA achieves ~35.39% higher accuracy than state-of-the-art methods in low-SNR, coherent-source scenarios. Crucially, it also consumes ~93% less energy than competing neural baselines on an embedded NVIDIA Jetson Xavier NX platform. This dual advantage in accuracy and efficiency establishes HYPERDOA as a robust and viable solution for mission-critical applications on edge devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10718v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.SC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajat Bhattacharjya, Woohyeok Park, Arnab Sarkar, Hyunwoo Oh, Mohsen Imani, Nikil Dutt</dc:creator>
    </item>
    <item>
      <title>DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems</title>
      <link>https://arxiv.org/abs/2510.10815</link>
      <description>arXiv:2510.10815v1 Announce Type: cross 
Abstract: Automating the formalization of mathematical statements for theorem proving remains a major challenge for Large Language Models (LLMs). LLMs struggle to identify and utilize the prerequisite mathematical knowledge and its corresponding formal representation in languages like Lean. Current retrieval-augmented autoformalization methods query external libraries using the informal statement directly, but overlook a fundamental limitation: informal mathematical statements are often complex and offer limited context on the underlying math concepts. To address this, we introduce DRIFT, a novel framework that enables LLMs to decompose informal mathematical statements into smaller, more tractable ''sub-components''. This facilitates targeted retrieval of premises from mathematical libraries such as Mathlib. Additionally, DRIFT retrieves illustrative theorems to help models use premises more effectively in formalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet, ConNF, and MiniF2F-test) and find that it consistently improves premise retrieval, nearly doubling the F1 score compared to the DPR baseline on ProofNet. Notably, DRIFT demonstrates strong performance on the out-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and 42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that retrieval effectiveness in mathematical autoformalization depends heavily on model-specific knowledge boundaries, highlighting the need for adaptive retrieval strategies aligned with each model's capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10815v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <category>cs.SC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Meiru Zhang, Philipp Borchert, Milan Gritta, Gerasimos Lampouras</dc:creator>
    </item>
    <item>
      <title>Gr\"obner Bases Native to Term-ordered Commutative Algebras, with Application to the Hodge Algebra of Minors</title>
      <link>https://arxiv.org/abs/2510.11212</link>
      <description>arXiv:2510.11212v1 Announce Type: cross 
Abstract: Motivated by better understanding the bideterminant (=product of minors) basis on the polynomial ring in $n \times m$ variables, we develop theory \&amp; algorithms for Gr\"obner bases in not only algebras with straightening law (ASLs or Hodge algebras), but in any commutative algebra over a field that comes equipped with a notion of "monomial" (generalizing the standard monomials of ASLs) and a suitable term order. Rather than treating such an algebra $A$ as a quotient of a polynomial ring and then "lifting" ideals from $A$ to ideals in the polynomial ring, the theory we develop is entirely "native" to $A$ and its given notion of monomial.
  When applied to the case of bideterminants, this enables us to package several standard results on bideterminants in a clean way that enables new results. In particular, once the theory is set up, it lets us give an almost-trivial proof of a universal Gr\"obner basis (in our sense) for the ideal of $t$-minors for any $t$. We note that here it was crucial that theory be native to $A$ and its given monomial structure, as in the standard monomial structure given by bideterminants each $t$-minor is a single variable rather than a sum of $t!$ many terms (in the "ordinary monomial" structure).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11212v1</guid>
      <category>math.AC</category>
      <category>cs.SC</category>
      <category>math.AG</category>
      <category>math.RA</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua A. Grochow, Abhiram Natarajan</dc:creator>
    </item>
    <item>
      <title>Ontolearn-A Framework for Large-scale OWL Class Expression Learning in Python</title>
      <link>https://arxiv.org/abs/2510.11561</link>
      <description>arXiv:2510.11561v1 Announce Type: cross 
Abstract: In this paper, we present Ontolearn-a framework for learning OWL class expressions over large knowledge graphs. Ontolearn contains efficient implementations of recent stateof-the-art symbolic and neuro-symbolic class expression learners including EvoLearner and DRILL. A learned OWL class expression can be used to classify instances in the knowledge graph. Furthermore, Ontolearn integrates a verbalization module based on an LLM to translate complex OWL class expressions into natural language sentences. By mapping OWL class expressions into respective SPARQL queries, Ontolearn can be easily used to operate over a remote triplestore. The source code of Ontolearn is available at https://github.com/dice-group/Ontolearn.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11561v1</guid>
      <category>cs.LG</category>
      <category>cs.SC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of Machine Learning Research 26 (2025) 1-6</arxiv:journal_reference>
      <dc:creator>Caglar Demir, Alkid Baci, N'Dah Jean Kouagou, Leonie Nora Sieger, Stefan Heindorf, Simon Bin, Lukas Bl\"ubaum, Alexander Bigerl, Axel-Cyrille Ngonga Ngomo</dc:creator>
    </item>
    <item>
      <title>Structured Kolmogorov-Arnold Neural ODEs for Interpretable Learning and Symbolic Discovery of Nonlinear Dynamics</title>
      <link>https://arxiv.org/abs/2506.18339</link>
      <description>arXiv:2506.18339v2 Announce Type: replace-cross 
Abstract: Understanding and modeling nonlinear dynamical systems is a fundamental challenge across science and engineering. Deep learning has shown remarkable potential for capturing complex system behavior, yet achieving models that are both accurate and physically interpretable remains difficult. To address this, we propose Structured Kolmogorov-Arnold Neural ODEs (SKANODEs), a framework that integrates structured state-space modeling with Kolmogorov-Arnold Networks (KANs). Within a Neural ODE architecture, SKANODE employs a fully trainable KAN as a universal function approximator to perform virtual sensing, recovering latent states that correspond to interpretable physical quantities such as displacements and velocities. Leveraging KAN's symbolic regression capability, SKANODE then extracts compact, interpretable expressions for the system's governing dynamics. Extensive experiments on simulated and real-world systems demonstrate that SKANODE achieves superior predictive accuracy, discovers physics-consistent dynamics, and reveals complex nonlinear behavior. Notably, it identifies hysteretic behavior in an F-16 aircraft and recovers a concise symbolic equation describing this phenomenon. SKANODE thus enables interpretable, data-driven discovery of physically grounded models for complex nonlinear dynamical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18339v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SC</category>
      <category>nlin.CD</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Liu, Kiran Bacsa, Loon Ching Tang, Eleni Chatzi</dc:creator>
    </item>
    <item>
      <title>Logistic-Gated Operators Enable Auditable Unit-Aware Thresholds in Symbolic Regression</title>
      <link>https://arxiv.org/abs/2510.05178</link>
      <description>arXiv:2510.05178v2 Announce Type: replace-cross 
Abstract: Symbolic regression promises readable equations but struggles to encode unit-aware thresholds and conditional logic. We propose logistic-gated operators (LGO) -- differentiable gates with learnable location and steepness -- embedded as typed primitives and mapped back to physical units for audit. Across two primary health datasets (ICU, NHANES), the hard-gate variant recovers clinically plausible cut-points: 71% (5/7) of assessed thresholds fall within 10% of guideline anchors and 100% within 20%, while using far fewer gates than the soft variant (ICU median 4.0 vs 10.0; NHANES 5.0 vs 12.5), and remaining within the competitive accuracy envelope of strong SR baselines. On predominantly smooth tasks, gates are pruned, preserving parsimony. The result is compact symbolic equations with explicit, unit-aware thresholds that can be audited against clinical anchors -- turning interpretability from a post-hoc explanation into a modeling constraint and equipping symbolic regression with a practical calculus for regime switching and governance-ready deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05178v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ou Deng, Ruichen Cong, Jianting Xu, Shoji Nishimura, Atsushi Ogihara, Qun Jin</dc:creator>
    </item>
  </channel>
</rss>
