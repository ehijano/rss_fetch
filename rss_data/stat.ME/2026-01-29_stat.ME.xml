<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 Jan 2026 03:02:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Connecting reflective asymmetries in multivariate spatial and spatio-temporal covariances</title>
      <link>https://arxiv.org/abs/2601.20132</link>
      <description>arXiv:2601.20132v1 Announce Type: new 
Abstract: In the analysis of multivariate spatial and univariate spatio-temporal data, it is commonly recognized that asymmetric dependence may exist, which can be addressed using an asymmetric (matrix or space-time, respectively) covariance function within a Gaussian process framework. This paper introduces a new paradigm for constructing asymmetric space-time covariances, which we refer to as "reflective asymmetric," by leveraging recently-introduced models for multivariate spatial data. We first provide new results for reflective asymmetric multivariate spatial models that extends their applicability. We then propose their asymmetric space-time extension, which come from a substantially different perspective than Lagrangian asymmetric space-time covariances. There are fewer parameters in the new models, one controls both the spatial and temporal marginal covariances, and the standard separable model is a special case. In simulation studies and analysis of the frequently-studied Irish wind data, these new models also improve model fit and prediction performance, and they can be easier to estimate. These features indicate broad applicability for improved analysis in environmental and other space-time data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20132v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Drew Yarger</dc:creator>
    </item>
    <item>
      <title>Online Change Point Detection for Multivariate Inhomogeneous Poisson Processes Time Series</title>
      <link>https://arxiv.org/abs/2601.20192</link>
      <description>arXiv:2601.20192v1 Announce Type: new 
Abstract: We study online change point detection for multivariate inhomogeneous Poisson point process time series. This setting arises commonly in applications such as earthquake seismology, climate monitoring, and epidemic surveillance, yet remains underexplored in the machine learning and statistics literature. We propose a method that uses low-rank matrices to represent the multivariate Poisson intensity functions, resulting in an adaptive nonparametric detection procedure. Our algorithm is single-pass and requires only constant computational cost per new observation, independent of the elapsed length of the time series. We provide theoretical guarantees to control the overall false alarm probability and characterize the detection delay under temporal dependence. We also develop a new Matrix Bernstein inequality for temporally dependent Poisson point process time series, which may be of independent interest. Numerical experiments demonstrate that our method is both statistically robust and computationally efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20192v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaokai Luo, Haotian Xu, Carlos Misael Madrid Padilla, Oscar Hernan Madrid Padilla</dc:creator>
    </item>
    <item>
      <title>Bias-Reduced Estimation of Finite Mixtures: An Application to Latent Group Structures in Panel Data</title>
      <link>https://arxiv.org/abs/2601.20197</link>
      <description>arXiv:2601.20197v1 Announce Type: new 
Abstract: Finite mixture models are widely used in econometric analyses to capture unobserved heterogeneity. This paper shows that maximum likelihood estimation of finite mixtures of parametric densities can suffer from substantial finite-sample bias in all parameters under mild regularity conditions. The bias arises from the influence of outliers in component densities with unbounded or large support and increases with the degree of overlap among mixture components. I show that maximizing the classification-mixture likelihood function, equipped with a consistent classifier, yields parameter estimates that are less biased than those obtained by standard maximum likelihood estimation (MLE). I then derive the asymptotic distribution of the resulting estimator and provide conditions under which oracle efficiency is achieved. Monte Carlo simulations show that conventional mixture MLE exhibits pronounced finite-sample bias, which diminishes as the sample size or the statistical distance between component densities tends to infinity. The simulations further show that the proposed estimation strategy generally outperforms standard MLE in finite samples in terms of both bias and mean squared errors under relatively weak assumptions. An empirical application to latent group panel structures using health administrative data shows that the proposed approach reduces out-of-sample prediction error by approximately 17.6% relative to the best results obtained from standard MLE procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20197v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rapha\"el Langevin</dc:creator>
    </item>
    <item>
      <title>Joint Estimation of Edge Probabilities for Multi-layer Networks via Neighborhood Smoothing</title>
      <link>https://arxiv.org/abs/2601.20219</link>
      <description>arXiv:2601.20219v1 Announce Type: new 
Abstract: In this paper we focus on jointly estimating the edge probabilities for multi-layer networks. We define a novel multi-layer graphon, a ternary function in contrast to the bivariate graphon function in the literature by introducing an additional latent layer position parameter, which is model-free and covers a wide range of multi-layer networks. We develop a computationally efficient two-step neighborhood smoothing algorithm to estimate the edge probabilities of multi-layer networks, which requires little tuning and fully utilize the similarity across both network layers and nodes. Numerical experiments demonstrate the advantages of our method over the existing state-of-the-art ones. A real Worldwide Food Import/Export Network dataset example is analyzed to illustrate the better performance of the proposed method over benchmark methods in terms of link prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20219v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yong He, Zizhou Huang, Bingyi Jing, Diqing Li</dc:creator>
    </item>
    <item>
      <title>Wavelet Tree Ensembles for Triangulable Manifolds</title>
      <link>https://arxiv.org/abs/2601.20254</link>
      <description>arXiv:2601.20254v1 Announce Type: new 
Abstract: We develop unbalanced Haar (UH) wavelet tree ensembles for regression on triangulable manifolds. Given data sampled on a triangulated manifold, we construct UH wavelet trees whose atoms are supported on geodesic triangles and form an orthonormal system in $L^2(\mu_n)$, where $\mu_n$ is the empirical measure on the sample, which allows us to use UH trees as weak learners in additive ensembles. Our construction extends classical UH wavelet trees from regular Euclidean grids to generic triangulable manifolds while preserving three key properties: (i) orthogonality and exact reconstruction at the sampled locations, (ii) recursive, data-driven partitions adapted to the geometry of the manifold via geodesic triangulations, and (iii) compatibility with optimization-based and Bayesian ensemble building. In Euclidean settings, the framework reduces to standard UH wavelet tree regression and provides a baseline for comparison. We illustrate the method on synthetic regression on the sphere and on climate anomaly fields on a spherical mesh, where UH ensembles on triangulated manifolds substantially outperform classical tree ensembles and non-adaptive mesh-based wavelets. For completeness, we also report results on image denoising on regular grids. A Bayesian variant (RUHWT) provides posterior uncertainty quantification for function estimates on manifolds. Our implementation is available at http://www.github.com/hrluo/WaveletTrees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20254v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hengrui Luo, Akira Horiguchi, Li Ma</dc:creator>
    </item>
    <item>
      <title>Confidence intervals for maximum unseen probabilities, with application to sequential sampling design</title>
      <link>https://arxiv.org/abs/2601.20320</link>
      <description>arXiv:2601.20320v1 Announce Type: new 
Abstract: Discovery problems often require deciding whether additional sampling is needed to detect all categories whose prevalence exceeds a prespecified threshold. We study this question under a Bernoulli product (incidence) model, where categories are observed only through presence--absence across sampling units. Our inferential target is the \emph{maximum unseen probability}, the largest prevalence among categories not yet observed. We develop nonasymptotic, distribution-free upper confidence bounds for this quantity in two regimes: bounded alphabets (finite and known number of categories) and unbounded alphabets (countably infinite under a mild summability condition). We characterise the limits of data-independent worst-case bounds, showing that in the unbounded regime no nontrivial data-independent procedure can be uniformly valid. We then propose data-dependent bounds in both regimes and establish matching lower bounds demonstrating their near-optimality. We compare empirically the resulting procedures in both simulated and real datasets. Finally, we use these bounds to construct sequential stopping rules with finite-sample guarantees, and demonstrate robustness to contamination that introduces spurious low-prevalence categories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20320v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Colombi, Mario Beraha, Amichai Painsky, Stefano Favaro</dc:creator>
    </item>
    <item>
      <title>SCORE: A Unified Framework for Overshoot Refund in Online FDR Control</title>
      <link>https://arxiv.org/abs/2601.20386</link>
      <description>arXiv:2601.20386v1 Announce Type: new 
Abstract: We propose a unified framework to enhance the power of online multiple hypothesis testing procedures based on $e$-values. While $e$-value-based methods offer robust online False Discovery Rate (FDR) control under minimal assumptions, they often suffer from power loss by discarding evidence that exceeds the rejection threshold. We address this inefficiency via the \textbf{S}equential \textbf{C}ontrol with \textbf{O}vershoot \textbf{R}efund for \textbf{E}-values (SCORE) framework, which leverages the inequality $\mathbb{I}(y \ge 1) \le y - (y-1)_+$ to reclaim this otherwise ``wasted'' evidence. This simple yet powerful insight yields a unified principle for improving a broad class of online testing algorithms. Building on this framework, we develop SCORE-enhanced versions of several state-of-the-art procedures, including SCORE-LOND, SCORE-LORD, and SCORE-SAFFRON, all of which strictly dominate their original counterparts while preserving valid finite-sample FDR control. Furthermore, under mild assumptions, SCORE permits retroactive updates of alpha-wealth by using the latest decision twice: first to determine its reward or loss, and then to refresh past wealth. Such a mechanism enables more aggressive testing strategies while maintaining valid FDR control, thereby further improving statistical power. The effectiveness of the proposed methods is validated through extensive simulation and real-data experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20386v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qi Kuang, Bowen Gang, Yin Xia</dc:creator>
    </item>
    <item>
      <title>Exact Graph Learning via Integer Programming</title>
      <link>https://arxiv.org/abs/2601.20589</link>
      <description>arXiv:2601.20589v1 Announce Type: new 
Abstract: Learning the dependence structure among variables in complex systems is a central problem across medical, natural, and social sciences. These structures can be naturally represented by graphs, and the task of inferring such graphs from data is known as graph learning or as causal discovery if the graphs are given a causal interpretation. Existing approaches typically rely on restrictive assumptions about the data-generating process, employ greedy oracle algorithms, or solve approximate formulations of the graph learning problem. As a result, they are either sensitive to violations of central assumptions or fail to guarantee globally optimal solutions. We address these limitations by introducing a nonparametric graph learning framework based on nonparametric conditional independence testing and integer programming. We reformulate the graph learning problem as an integer-programming problem and prove that solving the integer-programming problem provides a globally optimal solution to the original graph learning problem. Our method leverages efficient encodings of graphical separation criteria, enabling the exact recovery of larger graphs than was previously feasible. We provide an implementation in the openly available R package 'glip' which supports learning (acyclic) directed (mixed) graphs and chain graphs. From the resulting output one can compute representations of the corresponding Markov equivalence classes or weak equivalence classes. Empirically, we demonstrate that our approach is faster than other existing exact graph learning procedures for a large fraction of instances and graphs of various sizes. GLIP also achieves state-of-the-art performance on simulated data and benchmark datasets across all aforementioned classes of graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20589v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Kook, S{\o}ren Wengel Mogensen</dc:creator>
    </item>
    <item>
      <title>Causal Inference in Biomedical Imaging via Functional Linear Structural Equation Models</title>
      <link>https://arxiv.org/abs/2601.20610</link>
      <description>arXiv:2601.20610v1 Announce Type: new 
Abstract: Understanding the causal effects of organ-specific features from medical imaging on clinical outcomes is essential for biomedical research and patient care. We propose a novel Functional Linear Structural Equation Model (FLSEM) to capture the relationships among clinical outcomes, functional imaging exposures, and scalar covariates like genetics, sex, and age. Traditional methods struggle with the infinite-dimensional nature of exposures and complex covariates. Our FLSEM overcomes these challenges by establishing identifiable conditions using scalar instrumental variables. We develop the Functional Group Support Detection and Root Finding (FGS-DAR) algorithm for efficient variable selection, supported by rigorous theoretical guarantees, including selection consistency and accurate parameter estimation. We further propose a test statistic to test the nullity of the functional coefficient, establishing its null limit distribution. Our approach is validated through extensive simulations and applied to UK Biobank data, demonstrating robust performance in detecting causal relationships from medical imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20610v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ting Li, Ethan Fan, Tengfei Li, Hongtu Zhu</dc:creator>
    </item>
    <item>
      <title>Two-dose vs. Three-Dose Optimization Under Sample Size Constraint</title>
      <link>https://arxiv.org/abs/2601.20710</link>
      <description>arXiv:2601.20710v1 Announce Type: new 
Abstract: Dose optimization is a hallmark of Project Optimus for oncology drug development. The number of doses to include in a dose optimization study depends on the totality of evidence, which is often unclear in early-phase development. With equal sample sizes per dose, carrying three doses is clearly more advantageous than two for optimization. In this paper, we show that, even when the total sample size is fixed, it is still preferable to carry three unless there is very strong evidence that one can be dropped. A mathematical approximation is applied to guide the investigation, followed by a simulation study to complement the theoretical findings. Semi-quantitative guidance is provided for practitioners, addressing both randomized and non-randomized dose optimization while considering population homogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20710v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linda Sun, Yixin Ren, Cong Chen</dc:creator>
    </item>
    <item>
      <title>A General Mixture Loss Function to Optimize a Personalized Predictive Model</title>
      <link>https://arxiv.org/abs/2601.20788</link>
      <description>arXiv:2601.20788v2 Announce Type: new 
Abstract: Advances in precision medicine increasingly drive methodological innovation in health research. A key development is the use of personalized prediction models (PPMs), which are fit using a similar subpopulation tailored to a specific index patient, and have been shown to outperform one-size-fits-all models, particularly in terms of model discrimination performance. We propose a generalized loss function that enables tuning of the subpopulation size used to fit a PPM. This loss function allows joint optimization of discrimination and calibration, allowing both the performance measures and their relative weights to be specified by the user. To reduce computational burden, we conducted extensive simulation studies to identify practical bounds for the grid of subpopulation sizes. Based on these results, we recommend using a lower bound of 20\% and an upper bound of 70\% of the entire training dataset. We apply the proposed method to both simulated and real-world datasets and demonstrate that previously observed relationships between subpopulation size and model performance are robust. Furthermore, we show that the choice of performance measures in the loss function influences the optimal subpopulation size selected. These findings support the flexible and computationally efficient implementation of PPMs in precision health research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20788v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tatiana Krikella, Joel A. Dubin</dc:creator>
    </item>
    <item>
      <title>Plotting correlated data</title>
      <link>https://arxiv.org/abs/2601.20805</link>
      <description>arXiv:2601.20805v1 Announce Type: new 
Abstract: A very common task in data visualization is to plot many data points with some measured y-value as a function of fixed x-values. Uncertainties on the y-values are typically presented as vertical error bars that represent either a Frequentist confidence interval or Bayesian credible interval for each data point. Most of the time, these error bars represent a 68\% confidence/credibility level, which leads to the intuition that a model fits the data reasonably well if its prediction lies within the error bars of roughly two thirds of the data points. Unfortunately, this and other intuitions no longer work when the uncertainties of the data points are correlated. If the error bars only show the square root of diagonal elements of some covariance matrix with non-negligible off-diagonal elements, we simply do not have enough information in the plot to judge whether a drawn model line agrees well with the data or not. In this paper we will demonstrate this problem and discuss ways to add more information to the plots to make it easier to judge the agreement between the data and some model prediction in the plot, as well as glean some insight where the model might be deficient. This is done by explicitly showing the contribution of the first principal component of the uncertainties, and by displaying the conditional uncertainties of all data points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20805v1</guid>
      <category>stat.ME</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Koch</dc:creator>
    </item>
    <item>
      <title>Joint estimation of the basic reproduction number and serial interval using Sequential Bayes</title>
      <link>https://arxiv.org/abs/2601.20809</link>
      <description>arXiv:2601.20809v1 Announce Type: new 
Abstract: Early in an infectious disease outbreak, timely and accurate estimation of the basic reproduction number ($R_0$) and the serial interval (SI) is critical for understanding transmission dynamics and informing public health responses. While many methods estimate these quantities separately, and a small number jointly estimate them from incidence data, existing joint approaches are largely likelihood-based and do not fully exploit prior information. We propose a novel Bayesian framework for the joint estimation of $R_0$ and the serial interval using only case count data, implemented through a sequential Bayes approach. Our method assumes an SIR model and employs a mildly informative joint prior constructed by linking log-Gamma marginal distributions for $R_0$ and the SI via a Gaussian copula, explicitly accounting for their dependence. The prior is updated sequentially as new incidence data become available, allowing for real-time inference. We assess the performance of the proposed estimator through extensive simulation studies under correct model specification as well as under model misspecification, including when the true data come from an SEIR or SEAIR model, and under varying degrees of prior misspecification. Comparisons with the widely used White and Pagano likelihood-based joint estimator show that our approach yields substantially more precise and stable estimates of $R_0$, with comparable or improved bias, particularly in the early stages of an outbreak. Estimation of the SI is more sensitive to prior misspecification; however, when prior information is reasonably accurate, our method provides reliable SI estimates and remains more stable than the competing approach. We illustrate the practical utility of the proposed method using Canadian COVID-19 incidence data at both national and provincial levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20809v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tatiana Krikella, Jane M. Heffernan, Hanna Jankowski</dc:creator>
    </item>
    <item>
      <title>Effective Sample Size for Functional Spatial Data</title>
      <link>https://arxiv.org/abs/2601.20812</link>
      <description>arXiv:2601.20812v1 Announce Type: new 
Abstract: The effective sample size quantifies the amount of independent information contained in a dataset, accounting for redundancy due to correlation between observations. While widely used in geostatistics for scalar data, its extension to functional spatial data has remained largely unexplored. In this work, we introduce a novel definition of the effective sample size for functional geostatistical data, employing the trace-covariogram as a measure of correlation, and show that it retains the intuitive properties of the classical scalar ESS. We illustrate the behavior of this measure using a functional autoregressive process, demonstrating how serial dependence and the allocation of variability across eigen-directions influence the resulting functional ESS. Finally, the approach is applied to a real meteorological dataset of geometric vertical velocities over a portion of the Earth, showing how the method can quantify redundancy and determine the effective number of independent curves in functional spatial datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20812v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alfredo Alegr\'ia, John G\'omez, Jorge Mateu, Ronny Vallejos</dc:creator>
    </item>
    <item>
      <title>Scalable Decisions using a Bayesian Decision-Theoretic Approach</title>
      <link>https://arxiv.org/abs/2601.20031</link>
      <description>arXiv:2601.20031v1 Announce Type: cross 
Abstract: Randomized controlled experiments assess new policy impacts on performance metrics to inform launch decisions. Traditional approaches evaluate metrics independently despite correlations, and mixed results (e.g., positive revenue impact, negative customer experience) require manual judgment, hindering scalability. We propose a Bayesian decision-theoretic framework that systematically incorporates multiple objectives and trade-offs by comparing expected risks across decisions. Our approach combines experimenter-defined loss functions with observed evidence, using hierarchical models to leverage historical experiment learnings for prior information on treatment effects. Through real and simulated Amazon supply chain experiments, we demonstrate that compared to null hypothesis statistical testing, our method increases estimation efficiency via informative hierarchical priors and simplifies decision-making by systematically incorporating business preferences and costs for comprehensive, scalable decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20031v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hoiyi Ng, Guido Imbens</dc:creator>
    </item>
    <item>
      <title>Going NUTS with ADVI: Exploring various Bayesian Inference techniques with Facebook Prophet</title>
      <link>https://arxiv.org/abs/2601.20120</link>
      <description>arXiv:2601.20120v1 Announce Type: cross 
Abstract: Since its introduction, Facebook Prophet has attracted positive attention from both classical statisticians and the Bayesian statistics community. The model provides two built-in inference methods: maximum a posteriori estimation using the L-BFGS-B algorithm, and Markov Chain Monte Carlo (MCMC) sampling via the No-U-Turn Sampler (NUTS). While exploring various time-series forecasting problems using Bayesian inference with Prophet, we encountered limitations stemming from the inability to apply alternative inference techniques beyond those provided by default. Additionally, the fluent API design of Facebook Prophet proved insufficiently flexible for implementing our custom modeling ideas. To address these shortcomings, we developed a complete reimplementation of the Prophet model in PyMC, which enables us to extend the base model and evaluate and compare multiple Bayesian inference methods. In this paper, we present our PyMC-based implementation and analyze in detail the implementation of different Bayesian inference techniques. We consider full MCMC techniques, MAP estimation and Variational inference techniques on a time-series forecasting problem. We discuss in details the sampling approach, convergence diagnostics, forecasting metrics as well as their computational efficiency and detect possible issues which will be addressed in our future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20120v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 22nd International Conference for Informatics and Information Technologies, pp. 260-265, 2025, ISBN: 978-608-4699-22-4</arxiv:journal_reference>
      <dc:creator>Jovan Krajevski, Biljana Tojtovska Ribarski</dc:creator>
    </item>
    <item>
      <title>Empirical Likelihood-Based Fairness Auditing: Distribution-Free Certification and Flagging</title>
      <link>https://arxiv.org/abs/2601.20269</link>
      <description>arXiv:2601.20269v1 Announce Type: cross 
Abstract: Machine learning models in high-stakes applications, such as recidivism prediction and automated personnel selection, often exhibit systematic performance disparities across sensitive subpopulations, raising critical concerns regarding algorithmic bias. Fairness auditing addresses these risks through two primary functions: certification, which verifies adherence to fairness constraints; and flagging, which isolates specific demographic groups experiencing disparate treatment. However, existing auditing techniques are frequently limited by restrictive distributional assumptions or prohibitive computational overhead. We propose a novel empirical likelihood-based (EL) framework that constructs robust statistical measures for model performance disparities. Unlike traditional methods, our approach is non-parametric; the proposed disparity statistics follow asymptotically chi-square or mixed chi-square distributions, ensuring valid inference without assuming underlying data distributions. This framework uses a constrained optimization profile that admits stable numerical solutions, facilitating both large-scale certification and efficient subpopulation discovery. Empirically, the EL methods outperform bootstrap-based approaches, yielding coverage rates closer to nominal levels while reducing computational latency by several orders of magnitude. We demonstrate the practical utility of this framework on the COMPAS dataset, where it successfully flags intersectional biases, specifically identifying a significantly higher positive prediction rate for African-American males under 25 and a systemic under-prediction for Caucasian females relative to the population mean.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20269v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jie Tang, Chuanlong Xie, Xianli Zeng, Lixing Zhu</dc:creator>
    </item>
    <item>
      <title>Blessing of dimensionality in cross-validated bandwidth selection on the sphere</title>
      <link>https://arxiv.org/abs/2601.20442</link>
      <description>arXiv:2601.20442v1 Announce Type: cross 
Abstract: We study the asymptotic behavior of least-squares cross-validation bandwidth selection in kernel density estimation on the $d$-dimensional hypersphere, $d\geq 1$. We show that the exact rate of convergence with respect to the optimal bandwidth minimizing the mean integrated squared error, shown to exist under mild non-uniformity conditions, is $n^{-d/(2d+8)}$, thus approaching the $n^{-1/2}$ parametric rate as $d$ grows. This ``blessing of dimensionality'' in bandwidth selection offers theoretical support for utilizing the conceptually simpler cross-validation selector over plug-in techniques for larger dimensions $d$. We compare this result for bandwidth estimation on the $d$-dimensional Euclidean space through explicit expressions for the asymptotic variance functionals. Numerical experiments corroborate the speed of this convergence in an array of scenarios and dimensions, precisely illustrating the tipping dimension where cross-validation outperforms plug-in approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20442v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jos\'e E. Chac\'on, Eduardo Garc\'ia-Portugu\'es, Andrea Meil\'an-Vila</dc:creator>
    </item>
    <item>
      <title>Modeling extremal dependence in multivariate and spatial problems: a practical perspective</title>
      <link>https://arxiv.org/abs/2412.13453</link>
      <description>arXiv:2412.13453v3 Announce Type: replace 
Abstract: From environmental sciences to finance, there are growing needs for assessing the risk of more extreme events than those observed. Extrapolating extreme events beyond the range of the data is not obvious and requires advanced tools based on extreme value theory. Furthermore, the complexity of risk assessments often requires the inclusion of multiple variables. Extreme value theory provides very important tools for the analysis of multivariate or spatial extreme events, but these are not easily accessible to professionals without appropriate expertise. This article provides a minimal background on multivariate and spatial extremes and gives simple yet thorough instructions on how to analyse high-dimensional extremes using the R package ExtremalDep. After briefly introducing the statistical methodologies, we focus on road testing the package's toolbox through several real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13453v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Boris Beranger, Simone A. Padoan</dc:creator>
    </item>
    <item>
      <title>Robust functional PCA for relative data</title>
      <link>https://arxiv.org/abs/2412.19004</link>
      <description>arXiv:2412.19004v3 Announce Type: replace 
Abstract: This paper introduces a robust approach to functional principal component analysis (FPCA) for relative data, particularly density functions. While recent papers have studied density data within the Bayes space framework, there has been limited focus on developing robust methods to effectively handle anomalous observations and large noise. To address this, we extend the Mahalanobis distance concept to Bayes spaces, proposing its regularized version that accounts for the constraints inherent in density data. Based on this extension, we introduce a new method, robust density principal component analysis (RDPCA), for more accurate estimation of functional principal components in the presence of outliers. The method's performance is validated through simulations and real-world applications, showing its ability to improve covariance estimation and principal component analysis compared to traditional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19004v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy Oguamalam, Peter Filzmoser, Karel Hron, Alessandra Menafoglio, Una Radoji\v{c}i\'c</dc:creator>
    </item>
    <item>
      <title>Data-Adaptive Integration With Summary Data</title>
      <link>https://arxiv.org/abs/2506.11482</link>
      <description>arXiv:2506.11482v2 Announce Type: replace 
Abstract: Combining an internal individual-level study with readily available external summary statistics promises major efficiency gains at minimal additional cost, yet heterogeneity between sources can bias estimates for the internal target population. We develop a generalized entropy-balancing integration strategy that calibrates external moments to the internal covariate distribution, explicitly permitting a biased external sample. Our estimator of the internal-population mean is doubly robust: it remains consistent when either the outcome-regression model or the entropy-balancing modelis correctly specified. When multiple balancing specifications are plausible, we introduce a data-adaptive selection rule. We also provide easy-to-compute, fully estimable diagnostics-based on the Mahalanobis distance and the Pearson chi-square divergence-that pinpoint when integration is guaranteed to strictly outperform the internal sample mean. The approach is implemented in the R package daisy. Simulations and an application to nationwide public-access defibrillation records in Japan demonstrate meaningful precision gains while maintaining bias control under distributional shift.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11482v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kosuke Morikawa, Sho Komukai, Satoshi Hattori</dc:creator>
    </item>
    <item>
      <title>Refined thresholds for inconsistency: The effect of the graph associated with incomplete pairwise comparisons</title>
      <link>https://arxiv.org/abs/2510.27011</link>
      <description>arXiv:2510.27011v2 Announce Type: replace 
Abstract: The inconsistency of pairwise comparisons remains difficult to interpret in the absence of acceptability thresholds. The popular 10% cut-off rule proposed by Saaty has recently been applied to incomplete pairwise comparison matrices, which contain some unknown comparisons. This paper refines these inconsistency thresholds: we uncover that they depend not only on the size of the matrix and the number of missing entries, but also on the undirected graph whose edges represent the known pairwise comparisons. Therefore, using our exact thresholds is especially important if the filling in patterns coincide for a large number of matrices, as has been recommended in the literature. The strong association between the new threshold values and the spectral radius of the representing graph is also demonstrated. Our results can be integrated into software to continuously monitor inconsistency during the collection of pairwise comparisons and immediately detect potential errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27011v2</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kolos Csaba \'Agoston, L\'aszl\'o Csat\'o</dc:creator>
    </item>
    <item>
      <title>Modeling Issues with Eye Tracking Data</title>
      <link>https://arxiv.org/abs/2512.15950</link>
      <description>arXiv:2512.15950v5 Announce Type: replace 
Abstract: I describe and compare procedures for binary eye-tracking (ET) data. The basic GLM model is a logistic mixed model combined with random effects for persons and items. Additional models address error correlation in eye-tracking serial observations. In particular, three novel approaches are illustrated that address serial without the use of an observed lag-1 predictor: a first-order autoregressive model and a first-order moving average models obtained with generalized estimating equations, and a recurrent two-state survival model used with run-length encoded data. Altogether, the results of five different analyses point to unresolved issues in the analysis of eye-tracking data and new directions for analytic development. A more traditional model incorporating a lag-1 observed outcome for serial correlation is also included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15950v5</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gregory Camilli</dc:creator>
    </item>
    <item>
      <title>Inflation Target at Risk: A Time-varying Parameter Distributional Regression</title>
      <link>https://arxiv.org/abs/2403.12456</link>
      <description>arXiv:2403.12456v3 Announce Type: replace-cross 
Abstract: Inflation exhibits state-dependent, skewed, and fat-tailed dynamics that make risk a central concern for monetary policy. Accordingly, inflation risks are distributional and cannot be fully captured by mean-based models. We propose a flexible time-varying parameter distributional regression model that estimates the full conditional distribution of inflation, allowing macroeconomic drivers to have nonlinear and asymmetric effects across the distribution. Applied to U.S. inflation, the model captures major shifts in tail-risk probabilities. Analysis of risk drivers shows that deflationary pressures arise primarily from demand-side weakness and inflation persistence, whereas upside risks are driven mainly by supply-side shocks, particularly energy price inflation. Examining the impact of key drivers further reveals that the unemployment-inflation relationship weakens in the distributional tails. Energy price shocks, by contrast, have little effect on deflation risk but exhibit strongly time-varying and asymmetric effects on high-inflation risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12456v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yunyun Wang, Tatsushi Oka, Dan Zhu</dc:creator>
    </item>
    <item>
      <title>AGFS-Tractometry: A Novel Atlas-Guided Fine-Scale Tractometry Approach for Enhanced Along-Tract Group Statistical Comparison Using Diffusion MRI Tractography</title>
      <link>https://arxiv.org/abs/2507.10601</link>
      <description>arXiv:2507.10601v2 Announce Type: replace-cross 
Abstract: Diffusion MRI (dMRI) tractography is currently the only method for in vivo mapping of the brain's white matter (WM) connections. Tractometry is an advanced tractography analysis technique for along-tract profiling to investigate the morphology and microstructural properties along the fiber tracts. Tractometry has become an essential tool for studying local along-tract differences between different populations (e.g., health vs disease). In this study, we propose a novel atlas-guided fine-scale tractometry method, namely AGFS-Tractometry, that leverages tract spatial information and permutation testing to enhance the along-tract statistical analysis between populations. There are two major contributions in AGFS-Tractometry. First, we create a novel atlas-guided tract profiling template that enables consistent, fine-scale, along-tract parcellation of subject-specific fiber tracts. Second, we propose a novel nonparametric permutation testing group comparison method to enable simultaneous analysis across all along-tract parcels while correcting for multiple comparisons. We perform experimental evaluations on synthetic datasets with known group differences and in vivo real data. We compare AGFS-Tractometry with two state-of-the-art tractometry methods, including Automated Fiber-tract Quantification (AFQ) and BUndle ANalytics (BUAN). Our results show that the proposed AGFS-Tractometry obtains enhanced sensitivity and specificity in detecting local WM differences. In the real data analysis experiments, AGFS-Tractometry can identify more regions with significant differences, which are anatomically consistent with the existing literature. Overall, these demonstrate the ability of AGFS-Tractometry to detect subtle or spatially localized WM group-level differences. The created tract profiling template and related code are available at: https://github.com/ZhengRuixi/AGFS-Tractometry.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10601v2</guid>
      <category>q-bio.QM</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruixi Zheng, Wei Zhang, Yijie Li, Xi Zhu, Zhou Lan, Jarrett Rushmore, Yogesh Rathi, Nikos Makris, Lauren J. O'Donnell, Fan Zhang</dc:creator>
    </item>
    <item>
      <title>Sparse Equation Matching: A Derivative-Free Learning for General-Order Dynamical Systems</title>
      <link>https://arxiv.org/abs/2507.20072</link>
      <description>arXiv:2507.20072v2 Announce Type: replace-cross 
Abstract: Equation discovery is a fundamental learning task for uncovering the underlying dynamics of complex systems, with wide-ranging applications in areas such as brain connectivity analysis, climate modeling, gene regulation, and physical simulation. However, many existing approaches rely on accurate derivative estimation and are limited to first-order dynamical systems, restricting their applicability in real-world scenarios. In this work, we propose Sparse Equation Matching (SEM), a unified framework that encompasses several existing equation discovery methods under a common formulation. SEM introduces an integral-based sparse regression approach using Green's functions, enabling derivative-free estimation of differential operators and their associated driving functions in general-order dynamical systems. The effectiveness of SEM is demonstrated through extensive simulations, benchmarking its performance against derivative-based approaches. We then apply SEM to electroencephalographic (EEG) data recorded during multiple oculomotor tasks, collected from 52 participants in a brain-computer interface experiment. Our method identifies active brain regions across participants and reveals task-specific connectivity patterns. These findings offer valuable insights into brain connectivity and the underlying neural mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20072v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiaqiang Li, Jianbin Tan, Xueqin Wang</dc:creator>
    </item>
  </channel>
</rss>
