<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Mar 2025 02:18:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Langevin Bi-fidelity Importance Sampling for Failure Probability Estimation</title>
      <link>https://arxiv.org/abs/2503.17796</link>
      <description>arXiv:2503.17796v1 Announce Type: new 
Abstract: Estimating failure probability is one of the key tasks in the field of uncertainty quantification. In this domain, importance sampling has proven to be an effective estimation strategy; however, its efficiency heavily depends on the choice of the biasing distribution. An improperly selected biasing distribution can significantly increase estimation error. One way to solve this problem is to leverage a less expensive, lower-fidelity surrogate. Building on the accessibility to such a model and its derivative on the random uncertain inputs, we introduce an importance-sampling-based estimator, termed the Langevin bi-fidelity importance sampling (L-BF-IS), which uses score-function-based sampling algorithms to generate new samples and substantially reduces the mean square error (MSE) of failure probability estimation. The proposed method demonstrates lower estimation error, especially in high-dimensional ($\geq 100$) input spaces and when limited high-fidelity evaluations are available. The L-BF-IS estimator's effectiveness is validated through experiments with two synthetic functions and two real-world applications governed by partial differential equations. These real-world applications involve a composite beam, which is represented using a simplified Euler-Bernoulli equation as a low-fidelity surrogate, and a steady-state stochastic heat equation, for which a pre-trained neural operator serves as the low-fidelity surrogate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17796v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nuojin Cheng, Alireza Doostan</dc:creator>
    </item>
    <item>
      <title>Enhanced Vascular Flow Simulations in Aortic Aneurysm via Physics-Informed Neural Networks and Deep Operator Networks</title>
      <link>https://arxiv.org/abs/2503.17402</link>
      <description>arXiv:2503.17402v1 Announce Type: cross 
Abstract: Due to the limited accuracy of 4D Magnetic Resonance Imaging (MRI) in identifying hemodynamics in cardiovascular diseases, the challenges in obtaining patient-specific flow boundary conditions, and the computationally demanding and time-consuming nature of Computational Fluid Dynamics (CFD) simulations, it is crucial to explore new data assimilation algorithms that offer possible alternatives to these limitations. In the present work, we study Physics-Informed Neural Networks (PINNs), Deep Operator Networks (DeepONets), and their Physics-Informed extensions (PI-DeepONets) in predicting vascular flow simulations in the context of a 3D Abdominal Aortic Aneurysm (AAA) idealized model. PINN is a technique that combines deep neural networks with the fundamental principles of physics, incorporating the physics laws, which are given as partial differential equations, directly into loss functions used during the training process. On the other hand, DeepONet is designed to learn nonlinear operators from data and is particularly useful in studying parametric partial differential equations (PDEs), e.g., families of PDEs with different source terms, boundary conditions, or initial conditions. Here, we adapt the approaches to address the particular use case of AAA by integrating the 3D Navier-Stokes equations (NSE) as the physical laws governing fluid dynamics. In addition, we follow best practices to enhance the capabilities of the models by effectively capturing the underlying physics of the problem under study. The advantages and limitations of each approach are highlighted through a series of relevant application cases. We validate our results by comparing them with CFD simulations for benchmark datasets, demonstrating good agreements and emphasizing those cases where improvements in computational efficiency are observed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17402v1</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Oscar L. Cruz-Gonz\'alez, Val\'erie Deplano, Badih Ghattas</dc:creator>
    </item>
    <item>
      <title>Efficiently Vectorized MCMC on Modern Accelerators</title>
      <link>https://arxiv.org/abs/2503.17405</link>
      <description>arXiv:2503.17405v1 Announce Type: cross 
Abstract: With the advent of automatic vectorization tools (e.g., JAX's $\texttt{vmap}$), writing multi-chain MCMC algorithms is often now as simple as invoking those tools on single-chain code. Whilst convenient, for various MCMC algorithms this results in a synchronization problem -- loosely speaking, at each iteration all chains running in parallel must wait until the last chain has finished drawing its sample. In this work, we show how to design single-chain MCMC algorithms in a way that avoids synchronization overheads when vectorizing with tools like $\texttt{vmap}$ by using the framework of finite state machines (FSMs). Using a simplified model, we derive an exact theoretical form of the obtainable speed-ups using our approach, and use it to make principled recommendations for optimal algorithm design. We implement several popular MCMC algorithms as FSMs, including Elliptical Slice Sampling, HMC-NUTS, and Delayed Rejection, demonstrating speed-ups of up to an order of magnitude in experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17405v1</guid>
      <category>cs.MS</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugh Dance, Pierre Glaser, Peter Orbanz, Ryan Adams</dc:creator>
    </item>
    <item>
      <title>The Entropy and Crossentropy of Generalized Mallows Models</title>
      <link>https://arxiv.org/abs/2503.17521</link>
      <description>arXiv:2503.17521v1 Announce Type: cross 
Abstract: The Generalized Mallows Model (GMM) is a well known family of models for ranking data. A GMM is a distribution over $\mathbb{S}_n$, the set of permutations of n objects, characterized by a location parameter $\sigma \in \mathbb{S}_n$, known as central permutation and a set of dispersion parameters $\theta_{1:n-1}\in(0,1]$. The GMM shares many properties, such as having sufficient statistics, with exponential models, thus it can be seen as an exponential family with a discrete parameter $\sigma$. This paper shows that computing entropy, crossentropy and Kullback-Leibler divergence in the the class of GMM is tractable, paving the way for a better understanding of this exponential family.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17521v1</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marina Meil\u{a}</dc:creator>
    </item>
    <item>
      <title>Bayesian Optimization for CVaR-based portfolio optimization</title>
      <link>https://arxiv.org/abs/2503.17737</link>
      <description>arXiv:2503.17737v1 Announce Type: cross 
Abstract: Optimal portfolio allocation is often formulated as a constrained risk problem, where one aims to minimize a risk measure subject to some performance constraints. This paper presents new Bayesian Optimization algorithms for such constrained minimization problems, seeking to minimize the conditional value-at-risk (a computationally intensive risk measure) under a minimum expected return constraint. The proposed algorithms utilize a new acquisition function, which drives sampling towards the optimal region. Additionally, a new two-stage procedure is developed, which significantly reduces the number of evaluations of the expensive-to-evaluate objective function. The proposed algorithm's competitive performance is demonstrated through practical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17737v1</guid>
      <category>q-fin.PM</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert Millar, Jinglai Li</dc:creator>
    </item>
    <item>
      <title>Linear, nested, and quadratic ordered measures: Computation and incorporation into optimization problems</title>
      <link>https://arxiv.org/abs/2503.18097</link>
      <description>arXiv:2503.18097v1 Announce Type: cross 
Abstract: In this paper we address a unified mathematical optimization framework to compute a wide range of measures used in most operations research and data science contexts. The goal is to embed such metrics within general optimization models allowing their efficient computation. We assess the usefulness of this approach applying it to three different families of measures, namely linear, nested, and quadratic ordered measures. Computational results are reported showing the efficiency and accuracy of our methods as compared with standard implementations in numerical software packages. Finally, we illustrate this methodology by computing a number of optimal solutions with respect to different metrics on three well-known linear and combinatorial optimization problems: scenario analysis in linear programming, the traveling salesman and the weighted multicover set problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18097v1</guid>
      <category>math.OC</category>
      <category>cs.DM</category>
      <category>stat.CO</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor Blanco, Miguel A. Pozo, Justo Puerto, Alberto Torrejon</dc:creator>
    </item>
    <item>
      <title>Potentials and Limitations of Large-scale, Individual-level Mobile Location Data for Food Acquisition Analysis</title>
      <link>https://arxiv.org/abs/2503.18119</link>
      <description>arXiv:2503.18119v1 Announce Type: cross 
Abstract: Understanding food acquisition is crucial for developing strategies to combat food insecurity, a major public health concern. The emergence of large-scale mobile location data (typically exemplified by GPS data), which captures people's movement over time at high spatiotemporal resolutions, offer a new approach to study this topic. This paper evaluates the potential and limitations of large-scale GPS data for food acquisition analysis through a case study. Using a high-resolution dataset of 286 million GPS records from individuals in Jacksonville, Florida, we conduct a case study to assess the strengths of GPS data in capturing spatiotemporal patterns of food outlet visits while also discussing key limitations, such as potential data biases and algorithmic uncertainties. Our findings confirm that GPS data can generate valuable insights about food acquisition behavior but may significantly underestimate visitation frequency to food outlets. Robustness checks highlight how algorithmic choices-especially regarding food outlet classification and visit identification-can influence research results. Our research underscores the value of GPS data in place-based health studies while emphasizing the need for careful consideration of data coverage, representativeness, algorithmic choices, and the broader implications of study findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18119v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <category>stat.CO</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Duanya Lyu, Luyu Liu, Catherine Campbell, Yuxuan Zhang, Xiang Yan</dc:creator>
    </item>
    <item>
      <title>Efficient Inference in First Passage Time Models</title>
      <link>https://arxiv.org/abs/2503.18381</link>
      <description>arXiv:2503.18381v1 Announce Type: cross 
Abstract: First passage time models describe the time it takes for a random process to exit a region of interest and are widely used across various scientific fields. Fast and accurate numerical methods for computing the likelihood function in these models are essential for efficient statistical inference. Specifically, in mathematical psychology, generalized drift diffusion models (GDDMs) are an important class of first passage time models that describe the latent psychological processes underlying simple decision-making scenarios. GDDMs model the joint distribution over choices and response times as the first hitting time of a one-dimensional stochastic differential equation (SDE) to possibly time-varying upper and lower boundaries. They are widely applied to extract parameters associated with distinct cognitive and neural mechanisms. However, current likelihood computation methods struggle with common scenarios where drift rates covary dynamically with exogenous covariates in each trial, such as in the attentional drift diffusion model (aDDM). In this work, we propose a fast and flexible algorithm for computing the likelihood function of GDDMs based on a large class of SDEs satisfying the Cherkasov condition. Our method divides each trial into discrete stages, employs fast analytical results to compute stage-wise densities, and integrates these to compute the overall trial-wise likelihood. Numerical examples demonstrate that our method not only yields accurate likelihood evaluations for efficient statistical inference, but also significantly outperforms existing approaches in terms of speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18381v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sicheng Liu, Alexander Fengler, Michael J. Frank, Matthew T. Harrison</dc:creator>
    </item>
    <item>
      <title>lqmix: an R package for longitudinal data analysis via linear quantile mixtures</title>
      <link>https://arxiv.org/abs/2302.11363</link>
      <description>arXiv:2302.11363v3 Announce Type: replace 
Abstract: The analysis of longitudinal data gives the chance to observe how unit behaviors change over time, but it also poses a series of issues. These have been the focus of an extensive literature in the context of linear and generalized linear regression moving also, in the last ten years or so, to the context of linear quantile regression for continuous responses. In this paper, we present \textsf{lqmix}, a novel \textsf{R} package that assists in estimating a class of linear quantile regression models for longitudinal data, in the presence of time-constant and/or time-varying, unit-specific, random coefficients, with unspecified distribution. Model parameters are estimated in a maximum likelihood framework, via an extended EM algorithm, and parameters' standard errors are estimated via a block-bootstrap procedure. The analysis of a benchmark dataset is used to give details on the package functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.11363v3</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Alf\'o, Maria Francesca Marino, Maria Giovanna Ranalli, Nicola Salvati</dc:creator>
    </item>
    <item>
      <title>GraphGrad: Efficient Estimation of Sparse Polynomial Representations for General State-Space Models</title>
      <link>https://arxiv.org/abs/2411.15637</link>
      <description>arXiv:2411.15637v3 Announce Type: replace 
Abstract: State-space models (SSMs) are a powerful statistical tool for modelling time-varying systems via a latent state. In these models, the latent state is never directly observed. Instead, a sequence of observations related to the state is available. The state-space model is defined by the state dynamics and the observation model, both of which are described by parametric distributions. Estimation of parameters of these distributions is a very challenging, but essential, task for performing inference and prediction. Furthermore, it is typical that not all states of the system interact. We can therefore encode the interaction of the states via a graph, usually not fully connected. However, most parameter estimation methods do not take advantage of this feature. In this work, we propose GraphGrad, a fully automatic approach for obtaining sparse estimates of the state interactions of a non-linear state-space model via a polynomial approximation. This novel methodology unveils the latent structure of the data-generating process, allowing us to infer both the structure and value of a rich and efficient parameterisation of a general state-space model. Our method utilises a differentiable particle filter to optimise a Monte Carlo likelihood estimator. It also promotes sparsity in the estimated system through the use of suitable proximity updates, known to be more efficient and stable than subgradient methods. As shown in our paper, a number of well-known dynamical systems can be accurately represented and recovered by our method, providing basis for application to real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15637v3</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Cox, Emilie Chouzenoux, Victor Elvira</dc:creator>
    </item>
    <item>
      <title>Markov Renewal Proportional Hazards is All You Need</title>
      <link>https://arxiv.org/abs/2502.03479</link>
      <description>arXiv:2502.03479v5 Announce Type: replace-cross 
Abstract: Transition probability estimation plays a critical role in multi-state modeling, especially in clinical research. This paper investigates the application of semi-Markov and Markov renewal frameworks to the EBMT dataset, focusing on six clinical states encountered during hematopoietic stem cell transplantation. By comparing Aalen-Johansen (AJ) and Dabrowska-Sun-Horowitz (DSH) estimators, we demonstrate that semi-Markov models, which incorporate sojourn times, provide a more nuanced and temporally sensitive depiction of patient trajectories compared to memoryless Markov models. The DSH estimator consistently yields smoother probability curves, particularly for transitions involving prolonged states. We use empirical process theory and Burkholder-Davis-Gundy inequality to show weak convergence of the estimator. Future work includes extending the framework to accommodate advanced covariate structures and non-Markovian dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03479v5</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eliuvish Cuicizion</dc:creator>
    </item>
    <item>
      <title>Community Detection Analysis of Spatial Transcriptomics Data</title>
      <link>https://arxiv.org/abs/2503.12351</link>
      <description>arXiv:2503.12351v2 Announce Type: replace-cross 
Abstract: The spatial transcriptomics (ST) data produced by recent biotechnologies, such as CosMx and Xenium, contain huge amount of information about cancer tissue samples, which has great potential for cancer research via detection of community: a collection of cells with distinct cell-type composition and similar neighboring patterns. But existing clustering methods do not work well for community detection of CosMx ST data, and the commonly used kNN compositional data method shows lack of informative neighboring cell patterns for huge CosMx data. In this article, we propose a novel and more informative disk compositional data (DCD) method, which identifies neighboring patterns of each cell via taking into account of ST data features from recent new technologies. After initial processing ST data into DCD matrix, a new innovative and interpretable DCD-TMHC community detection method is proposed here. Extensive simulation studies and CosMx breast cancer data analysis clearly show that our proposed DCD-TMHC method is superior to other methods. Based on the communities detected by DCD-TMHC method for CosMx breast cancer data, the logistic regression analysis results demonstrate that DCD-TMHC method is clearly interpretable and superior, especially in terms of assessment for different stages of cancer. These suggest that our proposed novel, innovative, informative and interpretable DCD-TMHC method here will be helpful and have impact to future cancer research based on ST data, which can improve cancer diagnosis and monitor cancer treatment progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12351v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles Zhao, Susana Garcia-Recio, Brooke M. Felsheim</dc:creator>
    </item>
  </channel>
</rss>
