<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.MF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.MF</link>
    <description>q-fin.MF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.MF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Feb 2026 05:00:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Entropy-regularized penalization schemes for American options and reflected BSDEs with singular generators</title>
      <link>https://arxiv.org/abs/2602.18078</link>
      <description>arXiv:2602.18078v1 Announce Type: new 
Abstract: This paper extends our previous work in Chee et al. [9] to continuous-time optimal stopping problems, with a particular focus on American options within an exploratory framework. We pursue two main objectives. First, motivated by reinforcement learning applications, we introduce an entropy-regularized penalization scheme for continuous-time optimal stopping problems. The scheme is inspired by classical penalization techniques for reflected backward stochastic differential equations (RBSDEs) and provides a smooth approximation of the degenerate stopping rule inherent to the American option problem. This regularization promotes exploration, enables the use of gradient-based optimization methods, and leads naturally to policy improvement algorithms. We establish well-posedness and convergence properties of the scheme, and illustrate its numerical feasibility through low-dimensional experiments based on policy iteration and least-squares Monte Carlo methods. Second, from a theoretical perspective, we study the asymptotic limit of the entropy-regularized penalization as the penalization parameter tends to infinity. We show that the limiting value process solves a reflected BSDE with a logarithmically singular driver, and we prove existence and uniqueness of solutions to this new class of RBSDEs via a monotone limit argument. To the best of our knowledge, such equations have not previously been investigated in the literature</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18078v1</guid>
      <category>q-fin.MF</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Chee, Noufel Frikha, Libo Li</dc:creator>
    </item>
    <item>
      <title>A Monotone Limit Approach to Entropy-Regularized American Options</title>
      <link>https://arxiv.org/abs/2602.18062</link>
      <description>arXiv:2602.18062v1 Announce Type: cross 
Abstract: Recent advances in continuous-time optimal stopping have been driven by entropy-regularized formulations of randomized stopping problems, with most existing approaches relying on partial differential equation methods. In this paper, we propose a fully probabilistic framework based on the Doob-Meyer-Mertens decomposition of the Snell envelope and its representation through reflected backward stochastic differential equations. We introduce an entropy-regularized penalization scheme yielding a monotone approximation of the value function and establish explicit convergence rates under suitable regularity assumptions. In addition, we develop a policy improvement algorithm based on linear backward stochastic differential equations and illustrate its performance through a simple numerical experiment for an American-style max call option</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18062v1</guid>
      <category>q-fin.CP</category>
      <category>q-fin.MF</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Chee, Noufel Frikha, Libo Li</dc:creator>
    </item>
    <item>
      <title>Linear short rate model with several delays</title>
      <link>https://arxiv.org/abs/2402.16428</link>
      <description>arXiv:2402.16428v3 Announce Type: replace 
Abstract: This paper introduces a short rate model in continuous time that adds one or more memory (delay) components to the Merton model (Merton 1970, 1973) or the Vasi\v{c}ek model (Vasi\v{c}ek 1977) for the short rate. The distribution of the short rate in this model is normal, with the mean depending on past values of the short rate, and a limiting distribution exists for certain values of the parameters. The zero coupon bond price is an affine function of the short rate, whose coefficients satisfy a system of delay differential equations. This system can be solved analytically, obtaining a closed formula. An analytical expression for the instantaneous forward rate is given: it satisfies the risk neutral dynamics of the Heath-Jarrow-Morton model. Formulae for both forward looking and backward looking caplets on overnight risk free rates are presented. Finally, the proposed model is calibrated against forward looking caplets on SONIA rates and the United States yield curve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16428v3</guid>
      <category>q-fin.MF</category>
      <category>math.PR</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/17442508.2026.2630978</arxiv:DOI>
      <dc:creator>Alet Roux, \'Alvaro Guinea Juli\'a</dc:creator>
    </item>
  </channel>
</rss>
