<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Sep 2025 01:53:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Canonical Variate Analysis Biplot based on the Generalized Singular Value Decomposition</title>
      <link>https://arxiv.org/abs/2509.17463</link>
      <description>arXiv:2509.17463v1 Announce Type: new 
Abstract: Canonical Variate Analysis (CVA) is a multivariate statistical technique and a direct application of Linear Discriminant Analysis (LDA) that aims to find linear combinations of variables that best differentiate between groups in a dataset. The data is partitioned into groups based on some predetermined criteria, and then linear combinations of the original variables are derived such that they maximize the separation between the groups. However, a common limitation of this optimization in CVA is that the within cluster scatter matrix must be nonsingular, which restricts the use of datasets when the number of variables is larger than the number of observations. By applying the generalized singular value decomposition (GSVD), the same goal of CVA can be achieved regardless on the number of variables. In this paper we use this approach to show that CVA can be applied and graphical representations to such data can be constructed. Specifically, we will be looking at the construction of a CVA biplot for such data that will display observations as points and variables as axes in a reduced dimension. Finally, we present experimental results that confirm the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17463v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raeesa Ganey, Sugnet Lubbe</dc:creator>
    </item>
    <item>
      <title>Monte Carlo on a single sample</title>
      <link>https://arxiv.org/abs/2509.17025</link>
      <description>arXiv:2509.17025v1 Announce Type: cross 
Abstract: In this paper, we consider a Monte Carlo simulation method (MinMC) that approximates prices and risk measures for a range $\Gamma$ of model parameters at once. The simulation method that we study has recently gained popularity [HS20, FPP22, BDG24], and we provide a theoretical framework and convergence rates for it. In particular, we show that sample-based approximations to $\mathbb{E}_{\theta}[X]$, where $\theta$ denotes the model and $\mathbb{E}_{\theta}$ the expectation with respect to the distribution $P_\theta$ of the model $\theta$, can be obtained across all $\theta \in \Gamma$ by minimizing a map $V:H\rightarrow \mathbb{R}$ with $H$ a suitable function space. The minimization can be achieved easily by fitting a standard feedforward neural network with stochastic gradient descent. We show that MinMC, which uses only one sample for each model, significantly outperforms a traditional Monte Carlo method performed for multiple values of $\theta$, which are subsequently interpolated. Our case study suggests that MinMC might serve as a new benchmark for parameter-dependent Monte Carlo simulations, which appear not only in quantitative finance but also in many other areas of scientific computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17025v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nils Detering, Paul Eisenberg, Nicole Hufnagel</dc:creator>
    </item>
    <item>
      <title>Self-Tuned Rejection Sampling within Gibbs and a Case Study in Small Area Estimation</title>
      <link>https://arxiv.org/abs/2509.17155</link>
      <description>arXiv:2509.17155v1 Announce Type: cross 
Abstract: When preparing a Gibbs sampler, some conditionals may be unfamiliar distributions without well-known variate generation routines. Rejection sampling may be used to draw from such distributions exactly; however, it can be challenging to obtain practical proposal distributions. A practical proposal is one where accepted draws are not extremely rare occurrences and which is not too computationally intensive to use repeatedly within the Gibbs sampler. Consequently, approximate methods such as Metropolis-Hastings steps tend to be used in this setting. This work revisits the vertical weighted strips (VWS) method of proposal construction from arXiv:2401.09696 for univariate conditionals within Gibbs. VWS constructs a finite mixture based on the form of the target density and provides an upper bound on the rejection probability. The rejection probability can be reduced by refining terms in the finite mixture. Na\"{i}vely constructing a new proposal for each target encountered in a Gibbs sampler can be computationally impractical. Instead, we consider proposal distributions which persist over the Gibbs sampler and tune themselves gradually to avoid very high rejection probabilities while discarding mixture terms with low contribution. We explore a motivating application in small area estimation, applied to the estimation of county-level population counts of school-aged children in poverty. Here, a Gibbs sampler for a Bayesian model of interest includes a family of unfamiliar densities to be drawn for each observation in the data. Self-tuned VWS is applied to obtain exact draws within Gibbs while keeping the computational workload of proposal maintenance under control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17155v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew M. Raim, Kyle M. Irimata, James A. Livsey</dc:creator>
    </item>
    <item>
      <title>Covariance-Corrected WAIC for Bayesian Sequential Data Models</title>
      <link>https://arxiv.org/abs/2509.17980</link>
      <description>arXiv:2509.17980v1 Announce Type: cross 
Abstract: This paper introduces and develops a theoretical extension of the widely applicable information criterion (WAIC), called the Covariance-Corrected WAIC (CC-WAIC), that applied for Bayesian sequential data models. The CC-WAIC accounts for temporal or structural dependence by incorporating the full posterior covariance structure of the log-likelihood contributions, in contrast to the classical WAIC that assumes conditional independence among data. We exploit the limitations of classical WAIC in the sequential data contexts and derive the CC-WAIC criterion under a theoretical framework. In addition, we propose a bias correction based on effective sample size to improve estimation from Markov Chain Monte Carlo (MCMC) simulations. Furthermore, we highlight the advantages of CC-WAIC in terms of stability and appropriateness for dependent data. This new criterion is supported by formal mathematical derivations, illustrative examples, and discussion of implications for model selection in both classical and modern Bayesian applications. To evaluate the reliability of CC-WAIC under varying data regimes, we conduct simulation experiments across multiple time series lengths (small, medium, and large) and different levels of temporal dependence, enabling a comprehensive performance assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17980v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Safaa K. Kadhem</dc:creator>
    </item>
    <item>
      <title>Core-elements Subsampling for Alternating Least Squares</title>
      <link>https://arxiv.org/abs/2509.18024</link>
      <description>arXiv:2509.18024v1 Announce Type: cross 
Abstract: In this paper, we propose a novel element-wise subset selection method for the alternating least squares (ALS) algorithm, focusing on low-rank matrix factorization involving matrices with missing values, as commonly encountered in recommender systems. While ALS is widely used for providing personalized recommendations based on user-item interaction data, its high computational cost, stemming from repeated regression operations, poses significant challenges for large-scale datasets. To enhance the efficiency of ALS, we propose a core-elements subsampling method that selects a representative subset of data and leverages sparse matrix operations to approximate ALS estimations efficiently. We establish theoretical guarantees for the approximation and convergence of the proposed approach, showing that it achieves similar accuracy with significantly reduced computational time compared to full-data ALS. Extensive simulations and real-world applications demonstrate the effectiveness of our method in various scenarios, emphasizing its potential in large-scale recommendation systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18024v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dunyao Xue, Mengyu Li, Cheng Meng, Jingyi Zhang</dc:creator>
    </item>
    <item>
      <title>Kernel K-means clustering of distributional data</title>
      <link>https://arxiv.org/abs/2509.18037</link>
      <description>arXiv:2509.18037v1 Announce Type: cross 
Abstract: We consider the problem of clustering a sample of probability distributions from a random distribution on $\mathbb R^p$. Our proposed partitioning method makes use of a symmetric, positive-definite kernel $k$ and its associated reproducing kernel Hilbert space (RKHS) $\mathcal H$. By mapping each distribution to its corresponding kernel mean embedding in $\mathcal H$, we obtain a sample in this RKHS where we carry out the $K$-means clustering procedure, which provides an unsupervised classification of the original sample. The procedure is simple and computationally feasible even for dimension $p&gt;1$. The simulation studies provide insight into the choice of the kernel and its tuning parameter. The performance of the proposed clustering procedure is illustrated on a collection of Synthetic Aperture Radar (SAR) images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18037v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amparo Ba\'illo, Jose R. Berrendero, Mart\'in S\'anchez-Signorini</dc:creator>
    </item>
    <item>
      <title>Sparse Bayesian joint modal estimation for exploratory item factor analysis</title>
      <link>https://arxiv.org/abs/2411.03992</link>
      <description>arXiv:2411.03992v2 Announce Type: replace-cross 
Abstract: This study presents a scalable Bayesian estimation algorithm for sparse estimation in exploratory item factor analysis based on a classical Bayesian estimation method, namely Bayesian joint modal estimation (BJME). BJME estimates the model parameters and factor scores that maximize the complete-data joint posterior density. Simulation studies show that the proposed algorithm has high computational efficiency and accuracy in variable selection over latent factors and the recovery of the model parameters. Moreover, we conducted a real data analysis using large-scale data from a psychological assessment that targeted the Big Five personality traits. This result indicates that the proposed algorithm achieves computationally efficient parameter estimation and extracts the interpretable factor loading structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03992v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keiichiro Hijikata, Motonori Oka, Kensuke Okada</dc:creator>
    </item>
    <item>
      <title>Characteristic function-based tests for spatial randomness</title>
      <link>https://arxiv.org/abs/2504.07946</link>
      <description>arXiv:2504.07946v2 Announce Type: replace-cross 
Abstract: We introduce a new type of test for complete spatial randomness that applies to mapped point patterns in a rectangle or a cube of any dimension. This is the first test of its kind to be based on characteristic functions and utilizes a weighted $L_2$-distance between the empirical and uniform characteristic functions. The test shows surprising connections to Ripley's $K$-function and Zimmerman's $\bar{\omega}^2$ statistic. It is also simple to calculate and does not require adjusting for edge effects. An efficient algorithm is developed to find the asymptotic null distribution of the test statistic under the Cauchy weight function. This makes the test fast to compute. In simulations, our test shows varying sensitivity to different levels of spatial interaction depending on the scale parameter of the Cauchy weight function. Tests with different parameter values can be combined to create a Bonferroni-corrected omnibus test, which is more powerful than the popular $L$-test and the Clark-Evans test in most simulation settings of heterogeneity, aggregation and regularity, especially when the sample size is large. The simplicity of the empirical characteristic function makes it straightforward to extend our test to non-rectangular or sparsely sampled point patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07946v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Zeng, Dale L. Zimmerman</dc:creator>
    </item>
    <item>
      <title>Estimating the Heritability of Longitudinal Rate-of-Change: Genetic Insights into PSA Velocity in Prostate Cancer-Free Individuals</title>
      <link>https://arxiv.org/abs/2505.04773</link>
      <description>arXiv:2505.04773v2 Announce Type: replace-cross 
Abstract: Serum prostate-specific antigen (PSA) is widely used for prostate cancer screening. While the genetics of PSA levels has been studied to enhance screening accuracy, the genetic basis of PSA velocity, the rate of PSA change over time, remains unclear. The Prostate, Lung, Colorectal, and Ovarian (PLCO) Cancer Screening Trial, a large, randomized study with longitudinal PSA data (15,260 cancer-free males, averaging 5.34 samples per subject) and genome-wide genotype data, provides a unique opportunity to estimate PSA velocity heritability. We developed a mixed model to jointly estimate heritability of PSA levels at age 54 and PSA velocity. To accommodate the large dataset, we implemented two efficient computational approaches: a partitioning and meta-analysis strategy using average information restricted maximum likelihood (AI-REML), and a fast restricted Haseman-Elston (REHE) regression method. Simulations showed that both methods yield unbiased estimates of both heritability metrics, with AI-REML providing smaller variability in the estimation of velocity heritability than REHE. Applying AI-REML to PLCO data, we estimated heritability at 0.32 (s.e. = 0.07) for baseline PSA and 0.45 (s.e. = 0.18) for PSA velocity. These findings reveal a substantial genetic contribution to PSA velocity, supporting future genome-wide studies to identify variants affecting PSA dynamics and improve PSA-based screening.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04773v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pei Zhang, Xiaoyu Wang, Jianxin Shi, Paul S. Albert</dc:creator>
    </item>
  </channel>
</rss>
