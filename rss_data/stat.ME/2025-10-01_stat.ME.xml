<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 01 Oct 2025 04:01:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Calibrated Counterfactual Conformal Fairness ($C^3F$): Post-hoc, Shift-Aware Coverage Parity via Conformal Prediction and Counterfactual Regularization</title>
      <link>https://arxiv.org/abs/2509.25295</link>
      <description>arXiv:2509.25295v1 Announce Type: new 
Abstract: We present Calibrated Counterfactual Conformal Fairness ($C^3F$), a post-hoc procedure that targets group-conditional coverage parity under covariate shift. $C^3F$ combines importance-weighted conformal calibration with a counterfactual regularizer based on path-specific effects in a structural causal model. The method estimates group-specific nonconformity quantiles using likelihood-ratio weights so that coverage degrades gracefully with the second moment of the weights. We derive finite-sample lower bounds on group-wise coverage and a bound on the equalized conditional coverage gap, and we show first-order control of a counterfactual coverage-parity surrogate via smooth threshold regularization. The approach is model-agnostic, label-efficient, and deployable without retraining. Empirical evaluations on standard classification benchmarks demonstrate improved group-conditional coverage and competitive efficiency relative to shift-aware and fairness-oriented conformal baselines. We discuss practical considerations, including partial availability of sensitive attributes and robustness to structural causal misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25295v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Faruk Alpay, Taylan Alpay</dc:creator>
    </item>
    <item>
      <title>Bias-Reduced Estimation of Structural Equation Models</title>
      <link>https://arxiv.org/abs/2509.25419</link>
      <description>arXiv:2509.25419v1 Announce Type: new 
Abstract: Finite-sample bias is a pervasive challenge in the estimation of structural equation models (SEMs), especially when sample sizes are small or measurement reliability is low. A range of methods have been proposed to improve finite-sample bias in the SEM literature, ranging from analytic bias corrections to resampling-based techniques, with each carrying trade-offs in scope, computational burden, and statistical performance. We apply the reduced-bias M-estimation framework (RBM, Kosmidis &amp; Lunardon, 2024, J. R. Stat. Soc. Series B Stat. Methodol.) to SEMs. The RBM framework is attractive as it requires only first- and second-order derivatives of the log-likelihood, which renders it both straightforward to implement, and computationally more efficient compared to resampling-based alternatives such as bootstrap and jackknife. It is also robust to departures from modelling assumptions. Through extensive simulations studies under a range of experimental conditions, we illustrate that RBM estimators consistently reduce mean bias in the estimation of SEMs without inflating mean squared error. They also deliver improvements in both median bias and inference relative to maximum likelihood estimators, while maintaining robustness under non-normality. Our findings suggest that RBM offers a promising, practical, and broadly applicable tool for mitigating bias in the estimation of SEMs, particularly in small-sample research contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25419v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Haziq Jamil, Yves Rosseel, Oliver Kemp, Ioannis Kosmidis</dc:creator>
    </item>
    <item>
      <title>Joint Adaptive Penalty for Unbalanced Mediation Pathways</title>
      <link>https://arxiv.org/abs/2509.25527</link>
      <description>arXiv:2509.25527v1 Announce Type: new 
Abstract: Mediation analysis has been widely used to investigate how a treatment influences an outcome through intermediate variables, known as mediators. Analyzing a mediation mechanism typically requires assessing multiple model parameters that characterize distinct pathwise effects. Classical methods that estimate these parameters individually can be inefficient, particularly when the underlying pathwise effects exhibit substantial imbalance. To address this challenge, this work proposes a new joint adaptive penalty that integrates information across entire mediation mechanisms, thereby enhancing both parameter estimation and pathway selection. We establish theoretical guarantees for the proposed method under an asymptotic framework and conduct extensive numerical studies to demonstrate its superior performance in scenarios with unbalanced mediation pathways.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25527v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanying Jiang, Kris Sankaran, Yinqiu He</dc:creator>
    </item>
    <item>
      <title>Evaluating treatment effects on longitudinal outcomes with attrition due to death: Methods for a two-dimentional estimand with a case study in Quality of Life</title>
      <link>https://arxiv.org/abs/2509.25548</link>
      <description>arXiv:2509.25548v1 Announce Type: new 
Abstract: When longitudinal outcomes are evaluated in mortal populations, their non-existence after death complicates the analysis and its causal interpretation. Where popular methods often merge longitudinal outcome and survival into one scale or otherwise try to circumvent the problem of mortality, some highly relevant questions require survival to be acknowledged as a unique condition. "\textit{What are my chances of survival}" and "\textit{What can I expect for my condition while still alive}" reflect the intrinsically two-dimensional outcome of survival and longitudinal outcome while-alive. We define a two-dimensional causal while-alive estimand for a point exposure and compare two methods for estimation in an observational setting. Regression-Standardization models survival and the observed longitudinal outcome before standardizing the latter to a target population weighted by its estimated survival. Alternatively, Inverse Probability of Treatment and Censoring Weighting weights the observed outcomes twice, to account for censoring and differences in baseline-case-mix. Both approaches rely on the same causal identification assumptions, but require different models to be correctly specified. With its potential to extrapolate, Regression-Standardization is more efficient when all assumptions are met. We show finite sample performance in a simulation study and apply the methods to a case study on quality of life in oncology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25548v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dries Reynders, Doranne Thomassen, Satrajit Roychoudhury, Cecilie Delphin Amdal, Jammbe Z. Musoro, Willi Sauerbrei, Saskia le Cessie, Els Goetghebeur</dc:creator>
    </item>
    <item>
      <title>PPD-CPP: Pointwise predictive density calibrated-power prior in dynamically borrowing historical information</title>
      <link>https://arxiv.org/abs/2509.25688</link>
      <description>arXiv:2509.25688v1 Announce Type: new 
Abstract: Incorporating historical or real-world data into analyses of treatment effects for rare diseases has become increasingly popular. A major challenge, however, lies in determining the appropriate degree of congruence between historical and current data. In this study, we devote ourselves to the capacity of historical data in replicating the current data, and propose a new congruence measure/estimand $p_{CM}$. $p_{CM}$ quantifies the heterogeneity between two datasets following the idea of the marginal posterior predictive $p$-value, and its asymptotic properties were derived. Building upon $p_{CM}$, we develop the pointwise predictive density calibrated-power prior (PPD-CPP) to dynamically leverage historical information. PPD-CPP achieves the borrowing consistency and allows modeling the power parameter either as a fixed scalar or case-specific quantity informed by covariates. Simulation studies were conducted to demonstrate the performance of these methods and the methodology was illustrated using the Mother's Gift study and \textit{Ceriodaphnia dubia} toxicity test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25688v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shixuan Wang, Jing Zhang, Emily L. Kang, Bin Zhang</dc:creator>
    </item>
    <item>
      <title>Modeling Spatial Heterogeneity in Exposure Buffers and Risk: A Hierarchical Bayesian Approach</title>
      <link>https://arxiv.org/abs/2509.25708</link>
      <description>arXiv:2509.25708v1 Announce Type: new 
Abstract: Place-based epidemiology studies often rely on circular buffers to define "exposure" to spatially distributed risk factors, where the buffer radius represents a threshold beyond which exposure does not influence the outcome of interest. This approach is popular due to its simplicity and alignment with public health policies. However, buffer radii are often chosen relatively arbitrarily and assumed constant across the spatial domain. This may result in suboptimal statistical inference if these modeling choices are incorrect. To address this, we develop SVBR (Spatially-Varying Buffer Radii), a flexible hierarchical Bayesian spatial change points approach that treats buffer radii as unknown parameters and allows both radii and exposure effects to vary spatially. Through simulations, we find that SVBR improves estimation and inference for key model parameters compared to traditional methods. We also apply SVBR to study healthcare access in Madagascar, finding that proximity to healthcare facilities generally increases antenatal care usage, with clear spatial variation in this relationship. By relaxing rigid assumptions about buffer characteristics, our method offers a flexible, data-driven approach to accurately defining exposure and quantifying its impact. The newly developed methods are available in the R package EpiBuffer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25708v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saskia Comess, Daniel E Ho, Joshua L Warren</dc:creator>
    </item>
    <item>
      <title>Repulsive mixtures via the sparsity-inducing partition prior</title>
      <link>https://arxiv.org/abs/2509.25860</link>
      <description>arXiv:2509.25860v1 Announce Type: new 
Abstract: We introduce a novel prior distribution for modelling the weights in mixture models based on a generalisation of the Dirichlet distribution, the Selberg Dirichlet distribution. This distribution contains a repulsive term, which naturally penalises values that lie close to each other on the simplex, thus encouraging few dominating clusters. The repulsive behaviour induces additional sparsity on the number of components. We refer to this construction as sparsity-inducing partition (SIP) prior. By highlighting differences with the conventional Dirichlet distribution, we present relevant properties of the SIP prior and demonstrate their implications across a variety of mixture models, including finite mixtures with a fixed or random number of components, as well as repulsive mixtures. We propose an efficient posterior sampling algorithm and validate our model through an extensive simulation study as well as an application to a biomedical dataset describing children's Body Mass Index and eating behaviour.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25860v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alexander Mozdzen, Timothy Wertz, Maria De Iorio, Andrea Cremaschi, Gregor Kastner, Johan Eriksson</dc:creator>
    </item>
    <item>
      <title>Highly robust factored principal component analysis for matrix-valued outlier accommodation and explainable detection via matrix minimum covariance determinant</title>
      <link>https://arxiv.org/abs/2509.25957</link>
      <description>arXiv:2509.25957v1 Announce Type: new 
Abstract: Principal component analysis (PCA) is a classical and widely used method for dimensionality reduction, with applications in data compression, computer vision, pattern recognition, and signal processing. However, PCA is designed for vector-valued data and encounters two major challenges when applied to matrix-valued data with heavy-tailed distributions or outliers: (1) vectorization disrupts the intrinsic matrix structure, leading to information loss and the curse of dimensionality, and (2) PCA is highly sensitive to outliers. Factored PCA (FPCA) addresses the first issue through probabilistic modeling, using a matrix normal distribution that explicitly represents row and column covariances via a separable covariance structure, thereby preserving the two-way dependency and matrix form of the data. Building on FPCA, we propose highly robust FPCA (HRFPCA), a robust extension that replaces maximum likelihood estimators with the matrix minimum covariance determinant (MMCD) estimators. This modification enables HRFPCA to retain FPCA's ability to model matrix-valued data while achieving a breakdown point close to 50\%, substantially improving resistance to outliers. Furthermore, HRFPCA produces the score--orthogonal distance analysis (SODA) plot, which effectively visualizes and classifies matrix-valued outliers. Extensive simulations and real-data analyses demonstrate that HRFPCA consistently outperforms competing methods in robustness and outlier detection, underscoring its effectiveness and broad applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25957v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenhui Wu, Changchun Shang, Jianhua Zhao, Xuan Ma, Yue Wang</dc:creator>
    </item>
    <item>
      <title>Fuzzy Jump Models for Soft and Hard Clustering of Multivariate Time Series Data</title>
      <link>https://arxiv.org/abs/2509.26029</link>
      <description>arXiv:2509.26029v1 Announce Type: new 
Abstract: Statistical jump models have been recently introduced to detect persistent regimes by clustering temporal features and discouraging frequent regime changes. However, they are limited to hard clustering and thereby do not account for uncertainty in state assignments. This work presents an extension of the statistical jump model that incorporates uncertainty estimation in cluster membership. Leveraging the similarities between statistical jump models and the fuzzy c-means framework, our fuzzy jump model sequentially estimates time-varying state probabilities. Our approach offers high flexibility, as it supports both soft and hard clustering through the tuning of a fuzziness parameter, and it naturally accommodates multivariate time series data of mixed types. Through a simulation study, we evaluate the ability of the proposed model to accurately estimate the true latent-state distribution, demonstrating that it outperforms competing approaches under high cluster assignment uncertainty. We further demonstrate its utility on two empirical applications: first, by automatically identifying co-orbital regimes in the three-body problem, a novel application with important implications for understanding asteroid behavior and designing interplanetary mission trajectories; and second, on a financial dataset of five assets representing distinct market sectors (equities, bonds, foreign exchange, cryptocurrencies, and utilities), where the model accurately tracks both bull and bear market phases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26029v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Federico P. Cortese, Antonio Pievatolo, Elisa Maria Alessi</dc:creator>
    </item>
    <item>
      <title>Parameter estimation of the four-parameter Harris extended Weibull distribution with applications to real-life data</title>
      <link>https://arxiv.org/abs/2509.26162</link>
      <description>arXiv:2509.26162v1 Announce Type: new 
Abstract: This paper explores the extension of the classical two-parameter Weibull distribution to a four-parameter Harris extended Weibull (HEW) distribution. The flexibility of this probability distribution is illustrated by the varying shapes of HEW density function. Estimation of HEW parameters is explored using estimation methods such as the least-squares, maximum product of spacings, and minimum distance method. We provide Bayesian inference on the random parameters of the HEW distribution using Metropolis-Hastings algorithm to sample from the joint posterior distribution. Performance of the estimation methods is assessed using extensive simulations. The applicability of the distribution is demonstrated against three variants of the Weibull distribution on three real-life datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26162v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Prithul Chaturvedi, Himanshu Pokhriyal</dc:creator>
    </item>
    <item>
      <title>Staged Event Trees for Transparent Treatment Effect Estimation</title>
      <link>https://arxiv.org/abs/2509.26265</link>
      <description>arXiv:2509.26265v1 Announce Type: new 
Abstract: Average and conditional treatment effects are fundamental causal quantities used to evaluate the effectiveness of treatments in various critical applications, including clinical settings and policy-making. Beyond the gold-standard estimators from randomized trials, numerous methods have been proposed to estimate treatment effects using observational data. In this paper, we provide a novel characterization of widely used causal inference techniques within the framework of staged event trees, demonstrating their capacity to enhance treatment effect estimation. These models offer a distinct advantage due to their interpretability, making them particularly valuable for practical applications. We implement classical estimators within the framework of staged event trees and illustrate their capabilities through both simulation studies and real-world applications. Furthermore, we showcase how staged event trees explicitly and visually describe when standard causal assumptions, such as positivity, hold, further enhancing their practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26265v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gherardo Varando, Manuele Leonelli, Jordi Cerd\`a-Bautista, Vasileios Sitokonstantinou, Gustau Camps-Valls</dc:creator>
    </item>
    <item>
      <title>W-transforms: Uniformity-preserving transformations and induced dependence structures</title>
      <link>https://arxiv.org/abs/2509.26280</link>
      <description>arXiv:2509.26280v1 Announce Type: new 
Abstract: W-transforms are introduced as uniformity-preserving univariate transformations on the unit interval induced by distribution functions and piecewise strictly monotone functions, and their properties are investigated. When applied componentwise to random vectors with standard uniform univariate margins, W-transforms naturally serve as copula-to-copula transformations. Properties of the resulting W-transformed copulas, including their analytical form, density, measures of concordance, tail dependence and symmetries, are derived. A flexible parametric family of W-transforms is proposed as a special case to further enhance tractability. Illustrative examples highlight the introduced concepts, and improved dependence modelling is demonstrated in terms of a real-life dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26280v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marius Hofert, Zhiyuan Pang</dc:creator>
    </item>
    <item>
      <title>An Order of Magnitude Time Complexity Reduction for Gaussian Graphical Model Posterior Sampling Using a Reverse Telescoping Block Decomposition</title>
      <link>https://arxiv.org/abs/2509.26385</link>
      <description>arXiv:2509.26385v1 Announce Type: new 
Abstract: We consider the problem of fully Bayesian posterior estimation and uncertainty quantification in undirected Gaussian graphical models via Markov chain Monte Carlo (MCMC) under recently-developed element-wise graphical priors, such as the graphical horseshoe. Unlike the conjugate Wishart family, these priors are non-conjugate; but have the advantage that they naturally allow one to encode a prior belief of sparsity in the off-diagonal elements of the precision matrix, without imposing a structure on the entire matrix. Unfortunately, for a graph with $p$ nodes and with $n$ samples, the state-of-the-art MCMC approaches for the element-wise priors achieve a per iteration complexity of $O(p^4),$ which is prohibitive when $p\gg n$. In this regime, we develop a suitably reparameterized MCMC with per iteration complexity of $O(p^3)$, providing a one-order of magnitude improvement, and consequently bringing the computational cost at par with the conjugate Wishart family, which is also $O(p^3)$ due to a use of the classical Bartlett decomposition, but this decomposition does not apply outside the Wishart family. Importantly, the proposed benefit is obtained solely due to our reparameterization in an MCMC scheme targeting the true posterior, that reverses the recently developed telescoping block decomposition of Bhadra et al. (2024), in a suitable sense. There is no variational or any other approximate Bayesian computation scheme considered in this paper that compromises targeting the true posterior. Simulations and the analysis of a breast cancer data set confirm both the correctness and better algorithmic scaling of the proposed reverse telescoping sampler.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26385v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zejin Gao, Ksheera Sagar, Anindya Bhadra</dc:creator>
    </item>
    <item>
      <title>Spectral Bootstrap for Non-Parametric Simulation of Multivariate Extreme Events</title>
      <link>https://arxiv.org/abs/2509.26451</link>
      <description>arXiv:2509.26451v1 Announce Type: new 
Abstract: Inference in extreme value theory relies on a limited number of extreme observations, making estimation challenging. To address this limitation, we propose a non-parametric bootstrap procedure, the multivariate extreme spectral bootstrap procedure, relying on the spectral representation of multivariate generalized Paretodistributed random vectors. Unlike standard bootstrap methods, our approach preserves the joint tail behaviour of the data and generates additional synthetic extreme data, thereby improving the reliability of inference. We demonstrate the effectiveness of our procedure for the estimation of tail risk metrics, under both simulated and real data. The results highlight the potential of this method for enhancing risk assessment in high-dimensional extreme scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26451v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nisrine Madhar (UPCit\'e, LPSM), Juliette Legrand (LMBA, UBO), Maud Thomas (LPSM)</dc:creator>
    </item>
    <item>
      <title>Computationally and statistically efficient estimation of time-smoothed counterfactual curves</title>
      <link>https://arxiv.org/abs/2509.26554</link>
      <description>arXiv:2509.26554v1 Announce Type: new 
Abstract: Longitudinal causal inference is concerned with defining, identifying, and estimating the effect of a time-varying intervention on a time-varying outcome that is indexed by a follow-up time. In an observational study, Robins's generalized g-formula can identify causal effects induced by a broad class of time-varying interventions. Various methods for estimating the generalized g-formula have been posed for different outcome types, such as a failure event indicator by a specified time (e.g. mortality by 5 year follow-up), as well as continuous or dichotomous/multi-valued outcomes measures at a specified time (e.g. blood pressure in mm/hg or an indicator of high blood pressure at 5-year follow-up). Multiply-robust, data-adaptive estimators leverage flexible nonparametric estimation algorithms while allowing for statistical inference. However, extant methods do not accommodate time-smoothing when multiple outcomes are measured over time, which can lead to substantial loss of precision. We propose a novel multiply-robust estimator of the generalized g-formula that accommodates time-smoothing over numerous available outcome measures. Our approach accommodates any intervention that can be described as a Longitudinal Modified Treatment Policy, a flexible class suitable for binary, multi-valued, and continuous longitudinal treatments. Our method produces an estimate of the effect curve: the causal effect of the intervention on the outcome at each measurement time, taking into account censoring and non-monotonic outcome missingness patterns. In simulations we find that the proposed algorithm outperforms extant multiply-robust approaches for effect curve estimation in scenarios with high degrees of outcome missingness and when there is strong confounding. We apply the method to study longitudinal effects of union membership on wages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26554v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Herbert P. Susmann, Nicholas T. Williams, Richard Liu, Jessica G. Young, Iv\'an D\'iaz</dc:creator>
    </item>
    <item>
      <title>Stochasticity and Practical Identifiability in Epidemic Models: A Monte Carlo Perspective</title>
      <link>https://arxiv.org/abs/2509.26577</link>
      <description>arXiv:2509.26577v1 Announce Type: new 
Abstract: Assessing the practical identifiability of epidemic models is essential for determining whether parameters can be meaningfully estimated from observed data. Monte Carlo (MC) methods provide an accessible and intuitive framework; however, their standard implementation - perturbing deterministic trajectories with independent Gaussian noise - rests on assumptions poorly suited to epidemic processes, which are inherently stochastic, temporally correlated, and highly variable, especially in small populations or under slow transmission. In this study, we investigate the structure of stochastic variability in the classic Susceptible-Infected-Recovered (SIR) model across a range of epidemiological regimes, and assess whether it can be represented within the independent Gaussian noise framework. We show that continuous-time Markov chain (CTMC) trajectories consistently exhibit super-Poissonian variability and strong temporal dependence. Through coverage analysis, we further demonstrate that independent Gaussian noise systematically underestimates the variability of the underlying stochastic process, leading to overly optimistic conclusions about parameter identifiability. In addition, we propose a hybrid simulation approach that introduces time- and amplitude-dependent variability into deterministic ODE trajectories, preserving computational efficiency while capturing key features of epidemic stochasticity. Our findings highlight the limitations of the standard MC algorithm and provide a pathway for incorporating more realistic noise structures into epidemic inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26577v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chiara Mattamira, Olivia Prosper Feldman</dc:creator>
    </item>
    <item>
      <title>Crowdsourcing Without People: Modelling Clustering Algorithms as Experts</title>
      <link>https://arxiv.org/abs/2509.25395</link>
      <description>arXiv:2509.25395v1 Announce Type: cross 
Abstract: This paper introduces mixsemble, an ensemble method that adapts the Dawid-Skene model to aggregate predictions from multiple model-based clustering algorithms. Unlike traditional crowdsourcing, which relies on human labels, the framework models the outputs of clustering algorithms as noisy annotations. Experiments on both simulated and real-world datasets show that, although the mixsemble is not always the single top performer, it consistently approaches the best result and avoids poor outcomes. This robustness makes it a practical alternative when the true data structure is unknown, especially for non-expert users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25395v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordyn E. A. Lorentz, Katharine M. Clark</dc:creator>
    </item>
    <item>
      <title>One-shot Conditional Sampling: MMD meets Nearest Neighbors</title>
      <link>https://arxiv.org/abs/2509.25507</link>
      <description>arXiv:2509.25507v1 Announce Type: cross 
Abstract: How can we generate samples from a conditional distribution that we never fully observe? This question arises across a broad range of applications in both modern machine learning and classical statistics, including image post-processing in computer vision, approximate posterior sampling in simulation-based inference, and conditional distribution modeling in complex data settings. In such settings, compared with unconditional sampling, additional feature information can be leveraged to enable more adaptive and efficient sampling. Building on this, we introduce Conditional Generator using MMD (CGMMD), a novel framework for conditional sampling. Unlike many contemporary approaches, our method frames the training objective as a simple, adversary-free direct minimization problem. A key feature of CGMMD is its ability to produce conditional samples in a single forward pass of the generator, enabling practical one-shot sampling with low test-time complexity. We establish rigorous theoretical bounds on the loss incurred when sampling from the CGMMD sampler, and prove convergence of the estimated distribution to the true conditional distribution. In the process, we also develop a uniform concentration result for nearest-neighbor based functionals, which may be of independent interest. Finally, we show that CGMMD performs competitively on synthetic tasks involving complex conditional densities, as well as on practical applications such as image denoising and image super-resolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25507v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anirban Chatterjee, Sayantan Choudhury, Rohan Hore</dc:creator>
    </item>
    <item>
      <title>Optimal Nuisance Function Tuning for Estimating a Doubly Robust Functional under Proportional Asymptotics</title>
      <link>https://arxiv.org/abs/2509.25536</link>
      <description>arXiv:2509.25536v1 Announce Type: cross 
Abstract: In this paper, we explore the asymptotically optimal tuning parameter choice in ridge regression for estimating nuisance functions of a statistical functional that has recently gained prominence in conditional independence testing and causal inference. Given a sample of size $n$, we study estimators of the Expected Conditional Covariance (ECC) between variables $Y$ and $A$ given a high-dimensional covariate $X \in \mathbb{R}^p$. Under linear regression models for $Y$ and $A$ on $X$ and the proportional asymptotic regime $p/n \to c \in (0, \infty)$, we evaluate three existing ECC estimators and two sample splitting strategies for estimating the required nuisance functions. Since no consistent estimator of the nuisance functions exists in the proportional asymptotic regime without imposing further structure on the problem, we first derive debiased versions of the ECC estimators that utilize the ridge regression nuisance function estimators. We show that our bias correction strategy yields $\sqrt{n}$-consistent estimators of the ECC across different sample splitting strategies and estimator choices. We then derive the asymptotic variances of these debiased estimators to illustrate the nuanced interplay between the sample splitting strategy, estimator choice, and tuning parameters of the nuisance function estimators for optimally estimating the ECC. Our analysis reveals that prediction-optimal tuning parameters (i.e., those that optimally estimate the nuisance functions) may not lead to the lowest asymptotic variance of the ECC estimator -- thereby demonstrating the need to be careful in selecting tuning parameters based on the final goal of inference. Finally, we verify our theoretical results through extensive numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25536v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sean McGrath, Debarghya Mukherjee, Rajarshi Mukherjee, Zixiao Jolene Wang</dc:creator>
    </item>
    <item>
      <title>Conservative Decisions with Risk Scores</title>
      <link>https://arxiv.org/abs/2509.25588</link>
      <description>arXiv:2509.25588v1 Announce Type: cross 
Abstract: In binary classification applications, conservative decision-making that allows for abstention can be advantageous. To this end, we introduce a novel approach that determines the optimal cutoff interval for risk scores, which can be directly available or derived from fitted models. Within this interval, the algorithm refrains from making decisions, while outside the interval, classification accuracy is maximized. Our approach is inspired by support vector machines (SVM), but differs in that it minimizes the classification margin rather than maximizing it. We provide the theoretical optimal solution to this problem, which holds important practical implications. Our proposed method not only supports conservative decision-making but also inherently results in a risk-coverage curve. Together with the area under the curve (AUC), this curve can serve as a comprehensive performance metric for evaluating and comparing classifiers, akin to the receiver operating characteristic (ROC) curve. To investigate and illustrate our approach, we conduct both simulation studies and a real-world case study in the context of diagnosing prostate cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25588v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yishu Wei, Wen-Yee Lee, George Ekow Quaye, Xiaogang Su</dc:creator>
    </item>
    <item>
      <title>Coupling Generative Modeling and an Autoencoder with the Causal Bridge</title>
      <link>https://arxiv.org/abs/2509.25599</link>
      <description>arXiv:2509.25599v1 Announce Type: cross 
Abstract: We consider inferring the causal effect of a treatment (intervention) on an outcome of interest in situations where there is potentially an unobserved confounder influencing both the treatment and the outcome. This is achievable by assuming access to two separate sets of control (proxy) measurements associated with treatment and outcomes, which are used to estimate treatment effects through a function termed the em causal bridge (CB). We present a new theoretical perspective, associated assumptions for when estimating treatment effects with the CB is feasible, and a bound on the average error of the treatment effect when the CB assumptions are violated. From this new perspective, we then demonstrate how coupling the CB with an autoencoder architecture allows for the sharing of statistical strength between observed quantities (proxies, treatment, and outcomes), thus improving the quality of the CB estimates. Experiments on synthetic and real-world data demonstrate the effectiveness of the proposed approach in relation to the state-of-the-art methodology for proxy measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25599v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruolin Meng, Ming-Yu Chung, Dhanajit Brahma, Ricardo Henao, Lawrence Carin</dc:creator>
    </item>
    <item>
      <title>MuPlon: Multi-Path Causal Optimization for Claim Verification through Controlling Confounding</title>
      <link>https://arxiv.org/abs/2509.25715</link>
      <description>arXiv:2509.25715v1 Announce Type: cross 
Abstract: As a critical task in data quality control, claim verification aims to curb the spread of misinformation by assessing the truthfulness of claims based on a wide range of evidence. However, traditional methods often overlook the complex interactions between evidence, leading to unreliable verification results. A straightforward solution represents the claim and evidence as a fully connected graph, which we define as the Claim-Evidence Graph (C-E Graph). Nevertheless, claim verification methods based on fully connected graphs face two primary confounding challenges, Data Noise and Data Biases. To address these challenges, we propose a novel framework, Multi-Path Causal Optimization (MuPlon). MuPlon integrates a dual causal intervention strategy, consisting of the back-door path and front-door path. In the back-door path, MuPlon dilutes noisy node interference by optimizing node probability weights, while simultaneously strengthening the connections between relevant evidence nodes. In the front-door path, MuPlon extracts highly relevant subgraphs and constructs reasoning paths, further applying counterfactual reasoning to eliminate data biases within these paths. The experimental results demonstrate that MuPlon outperforms existing methods and achieves state-of-the-art performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25715v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanghui Guo, Shimin Di, Pasquale De Meo, Zhangze Chen, Jia Zhu</dc:creator>
    </item>
    <item>
      <title>Characterization and Learning of Causal Graphs with Latent Confounders and Post-treatment Selection from Interventional Data</title>
      <link>https://arxiv.org/abs/2509.25800</link>
      <description>arXiv:2509.25800v1 Announce Type: cross 
Abstract: Interventional causal discovery seeks to identify causal relations by leveraging distributional changes introduced by interventions, even in the presence of latent confounders. Beyond the spurious dependencies induced by latent confounders, we highlight a common yet often overlooked challenge in the problem due to post-treatment selection, in which samples are selectively included in datasets after interventions. This fundamental challenge widely exists in biological studies; for example, in gene expression analysis, both observational and interventional samples are retained only if they meet quality control criteria (e.g., highly active cells). Neglecting post-treatment selection may introduce spurious dependencies and distributional changes under interventions, which can mimic causal responses, thereby distorting causal discovery results and challenging existing causal formulations. To address this, we introduce a novel causal formulation that explicitly models post-treatment selection and reveals how its differential reactions to interventions can distinguish causal relations from selection patterns, allowing us to go beyond traditional equivalence classes toward the underlying true causal structure. We then characterize its Markov properties and propose a Fine-grained Interventional equivalence class, named FI-Markov equivalence, represented by a new graphical diagram, F-PAG. Finally, we develop a provably sound and complete algorithm, F-FCI, to identify causal relations, latent confounders, and post-treatment selection up to $\mathcal{FI}$-Markov equivalence, using both observational and interventional data. Experimental results on synthetic and real-world datasets demonstrate that our method recovers causal relations despite the presence of both selection and latent confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25800v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gongxu Luo, Loka Li, Guangyi Chen, Haoyue Dai, Kun Zhang</dc:creator>
    </item>
    <item>
      <title>Nonparametric inference under shape constraints: past, present and future</title>
      <link>https://arxiv.org/abs/2509.26040</link>
      <description>arXiv:2509.26040v1 Announce Type: cross 
Abstract: We survey the field of nonparametric inference under shape constraints, providing a historical overview and a perspective on its current state. An outlook and some open problems offer thoughts on future directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26040v1</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richard J. Samworth</dc:creator>
    </item>
    <item>
      <title>Spectral gap of Metropolis-within-Gibbs under log-concavity</title>
      <link>https://arxiv.org/abs/2509.26175</link>
      <description>arXiv:2509.26175v1 Announce Type: cross 
Abstract: The Metropolis-within-Gibbs (MwG) algorithm is a widely used Markov Chain Monte Carlo method for sampling from high-dimensional distributions when exact conditional sampling is intractable. We study MwG with Random Walk Metropolis (RWM) updates, using proposal variances tuned to match the target's conditional variances. Assuming the target $\pi$ is a $d$-dimensional log-concave distribution with condition number $\kappa$, we establish a spectral gap lower bound of order $\mathcal{O}(1/\kappa d)$ for the random-scan version of MwG, improving on the previously available $\mathcal{O}(1/\kappa^2 d)$ bound. This is obtained by developing sharp estimates of the conductance of one-dimensional RWM kernels, which can be of independent interest. The result shows that MwG can mix substantially faster with variance-adaptive proposals and that its mixing performance is just a constant factor worse than that of the exact Gibbs sampler, thus providing theoretical support to previously observed empirical behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26175v1</guid>
      <category>stat.ML</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cecilia Secchi, Giacomo Zanella</dc:creator>
    </item>
    <item>
      <title>Persuasion Effects in Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2509.26517</link>
      <description>arXiv:2509.26517v1 Announce Type: cross 
Abstract: We develop a framework for identifying and estimating persuasion effects in regression discontinuity (RD) designs. The RD persuasion rate measures the probability that individuals at the threshold would take the action if exposed to a persuasive message, given that they would not take the action without exposure. We present identification results for both sharp and fuzzy RD designs, derive sharp bounds under various data scenarios, and extend the analysis to local compliers. Estimation and inference rely on local polynomial regression, enabling straightforward implementation with standard RD tools. Applications to public health and media illustrate its empirical relevance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26517v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sung Jae Jun, Sokbae Lee</dc:creator>
    </item>
    <item>
      <title>Intraday FX Volatility-Curve Forecasting with Functional GARCH Approaches</title>
      <link>https://arxiv.org/abs/2311.18477</link>
      <description>arXiv:2311.18477v3 Announce Type: replace 
Abstract: This paper seeks to forecast intraday volatility curves for major foreign exchange (FX) currencies using functional GARCH models. Intraday return curves are observed at a daily frequency, yet preserve the full high-frequency trading structure, enabling volatility analysis at the intraday level. We demonstrate that the USD/EUR, USD/GBP, and USD/JPY intraday return curves exhibit strong cross-dependence, while individually they are serially uncorrelated but display long-range conditional heteroskedasticity. Embedding cross-currency dependence via multi-level functional principal component analysis and adding intraday bid-ask spread curves as exogenous drivers significantly improves intraday and day-ahead volatility forecasts relative to functional and realised-volatility baselines. The precise volatility forecasts motivate the construction of intraday Value-at-Risk (VaR). An intraday risk management application highlights that predicted intraday VaR curves can help mitigate dramatic losses in intraday trading strategies, showcasing their practical economic benefits in FX markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18477v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fearghal Kearney, Han Lin Shang, Yuqian Zhao</dc:creator>
    </item>
    <item>
      <title>Combining an experimental study with external data: study designs and identification strategies</title>
      <link>https://arxiv.org/abs/2406.03302</link>
      <description>arXiv:2406.03302v2 Announce Type: replace 
Abstract: There is increasing interest in combining information from experimental studies, including randomized and single-group trials, with information from external experimental or observational data sources. Such efforts are usually motivated by the desire to compare treatments evaluated in different studies -- for instance, through the introduction of external treatment groups -- or to estimate treatment effects with greater precision. Proposals to combine experimental studies with external data were made at least as early as the 1970s, but in recent years have come under increasing consideration by regulatory agencies involved in drug and device evaluation, particularly with the increasing availability of rich observational data. In this paper, we describe basic templates of study designs and data structures for combining information from experimental studies with external data, and use the potential (counterfactual) outcomes framework to elaborate identification strategies for potential outcome means and average treatment effects in these designs. In formalizing designs and identification strategies for combining information from experimental studies with external data, we hope to provide a conceptual foundation to support the systematic use and evaluation of such efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03302v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lawson Ung, Guanbo Wang, Sebastien Haneuse, Miguel A. Hern\'an, Issa J. Dahabreh</dc:creator>
    </item>
    <item>
      <title>A unified framework for multivariate two-sample and k-sample kernel-based quadratic distance goodness-of-fit tests</title>
      <link>https://arxiv.org/abs/2407.16374</link>
      <description>arXiv:2407.16374v2 Announce Type: replace 
Abstract: In the statistical literature, as well as in artificial intelligence and machine learning, measures of discrepancy between two probability distributions are largely used to develop measures of goodness-of-fit. We concentrate on quadratic distances, which depend on a non-negative definite kernel. We propose a unified framework for the study of two-sample and k-sample goodness of fit tests based on the concept of matrix distance. We provide a succinct review of the goodness of fit literature related to the use of distance measures, and specifically to quadratic distances. We show that the quadratic distance kernel-based two-sample test has the same functional form with the maximum mean discrepancy test. We develop tests for the $k$-sample scenario, where the two-sample problem is a special case. We derive their asymptotic distribution under the null hypothesis and discuss computational aspects of the test procedures. We assess their performance, in terms of level and power, via extensive simulations and a real data example. The proposed framework is implemented in the QuadratiK package, available in both R and Python environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16374v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Marianthi Markatou, Giovanni Saraceno</dc:creator>
    </item>
    <item>
      <title>Conformal inference for cell type annotation with graph-structured constraints</title>
      <link>https://arxiv.org/abs/2410.23786</link>
      <description>arXiv:2410.23786v2 Announce Type: replace 
Abstract: Conformal inference is a method that provides prediction sets for machine learning models, operating independently of the underlying distributional assumptions and relying solely on the exchangeability of training and test data. Despite its wide applicability and popularity, its application in graph-structured problems remains underexplored. This paper addresses this gap by developing an approach that leverages the rich information encoded in the graph structure of predicted classes to enhance the interpretability of conformal sets. Using a motivating example from genomics, specifically imaging-based spatial transcriptomics data and single-cell RNA sequencing data, we demonstrate how incorporating graph-structured constraints can improve the interpretation of cell type predictions. This approach aims to generate more coherent conformal sets that align with the inherent relationships among classes, facilitating clearer and more intuitive interpretations of model predictions. Additionally, we provide a technique to address non-exchangeability, particularly when the distribution of the response variable changes between training and test datasets. We implemented our method in the open-source R package scConform, available at https://github.com/ccb-hms/scConform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23786v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniela Corbetta, Livio Finos, Ludwig Geistlinger, Davide Risso</dc:creator>
    </item>
    <item>
      <title>New Axioms for Dependence Measurement and Powerful Tests</title>
      <link>https://arxiv.org/abs/2412.00066</link>
      <description>arXiv:2412.00066v2 Announce Type: replace 
Abstract: We build a context-free, comprehensive, flexible, and sound footing for measuring the dependence of two variables based on three new axioms, updating Renyi's (1959) seven postulates. We illustrate the superior footing of axioms by Vinod's (2014) asymmetric matrix of generalized correlation coefficients R*. We list five limitations explaining the poorer footing of axiom-failing Hellinger correlation proposed in 2022. We also describe a new implementation of a one-sided test with Taraldsen's (2021) exact density. This paper provides a new table for more powerful one-sided tests using the exact Taraldsen density and includes a published example where using Taraldsen's method makes a practical difference. The code to implement all our proposals is in R packages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00066v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hrishikesh D Vinod</dc:creator>
    </item>
    <item>
      <title>COADVISE: Covariate Adjustment with Variable Selection in Randomized Controlled Trials</title>
      <link>https://arxiv.org/abs/2501.08945</link>
      <description>arXiv:2501.08945v5 Announce Type: replace 
Abstract: Adjusting for covariates in randomized controlled trials can enhance the credibility and efficiency of treatment effect estimation. However, handling numerous covariates and their complex (non-linear) transformations poses a challenge. Motivated by the case study of the Best Apnea Interventions for Research (BestAIR) trial data from the National Sleep Research Resource (NSRR), where the number of covariates (p=114) is comparable to the sample size (N=196), we propose a principled Covariate Adjustment with Variable Selection (COADVISE) framework. COADVISE enables variable selection for covariates most relevant to the outcome while accommodating both linear and nonlinear adjustments. This framework ensures consistent estimates with improved efficiency over unadjusted estimators and provides robust variance estimation, even under outcome model misspecification. We demonstrate efficiency gains through theoretical analysis, extensive simulations, and a re-analysis of the BestAIR trial data to compare alternative variable selection strategies, offering cautionary recommendations. A user-friendly R package, Coadvise, is available to facilitate practical implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08945v5</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Liu, Ke Zhu, Larry Han, Shu Yang</dc:creator>
    </item>
    <item>
      <title>Estimation of relative risk, odds ratio and their logarithms with guaranteed accuracy and controlled sample size ratio</title>
      <link>https://arxiv.org/abs/2503.04876</link>
      <description>arXiv:2503.04876v2 Announce Type: replace 
Abstract: Given two populations from which independent binary observations are taken with parameters $p_1$ and $p_2$ respectively, estimators are proposed for the relative risk $p_1/p_2$, the odds ratio $p_1(1-p_2)/(p_2(1-p_1))$ and their logarithms. The sampling strategy used by the estimators is based on two-stage sequential sampling applied to each population, where the sample sizes of the second stage are computed from the results observed in the first stage. The estimators guarantee that the relative mean-square error, or the mean-square error for the logarithmic versions, is less than a target value for any $p_1, p_2 \in (0,1)$, and the ratio of average sample sizes from the two populations is close to a prescribed value. The estimators can also be used with group sampling, whereby samples are taken in batches of fixed size from the two populations simultaneously, each batch containing samples from the two populations. The efficiency of the estimators with respect to the Cram\'er-Rao bound is good, and in particular it is close to $1$ for small values of the target error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04876v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luis Mendo</dc:creator>
    </item>
    <item>
      <title>A Statistical Framework for Co-Mediators of Zero-Inflated Single-Cell RNA-Seq Data</title>
      <link>https://arxiv.org/abs/2507.06113</link>
      <description>arXiv:2507.06113v3 Announce Type: replace 
Abstract: Single-cell RNA sequencing (scRNA-seq) has revolutionized the study of cellular heterogeneity, enabling detailed molecular profiling at the individual cell level. However, integrating high-dimensional single-cell data into causal mediation analysis remains challenging due to zero inflation and complex mediator structures. We propose a novel mediation framework leveraging zero-inflated negative binomial models to characterize cell-level mediator distributions and beta regression for zero-inflation proportions. The model can identify expression level as well as expressed proportion that could mediate disease-leading causal pathway. Extensive simulation studies demonstrate improved power and controlled false discovery rates. We further illustrate the utility of this approach through application to ROSMAP single-cell transcriptomic data, uncovering biologically meaningful mediation effects that enhance understanding of disease mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06113v3</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seungjun Ahn, Li Chen, Maaike van Gerwen, Panos Roussos, Zhigang Li</dc:creator>
    </item>
    <item>
      <title>Mapping beyond diseases: Controlled variable selection for secondary phenotypes using tilted knockoffs</title>
      <link>https://arxiv.org/abs/2508.18548</link>
      <description>arXiv:2508.18548v2 Announce Type: replace 
Abstract: Researchers in biomedical studies often work with samples that are not selected uniformly at random from the population of interest, a major example being a case-control study. While these designs are motivated by specific scientific questions, it is often of interest to use the data collected to pursue secondary lines of investigations. In these cases, ignoring the fact that observations are not sampled uniformly at random can lead to spurious results. For example, in a case-control study, one might identify a spurious association between an exposure and a secondary phenotype when both affect the case-control status. This phenomenon is known as collider bias in the causal inference literature. While tests of independence under biased sampling are available, these methods typically do not apply when the number of variables is large.
  Here, we are interested in using the biased sample to select important exposures among a multitude of possible variables with replicability guarantees. While the model-X knockoff framework has been developed to test conditional independence hypotheses with False Discovery Rate (FDR) control, we show that its naive application fails to control FDR in the presence of biased sampling. We show how tilting the population distribution with the selection probability and constructing knockoff variables according to this tilted distribution instead leads to selection with FDR control. We study the FDR and power of the tilted knockoff method using simulated examples, and apply it to identify genetic underpinning of endophenotypes in a case-control study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18548v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qian Zhao, Susan Service, Carrie E. Bearden, Carlos Lopez-Jaramillo, Nelson Freimer, Chiara Sabatti</dc:creator>
    </item>
    <item>
      <title>A decision-theoretic framework for uncertainty quantification in epidemiological modelling</title>
      <link>https://arxiv.org/abs/2509.20013</link>
      <description>arXiv:2509.20013v2 Announce Type: replace 
Abstract: Estimating, understanding, and communicating uncertainty is fundamental to statistical epidemiology, where model-based estimates regularly inform real-world decisions. However, sources of uncertainty are rarely formalised, and existing classifications are often defined inconsistently. This lack of structure hampers interpretation, model comparison, and targeted data collection. Connecting ideas from machine learning, information theory, experimental design, and health economics, we present a first-principles decision-theoretic framework that defines uncertainty as the expected loss incurred by making an estimate based on incomplete information, arguing that this is a highly useful and practically relevant definition for epidemiology. We show how reasoning about future data leads to a notion of expected uncertainty reduction, which induces formal definitions of reducible and irreducible uncertainty. We demonstrate our approach using a case study of SARS-CoV-2 wastewater surveillance in Aotearoa New Zealand, estimating the uncertainty reduction if wastewater surveillance were expanded to the full population. We then connect our framework to relevant literature from adjacent fields, showing how it unifies and extends many of these ideas and how it allows these ideas to be applied to a wider range of models. Altogether, our framework provides a foundation for more reliable, consistent, and policy-relevant uncertainty quantification in infectious disease epidemiology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20013v2</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nicholas Steyn, Freddie Bickford Smith, Cathal Mills, Vik Shirvaikar, Christl A Donnelly, Kris V Parag</dc:creator>
    </item>
    <item>
      <title>An exploration of sequential Bayesian variable selection -- A comment on Garc\'{i}a-Donato et al. (2025). "Model uncertainty and missing data: An objective Bayesian perspective"</title>
      <link>https://arxiv.org/abs/2509.22901</link>
      <description>arXiv:2509.22901v2 Announce Type: replace 
Abstract: Our comment on Garc\'ia-Donato et al. (2025). "Model uncertainty and missing data: An objective Bayesian perspective" explores a further extension of the proposed methodology. Specifically, we consider the sequential setting where (potentially missing) data accumulate over time, with the goal of continuously monitoring statistical evidence, as opposed to assessing it only once data collection terminates. We explore a new variable selection method based on sequential model confidence sets, as proposed by Arnold et al. (2024), and show that it can help stabilise the inference of Garc\'ia-Donato et al. (2025). To be published as "Invited discussion" in Bayesian Analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22901v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebastian Arnold, Alexander Ly</dc:creator>
    </item>
    <item>
      <title>Sign and signed rank tests for paired functions</title>
      <link>https://arxiv.org/abs/2509.24170</link>
      <description>arXiv:2509.24170v2 Announce Type: replace 
Abstract: Simple nonparametric tests for paired functional data are an understudied area, despite recent advances in similar tests for other types of functional data. While the sign test has received limited treatment, the signed rank-type test has not previously been examined. The aim of the present work is to develop and evaluate these types of tests for functional data. We derive a simple, theoretical framework for both sign and signed rank tests for pairs of functions. In particular, we demonstrate that doubly ranked testing -- a newly developed framework for testing hypotheses involving functional data -- is a useful conduit for examining hypotheses regarding pairs o,f functions. We briefly examine the operating characteristics of all derived tests. We also use the described approaches to re-analyze pairs of functions from a randomized crossover study of heart health during simulated flight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24170v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark J. Meyer</dc:creator>
    </item>
    <item>
      <title>Collective Counterfactual Explanations: Balancing Individual Goals and Collective Dynamics</title>
      <link>https://arxiv.org/abs/2402.04579</link>
      <description>arXiv:2402.04579v2 Announce Type: replace-cross 
Abstract: Counterfactual explanations provide individuals with cost-optimal recommendations to achieve their desired outcomes. However, when a significant number of individuals seek similar state modifications, this individual-centric approach can inadvertently create competition and introduce unforeseen costs. Additionally, disregarding the underlying data distribution may lead to recommendations that individuals perceive as unusual or impractical. To address these challenges, we propose a novel framework that extends standard counterfactual explanations by incorporating a population dynamics model. This framework penalizes deviations from equilibrium after individuals follow the recommendations, effectively mitigating externalities caused by correlated changes across the population. By balancing individual modification costs with their impact on others, our method ensures more equitable and efficient outcomes. We show how this approach reframes the counterfactual explanation problem from an individual-centric task to a collective optimization problem. Augmenting our theoretical insights, we design and implement scalable algorithms for computing collective counterfactuals, showcasing their effectiveness and advantages over existing recourse methods, particularly in aligning with collective objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04579v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahmad-Reza Ehyaei, Ali Shirali, Samira Samadi</dc:creator>
    </item>
    <item>
      <title>A Dimension-Agnostic Bootstrap Anderson-Rubin Test For Instrumental Variable Regressions</title>
      <link>https://arxiv.org/abs/2412.01603</link>
      <description>arXiv:2412.01603v2 Announce Type: replace-cross 
Abstract: Weak-identification-robust tests for instrumental variable (IV) regressions are typically developed separately depending on whether the number of IVs is treated as fixed or increasing with the sample size, forcing researchers to make a stance on the asymptotic behavior, which is often ambiguous in practice. This paper proposes a bootstrap-based, dimension-agnostic Anderson-Rubin (AR) test that achieves correct asymptotic size regardless of whether the number of IVs is fixed or diverging, and even accommodates cases where the number of IVs exceeds the sample size. By incorporating ridge regularization, our approach reduces the effective rank of the projection matrix and yields regimes where the limiting distribution of the AR statistic can be a weighted chi-squared, a normal, or a mixture of the two. Strong approximation results ensure that the bootstrap procedure remains uniformly valid across all regimes, while also delivering substantial power gains over existing methods by exploiting rank reduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01603v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dennis Lim, Wenjie Wang, Yichong Zhang</dc:creator>
    </item>
    <item>
      <title>A Position Paper on the Automatic Generation of Machine Learning Leaderboards</title>
      <link>https://arxiv.org/abs/2505.17465</link>
      <description>arXiv:2505.17465v2 Announce Type: replace-cross 
Abstract: An important task in machine learning (ML) research is comparing prior work, which is often performed via ML leaderboards: a tabular overview of experiments with comparable conditions (e.g., same task, dataset, and metric). However, the growing volume of literature creates challenges in creating and maintaining these leaderboards. To ease this burden, researchers have developed methods to extract leaderboard entries from research papers for automated leaderboard curation. Yet, prior work varies in problem framing, complicating comparisons and limiting real-world applicability. In this position paper, we present the first overview of Automatic Leaderboard Generation (ALG) research, identifying fundamental differences in assumptions, scope, and output formats. We propose an ALG unified conceptual framework to standardise how the ALG task is defined. We offer ALG benchmarking guidelines, including recommendations for datasets and metrics that promote fair, reproducible evaluation. Lastly, we outline challenges and new directions for ALG, such as, advocating for broader coverage by including all reported results and richer metadata.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17465v2</guid>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roelien C Timmer, Yufang Hou, Stephen Wan</dc:creator>
    </item>
  </channel>
</rss>
