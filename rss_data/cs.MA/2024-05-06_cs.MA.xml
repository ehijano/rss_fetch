<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.MA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.MA</link>
    <description>cs.MA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.MA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 May 2024 04:01:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Enhancing Cooperation through Selective Interaction and Long-term Experiences in Multi-Agent Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2405.02654</link>
      <description>arXiv:2405.02654v1 Announce Type: new 
Abstract: The significance of network structures in promoting group cooperation within social dilemmas has been widely recognized. Prior studies attribute this facilitation to the assortment of strategies driven by spatial interactions. Although reinforcement learning has been employed to investigate the impact of dynamic interaction on the evolution of cooperation, there remains a lack of understanding about how agents develop neighbour selection behaviours and the formation of strategic assortment within an explicit interaction structure. To address this, our study introduces a computational framework based on multi-agent reinforcement learning in the spatial Prisoner's Dilemma game. This framework allows agents to select dilemma strategies and interacting neighbours based on their long-term experiences, differing from existing research that relies on preset social norms or external incentives. By modelling each agent using two distinct Q-networks, we disentangle the coevolutionary dynamics between cooperation and interaction. The results indicate that long-term experience enables agents to develop the ability to identify non-cooperative neighbours and exhibit a preference for interaction with cooperative ones. This emergent self-organizing behaviour leads to the clustering of agents with similar strategies, thereby increasing network reciprocity and enhancing group cooperation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02654v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyu Ren, Xiao-Jun Zeng</dc:creator>
    </item>
    <item>
      <title>Traffic Performance GPT (TP-GPT): Real-Time Data Informed Intelligent ChatBot for Transportation Surveillance and Management</title>
      <link>https://arxiv.org/abs/2405.03076</link>
      <description>arXiv:2405.03076v1 Announce Type: new 
Abstract: The digitization of traffic sensing infrastructure has significantly accumulated an extensive traffic data warehouse, which presents unprecedented challenges for transportation analytics. The complexities associated with querying large-scale multi-table databases require specialized programming expertise and labor-intensive development. Additionally, traditional analysis methods have focused mainly on numerical data, often neglecting the semantic aspects that could enhance interpretability and understanding. Furthermore, real-time traffic data access is typically limited due to privacy concerns. To bridge this gap, the integration of Large Language Models (LLMs) into the domain of traffic management presents a transformative approach to addressing the complexities and challenges inherent in modern transportation systems. This paper proposes an intelligent online chatbot, TP-GPT, for efficient customized transportation surveillance and management empowered by a large real-time traffic database. The innovative framework leverages contextual and generative intelligence of language models to generate accurate SQL queries and natural language interpretations by employing transportation-specialized prompts, Chain-of-Thought prompting, few-shot learning, multi-agent collaboration strategy, and chat memory. Experimental study demonstrates that our approach outperforms state-of-the-art baselines such as GPT-4 and PaLM 2 on a challenging traffic-analysis benchmark TransQuery. TP-GPT would aid researchers and practitioners in real-time transportation surveillance and management in a privacy-preserving, equitable, and customizable manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03076v1</guid>
      <category>cs.MA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bingzhang Wang (Joey),  Zhiyu (Joey),  Cai, Muhammad Monjurul Karim, Chenxi Liu, Yinhai Wang</dc:creator>
    </item>
    <item>
      <title>A Multi-Agent Rollout Approach for Highway Bottleneck Decongenston in Mixed Autonomy</title>
      <link>https://arxiv.org/abs/2405.03132</link>
      <description>arXiv:2405.03132v1 Announce Type: new 
Abstract: The integration of autonomous vehicles (AVs) into the existing transportation infrastructure offers a promising solution to alleviate congestion and enhance mobility. This research explores a novel approach to traffic optimization by employing a multi-agent rollout approach within a mixed autonomy environment. The study concentrates on coordinating the speed of human-driven vehicles by longitudinally controlling AVs, aiming to dynamically optimize traffic flow and alleviate congestion at highway bottlenecks in real-time. We model the problem as a decentralized partially observable Markov decision process (Dec-POMDP) and propose an improved multi-agent rollout algorithm. By employing agent-by-agent policy iterations, our approach implicitly considers cooperation among multiple agents and seamlessly adapts to complex scenarios where the number of agents dynamically varies. Validated in a real-world network with varying AV penetration rates and traffic flow, the simulations demonstrate that the multi-agent rollout algorithm significantly enhances performance, reducing average travel time on bottleneck segments by 9.42% with a 10% AV penetration rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03132v1</guid>
      <category>cs.MA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lu Liu, Maonan Wang, Man-On Pun, Xi Xiong</dc:creator>
    </item>
    <item>
      <title>ROS2swarm - A ROS 2 Package for Swarm Robot Behaviors</title>
      <link>https://arxiv.org/abs/2405.02438</link>
      <description>arXiv:2405.02438v1 Announce Type: cross 
Abstract: Developing reusable software for mobile robots is still challenging. Even more so for swarm robots, despite the desired simplicity of the robot controllers. Prototyping and experimenting are difficult due to the multi-robot setting and often require robot-robot communication. Also, the diversity of swarm robot hardware platforms increases the need for hardware-independent software concepts. The main advantages of the commonly used robot software architecture ROS 2 are modularity and platform independence. We propose a new ROS 2 package, ROS2swarm, for applications of swarm robotics that provides a library of ready-to-use swarm behavioral primitives. We show the successful application of our approach on three different platforms, the TurtleBot3 Burger, the TurtleBot3 Waffle Pi, and the Jackal UGV, and with a set of different behavioral primitives, such as aggregation, dispersion, and collective decision-making. The proposed approach is easy to maintain, extendable, and has good potential for simplifying swarm robotics experiments in future applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02438v1</guid>
      <category>cs.RO</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ICRA46639.2022.9812417</arxiv:DOI>
      <dc:creator>Tanja Katharina Kaiser, Marian Johannes Begemann, Tavia Plattenteich, Lars Schilling, Georg Schildbach, Heiko Hamann</dc:creator>
    </item>
    <item>
      <title>A Network Simulation of OTC Markets with Multiple Agents</title>
      <link>https://arxiv.org/abs/2405.02480</link>
      <description>arXiv:2405.02480v1 Announce Type: cross 
Abstract: We present a novel agent-based approach to simulating an over-the-counter (OTC) financial market in which trades are intermediated solely by market makers and agent visibility is constrained to a network topology. Dynamics, such as changes in price, result from agent-level interactions that ubiquitously occur via market maker agents acting as liquidity providers. Two additional agents are considered: trend investors use a deep convolutional neural network paired with a deep Q-learning framework to inform trading decisions by analysing price history; and value investors use a static price-target to determine their trade directions and sizes. We demonstrate that our novel inclusion of a network topology with market makers facilitates explorations into various market structures. First, we present the model and an overview of its mechanics. Second, we validate our findings via comparison to the real-world: we demonstrate a fat-tailed distribution of price changes, auto-correlated volatility, a skew negatively correlated to market maker positioning, predictable price-history patterns and more. Finally, we demonstrate that our network-based model can lend insights into the effect of market-structure on price-action. For example, we show that markets with sparsely connected intermediaries can have a critical point of fragmentation, beyond which the market forms distinct clusters and arbitrage becomes rapidly possible between the prices of different market makers. A discussion is provided on future work that would be beneficial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02480v1</guid>
      <category>econ.EM</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James T. Wilkinson, Jacob Kelter, John Chen, Uri Wilensky</dc:creator>
    </item>
    <item>
      <title>Innate Motivation for Robot Swarms by Minimizing Surprise: From Simple Simulations to Real-World Experiments</title>
      <link>https://arxiv.org/abs/2405.02579</link>
      <description>arXiv:2405.02579v1 Announce Type: cross 
Abstract: Applications of large-scale mobile multi-robot systems can be beneficial over monolithic robots because of higher potential for robustness and scalability. Developing controllers for multi-robot systems is challenging because the multitude of interactions is hard to anticipate and difficult to model. Automatic design using machine learning or evolutionary robotics seem to be options to avoid that challenge, but bring the challenge of designing reward or fitness functions. Generic reward and fitness functions seem unlikely to exist and task-specific rewards often have undesired side effects. Approaches of so-called innate motivation try to avoid the specific formulation of rewards and work instead with different drivers, such as curiosity. Our approach to innate motivation is to minimize surprise, which we implement by maximizing the accuracy of the swarm robot's sensor predictions using neuroevolution. A unique advantage of the swarm robot case is that swarm members populate the robot's environment and can trigger more active behaviors in a self-referential loop. We summarize our previous simulation-based results concerning behavioral diversity, robustness, scalability, and engineered self-organization, and put them into context. In several new studies, we analyze the influence of the optimizer's hyperparameters, the scalability of evolved behaviors, and the impact of realistic robot simulations. Finally, we present results using real robots that show how the reality gap can be bridged.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02579v1</guid>
      <category>cs.RO</category>
      <category>cs.MA</category>
      <category>cs.NE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TRO.2022.3181004</arxiv:DOI>
      <dc:creator>Tanja Katharina Kaiser, Heiko Hamann</dc:creator>
    </item>
    <item>
      <title>Communication Modalities</title>
      <link>https://arxiv.org/abs/2405.02606</link>
      <description>arXiv:2405.02606v1 Announce Type: cross 
Abstract: Epistemic analysis of distributed systems is one of the biggest successes among applications of logic in computer science. The reason for that is that agents' actions are necessarily guided by their knowledge. Thus, epistemic modal logic, with its knowledge and belief modalities (and group versions thereof), has played a vital role in establishing both impossibility results and necessary conditions for solvable distributed tasks. In distributed systems, knowledge is largely attained via communication. It has been standard in both distributed systems and dynamic epistemic logic to treat incoming messages as trustworthy, thus, creating difficulties in the epistemic analysis of byzantine distributed systems where faulty agents may lie. In this paper, we argue that handling such communication scenarios calls for additional modalities representing the informational content of messages that should not be taken at face value. We present two such modalities: hope for the case of fully byzantine agents and creed for non-uniform communication protocols in general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02606v1</guid>
      <category>cs.DC</category>
      <category>cs.LO</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roman Kuznets</dc:creator>
    </item>
    <item>
      <title>Linear Convergence of Independent Natural Policy Gradient in Games with Entropy Regularization</title>
      <link>https://arxiv.org/abs/2405.02769</link>
      <description>arXiv:2405.02769v1 Announce Type: cross 
Abstract: This work focuses on the entropy-regularized independent natural policy gradient (NPG) algorithm in multi-agent reinforcement learning. In this work, agents are assumed to have access to an oracle with exact policy evaluation and seek to maximize their respective independent rewards. Each individual's reward is assumed to depend on the actions of all the agents in the multi-agent system, leading to a game between agents. We assume all agents make decisions under a policy with bounded rationality, which is enforced by the introduction of entropy regularization. In practice, a smaller regularization implies the agents are more rational and behave closer to Nash policies. On the other hand, agents with larger regularization acts more randomly, which ensures more exploration. We show that, under sufficient entropy regularization, the dynamics of this system converge at a linear rate to the quantal response equilibrium (QRE). Although regularization assumptions prevent the QRE from approximating a Nash equilibrium, our findings apply to a wide range of games, including cooperative, potential, and two-player matrix games. We also provide extensive empirical results on multiple games (including Markov games) as a verification of our theoretical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02769v1</guid>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youbang Sun, Tao Liu, P. R. Kumar, Shahin Shahrampour</dc:creator>
    </item>
    <item>
      <title>Algorithmic collusion in a two-sided market: A rideshare example</title>
      <link>https://arxiv.org/abs/2405.02835</link>
      <description>arXiv:2405.02835v1 Announce Type: cross 
Abstract: With dynamic pricing on the rise, firms are using sophisticated algorithms for price determination. These algorithms are often non-interpretable and there has been a recent interest in their seemingly emergent ability to tacitly collude with each other without any prior communication whatsoever. Most of the previous works investigate algorithmic collusion on simple reinforcement learning (RL) based algorithms operating on a basic market model. Instead, we explore the collusive tendencies of Proximal Policy Optimization (PPO), a state-of-the-art continuous state/action space RL algorithm, on a complex double-sided hierarchical market model of rideshare. For this purpose, we extend a mathematical program network (MPN) based rideshare model to a temporal multi origin-destination setting and use PPO to solve for a repeated duopoly game. Our results indicate that PPO can either converge to a competitive or a collusive equilibrium depending upon the underlying market characteristics, even when the hyper-parameters are held constant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02835v1</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pravesh Koirala, Forrest Laine</dc:creator>
    </item>
    <item>
      <title>Modelling Opaque Bilateral Market Dynamics in Financial Trading: Insights from a Multi-Agent Simulation Study</title>
      <link>https://arxiv.org/abs/2405.02849</link>
      <description>arXiv:2405.02849v1 Announce Type: cross 
Abstract: Exploring complex adaptive financial trading environments through multi-agent based simulation methods presents an innovative approach within the realm of quantitative finance. Despite the dominance of multi-agent reinforcement learning approaches in financial markets with observable data, there exists a set of systematically significant financial markets that pose challenges due to their partial or obscured data availability. We, therefore, devise a multi-agent simulation approach employing small-scale meta-heuristic methods. This approach aims to represent the opaque bilateral market for Australian government bond trading, capturing the bilateral nature of bank-to-bank trading, also referred to as "over-the-counter" (OTC) trading, and commonly occurring between "market makers". The uniqueness of the bilateral market, characterized by negotiated transactions and a limited number of agents, yields valuable insights for agent-based modelling and quantitative finance. The inherent rigidity of this market structure, which is at odds with the global proliferation of multilateral platforms and the decentralization of finance, underscores the unique insights offered by our agent-based model. We explore the implications of market rigidity on market structure and consider the element of stability, in market design. This extends the ongoing discourse on complex financial trading environments, providing an enhanced understanding of their dynamics and implications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02849v1</guid>
      <category>q-fin.CP</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alicia Vidler, Toby Walsh</dc:creator>
    </item>
    <item>
      <title>Self-Organized Construction by Minimal Surprise</title>
      <link>https://arxiv.org/abs/2405.02980</link>
      <description>arXiv:2405.02980v1 Announce Type: cross 
Abstract: For the robots to achieve a desired behavior, we can program them directly, train them, or give them an innate driver that makes the robots themselves desire the targeted behavior. With the minimal surprise approach, we implant in our robots the desire to make their world predictable. Here, we apply minimal surprise to collective construction. Simulated robots push blocks in a 2D torus grid world. In two variants of our experiment we either allow for emergent behaviors or predefine the expected environment of the robots. In either way, we evolve robot behaviors that move blocks to structure their environment and make it more predictable. The resulting controllers can be applied in collective construction by robots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02980v1</guid>
      <category>cs.RO</category>
      <category>cs.MA</category>
      <category>cs.NE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/FAS-W.2019.00057</arxiv:DOI>
      <dc:creator>Tanja Katharina Kaiser, Heiko Hamann</dc:creator>
    </item>
    <item>
      <title>A survey to measure cognitive biases influencing mobility choices</title>
      <link>https://arxiv.org/abs/2405.03250</link>
      <description>arXiv:2405.03250v1 Announce Type: cross 
Abstract: In this paper, we describe a survey about the perceptions of 4 mobility modes (car, bus, bicycle, walking) and the preferences of users for 6 modal choice factors. This survey has gathered 650 answers in 2023, that are published as open data. In this study, we analyse these results to highlight the influence of 3 cognitive biases on mobility decisions: halo bias, choice-supportive bias, and reactance. These cognitive biases are proposed as plausible explanations of the observed behaviour, where the population tends to stick to individual cars despite urban policies aiming at favouring soft mobility. This model can serve as the basis for a simulator of mobility decisions in a virtual town, and the gathered data can be used to initialise this population with realistic attributes. Work is ongoing to design a simulation-based serious game where the player takes the role of an urban manager faced with planning choices to make their city more sustainable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03250v1</guid>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Carole Adam</dc:creator>
    </item>
    <item>
      <title>Multicopy Reinforcement Learning Agents</title>
      <link>https://arxiv.org/abs/2309.10908</link>
      <description>arXiv:2309.10908v2 Announce Type: replace 
Abstract: This paper examines a novel type of multi-agent problem, in which an agent makes multiple identical copies of itself in order to achieve a single agent task better or more efficiently. This strategy improves performance if the environment is noisy and the task is sometimes unachievable by a single agent copy. We propose a learning algorithm for this multicopy problem which takes advantage of the structure of the value function to efficiently learn how to balance the advantages and costs of adding additional copies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10908v2</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alicia P. Wolfe, Oliver Diamond, Brigitte Goeler-Slough, Remi Feuerman, Magdalena Kisielinska, Victoria Manfredi</dc:creator>
    </item>
    <item>
      <title>mABC: multi-Agent Blockchain-Inspired Collaboration for root cause analysis in micro-services architecture</title>
      <link>https://arxiv.org/abs/2404.12135</link>
      <description>arXiv:2404.12135v2 Announce Type: replace 
Abstract: The escalating complexity of micro-services architecture in cloud-native technologies poses significant challenges for maintaining system stability and efficiency. To conduct root cause analysis (RCA) and resolution of alert events, we propose a pioneering framework, multi-Agent Blockchain-inspired Collaboration for root cause analysis in micro-services architecture (mABC), to revolutionize the AI for IT operations (AIOps) domain, where multiple agents based on the powerful large language models (LLMs) perform blockchain-inspired voting to reach a final agreement following a standardized process for processing tasks and queries provided by Agent Workflow. Specifically, seven specialized agents derived from Agent Workflow each provide valuable insights towards root cause analysis based on their expertise and the intrinsic software knowledge of LLMs collaborating within a decentralized chain. To avoid potential instability issues in LLMs and fully leverage the transparent and egalitarian advantages inherent in a decentralized structure, mABC adopts a decision-making process inspired by blockchain governance principles while considering the contribution index and expertise index of each agent. Experimental results on the public benchmark AIOps challenge dataset and our created train-ticket dataset demonstrate superior performance in accurately identifying root causes and formulating effective solutions, compared to previous strong baselines. The ablation study further highlights the significance of each component within mABC, with Agent Workflow, multi-agent, and blockchain-inspired voting being crucial for achieving optimal performance. mABC offers a comprehensive automated root cause analysis and resolution in micro-services architecture and achieves a significant improvement in the AIOps domain compared to existing baselines</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12135v2</guid>
      <category>cs.MA</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Zhang, Hongcheng Guo, Jian Yang, Yi Zhang, Chaoran Yan, Zhoujin Tian, Hangyuan Ji, Zhoujun Li, Tongliang Li, Tieqiao Zheng, Chao Chen, Yi Liang, Xu Shi, Liangfan Zheng, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>Simulation of Crowd Egress with Environmental Stressors</title>
      <link>https://arxiv.org/abs/2206.01393</link>
      <description>arXiv:2206.01393v4 Announce Type: replace-cross 
Abstract: This article introduces a modeling framework to characterize evacuee response to environmental stimuli during emergency egress. The model is developed in consistency with stress theory, which explains how an organism reacts to environmental stressors. We integrate the theory into the well-known social-force model, and develop a framework to simulate crowd evacuation behavior in multi-compartment buildings. Our method serves as a theoretical basis to study crowd movement at bottlenecks, and simulate their herding and way-finding behavior in normal and hazardous conditions. The pre-movement behavior is also briefly investigated by using opinion dynamics with social group model. The algorithms have been partly tested in FDS+EVAC as well as our simulation platform crowdEgress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.01393v4</guid>
      <category>physics.soc-ph</category>
      <category>cs.MA</category>
      <category>nlin.AO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Wang, Xiaoda Wang, Peter Luh, Christian Wilkie, Timo Korhonen, Neal Olderman</dc:creator>
    </item>
    <item>
      <title>Parallel Optimization with Hard Safety Constraints for Cooperative Planning of Connected Autonomous Vehicles</title>
      <link>https://arxiv.org/abs/2303.03090</link>
      <description>arXiv:2303.03090v2 Announce Type: replace-cross 
Abstract: The development of connected autonomous vehicles (CAVs) facilitates the enhancement of traffic efficiency in complicated scenarios. In unsignalized roundabout scenarios, difficulties remain unsolved in developing an effective and efficient coordination strategy for CAVs. In this paper, we formulate the cooperative autonomous driving problem of CAVs in the roundabout scenario as a constrained optimal control problem, and propose a computationally-efficient parallel optimization framework to generate strategies for CAVs such that the travel efficiency is improved with hard safety guarantees. All constraints involved in the roundabout scenario are addressed appropriately with convex approximation, such that the convexity property of the reformulated optimization problem is exhibited. Then, a parallel optimization algorithm is presented to solve the reformulated optimization problem, where an embodied iterative nearest neighbor search strategy to determine the optimal passing sequence in the roundabout scenario. It is noteworthy that the travel efficiency in the roundabout scenario is enhanced and the computation burden is considerably alleviated with the innovation development. We also examine the proposed method in CARLA simulator and perform thorough comparisons with a rule-based baseline and the commonly used IPOPT optimization solver to demonstrate the effectiveness and efficiency of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.03090v2</guid>
      <category>cs.RO</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenmin Huang, Haichao Liu, Shaojie Shen, Jun Ma</dc:creator>
    </item>
    <item>
      <title>Collision Avoidance and Navigation for a Quadrotor Swarm Using End-to-end Deep Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2309.13285</link>
      <description>arXiv:2309.13285v2 Announce Type: replace-cross 
Abstract: End-to-end deep reinforcement learning (DRL) for quadrotor control promises many benefits -- easy deployment, task generalization and real-time execution capability. Prior end-to-end DRL-based methods have showcased the ability to deploy learned controllers onto single quadrotors or quadrotor teams maneuvering in simple, obstacle-free environments. However, the addition of obstacles increases the number of possible interactions exponentially, thereby increasing the difficulty of training RL policies. In this work, we propose an end-to-end DRL approach to control quadrotor swarms in environments with obstacles. We provide our agents a curriculum and a replay buffer of the clipped collision episodes to improve performance in obstacle-rich environments. We implement an attention mechanism to attend to the neighbor robots and obstacle interactions - the first successful demonstration of this mechanism on policies for swarm behavior deployed on severely compute-constrained hardware. Our work is the first work that demonstrates the possibility of learning neighbor-avoiding and obstacle-avoiding control policies trained with end-to-end DRL that transfers zero-shot to real quadrotors. Our approach scales to 32 robots with 80% obstacle density in simulation and 8 robots with 20% obstacle density in physical deployment. Video demonstrations are available on the project website at: https://sites.google.com/view/obst-avoid-swarm-rl.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13285v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhehui Huang, Zhaojing Yang, Rahul Krupani, Bask{\i}n \c{S}enba\c{s}lar, Sumeet Batra, Gaurav S. Sukhatme</dc:creator>
    </item>
    <item>
      <title>Peer Learning: Learning Complex Policies in Groups from Scratch via Action Recommendations</title>
      <link>https://arxiv.org/abs/2312.09950</link>
      <description>arXiv:2312.09950v2 Announce Type: replace-cross 
Abstract: Peer learning is a novel high-level reinforcement learning framework for agents learning in groups. While standard reinforcement learning trains an individual agent in trial-and-error fashion, all on its own, peer learning addresses a related setting in which a group of agents, i.e., peers, learns to master a task simultaneously together from scratch. Peers are allowed to communicate only about their own states and actions recommended by others: "What would you do in my situation?". Our motivation is to study the learning behavior of these agents. We formalize the teacher selection process in the action advice setting as a multi-armed bandit problem and therefore highlight the need for exploration. Eventually, we analyze the learning behavior of the peers and observe their ability to rank the agents' performance within the study group and understand which agents give reliable advice. Further, we compare peer learning with single agent learning and a state-of-the-art action advice baseline. We show that peer learning is able to outperform single-agent learning and the baseline in several challenging discrete and continuous OpenAI Gym domains. Doing so, we also show that within such a framework complex policies from action recommendations beyond discrete action spaces can evolve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.09950v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1609/aaai.v38i10.29061</arxiv:DOI>
      <arxiv:journal_reference>AAAI, vol. 38, no. 10, pp. 11766-11774, Mar. 2024</arxiv:journal_reference>
      <dc:creator>Cedric Derstroff, Mattia Cerrato, Jannis Brugger, Jan Peters, Stefan Kramer</dc:creator>
    </item>
    <item>
      <title>When Computing follows Vehicles: Decentralized Mobility-Aware Resource Allocation for Edge-to-Cloud Continuum</title>
      <link>https://arxiv.org/abs/2404.13179</link>
      <description>arXiv:2404.13179v2 Announce Type: replace-cross 
Abstract: The transformation of smart mobility is unprecedented--Autonomous, shared and electric connected vehicles, along with the urgent need to meet ambitious net-zero targets by shifting to low-carbon transport modalities result in new traffic patterns and requirements for real-time computation at large-scale, for instance, augmented reality applications. The cloud computing paradigm can neither respond to such low-latency requirements nor adapt resource allocation to such dynamic spatio-temporal service requests. This paper addresses this grand challenge by introducing a novel decentralized optimization framework for mobility-aware edge-to-cloud resource allocation, service offloading, provisioning and load-balancing. In contrast to related work, this framework comes with superior efficiency and cost-effectiveness under evaluation in real-world traffic settings and mobility datasets. This breakthrough capability of 'computing follows vehicles' proves able to reduce utilization variance by more than 40 times, while preventing service deadline violations by 14%-34%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13179v2</guid>
      <category>cs.DC</category>
      <category>cs.MA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeinab Nezami, Emmanouil Chaniotakis, Evangelos Pournaras</dc:creator>
    </item>
    <item>
      <title>Sample-Efficient Robust Multi-Agent Reinforcement Learning in the Face of Environmental Uncertainty</title>
      <link>https://arxiv.org/abs/2404.18909</link>
      <description>arXiv:2404.18909v2 Announce Type: replace-cross 
Abstract: To overcome the sim-to-real gap in reinforcement learning (RL), learned policies must maintain robustness against environmental uncertainties. While robust RL has been widely studied in single-agent regimes, in multi-agent environments, the problem remains understudied -- despite the fact that the problems posed by environmental uncertainties are often exacerbated by strategic interactions. This work focuses on learning in distributionally robust Markov games (RMGs), a robust variant of standard Markov games, wherein each agent aims to learn a policy that maximizes its own worst-case performance when the deployed environment deviates within its own prescribed uncertainty set. This results in a set of robust equilibrium strategies for all agents that align with classic notions of game-theoretic equilibria. Assuming a non-adaptive sampling mechanism from a generative model, we propose a sample-efficient model-based algorithm (DRNVI) with finite-sample complexity guarantees for learning robust variants of various notions of game-theoretic equilibria. We also establish an information-theoretic lower bound for solving RMGs, which confirms the near-optimal sample complexity of DRNVI with respect to problem-dependent factors such as the size of the state space, the target accuracy, and the horizon length.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18909v2</guid>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laixi Shi, Eric Mazumdar, Yuejie Chi, Adam Wierman</dc:creator>
    </item>
  </channel>
</rss>
