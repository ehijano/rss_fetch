<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Sep 2024 01:46:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The role of data embedding in quantum autoencoders for improved anomaly detection</title>
      <link>https://arxiv.org/abs/2409.04519</link>
      <description>arXiv:2409.04519v1 Announce Type: cross 
Abstract: The performance of Quantum Autoencoders (QAEs) in anomaly detection tasks is critically dependent on the choice of data embedding and ansatz design. This study explores the effects of three data embedding techniques, data re-uploading, parallel embedding, and alternate embedding, on the representability and effectiveness of QAEs in detecting anomalies. Our findings reveal that even with relatively simple variational circuits, enhanced data embedding strategies can substantially improve anomaly detection accuracy and the representability of underlying data across different datasets. Starting with toy examples featuring low-dimensional data, we visually demonstrate the effect of different embedding techniques on the representability of the model. We then extend our analysis to complex, higher-dimensional datasets, highlighting the significant impact of embedding methods on QAE performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04519v1</guid>
      <category>quant-ph</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jack Y. Araz, Michael Spannowsky</dc:creator>
    </item>
    <item>
      <title>Statistics for Differential Topological Properties between Data Sets with an Application to Reservoir Computers</title>
      <link>https://arxiv.org/abs/2409.04571</link>
      <description>arXiv:2409.04571v1 Announce Type: cross 
Abstract: It is common for researchers to record long, multiple time series from experiments or calculations. But sometimes there are no good models for the systems or no applicable mathematical theorems that can tell us when there are basic relationships between subsets of the time series data such as continuity, differentiability, embeddings, etc. The data is often higher dimensional and simple plotting will not guide us. At that point fitting the data to polynomials, Fourier series, etc. becomes uncertain. Even at the simplest level, having data that shows there is a function between the data subsets is useful and a negative answer means that more particular data fitting or analysis will be suspect and probably fail. We show here statistics that test time series subsets for basic mathematical properties and relations between them that not only indicate when more specific analyses are safe to do, but whether the systems are operating correctly. We apply these statistics to examples from reservoir computing where an important property of reservoir computers is that the reservoir system establishes an embedding of the drive system in order to make any other calculations with the reservoir computer successful.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04571v1</guid>
      <category>nlin.CD</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Louis Pecora, Thomas Carroll</dc:creator>
    </item>
    <item>
      <title>Multi-label Classification of Parameter Constraints in BSM Extensions using Deep Learning</title>
      <link>https://arxiv.org/abs/2409.05453</link>
      <description>arXiv:2409.05453v1 Announce Type: cross 
Abstract: The shortcomings of the Standard Model (SM) motivate its extension to accommodate new expected phenomena, such as dark matter and neutrino masses. However, such extensions are generally more complex due to the presence of a larger number of free parameters as well as additional phenomenology. Understanding how current theoretical and experimental constraints, individually and collectively, affect the parameter spaces of new models is of utmost importance in achieving testable predictions and targeted model-building that aims to solve certain issues. We present a comprehensive approach of using Deep Learning (DL) for the multi-label classification (MLC) of theoretical and experimental limits on the two-Higgs doublet model augmented by a real singlet (N2HDM), as a representative case. This approach can be generalized to any extension beyond the SM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05453v1</guid>
      <category>hep-ph</category>
      <category>hep-ex</category>
      <category>hep-th</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maien Binjonaid</dc:creator>
    </item>
    <item>
      <title>Analysis of neutron time-of-flight spectra with a Bayesian unfolding methodology</title>
      <link>https://arxiv.org/abs/2401.17348</link>
      <description>arXiv:2401.17348v3 Announce Type: replace 
Abstract: We have developed an innovative methodology for obtaining the neutron energy distribution from a time-of-flight (TOF) measurement based on the iterative Bayesian unfolding method and accurate Monte Carlo simulations. This methodology has been validated through the analysis of a realistic virtual $\beta$-decay experiment, including the most relevant systematic effects in a real experiment. The proposed methodology allowed for obtaining accurate results over the energy range above the neutron detection threshold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17348v3</guid>
      <category>physics.data-an</category>
      <category>nucl-ex</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>A. P\'erez de Rada Fiol, D. Cano-Ott, T. Mart\'inez, V. Alcayne, E. Mendoza, J. Plaza, A. Sanchez-Caballero, D. Villamar\'in</dc:creator>
    </item>
    <item>
      <title>Higher-order null models as a lens for social systems</title>
      <link>https://arxiv.org/abs/2402.18470</link>
      <description>arXiv:2402.18470v5 Announce Type: replace-cross 
Abstract: Despite the widespread adoption of higher-order mathematical structures such as hypergraphs, methodological tools for their analysis lag behind those for traditional graphs. This work addresses a critical gap in this context by proposing two micro-canonical random null models for directed hypergraphs: the Directed Hypergraph Configuration Model (DHCM) and the Directed Hypergraph JOINT Model (DHJM). These models preserve essential structural properties of directed hypergraphs such as node in- and out-degree sequences and hyperedge head and tail size sequences, or their joint tensor. We also describe two efficient MCMC algorithms, NuDHy-Degs and NuDHy-JOINT, to sample random hypergraphs from these ensembles.
  To showcase the interdisciplinary applicability of the proposed null models, we present three distinct use cases in sociology, epidemiology, and economics. First, we reveal the oscillatory behavior of increased homophily in opposition parties in the US Congress over a 40-year span, emphasizing the role of higher-order structures in quantifying political group homophily. Second, we investigate non-linear contagion in contact hyper-networks, demonstrating that disparities between simulations and theoretical predictions can be explained by considering higher-order joint degree distributions. Last, we examine the economic complexity of countries in the global trade network, showing that local network properties preserved by NuDHy explain the main structural economic complexity indexes.
  This work advances the development of null models for directed hypergraphs, addressing the intricate challenges posed by their complex entity relations, and providing a versatile suite of tools for researchers across various domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18470v5</guid>
      <category>cs.SI</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevX.14.031032</arxiv:DOI>
      <dc:creator>Giulia Preti, Adriano Fazzone, Giovanni Petri, Gianmarco De Francisci Morales</dc:creator>
    </item>
    <item>
      <title>OmniJet-$\alpha$: The first cross-task foundation model for particle physics</title>
      <link>https://arxiv.org/abs/2403.05618</link>
      <description>arXiv:2403.05618v2 Announce Type: replace-cross 
Abstract: Foundation models are multi-dataset and multi-task machine learning methods that once pre-trained can be fine-tuned for a large variety of downstream applications. The successful development of such general-purpose models for physics data would be a major breakthrough as they could improve the achievable physics performance while at the same time drastically reduce the required amount of training time and data.
  We report significant progress on this challenge on several fronts. First, a comprehensive set of evaluation methods is introduced to judge the quality of an encoding from physics data into a representation suitable for the autoregressive generation of particle jets with transformer architectures (the common backbone of foundation models). These measures motivate the choice of a higher-fidelity tokenization compared to previous works. Finally, we demonstrate transfer learning between an unsupervised problem (jet generation) and a classic supervised task (jet tagging) with our new OmniJet-$\alpha$ model. This is the first successful transfer between two different and actively studied classes of tasks and constitutes a major step in the building of foundation models for particle physics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05618v2</guid>
      <category>hep-ph</category>
      <category>cs.LG</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1088/2632-2153/ad66ad</arxiv:DOI>
      <arxiv:journal_reference>Mach. Learn.: Sci. Technol. 5 035031 (2024)</arxiv:journal_reference>
      <dc:creator>Joschka Birk, Anna Hallin, Gregor Kasieczka</dc:creator>
    </item>
    <item>
      <title>Graph Neural Network-Based Track Finding in the LHCb Vertex Detector</title>
      <link>https://arxiv.org/abs/2407.12119</link>
      <description>arXiv:2407.12119v2 Announce Type: replace-cross 
Abstract: The next decade will see an order of magnitude increase in data collected by high-energy physics experiments, driven by the High-Luminosity LHC (HL-LHC). The reconstruction of charged particle trajectories (tracks) has always been a critical part of offline data processing pipelines. The complexity of HL-LHC data will however increasingly mandate track finding in all stages of an experiment's real-time processing. This paper presents a GNN-based track-finding pipeline tailored for the Run 3 LHCb experiment's vertex detector and benchmarks its physics performance and computational cost against existing classical algorithms on GPU architectures. A novelty of our work compared to existing GNN tracking pipelines is batched execution, in which the GPU evaluates the pipeline on hundreds of events in parallel. We evaluate the impact of neural-network quantisation on physics and computational performance, and comment on the outlook for GNN tracking algorithms for other parts of the LHCb track-finding pipeline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12119v2</guid>
      <category>physics.ins-det</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anthony Correia, Fotis I. Giasemis, Nabil Garroum, Vladimir Vava Gligorov, Bertrand Granado</dc:creator>
    </item>
  </channel>
</rss>
