<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 Oct 2025 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Differential Density Analysis in Single-Cell Genomics Using Specially Designed Exponential Families</title>
      <link>https://arxiv.org/abs/2510.24948</link>
      <description>arXiv:2510.24948v1 Announce Type: new 
Abstract: Recent advances in high-resolution sequencing have paved the way for population-scale analysis in single-cell RNA-sequencing (scRNA-seq) data. scRNA-seq data, in particular, have proven to be extremely powerful in profiling a variety of outcomes such as disease and aging. The abundance of scRNA-seq data makes it possible to model each individual's gene expression as a probability density across cells, offering a richer representation than summary statistics such as means or variances, and allowing for more nuanced group comparisons. To this end, we propose a model-agnostic framework for density estimation and inference based on specially designed exponential families~(SEF), which accommodates diverse underlying models without requiring prior specifications. The proposed method enables estimation and visualization for both individual-specific and group-level gene expression densities, as well as conducting formal hypothesis testing for expression density difference across groups of interest. It relies on relaxed assumptions with established asymptotic properties and a consistent covariance estimator for valid inference. Through simulation under various scenarios, the SEF-based approach demonstrates good error control and improved statistical power over competing methods,including pseudo-bulk tests and moment estimators. Application to a population-scale scRNA-seq dataset from patients with systemic lupus erythematosus identified genes and gene sets that are missed from pseudo-bulk based tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24948v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hanxuan Ye, Zachary Qian, Hongzhe Li</dc:creator>
    </item>
    <item>
      <title>Bayesian Spatial Point Process Modeling for Cluster Randomized Trials</title>
      <link>https://arxiv.org/abs/2510.24969</link>
      <description>arXiv:2510.24969v1 Announce Type: new 
Abstract: Cluster randomized trials (CRTs) offer a practical alternative for addressing logistical challenges and ensuring feasibility in community health, education, and prevention studies, even though randomized controlled trials are considered the gold standard in evaluating therapeutic interventions. Despite their utility, CRTs are often criticized for limited precision and complex modeling requirements. Advances in robust Bayesian methods and the incorporation of spatial correlation into CRT design and analysis remain relatively underdeveloped. This paper introduces a Bayesian spatial point process framework that models individuals nested within geographic clusters while explicitly accounting for spatial dependence. We demonstrate that conventional non-spatial models consistently underestimate uncertainty and lead to misleading inferences, whereas our spatial approach improves estimation stability, controls type I error, and enhances statistical power. Our results underscore the value and need for wider adoption of spatial methods in CRT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24969v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jooyeon Lee, M. S., Evan Kwiatkowski, Ph. D</dc:creator>
    </item>
    <item>
      <title>Bayesian Adaptive Polynomial Chaos Expansions</title>
      <link>https://arxiv.org/abs/2510.25036</link>
      <description>arXiv:2510.25036v1 Announce Type: new 
Abstract: Polynomial chaos expansions (PCE) are widely used for uncertainty quantification (UQ) tasks, particularly in the applied mathematics community. However, PCE has received comparatively less attention in the statistics literature, and fully Bayesian formulations remain rare, especially with implementations in R. Motivated by the success of adaptive Bayesian machine learning models such as BART, BASS, and BPPR, we develop a new fully Bayesian adaptive PCE method with an efficient and accessible R implementation: khaos. Our approach includes a novel proposal distribution that enables data-driven interaction selection, and supports a modified g-prior tailored to PCE structure. Through simulation studies and real-world UQ applications, we demonstrate that Bayesian adaptive PCE provides competitive performance for surrogate modeling, global sensitivity analysis, and ordinal regression tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25036v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kellin N. Rumsey, Devin Francom, Graham C. Gibson, J. Derek Tucker, Gabriel Huerta</dc:creator>
    </item>
    <item>
      <title>Designing a quasi-experiment to study the clinical impact of adaptive risk prediction models</title>
      <link>https://arxiv.org/abs/2510.25052</link>
      <description>arXiv:2510.25052v1 Announce Type: new 
Abstract: Clinical risk prediction is a valuable tool for guiding healthcare interventions toward those most likely to benefit. Yet, evaluating the pairing of a risk prediction model with an intervention using randomized controlled trials (RCTs) presents substantial challenges to today's healthcare systems. This makes quasi-experimental designs, which can offer nearly the same level of evidence as an RCT, an attractive alternative. However, existing quasi-experimental designs do not allow models and thresholds to adapt. As a result, they struggle to serve new populations, meet emerging trends, and address practical issues. To address this gap, we introduce regression discontinuity designs for evaluating risk prediction models paired with specific interventions in a modern healthcare system. In our designs, treatment is assigned when predicted risk crosses a defined threshold, with the design explicitly accommodating adaptations in both the model and threshold. We account for the interference that arises from these adaptations to estimate the local average treatment effect in a valid and efficient way. To that end, we characterize interference and provide sufficient conditions for identification. Estimators are introduced, and their performance is evaluated in a simulation that emulates how cardiovascular risk calculators could guide interventions in primary care settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25052v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valerie Odeh-Couvertier, Gabriel Zayas-Caban, Brian Patterson, Amy Cochran</dc:creator>
    </item>
    <item>
      <title>Bayesian probabilistic projections of proportions with limited data: An application to subnational contraceptive method supply shares</title>
      <link>https://arxiv.org/abs/2510.25153</link>
      <description>arXiv:2510.25153v1 Announce Type: new 
Abstract: Engaging the private sector in contraceptive method supply is critical for creating equitable, sustainable, and accessible healthcare systems. To achieve this, it is essential to understand where women obtain their modern contraceptives. While national-level estimates provide valuable insights into overall trends in contraceptive supply, they often obscure variation within and across subnational regions. Addressing localized needs has become increasingly important as countries adopt decentralized models for family planning services. Decentralization has also underscored the need for reliable subnational estimates of key family planning indicators. The absence of regularly collected subnational data has hindered effective monitoring and decision-making. To bridge this gap, we propose a novel approach that leverages latent attributes in Demographic and Health Survey (DHS) data to produce Bayesian probabilistic projections of contraceptive method supply shares (the proportions of modern contraceptive methods supplied by public and private sectors) with limited data. Our modeling framework is built on Bayesian hierarchical models. Using penalized splines to track public and private supply shares over time, we leverage the spatial nature of the data and incorporate a correlation structure between recent supply share observations at national and subnational levels. This framework contributes to the domain of subnational estimation of proportions in data-sparse settings, outperforming comparable and previous approaches. As decentralization continues to reshape family planning services, producing reliable subnational estimates of key indicators is increasingly vital for researchers and policymakers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25153v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hannah Comiskey, Niamh Cahill, Leontine Alkema, David Fraizer, Worapree Maneesoonthorn</dc:creator>
    </item>
    <item>
      <title>TabMGP: Martingale Posterior with TabPFN</title>
      <link>https://arxiv.org/abs/2510.25154</link>
      <description>arXiv:2510.25154v1 Announce Type: new 
Abstract: Bayesian inference provides principled uncertainty quantification but is often limited by challenges of prior elicitation, likelihood misspecification, and computational burden. The martingale posterior (MGP, Fong et al., 2023) offers an alternative, replacing prior-likelihood elicitation with a predictive rule - namely, a sequence of one-step-ahead predictive distributions - for forward data generation. The utility of MGPs depends on the choice of predictive rule, yet the literature has offered few compelling examples. Foundation transformers are well-suited here, as their autoregressive generation mirrors this forward simulation and their general-purpose design enables rich predictive modeling. We introduce TabMGP, an MGP built on TabPFN, a transformer foundation model that is currently state-of-the-art for tabular data. TabMGP produces credible sets with near-nominal coverage and often outperforms both existing MGP constructions and standard Bayes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25154v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenyon Ng, Edwin Fong, David T. Frazier, Jeremias Knoblauch, Susan Wei</dc:creator>
    </item>
    <item>
      <title>Improving time series estimation and prediction via transfer learning</title>
      <link>https://arxiv.org/abs/2510.25236</link>
      <description>arXiv:2510.25236v1 Announce Type: new 
Abstract: There are many time series in the literature with high dimension yet limited sample sizes, such as macroeconomic variables, and it is almost impossible to obtain efficient estimation and accurate prediction by using the corresponding datasets themselves. This paper fills the gap by introducing a novel representation-based transfer learning framework for vector autoregressive models, and information from related source datasets with rich observations can be leveraged to enhance estimation efficiency through representation learning. A two-stage regularized estimation procedure is proposed with well established non-asymptotic properties, and algorithms with alternating updates are suggested to search for the estimates. Our transfer learning framework can handle time series with varying sample sizes and asynchronous starting and/or ending time points, thereby offering remarkable flexibility in integrating information from diverse datasets. Simulation experiments are conducted to evaluate the finite-sample performance of the proposed methodology, and its usefulness is demonstrated by an empirical analysis on 20 macroeconomic variables from Japan and another nine countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25236v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchang Lin, Qianqian Zhu, Guodong Li</dc:creator>
    </item>
    <item>
      <title>Nonparametric bounds for vaccine effects in randomized trials</title>
      <link>https://arxiv.org/abs/2510.25296</link>
      <description>arXiv:2510.25296v1 Announce Type: new 
Abstract: Vaccine randomized trials are typically designed to be blinded, ensuring that the estimated vaccine efficacy (VE) reflects the immunological effect of the vaccine. When blinding is broken, however, the estimated VE reflects not only the immunological effect but also behavioral effects stemming from participants' awareness of their treatment status. Recent work has proposed alternative causal estimands to the standard VE to address this issue, but their point identification results require a strong assumption: the absence of unmeasured common causes of infection risk and participants' belief about whether they received the vaccine. Personality traits, for example, may plausibly violate this assumption. We relax this assumption and derive nonparametric causal bounds for different types of VE. We construct these bounds using two approaches: linear programming-based and monotonicity-based methods. We further consider several possible causal structures for vaccine trials and show how the nonparametric bounds differ across these scenarios. Finally, we illustrate the performance of the proposed bounds using fully synthetic data and a semi-synthetic data example based on a COVID-19 vaccine trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25296v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rachel Axelrod, Uri Obolski, Daniel Nevo</dc:creator>
    </item>
    <item>
      <title>Tuning-Free Sampling via Optimization on the Space of Probability Measures</title>
      <link>https://arxiv.org/abs/2510.25315</link>
      <description>arXiv:2510.25315v1 Announce Type: new 
Abstract: We introduce adaptive, tuning-free step size schedules for gradient-based sampling algorithms obtained as time-discretizations of Wasserstein gradient flows. The result is a suite of tuning-free sampling algorithms, including tuning-free variants of the unadjusted Langevin algorithm (ULA), stochastic gradient Langevin dynamics (SGLD), mean-field Langevin dynamics (MFLD), Stein variational gradient descent (SVGD), and variational gradient descent (VGD). More widely, our approach yields tuning-free algorithms for solving a broad class of stochastic optimization problems over the space of probability measures. Under mild assumptions (e.g., geodesic convexity and locally bounded stochastic gradients), we establish strong theoretical guarantees for our approach. In particular, we recover the convergence rate of optimally tuned versions of these algorithms up to logarithmic factors, in both nonsmooth and smooth settings. We then benchmark the performance of our methods against comparable existing approaches. Across a variety of tasks, our algorithms achieve similar performance to the optimal performance of existing algorithms, with no need to tune a step size parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25315v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louis Sharrock, Christopher Nemeth</dc:creator>
    </item>
    <item>
      <title>Asymmetric Huber Periodogram</title>
      <link>https://arxiv.org/abs/2510.25316</link>
      <description>arXiv:2510.25316v1 Announce Type: new 
Abstract: This paper introduces a novel spectral M-estimator, called the asymmetric Huber periodogram (AHP), for periodicity detection in time series. The AHP is constructed from trigonometric asymmetric Huber regression, where a specially designed check function is used to substitute the squared L2 norm that defines the ordinary periodogram (PG). The AHP is statistically more efficient than the quantile periodogram (QP), while offering a more comprehensive picture than the Huber periodogram (HP) by examining the data across the entire range of the asymmetric parameter. We prove the theoretical properties of the AHP and investigate the relationship between the AHP and the so-called asymmetric Huber spectrum (AHS). Finally, simulations and three real-world data examples demonstrate that the AHP's capability in detecting periodicity and its robustness against outliers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25316v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tianbo Chen</dc:creator>
    </item>
    <item>
      <title>Latent variable estimation with composite Hilbert space Gaussian processes</title>
      <link>https://arxiv.org/abs/2510.25371</link>
      <description>arXiv:2510.25371v1 Announce Type: new 
Abstract: We develop a scalable class of models for latent variable estimation using composite Gaussian processes, with a focus on derivative Gaussian processes. We jointly model multiple data sources as outputs to improve the accuracy of latent variable inference under a single probabilistic framework. Similarly specified exact Gaussian processes scale poorly with large datasets. To overcome this, we extend the recently developed Hilbert space approximation methods for Gaussian processes to obtain a reduced-rank representation of the composite covariance function through its spectral decomposition. Specifically, we derive and analyze the spectral decomposition of derivative covariance functions and further study their properties theoretically. Using these spectral decompositions, our methods easily scale up to data scenarios involving thousands of samples. We validate our methods in terms of latent variable estimation accuracy, uncertainty calibration, and inference speed across diverse simulation scenarios. Finally, using a real world case study from single-cell biology, we demonstrate the potential of our models in estimating latent cellular ordering given gene expression levels, thus enhancing our understanding of the underlying biological process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25371v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Soham Mukherjee, Javier Enrique Aguilar, Marcello Zago, Manfred Claassen, Paul-Christian B\"urkner</dc:creator>
    </item>
    <item>
      <title>Distributional Evaluation of Generative Models via Relative Density Ratio</title>
      <link>https://arxiv.org/abs/2510.25507</link>
      <description>arXiv:2510.25507v1 Announce Type: new 
Abstract: We propose a functional evaluation metric for generative models based on the relative density ratio (RDR) designed to characterize distributional differences between real and generated samples. We show that the RDR as a functional summary of the goodness-of-fit for the generative model, possesses several desirable theoretical properties. It preserves $\phi$-divergence between two distributions, enables sample-level evaluation that facilitates downstream investigations of feature-specific distributional differences, and has a bounded range that affords clear interpretability and numerical stability. Functional estimation of the RDR is achieved efficiently through convex optimization on the variational form of $\phi$-divergence. We provide theoretical convergence rate guarantees for general estimators based on M-estimator theory, as well as the convergence rates of neural network-based estimators when the true ratio is in the anisotropic Besov space. We demonstrate the power of the proposed RDR-based evaluation through numerical experiments on MNIST, CelebA64, and the American Gut project microbiome data. We show that the estimated RDR not only allows for an effective comparison of the overall performance of competing generative models, but it can also offer a convenient means of revealing the nature of the underlying goodness-of-fit. This enables one to assess support overlap, coverage, and fidelity while pinpointing regions of the sample space where generators concentrate and revealing the features that drive the most salient distributional differences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25507v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yuliang Xu, Yun Wei, Li Ma</dc:creator>
    </item>
    <item>
      <title>Robust variable selection for spatial point processes observed with noise</title>
      <link>https://arxiv.org/abs/2510.25550</link>
      <description>arXiv:2510.25550v1 Announce Type: new 
Abstract: We propose a method for variable selection in the intensity function of spatial point processes that combines sparsity-promoting estimation with noise-robust model selection. As high-resolution spatial data becomes increasingly available through remote sensing and automated image analysis, identifying spatial covariates that influence the localization of events is crucial to understand the underlying mechanism. However, results from automated acquisition techniques are often noisy, for example due to measurement uncertainties or detection errors, which leads to spurious displacements and missed events. We study the impact of such noise on sparse point-process estimation across different models, including Poisson and Thomas processes. To improve noise robustness, we propose to use stability selection based on point-process subsampling and to incorporate a non-convex best-subset penalty to enhance model-selection performance. In extensive simulations, we demonstrate that such an approach reliably recovers true covariates under diverse noise scenarios and improves both selection accuracy and stability. We then apply the proposed method to a forestry data set, analyzing the distribution of trees in relation to elevation and soil nutrients in a tropical rain forest. This shows the practical utility of the method, which provides a systematic framework for robust variable selection in spatial point-process models under noise, without requiring additional knowledge of the process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25550v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dominik Sturm, Ivo F. Sbalzarini</dc:creator>
    </item>
    <item>
      <title>Automatic selection of hyper-parameters via the use of softened profile likelihood</title>
      <link>https://arxiv.org/abs/2510.25632</link>
      <description>arXiv:2510.25632v1 Announce Type: new 
Abstract: We extend a heuristic method for automatic dimensionality selection, which maximizes a profile likelihood to identify "elbows" in scree plots. Our extension enables researchers to make automatic choices of multiple hyper-parameters simultaneously. To facilitate our extension to multi-dimensions, we propose a "softened" profile likelihood. We present two distinct parameterizations of our solution and demonstrate our approach on elastic nets, support vector machines, and neural networks. We also briefly discuss applications of our method to other data-analytic tasks than hyper-parameter selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25632v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gengyang Chen, Mu Zhu</dc:creator>
    </item>
    <item>
      <title>Existence and optimisation of the partial correlation graphical lasso</title>
      <link>https://arxiv.org/abs/2510.25712</link>
      <description>arXiv:2510.25712v1 Announce Type: new 
Abstract: The partial correlation graphical LASSO (PCGLASSO) is a penalised likelihood method for Gaussian graphical models which provides scale invariant sparse estimation of the precision matrix and improves upon the popular graphical LASSO method. However, the PCGLASSO suffers from computational challenges due to the non-convexity of its associated optimisation problem. This paper provides some important breakthroughs in the computation of the PCGLASSO. First, the existence of the PCGLASSO estimate is proven when the sample size is smaller than the dimension - a case in which the maximum likelihood estimate does not exist. This means that the PCGLASSO can be used with any Gaussian data. Second, a new alternating algorithm for computing the PCGLASSO is proposed and implemented in the R package PCGLASSO available at https://github.com/JackStorrorCarter/PCGLASSO. This was the first publicly available implementation of the PCGLASSO and provides competitive computation time for moderate dimension size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25712v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jack Storror Carter, Cesare Molinari</dc:creator>
    </item>
    <item>
      <title>Statistical Process Monitoring based on Functional Data Analysis</title>
      <link>https://arxiv.org/abs/2510.25742</link>
      <description>arXiv:2510.25742v1 Announce Type: new 
Abstract: In modern industrial settings, advanced acquisition systems allow for the collection of data in the form of profiles, that is, as functional relationships linking responses to explanatory variables. In this context, statistical process monitoring (SPM) aims to assess the stability of profiles over time in order to detect unexpected behavior. This review focuses on SPM methods that model profiles as functional data, i.e., smooth functions defined over a continuous domain, and apply functional data analysis (FDA) tools to address limitations of traditional monitoring techniques. A reference framework for monitoring multivariate functional data is first presented. This review then offers a focused survey of several recent FDA-based profile monitoring methods that extend this framework to address common challenges encountered in real-world applications. These include approaches that integrate additional functional covariates to enhance detection power, a robust method designed to accommodate outlying observations, a real-time monitoring technique for partially observed profiles, and two adaptive strategies that target the characteristics of the out-of-control distribution. These methods are all implemented in the R package funcharts, available on CRAN. Finally, a review of additional existing FDA-based profile monitoring methods is also presented, along with suggestions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25742v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabio Centofanti</dc:creator>
    </item>
    <item>
      <title>Human- vs. AI-generated tests: dimensionality and information accuracy in latent trait evaluation</title>
      <link>https://arxiv.org/abs/2510.24739</link>
      <description>arXiv:2510.24739v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) and large language models (LLMs) are increasingly used in social and psychological research. Among potential applications, LLMs can be used to generate, customise, or adapt measurement instruments. This study presents a preliminary investigation of AI-generated questionnaires by comparing two ChatGPT-based adaptations of the Body Awareness Questionnaire (BAQ) with the validated human-developed version. The AI instruments were designed with different levels of explicitness in content and instructions on construct facets, and their psychometric properties were assessed using a Bayesian Graded Response Model. Results show that although surface wording between AI and original items was similar, differences emerged in dimensionality and in the distribution of item and test information across latent traits. These findings illustrate the importance of applying statistical measures of accuracy to ensure the validity and interpretability of AI-driven tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24739v1</guid>
      <category>cs.HC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mario Angelelli, Morena Oliva, Serena Arima, Enrico Ciavolino</dc:creator>
    </item>
    <item>
      <title>How can methods for classifying and clustering trajectories be used for prevention trials? An example in Alzheimer's disease area</title>
      <link>https://arxiv.org/abs/2510.24751</link>
      <description>arXiv:2510.24751v1 Announce Type: cross 
Abstract: Background: Clinical trials are designed to prove the efficacy of an intervention by means of model-based approaches involving parametric hypothesis testing. Issues arise when no effect is observed in the study population. Indeed, an effect may be present in a subgroup and the statistical test cannot detect it. To investigate this possibility, we proposed to change the paradigm to a data-driven approach. We selected exploratory methods to provide another perspective on the data and to identify particular homogeneous subgroups of subjects within which an effect might be detected. In the setting of prevention trials, the endpoint is a trajectory of repeated measures. In the settings of prevention trials, the endpoint is a trajectory of repeated measures, which requires the use of methods that can take data autocorrelation into account. The primary aim of this work was to explore the applicability of different methods for clustering and classifying trajectories. Methods: The Multidomain Alzheimer Preventive Trial (MAPT) was a three-year randomized controlled trial with four parallel arms (NCT00672685). The primary outcome was a composite Z-score combining four cognitive tests. The data were analyzed by quadratic mixed effects model. This study was inconclusive. Exploratory analysis is therefore relevant to investigate the use of data-driven methods for trajectory classification. The methods used were unsupervised: k-means for longitudinal data, Hierarchical Cluster Analysis (HCA), graphic semiology, and supervised analysis with dichotomous classification according to responder status. Results: Using k-means for longitudinal data, three groups were obtained and one of these groups showed cognitive decline over the three years of follow-up. This method could be applied directly to the primary outcome, the composite Z-score with repeated observations over time. With the two others unsupervised methods, we were unable to process longitudinal data directly. It was therefore necessary to choose an indicator of change in trajectories and to consider the rate of change between two measurements. For the HCA method, Ward's aggregation was performed. The Euclidean distance and rates of change were applied for the graphic semiology method. Lastly, as there were no objective criteria to define responder status, we defined our responders based on clinical criteria. Discussion: In the princeps study, the prevention trial was found to be inconclusive, likely due to the heterogeneity of the population, which may have masked a treatment effect later identified in a refined subgroup of high Beta Amyloid subjects. So, we have adopted an alternative unsupervised approach to subject stratification based on their trajectories. We could then identify patterns of similar trajectories of cognitive decline and also highlight the potential problem of a large heterogeneity of the profiles, maybe due to the final endpoint considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24751v1</guid>
      <category>stat.AP</category>
      <category>q-bio.NC</category>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>C\'eline Bougel (CERPOP), S\'ebastien D\'ejean (IMT, UT3), Caroline Giulioli (CERPOP), Philippe Saint-Pierre (IMT, UT3), Nicolas Savy (IMT, UT3), Sandrine Andrieu (CERPOP)</dc:creator>
    </item>
    <item>
      <title>Dynamic Spatial Treatment Effects and Network Fragility: Theory and Evidence from European Banking</title>
      <link>https://arxiv.org/abs/2510.24775</link>
      <description>arXiv:2510.24775v1 Announce Type: cross 
Abstract: This paper develops and empirically implements a continuous functional framework for analyzing systemic risk in financial networks, building on the dynamic spatial treatment effect methodology established in our previous studies. We extend the Navier-Stokes-based approach from our previous studies to characterize contagion dynamics in the European banking system through the spectral properties of network evolution operators. Using high-quality bilateral exposure data from the European Banking Authority Transparency Exercise (2014-2023), we estimate the causal impact of the COVID-19 pandemic on network fragility using spatial difference-in-differences methods adapted from our previous studies. Our empirical analysis reveals that COVID-19 elevated network fragility, measured by the algebraic connectivity $\lambda_2$ of the system Laplacian, by 26.9% above pre-pandemic levels (95% CI: [7.4%, 46.5%], p&lt;0.05), with effects persisting through 2023. Paradoxically, this occurred despite a 46% reduction in the number of banks, demonstrating that consolidation increased systemic vulnerability by intensifying interconnectedness-consistent with theoretical predictions from continuous spatial dynamics. Our findings validate the key predictions from \citet{kikuchi2024dynamical}: treatment effects amplify over time through spatial spillovers, consolidation increases fragility when coupling strength rises, and systems exhibit structural hysteresis preventing automatic reversion to pre-shock equilibria. The results demonstrate the empirical relevance of continuous functional methods for financial stability analysis and provide new insights for macroprudential policy design. We propose network-based capital requirements targeting spectral centrality and stress testing frameworks incorporating diffusion dynamics to address the coupling externalities identified in our analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24775v1</guid>
      <category>econ.EM</category>
      <category>q-fin.GN</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Dual-Channel Technology Diffusion: Spatial Decay and Network Contagion in Supply Chain Networks</title>
      <link>https://arxiv.org/abs/2510.24781</link>
      <description>arXiv:2510.24781v1 Announce Type: cross 
Abstract: This paper develops a dual-channel framework for analyzing technology diffusion that integrates spatial decay mechanisms from continuous functional analysis with network contagion dynamics from spectral graph theory. Building on our previous studies, which establish Navier-Stokes-based approaches to spatial treatment effects and financial network fragility, we demonstrate that technology adoption spreads simultaneously through both geographic proximity and supply chain connections. Using comprehensive data on six technologies adopted by 500 firms over 2010-2023, we document three key findings. First, technology adoption exhibits strong exponential geographic decay with spatial decay rate $\kappa \approx 0.043$ per kilometer, implying a spatial boundary of $d^* \approx 69$ kilometers beyond which spillovers are negligible (R-squared = 0.99). Second, supply chain connections create technology-specific networks whose algebraic connectivity ($\lambda_2$) increases 300-380 percent as adoption spreads, with correlation between $\lambda_2$ and adoption exceeding 0.95 across all technologies. Third, traditional difference-in-differences methods that ignore spatial and network structure exhibit 61 percent bias in estimated treatment effects. An event study around COVID-19 reveals that network fragility increased 24.5 percent post-shock, amplifying treatment effects through supply chain spillovers in a manner analogous to financial contagion documented in our recent study. Our framework provides micro-foundations for technology policy: interventions have spatial reach of 69 kilometers and network amplification factor of 10.8, requiring coordinated geographic and supply chain targeting for optimal effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24781v1</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Topic Analysis with Side Information: A Neural-Augmented LDA Approach</title>
      <link>https://arxiv.org/abs/2510.24918</link>
      <description>arXiv:2510.24918v1 Announce Type: cross 
Abstract: Traditional topic models such as Latent Dirichlet Allocation (LDA) have been widely used to uncover latent structures in text corpora, but they often struggle to integrate auxiliary information such as metadata, user attributes, or document labels. These limitations restrict their expressiveness, personalization, and interpretability. To address this, we propose nnLDA, a neural-augmented probabilistic topic model that dynamically incorporates side information through a neural prior mechanism. nnLDA models each document as a mixture of latent topics, where the prior over topic proportions is generated by a neural network conditioned on auxiliary features. This design allows the model to capture complex nonlinear interactions between side information and topic distributions that static Dirichlet priors cannot represent. We develop a stochastic variational Expectation-Maximization algorithm to jointly optimize the neural and probabilistic components. Across multiple benchmark datasets, nnLDA consistently outperforms LDA and Dirichlet-Multinomial Regression in topic coherence, perplexity, and downstream classification. These results highlight the benefits of combining neural representation learning with probabilistic topic modeling in settings where side information is available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24918v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Biyi Fang, Kripa Rajshekhar, Truong Vo, Diego Klabjan</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of homogenized invariant measures from multiscale data via Hermite expansion</title>
      <link>https://arxiv.org/abs/2510.25521</link>
      <description>arXiv:2510.25521v1 Announce Type: cross 
Abstract: We consider the problem of density estimation in the context of multiscale Langevin diffusion processes, where a single-scale homogenized surrogate model can be derived. In particular, our aim is to learn the density of the invariant measure of the homogenized dynamics from a continuous-time trajectory generated by the full multiscale system. We propose a spectral method based on a truncated Fourier expansion with Hermite functions as orthonormal basis. The Fourier coefficients are computed directly from the data owing to the ergodic theorem. We prove that the resulting density estimator is robust and converges to the invariant density of the homogenized model as the scale separation parameter vanishes, provided the time horizon and the number of Fourier modes are suitably chosen in relation to the multiscale parameter. The accuracy and reliability of this methodology is further demonstrated through a series of numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25521v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaroslav I. Borodavka, Max Hirsch, Sebastian Krumscheid, Andrea Zanoni</dc:creator>
    </item>
    <item>
      <title>Hierarchical additive interaction modelling with Gaussian process prior and its efficient implementation for multidimensional grid data</title>
      <link>https://arxiv.org/abs/2305.07073</link>
      <description>arXiv:2305.07073v4 Announce Type: replace 
Abstract: Additive Gaussian process (GP) models offer flexible tools for modelling complex non-linear relationships and interaction effects among covariates. While most studies have focused on predictive performance, relatively little attention has been given to identifying the underlying interaction structure, which may be of scientific interest in many applications. In practice, the use of additive GP models in this context has been limited by the cubic computational cost and quadratic storage requirements of GP inference. This paper presents a fast hierarchical additive interaction GP model for multi-dimensional grid data. A hierarchical ANOVA decomposition kernel forms the foundation of our model, which incorporate main and interaction effects under the principle of marginality. Kernel centring ensures identifiability and provides a unique, interpretable decomposition of lower- and higher-order effects. For datasets forming a multi-dimensional grid, efficient implementation is achieved by exploiting the Kronecker product structure of the covariance matrix. Our contribution is to extend Kronecker-based computation to handle any interaction structure within the proposed class of hierarchical additive GP models, whereas previous methods were limited to separable or fully saturated cases. The benefits of the proposed approach are demonstrated through simulation studies and an application to high-frequency nitrogen dioxide concentration data in London.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.07073v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sahoko Ishida, Francesca Panero, Wicher Bergsma</dc:creator>
    </item>
    <item>
      <title>Replicable Bandits for Digital Health Interventions</title>
      <link>https://arxiv.org/abs/2407.15377</link>
      <description>arXiv:2407.15377v5 Announce Type: replace 
Abstract: Adaptive treatment assignment algorithms, such as bandit algorithms, are increasingly used in digital health intervention clinical trials. Frequently, the data collected from these trials is used to conduct causal inference and related data analyses to decide how to refine the intervention, and whether to roll-out the intervention more broadly. This work studies inference for estimands that depend on the adaptive algorithm itself; a simple example is the mean reward under the adaptive algorithm. Specifically, we investigate the replicability of statistical analyses concerning such estimands when using data from trials deploying adaptive treatment assignment algorithms. We demonstrate that many standard statistical estimators can be inconsistent and fail to be replicable across repetitions of the clinical trial, even as the sample size grows large. We show that this non-replicability is intimately related to properties of the adaptive algorithm itself. We introduce a formal definition of a "replicable bandit algorithm" and prove that under such algorithms, a wide variety of common statistical estimators are guaranteed to be consistent and asymptotically normal. We present both theoretical results and simulation studies based on a mobile health oral health self-care intervention. Our findings underscore the importance of designing adaptive algorithms with replicability in mind, especially for settings like digital health, where deployment decisions rely heavily on replicated evidence. We conclude by discussing open questions on the connections between algorithm design, statistical inference, and experimental replicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15377v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Statistical Science, 2025</arxiv:journal_reference>
      <dc:creator>Kelly W. Zhang, Nowell Closser, Anna L. Trella, Susan A. Murphy</dc:creator>
    </item>
    <item>
      <title>Variable selection in convex nonparametric least squares via structured Lasso: An application to the Swedish electricity distribution networks</title>
      <link>https://arxiv.org/abs/2409.01911</link>
      <description>arXiv:2409.01911v3 Announce Type: replace 
Abstract: We study the problem of variable selection in convex nonparametric least squares (CNLS). Whereas the least absolute shrinkage and selection operator (Lasso) is a popular technique for least squares, its variable selection performance is unknown in CNLS problems. In this work, we investigate the performance of the Lasso estimator and find out it is usually unable to select variables efficiently. Exploiting the unique structure of the subgradients in CNLS, we develop a structured Lasso method by combining $\ell_1$-norm and $\ell_{\infty}$-norm. The relaxed version of the structured Lasso is proposed for achieving model sparsity and predictive performance simultaneously, where we can control the two effects--variable selection and model shrinkage--using separate tuning parameters. A Monte Carlo study is implemented to verify the finite sample performance of the proposed approaches. We also use real data from Swedish electricity distribution networks to illustrate the effects of the proposed variable selection techniques. The results from the simulation and application confirm that the proposed structured Lasso performs favorably, generally leading to sparser and more accurate predictive models, relative to the conventional Lasso methods in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01911v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiqiang Liao, Zhaonan Qu</dc:creator>
    </item>
    <item>
      <title>Correcting nonignorable nonresponse bias in turnout estimation using callback data</title>
      <link>https://arxiv.org/abs/2504.14169</link>
      <description>arXiv:2504.14169v3 Announce Type: replace 
Abstract: Overestimation of turnout has long been an issue in election surveys, with nonresponse bias or voter overrepresentation identified as major sources of bias. However, adjusting for nonignorable nonresponse bias is substantially challenging. Based on the ANES Non-Response Follow-Up study concerning the 2020 U.S. presidential election, we investigate the role of callback data, i.e., records of contact attempts in the survey course, in adjusting for nonresponse bias in the estimation of turnout. We propose a stableness of resistance assumption to account for nonignorable missingness in the outcome, which states that the impact of the missing outcome on the response propensity is stable in the first two call attempts. Under this assumption and by integrating with covariates information from the census data, we establish identifiability and develop estimation methods for turnout. Our methods produce estimates very close to the official turnout and successfully capture the trend of declining willingness to vote as response reluctance increases. This work highlights the importance of adjusting for nonignorable nonresponse bias and demonstrates the potential of widely available callback data for political surveys.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14169v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Li, Naiwen Ying, Kendrick Qijun Li, Xu Shi, Wang Miao</dc:creator>
    </item>
    <item>
      <title>Efficient Adaptive Experimentation with Noncompliance</title>
      <link>https://arxiv.org/abs/2505.17468</link>
      <description>arXiv:2505.17468v2 Announce Type: replace 
Abstract: We study the problem of estimating the average treatment effect (ATE) in adaptive experiments where treatment can only be encouraged -- rather than directly assigned -- via a binary instrumental variable. Building on semiparametric efficiency theory, we derive the efficiency bound for ATE estimation under arbitrary, history-dependent instrument-assignment policies, and show it is minimized by a variance-aware allocation rule that balances outcome noise and compliance variability. Leveraging this insight, we introduce AMRIV -- an Adaptive, Multiply-Robust estimator for Instrumental-Variable settings with variance-optimal assignment. AMRIV pairs (i) an online policy that adaptively approximates the optimal allocation with (ii) a sequential, influence-function-based estimator that attains the semiparametric efficiency bound while retaining multiply-robust consistency. We establish asymptotic normality, explicit convergence rates, and anytime-valid asymptotic confidence sequences that enable sequential inference. Finally, we demonstrate the practical effectiveness of our approach through empirical studies, showing that adaptive instrument assignment, when combined with the AMRIV estimator, yields improved efficiency and robustness compared to existing baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17468v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miruna Oprescu, Brian M Cho, Nathan Kallus</dc:creator>
    </item>
    <item>
      <title>Debiased machine learning for combining probability and non-probability survey data</title>
      <link>https://arxiv.org/abs/2508.08948</link>
      <description>arXiv:2508.08948v2 Announce Type: replace 
Abstract: We consider the problem of estimating the finite population mean $\bar{Y}$ of an outcome variable $Y$ using data from a nonprobability sample and auxiliary information from a probability sample. Existing double robust (DR) estimators of this mean $\bar{Y}$ require the estimation of two nuisance functions: the conditional probability of selection into the nonprobability sample given covariates $X$ that are observed in both samples, and the conditional expectation of $Y$ given $X$. These nuisance functions can be estimated using parametric models, but the resulting estimator of $\bar{Y}$ will typically be biased if both parametric models are misspecified. It would therefore be advantageous to be able to use more flexible data-adaptive / machine-learning estimators of the nuisance functions. Here, we develop a general framework for the valid use of DR estimators of $\bar{Y}$ when the design of the probability sample uses sampling without replacement at the first stage and data-adaptive / machine-learning estimators are used for the nuisance functions. We prove that several DR estimators of $\bar{Y}$, including targeted maximum likelihood estimators, are asymptotically normally distributed when the estimators of the nuisance functions converge faster than the $n^{1/4}$ rate and cross-fitting is used. We present a simulation study that demonstrates good performance of these DR estimators compared to the corresponding DR estimators that rely on at least one correctly specified parametric model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08948v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaun Seaman</dc:creator>
    </item>
    <item>
      <title>From Stochasticity to Signal: A Bayesian Latent State Model for Reliable Measurement with LLMs</title>
      <link>https://arxiv.org/abs/2510.23874</link>
      <description>arXiv:2510.23874v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly used to automate classification tasks in business, such as analyzing customer satisfaction from text. However, the inherent stochasticity of LLMs, in terms of their tendency to produce different outputs for the same input, creates a significant measurement error problem that is often neglected with a single round of output, or addressed with ad-hoc methods like majority voting. Such naive approaches fail to quantify uncertainty and can produce biased estimates of population-level metrics. In this paper, we propose a principled solution by reframing LLM variability as a statistical measurement error problem and introducing a Bayesian latent state model to address it. Our model treats the true classification (e.g., customer dissatisfaction) as an unobserved latent variable and the multiple LLM ratings as noisy measurements of this state. This framework allows for the simultaneous estimation of the LLM's false positive and false negative error rates, the underlying base rate of the phenomenon in the population, the posterior probability of the true state for each individual observation, and the causal impact of a business intervention, if any, on the latent state. Through simulation studies, we demonstrate that our model accurately recovers true parameters where naive methods fail. We conclude that this methodology provides a general and reliable framework for converting noisy, probabilistic outputs from LLMs into accurate and actionable insights for scientific and business applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23874v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yichi Zhang, Ignacio Martinez</dc:creator>
    </item>
    <item>
      <title>Latent Factor Analysis in Short Panels</title>
      <link>https://arxiv.org/abs/2306.14004</link>
      <description>arXiv:2306.14004v3 Announce Type: replace-cross 
Abstract: We develop a pseudo maximum likelihood method for latent factor analysis in short panels without imposing sphericity nor Gaussianity. We derive an asymptotically uniformly most powerful invariant test for the number of factors. On a large panel of monthly U.S. stock returns, we separate month after month systematic and idiosyncratic risks in short subperiods of bear vs. bull market. We observe an uptrend in the paths of total and idiosyncratic volatilities. The systematic risk explains a large part of the cross-sectional total variance in bear markets but is not driven by a single factor and not spanned by observed factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14004v3</guid>
      <category>econ.EM</category>
      <category>q-fin.PR</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alain-Philippe Fortin, Patrick Gagliardini, Olivier Scaillet</dc:creator>
    </item>
    <item>
      <title>Transfer Learning for Kernel-based Regression</title>
      <link>https://arxiv.org/abs/2310.13966</link>
      <description>arXiv:2310.13966v3 Announce Type: replace-cross 
Abstract: In recent years, transfer learning has garnered significant attention. Its ability to leverage knowledge from related studies to improve generalization performance in a target study has made it highly appealing. This paper focuses on investigating the transfer learning problem within the context of nonparametric regression over a reproducing kernel Hilbert space. The aim is to bridge the gap between practical effectiveness and theoretical guarantees. We specifically consider two scenarios: one where the transferable sources are known and another where they are unknown. For the known transferable source case, we propose a two-step kernel-based estimator by solely using kernel ridge regression. For the unknown case, we develop a novel method based on an efficient aggregation algorithm, which can automatically detect and alleviate the effects of negative sources. This paper provides the statistical properties of the desired estimators and establishes the minimax rate. Through extensive numerical experiments on synthetic data and real examples, we validate our theoretical findings and demonstrate the effectiveness of our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13966v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chao Wang, Caixing Wang, Xin He, Xingdong Feng</dc:creator>
    </item>
    <item>
      <title>Knots and variance ordering of sequential Monte Carlo algorithms</title>
      <link>https://arxiv.org/abs/2510.01901</link>
      <description>arXiv:2510.01901v2 Announce Type: replace-cross 
Abstract: Sequential Monte Carlo algorithms, or particle filters, are widely used for approximating intractable integrals, particularly those arising in Bayesian inference and state-space models. We introduce a new variance reduction technique, the knot operator, which improves the efficiency of particle filters by incorporating potential function information into part, or all, of a transition kernel. The knot operator induces a partial ordering of Feynman-Kac models that implies an order on the asymptotic variance of particle filters, offering a new approach to algorithm design. We discuss connections to existing strategies for designing efficient particle filters, including model marginalisation. Our theory generalises such techniques and provides quantitative asymptotic variance ordering results. We revisit the fully-adapted (auxiliary) particle filter using our theory of knots to show how a small modification guarantees an asymptotic variance ordering for all relevant test functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01901v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua J Bon, Anthony Lee</dc:creator>
    </item>
  </channel>
</rss>
