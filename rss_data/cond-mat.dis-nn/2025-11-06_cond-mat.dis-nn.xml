<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Nov 2025 05:00:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Revisiting Nishimori multicriticality through the lens of information measures</title>
      <link>https://arxiv.org/abs/2511.02907</link>
      <description>arXiv:2511.02907v1 Announce Type: cross 
Abstract: The quantum error correction threshold is closely related to the Nishimori physics of random statistical models. We extend quantum information measures such as coherent information beyond the Nishimori line and establish them as sharp indicators of phase transitions. We derive exact inequalities for several generalized measures, demonstrating that each attains its extremum along the Nishimori line. Using a fermionic transfer matrix method, we compute these quantities in the 2d $\pm J$ random-bond Ising model-corresponding to a surface code under bit-flip noise-on system sizes up to $512$ and over $10^7$ disorder realizations. All critical points extracted from statistical and information-theoretic indicators coincide with high precision at $p_c=0.1092212(4)$, with the coherent information exhibiting the smallest finite-size effects. We further analyze the domain-wall free energy distribution and confirm its scale invariance at the multicritical point.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02907v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.str-el</category>
      <category>quant-ph</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhou-Quan Wan, Xu-Dong Dai, Guo-Yi Zhu</dc:creator>
    </item>
    <item>
      <title>Intrinsic viscous liquid dynamics</title>
      <link>https://arxiv.org/abs/2511.02991</link>
      <description>arXiv:2511.02991v1 Announce Type: cross 
Abstract: When liquids are cooled, their dynamics are slowed, and if crystallization is avoided, they will solidify into an amorphous structure referred to as a glass. Experiments show that chemically distinct glass-forming liquids have universal features of the spectrum and temperature dependence of the main structural relaxation. We introduce Randium, a generic energetically coarse-grained model of viscous liquids, and demonstrate that the intrinsic dynamics of viscous liquids emerges. These results suggest that Randium belongs to a universal class of systems whose dynamics capture the essential physics of viscous liquid relaxation, bridging microscopic molecular models and coarse-grained theoretical descriptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02991v1</guid>
      <category>cond-mat.soft</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.mtrl-sci</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ulf R. Pedersen</dc:creator>
    </item>
    <item>
      <title>Precise asymptotic analysis of Sobolev training for random feature models</title>
      <link>https://arxiv.org/abs/2511.03050</link>
      <description>arXiv:2511.03050v1 Announce Type: cross 
Abstract: Gradient information is widely useful and available in applications, and is therefore natural to include in the training of neural networks. Yet little is known theoretically about the impact of Sobolev training -- regression with both function and gradient data -- on the generalization error of highly overparameterized predictive models in high dimensions. In this paper, we obtain a precise characterization of this training modality for random feature (RF) models in the limit where the number of trainable parameters, input dimensions, and training data tend proportionally to infinity. Our model for Sobolev training reflects practical implementations by sketching gradient data onto finite dimensional subspaces. By combining the replica method from statistical physics with linearizations in operator-valued free probability theory, we derive a closed-form description for the generalization errors of the trained RF models. For target functions described by single-index models, we demonstrate that supplementing function data with additional gradient data does not universally improve predictive performance. Rather, the degree of overparameterization should inform the choice of training method. More broadly, our results identify settings where models perform optimally by interpolating noisy function and gradient data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03050v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katharine E Fisher, Matthew TC Li, Youssef Marzouk, Timo Schorlepp</dc:creator>
    </item>
    <item>
      <title>Symmetry Breaking and Mie-tronic Supermodes in Nonlocal Metasurfaces</title>
      <link>https://arxiv.org/abs/2511.03560</link>
      <description>arXiv:2511.03560v1 Announce Type: cross 
Abstract: Breaking symmetry in Mie-resonant metasurfaces challenges the conventional view that it weakens optical confinement. Within the Mie-tronics framework, we show that symmetry breaking can instead enhance light trapping by strengthening in-plane nonlocal coupling pathways. Through diffraction and multiple-scattering analyses, we demonstrate that diffractive bands and Mie-tronic supermodes originate from the same underlying Mie resonances but differ fundamentally in physical nature. Finite arrays exhibit Q-factor enhancement driven by redistributed radiation channels, reversing the trend predicted by infinite-lattice theory. We further show that controlled symmetry breaking opens new electromagnetic coupling channels, enabling polarization conversion in nonlocal metasurfaces. These findings establish a unified wave picture linking scattering and diffraction theories and outline design principles for multifunctional metasurfaces that exploit nonlocality for advanced light manipulation, computation, and emission control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03560v1</guid>
      <category>physics.optics</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.mes-hall</category>
      <category>physics.app-ph</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thanh Xuan Hoang, Ayan Nussupbekov, Jie Ji, Daniel Leykam, Jaime Gomez Rivas, Yuri Kivshar</dc:creator>
    </item>
    <item>
      <title>Circular Rosenzweig-Porter random matrix ensemble</title>
      <link>https://arxiv.org/abs/2111.08031</link>
      <description>arXiv:2111.08031v3 Announce Type: replace 
Abstract: The Rosenzweig-Porter random matrix ensemble serves as a qualitative phenomenological model for the level statistics and fractality of eigenstates across the many-body localization transition in static systems. We propose a unitary (circular) analogue of this ensemble, which similarly captures the phenomenology of many-body localization in periodically driven (Floquet) systems. We define this ensemble as the outcome of a Dyson Brownian motion process. We show numerical evidence that this ensemble shares some key statistical properties with the Rosenzweig-Porter ensemble for both the eigenvalues and the eigenstates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.08031v3</guid>
      <category>cond-mat.dis-nn</category>
      <category>quant-ph</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.21468/SciPostPhys.12.3.082</arxiv:DOI>
      <arxiv:journal_reference>SciPost Phys. 12, 082 (2022)</arxiv:journal_reference>
      <dc:creator>Wouter Buijsman, Yevgeny Bar Lev</dc:creator>
    </item>
    <item>
      <title>A note on the dynamics of extended-context disordered kinetic spin models</title>
      <link>https://arxiv.org/abs/2507.18461</link>
      <description>arXiv:2507.18461v2 Announce Type: replace 
Abstract: Inspired by striking advances in language modeling, there has recently been much interest in developing autogressive sequence models that are amenable to analytical study. In this short note, we consider extensions of simple disordered kinetic glass models from statistical physics. These models have tunable correlations, are easy to sample, and can be solved exactly when the state space dimension is large. In particular, we give an expository derivation of the dynamical mean field theories that describe their asymptotic statistics. We therefore propose that they constitute an interesting set of toy models for autoregressive sequence generation, in which one might study learning dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18461v2</guid>
      <category>cond-mat.dis-nn</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacob A. Zavatone-Veth, Cengiz Pehlevan</dc:creator>
    </item>
    <item>
      <title>How does training shape the Riemannian geometry of neural network representations?</title>
      <link>https://arxiv.org/abs/2301.11375</link>
      <description>arXiv:2301.11375v4 Announce Type: replace-cross 
Abstract: In machine learning, there is a long history of trying to build neural networks that can learn from fewer example data by baking in strong geometric priors. However, it is not always clear a priori what geometric constraints are appropriate for a given task. Here, we explore the possibility that one can uncover useful geometric inductive biases by studying how training molds the Riemannian geometry induced by unconstrained neural network feature maps. We first show that at infinite width, neural networks with random parameters induce highly symmetric metrics on input space. This symmetry is broken by feature learning: networks trained to perform classification tasks learn to magnify local areas along decision boundaries. This holds in deep networks trained on high-dimensional image classification tasks, and even in self-supervised representation learning. These results begin to elucidate how training shapes the geometry induced by unconstrained neural network feature maps, laying the groundwork for an understanding of this richly nonlinear form of feature learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.11375v4</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>stat.ML</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 3rd Workshop on Symmetry and Geometry in Neural Representations (NeurReps) (2025)</arxiv:journal_reference>
      <dc:creator>Jacob A. Zavatone-Veth, Sheng Yang, Julian A. Rubinfien, Cengiz Pehlevan</dc:creator>
    </item>
    <item>
      <title>Large Orders and Strong-Coupling Limit in Functional Renormalization</title>
      <link>https://arxiv.org/abs/2410.02361</link>
      <description>arXiv:2410.02361v2 Announce Type: replace-cross 
Abstract: We study the large-order behavior of the functional renormalization group (FRG). For a model in dimension zero, we establish Borel-summability for a large class of microscopic couplings. Writing the derivatives of FRG as contour integrals, we express the Borel-transform as well as the original series as integrals. Taking the strong-coupling limit in this representation, we show that all short-ranged microscopic disorders flow to the same universal fixed point. Our results are relevant for FRG in disordered elastic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02361v2</guid>
      <category>hep-th</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mikhail N. Semeikin, Kay Joerg Wiese</dc:creator>
    </item>
    <item>
      <title>Surmise for random matrices' level spacing distributions beyond nearest-neighbors</title>
      <link>https://arxiv.org/abs/2504.20134</link>
      <description>arXiv:2504.20134v2 Announce Type: replace-cross 
Abstract: Correlations between energy levels can help distinguish whether a many-body system is of integrable or chaotic nature. The study of short-range and long-range spectral correlations generally involves quantities which are very different, unless one uses the $k$-th nearest neighbor ($k$NN) level spacing distributions. For nearest-neighbor (NN) spectral spacings, the distribution in random matrices is well captured by the Wigner surmise. This well-known approximation, derived exactly for a 2$\times$2 matrix, is simple and satisfactorily describes the NN spacings of larger matrices. There have been attempts in the literature to generalize Wigner's surmise to further away neighbors. However, as we show, the current proposal in the literature fails to accurately capture numerical data. Using the known variance of the distributions from random matrix theory, we propose a corrected surmise for the $k$NN spectral distributions. This surmise better characterizes spectral correlations while retaining the simplicity of Wigner's surmise. We test the predictions against numerical results and show that the corrected surmise is systematically more accurate at capturing data from random matrices. Using the XXZ spin chain with random on-site disorder, we illustrate how these results can be used as a refined probe of many-body quantum chaos for both short- and long-range spectral correlations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20134v2</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1088/1751-8121/ae15ca</arxiv:DOI>
      <arxiv:journal_reference>J. Phys. A: Math. Theor. 58 445206 (2025)</arxiv:journal_reference>
      <dc:creator>Ruth Shir, Pablo Martinez-Azcona, Aur\'elia Chenu</dc:creator>
    </item>
    <item>
      <title>Recurrent Self-Attention Dynamics: An Energy-Agnostic Perspective from Jacobians</title>
      <link>https://arxiv.org/abs/2505.19458</link>
      <description>arXiv:2505.19458v4 Announce Type: replace-cross 
Abstract: The theoretical understanding of self-attention (SA) has been steadily progressing. A prominent line of work studies a class of SA layers that admit an energy function decreased by state updates. While it provides valuable insights into inherent biases in signal propagation, it often relies on idealized assumptions or additional constraints not necessarily present in standard SA. Thus, to broaden our understanding, this work aims to relax these energy constraints and provide an energy-agnostic characterization of inference dynamics by dynamical systems analysis. In more detail, we first consider relaxing the symmetry and single-head constraints traditionally required in energy-based formulations. Next, we show that analyzing the Jacobian matrix of the state is highly valuable when investigating more general SA architectures without necessarily admitting an energy function. It reveals that the normalization layer plays an essential role in suppressing the Lipschitzness of SA and the Jacobian's complex eigenvalues, which correspond to the oscillatory components of the dynamics. In addition, the Lyapunov exponents computed from the Jacobians demonstrate that the normalized dynamics lie close to a critical state, and this criticality serves as a strong indicator of high inference performance. Furthermore, the Jacobian perspective also enables us to develop regularization methods for training and a pseudo-energy for monitoring inference dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19458v4</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.NE</category>
      <category>stat.ML</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akiyoshi Tomihari, Ryo Karakida</dc:creator>
    </item>
    <item>
      <title>Neural Importance Resampling: A Practical Sampling Strategy for Neural Quantum States</title>
      <link>https://arxiv.org/abs/2507.20510</link>
      <description>arXiv:2507.20510v2 Announce Type: replace-cross 
Abstract: Neural quantum states (NQS) have emerged as powerful tools for simulating many-body quantum systems, but their practical use is often hindered by limitations of current sampling techniques. Markov chain Monte Carlo (MCMC) methods suffer from slow mixing and require manual tuning, while autoregressive NQS impose restrictive architectural constraints that complicate the enforcement of symmetries and the construction of determinant-based multi-state wave functions. In this work, we introduce Neural Importance Resampling (NIR), a new sampling algorithm that combines importance resampling with a separately trained autoregressive proposal network. This approach enables efficient and unbiased sampling without constraining the NQS architecture. We demonstrate that NIR supports stable and scalable training, including for multi-state NQS, and mitigates issues faced by MCMC and autoregressive approaches. Numerical experiments on the 2D transverse-field Ising and $J_1$-$J_2$ Heisenberg models show that NIR outperforms MCMC in challenging regimes and yields results competitive with state of the art methods. Our results establish NIR as a robust alternative for sampling in variational NQS algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20510v2</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eimantas Ledinauskas, Egidijus Anisimovas</dc:creator>
    </item>
  </channel>
</rss>
