<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Sep 2025 04:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Evaluating Large Language Models for Functional and Maintainable Code in Industrial Settings: A Case Study at ASML</title>
      <link>https://arxiv.org/abs/2509.12395</link>
      <description>arXiv:2509.12395v1 Announce Type: new 
Abstract: Large language models have shown impressive performance in various domains, including code generation across diverse open-source domains. However, their applicability in proprietary industrial settings, where domain-specific constraints and code interdependencies are prevalent, remains largely unexplored. We present a case study conducted in collaboration with the leveling department at ASML to investigate the performance of LLMs in generating functional, maintainable code within a closed, highly specialized software environment.
  We developed an evaluation framework tailored to ASML's proprietary codebase and introduced a new benchmark. Additionally, we proposed a new evaluation metric, build@k, to assess whether LLM-generated code successfully compiles and integrates within real industrial repositories. We investigate various prompting techniques, compare the performance of generic and code-specific LLMs, and examine the impact of model size on code generation capabilities, using both match-based and execution-based metrics. The findings reveal that prompting techniques and model size have a significant impact on output quality, with few-shot and chain-of-thought prompting yielding the highest build success rates. The difference in performance between the code-specific LLMs and generic LLMs was less pronounced and varied substantially across different model families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12395v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yash Mundhra, Max Valk, Maliheh Izadi</dc:creator>
    </item>
    <item>
      <title>Understanding Prompt Management in GitHub Repositories: A Call for Best Practices</title>
      <link>https://arxiv.org/abs/2509.12421</link>
      <description>arXiv:2509.12421v1 Announce Type: new 
Abstract: The rapid adoption of foundation models (e.g., large language models) has given rise to promptware, i.e., software built using natural language prompts. Effective management of prompts, such as organization and quality assurance, is essential yet challenging. In this study, we perform an empirical analysis of 24,800 open-source prompts from 92 GitHub repositories to investigate prompt management practices and quality attributes. Our findings reveal critical challenges such as considerable inconsistencies in prompt formatting, substantial internal and external prompt duplication, and frequent readability and spelling issues. Based on these findings, we provide actionable recommendations for developers to enhance the usability and maintainability of open-source prompts within the rapidly evolving promptware ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12421v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Li, Hicham Masri, Filipe R. Cogo, Abdul Ali Bangash, Bram Adams, Ahmed E. Hassan</dc:creator>
    </item>
    <item>
      <title>From Legacy Fortran to Portable Kokkos:An Autonomous Agentic AI Workflow</title>
      <link>https://arxiv.org/abs/2509.12443</link>
      <description>arXiv:2509.12443v1 Announce Type: new 
Abstract: Scientific applications continue to rely on legacy Fortran codebases originally developed for homogeneous, CPU-based systems. As High-Performance Computing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many accelerators lack native Fortran bindings, creating an urgent need to modernize legacy codes for portability. Frameworks like Kokkos provide performance portability and a single-source C++ abstraction, but manual Fortran-to-Kokkos porting demands significant expertise and time. Large language models (LLMs) have shown promise in source-to-source code generation, yet their use in fully autonomous workflows for translating and optimizing parallel code remains largely unexplored, especially for performance portability across diverse hardware.
  This paper presents an agentic AI workflow where specialized LLM "agents" collaborate to translate, validate, compile, run, test, debug, and optimize Fortran kernels into portable Kokkos C++ programs. Results show the pipeline modernizes a range of benchmark kernels, producing performance-portable Kokkos codes across hardware partitions. Paid OpenAI models such as GPT-5 and o4-mini-high executed the workflow for only a few U.S. dollars, generating optimized codes that surpassed Fortran baselines, whereas open-source models like Llama4-Maverick often failed to yield functional codes.
  This work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos transformation and offers a pathway for autonomously modernizing legacy scientific applications to run portably and efficiently on diverse supercomputers. It further highlights the potential of LLM-driven agentic systems to perform structured, domain-specific reasoning tasks in scientific and systems-oriented applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12443v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sparsh Gupta, Kamalavasan Kamalakkannan, Maxim Moraru, Galen Shipman, Patrick Diehl</dc:creator>
    </item>
    <item>
      <title>Perspectives, Needs and Challenges for Sustainable Software Engineering Teams: A FinServ Case Study</title>
      <link>https://arxiv.org/abs/2509.12466</link>
      <description>arXiv:2509.12466v1 Announce Type: new 
Abstract: Sustainable Software Engineering (SSE) is slowly becoming an industry need for reasons including reputation enhancement, improved profits and more efficient practices. However, SSE has many definitions, and this is a challenge for organisations trying to build a common and broadly agreed understanding of the term. Although much research effort has gone into identifying general SSE practices, there is a gap in understanding the sustainability needs of specific organisational contexts, such as financial services, which are highly data-driven, operate under strict regulatory requirements, and handle millions of transactions day to day. To address this gap, our research focuses on a financial services company (FinServCo) that invited us to investigate perceptions of sustainability in their IT function: how it could be put into practice, who is responsible for it, and what the challenges are. We conducted an exploratory qualitative case study using interviews and a focus group with six higher management employees and 16 software engineers comprising various experience levels from junior developers to team leaders. Our study found a clear divergence in how sustainability is perceived between organisational levels. Higher management emphasised technical and economic sustainability, focusing on cloud migration and business continuity through data availability. In contrast, developers highlighted human-centric concerns such as workload management and stress reduction. Scepticism toward organisational initiatives was also evident, with some developers viewing them as a PR strategy. Many participants expressed a preference for a dedicated sustainability team, drawing analogies to internal structures for security governance. The disconnect between organisational goals and individual developer needs highlights the importance of context-sensitive, co-designed interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12466v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Satwik Ghanta, Peggy Gregory, Gul Calikli</dc:creator>
    </item>
    <item>
      <title>Good Vibrations? A Qualitative Study of Co-Creation, Communication, Flow, and Trust in Vibe Coding</title>
      <link>https://arxiv.org/abs/2509.12491</link>
      <description>arXiv:2509.12491v1 Announce Type: new 
Abstract: Vibe coding, a term coined by Andrej Karpathy in February 2025, has quickly become a compelling and controversial natural language programming paradigm in AI-assisted software development. Centered on iterative co-design with an AI assistant, vibe coding emphasizes flow and experimentation over strict upfront specification. While initial studies have begun to explore this paradigm, most focus on analyzing code artifacts or proposing theories with limited empirical backing. There remains a need for a grounded understanding of vibe coding as it is perceived and experienced by developers. We present the first systematic qualitative investigation of vibe coding perceptions and practice. Drawing on over 190,000 words from semi-structured interviews, Reddit threads, and LinkedIn posts, we characterize what vibe coding is, why and how developers use it, where it breaks down, and which emerging practices aim to support it. We propose a qualitatively grounded theory of vibe coding centered on conversational interaction with AI, co-creation, and developer flow and joy. We find that AI trust regulates movement along a continuum from delegation to co-creation and supports the developer experience by sustaining flow. We surface recurring pain points and risks in areas including specification, reliability, debugging, latency, code review burden, and collaboration. We also present best practices that have been discovered and shared to mitigate these challenges. We conclude with implications for the future of AI dev tools and directions for researchers investigating vibe coding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12491v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Veronica Pimenova, Sarah Fakhoury, Christian Bird, Margaret-Anne Storey, Madeline Endres</dc:creator>
    </item>
    <item>
      <title>Ensembling Large Language Models for Code Vulnerability Detection: An Empirical Evaluation</title>
      <link>https://arxiv.org/abs/2509.12629</link>
      <description>arXiv:2509.12629v1 Announce Type: new 
Abstract: Code vulnerability detection is crucial for ensuring the security and reliability of modern software systems. Recently, Large Language Models (LLMs) have shown promising capabilities in this domain. However, notable discrepancies in detection results often arise when analyzing identical code segments across different training stages of the same model or among architecturally distinct LLMs. While such inconsistencies may compromise detection stability, they also highlight a key opportunity: the latent complementarity among models can be harnessed through ensemble learning to create more robust vulnerability detection systems. In this study, we explore the potential of ensemble learning to enhance the performance of LLMs in source code vulnerability detection. We conduct comprehensive experiments involving five LLMs (i.e., DeepSeek-Coder-6.7B, CodeLlama-7B, CodeLlama-13B, CodeQwen1.5-7B, and StarCoder2-15B), using three ensemble strategies (i.e., Bagging, Boosting, and Stacking). These experiments are carried out across three widely adopted datasets (i.e., Devign, ReVeal, and BigVul). Inspired by Mixture of Experts (MoE) techniques, we further propose Dynamic Gated Stacking (DGS), a Stacking variant tailored for vulnerability detection. Our results demonstrate that ensemble approaches can significantly improve detection performance, with Boosting excelling in scenarios involving imbalanced datasets. Moreover, DGS consistently outperforms traditional Stacking, particularly in handling class imbalance and multi-class classification tasks. These findings offer valuable insights into building more reliable and effective LLM-based vulnerability detection systems through ensemble learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12629v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhihong Sun, Jia Li, Yao Wan, Chuanyi Li, Hongyu Zhang, Zhi jin, Ge Li, Hong Liu, Chen Lyu, Songlin Hu</dc:creator>
    </item>
    <item>
      <title>When Large Language Models Meet UAVs: How Far Are We?</title>
      <link>https://arxiv.org/abs/2509.12795</link>
      <description>arXiv:2509.12795v1 Announce Type: new 
Abstract: The integration of unmanned aerial vehicles (UAVs) and large language models (LLMs) has emerged as a research direction of growing interest, with the potential to address challenges in autonomous decision-making, human-UAV interaction, and real-time adaptability. However, existing studies have remained largely in preliminary exploration with a limited understanding of real-world practice, risking a misalignment between academic research and practical needs and hindering the translation of results. To examine and address these potential challenges, we conducted an empirical study of 74 selected papers and 56 public GitHub projects, identified nine task types for LLMs in UAV systems, and quantified their distribution. Our findings show that academic research emphasizes theoretical modeling and task optimization with dispersed attention across tasks. In contrast, industrial projects focus on flight control, task planning, and human-machine interaction, prioritizing operability and efficiency. To further capture industry perspectives, we distributed an online questionnaire. We obtained 52 valid responses: 40.4% of practitioners have attempted to apply LLMs to UAV tasks. We further identify factors that impede real-world integration, including technological maturity, performance, safety, cost, and other considerations. Finally, we highlight challenges for future development and provide recommendations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12795v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihua Chen, Xingle Que, Jiashuo Zhang, Ting Chen, Guangshun Li, Jiachi Chen</dc:creator>
    </item>
    <item>
      <title>LLM-Based Approach for Enhancing Maintainability of Automotive Architectures</title>
      <link>https://arxiv.org/abs/2509.12798</link>
      <description>arXiv:2509.12798v1 Announce Type: new 
Abstract: There are many bottlenecks that decrease the flexibility of automotive systems, making their long-term maintenance, as well as updates and extensions in later lifecycle phases increasingly difficult, mainly due to long re-engineering, standardization, and compliance procedures, as well as heterogeneity and numerosity of devices and underlying software components involved. In this paper, we explore the potential of Large Language Models (LLMs) when it comes to the automation of tasks and processes that aim to increase the flexibility of automotive systems. Three case studies towards achieving this goal are considered as outcomes of early-stage research: 1) updates, hardware abstraction, and compliance, 2) interface compatibility checking, and 3) architecture modification suggestions. For proof-of-concept implementation, we rely on OpenAI's GPT-4o model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12798v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nenad Petrovic, Lukasz Mazur, Alois Knoll</dc:creator>
    </item>
    <item>
      <title>SateLight: A Satellite Application Update Framework for Satellite Computing</title>
      <link>https://arxiv.org/abs/2509.12809</link>
      <description>arXiv:2509.12809v1 Announce Type: new 
Abstract: Satellite computing is an emerging paradigm that empowers satellites to perform onboard processing tasks (i.e., \textit{satellite applications}), thereby reducing reliance on ground-based systems and improving responsiveness. However, enabling application software updates in this context remains a fundamental challenge due to application heterogeneity, limited ground-to-satellite bandwidth, and harsh space conditions. Existing software update approaches, designed primarily for terrestrial systems, fail to address these constraints, as they assume abundant computational capacity and stable connectivity.
  To address this gap, we propose SateLight, a practical and effective satellite application update framework tailored for satellite computing. SateLight leverages containerization to encapsulate heterogeneous applications, enabling efficient deployment and maintenance. SateLight further integrates three capabilities: (1) a content-aware differential strategy that minimizes communication data volume, (2) a fine-grained onboard update design that reconstructs target applications, and (3) a layer-based fault-tolerant recovery mechanism to ensure reliability under failure-prone space conditions. Experimental results on a satellite simulation environment with 10 representative satellite applications demonstrate that SateLight reduces transmission latency by up to 91.18% (average 56.54%) compared to the best currently available baseline. It also consistently ensures 100% update correctness across all evaluated applications. Furthermore, a case study on a real-world in-orbit satellite demonstrates the practicality of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12809v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinfeng Wen, Jianshu Zhao, Zixi Zhu, Xiaomin Zhang, Qi Liang, Ao Zhou, Shangguang Wang</dc:creator>
    </item>
    <item>
      <title>Evaluating Large Language Models for Code Translation: Effects of Prompt Language and Prompt Design</title>
      <link>https://arxiv.org/abs/2509.12973</link>
      <description>arXiv:2509.12973v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown promise for automated source-code translation, a capability critical to software migration, maintenance, and interoperability. Yet comparative evidence on how model choice, prompt design, and prompt language shape translation quality across multiple programming languages remains limited. This study conducts a systematic empirical assessment of state-of-the-art LLMs for code translation among C++, Java, Python, and C#, alongside a traditional baseline (TransCoder). Using BLEU and CodeBLEU, we quantify syntactic fidelity and structural correctness under two prompt styles (concise instruction and detailed specification) and two prompt languages (English and Arabic), with direction-aware evaluation across language pairs. Experiments show that detailed prompts deliver consistent gains across models and translation directions, and English prompts outperform Arabic by 13-15%. The top-performing model attains the highest CodeBLEU on challenging pairs such as Java to C# and Python to C++. Our evaluation shows that each LLM outperforms TransCoder across the benchmark. These results demonstrate the value of careful prompt engineering and prompt language choice, and provide practical guidance for software modernization and cross-language interoperability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12973v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aamer Aljagthami, Mohammed Banabila, Musab Alshehri, Mohammed Kabini, Mohammad D. Alahmadi</dc:creator>
    </item>
    <item>
      <title>Validating Solidity Code Defects using Symbolic and Concrete Execution powered by Large Language Models</title>
      <link>https://arxiv.org/abs/2509.13023</link>
      <description>arXiv:2509.13023v1 Announce Type: new 
Abstract: The high rate of false alarms from static analysis tools and Large Language Models (LLMs) complicates vulnerability detection in Solidity Smart Contracts, demanding methods that can formally or empirically prove the presence of defects. This paper introduces a novel detection pipeline that integrates custom Slither-based detectors, LLMs, Kontrol, and Forge. Our approach is designed to reliably detect defects and generate proofs.  We currently perform experiments with promising results for seven types of critical defects. We demonstrate the pipeline's efficacy by presenting our findings for three vulnerabilities -- Reentrancy, Complex Fallback, and Faulty Access Control Policies -- that are challenging for current verification solutions, which often generate false alarms or fail to detect them entirely. We highlight the potential of either symbolic or concrete execution in correctly classifying such code faults. By chaining these instruments, our method effectively validates true positives, significantly reducing the manual verification burden. Although we identify potential limitations, such as the inconsistency and the cost of LLMs, our findings establish a robust framework for combining heuristic analysis with formal verification to achieve more reliable and automated smart contract auditing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13023v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.427.7</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 427, 2025, pp. 98-116</arxiv:journal_reference>
      <dc:creator>\c{S}tefan-Claudiu Susan ("Alexandru Ioan Cuza", University of Ia\c{s}i, Department of Computer Science), Andrei Arusoaie ("Alexandru Ioan Cuza", University of Ia\c{s}i, Department of Computer Science), Dorel Lucanu ("Alexandru Ioan Cuza", University of Ia\c{s}i, Department of Computer Science)</dc:creator>
    </item>
    <item>
      <title>GView: A Survey of Binary Forensics via Visual, Semantic, and AI-Enhanced Analysis</title>
      <link>https://arxiv.org/abs/2509.13025</link>
      <description>arXiv:2509.13025v1 Announce Type: new 
Abstract: Cybersecurity threats continue to become more sophisticated and diverse in their artifacts, boosting both their volume and complexity. To overcome those challenges, we present GView, an open-source forensic analysis framework with visual and AI-enhanced reasoning. It started with focus on the practical cybersecurity industry. It has evolved significantly, incorporating large language models (LLMs) to dynamically enhance reasoning and ease the forensic workflows. This paper surveys both the current state of GView with its published papers alongside those that are in the publishing process. It also includes its innovative use of logical inference through predicates and inference rules for both the analyzed documents and the user's actions for better suggestions. We highlight the extensible architecture, showcasing its potential as a bridge between the practical forensics worlds with the academic research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13025v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.427.9</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 427, 2025, pp. 134-140</arxiv:journal_reference>
      <dc:creator>Raul Zaharia (Al. I. Cuza University,Bitdefender), Drago\c{s} Gavrilu\c{t} (Al. I. Cuza University,Bitdefender), Gheorghi\c{t}\u{a} Mutu (Al. I. Cuza University,Bitdefender)</dc:creator>
    </item>
    <item>
      <title>Automating Code Generation for Semiconductor Equipment Control from Developer Utterances with LLMs</title>
      <link>https://arxiv.org/abs/2509.13055</link>
      <description>arXiv:2509.13055v1 Announce Type: new 
Abstract: Semiconductors form the backbone of modern electronics, with their manufacturing and testing relying on highly specialized equipment and domain-specific programming languages. Equipment languages such as the Algorithmic Pattern Generator (ALPG) are critical for precise hardware control but are challenging to program due to their low-level syntax and steep learning curve. While large language models (LLMs) have shown promise in generating high-level code from natural language, their effectiveness on low-level equipment languages remains limited. To address this, we propose Progressive Knowledge Enhancement (PKE), a novel multi-stage prompting framework that progressively extracts and activates the latent knowledge within LLMs, guiding them from simple to complex examples without extensive fine-tuning. Empirical evaluation on an industrial ALPG dataset shows that PKE significantly outperforms standard prompting and surpasses state-of-the-art methods in generating correct ALPG code, achieving 11.1\% and 15.2\% higher exact match scores compared to the second-best technique. Further analysis of individual components confirms that progressive knowledge extraction based on difficulty enhances accuracy. Our study offer a practical approach to boosting LLM capabilities for specialized low-level programming, supporting greater productivity in semiconductor software development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13055v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youngkyoung Kim, Sanghyeok Park, Misoo Kim, Gangho Yoon, Eunseok Lee, Simon S. Woo</dc:creator>
    </item>
    <item>
      <title>Accelerating Discovery: Rapid Literature Screening with LLMs</title>
      <link>https://arxiv.org/abs/2509.13103</link>
      <description>arXiv:2509.13103v1 Announce Type: new 
Abstract: Background: Conducting Multi Vocal Literature Reviews (MVLRs) is often time and effort-intensive. Researchers must review and filter a large number of unstructured sources, which frequently contain sparse information and are unlikely to be included in the final study. Our experience conducting an MVLR on Context-Aware Software Systems (CASS) Testing in the avionics domain exemplified this challenge, with over 8,000 highly heterogeneous documents requiring review. Therefore, we developed a Large Language Model (LLM) assistant to support the search and filtering of documents. Aims: To develop and validate an LLM based tool that can support researchers in performing the search and filtering of documents for an MVLR without compromising the rigor of the research protocol. Method: We applied sound engineering practices to develop an on-premises LLM-based tool incorporating Retrieval Augmented Generation (RAG) to process candidate sources. Progress towards the aim was quantified using the Positive Percent Agreement (PPA) as the primary metric to ensure the performance of the LLM based tool. Convenience sampling, supported by human judgment and statistical sampling, were used to verify and validate the tool's quality-in-use. Results: The tool currently demonstrates a PPA agreement with human researchers of 90% for sources that are not relevant to the study. Development details are shared to support domain-specific adaptation of the tool. Conclusions: Using LLM-based tools to support academic researchers in rigorous MVLR is feasible. These tools can free valuable time for higher-level, abstract tasks. However, researcher participation remains essential to ensure that the tool supports thorough research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13103v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Santiago Matalonga, Domenico Amalfitano, Jean Carlo Rossa Hauck, Mart\'in Solari, Guilherme H. Travassos</dc:creator>
    </item>
    <item>
      <title>Vulnerability Patching Across Software Products and Software Components: A Case Study of Red Hat's Product Portfolio</title>
      <link>https://arxiv.org/abs/2509.13117</link>
      <description>arXiv:2509.13117v1 Announce Type: new 
Abstract: Motivated by software maintenance and the more recent concept of security debt, the paper presents a time series analysis of vulnerability patching of Red Hat's products and components between 1999 and 2024. According to the results based on segmented regression analysis, the amounts of vulnerable products and components have not been stable; a linear trend describes many of the series well. Nor do the amounts align well with trends characterizing vulnerabilities in general. There are also visible breakpoints indicating that the linear trend is not universally applicable and that the growing security debt may be stabilizing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13117v1</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jukka Ruohonen, Sani Abdullahi, Abhishek Tiwari</dc:creator>
    </item>
    <item>
      <title>Optimizing Code Embeddings and ML Classifiers for Python Source Code Vulnerability Detection</title>
      <link>https://arxiv.org/abs/2509.13134</link>
      <description>arXiv:2509.13134v1 Announce Type: new 
Abstract: In recent years, the growing complexity and scale of source code have rendered manual software vulnerability detection increasingly impractical. To address this challenge, automated approaches leveraging machine learning and code embeddings have gained substantial attention. This study investigates the optimal combination of code embedding techniques and machine learning classifiers for vulnerability detection in Python source code. We evaluate three embedding techniques, i.e., Word2Vec, CodeBERT, and GraphCodeBERT alongside two deep learning classifiers, i.e., Bidirectional Long Short-Term Memory (BiLSTM) networks and Convolutional Neural Networks (CNN). While CNN paired with GraphCodeBERT exhibits strong performance, the BiLSTM model using Word2Vec consistently achieves superior overall results. These findings suggest that, despite the advanced architectures of recent models like CodeBERT and GraphCodeBERT, classical embeddings such as Word2Vec, when used with sequence-based models like BiLSTM, can offer a slight yet consistent performance advantage. The study underscores the critical importance of selecting appropriate combinations of embeddings and classifiers to enhance the effectiveness of automated vulnerability detection systems, particularly for Python source code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13134v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Talaya Farasat, Joachim Posegga</dc:creator>
    </item>
    <item>
      <title>Towards the Next Generation of Software: Insights from Grey Literature on AI-Native Applications</title>
      <link>https://arxiv.org/abs/2509.13144</link>
      <description>arXiv:2509.13144v1 Announce Type: new 
Abstract: Background: The rapid advancement of large language models (LLMs) has given rise to AI-native applications, a new paradigm in software engineering that fundamentally redefines how software is designed, developed, and evolved. Despite their growing prominence, AI-native applications still lack a unified engineering definition and architectural blueprint, leaving practitioners without systematic guidance for system design, quality assurance, and technology selection.
  Objective: This study seeks to establish a comprehensive understanding of AI-native applications by identifying their defining characteristics, key quality attributes, and typical technology stacks, as well as by clarifying the opportunities and challenges they present.
  Method: We conducted a grey literature review, integrating conceptual perspectives retrieved from targeted Google and Bing searches with practical insights derived from leading open-source projects on GitHub. A structured protocol encompassing source selection, quality assessment, and thematic analysis was applied to synthesize findings across heterogeneous sources.
  Results: We finally identified 106 studies based on the selection criteria. The analysis reveals that AI-native applications are distinguished by two core pillars: the central role of AI as the system's intelligence paradigm and their inherently probabilistic, non-deterministic nature. Critical quality attributes include reliability, usability, performance efficiency, and AI-specific observability. In addition, a typical technology stack has begun to emerge, comprising LLM orchestration frameworks, vector databases, and AI-native observability platforms. These systems emphasize response quality, cost-effectiveness, and outcome predictability, setting them apart from conventional software systems.
  Conclusion: This study is the first to propose a dual-layered engineering blueprint...</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13144v1</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lingli Cao, Shanshan Li, Ying Fan, Danyang Li, Chenxing Zhong</dc:creator>
    </item>
    <item>
      <title>An End to End Edge to Cloud Data and Analytics Strategy</title>
      <link>https://arxiv.org/abs/2509.12296</link>
      <description>arXiv:2509.12296v1 Announce Type: cross 
Abstract: There is an exponential growth of connected Internet of Things (IoT) devices. These have given rise to applications that rely on real time data to make critical decisions quickly. Enterprises today are adopting cloud at a rapid pace. There is a critical need to develop secure and efficient strategy and architectures to best leverage capabilities of cloud and edge assets. This paper provides an end to end secure edge to cloud data and analytics strategy. To enable real life implementation, the paper provides reference architectures for device layer, edge layer and cloud layer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12296v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ICCCNT54827.2022.9984604</arxiv:DOI>
      <arxiv:journal_reference>2022 13th International Conference on Computing Communication and Networking Technologies (ICCCNT). IEEE, 2022</arxiv:journal_reference>
      <dc:creator>Vijay Kumar Butte, Sujata Butte</dc:creator>
    </item>
    <item>
      <title>Out of Distribution Detection in Self-adaptive Robots with AI-powered Digital Twins</title>
      <link>https://arxiv.org/abs/2509.12982</link>
      <description>arXiv:2509.12982v1 Announce Type: cross 
Abstract: Self-adaptive robots (SARs) in complex, uncertain environments must proactively detect and address abnormal behaviors, including out-of-distribution (OOD) cases. To this end, digital twins offer a valuable solution for OOD detection. Thus, we present a digital twin-based approach for OOD detection (ODiSAR) in SARs. ODiSAR uses a Transformer-based digital twin to forecast SAR states and employs reconstruction error and Monte Carlo dropout for uncertainty quantification. By combining reconstruction error with predictive variance, the digital twin effectively detects OOD behaviors, even in previously unseen conditions. The digital twin also includes an explainability layer that links potential OOD to specific SAR states, offering insights for self-adaptation. We evaluated ODiSAR by creating digital twins of two industrial robots: one navigating an office environment, and another performing maritime ship navigation. In both cases, ODiSAR forecasts SAR behaviors (i.e., robot trajectories and vessel motion) and proactively detects OOD events. Our results showed that ODiSAR achieved high detection performance -- up to 98\% AUROC, 96\% TNR@TPR95, and 95\% F1-score -- while providing interpretable insights to support self-adaptation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12982v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erblin Isaku, Hassan Sartaj, Shaukat Ali, Beatriz Sanguino, Tongtong Wang, Guoyuan Li, Houxiang Zhang, Thomas Peyrucain</dc:creator>
    </item>
    <item>
      <title>Try-Mopsa: Relational Static Analysis in Your Pocket</title>
      <link>https://arxiv.org/abs/2509.13128</link>
      <description>arXiv:2509.13128v1 Announce Type: cross 
Abstract: Static analyzers are complex pieces of software with large dependencies. They can be difficult to install, which hinders adoption and creates barriers for students learning static analysis. This work introduces Try-Mopsa: a scaled-down version of the Mopsa static analysis platform, compiled into JavaScript to run purely as a client-side application in web browsers. Try-Mopsa provides a responsive interface that works on both desktop and mobile devices. Try-Mopsa features all the core components of Mopsa. In particular, it supports relational numerical domains. We present the interface, changes and adaptations required to have a pure JavaScript version of Mopsa. We envision Try-Mopsa as a convenient platform for onboarding or teaching purposes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13128v1</guid>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rapha\"el Monat</dc:creator>
    </item>
    <item>
      <title>Evolution of Programmers' Trust in Generative AI Programming Assistants</title>
      <link>https://arxiv.org/abs/2509.13253</link>
      <description>arXiv:2509.13253v1 Announce Type: cross 
Abstract: Motivation. Trust in generative AI programming assistants is a vital attitude that impacts how programmers use those programming assistants. Programmers that are over-trusting may be too reliant on their tools, leading to incorrect or vulnerable code; programmers that are under-trusting may avoid using tools that can improve their productivity and well-being.
  Methods. Since trust is a dynamic attitude that may change over time, this study aims to understand programmers' evolution of trust after immediate (one hour) and extended (10 days) use of GitHub Copilot. We collected survey data from 71 upper-division computer science students working on a legacy code base, representing a population that is about to enter the workforce. In this study, we quantitatively measure student trust levels and qualitatively uncover why student trust changes.
  Findings. Student trust, on average, increased over time. After completing a project with Copilot, however, students felt that Copilot requires a competent programmer to complete some tasks manually. Students mentioned that seeing Copilot's correctness, understanding how Copilot uses context from the code base, and learning some basics of natural language processing contributed to their elevated trust.
  Implications. Our study helps instructors and industry managers understand the factors that influence how students calibrate their trust with AI assistants. We make four pedagogical recommendations, which are that CS educators should 1) provide opportunities for students to work with Copilot on challenging software engineering tasks to calibrate their trust, 2) teach traditional skills of comprehending, debugging, and testing so students can verify output, 3) teach students about the basics of natural language processing, and 4) explicitly introduce and demonstrate the range of features available in Copilot.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13253v1</guid>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anshul Shah, Thomas Rexin, Elena Tomson, Leo Porter, William G. Griswold, Adalbert Gerald Soosai Raj</dc:creator>
    </item>
    <item>
      <title>TRANSAGENT: An LLM-Based Multi-Agent System for Code Translation</title>
      <link>https://arxiv.org/abs/2409.19894</link>
      <description>arXiv:2409.19894v3 Announce Type: replace 
Abstract: Code translation converts code from one programming language to another while maintaining its original functionality, which is crucial for software migration, system refactoring, and cross-platform development. Traditional rule-based methods rely on manually-written rules, which can be time-consuming and often result in less readable code. To overcome this, learning-based methods have been developed, leveraging parallel data to train models for automated code translation. More recently, the advance of Large Language Models (LLMs) further boosts learning-based code translation. Although promising, LLM-translated program still suffers from diverse quality issues (e.g., syntax errors and semantic errors). In particular, it can be challenging for LLMs to self-debug these errors when simply provided with the corresponding error messages.
  In this work, we propose a novel LLM-based multi-agent system TRANSAGENT, which enhances LLM-based code translation by fixing the syntax errors and semantic errors with the synergy between four LLM-based agents, including Initial Code Translator, Syntax Error Fixer, Code Aligner, and Semantic Error Fixer. The main insight of TRANSAGENT is to first localize the error code block in the target program based on the execution alignment between the target and source program, which can narrow down the fixing space and thus lower down the fixing difficulties. To evaluate TRANSAGENT, we first construct a new benchmark from recent programming tasks to mitigate the potential data leakage issue. On our benchmark, TRANSAGENT outperforms the latest LLM-based code translation technique UniTrans in both translation effectiveness and efficiency; additionally, our evaluation on different LLMs show the generalization of TRANSAGENT and our ablation study shows the contribution of each agent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19894v3</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiqiang Yuan, Weitong Chen, Hanlin Wang, Kai Yu, Xin Peng, Yiling Lou</dc:creator>
    </item>
    <item>
      <title>Enhancing Automated Loop Invariant Generation for Complex Programs with Large Language Models</title>
      <link>https://arxiv.org/abs/2412.10483</link>
      <description>arXiv:2412.10483v3 Announce Type: replace 
Abstract: Automated program verification has always been an important component of building trustworthy software. While the analysis of real-world programs remains a theoretical challenge, the automation of loop invariant analysis has effectively resolved the problem. However, real-world programs that often mix complex data structures and control flows pose challenges to traditional loop invariant generation tools. To enhance the applicability of invariant generation techniques, we proposed ACInv, an Automated Complex program loop Invariant generation tool, which combines static analysis with Large Language Models (LLMs) to generate the proper loop invariants. We utilize static analysis to extract the necessary information for each loop and embed it into prompts for the LLM to generate invariants for each loop. Subsequently, we employ an LLM-based evaluator to assess the generated invariants, refining them by either strengthening, weakening, or rejecting them based on their correctness, ultimately obtaining enhanced invariants. We conducted experiments on ACInv, which showed that ACInv outperformed previous tools on data sets with data structures, and maintained similar performance to the state-of-the-art tool AutoSpec on numerical programs without data structures. For the total data set, ACInv can solve 21% more examples than AutoSpec and can generate reference data structure templates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10483v3</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruibang Liu, Minyu Chen, Ling-I Wu, Jingyu Ke, Guoqiang Li</dc:creator>
    </item>
    <item>
      <title>A Time Series Analysis of Assertions in the Linux Kernel</title>
      <link>https://arxiv.org/abs/2412.19465</link>
      <description>arXiv:2412.19465v2 Announce Type: replace 
Abstract: Assertions are a classical and typical software development technique. These are extensively used also in operating systems and their kernels, including the Linux kernel. The paper fills a gap in existing knowledge by empirically examining the longitudinal evolution of assertion use in the Linux kernel. According to the results, the use of assertions that cause a kernel panic has slightly but not substantially decreased from the kernel's third to the sixth release series. At the same time, the use of softer assertion variants has increased; these do not cause a panic by default but instead produce warnings. With these time series results, the paper contributes to the existing but limited empirical knowledge base about operating system kernels and their long-term evolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19465v2</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-032-05188-2_1</arxiv:DOI>
      <dc:creator>Jukka Ruohonen</dc:creator>
    </item>
    <item>
      <title>Tracing Vulnerability Propagation Across Open Source Software Ecosystems</title>
      <link>https://arxiv.org/abs/2505.04307</link>
      <description>arXiv:2505.04307v2 Announce Type: replace 
Abstract: The paper presents a traceability analysis of how over 84 thousand vulnerabilities have propagated across 28 open source software ecosystems. According to the results, the propagation sequences have been complex in general, although GitHub, Debian, and Ubuntu stand out. Furthermore, the associated propagation delays have been lengthy, and these do not correlate well with the number of ecosystems involved in the associated sequences. Nor does the presence or absence of particularly ecosystems in the sequences yield clear, interpretable patterns. With these results, the paper contributes to the overlapping knowledge bases about software ecosystems, traceability, and vulnerabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04307v2</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-032-05188-2_21</arxiv:DOI>
      <dc:creator>Jukka Ruohonen, Qusai Ramadan</dc:creator>
    </item>
    <item>
      <title>Worst-Case Symbolic Constraints Analysis and Generalisation with Large Language Models</title>
      <link>https://arxiv.org/abs/2506.08171</link>
      <description>arXiv:2506.08171v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated strong performance on coding tasks such as generation, completion and repair, but their ability to handle complex symbolic reasoning over code still remains underexplored. We introduce the task of worst-case symbolic constraints analysis, which requires inferring the symbolic constraints that characterise worst-case program executions; these constraints can be solved to obtain inputs that expose performance bottlenecks or denial-of-service vulnerabilities in software systems. We show that even state-of-the-art LLMs (e.g., GPT-5) struggle when applied directly on this task. To address this challenge, we propose WARP, an innovative neurosymbolic approach that computes worst-case constraints on smaller concrete input sizes using existing program analysis tools, and then leverages LLMs to generalise these constraints to larger input sizes. Concretely, WARP comprises: (1) an incremental strategy for LLM-based worst-case reasoning, (2) a solver-aligned neurosymbolic framework that integrates reinforcement learning with SMT (Satisfiability Modulo Theories) solving, and (3) a curated dataset of symbolic constraints. Experimental results show that WARP consistently improves performance on worst-case constraint reasoning. Leveraging the curated constraint dataset, we use reinforcement learning to fine-tune a model, WARP-1.0-3B, which significantly outperforms size-matched and even larger baselines. These results demonstrate that incremental constraint reasoning enhances LLMs' ability to handle symbolic reasoning and highlight the potential for deeper integration between neural learning and formal methods in rigorous program analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08171v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Koh, Yannic Noller, Corina S. Pasareanu, Adrians Skapars, Youcheng Sun</dc:creator>
    </item>
    <item>
      <title>Breaking Single-Tester Limits: Multi-Agent LLMs for Multi-User Feature Testing</title>
      <link>https://arxiv.org/abs/2506.17539</link>
      <description>arXiv:2506.17539v3 Announce Type: replace 
Abstract: The growing dependence on mobile phones and their apps has made multi-user interactive features, like chat calls, live streaming, and video conferencing, indispensable for bridging the gaps in social connectivity caused by physical and situational barriers. However, automating these interactive features for testing is fraught with challenges, owing to their inherent need for timely, dynamic, and collaborative user interactions, which current automated testing methods inadequately address. Inspired by the concept of agents designed to autonomously and collaboratively tackle problems, we propose MAdroid, a novel multi-agent approach powered by the Large Language Models (LLMs) to automate the multi-user interactive task for app feature testing. Specifically, MAdroid employs two functional types of multi-agents: user agents (Operator) and supervisor agents (Coordinator and Observer). Each agent takes a specific role: the Coordinator directs the interactive task; the Operator mimics user interactions on the device; and the Observer monitors and reviews the task automation process. Our evaluation, which included 41 multi-user interactive tasks, demonstrates the effectiveness of our approach, achieving 82.9% of the tasks with 96.8% action similarity, outperforming the ablation studies and state-of-the-art baselines. Additionally, a preliminary investigation underscores MAdroid's practicality by helping identify 11 multi-user interactive bugs during regression app testing, confirming its potential value in real-world software development contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17539v3</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sidong Feng, Changhao Du, Huaxiao Liu, Qingnan Wang, Zhengwei Lv, Mengfei Wang, Chunyang Chen</dc:creator>
    </item>
    <item>
      <title>InferLog: Accelerating LLM Inference for Online Log Parsing via ICL-oriented Prefix Caching</title>
      <link>https://arxiv.org/abs/2507.08523</link>
      <description>arXiv:2507.08523v3 Announce Type: replace 
Abstract: Modern software systems generate massive volumes of runtime logs, necessitating efficient and accurate log parsing to enable critical downstream tasks such as anomaly detection and root cause analysis. Recently, large language models (LLMs) have achieved advanced accuracy on log parsing, but their deployment in production environments faces two major limitations: (1) the privacy risks associated with commercial LLMs, driving the adoption of local deployment, and (2) the stringent latency and throughput requirements imposed by high-volume log streams, which existing LLM-based parsers fail to meet. Although recent efforts have reduced the number of LLM queries, they overlook the high latency of the LLM invocations, where concurrent log parsing requests can cause serve performance degradation of LLM inference system.
  In this study, we present InferLog, the first LLM inference optimization method for online log parsing. Our key insight is that the inference efficiency emerges as the vital bottleneck in LLM-based online log parsing, rather than parsing accuracy. InferLog accelerates inference by designing (1) A Prefix-aware ICL Refinement policy to refine the examples and permutation of in-context learning to improve the prefix caching efficiency. (2) A rapid and task-specific configuration tuning pipeline based on meta-learning to find the optimal LLM scheduling-related configuration for dynamic log parsing workloads. The experimental results based on Loghub dataset and vLLM demonstrate that InferLog significantly outperforms existing inference optimization methods and markedly accelerates the state-of-the-art LLM-based log parser without compromising parsing accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08523v3</guid>
      <category>cs.SE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3744916.3764523</arxiv:DOI>
      <dc:creator>Yilun Wang, Pengfei Chen, Haiyu Huang, Zilong He, Gou Tan, Chuanfu Zhang, Jingkai He, Zibin Zheng</dc:creator>
    </item>
    <item>
      <title>GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries</title>
      <link>https://arxiv.org/abs/2508.00033</link>
      <description>arXiv:2508.00033v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have advanced rapidly as tools for automating code generation in scientific research, yet their ability to interpret and use unfamiliar Python APIs for complex computational experiments remains poorly characterized. This study systematically benchmarks a selection of state-of-the-art LLMs in generating functional Python code for two increasingly challenging scenarios: conversational data analysis with the \textit{ParShift} library, and synthetic data generation and clustering using \textit{pyclugen} and \textit{scikit-learn}. Both experiments use structured, zero-shot prompts specifying detailed requirements but omitting in-context examples. Model outputs are evaluated quantitatively for functional correctness and prompt compliance over multiple runs, and qualitatively by analyzing the errors produced when code execution fails. Results show that only a small subset of models consistently generate correct, executable code. GPT-4.1 achieved a 100\% success rate across all runs in both experimental tasks, whereas most other models succeeded in fewer than half of the runs, with only Grok-3 and Mistral-Large approaching comparable performance. In addition to benchmarking LLM performance, this approach helps identify shortcomings in third-party libraries, such as unclear documentation or obscure implementation bugs. Overall, these findings highlight current limitations of LLMs for end-to-end scientific automation and emphasize the need for careful prompt design, comprehensive library documentation, and continued advances in language model capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00033v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/fi17090412</arxiv:DOI>
      <arxiv:journal_reference>Future Internet. 2025; 17(9):412</arxiv:journal_reference>
      <dc:creator>Nuno Fachada, Daniel Fernandes, Carlos M. Fernandes, Bruno D. Ferreira-Saraiva, Jo\~ao P. Matos-Carvalho</dc:creator>
    </item>
    <item>
      <title>Executable Ontologies: Synthesizing Event Semantics with Dataflow Architecture</title>
      <link>https://arxiv.org/abs/2509.09775</link>
      <description>arXiv:2509.09775v2 Announce Type: replace-cross 
Abstract: This paper presents boldsea, Boldachev's semantic-event approach -- an architecture for modeling complex dynamic systems using executable ontologies -- semantic models that act as dynamic structures, directly controlling process execution. We demonstrate that integrating event semantics with a dataflow architecture addresses the limitations of traditional Business Process Management (BPM) systems and object-oriented semantic technologies. The paper presents the formal BSL (boldsea Semantic Language), including its BNF grammar, and outlines the boldsea-engine's architecture, which directly interprets semantic models as executable algorithms without compilation. It enables the modification of event models at runtime, ensures temporal transparency, and seamlessly merges data and business logic within a unified semantic framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09775v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.FL</category>
      <category>cs.SE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aleksandr Boldachev</dc:creator>
    </item>
    <item>
      <title>Your Compiler is Backdooring Your Model: Understanding and Exploiting Compilation Inconsistency Vulnerabilities in Deep Learning Compilers</title>
      <link>https://arxiv.org/abs/2509.11173</link>
      <description>arXiv:2509.11173v2 Announce Type: replace-cross 
Abstract: Deep learning (DL) compilers are core infrastructure in modern DL systems, offering flexibility and scalability beyond vendor-specific libraries. This work uncovers a fundamental vulnerability in their design: can an official, unmodified compiler alter a model's semantics during compilation and introduce hidden backdoors? We study both adversarial and natural settings. In the adversarial case, we craft benign models where triggers have no effect pre-compilation but become effective backdoors after compilation. Tested on six models, three commercial compilers, and two hardware platforms, our attack yields 100% success on triggered inputs while preserving normal accuracy and remaining undetected by state-of-the-art detectors. The attack generalizes across compilers, hardware, and floating-point settings. In the natural setting, we analyze the top 100 HuggingFace models (including one with 220M+ downloads) and find natural triggers in 31 models. This shows that compilers can introduce risks even without adversarial manipulation.
  Our results reveal an overlooked threat: unmodified DL compilers can silently alter model semantics. To our knowledge, this is the first work to expose inherent security risks in DL compiler design, opening a new direction for secure and trustworthy ML.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11173v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simin Chen, Jinjun Peng, Yixin He, Junfeng Yang, Baishakhi Ray</dc:creator>
    </item>
  </channel>
</rss>
