<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Apr 2024 04:00:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On the best approximation by finite Gaussian mixtures</title>
      <link>https://arxiv.org/abs/2404.08913</link>
      <description>arXiv:2404.08913v1 Announce Type: new 
Abstract: We consider the problem of approximating a general Gaussian location mixture by finite mixtures. The minimum order of finite mixtures that achieve a prescribed accuracy (measured by various $f$-divergences) is determined within constant factors for the family of mixing distributions with compactly support or appropriate assumptions on the tail probability including subgaussian and subexponential. While the upper bound is achieved using the technique of local moment matching, the lower bound is established by relating the best approximation error to the low-rank approximation of certain trigonometric moment matrices, followed by a refined spectral analysis of their minimum eigenvalue. In the case of Gaussian mixing distributions, this result corrects a previous lower bound in [Allerton Conference 48 (2010) 620-628].</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08913v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yun Ma, Yihong Wu, Pengkun Yang</dc:creator>
    </item>
    <item>
      <title>Controlling the False Discovery Rate in Subspace Selection</title>
      <link>https://arxiv.org/abs/2404.09142</link>
      <description>arXiv:2404.09142v1 Announce Type: new 
Abstract: Controlling the false discovery rate (FDR) is a popular approach to multiple testing, variable selection, and related problems of simultaneous inference. In many contemporary applications, models are not specified by discrete variables, which necessitates a broadening of the scope of the FDR control paradigm. Motivated by the ubiquity of low-rank models for high-dimensional matrices, we present methods for subspace selection in principal components analysis that provide control on a geometric analog of FDR that is adapted to subspace selection. Our methods crucially rely on recently-developed tools from random matrix theory, in particular on a characterization of the limiting behavior of eigenvectors and the gaps between successive eigenvalues of large random matrices. Our procedure is parameter-free, and we show that it provides FDR control in subspace selection for common noise models considered in the literature. We demonstrate the utility of our algorithm with numerical experiments on synthetic data and on problems arising in single-cell RNA sequencing and hyperspectral imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09142v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mateo D\'iaz, Venkat Chandrasekaran</dc:creator>
    </item>
    <item>
      <title>Minimax Optimal rates of convergence in the shuffled regression, unlinked regression, and deconvolution under vanishing noise</title>
      <link>https://arxiv.org/abs/2404.09306</link>
      <description>arXiv:2404.09306v1 Announce Type: new 
Abstract: Shuffled regression and unlinked regression represent intriguing challenges that have garnered considerable attention in many fields, including but not limited to ecological regression, multi-target tracking problems, image denoising, etc. However, a notable gap exists in the existing literature, particularly in vanishing noise, i.e., how the rate of estimation of the underlying signal scales with the error variance. This paper aims to bridge this gap by delving into the monotone function estimation problem under vanishing noise variance, i.e., we allow the error variance to go to $0$ as the number of observations increases. Our investigation reveals that, asymptotically, the shuffled regression problem exhibits a comparatively simpler nature than the unlinked regression; if the error variance is smaller than a threshold, then the minimax risk of the shuffled regression is smaller than that of the unlinked regression. On the other hand, the minimax estimation error is of the same order in the two problems if the noise level is larger than that threshold. Our analysis is quite general in that we do not assume any smoothness of the underlying monotone link function. Because these problems are related to deconvolution, we also provide bounds for deconvolution in a similar context. Through this exploration, we contribute to understanding the intricate relationships between these statistical problems and shed light on their behaviors when subjected to the nuanced constraint of vanishing noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09306v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cecile Durot, Debarghya Mukherjee</dc:creator>
    </item>
    <item>
      <title>Nonparametric density estimation for the small jumps of L\'evy processes</title>
      <link>https://arxiv.org/abs/2404.09725</link>
      <description>arXiv:2404.09725v1 Announce Type: new 
Abstract: We consider the problem of estimating the density of the process associated with the small jumps of a pure jump L\'evy process, possibly of infinite variation, from discrete observations of one trajectory. The interest of such a question lies on the observation that even when the L\'evy measure is known, the density of the increments of the small jumps of the process cannot be computed. We discuss results both from low and high frequency observations. In a low frequency setting, assuming the L\'evy density associated with the jumps larger than $\varepsilon\in (0,1]$ in absolute value is known, a spectral estimator relying on the convolution structure of the problem achieves minimax parametric rates of convergence with respect to the integrated $L_2$ loss, up to a logarithmic factor. In a high frequency setting, we remove the assumption on the knowledge of the L\'evy measure of the large jumps and show that the rate of convergence depends both on the sampling scheme and on the behaviour of the L\'evy measure in a neighborhood of zero. We show that the rate we find is minimax up to a log-factor. An adaptive penalized procedure is studied to select the cutoff parameter. These results are extended to encompass the case where a Brownian component is present in the L\'evy process. Furthermore, we illustrate the performances of our procedures through an extensive simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09725v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>C\'eline Duval, Taher Jalal, Ester Mariucci</dc:creator>
    </item>
    <item>
      <title>Data Models With Two Manifestations of Imprecision</title>
      <link>https://arxiv.org/abs/2404.09741</link>
      <description>arXiv:2404.09741v1 Announce Type: new 
Abstract: Motivated by recently emerging problems in machine learning and statistics, we propose data models which relax the familiar i.i.d. assumption. In essence, we seek to understand what it means for data to come from a set of probability measures. We show that our frequentist data models, parameterized by such sets, manifest two aspects of imprecision. We characterize the intricate interplay of these manifestations, aggregate (ir)regularity and local (ir)regularity, where a much richer set of behaviours compared to an i.i.d. model is possible. In doing so we shed new light on the relationship between non-stationary, locally precise and stationary, locally imprecise data models. We discuss possible applications of these data models in machine learning and how the set of probabilities can be estimated. For the estimation of aggregate irregularity, we provide a negative result but argue that it does not warrant pessimism. Understanding these frequentist aspects of imprecise probabilities paves the way for deriving generalization of proper scoring rules and calibration to the imprecise case, which can then contribute to tackling practical problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09741v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Christian Fr\"ohlich, Robert C. Williamson</dc:creator>
    </item>
    <item>
      <title>Testing for homogeneity of several functional variables via multiple maximum variance discrepancy</title>
      <link>https://arxiv.org/abs/2404.09938</link>
      <description>arXiv:2404.09938v1 Announce Type: new 
Abstract: This paper adresses the problem of testing for the equality of $k$ probability distributions on Hilbert spaces, with $k\geqslant 2$. We introduce a generalization of the maximum variance discrepancy called multiple maximum variance discrepancy (MMVD). Then, a consistent estimator of this measure is proposed as test statistic, and its asymptotic distribution under the null hypothesis is derived. A simulation study comparing the proposed test with existing ones is provided</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09938v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Armando Sosth\`ene Kali Balogoun, Guy Martial Nkiet</dc:creator>
    </item>
    <item>
      <title>State-Space Systems as Dynamic Generative Models</title>
      <link>https://arxiv.org/abs/2404.08717</link>
      <description>arXiv:2404.08717v1 Announce Type: cross 
Abstract: A probabilistic framework to study the dependence structure induced by deterministic discrete-time state-space systems between input and output processes is introduced. General sufficient conditions are formulated under which output processes exist and are unique once an input process has been fixed, a property that in the deterministic state-space literature is known as the echo state property. When those conditions are satisfied, the given state-space system becomes a generative model for probabilistic dependences between two sequence spaces. Moreover, those conditions guarantee that the output depends continuously on the input when using the Wasserstein metric. The output processes whose existence is proved are shown to be causal in a specific sense and to generalize those studied in purely deterministic situations. The results in this paper constitute a significant stochastic generalization of sufficient conditions for the deterministic echo state property to hold, in the sense that the stochastic echo state property can be satisfied under contractivity conditions that are strictly weaker than those in deterministic situations. This means that state-space systems can induce a purely probabilistic dependence structure between input and output sequence spaces even when there is no functional relation between those two spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08717v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Juan-Pablo Ortega, Florian Rossmannek</dc:creator>
    </item>
    <item>
      <title>Convergence of coordinate ascent variational inference for log-concave measures via optimal transport</title>
      <link>https://arxiv.org/abs/2404.08792</link>
      <description>arXiv:2404.08792v1 Announce Type: cross 
Abstract: Mean field variational inference (VI) is the problem of finding the closest product (factorized) measure, in the sense of relative entropy, to a given high-dimensional probability measure $\rho$. The well known Coordinate Ascent Variational Inference (CAVI) algorithm aims to approximate this product measure by iteratively optimizing over one coordinate (factor) at a time, which can be done explicitly. Despite its popularity, the convergence of CAVI remains poorly understood. In this paper, we prove the convergence of CAVI for log-concave densities $\rho$. If additionally $\log \rho$ has Lipschitz gradient, we find a linear rate of convergence, and if also $\rho$ is strongly log-concave, we find an exponential rate. Our analysis starts from the observation that mean field VI, while notoriously non-convex in the usual sense, is in fact displacement convex in the sense of optimal transport when $\rho$ is log-concave. This allows us to adapt techniques from the optimization literature on coordinate descent algorithms in Euclidean space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08792v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuel Arnese, Daniel Lacker</dc:creator>
    </item>
    <item>
      <title>Extending Mean-Field Variational Inference via Entropic Regularization: Theory and Computation</title>
      <link>https://arxiv.org/abs/2404.09113</link>
      <description>arXiv:2404.09113v1 Announce Type: cross 
Abstract: Variational inference (VI) has emerged as a popular method for approximate inference for high-dimensional Bayesian models. In this paper, we propose a novel VI method that extends the naive mean field via entropic regularization, referred to as $\Xi$-variational inference ($\Xi$-VI). $\Xi$-VI has a close connection to the entropic optimal transport problem and benefits from the computationally efficient Sinkhorn algorithm. We show that $\Xi$-variational posteriors effectively recover the true posterior dependency, where the dependence is downweighted by the regularization parameter. We analyze the role of dimensionality of the parameter space on the accuracy of $\Xi$-variational approximation and how it affects computational considerations, providing a rough characterization of the statistical-computational trade-off in $\Xi$-VI. We also investigate the frequentist properties of $\Xi$-VI and establish results on consistency, asymptotic normality, high-dimensional asymptotics, and algorithmic stability. We provide sufficient criteria for achieving polynomial-time approximate inference using the method. Finally, we demonstrate the practical advantage of $\Xi$-VI over mean-field variational inference on simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09113v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bohan Wu, David Blei</dc:creator>
    </item>
    <item>
      <title>Central and noncentral moments of the multivariate hypergeometric distribution</title>
      <link>https://arxiv.org/abs/2404.09118</link>
      <description>arXiv:2404.09118v1 Announce Type: cross 
Abstract: In this short note, explicit formulas are developed for the central and noncentral moments of the multivariate hypergeometric distribution. A numerical implementation is provided in Mathematica for fast evaluations. This work complements the paper by Ouimet (2021), where analogous formulas were derived and implemented in Mathematica for the multinomial distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09118v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>The central limit theorem for sum-functions of m-tuples of spacings</title>
      <link>https://arxiv.org/abs/2404.09581</link>
      <description>arXiv:2404.09581v1 Announce Type: cross 
Abstract: Let n points be taken at random on a circle of unit circumference and clockwise ordered. Uniform spacings are defined as the clockwise arc-lengths between the successive points from this sample. We are interested in the asymptotic behavior of the sum of functions of the m-tuples of successive spacings under the assumption that m can grow together with n. Asymptotic formulas for the expectation and variance of the sum and its asymptotic normality are established.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09581v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sherzod M. Mirakhmedov</dc:creator>
    </item>
    <item>
      <title>Finite-sample expansions for the optimal error probability in asymmetric binary hypothesis testing</title>
      <link>https://arxiv.org/abs/2404.09605</link>
      <description>arXiv:2404.09605v1 Announce Type: cross 
Abstract: The problem of binary hypothesis testing between two probability measures is considered. New sharp bounds are derived for the best achievable error probability of such tests based on independent and identically distributed observations. Specifically, the asymmetric version of the problem is examined, where different requirements are placed on the two error probabilities. Accurate nonasymptotic expansions with explicit constants are obtained for the error probability, using tools from large deviations and Gaussian approximation. Examples are shown indicating that, in the asymmetric regime, the approximations suggested by the new bounds are significantly more accurate than the approximations provided by either of the two main earlier approaches -- normal approximation and error exponents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09605v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valentinian Lungu, Ioannis Kontoyiannis</dc:creator>
    </item>
    <item>
      <title>Invariant Subspace Decomposition</title>
      <link>https://arxiv.org/abs/2404.09962</link>
      <description>arXiv:2404.09962v1 Announce Type: cross 
Abstract: We consider the task of predicting a response Y from a set of covariates X in settings where the conditional distribution of Y given X changes over time. For this to be feasible, assumptions on how the conditional distribution changes over time are required. Existing approaches assume, for example, that changes occur smoothly over time so that short-term prediction using only the recent past becomes feasible. In this work, we propose a novel invariance-based framework for linear conditionals, called Invariant Subspace Decomposition (ISD), that splits the conditional distribution into a time-invariant and a residual time-dependent component. As we show, this decomposition can be utilized both for zero-shot and time-adaptation prediction tasks, that is, settings where either no or a small amount of training data is available at the time points we want to predict Y at, respectively. We propose a practical estimation procedure, which automatically infers the decomposition using tools from approximate joint matrix diagonalization. Furthermore, we provide finite sample guarantees for the proposed estimator and demonstrate empirically that it indeed improves on approaches that do not use the additional invariant structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09962v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Margherita Lazzaretto, Jonas Peters, Niklas Pfister</dc:creator>
    </item>
    <item>
      <title>One-step corrected projected stochastic gradient descent for statistical estimation</title>
      <link>https://arxiv.org/abs/2306.05896</link>
      <description>arXiv:2306.05896v2 Announce Type: replace 
Abstract: A generic, fast and asymptotically efficient method for parametric estimation is described. It is based on the projected stochastic gradient descent on the log-likelihood function corrected by a single step of the Fisher scoring algorithm. We show theoretically and by simulations that it is an interesting alternative to the usual stochastic gradient descent with averaging or the adaptative stochastic gradient descent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.05896v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandre Brouste, Youssef Esstafa</dc:creator>
    </item>
    <item>
      <title>On posterior consistency of data assimilation with Gaussian process priors: the 2D Navier-Stokes equations</title>
      <link>https://arxiv.org/abs/2307.08136</link>
      <description>arXiv:2307.08136v2 Announce Type: replace 
Abstract: We consider a non-linear Bayesian data assimilation model for the periodic two-dimensional Navier-Stokes equations with initial condition modelled by a Gaussian process prior. We show that if the system is updated with sufficiently many discrete noisy measurements of the velocity field, then the posterior distribution eventually concentrates near the ground truth solution of the time evolution equation, and in particular that the initial condition is recovered consistently by the posterior mean vector field. We further show that the convergence rate can in general not be faster than inverse logarithmic in sample size, but describe specific conditions on the initial conditions when faster rates are possible. In the proofs we provide an explicit quantitative estimate for backward uniqueness of solutions of the two-dimensional Navier-Stokes equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.08136v2</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Richard Nickl, Edriss S. Titi</dc:creator>
    </item>
    <item>
      <title>A Distributionally Robust Estimator that Dominates the Empirical Average</title>
      <link>https://arxiv.org/abs/2402.10418</link>
      <description>arXiv:2402.10418v3 Announce Type: replace 
Abstract: We leverage the duality between risk-averse and distributionally robust optimization (DRO) to devise a distributionally robust estimator that strictly outperforms the empirical average for all probability measures having triple squared variance greater than the central fourth-order moment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10418v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolas Koumpis, Dionysis Kalogerias</dc:creator>
    </item>
    <item>
      <title>A Combinatorial Central Limit Theorem for Stratified Randomization</title>
      <link>https://arxiv.org/abs/2402.14764</link>
      <description>arXiv:2402.14764v2 Announce Type: replace 
Abstract: This paper establishes a combinatorial central limit theorem for stratified randomization, which holds under a Lindeberg-type condition. The theorem allows for an arbitrary number or sizes of strata, with the sole requirement being that each stratum contains at least two units. This flexibility accommodates both a growing number of large and small strata simultaneously, while imposing minimal conditions. We then apply this result to derive the asymptotic distributions of two test statistics proposed for instrumental variables settings in the presence of potentially many strata of unrestricted sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14764v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Purevdorj Tuvaandorj</dc:creator>
    </item>
    <item>
      <title>Double Cross-fit Doubly Robust Estimators: Beyond Series Regression</title>
      <link>https://arxiv.org/abs/2403.15175</link>
      <description>arXiv:2403.15175v2 Announce Type: replace 
Abstract: Doubly robust estimators with cross-fitting have gained popularity in causal inference due to their favorable structure-agnostic error guarantees. However, when additional structure, such as H\"{o}lder smoothness, is available then more accurate "double cross-fit doubly robust" (DCDR) estimators can be constructed by splitting the training data and undersmoothing nuisance function estimators on independent samples. We study a DCDR estimator of the Expected Conditional Covariance, a functional of interest in causal inference and conditional independence testing, and derive a series of increasingly powerful results with progressively stronger assumptions. We first provide a structure-agnostic error analysis for the DCDR estimator with no assumptions on the nuisance functions or their estimators. Then, assuming the nuisance functions are H\"{o}lder smooth, but without assuming knowledge of the true smoothness level or the covariate density, we establish that DCDR estimators with several linear smoothers are semiparametric efficient under minimal conditions and achieve fast convergence rates in the non-$\sqrt{n}$ regime. When the covariate density and smoothnesses are known, we propose a minimax rate-optimal DCDR estimator based on undersmoothed kernel regression. Moreover, we show an undersmoothed DCDR estimator satisfies a slower-than-$\sqrt{n}$ central limit theorem, and that inference is possible even in the non-$\sqrt{n}$ regime. Finally, we support our theoretical results with simulations, providing intuition for double cross-fitting and undersmoothing, demonstrating where our estimator achieves semiparametric efficiency while the usual "single cross-fit" estimator fails, and illustrating asymptotic normality for the undersmoothed DCDR estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15175v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alec McClean, Sivaraman Balakrishnan, Edward H. Kennedy, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Theoretical Guarantees for the Subspace-Constrained Tyler's Estimator</title>
      <link>https://arxiv.org/abs/2403.18658</link>
      <description>arXiv:2403.18658v2 Announce Type: replace 
Abstract: This work analyzes the subspace-constrained Tyler's estimator (STE) designed for recovering a low-dimensional subspace within a dataset that may be highly corrupted with outliers. It assumes a weak inlier-outlier model and allows the fraction of inliers to be smaller than a fraction that leads to computational hardness of the robust subspace recovery problem. It shows that in this setting, if the initialization of STE, which is an iterative algorithm, satisfies a certain condition, then STE can effectively recover the underlying subspace. It further shows that under the generalized haystack model, STE initialized by the Tyler's M-estimator (TME), can recover the subspace when the fraction of iniliers is too small for TME to handle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18658v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gilad Lerman, Feng Yu, Teng Zhang</dc:creator>
    </item>
    <item>
      <title>Universal Inference Meets Random Projections: A Scalable Test for Log-concavity</title>
      <link>https://arxiv.org/abs/2111.09254</link>
      <description>arXiv:2111.09254v4 Announce Type: replace-cross 
Abstract: Shape constraints yield flexible middle grounds between fully nonparametric and fully parametric approaches to modeling distributions of data. The specific assumption of log-concavity is motivated by applications across economics, survival modeling, and reliability theory. However, there do not currently exist valid tests for whether the underlying density of given data is log-concave. The recent universal inference methodology provides a valid test. The universal test relies on maximum likelihood estimation (MLE), and efficient methods already exist for finding the log-concave MLE. This yields the first test of log-concavity that is provably valid in finite samples in any dimension, for which we also establish asymptotic consistency results. Empirically, we find that a random projections approach that converts the d-dimensional testing problem into many one-dimensional problems can yield high power, leading to a simple procedure that is statistically and computationally efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.09254v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robin Dunn, Aditya Gangrade, Larry Wasserman, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Non-parametric estimation of the reaction term in semi-linear SPDEs with spatial ergodicity</title>
      <link>https://arxiv.org/abs/2307.05457</link>
      <description>arXiv:2307.05457v2 Announce Type: replace-cross 
Abstract: This paper discusses the non-parametric estimation of a non-linear reaction term in a semi-linear parabolic stochastic partial differential equation (SPDE). The estimator's consistency is due to the spatial ergodicity of the SPDE while the time horizon remains fixed. The analysis of the estimation error requires the concentration of spatial averages of non-linear transformations of the SPDE. The method developed in this paper combines the Clark-Ocone formula from Malliavin calculus with the Markovianity of the SPDE and density estimates. The resulting variance bound utilises the averaging effect of the conditional expectation in the Clark-Ocone formula. The method is applied to two realistic asymptotic regimes. The focus is on a coupling between the diffusivity and the noise level, where both tend to zero. Secondly, the observation of a fixed SPDE on a growing spatial observation window is considered. Furthermore, we prove the concentration of the occupation time around the occupation measure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.05457v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sascha Gaudlitz</dc:creator>
    </item>
    <item>
      <title>Sandwich Boosting for Accurate Estimation in Partially Linear Models for Grouped Data</title>
      <link>https://arxiv.org/abs/2307.11401</link>
      <description>arXiv:2307.11401v2 Announce Type: replace-cross 
Abstract: We study partially linear models in settings where observations are arranged in independent groups but may exhibit within-group dependence. Existing approaches estimate linear model parameters through weighted least squares, with optimal weights (given by the inverse covariance of the response, conditional on the covariates) typically estimated by maximising a (restricted) likelihood from random effects modelling or by using generalised estimating equations. We introduce a new 'sandwich loss' whose population minimiser coincides with the weights of these approaches when the parametric forms for the conditional covariance are well-specified, but can yield arbitrarily large improvements in linear parameter estimation accuracy when they are not. Under relatively mild conditions, our estimated coefficients are asymptotically Gaussian and enjoy minimal variance among estimators with weights restricted to a given class of functions, when user-chosen regression methods are used to estimate nuisance functions. We further expand the class of functional forms for the weights that may be fitted beyond parametric models by leveraging the flexibility of modern machine learning methods within a new gradient boosting scheme for minimising the sandwich loss. We demonstrate the effectiveness of both the sandwich loss and what we call 'sandwich boosting' in a variety of settings with simulated and real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11401v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elliot H. Young, Rajen D. Shah</dc:creator>
    </item>
    <item>
      <title>Concentrated Differential Privacy for Bandits</title>
      <link>https://arxiv.org/abs/2309.00557</link>
      <description>arXiv:2309.00557v3 Announce Type: replace-cross 
Abstract: Bandits serve as the theoretical foundation of sequential learning and an algorithmic foundation of modern recommender systems. However, recommender systems often rely on user-sensitive data, making privacy a critical concern. This paper contributes to the understanding of Differential Privacy (DP) in bandits with a trusted centralised decision-maker, and especially the implications of ensuring zero Concentrated Differential Privacy (zCDP). First, we formalise and compare different adaptations of DP to bandits, depending on the considered input and the interaction protocol. Then, we propose three private algorithms, namely AdaC-UCB, AdaC-GOPE and AdaC-OFUL, for three bandit settings, namely finite-armed bandits, linear bandits, and linear contextual bandits. The three algorithms share a generic algorithmic blueprint, i.e. the Gaussian mechanism and adaptive episodes, to ensure a good privacy-utility trade-off. We analyse and upper bound the regret of these three algorithms. Our analysis shows that in all of these settings, the prices of imposing zCDP are (asymptotically) negligible in comparison with the regrets incurred oblivious to privacy. Next, we complement our regret upper bounds with the first minimax lower bounds on the regret of bandits with zCDP. To prove the lower bounds, we elaborate a new proof technique based on couplings and optimal transport. We conclude by experimentally validating our theoretical results for the three different settings of bandits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.00557v3</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Achraf Azize, Debabrota Basu</dc:creator>
    </item>
    <item>
      <title>On the logic of interventionist counterfactuals under indeterministic causal laws</title>
      <link>https://arxiv.org/abs/2312.07223</link>
      <description>arXiv:2312.07223v2 Announce Type: replace-cross 
Abstract: We investigate the generalization of causal models to the case of indeterministic causal laws that was suggested in Halpern (2000). We give an overview of what differences in modeling are enforced by this more general perspective, and propose an implementation of generalized models in the style of the causal team semantics of Barbero &amp; Sandu (2020). In these models, the laws are not represented by functions (as in the deterministic case), but more generally by relations.
  We analyze significant differences in the axiomatization of interventionist counterfactuals in the indeterministic vs. the deterministic case, and provide strongly complete axiomatizations over the full class of indeterministic models and over its recursive subclass.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07223v2</guid>
      <category>cs.LO</category>
      <category>math.LO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-56940-1_11</arxiv:DOI>
      <dc:creator>Fausto Barbero</dc:creator>
    </item>
    <item>
      <title>Convex SGD: Generalization Without Early Stopping</title>
      <link>https://arxiv.org/abs/2401.04067</link>
      <description>arXiv:2401.04067v2 Announce Type: replace-cross 
Abstract: We consider the generalization error associated with stochastic gradient descent on a smooth convex function over a compact set. We show the first bound on the generalization error that vanishes when the number of iterations $T$ and the dataset size $n$ go to zero at arbitrary rates; our bound scales as $\tilde{O}(1/\sqrt{T} + 1/\sqrt{n})$ with step-size $\alpha_t = 1/\sqrt{t}$. In particular, strong convexity is not needed for stochastic gradient descent to generalize well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.04067v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julien Hendrickx, Alex Olshevsky</dc:creator>
    </item>
  </channel>
</rss>
