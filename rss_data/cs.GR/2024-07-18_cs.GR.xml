<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Jul 2024 01:57:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 18 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>SENC: Handling Self-collision in Neural Cloth Simulation</title>
      <link>https://arxiv.org/abs/2407.12479</link>
      <description>arXiv:2407.12479v1 Announce Type: new 
Abstract: We present SENC, a novel self-supervised neural cloth simulator that addresses the challenge of cloth self-collision. This problem has remained unresolved due to the gap in simulation setup between recent collision detection and response approaches and self-supervised neural simulators. The former requires collision-free initial setups, while the latter necessitates random cloth instantiation during training. To tackle this issue, we propose a novel loss based on Global Intersection Analysis (GIA). This loss extracts the volume surrounded by the cloth region that forms the penetration. By constructing an energy based on this volume, our self-supervised neural simulator can effectively address cloth self-collisions. Moreover, we develop a self-collision-aware graph neural network capable of learning to handle self-collisions, even for parts that are topologically distant from one another. Additionally, we introduce an effective external force scheme that enables the simulation to learn the cloth's behavior in response to random external forces. We validate the efficacy of SENC through extensive quantitative and qualitative experiments, demonstrating that it effectively reduces cloth self-collision while maintaining high-quality animation results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12479v1</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhouyingcheng Liao, Sinan Wang, Taku Komura</dc:creator>
    </item>
    <item>
      <title>Interconnected virtual space and Theater. Practice as research on theater stage in the era of the network</title>
      <link>https://arxiv.org/abs/2407.11989</link>
      <description>arXiv:2407.11989v1 Announce Type: cross 
Abstract: Since 2014, we have been conducting experiments based on a multidisciplinary collaboration between specialists in theatrical staging and researchers in virtual reality, digital art, and video games. This team focused its work on the similarities and differencesthat exist between real physical actors (actor-performers) and virtual digital actors (avatars). From this multidisciplinary approach, experimental research-creation projects have emerged and rely on a physical actor playing with the image of an avatar, controlled by another physical actor via the intermediary of a low-cost motion-capture system. In the first part of the paper, we will introduce the scenographic design on which our presentation is based, and the modifications we have made in relation to our previous work. Next, in the second section, we will discuss in detail the impact of augmenting the player's game using an avatar, compared to the scenic limitations of the theatrical stage. In part three of the paper, we will discuss the software-related aspects of the project, focusing on exchanges between the different components of our design and describing the algorithms enabling us to utilize the real-time movement of a player via various capture devices. To conclude, we will examine in detail how our experimental system linking physical actors and avatars profoundly alters the nature of collaboration between directors, actors, and digital artists in terms of actor/avatar direction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11989v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/9781119549765.ch8</arxiv:DOI>
      <dc:creator>Georges Gagner\'e (INREV), C\'edric Plessiet (INREV), R\'emy Sohier (INREV)</dc:creator>
    </item>
    <item>
      <title>Flowers Revisited: A Preliminary Replication of Flowers et al. 1997</title>
      <link>https://arxiv.org/abs/2407.11992</link>
      <description>arXiv:2407.11992v1 Announce Type: cross 
Abstract: In 1997, Flowers, Buhman, and Turnage published a paper titled ``Cross-Modal Equivalence of Visual and Auditory Scatterplots for Exploring Bivariate Data Samples.'' This paper examined our capacity to assess the relationship between two data variables when presented through visual or auditory scatterplots. Twenty-seven years later, we have replicated the first part of this influential study and present the preliminary findings of our replication, initially involving 21 participants. In addition to purely auditory and visual scatterplots, we introduced audiovisual scatterplots as a third condition in our experiment. Our initial findings mirror those of Flowers et al.'s original research. With this extended abstract, we also aim to spark a discussion about the significance of replication studies for our research community in general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11992v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <category>cs.MM</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kajetan Enge, Liam Fabry, Robert H\"oldrich</dc:creator>
    </item>
    <item>
      <title>From Top-Right to User-Right: Perceptual Prioritization of Point-Feature Label Positions</title>
      <link>https://arxiv.org/abs/2407.11996</link>
      <description>arXiv:2407.11996v1 Announce Type: cross 
Abstract: In cartography, Geographic Information Systems (GIS), and the entire field of visualization, the position of a label relative to its point feature is pivotal for ensuring visualization readability and improving the user experience. The label placement is governed by the Position Priority Order (PPO), a systematic raking of potential label positions around a point feature according to predetermined priorities. Traditional PPOs have relied heavily on typographic and cartographic conventions established decades ago, which may no longer align with the expectations of today's users. Our extensive user study introduces the Perceptual Position Priority Order (PerceptPPO), a user-validated PPO that significantly departed from traditional conventions. A key finding of our research is the identification of an exact order of label positions, with labels placed at the top of point features being significantly preferred by users, contrary to the conventional top-right position. Furthermore, we performed a supplemental user study to find users' preferred label density - an area scarcely explored in prior research - of a generic map. Finally, we conducted a comparative user study assessing the perceived quality of PerceptPPO compared to existing PPOs. The outcome confirmed PerceptPPO's superior perception among users, advocating its adoption not only in future cartographic and GIS applications but also across various types of visualizations. The effectiveness of PerceptPPO is supported by nearly 800 participants from 48 countries, who collectively contributed to over 45,500 pairwise comparisons across three studies. Our research not only proposes a novel PPO to the research community but also offers practical guidance for designers and application developers aiming to optimize user engagement and comprehension, paving the way for more intuitive and accessible visual solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11996v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Petr Bob\'ak, Ladislav \v{C}mol\'ik, Martin \v{C}ad\'ik</dc:creator>
    </item>
    <item>
      <title>Generating 3D House Wireframes with Semantics</title>
      <link>https://arxiv.org/abs/2407.12267</link>
      <description>arXiv:2407.12267v1 Announce Type: cross 
Abstract: We present a new approach for generating 3D house wireframes with semantic enrichment using an autoregressive model. Unlike conventional generative models that independently process vertices, edges, and faces, our approach employs a unified wire-based representation for improved coherence in learning 3D wireframe structures. By re-ordering wire sequences based on semantic meanings, we facilitate seamless semantic integration during sequence generation. Our two-phase technique merges a graph-based autoencoder with a transformer-based decoder to learn latent geometric tokens and generate semantic-aware wireframes. Through iterative prediction and decoding during inference, our model produces detailed wireframes that can be easily segmented into distinct components, such as walls, roofs, and rooms, reflecting the semantic essence of the shape. Empirical results on a comprehensive house dataset validate the superior accuracy, novelty, and semantic fidelity of our model compared to existing generative models. More results and details can be found on https://vcc.tech/research/2024/3DWire.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12267v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xueqi Ma, Yilin Liu, Wenjun Zhou, Ruowei Wang, Hui Huang</dc:creator>
    </item>
    <item>
      <title>A Practical Solver for Scalar Data Topological Simplification</title>
      <link>https://arxiv.org/abs/2407.12399</link>
      <description>arXiv:2407.12399v1 Announce Type: cross 
Abstract: This paper presents a practical approach for the optimization of topological simplification, a central pre-processing step for the analysis and visualization of scalar data. Given an input scalar field f and a set of "signal" persistence pairs to maintain, our approach produces an output field g that is close to f and which optimizes (i) the cancellation of "non-signal" pairs, while (ii) preserving the "signal" pairs. In contrast to pre-existing simplification algorithms, our approach is not restricted to persistence pairs involving extrema and can thus address a larger class of topological features, in particular saddle pairs in three-dimensional scalar data. Our approach leverages recent generic persistence optimization frameworks and extends them with tailored accelerations specific to the problem of topological simplification. Extensive experiments report substantial accelerations over these frameworks, thereby making topological simplification optimization practical for real-life datasets. Our approach enables a direct visualization and analysis of the topologically simplified data, e.g., via isosurfaces of simplified topology (fewer components and handles). We apply our approach to the extraction of prominent filament structures in three-dimensional data. Specifically, we show that our pre-simplification of the data leads to practical improvements over standard topological techniques for removing filament loops. We also show how our approach can be used to repair genus defects in surface processing. Finally, we provide a C++ implementation for reproducibility purposes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12399v1</guid>
      <category>cs.LG</category>
      <category>cs.CG</category>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohamed Kissi, Mathieu Pont, Joshua A. Levine, Julien Tierny</dc:creator>
    </item>
    <item>
      <title>Decoupled Edge Physics algorithms for collaborative XR simulations</title>
      <link>https://arxiv.org/abs/2407.12486</link>
      <description>arXiv:2407.12486v1 Announce Type: cross 
Abstract: This work proposes a novel approach to transform any modern game engine pipeline, for optimized performance and enhanced user experiences in Extended Reality (XR) environments. Decoupling the physics engine from the game engine pipeline and using a client-server N-1 architecture creates a scalable solution, efficiently serving multiple graphics clients on Head-Mounted Displays (HMDs) with a single physics engine on edge-cloud infrastructure. This approach ensures better synchronization in multiplayer scenarios without introducing overhead in single-player experiences, maintaining session continuity despite changes in user participation. Relocating the Physics Engine to an edge or cloud node reduces strain on local hardware, dedicating more resources to high-quality rendering and unlocking the full potential of untethered HMDs. We present four algorithms that decouple the physics engine, increasing frame rates and Quality of Experience (QoE) in VR simulations, supporting advanced interactions, numerous physics objects, and multi-user sessions with over 100 concurrent users. Incorporating a Geometric Algebra interpolator reduces inter-calls between dissected parts, maintaining QoE and easing network stress. Experimental validation, with more than 100 concurrent users, 10,000 physics objects, and softbody simulations, confirms the technical viability of the proposed architecture, showcasing transformative capabilities for more immersive and collaborative XR applications without compromising performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12486v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>George Kokiadis, Antonis Protopsaltis, Michalis Morfiadakis, Nick Lydatakis, George Papagiannakis</dc:creator>
    </item>
    <item>
      <title>EmoFace: Audio-driven Emotional 3D Face Animation</title>
      <link>https://arxiv.org/abs/2407.12501</link>
      <description>arXiv:2407.12501v1 Announce Type: cross 
Abstract: Audio-driven emotional 3D face animation aims to generate emotionally expressive talking heads with synchronized lip movements. However, previous research has often overlooked the influence of diverse emotions on facial expressions or proved unsuitable for driving MetaHuman models. In response to this deficiency, we introduce EmoFace, a novel audio-driven methodology for creating facial animations with vivid emotional dynamics. Our approach can generate facial expressions with multiple emotions, and has the ability to generate random yet natural blinks and eye movements, while maintaining accurate lip synchronization. We propose independent speech encoders and emotion encoders to learn the relationship between audio, emotion and corresponding facial controller rigs, and finally map into the sequence of controller values. Additionally, we introduce two post-processing techniques dedicated to enhancing the authenticity of the animation, particularly in blinks and eye movements. Furthermore, recognizing the scarcity of emotional audio-visual data suitable for MetaHuman model manipulation, we contribute an emotional audio-visual dataset and derive control parameters for each frames. Our proposed methodology can be applied in producing dialogues animations of non-playable characters (NPCs) in video games, and driving avatars in virtual reality environments. Our further quantitative and qualitative experiments, as well as an user study comparing with existing researches show that our approach demonstrates superior results in driving 3D facial models. The code and sample data are available at https://github.com/SJTU-Lucy/EmoFace.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12501v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/VR58804.2024.00060</arxiv:DOI>
      <dc:creator>Chang Liu, Qunfen Lin, Zijiao Zeng, Ye Pan</dc:creator>
    </item>
    <item>
      <title>Generalizable Human Gaussians for Sparse View Synthesis</title>
      <link>https://arxiv.org/abs/2407.12777</link>
      <description>arXiv:2407.12777v1 Announce Type: cross 
Abstract: Recent progress in neural rendering has brought forth pioneering methods, such as NeRF and Gaussian Splatting, which revolutionize view rendering across various domains like AR/VR, gaming, and content creation. While these methods excel at interpolating {\em within the training data}, the challenge of generalizing to new scenes and objects from very sparse views persists. Specifically, modeling 3D humans from sparse views presents formidable hurdles due to the inherent complexity of human geometry, resulting in inaccurate reconstructions of geometry and textures. To tackle this challenge, this paper leverages recent advancements in Gaussian Splatting and introduces a new method to learn generalizable human Gaussians that allows photorealistic and accurate view-rendering of a new human subject from a limited set of sparse views in a feed-forward manner. A pivotal innovation of our approach involves reformulating the learning of 3D Gaussian parameters into a regression process defined on the 2D UV space of a human template, which allows leveraging the strong geometry prior and the advantages of 2D convolutions. In addition, a multi-scaffold is proposed to effectively represent the offset details. Our method outperforms recent methods on both within-dataset generalization as well as cross-dataset generalization settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12777v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youngjoong Kwon, Baole Fang, Yixing Lu, Haoye Dong, Cheng Zhang, Francisco Vicente Carrasco, Albert Mosella-Montoro, Jianjin Xu, Shingo Takagi, Daeil Kim, Aayush Prakash, Fernando De la Torre</dc:creator>
    </item>
    <item>
      <title>SMooDi: Stylized Motion Diffusion Model</title>
      <link>https://arxiv.org/abs/2407.12783</link>
      <description>arXiv:2407.12783v1 Announce Type: cross 
Abstract: We introduce a novel Stylized Motion Diffusion model, dubbed SMooDi, to generate stylized motion driven by content texts and style motion sequences. Unlike existing methods that either generate motion of various content or transfer style from one sequence to another, SMooDi can rapidly generate motion across a broad range of content and diverse styles. To this end, we tailor a pre-trained text-to-motion model for stylization. Specifically, we propose style guidance to ensure that the generated motion closely matches the reference style, alongside a lightweight style adaptor that directs the motion towards the desired style while ensuring realism. Experiments across various applications demonstrate that our proposed framework outperforms existing methods in stylized motion generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12783v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lei Zhong, Yiming Xie, Varun Jampani, Deqing Sun, Huaizu Jiang</dc:creator>
    </item>
    <item>
      <title>Diverse Part Synthesis for 3D Shape Creation</title>
      <link>https://arxiv.org/abs/2401.09384</link>
      <description>arXiv:2401.09384v3 Announce Type: replace 
Abstract: Methods that use neural networks for synthesizing 3D shapes in the form of a part-based representation have been introduced over the last few years. These methods represent shapes as a graph or hierarchy of parts and enable a variety of applications such as shape sampling and reconstruction. However, current methods do not allow easily regenerating individual shape parts according to user preferences. In this paper, we investigate techniques that allow the user to generate multiple, diverse suggestions for individual parts. Specifically, we experiment with multimodal deep generative models that allow sampling diverse suggestions for shape parts and focus on models which have not been considered in previous work on shape synthesis. To provide a comparative study of these techniques, we introduce a method for synthesizing 3D shapes in a part-based representation and evaluate all the part suggestion techniques within this synthesis method. In our method, which is inspired by previous work, shapes are represented as a set of parts in the form of implicit functions which are then positioned in space to form the final shape. Synthesis in this representation is enabled by a neural network architecture based on an implicit decoder and a spatial transformer. We compare the various multimodal generative models by evaluating their performance in generating part suggestions. Our contribution is to show with qualitative and quantitative evaluations which of the new techniques for multimodal part generation perform the best and that a synthesis method based on the top-performing techniques allows the user to more finely control the parts that are generated in the 3D shapes while maintaining high shape fidelity when reconstructing shapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09384v3</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanran Guan, Oliver van Kaick</dc:creator>
    </item>
    <item>
      <title>Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold</title>
      <link>https://arxiv.org/abs/2305.10973</link>
      <description>arXiv:2305.10973v2 Announce Type: replace-cross 
Abstract: Synthesizing visual content that meets users' needs often requires flexible and precise controllability of the pose, shape, expression, and layout of the generated objects. Existing approaches gain controllability of generative adversarial networks (GANs) via manually annotated training data or a prior 3D model, which often lack flexibility, precision, and generality. In this work, we study a powerful yet much less explored way of controlling GANs, that is, to "drag" any points of the image to precisely reach target points in a user-interactive manner, as shown in Fig.1. To achieve this, we propose DragGAN, which consists of two main components: 1) a feature-based motion supervision that drives the handle point to move towards the target position, and 2) a new point tracking approach that leverages the discriminative generator features to keep localizing the position of the handle points. Through DragGAN, anyone can deform an image with precise control over where pixels go, thus manipulating the pose, shape, expression, and layout of diverse categories such as animals, cars, humans, landscapes, etc. As these manipulations are performed on the learned generative image manifold of a GAN, they tend to produce realistic outputs even for challenging scenarios such as hallucinating occluded content and deforming shapes that consistently follow the object's rigidity. Both qualitative and quantitative comparisons demonstrate the advantage of DragGAN over prior approaches in the tasks of image manipulation and point tracking. We also showcase the manipulation of real images through GAN inversion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.10973v2</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3588432.3591500</arxiv:DOI>
      <dc:creator>Xingang Pan, Ayush Tewari, Thomas Leimk\"uhler, Lingjie Liu, Abhimitra Meka, Christian Theobalt</dc:creator>
    </item>
    <item>
      <title>Street TryOn: Learning In-the-Wild Virtual Try-On from Unpaired Person Images</title>
      <link>https://arxiv.org/abs/2311.16094</link>
      <description>arXiv:2311.16094v3 Announce Type: replace-cross 
Abstract: Most virtual try-on research is motivated to serve the fashion business by generating images to demonstrate garments on studio models at a lower cost. However, virtual try-on should be a broader application that also allows customers to visualize garments on themselves using their own casual photos, known as in-the-wild try-on. Unfortunately, the existing methods, which achieve plausible results for studio try-on settings, perform poorly in the in-the-wild context. This is because these methods often require paired images (garment images paired with images of people wearing the same garment) for training. While such paired data is easy to collect from shopping websites for studio settings, it is difficult to obtain for in-the-wild scenes.
  In this work, we fill the gap by (1) introducing a StreetTryOn benchmark to support in-the-wild virtual try-on applications and (2) proposing a novel method to learn virtual try-on from a set of in-the-wild person images directly without requiring paired data. We tackle the unique challenges, including warping garments to more diverse human poses and rendering more complex backgrounds faithfully, by a novel DensePose warping correction method combined with diffusion-based conditional inpainting. Our experiments show competitive performance for standard studio try-on tasks and SOTA performance for street try-on and cross-domain try-on tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16094v3</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aiyu Cui, Jay Mahajan, Viraj Shah, Preeti Gomathinayagam, Chang Liu, Svetlana Lazebnik</dc:creator>
    </item>
  </channel>
</rss>
