<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Feb 2025 05:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Accessing LLMs for Front-end Software Architecture Knowledge</title>
      <link>https://arxiv.org/abs/2502.19518</link>
      <description>arXiv:2502.19518v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated significant promise in automating software development tasks, yet their capabilities with respect to software design tasks remains largely unclear. This study investigates the capabilities of an LLM in understanding, reproducing, and generating structures within the complex VIPER architecture, a design pattern for iOS applications. We leverage Bloom's taxonomy to develop a comprehensive evaluation framework to assess the LLM's performance across different cognitive domains such as remembering, understanding, applying, analyzing, evaluating, and creating. Experimental results, using ChatGPT 4 Turbo 2024-04-09, reveal that the LLM excelled in higher-order tasks like evaluating and creating, but faced challenges with lower-order tasks requiring precise retrieval of architectural details. These findings highlight both the potential of LLMs to reduce development costs and the barriers to their effective application in real-world software design scenarios. This study proposes a benchmark format for assessing LLM capabilities in software architecture, aiming to contribute toward more robust and accessible AI-driven development tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19518v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L. P. Franciscatto Guerra, N. Ernst</dc:creator>
    </item>
    <item>
      <title>Unveiling Security Weaknesses in Autonomous Driving Systems: An In-Depth Empirical Study</title>
      <link>https://arxiv.org/abs/2502.19687</link>
      <description>arXiv:2502.19687v1 Announce Type: new 
Abstract: The advent of Autonomous Driving Systems (ADS) has marked a significant shift towards intelligent transportation, with implications for public safety and traffic efficiency. While these systems integrate a variety of technologies and offer numerous benefits, their security is paramount, as vulnerabilities can have severe consequences for safety and trust. This study aims to systematically investigate potential security weaknesses in the codebases of prominent open-source ADS projects using CodeQL, a static code analysis tool. The goal is to identify common vulnerabilities, their distribution and persistence across versions to enhance the security of ADS. We selected three representative open-source ADS projects, Autoware, AirSim, and Apollo, based on their high GitHub star counts and Level 4 autonomous driving capabilities. Using CodeQL, we analyzed multiple versions of these projects to identify vulnerabilities, focusing on CWE categories such as CWE-190 (Integer Overflow or Wraparound) and CWE-20 (Improper Input Validation). We also tracked the lifecycle of these vulnerabilities across software versions. This approach allows us to systematically analyze vulnerabilities in projects, which has not been extensively explored in previous ADS research. Our analysis revealed that specific CWE categories, particularly CWE-190 (59.6%) and CWE-20 (16.1%), were prevalent across the selected ADS projects. These vulnerabilities often persisted for over six months, spanning multiple version iterations. The empirical assessment showed a direct link between the severity of these vulnerabilities and their tangible effects on ADS performance. These security issues among ADS still remain to be resolved. Our findings highlight the need for integrating static code analysis into ADS development to detect and mitigate common vulnerabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19687v1</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenyuan Cheng, Zengyang Li, Peng Liang, Ran Mo, Hui Liu</dc:creator>
    </item>
    <item>
      <title>Bridging the PLC Binary Analysis Gap: A Cross-Compiler Dataset and Neural Framework for Industrial Control Systems</title>
      <link>https://arxiv.org/abs/2502.19725</link>
      <description>arXiv:2502.19725v1 Announce Type: new 
Abstract: Industrial Control Systems (ICS) rely heavily on Programmable Logic Controllers (PLCs) to manage critical infrastructure, yet analyzing PLC executables remains challenging due to diverse proprietary compilers and limited access to source code. To bridge this gap, we introduce PLC-BEAD, a comprehensive dataset containing 2431 compiled binaries from 700+ PLC programs across four major industrial compilers (CoDeSys, GEB, OpenPLC-V2, OpenPLC-V3). This novel dataset uniquely pairs each binary with its original Structured Text source code and standardized functionality labels, enabling both binary-level and source-level analysis. We demonstrate the dataset's utility through PLCEmbed, a transformer-based framework for binary code analysis that achieves 93\% accuracy in compiler provenance identification and 42\% accuracy in fine-grained functionality classification across 22 industrial control categories. Through comprehensive ablation studies, we analyze how compiler optimization levels, code patterns, and class distributions influence model performance. We provide detailed documentation of the dataset creation process, labeling taxonomy, and benchmark protocols to ensure reproducibility. Both PLC-BEAD and PLCEmbed are released as open-source resources to foster research in PLC security, reverse engineering, and ICS forensics, establishing new baselines for data-driven approaches to industrial cybersecurity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19725v1</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yonatan Gizachew Achamyeleh, Shih-Yuan Yu, Gustavo Quir\'os Araya, Mohammad Abdullah Al Faruque</dc:creator>
    </item>
    <item>
      <title>ConvCodeWorld: Benchmarking Conversational Code Generation in Reproducible Feedback Environments</title>
      <link>https://arxiv.org/abs/2502.19852</link>
      <description>arXiv:2502.19852v1 Announce Type: new 
Abstract: Large language models (LLMs) have proven invaluable for code generation, particularly in interactive settings. However, existing code generation benchmarks fail to capture the diverse feedback encountered in multi-turn interactions, limiting our ability to evaluate LLMs in these contexts. To address this gap, we present a set of novel benchmarks that explicitly model the quality of feedback provided to code generation LLMs. Our contributions are threefold: First, we introduce CONVCODEWORLD, a novel and reproducible environment for benchmarking interactive code generation. CONVCODEWORLD simulates 9 distinct interactive code generation scenarios while systematically combining three types of feedback: (a) compilation feedback; (b) execution feedback with varying test coverage; (c) verbal feedback generated by GPT-4o with different levels of expertise. Second, we introduce CONVCODEBENCH, a fast, static version of benchmark that uses pre-generated feedback logs, eliminating the need for costly dynamic verbal feedback generation while maintaining strong Spearman's rank correlations (0.82 to 0.99) with CONVCODEWORLD. Third, extensive evaluations of both closed-source and open-source LLMs including R1-Distill on CONVCODEWORLD reveal key insights: (a) LLM performance varies significantly based on the feedback provided; (b) Weaker LLMs, with sufficient feedback, can outperform single-turn results of state-of-the-art LLMs without feedback; (c) Training on a specific feedback combination can limit an LLM's ability to utilize unseen combinations; (d) LLMs solve problems in fewer turns (high MRR) may not solve as many problems overall (high Recall), and vice versa. All implementations and benchmarks will be made publicly available at https://huggingface.co/spaces/ConvCodeWorld/ConvCodeWorld</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19852v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hojae Han, Seung-won Hwang, Rajhans Samdani, Yuxiong He</dc:creator>
    </item>
    <item>
      <title>RacerF: Data Race Detection with Frama-C (Competition Contribution)</title>
      <link>https://arxiv.org/abs/2502.20052</link>
      <description>arXiv:2502.20052v1 Announce Type: new 
Abstract: RacerF is a static analyser for detection of data races in multithreaded C programs implemented as a plugin of the Frama-C platform. The approach behind RacerF is mostly heuristic and relies on analysis of the sequential behaviour of particular threads whose results are generalised using a combination of under- and over-approximating techniques to allow analysis of the multithreading behaviour. In particular, in SV-COMP'25, RacerF relies on the Frama-C's abstract interpreter EVA to perform the analysis of the sequential behaviour. Although RacerF does not provide any formal guarantees, it ranked second in the NoDataRace-Main sub-category, providing the largest number of correct results (when excluding metaverifiers) and just 4 false positives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20052v1</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom\'a\v{s} Dac\'ik, Tom\'a\v{s} Vojnar</dc:creator>
    </item>
    <item>
      <title>SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning</title>
      <link>https://arxiv.org/abs/2502.20127</link>
      <description>arXiv:2502.20127v1 Announce Type: new 
Abstract: Mainstream issue-resolving frameworks predominantly rely on commercial models, leading to high costs and privacy concerns. Existing training approaches for issue resolving struggle with poor generalization and fail to fully leverage open-source development resources. We propose Subtask-oriented Reinforced Fine-Tuning (SoRFT), a novel training approach to enhance the issue resolving capability of LLMs. We decomposes issue resolving into structured subtasks: file localization, function localization, line localization, and code edit generation. SoRFT consists of two training stages: (1) rejection-sampled supervised fine-tuning, Chain of Thought (CoT) data is filtered using ground-truth before fine-tuning the LLM, and (2) rule-based reinforcement learning, which leverages PPO with ground-truth based rewards. We evaluate the SoRFT-trained model on SWE-Bench Verified and SWE-Bench Lite, achieving state-of-the-art (SOTA) performance among open-source models (e.g., resolve 21.4% issues on SWE-Bench Verified with SoRFT-Qwen-7B). The experimental results demonstrate that SoRFT significantly enhances issue-resolving performance, improves model generalization, and provides a cost-efficient alternative to commercial models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20127v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Zexiong Ma, Chao Peng, Pengfei Gao, Xiangxin Meng, Yanzhen Zou, Bing Xie</dc:creator>
    </item>
    <item>
      <title>Gender Dynamics in Software Engineering: Insights from Research on Concurrency Bug Reproduction</title>
      <link>https://arxiv.org/abs/2502.20289</link>
      <description>arXiv:2502.20289v1 Announce Type: new 
Abstract: Reproducing concurrency bugs is a complex task due to their unpredictable behavior. Researchers, regardless of gender, are contributing to automating this complex task to aid software developers. While some studies have investigated gender roles in the broader software industry, limited research exists on gender representation specifically among researchers working in concurrent bug reproduction. To address this gap, in this paper, we present a literature review to assess the gender ratio in this field. We also explore potential variations in technique selection and bug-type focus across genders. Our findings indicate that female researchers are underrepresented compared to their male counterparts in this area, with a current male-to-female author ratio of 29:6. Through this study, we emphasize the importance of fostering gender equity in software engineering research, ensuring a diversity of perspectives in the development of automated bug reproduction tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20289v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tarannum Shaila Zaman, Macharla Hemanth Kishan, Lutfun Nahar Lota</dc:creator>
    </item>
    <item>
      <title>Systems-of-Systems for Environmental Sustainability: A Systematic Mapping Study</title>
      <link>https://arxiv.org/abs/2502.20021</link>
      <description>arXiv:2502.20021v1 Announce Type: cross 
Abstract: Environmental sustainability in Systems-of-Systems (SoS) is an emerging field that seeks to integrate technological solutions to promote the efficient management of natural resources. While systematic reviews address sustainability in the context of Smart Cities (a category of SoS), a systematic study synthesizing the existing knowledge on environmental sustainability applied to SoS in general does not exist. Although literature includes other types of sustainability, such as financial and social, this study focuses on environmental sustainability, analyzing how SoS contribute to sustainable practices such as carbon emission reduction, energy efficiency, and biodiversity conservation. We conducted a Systematic Mapping Study to identify the application domains of SoS in sustainability, the challenges faced, and research opportunities. We planned and executed a research protocol including an automated search over four scientific databases. Of 926 studies retrieved, we selected, analyzed, and reported the results of 39 relevant studies. Our findings reveal that most studies focus on Smart Cities and Smart Grids, while applications such as sustainable agriculture and wildfire prevention are less explored. We identified challenges such as system interoperability, scalability, and data governance. Finally, we propose future research directions for SoS and environmental sustainability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20021v1</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ana Clara Ara\'ujo Gomes da Silva, Gilmar Teixeira Junior, L\'ivia Mancine C. de Campos, Renato F. Bulc\~ao-Neto, Valdemar Vicente Graciano Neto</dc:creator>
    </item>
    <item>
      <title>CodeRAG-Bench: Can Retrieval Augment Code Generation?</title>
      <link>https://arxiv.org/abs/2406.14497</link>
      <description>arXiv:2406.14497v2 Announce Type: replace 
Abstract: While language models (LMs) have proven remarkably adept at generating code, many programs are challenging for LMs to generate using their parametric knowledge alone. Providing external contexts such as library documentation can facilitate generating accurate and functional code. Despite the success of retrieval-augmented generation (RAG) in various text-oriented tasks, its potential for improving code generation remains under-explored. In this work, we conduct a systematic, large-scale analysis by asking: in what scenarios can retrieval benefit code generation models? and what challenges remain? We first curate a comprehensive evaluation benchmark, CodeRAG-Bench, encompassing three categories of code generation tasks, including basic programming, open-domain, and repository-level problems. We aggregate documents from five sources for models to retrieve contexts: competition solutions, online tutorials, library documentation, StackOverflow posts, and GitHub repositories. We examine top-performing models on CodeRAG-Bench by providing contexts retrieved from one or multiple sources. While notable gains are made in final code generation by retrieving high-quality contexts across various settings, our analysis reveals room for improvement -- current retrievers still struggle to fetch useful contexts especially with limited lexical overlap, and generators fail to improve with limited context lengths or abilities to integrate additional contexts. We hope CodeRAG-Bench serves as an effective testbed to encourage further development of advanced code-oriented RAG methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14497v2</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zora Zhiruo Wang, Akari Asai, Xinyan Velocity Yu, Frank F. Xu, Yiqing Xie, Graham Neubig, Daniel Fried</dc:creator>
    </item>
    <item>
      <title>Bugfix: a standard language, database schema and repository for research on bugs and automatic program repair</title>
      <link>https://arxiv.org/abs/2502.15599</link>
      <description>arXiv:2502.15599v2 Announce Type: replace 
Abstract: Automatic Program Repair (APR) is a brilliant idea: when detecting a bug, also provide suggestions for correcting the program. Progress towards that goal is hindered by the absence of a common frame of reference for the multiplicity of APR ideas, methods, tools, programming languages and environments.
  Bugfix is an effort at providing such a framework: a standardized set of notations, tools and interfaces, as well as a database of bugs and fixes, for use by the APR research community to try out ideas and compare results.
  The most directly visible component of the Bugfix effort is the Bugfix language, a human-readable formalism making it possible to describe elements of the following kinds: a bug (described abstractly, for example the permutation of two arguments in a call); a bug example (an actual occurrence of a bug, in a specific code written in a specific programming language, and usually recorded in some repository); a fix (a particular correction of a bug, obtained for example by reversing the misplaced arguments); an application (an entity that demonstrates how a actual code example matches with a fix); a construct (the abstract description of a programming mechanism, for example a ``while'' loop, independently of its realization in a programming language; and a language (a description of how a particular programming language includes certain constructs and provides specific concrete syntax for each of them -- for example Java includes loop, assignment etc. and has a defined format for each of them).
  A JSON API provides it in a form accessible to tools. Bugfix includes a repository containing a considerable amount of bugs, examples and fixes.
  Note: An early step towards this article was a short contribution (Ref [1]) to the 2024 ICSE. The present text reuses a few elements of introduction and motivation but is otherwise thoroughly reworked and extended.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15599v2</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victoria Kananchuk, Ilgiz Mustafin, Bertrand Meyer</dc:creator>
    </item>
    <item>
      <title>Programming Really Is Simple Mathematics</title>
      <link>https://arxiv.org/abs/2502.17149</link>
      <description>arXiv:2502.17149v3 Announce Type: replace 
Abstract: A re-construction of the fundamentals of programming as a small mathematical theory (PRISM) based on elementary set theory. Highlights:
  $\bullet$ Zero axioms. No properties are assumed, all are proved (from standard set theory).
  $\bullet$ A single concept covers specifications and programs.
  $\bullet$ Its definition only involves one relation and one set.
  $\bullet$ Everything proceeds from three operations: choice, composition and restriction.
  $\bullet$ These techniques suffice to derive the axioms of classic papers on the "laws of programming" as consequences and prove them mechanically.
  $\bullet$ The ordinary subset operator suffices to define both the notion of program correctness and the concepts of specialization and refinement.
  $\bullet$ From this basis, the theory deduces dozens of theorems characterizing important properties of programs and programming.
  $\bullet$ All these theorems have been mechanically verified (using Isabelle/HOL); the proofs are available in a public repository. This paper is a considerable extension and rewrite of an earlier contribution [arXiv:1507.00723]</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17149v3</guid>
      <category>cs.SE</category>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bertrand Meyer, Reto Weber</dc:creator>
    </item>
    <item>
      <title>UOOR: Seamless and Traceable Requirements</title>
      <link>https://arxiv.org/abs/2502.18617</link>
      <description>arXiv:2502.18617v2 Announce Type: replace 
Abstract: In industrial practice, requirements are an indispensable element of any serious software project. In the academic study of software engineering, requirements are one of the heavily researched subjects. And yet requirements engineering, as practiced in industry, makes shockingly sparse use of the concepts propounded in the requirements literature. The present paper starts from an assumption about the causes for this situation and proposes a remedy to redress it. The posited explanation is that change is the major factor affecting the practical application of even the best-intentioned requirements techniques. No sooner has the ink dried on the specifications than the system environment and stakeholders' views of the system begin to evolve.
  The proposed solution is a requirements engineering method, called UOOR, which unifies many known requirements concepts and a few new ones in a framework entirely devised to accommodate and support seamless change throughout the project lifecycle.
  The method encompasses the commonly used requirements techniques, namely, scenarios, and integrates them into the seamless software development process. The work presented here introduces the notion of seamless requirements traceability, which relies on the propagation of traceability links, themselves based on formal properties of relations between project artifacts. As a proof of concept, the paper presents a traceability tool to be integrated into a general-purpose IDE that provides the ability to link requirements to other software project artifacts, display notifications of changes in requirements, and trace those changes to the related project elements.
  The UOOR approach is not just a theoretical proposal but has been designed for practical use and has been applied to a significant real-world case study: Roborace, a competition of autonomous racing cars.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18617v2</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Naumcheva, Sophie Ebersold, Jean-Michel Bruel, Bertrand Meyer</dc:creator>
    </item>
  </channel>
</rss>
