<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Sep 2025 01:18:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Assessing the effectiveness of barrier allocation strategies against the propagation of phytopathogens and pests with percolation</title>
      <link>https://arxiv.org/abs/2509.07278</link>
      <description>arXiv:2509.07278v1 Announce Type: new 
Abstract: We investigate the connectivity properties of square lattices with nearest-neighbor interactions, where some sites have a reduced coordination number, meaning that certain sites can only connect through three or two adjacent sites. This model is similar to the random placement of physical barriers in plantations aimed at decreasing connectivity between susceptible individuals, which could help prevent the spread of phytopathogens and pests. In this way, we estimate the percolation threshold as a function of the fraction of sites with a reduced coordination number ($p_d$), finding that the critical curves can be well described by a $q$-exponential function. Additionally, we establish the correlation between $p_d$ and the fraction of barriers effectively placed, which follows a power law behavior. The latter is helpful in estimating the relative costs of the barrier allocation strategies. In particular, we found that the allocations of two barriers per site model $\{ \ulcorner, \lrcorner \}$ can produce savings between 5% and 10% of the strategy cost compared to the independently random barrier allocations (joint site-bond percolation). From an agroecology perspective, adding barriers to the plantation gives farmers the opportunity to sow more vulnerable plant varieties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07278v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>E. G. Garc\'ia Prieto, G. Garc\'ia Morales, J. D. Silva Montiel, D. Rosales Herrera, J. R. Alvarado Garc\'ia, A. Fern\'andez T\'ellez, Y. Mart\'inez Laguna, J. F. L\'opez-Olgu\'in, J. E. Ram\'irez</dc:creator>
    </item>
    <item>
      <title>Forecasting dementia incidence</title>
      <link>https://arxiv.org/abs/2509.07874</link>
      <description>arXiv:2509.07874v1 Announce Type: new 
Abstract: This paper estimates the stochastic process of how dementia incidence evolves over time. We proceed in two steps: first, we estimate a time trend for dementia using a multi-state Cox model. The multi-state model addresses problems of both interval censoring arising from infrequent measurement and also measurement error in dementia. Second, we feed the estimated mean and variance of the time trend into a Kalman filter to infer the population level dementia process. Using data from the English Longitudinal Study of Aging (ELSA), we find that dementia incidence is no longer declining in England. Furthermore, our forecast is that future incidence remains constant, although there is considerable uncertainty in this forecast. Our two-step estimation procedure has significant computational advantages by combining a multi-state model with a time series method. To account for the short sample that is available for dementia, we derive expressions for the Kalman filter's convergence speed, size, and power to detect changes and conclude our estimator performs well even in short samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07874v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>J\'er\^ome R. Simons, Yuntao Chen, Eric Brunner, Eric French</dc:creator>
    </item>
    <item>
      <title>Loss Functions for Detecting Outliers in Panel Data</title>
      <link>https://arxiv.org/abs/2509.07014</link>
      <description>arXiv:2509.07014v1 Announce Type: cross 
Abstract: The detection of outliers is of critical importance in the assurance of data quality. Outliers may exist in observed data or in data derived from these observed data, such as estimates and forecasts. An outlier may indicate a problem with its data generation process or may simply be a true statement about the world. Without making any distributional assumptions, this paper proposes the use of loss functions to detect these outliers in panel data. An unsigned loss function is derived axiomatically. A signed loss function is developed to account for positive and negative outliers separately. In the case of nominal time an exact parametrization of the loss function is obtained. A time-invariant loss function permits the comparison of data at multiple times on the same basis. Several examples are provided, including an example in which the outliers are classified by another variable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07014v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Charles D. Coleman, Thomas Bryan</dc:creator>
    </item>
    <item>
      <title>Fast phase prediction of charged polymer blends by white-box machine learning surrogates</title>
      <link>https://arxiv.org/abs/2509.07164</link>
      <description>arXiv:2509.07164v2 Announce Type: cross 
Abstract: Compatibilized polymer blends are a complex, yet versatile and widespread category of material. When the components of a binary blend are immiscible, they are typically driven towards a macrophase-separated state, but with the introduction of electrostatic interactions, they can be either homogenized or shifted to microphase separation. However, both experimental and simulation approaches face significant challenges in efficiently exploring the vast design space of charge-compatibilized polymer blends, encompassing chemical interactions, architectural properties, and composition. In this work, we introduce a white-box machine learning approach integrated with polymer field theory to predict the phase behavior of these systems, which is significantly more accurate than conventional black-box machine learning approaches. The random phase approximation (RPA) calculation is used as a testbed to determine polymer phases. Instead of directly predicting the polymer phase output of RPA calculations from a large input space by a machine learning model, we build a parallel partial Gaussian process model to predict the most computationally intensive component of the RPA calculation that only involves polymer architecture parameters as inputs. This approach substantially reduces the computational cost of the RPA calculation across a vast input space with nearly 100% accuracy for out-of-sample prediction, enabling rapid screening of polymer blend charge-compatibilization designs. More broadly, the white-box machine learning strategy offers a promising approach for dramatic acceleration of polymer field-theoretic methods for mapping out polymer phase behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07164v2</guid>
      <category>cond-mat.soft</category>
      <category>stat.AP</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Clayton Ellis, Xinyi Fang, Christopher Balzer, Timothy Quah, M. Scott Shell, Glenn H. Fredrickson, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>Monitoring Adverse Events Through Bayesian Nonparametric Clustering Across Studies</title>
      <link>https://arxiv.org/abs/2509.07267</link>
      <description>arXiv:2509.07267v1 Announce Type: cross 
Abstract: We introduce a Bayesian nonparametric inference approach for aggregate adverse event (AE) monitoring across studies. The proposed model seamlessly integrates external data from historical trials to define a relevant background rate and accommodates varying levels of covariate granularity (ranging from patient-level details to study-level aggregated summary data). Inference is based on a covariate-dependent product partition model (PPMx). A central element of the model is the ability to group experimental units with similar profiles. We introduce a pairwise similarity measure, with which we set up a random partition of experimental units with comparable covariate profiles, thereby improving the precision of AE rate estimation. Importantly, the proposed framework supports real-time safety monitoring under blinding with a seamless transition to unblinded analyses when indicated. Using one case study and simulation studies, we demonstrate the model's ability to detect safety signals and assess risk under diverse trial scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07267v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Yuan, Kevin Roberts, Noirrit Kiran Chandra, Yuan Ji, Peter M\"uller</dc:creator>
    </item>
    <item>
      <title>An Extension of the d-Variate FGM Copula with Application</title>
      <link>https://arxiv.org/abs/2509.07281</link>
      <description>arXiv:2509.07281v1 Announce Type: cross 
Abstract: We introduce an extended d-variate Farlie-Gumbel-Morgenstern (FGM) copula that incorporates additional parameters based on Legendre polynomials to enhance the representation of multivariate dependence structures. Within an i.i.d. framework, we derive closed-form estimators for these parameters and establish their unbiasedness, consistency, and asymptotic normality. A simulation study illustrates the finite-sample performance of the estimators. The model is applied to the Bearing dataset, previously studied by Ota and Kimura (2021) through a d-variate FGM copula and by Longla and Mous-Abou (2025) using an extended bivariate FGM copula. Our analysis shows that the classical d-variate FGM copula does not adequately represent the dependence in this dataset. Based on estimation results and model selection criteria, we propose a reduced version of the extended model as a more appropriate copula specification for the Bearing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07281v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mous-Abou Hamadou, Martial Longla</dc:creator>
    </item>
    <item>
      <title>Bias reduction in g-computation for covariate adjustment in randomized clinical trials</title>
      <link>https://arxiv.org/abs/2509.07369</link>
      <description>arXiv:2509.07369v1 Announce Type: cross 
Abstract: G-computation is a powerful method for estimating unconditional treatment effects with covariate adjustment in randomized clinical trials. It typically relies on fitting canonical generalized linear models. However, this could be problematic for small sample sizes or in the presence of rare events. Common issues include underestimation of the variance and the potential nonexistence of maximum likelihood estimators. Bias reduction methods are commonly employed to address these issues, including Firth correction which guarantees the existence of corresponding estimates. Yet, their application within g-computation remains underexplored. In this article, we analyze the asymptotic bias of g-computation estimators and propose a novel bias-reduction method that improves both estimation and inference. Our approach performs a debiasing surgery via generalized Oaxaca-Blinder estimators and thus the resulting estimators are guaranteed to be bounded. The proposed debiased estimators use slightly modified versions of maximum likelihood or Firth correction estimators for nuisance parameters. Inspired by the proposed debiased estimators, we also introduce a simple small-sample bias adjustment for variance estimation, further improving finite-sample inference validity. Through extensive simulations, we demonstrate that our proposed method offers superior finite-sample performance, effectively addressing the bias-efficiency tradeoff. Finally, we illustrate its practical utility by reanalyzing a completed randomized clinical trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07369v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Zhang, Lin Liu, Haitao Chu</dc:creator>
    </item>
    <item>
      <title>Bayesian Pliable Lasso with Horseshoe Prior for Interaction Effects in GLMs with Missing Responses</title>
      <link>https://arxiv.org/abs/2509.07501</link>
      <description>arXiv:2509.07501v1 Announce Type: cross 
Abstract: Sparse regression problems, where the goal is to identify a small set of relevant predictors, often require modeling not only main effects but also meaningful interactions through other variables. While the pliable lasso has emerged as a powerful frequentist tool for modeling such interactions under strong heredity constraints, it lacks a natural framework for uncertainty quantification and incorporation of prior knowledge. In this paper, we propose a Bayesian pliable lasso that extends this approach by placing sparsity-inducing priors, such as the horseshoe, on both main and interaction effects. The hierarchical prior structure enforces heredity constraints while adaptively shrinking irrelevant coefficients and allowing important effects to persist. We extend this framework to Generalized Linear Models (GLMs) and develop a tailored approach to handle missing responses. To facilitate posterior inference, we develop an efficient Gibbs sampling algorithm based on a reparameterization of the horseshoe prior. Our Bayesian framework yields sparse, interpretable interaction structures, and principled measures of uncertainty. Through simulations and real-data studies, we demonstrate its advantages over existing methods in recovering complex interaction patterns under both complete and incomplete data.
  Our method is implemented in the package \texttt{hspliable} available on Github.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07501v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>The Tien Mai</dc:creator>
    </item>
    <item>
      <title>Clustering methods for Categorical Time Series and Sequences : A scoping review</title>
      <link>https://arxiv.org/abs/2509.07885</link>
      <description>arXiv:2509.07885v1 Announce Type: cross 
Abstract: Objective: To provide an overview of clustering methods for categorical time series (CTS), a data structure commonly found in epidemiology, sociology, biology, and marketing, and to support method selection in regards to data characteristics.
  Methods: We searched PubMed, Web of Science, and Google Scholar, from inception up to November 2024 to identify articles that propose and evaluate clustering techniques for CTS. Methods were classified according to three major families -- distance-based, feature-based, and model-based -- and assessed on their ability to handle data challenges such as variable sequence length, multivariate data, continuous time, missing data, time-invariant covariates, and large data volumes.
  Results: Out of 14607 studies, we included 124 articles describing 129 methods, spanning domains such as artificial intelligence, social sciences, and epidemiology. Distance-based methods, particularly those using Optimal Matching, were most prevalent, with 56 methods. We identified 28 model-based methods, which demonstrated superior flexibility for handling complex data structures such as multivariate data, continuous time and time-invariant covariates. We also recorded 45 feature-based approaches, which were on average more scalable but less flexible. A searchable Web application was developed to facilitate method selection based on dataset characteristics ( https://cts-clustering-scoping-review-7sxqj3sameqvmwkvnzfynz.streamlit.app/ )
  Discussion: While distance-based methods dominate, model-based approaches offer the richest modeling potential but are less scalable. Feature-based methods favor performance over flexibility, with limited support for complex data structures.
  Conclusion: This review highlights methodological diversity and gaps in CTS clustering. The proposed typology aims to guide researchers in selecting appropriate methods for their specific use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07885v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ottavio Khalifa, Viet-Thi Tran, Alan Balendran, Fran\c{c}ois Petit</dc:creator>
    </item>
    <item>
      <title>Scalable Estimation of Multinomial Response Models with Random Consideration Sets</title>
      <link>https://arxiv.org/abs/2308.12470</link>
      <description>arXiv:2308.12470v5 Announce Type: replace-cross 
Abstract: A common assumption in the fitting of unordered multinomial response models for $J$ mutually exclusive categories is that the responses arise from the same set of $J$ categories across subjects. However, when responses measure a choice made by the subject, it is more appropriate to condition the distribution of multinomial responses on a subject-specific consideration set, drawn from the power set of $\{1,2,\ldots,J\}$. This leads to a mixture of multinomial response models governed by a probability distribution over the $J^{\ast} = 2^J -1$ consideration sets. We introduce a novel method for estimating such generalized multinomial response models based on the fundamental result that any mass distribution over $J^{\ast}$ consideration sets can be represented as a mixture of products of $J$ component-specific inclusion-exclusion probabilities. Moreover, under time-invariant consideration sets, the conditional posterior distribution of consideration sets is sparse. These features enable a scalable MCMC algorithm for sampling the posterior distribution of parameters, random effects, and consideration sets. Under regularity conditions, the posterior distributions of the marginal response probabilities and the model parameters satisfy consistency. The methodology is demonstrated in a longitudinal data set on weekly cereal purchases that cover $J = 101$ brands, a dimension substantially beyond the reach of existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12470v5</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siddhartha Chib, Kenichi Shimizu</dc:creator>
    </item>
    <item>
      <title>Improving the Estimation of Lifetime Effects in A/B Testing via Treatment Locality</title>
      <link>https://arxiv.org/abs/2407.19618</link>
      <description>arXiv:2407.19618v3 Announce Type: replace-cross 
Abstract: Utilizing randomized experiments to evaluate the effect of short-term treatments on the short-term outcomes has been well understood and become the golden standard in industrial practice. However, as service systems become increasingly dynamical and personalized, much focus is shifting toward maximizing long-term outcomes, such as customer lifetime value, through lifetime exposure to interventions. Our goal is to assess the impact of treatment and control policies on long-term outcomes from relatively short-term observations, such as those generated by A/B testing. A key managerial observation is that many practical treatments are local, affecting only targeted states while leaving other parts of the policy unchanged. This paper rigorously investigates whether and how such locality can be exploited to improve estimation of long-term effects in Markov Decision Processes (MDPs), a fundamental model of dynamic systems. We first develop optimal inference techniques for general A/B testing in MDPs and establish corresponding efficiency bounds. We then propose methods to harness the localized structure by sharing information on the non-targeted states. Our new estimator can achieve a linear reduction with the number of test arms for a major part of the variance without sacrificing unbiasedness. It also matches a tighter variance lower bound that accounts for locality. Furthermore, we extend our framework to a broad class of differentiable estimators, which encompasses many widely used approaches in practice. We show that all such estimators can benefit from variance reduction through information sharing without increasing their bias. Together, these results provide both theoretical foundations and practical tools for conducting efficient experiments in dynamic service systems with local treatments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19618v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuze Chen, David Simchi-Levi, Chonghuan Wang</dc:creator>
    </item>
    <item>
      <title>Pilot Study on Generative AI and Critical Thinking in Higher Education Classrooms</title>
      <link>https://arxiv.org/abs/2509.00167</link>
      <description>arXiv:2509.00167v3 Announce Type: replace-cross 
Abstract: Generative AI (GAI) tools have seen rapid adoption in educational settings, yet their role in fostering critical thinking remains underexplored. While previous studies have examined GAI as a tutor for specific lessons or as a tool for completing assignments, few have addressed how students critically evaluate the accuracy and appropriateness of GAI-generated responses. This pilot study investigates students' ability to apply structured critical thinking when assessing Generative AI outputs in introductory Computational and Data Science courses. Given that GAI tools often produce contextually flawed or factually incorrect answers, we designed learning activities that require students to analyze, critique, and revise AI-generated solutions. Our findings offer initial insights into students' ability to engage critically with GAI content and lay the groundwork for more comprehensive studies in future semesters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00167v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>W. F. Lamberti, S. R. Lawrence, D. White, S. Kim, S. Abdullah</dc:creator>
    </item>
  </channel>
</rss>
