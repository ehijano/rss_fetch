<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.CP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.CP</link>
    <description>q-fin.CP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.CP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 Oct 2025 01:57:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>LEMs: A Primer On Large Execution Models</title>
      <link>https://arxiv.org/abs/2509.25211</link>
      <description>arXiv:2509.25211v1 Announce Type: cross 
Abstract: This paper introduces Large Execution Models (LEMs), a novel deep learning framework that extends transformer-based architectures to address complex execution problems with flexible time boundaries and multiple execution constraints. Building upon recent advances in neural VWAP execution strategies, LEMs generalize the approach from fixed-duration orders to scenarios where execution duration is bounded between minimum and maximum time horizons, similar to share buyback contract structures. The proposed architecture decouples market information processing from execution allocation decisions: a common feature extraction pipeline using Temporal Kolmogorov-Arnold Networks (TKANs), Variable Selection Networks (VSNs), and multi-head attention mechanisms processes market data to create informational context, while independent allocation networks handle the specific execution logic for different scenarios (fixed quantity vs. fixed notional, buy vs. sell orders). This architectural separation enables a unified model to handle diverse execution objectives while leveraging shared market understanding across scenarios. Through comprehensive empirical evaluation on intraday cryptocurrency markets and multi-day equity trading using DOW Jones constituents, we demonstrate that LEMs achieve superior execution performance compared to traditional benchmarks by dynamically optimizing execution paths within flexible time constraints. The unified model architecture enables deployment across different execution scenarios (buy/sell orders, varying duration boundaries, volume/notional targets) through a single framework, providing significant operational advantages over asset-specific approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25211v1</guid>
      <category>cs.LG</category>
      <category>q-fin.CP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Remi Genet, Hugo Inzirillo</dc:creator>
    </item>
    <item>
      <title>AlphaSAGE: Structure-Aware Alpha Mining via GFlowNets for Robust Exploration</title>
      <link>https://arxiv.org/abs/2509.25055</link>
      <description>arXiv:2509.25055v2 Announce Type: replace 
Abstract: The automated mining of predictive signals, or alphas, is a central challenge in quantitative finance. While Reinforcement Learning (RL) has emerged as a promising paradigm for generating formulaic alphas, existing frameworks are fundamentally hampered by a triad of interconnected issues. First, they suffer from reward sparsity, where meaningful feedback is only available upon the completion of a full formula, leading to inefficient and unstable exploration. Second, they rely on semantically inadequate sequential representations of mathematical expressions, failing to capture the structure that determine an alpha's behavior. Third, the standard RL objective of maximizing expected returns inherently drives policies towards a single optimal mode, directly contradicting the practical need for a diverse portfolio of non-correlated alphas. To overcome these challenges, we introduce AlphaSAGE (Structure-Aware Alpha Mining via Generative Flow Networks for Robust Exploration), a novel framework is built upon three cornerstone innovations: (1) a structure-aware encoder based on Relational Graph Convolutional Network (RGCN); (2) a new framework with Generative Flow Networks (GFlowNets); and (3) a dense, multi-faceted reward structure. Empirical results demonstrate that AlphaSAGE outperforms existing baselines in mining a more diverse, novel, and highly predictive portfolio of alphas, thereby proposing a new paradigm for automated alpha mining. Our code is available at https://github.com/BerkinChen/AlphaSAGE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25055v2</guid>
      <category>q-fin.CP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Binqi Chen, Hongjun Ding, Ning Shen, Jinsheng Huang, Taian Guo, Luchen Liu, Ming Zhang</dc:creator>
    </item>
    <item>
      <title>A discretization scheme for path-dependent FBSDEs and PDEs</title>
      <link>https://arxiv.org/abs/2308.07029</link>
      <description>arXiv:2308.07029v2 Announce Type: replace-cross 
Abstract: This study develops a numerical scheme for path-dependent FBSDEs and PDEs. We introduce a Picard iteration method for solving path-dependent FBSDEs, prove its convergence to the true solution, and establish its rate of convergence. A key contribution of our approach is a novel estimator for the martingale integrand in the FBSDE, specifically designed to handle path-dependence more reliably than existing methods. We derive a concentration inequality that quantifies the statistical error of this estimator in a Monte Carlo framework. Based on these results, we investigate a supervised learning method with neural networks for solving path-dependent PDEs. The proposed algorithm is fully implementable and adaptable to a broad class of path-dependent problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.07029v2</guid>
      <category>math.PR</category>
      <category>q-fin.CP</category>
      <category>q-fin.MF</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiuk Jang, Hyungbin Park</dc:creator>
    </item>
  </channel>
</rss>
