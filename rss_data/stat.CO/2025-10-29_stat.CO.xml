<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 Oct 2025 01:40:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Multi-Dimensional Wasserstein Distance Implementation in Scipy</title>
      <link>https://arxiv.org/abs/2510.23651</link>
      <description>arXiv:2510.23651v1 Announce Type: new 
Abstract: The Wasserstein distance, also known as the Earth mover distance or optimal transport distance, is a widely used measure of similarity between probability distributions. This paper presents an linear programming based implementation of the multi-dimensional Wasserstein distance function in Scipy, a powerful scientific computing package in Python. Building upon the existing one-dimensional scipy.stats.wasserstein_distance function, our work extends its capabilities to handle multi-dimensional distributions. To compute the multi-dimensional Wasserstein distance, we developed an implementation that transforms the problem into a linear programming problem. We utilized the scipy linear programming solver to effectively solve this transformed problem. The proposed implementation includes thorough documentation and comprehensive test cases to ensure accuracy and reliability. The resulting feature is set to be merged into the main Scipy development branch and will be included in the upcoming release, further enhancing the capabilities of Scipy in the field of multi-dimensional statistical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23651v1</guid>
      <category>stat.CO</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zehao Lu</dc:creator>
    </item>
    <item>
      <title>Fast Bayesian Multilevel Quasi-Monte Carlo</title>
      <link>https://arxiv.org/abs/2510.24604</link>
      <description>arXiv:2510.24604v1 Announce Type: new 
Abstract: Existing multilevel quasi-Monte Carlo (MLQMC) methods often rely on multiple independent randomizations of a low-discrepancy (LD) sequence to estimate statistical errors on each level. While this approach is standard, it can be less efficient than simply increasing the number of points from a single LD sequence. However, a single LD sequence does not permit statistical error estimates in the current framework. We propose to recast the MLQMC problem in a Bayesian cubature framework, which uses a single LD sequence and quantifies numerical error through the posterior variance of a Gaussian process (GP) model. When paired with certain LD sequences, GP regression and hyperparameter optimization can be carried out at only $\mathcal{O}(n \log n)$ cost, where $n$ is the number of samples. Building on the adaptive sample allocation used in traditional MLQMC, where the number of samples is doubled on the level with the greatest expected benefit, we introduce a new Bayesian utility function that balances the computational cost of doubling against the anticipated reduction in posterior uncertainty. We also propose a new digitally-shift-invariant (DSI) kernel of adaptive smoothness, which combines multiple higher-order DSI kernels through a weighted sum of smoothness parameters, for use with fast digital net GPs. A series of numerical experiments illustrate the performance of our fast Bayesian MLQMC method and error estimates for both single-level problems and multilevel problems with a fixed number of levels. The Bayesian error estimates obtained using digital nets are found to be reliable, although, in some cases, mildly conservative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24604v1</guid>
      <category>stat.CO</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aleksei G. Sorokin, Pieterjan Robbe, Gianluca Geraci, Michael S. Eldred, Fred J. Hickernell</dc:creator>
    </item>
    <item>
      <title>Testing-driven Variable Selection in Bayesian Modal Regression</title>
      <link>https://arxiv.org/abs/2510.23831</link>
      <description>arXiv:2510.23831v1 Announce Type: cross 
Abstract: We propose a Bayesian variable selection method in the framework of modal regression for heavy-tailed responses. An efficient expectation-maximization algorithm is employed to expedite parameter estimation. A test statistic is constructed to exploit the shape of the model error distribution to effectively separate informative covariates from unimportant ones. Through simulations, we demonstrate and evaluate the efficacy of the proposed method in identifying important covariates in the presence of non-Gaussian model errors. Finally, we apply the proposed method to analyze two datasets arising in genetic and epigenetic studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23831v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiasong Duan, Hongmei Zhang, Xianzheng Huang</dc:creator>
    </item>
    <item>
      <title>Self-Normalized Quantile Empirical Saddlepoint Approximation</title>
      <link>https://arxiv.org/abs/2510.24352</link>
      <description>arXiv:2510.24352v1 Announce Type: cross 
Abstract: We propose a density-free method for frequentist inference on population quantiles, termed Self-Normalized Quantile Empirical Saddlepoint Approximation (SNQESA). The approach builds a self-normalized pivot from the indicator score for a fixed quantile threshold and then employs a constrained empirical saddlepoint approximation to obtain highly accurate tail probabilities. Inverting these tail areas yields confidence intervals and tests without estimating the unknown density at the target quantile, thereby eliminating bandwidth selection and the boundary issues that affect kernel-based Wald/Hall-Sheather intervals. Under mild local regularity, the resulting procedures attain higher-order tail accuracy and second-order coverage after inversion. Because the pivot is anchored in a bounded Bernoulli reduction, the method remains reliable for skewed and heavy-tailed distributions and for extreme quantiles. Extensive Monte Carlo experiments across light, heavy, and multimodal distributions demonstrate that SNQESA delivers stable coverage and competitive interval lengths in small to moderate samples while being orders of magnitude faster than large-B resampling schemes. An empirical study on Value-at-Risk with rolling windows further highlights the gains in tail performance and computational efficiency. The framework naturally extends to two-sample quantile differences and to regression-type settings, offering a practical, analytically transparent alternative to kernel, bootstrap, and empirical-likelihood methods for distribution-free quantile inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24352v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hou Jian, Meng Tan, Tian Maozai</dc:creator>
    </item>
    <item>
      <title>Amortized variational transdimensional inference</title>
      <link>https://arxiv.org/abs/2506.04749</link>
      <description>arXiv:2506.04749v2 Announce Type: replace 
Abstract: The expressiveness of flow-based models combined with stochastic variational inference (SVI) has expanded the application of optimization-based Bayesian inference to highly complex problems. However, despite the importance of multi-model Bayesian inference, defined over a transdimensional joint model and parameter space, flow-based SVI has been limited to problems defined over a fixed-dimensional parameter space. We introduce CoSMIC normalizing flows (COntextually-Specified Masking for Identity-mapped Components), an extension to neural autoregressive conditional normalizing flow architectures that enables use of a single amortized variational density for inference over a transdimensional (multi-model) conditional target distribution. We propose a combined stochastic variational transdimensional inference (VTI) approach to training CoSMIC flows using ideas from Bayesian optimization and Monte Carlo gradient estimation. Numerical experiments show the performance of VTI on challenging problems that scale to high-cardinality model spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04749v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Laurence Davies, Dan Mackinlay, Rafael Oliveira, Scott A. Sisson</dc:creator>
    </item>
    <item>
      <title>MECfda: An R Package for Bias Correction Due to Measurement Error in Functional and Scalar Covariates in Scalar-on-Function Regression Models</title>
      <link>https://arxiv.org/abs/2510.21661</link>
      <description>arXiv:2510.21661v2 Announce Type: replace-cross 
Abstract: Functional data analysis (FDA) deals with high-resolution data recorded over a continuum, such as time, space or frequency. Device-based assessments of physical activity or sleep are objective yet still prone to measurement error. We present MECfda, an R package that (i) fits scalar-on-function, generalized scalar-on-function, and functional quantile regression models, and (ii) provides bias-corrected estimation when functional covariates are measured with error. By unifying these tools under a consistent syntax, MECfda enables robust inference for FDA applications that involve noisy functional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21661v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heyang Ji, Ufuk Beyaztas, Nicolas Escobar-Velasquez, Yuanyuan Luan, Xiwei Chen, Mengli Zhang, Roger Zoh, Lan Xue, Carmen Tekwe</dc:creator>
    </item>
  </channel>
</rss>
