<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 04 Jul 2025 01:21:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Entropic optimal transport beyond product reference couplings: the Gaussian case on Euclidean space</title>
      <link>https://arxiv.org/abs/2507.01709</link>
      <description>arXiv:2507.01709v1 Announce Type: new 
Abstract: The optimal transport problem with squared Euclidean cost consists in finding a coupling between two input measures that maximizes correlation. Consequently, the optimal coupling is often singular with respect to Lebesgue measure. Regularizing the optimal transport problem with an entropy term yields an approximation called entropic optimal transport. Entropic penalties steer the induced coupling toward a reference measure with desired properties. For instance, when seeking a diffuse coupling, the most popular reference measures are the Lebesgue measure and the product of the two input measures. In this work, we study the case where the reference coupling is not necessarily assumed to be a product. We focus on the Gaussian case as a motivating paradigm, and provide a reduction of this more general optimal transport criterion to a matrix optimization problem. This reduction enables us to provide a complete description of the solution, both in terms of the primal variable and the dual variables. We argue that flexibility in terms of the reference measure can be important in statistical contexts, for instance when one has prior information, when there is uncertainty regarding the measures to be coupled, or to reduce bias when the entropic problem is used to estimate the un-regularized transport problem. In particular, we show in numerical examples that choosing a suitable reference plan allows to reduce the bias caused by the entropic penalty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01709v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Freulon, Nikitas Georgakis, Victor Panaretos</dc:creator>
    </item>
    <item>
      <title>High dimensional convergence rates for sparse precision estimators for matrix-variate data</title>
      <link>https://arxiv.org/abs/2507.01741</link>
      <description>arXiv:2507.01741v1 Announce Type: new 
Abstract: In several applications, the underlying structure of the data allows for the samples to be organized into a matrix variate form. In such settings, the underlying row and column covariance matrices are fundamental quantities of interest. We focus our attention on two popular estimators that have been proposed in the literature: a penalized sparse estimator called SMGM and a heuristic sample covariance estimator. We establish convergence rates for these estimators in relevant high-dimensional settings, where the row and column dimensions of the matrix are allowed to increase with the sample size. We show that high-dimensional convergence rate analyses for the SMGM estimator in previous literature are incorrect. We discuss the critical errors in these proofs, and present a different and novel approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01741v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongqiang Sun, Kshitij Khare</dc:creator>
    </item>
    <item>
      <title>Uniform Validity of the Subset Anderson-Rubin Test under Heteroskedasticity and Nonlinearity</title>
      <link>https://arxiv.org/abs/2507.01167</link>
      <description>arXiv:2507.01167v1 Announce Type: cross 
Abstract: We consider the Anderson-Rubin (AR) statistic for a general set of nonlinear moment restrictions. The statistic is based on the criterion function of the continuous updating estimator (CUE) for a subset of parameters not constrained under the Null. We treat the data distribution nonparametrically with parametric moment restrictions imposed under the Null. We show that subset tests and confidence intervals based on the AR statistic are uniformly valid over a wide range of distributions that include moment restrictions with general forms of heteroskedasticity. We show that the AR based tests have correct asymptotic size when parameters are unidentified, partially identified, weakly or strongly identified. We obtain these results by constructing an upper bound that is using a novel perturbation and regularization approach applied to the first order conditions of the CUE. Our theory applies to both cross-sections and time series data and does not assume stationarity in time series settings or homogeneity in cross-sectional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01167v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Atsushi Inoue, \`Oscar Jord\`a, Guido M. Kuersteiner</dc:creator>
    </item>
    <item>
      <title>A robust Likelihood Ratio Test for high-dimensional MANOVA -- with excellent performance</title>
      <link>https://arxiv.org/abs/2507.01261</link>
      <description>arXiv:2507.01261v1 Announce Type: cross 
Abstract: The present paper answers the following questions related with high-dimensional manova: (i) is it possible to develop a likelihood ratio test for high-dimensional manova? (ii) would such test perform well? (iii) would it be able to outperform existing tests? (iv) would it be applicable to extremely small samples? (v) would it be applicable to non-normal random variables, as uniform, extremely skewed distributions, or even heavy tailed distributions with success? (vi) would it have a nice, rather simple to compute and well performing, asymptotic distribution? And what about if the answer to all the above questions would be a clear 'Yes'? Surprisingly enough, it is exactly the case. Extensive simulations, with both normal and non-normal distributions, some of which are heavy tailed and/or highly skewed, and even discrete distributions, are carried out in order to evaluate the performance of the proposed test and to compare its performance with other tests. Two real data applications are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01261v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Carlos A. Coelho</dc:creator>
    </item>
    <item>
      <title>Regularity Conditions for Critical Point Convergence</title>
      <link>https://arxiv.org/abs/2507.01854</link>
      <description>arXiv:2507.01854v1 Announce Type: cross 
Abstract: We focus on a sequence of functions $\{f_n\}$, defined on a compact manifold with boundary $S$, converging in the $C^k$ metric to a limit $f$. A common assumption implicitly made in the empirical sciences is that when such functions represent random processes derived from data, the topological features of $f_n$ will eventually resemble those of $f$. In this work, we investigate the validity of this claim under various regularity assumptions, with the goal of finding conditions sufficient for the number of local maxima, minima and saddle of such functions to converge. In the $C^1$ setting, we do so by employing lesser-known variants of the Poincar\'{e}-Hopf and mountain pass theorems, and in the $C^2$ setting we pursue an approach inspired by the homotopy-based proof of the Morse Lemma. To aid practical use, we end by reformulating our central theorems in the language of the empirical processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01854v1</guid>
      <category>math.GN</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas J. Maullin-Sapey, Samuel Davenport</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Bernstein Von-Mises Theorems for Covariance and Precision Matrices</title>
      <link>https://arxiv.org/abs/2309.08556</link>
      <description>arXiv:2309.08556v2 Announce Type: replace 
Abstract: This paper aims to examine the characteristics of the posterior distribution of covariance/precision matrices in a "large $p$, large $n$" scenario, where $p$ represents the number of variables and $n$ is the sample size. Our analysis focuses on establishing asymptotic normality of the posterior distribution of the entire covariance/precision matrices under specific growth restrictions on $p_n$ and other mild assumptions. In particular, the limiting distribution turns out to be a symmetric matrix variate normal distribution whose parameters depend on the maximum likelihood estimate. Our results hold for a wide class of prior distributions which includes standard choices used by practitioners. Next, we consider Gaussian graphical models which induce sparsity in the precision matrix. Asymptotic normality of the corresponding posterior distribution is established under mild assumptions on the prior and true data-generating mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08556v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Partha Sarkar, Kshitij Khare, Malay Ghosh, Matt P. Wand</dc:creator>
    </item>
    <item>
      <title>A Trek Rule for the Lyapunov Equation</title>
      <link>https://arxiv.org/abs/2407.21223</link>
      <description>arXiv:2407.21223v4 Announce Type: replace 
Abstract: The Lyapunov equation is a linear matrix equation characterizing the cross-sectional steady-state covariance matrix of a Gaussian Markov process. We show a new version of the trek rule for this equation, which links the graphical structure of the drift of the process to the entries of the steady-state covariance matrix. In general, the trek rule is a power series expansion of the covariance matrix in the entries of the drift and volatility matrices. For acyclic models it simplifies to a polynomial in the off-diagonal entries of the drift matrix. Using the trek rule we can give relatively explicit formulas for the entries of the covariance matrix for some special cases of the drift matrix. These results illustrate notable differences between covariance models entailed by the Lyapunov equation and those entailed by linear additive noise models. To further explore differences and similarities between these two model classes, we use the trek rule to derive a new lower bound on the marginal variances in the acyclic case. This sheds light on the phenomenon, well known for the linear additive noise model, that the variances in the acyclic case tend to increase along a topological ordering of the variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21223v4</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niels Richard Hansen</dc:creator>
    </item>
    <item>
      <title>Adaptive exact recovery in sparse nonparametric models</title>
      <link>https://arxiv.org/abs/2411.04320</link>
      <description>arXiv:2411.04320v2 Announce Type: replace 
Abstract: We observe an unknown regression function of $d$ variables $f(\boldsymbol{t})$, $\boldsymbol{t} \in[0,1]^d$, in the Gaussian white noise model of intensity $\varepsilon&gt;0$. We assume that the function $f$ is regular and that it is a sum of $k$-variate functions, where $k$ varies from $1$ to $s$ ($1\leq s\leq d$). These functions are unknown to us and only few of them are nonzero. In this article, we address the problem of identifying the nonzero components of $f$ in the case when $d=d_\varepsilon\to \infty$ as $\varepsilon\to 0$ and $s$ is either fixed or $s=s_\varepsilon\to \infty$, $s=o(d)$ as $\varepsilon\to \infty$. This may be viewed as a variable selection problem. We derive the conditions when exact variable selection in the model at hand is possible and provide a selection procedure that achieves this type of selection. The procedure is adaptive to a degree of model sparsity described by the sparsity parameter $\beta\in(0,1)$. We also derive conditions that make the exact variable selection impossible. Our results augment previous work in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04320v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Natalia Stepanova, Marie Turcicova</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes Estimation for Lasso-Type Regularizers: Analysis of Automatic Relevance Determination</title>
      <link>https://arxiv.org/abs/2501.11280</link>
      <description>arXiv:2501.11280v4 Announce Type: replace 
Abstract: This paper focuses on linear regression models with non-conjugate sparsity-inducing regularizers such as lasso and group lasso. Although the empirical Bayes approach enables us to estimate the regularization parameter, little is known on the properties of the estimators. In particular, many aspects regarding the specific conditions under which the mechanism of automatic relevance determination (ARD) occurs remain unexplained. In this paper, we derive the empirical Bayes estimators for the group lasso regularized linear regression models with limited parameters. It is shown that the estimators diverge under a specific condition, giving rise to the ARD mechanism. We also prove that empirical Bayes methods can produce the ARD mechanism in general regularized linear regression models and clarify the conditions under which models such as ridge, lasso, and group lasso can do so.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11280v4</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tsukasa Yoshida, Kazuho Watanabe</dc:creator>
    </item>
    <item>
      <title>Algebraic Constraints for Linear Acyclic Causal Models</title>
      <link>https://arxiv.org/abs/2505.00215</link>
      <description>arXiv:2505.00215v2 Announce Type: replace 
Abstract: In this paper we study the space of second- and third-order moment tensors of random vectors which satisfy a Linear Non-Gaussian Acyclic Model (LiNGAM). In such a causal model each entry $X_i$ of the random vector $X$ corresponds to a vertex $i$ of a directed acyclic graph $G$ and can be expressed as a linear combination of its direct causes $\{X_j: j\to i\}$ and random noise. For any directed acyclic graph $G$, we show that a random vector $X$ arises from a LiNGAM with graph $G$ if and only if certain easy-to-construct matrices, whose entries are second- and third-order moments of $X$, drop rank. This determinantal characterization extends previous results proven for polytrees and generalizes the well-known local Markov property for Gaussian models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00215v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cole Gigliotti, Elina Robeva</dc:creator>
    </item>
    <item>
      <title>Simple, Efficient Entropy Estimation using Harmonic Numbers</title>
      <link>https://arxiv.org/abs/2505.20153</link>
      <description>arXiv:2505.20153v3 Announce Type: replace 
Abstract: The estimation of entropy, a fundamental measure of uncertainty, is central to diverse data applications. For discrete random variables, however, efficient entropy estimation presents challenges, particularly when the cardinality of the support set is large relative to the available sample size. This is because, without other assumptions, there may be insufficient data to adequately characterize a probability mass function. Further complications stem from the dependence among transformations of empirical frequencies within the sample. This paper demonstrates that a simple entropy estimator based on the harmonic number function achieves asymptotic efficiency on discrete random variables with tail probabilities satisfying $p_j =o(j^{-2})$ as $j\rightarrow\infty$. This result renders statistical inference newly feasible for all but very heavy-tailed probability mass functions. Moreover, its strong mean squared error bounds coupled with simple implementation make this estimator an attractive replacement over others in application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20153v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Octavio C\'esar Mesner</dc:creator>
    </item>
    <item>
      <title>Static and Dynamic BART for Rank-Order Data</title>
      <link>https://arxiv.org/abs/2308.10231</link>
      <description>arXiv:2308.10231v3 Announce Type: replace-cross 
Abstract: Ranking lists are often provided at regular time intervals in a range of applications, including economics, sports, marketing, and politics. Most popular methods for rank-order data postulate a linear specification for the latent scores, which determine the observed ranks, and ignore the temporal dependence of the ranking lists. To address these issues, novel nonparametric static (ROBART) and autoregressive (ARROBART) models are developed, with latent scores defined as nonlinear Bayesian additive regression tree functions of covariates. To make inferences in the dynamic ARROBART model, closed-form filtering, predictive, and smoothing distributions for the latent time-varying scores are derived. These results are applied in a Gibbs sampler with data augmentation for posterior inference. The proposed methods are shown to outperform existing competitors in simulation studies, static data applications to electoral data, stated preferences for sushi and movies, and dynamic data applications to economic complexity rankings of countries and weekly pollster rankings of NCAA football teams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.10231v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Iacopini, Eoghan O'Neill, Luca Rossini</dc:creator>
    </item>
    <item>
      <title>Skew-symmetric approximations of posterior distributions</title>
      <link>https://arxiv.org/abs/2409.14167</link>
      <description>arXiv:2409.14167v2 Announce Type: replace-cross 
Abstract: Routinely-implemented deterministic approximations of posterior distributions from, e.g., Laplace method, variational Bayes and expectation-propagation, generally rely on symmetric approximating densities, often taken to be Gaussian. This choice facilitates optimization and inference, but typically affects the quality of the overall approximation. In fact, even in basic parametric models, the posterior distribution often displays asymmetries that yield bias and a reduced accuracy when considering symmetric approximations. Recent research has moved towards more flexible approximating densities that incorporate skewness. However, current solutions are often model-specific, lack general supporting theory, increase the computational complexity of the optimization problem, and do not provide a broadly-applicable solution to include skewness in any symmetric approximation. This article addresses such a gap by introducing a general and provably-optimal strategy to perturb any off-the-shelf symmetric approximation of a generic posterior distribution. Crucially, this novel perturbation is derived without additional optimization steps, and yields a similarly-tractable approximation within the class of skew-symmetric densities that provably enhances the finite-sample accuracy of the original symmetric counterpart. Furthermore, under suitable assumptions, it improves the convergence rate to the exact posterior by at least a $\sqrt{n}$ factor, in asymptotic regimes. These advancements are illustrated in numerical studies focusing on skewed perturbations of state-of-the-art Gaussian approximations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14167v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Pozza, Daniele Durante, Botond Szabo</dc:creator>
    </item>
    <item>
      <title>Long-Context Linear System Identification</title>
      <link>https://arxiv.org/abs/2410.05690</link>
      <description>arXiv:2410.05690v2 Announce Type: replace-cross 
Abstract: This paper addresses the problem of long-context linear system identification, where the state $x_t$ of a dynamical system at time $t$ depends linearly on previous states $x_s$ over a fixed context window of length $p$. We establish a sample complexity bound that matches the i.i.d. parametric rate up to logarithmic factors for a broad class of systems, extending previous works that considered only first-order dependencies. Our findings reveal a learning-without-mixing phenomenon, indicating that learning long-context linear autoregressive models is not hindered by slow mixing properties potentially associated with extended context windows. Additionally, we extend these results to (i) shared low-rank representations, where rank-regularized estimators improve the dependence of the rates on the dimensionality, and (ii) misspecified context lengths in strictly stable systems, where shorter contexts offer statistical advantages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05690v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>O. K. Y\"uksel, M. Even, N. Flammarion, "Long-Context Linear System Identification", in The Thirteenth International Conference on Learning Representations, 2025</arxiv:journal_reference>
      <dc:creator>O\u{g}uz Kaan Y\"uksel, Mathieu Even, Nicolas Flammarion</dc:creator>
    </item>
    <item>
      <title>Wireless Network Topology Inference: A Markov Chains Approach</title>
      <link>https://arxiv.org/abs/2501.17532</link>
      <description>arXiv:2501.17532v2 Announce Type: replace-cross 
Abstract: We address the problem of inferring the topology of a wireless network using limited observational data. Specifically, we assume that we can detect when a node is transmitting, but no further information regarding the transmission is available. We propose a novel network estimation procedure grounded in the following abstract problem: estimating the parameters of a finite discrete-time Markov chain by observing, at each time step, which states are visited by multiple ``anonymous'' copies of the chain. We develop a consistent estimator that approximates the transition matrix of the chain in the operator norm, with the number of required samples scaling roughly linearly with the size of the state space. Applying this estimation procedure to wireless networks, our numerical experiments demonstrate that the proposed method accurately infers network topology across a wide range of parameters, consistently outperforming transfer entropy, particularly under conditions of high network congestion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17532v2</guid>
      <category>cs.NI</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Martin, Tristan Pryer, Luca Zanetti</dc:creator>
    </item>
    <item>
      <title>Query Complexity of Classical and Quantum Channel Discrimination</title>
      <link>https://arxiv.org/abs/2504.12989</link>
      <description>arXiv:2504.12989v2 Announce Type: replace-cross 
Abstract: Quantum channel discrimination has been studied from an information-theoretic perspective, wherein one is interested in the optimal decay rate of error probabilities as a function of the number of unknown channel accesses. In this paper, we study the query complexity of quantum channel discrimination, wherein the goal is to determine the minimum number of channel uses needed to reach a desired error probability. To this end, we show that the query complexity of binary channel discrimination depends logarithmically on the inverse error probability and inversely on the negative logarithm of the (geometric and Holevo) channel fidelity. As a special case of these findings, we precisely characterize the query complexity of discriminating two classical channels and two classical-quantum channels. Furthermore, by obtaining a tighter characterization of the sample complexity of quantum hypothesis testing, including prior probabilities, we provide a more precise characterization of query complexity when the error probability does not exceed a fixed threshold. We also provide lower and upper bounds on the query complexity of binary asymmetric channel discrimination and multiple quantum channel discrimination. For the former, the query complexity depends on the geometric R\'enyi and Petz R\'enyi channel divergences, while for the latter, it depends on the negative logarithm of the (geometric and Uhlmann) channel fidelity. For multiple channel discrimination, the upper bound scales as the logarithm of the number of channels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.12989v2</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Theshani Nuradha, Mark M. Wilde</dc:creator>
    </item>
    <item>
      <title>Generalization performance of narrow one-hidden layer networks in the teacher-student setting</title>
      <link>https://arxiv.org/abs/2507.00629</link>
      <description>arXiv:2507.00629v2 Announce Type: replace-cross 
Abstract: Understanding the generalization abilities of neural networks for simple input-output distributions is crucial to account for their learning performance on real datasets. The classical teacher-student setting, where a network is trained from data obtained thanks to a label-generating teacher model, serves as a perfect theoretical test bed. In this context, a complete theoretical account of the performance of fully connected one-hidden layer networks in the presence of generic activation functions is lacking. In this work, we develop such a general theory for narrow networks, i.e. networks with a large number of hidden units, yet much smaller than the input dimension. Using methods from statistical physics, we provide closed-form expressions for the typical performance of both finite temperature (Bayesian) and empirical risk minimization estimators, in terms of a small number of weight statistics. In doing so, we highlight the presence of a transition where hidden neurons specialize when the number of samples is sufficiently large and proportional to the number of parameters of the network. Our theory accurately predicts the generalization error of neural networks trained on regression or classification tasks with either noisy full-batch gradient descent (Langevin dynamics) or full-batch gradient descent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00629v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean Barbier, Federica Gerace, Alessandro Ingrosso, Clarissa Lauditi, Enrico M. Malatesta, Gibbs Nwemadji, Rodrigo P\'erez Ortiz</dc:creator>
    </item>
  </channel>
</rss>
