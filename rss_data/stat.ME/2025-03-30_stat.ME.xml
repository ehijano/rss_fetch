<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 31 Mar 2025 04:00:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A novel smoothing-based goodness-of-fit test of covariance for multivariate sparse functional data</title>
      <link>https://arxiv.org/abs/2503.21913</link>
      <description>arXiv:2503.21913v1 Announce Type: new 
Abstract: Accurately specifying covariance structures is critical for valid inference in longitudinal and functional data analysis, particularly when data are sparsely observed. In this study, we develop a global goodness-of-fit test to assess parametric covariance structures in multivariate sparse functional data. Our contribution is twofold. First, we extend the univariate goodness-of-fit test proposed by Chen et al. (2019) to better accommodate sparse data by improving error variance estimation and applying positive semi-definite smoothing to covariance estimation. These corrections ensure appropriate Type I error control under sparse designs. Second, we introduce a multivariate extension of the improved test that jointly evaluates covariance structures across multiple outcomes, employing novel test statistics based on the maximum and $\ell_2$ norms to account for inter-outcome dependencies and enhance statistical power. Through extensive simulation studies, we demonstrate that the proposed methods maintain proper Type I error rates and achieve greater power than univariate tests with multiple testing adjustments. Applications to longitudinal neuroimaging and clinical data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) and the Parkinson's Progression Marker Initiative (PPMI) illustrate the practical utility of the proposed methods for evaluating covariance structures in sparse multivariate longitudinal data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21913v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dhrubajyoti Ghosh, Zhuolin Song, Luo Xiao, Sheng Luo</dc:creator>
    </item>
    <item>
      <title>A Semiparametric Quantile Single-Index Model for Zero-Inflated Outcomes</title>
      <link>https://arxiv.org/abs/2503.21924</link>
      <description>arXiv:2503.21924v1 Announce Type: new 
Abstract: We consider the complex data modeling problem motivated by the zero-inflated and overdispersed data from microbiome studies. Analyzing how microbiome abundance is associated with human biological features, such as BMI, is of great importance for host health. Methods based on parametric distributional assumptions, such as zero-inflated Poisson and zero-inflated Negative Binomial regression, have been widely used in modeling such data, yet the parametric assumptions are restricted and hard to verify in real-world applications. We relax the parametric assumptions and propose a semiparametric single-index quantile regression model. It is flexible to include a wide range of possible association functions and adaptable to the various zero proportions across subjects, which relaxes the strong parametric distributional assumptions of most existing zero-inflated data modeling approaches. We establish the asymptotic properties for the index coefficients estimator and quantile regression curve estimation. Through extensive simulation studies, we demonstrate the superior performance of the proposed method regarding model fitting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21924v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.5705/ss.202024.0104</arxiv:DOI>
      <dc:creator>Zirui Wang, Tianying Wang</dc:creator>
    </item>
    <item>
      <title>GLM Inference with AI-Generated Synthetic Data Using Misspecified Linear Regression</title>
      <link>https://arxiv.org/abs/2503.21968</link>
      <description>arXiv:2503.21968v1 Announce Type: new 
Abstract: Privacy concerns in data analysis have led to the growing interest in synthetic data, which strives to preserve the statistical properties of the original dataset while ensuring privacy by excluding real records. Recent advances in deep neural networks and generative artificial intelligence have facilitated the generation of synthetic data. However, although prediction with synthetic data has been the focus of recent research, statistical inference with synthetic data remains underdeveloped. In particular, in many settings, including generalized linear models (GLMs), the estimator obtained using synthetic data converges much more slowly than in standard settings. To address these limitations, we propose a method that leverages summary statistics from the original data. Using a misspecified linear regression estimator, we then develop inference that greatly improves the convergence rate and restores the standard root-$n$ behavior for GLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21968v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nir Keret, Ali Shojaie</dc:creator>
    </item>
    <item>
      <title>Discussion of "Robust Distance Covariance" by S. Leyder, J. Raymaekers, and P.J. Rousseeuw</title>
      <link>https://arxiv.org/abs/2503.22021</link>
      <description>arXiv:2503.22021v1 Announce Type: new 
Abstract: Distance covariance and distance correlation have long been regarded as natural measures of dependence between two random vectors, and have been used in a variety of situations for testing independence. Despite their popularity, the robustness of their empirical versions remain highly undiscovered. The paper named "Robust Distance Covariance" by S. Leyder, J. Raymaekers, and P.J. Rousseeuw (below referred to as [LRR]), which this article is discussing about, has provided a welcome addition to the literature. Among some intriguing results in [LRR], we find ourselves particularly interested in the so-called "robustness by transformation" that was highlighted when they used a clever trick named "the biloop transformation" to obtain a bounded and redescending influence function. Building on the measure-transportation-based notions of directional ranks and signs, we show how the "robustness via transformation" principle emphasized by [LRR] extends beyond the case of bivariate independence that [LRR] has investigated and also applies in higher-dimension Euclidean spaces and on compact manifolds. The case of directional variables (taking values on (hyper)spheres) is given special attention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22021v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hallin Marc, Davide La Vecchia, Hang Liu, Xinyi Xu</dc:creator>
    </item>
    <item>
      <title>Parapolitics and Roll-Call Voting in Colombia: A Bayesian Euclidean and Spherical Spatial Analysis</title>
      <link>https://arxiv.org/abs/2503.22045</link>
      <description>arXiv:2503.22045v1 Announce Type: new 
Abstract: This study presents a Bayesian spatial voting analysis of the Colombian Senate during the 2006-2010 legislative period, leveraging a newly constructed roll-call dataset comprising 147 senators and 136 plenary votes. We estimate legislators' ideal points under two alternative geometric frameworks: A traditional Euclidean model and a circular model that embeds preferences on the unit circle. Both models are implemented using Markov Chain Monte Carlo methods, with the circular specification capturing geodesic distances and von Mises-distributed latent traits. The results reveal a latent structure in voting behavior best characterized not by a conventional left-right ideological continuum but by an opposition-non-opposition alignment. Using Bayesian logistic regression, we further investigate the association between senators' ideal points and their involvement in the para-politics scandal. Findings indicate a significant and robust relationship between political alignment and para-politics implication, suggesting that extralegal influence was systematically related to senators' legislative behavior during this period.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22045v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Sosa, Carolina Luque, Juan Valero</dc:creator>
    </item>
    <item>
      <title>An Improved Satterthwaite Effective Degrees of Freedom Correction for Weighted Syntheses of Variance</title>
      <link>https://arxiv.org/abs/2503.22080</link>
      <description>arXiv:2503.22080v1 Announce Type: new 
Abstract: This article presents an improved approximation for the effective degrees of freedom in the Satterthwaite (1941, 1946) method which estimates the distribution of a weighted combination of variance components The standard Satterthwaite approximation assumes a scaled chisquare distribution for the composite variance estimator but is known to be biased downward when component degrees of freedom are small. Building on recent work by von Davier (2025) we propose an adjusted estimator that corrects this bias by modifying both the numerator and denominator of the traditional formula. The new approximation incorporates a weighted average of component degrees of freedom and a scaling factor that ensures consistency as the number of components or their degrees of freedom increases. We demonstrate the utility of this adjustment in practical settings including Rubins (1987) total variance estimation in multiple imputations where weighted variance combinations are common. The proposed estimator generalizes von Daviers (2025) unweighted case and more accurately approximates synthetic variance estimators with arbitrary weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22080v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matthias von Davier</dc:creator>
    </item>
    <item>
      <title>Tripartite models for estimating the value of drug candidates and decision tools</title>
      <link>https://arxiv.org/abs/2503.22117</link>
      <description>arXiv:2503.22117v1 Announce Type: new 
Abstract: Consider two similar drug companies with access to similar chemical libraries and synthesis methods, who each run an R&amp;D program. The programs have the same number of stages, which each take the same amount of time, with the same costs, with the same historic stepwise progression rates, and which aim to address the same therapeutic indication. Now let us suppose one of these companies invests in new scientific tools that make it unusually good at critical progression decisions, while the other company does not. How do we assess the difference in value between the two programs? Surprisingly, standard discounted cash flow valuation methods, such as risk-adjusted net present value (rNPV), ubiquitous in drug industry portfolio management and venture capital, are largely useless in this case. They fail to value the decisions that make drug candidates more or less valuable because rNPV conflates wrong decisions to progress bad candidates with right decisions to progress good ones. The purpose of this paper is to set out a new class of valuation model that logically links the value of therapeutic assets with the value of "decisions tools" that are used to design, optimize, and test those assets. Our model makes clear the interaction between asset value and decision tool value. It also makes clear the downstream consequences of better, or worse, upstream decisions. This new approach may support more effective allocation of R&amp;D capital; helping fund therapeutic assets that are developed using good decision tools, and funding better decision tools to distinguish between good and bad therapeutic assets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22117v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>John Mellnik, Jack Scannell</dc:creator>
    </item>
    <item>
      <title>Powering RCTs for marginal effects with GLMs using prognostic score adjustment</title>
      <link>https://arxiv.org/abs/2503.22284</link>
      <description>arXiv:2503.22284v1 Announce Type: new 
Abstract: In randomized clinical trials (RCTs), the accurate estimation of marginal treatment effects is crucial for determining the efficacy of interventions. Enhancing the statistical power of these analyses is a key objective for statisticians. The increasing availability of historical data from registries, prior trials, and health records presents an opportunity to improve trial efficiency. However, many methods for historical borrowing compromise strict type-I error rate control. Building on the work by Schuler et al. [2022] on prognostic score adjustment for linear models, this paper extends the methodology to the plug-in analysis proposed by Rosenblum et al. [2010] using generalized linear models (GLMs) to further enhance the efficiency of RCT analyses without introducing bias. Specifically, we train a prognostic model on historical control data and incorporate the resulting prognostic scores as covariates in the plug-in GLM analysis of the trial data. This approach leverages the predictive power of historical data to improve the precision of marginal treatment effect estimates. We demonstrate that this method achieves local semi-parametric efficiency under the assumption of an additive treatment effect on the link scale. We expand the GLM plug-in method to include negative binomial regression. Additionally, we provide a straightforward formula for conservatively estimating the asymptotic variance, facilitating power calculations that reflect these efficiency gains. Our simulation study supports the theory. Even without an additive treatment effect, we observe increased power or reduced standard error. While population shifts from historical to trial data may dilute benefits, they do not introduce bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22284v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emilie H{\o}jbjerre-Frandsen, Mark J. van der Laan, Alejandro Schuler</dc:creator>
    </item>
    <item>
      <title>On Bessel's Correction -- Unbiased Sample Variance, the 'Bariance' and Runtime-Optimized Unbiased Sample Variance Estimators</title>
      <link>https://arxiv.org/abs/2503.22333</link>
      <description>arXiv:2503.22333v1 Announce Type: new 
Abstract: Bessel's correction adjusts the denominator in the sample variance formula from n to n - 1 to produce an unbiased estimator for the population variance. This paper includes rigorous derivations, geometric interpretations, and visualizations. It then introduces the concept of 'bariance', an alternative pairwise distances intuition of sample dispersion without an arithmetic mean. Finally, we address practical concerns raised in Rosenthal's article advocating the use of n-based estimates from a more holistic MSE-based viewpoint for pedagogical reasons and in certain practical contexts. Finally, the empirical part using simulation reveals that the run-time of estimating population variance can be significantly shortened when using an algebraically optimized bariance approach using scalar sums to estimate an unbiased variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22333v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felix Reichel</dc:creator>
    </item>
    <item>
      <title>An integrated method for clustering and association network inference</title>
      <link>https://arxiv.org/abs/2503.22467</link>
      <description>arXiv:2503.22467v1 Announce Type: new 
Abstract: We consider high dimensional Gaussian graphical models inference. These models provide a rigorous framework to describe a network of statistical dependencies between entities, such as genes in genomic regulation studies or species in ecology. Penalized methods, including the standard Graphical-Lasso, are well-known approaches to infer the parameters of these models. As the number of variables in the model (of entities in the network) grow, the network inference and interpretation become more complex. We propose Normal-Block, a new model that clusters variables and consider a network at the cluster level. Normal-Block both adds structure to the network and reduces its size. We build on Graphical-Lasso to add a penalty on the network's edges and limit the detection of spurious dependencies, we also propose a zero-inflated version of the model to account for real-world data properties. For the inference procedure, we propose a direct heuristic method and another more rigorous one that simultaneously infers the clustering of variables and the association network between clusters, using a penalized variational Expectation-Maximization approach. An implementation of the model in R, in a package called normalblockr, is available on github (https://github.com/jeannetous/normalblockr). We present the results in terms of clustering and network inference using both simulated data and various types of real-world data (proteomics, words occurrences on webpages, and microbiota distribution).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22467v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeanne Tous, Julien Chiquet</dc:creator>
    </item>
    <item>
      <title>Optimal treatment regimes for the net benefit of a treatment</title>
      <link>https://arxiv.org/abs/2503.22580</link>
      <description>arXiv:2503.22580v1 Announce Type: new 
Abstract: We developed a mathematical setup inspired by Buyse's generalized pairwise comparisons to define a notion of optimal individualized treatment rule (ITR) in the presence of prioritized outcomes in a randomized controlled trial, terming such an ITR pairwise optimal. We present two approaches to estimate pairwise optimal ITRs. The first is a variant of the k-nearest neighbors algorithm. The second is a meta-learner based on a randomized bagging scheme, allowing the use of any classification algorithm for constructing an ITR. We study the behavior of these estimation schemes from a theoretical standpoint and through Monte Carlo simulations and illustrate their use on trial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22580v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Petit, G\'erard Biau, Rapha\"el Porcher</dc:creator>
    </item>
    <item>
      <title>A Unified Approach for Estimating Various Treatment Effects in Causal Inference</title>
      <link>https://arxiv.org/abs/2503.22616</link>
      <description>arXiv:2503.22616v1 Announce Type: new 
Abstract: In this paper, we introduce a unified estimator to analyze various treatment effects in causal inference, including but not limited to the average treatment effect (ATE) and the quantile treatment effect (QTE). The proposed estimator is developed under the statistical functional and cumulative distribution function structure, which leads to a flexible and robust estimator and covers some frequent treatment effects. In addition, our approach also takes variable selection into account, so that informative and network structure in confounders can be identified and be implemented in our estimation procedure. The theoretical properties, including variable selection consistency and asymptotic normality of the statistical functional estimator, are established. Various treatment effects estimations are also conducted in numerical studies, and the results reveal that the proposed estimator generally outperforms the existing methods and is more efficient than its competitors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22616v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kuan-Hsun Wu, Li-Pang Chen</dc:creator>
    </item>
    <item>
      <title>Binary AddiVortes: (Bayesian) Additive Voronoi Tessellations for Binary Classification with an application to Predicting Home Mortgage Application Outcomes</title>
      <link>https://arxiv.org/abs/2503.21792</link>
      <description>arXiv:2503.21792v1 Announce Type: cross 
Abstract: The Additive Voronoi Tessellations (AddiVortes) model is a multivariate regression model that uses multiple Voronoi tessellations to partition the covariate space for an additive ensemble model. In this paper, the AddiVortes framework is extended to binary classification by incorporating a probit model with a latent variable formulation. Specifically, we utilise a data augmentation technique, where a latent variable is introduced and the binary response is determined via thresholding. In most cases, the AddiVortes model outperforms random forests, BART and other leading black-box regression models when compared using a range of metrics. A comprehensive analysis is conducted using AddiVortes to predict an individual's likelihood of being approved for a home mortgage, based on a range of covariates. This evaluation highlights the model's effectiveness in capturing complex relationships within the data and its potential for improving decision-making in mortgage approval processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21792v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam J. Stone, Emmanuel Ogundimu, John Paul Gosling</dc:creator>
    </item>
    <item>
      <title>Rolled Gaussian process models for curves on manifolds</title>
      <link>https://arxiv.org/abs/2503.21980</link>
      <description>arXiv:2503.21980v1 Announce Type: cross 
Abstract: Given a planar curve, imagine rolling a sphere along that curve without slipping or twisting, and by this means tracing out a curve on the sphere. It is well known that such a rolling operation induces a local isometry between the sphere and the plane so that the two curves uniquely determine each other, and moreover, the operation extends to a general class of manifolds in any dimension. We use rolling to construct an analogue of a Gaussian process on a manifold starting from a Euclidean Gaussian process. The resulting model is generative, and is amenable to statistical inference given data as curves on a manifold. We illustrate with examples on the unit sphere, symmetric positive-definite matrices, and with a robotics application involving 3D orientations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21980v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Preston, Karthik Bharath, Pablo Lopez-Custodio, Alfred Kume</dc:creator>
    </item>
    <item>
      <title>Hierarchical models for small area estimation using zero-inflated forest inventory variables: comparison and implementation</title>
      <link>https://arxiv.org/abs/2503.22103</link>
      <description>arXiv:2503.22103v1 Announce Type: cross 
Abstract: National Forest Inventory (NFI) data are typically limited to sparse networks of sample locations due to cost constraints. While traditional design-based estimators provide reliable forest parameter estimates for large areas, there is increasing interest in model-based small area estimation (SAE) methods to improve precision for smaller spatial, temporal, or biophysical domains. SAE methods can be broadly categorized into area- and unit-level models, with unit-level models offering greater flexibility -- making them the focus of this study. Ensuring valid inference requires satisfying model distributional assumptions, which is particularly challenging for NFI variables that exhibit positive support and zero inflation, such as forest biomass, carbon, and volume. Here, we evaluate a class of two-stage unit-level hierarchical Bayesian models for estimating forest biomass at the county-level in Washington and Nevada, United States. We compare these models to simpler Bayesian single-stage and two-stage frequentist approaches. To assess estimator performance, we employ simulated populations and cross-validation techniques. Results indicate that small area estimators that incorporate a two-stage approach to account for zero inflation, county-specific random intercepts and residual variances, and spatial random effects provide the most reliable county-level estimates. Additionally, findings suggest that unit-level cross-validation within the training dataset is as effective as area-level validation using simulated populations for model selection. We also illustrate the usefulness of simulated populations for better assessing qualities of the various estimators considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22103v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grayson W. White, Andrew O. Finley, Josh K. Yamamoto, Jennifer L. Green, Tracey S. Frescino, Hans-Erik Andersen</dc:creator>
    </item>
    <item>
      <title>Asymptotic Behavior of Principal Component Projections for Multivariate Extremes</title>
      <link>https://arxiv.org/abs/2503.22296</link>
      <description>arXiv:2503.22296v1 Announce Type: cross 
Abstract: The extremal dependence structure of a regularly varying $d$-dimensional random vector can be described by its angular measure. The standard nonparametric estimator of this measure is the empirical measure of the observed angles of the $k$ random vectors with largest norm, for a suitably chosen number $k$. Due to the curse of dimensionality, for moderate or large $d$, this estimator is often inaccurate. If the angular measure is concentrated on a vicinity of a lower dimensional subspace, then first projecting the data on a lower dimensional subspace obtained by a principal component analysis of the angles of extreme observations can substantially improve the performance of the estimator.
  We derive the asymptotic behavior of such PCA projections and the resulting excess risk. In particular, it is shown that, under mild conditions, the excess risk (as a function of $k$) decreases much faster than it was suggested by empirical risk bounds obtained in \cite{DS21}. Moreover, functional limit theorems for local empirical processes of the (empirical) reconstruction error of projections uniformly over neighborhoods of the true optimal projection are established. Based on these asymptotic results, we propose a data-driven method to select the dimension of the projection space. Finally, the finite sample performance of resulting estimators is examined in a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22296v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Holger Drees</dc:creator>
    </item>
    <item>
      <title>Generative Reliability-Based Design Optimization Using In-Context Learning Capabilities of Large Language Models</title>
      <link>https://arxiv.org/abs/2503.22401</link>
      <description>arXiv:2503.22401v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable in-context learning capabilities, enabling flexible utilization of limited historical information to play pivotal roles in reasoning, problem-solving, and complex pattern recognition tasks. Inspired by the successful applications of LLMs in multiple domains, this paper proposes a generative design method by leveraging the in-context learning capabilities of LLMs with the iterative search mechanisms of metaheuristic algorithms for solving reliability-based design optimization problems. In detail, reliability analysis is performed by engaging the LLMs and Kriging surrogate modeling to overcome the computational burden. By dynamically providing critical information of design points to the LLMs with prompt engineering, the method enables rapid generation of high-quality design alternatives that satisfy reliability constraints while achieving performance optimization. With the Deepseek-V3 model, three case studies are used to demonstrated the performance of the proposed approach. Experimental results indicate that the proposed LLM-RBDO method successfully identifies feasible solutions that meet reliability constraints while achieving a comparable convergence rate compared to traditional genetic algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22401v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhonglin Jiang, Qian Tang, Zequn Wang</dc:creator>
    </item>
    <item>
      <title>Gridding and Parameter Expansion for Scalable Latent Gaussian Models of Spatial Multivariate Data</title>
      <link>https://arxiv.org/abs/2101.03579</link>
      <description>arXiv:2101.03579v2 Announce Type: replace 
Abstract: Scalable spatial GPs for massive datasets can be built via sparse Directed Acyclic Graphs (DAGs) where a small number of directed edges is sufficient to flexibly characterize spatial dependence. The DAG can be used to devise fast algorithms for posterior sampling of the latent process, but these may exhibit pathological behavior in estimating covariance parameters. In this article, we introduce gridding and parameter expansion methods to improve the practical performance of MCMC algorithms in terms of effective sample size per unit time (ESS/s). Gridding is a model-based strategy that reduces the number of expensive operations necessary during MCMC on irregularly spaced data. Parameter expansion reduces dependence in posterior samples in spatial regression for high resolution data. These two strategies lead to computational gains in the big data settings on which we focus. We consider popular constructions of univariate spatial processes based on Mat\'ern covariance functions and multivariate coregionalization models for Gaussian outcomes in extensive analyses of synthetic datasets comparing with alternative methods. We demonstrate effectiveness of our proposed methods in a forestry application using remotely sensed data from NASA's Goddard LiDAR, Hyper-Spectral, and Thermal imager (G-LiHT).</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.03579v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/25-BA1515</arxiv:DOI>
      <dc:creator>Michele Peruzzi, Sudipto Banerjee, David B. Dunson, Andrew O. Finley</dc:creator>
    </item>
    <item>
      <title>Personalised Decision-Making without Counterfactuals</title>
      <link>https://arxiv.org/abs/2301.11976</link>
      <description>arXiv:2301.11976v2 Announce Type: replace 
Abstract: This article is a response to recent proposals by Pearl and others for a new approach to personalised treatment decisions, in contrast to the traditional one based on statistical decision theory. We argue that this approach is dangerously misguided and should not be used in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.11976v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Philip Dawid, Stephen Senn</dc:creator>
    </item>
    <item>
      <title>Probabilistic Rainfall Downscaling: Joint Generalized Neural Models with Censored Spatial Gaussian Copula</title>
      <link>https://arxiv.org/abs/2308.09827</link>
      <description>arXiv:2308.09827v2 Announce Type: replace 
Abstract: This work introduces a novel approach for generating conditional probabilistic rainfall forecasts with temporal and spatial dependence. A two-step procedure is employed. Firstly, marginal location-specific distributions are jointly modelled. Secondly, a spatial dependency structure is learned to ensure spatial coherence among these distributions. To learn marginal distributions over rainfall values, we introduce joint generalised neural models which expand generalised linear models with a deep neural network to parameterise a distribution over the outcome space. To understand the spatial dependency structure of the data, a censored latent Gaussian copula model is presented and trained via scoring rules. Leveraging the underlying spatial structure, we construct a distance matrix between locations, transformed into a covariance matrix by a Gaussian Process Kernel depending on a small set of parameters. To estimate these parameters, we propose a general framework for the estimation of Gaussian copulas employing scoring rules as a measure of divergence between distributions. Uniting our two contributions, namely the joint generalised neural model and the censored latent Gaussian copulas into a single model, our probabilistic approach generates forecasts on short to long-term durations, suitable for locations outside the training set. We demonstrate its efficacy using a large UK rainfall data set, outperforming existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.09827v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Huk, Rilwan A. Adewoyin, Ritabrata Dutta</dc:creator>
    </item>
    <item>
      <title>Multiple Imputation of Hierarchical Nonlinear Time Series Data with an Application to School Enrollment Data</title>
      <link>https://arxiv.org/abs/2401.01872</link>
      <description>arXiv:2401.01872v2 Announce Type: replace 
Abstract: International comparisons of hierarchical time series data sets based on survey data, such as annual country-level estimates of school enrollment rates, can suffer from large amounts of missing data due to differing coverage of surveys across countries and across times. A popular approach to handling missing data in these settings is through multiple imputation, which can be especially effective when there is an auxiliary variable that is strongly predictive of and has a smaller amount of missing data than the variable of interest. However, standard methods for multiple imputation of hierarchical time series data can perform poorly when the auxiliary variable and the variable of interest have a nonlinear relationship. Performance can also suffer if the multiple imputations are used to estimate an analysis model that makes different assumptions about the data compared to the imputation model, leading to uncongeniality between analysis and imputation models. We propose a Bayesian method for multiple imputation of hierarchical nonlinear time series data that uses a sequential decomposition of the joint distribution and incorporates smoothing splines to account for nonlinear relationships between variables. We compare the proposed method with existing multiple imputation methods through a simulation study and an application to secondary school enrollment data. We find that the proposed method can lead to substantial performance increases for estimation of parameters in uncongenial analysis models and for prediction of individual missing values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01872v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daphne H. Liu, Adrian E. Raftery</dc:creator>
    </item>
    <item>
      <title>Robust Inference for Generalized Linear Mixed Models: An Approach Based on Score Sign Flipping</title>
      <link>https://arxiv.org/abs/2401.17993</link>
      <description>arXiv:2401.17993v2 Announce Type: replace 
Abstract: Despite the versatility of generalized linear mixed models in handling complex experimental designs, they often suffer from misspecification and convergence problems. This makes inference on the values of coefficients problematic. To address these challenges, we propose a robust extension of the score-based statistical test using sign-flipping transformations. Our approach efficiently handles within-variance structure and heteroscedasticity, ensuring accurate regression coefficient testing. The approach is illustrated by analyzing the reduction of health issues over time for newly adopted children. The model is characterized by a binomial response with unbalanced frequencies and several categorical and continuous predictors. The proposed approach efficiently deals with critical problems related to longitudinal nonlinear models, surpassing common statistical approaches such as generalized estimating equations and generalized linear mixed models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17993v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Angela Andreella, Jelle Goeman, Jesse Hemerik, Livio Finos</dc:creator>
    </item>
    <item>
      <title>Model-based Clustering of Multi-Dimensional Zero-Inflated Counts via the EM Algorithm</title>
      <link>https://arxiv.org/abs/2406.00245</link>
      <description>arXiv:2406.00245v3 Announce Type: replace 
Abstract: Zero-inflated count data arise in various fields, including health, biology, economics, and the social sciences. These data are often modelled using probabilistic distributions such as zero-inflated Poisson (ZIP), zero-inflated negative binomial (ZINB), or zero-inflated binomial (ZIB). To account for heterogeneity in the data, it is often useful to cluster observations into groups that may explain underlying differences in the data-generating process. This paper focuses on model-based clustering for zero-inflated counts when observations are structured in a matrix form rather than a vector. We propose a clustering framework based on mixtures of ZIP or ZINB distributions, with both the count and zero components depending on cluster assignments. Our approach incorporates covariates through a log-linear structure for the mean parameter and includes a size factor to adjust for differences in total sampling or exposure. Model parameters and cluster assignments are estimated via the Expectation-Maximization (EM) algorithm. We assess the performance of our proposed methodology through simulation studies evaluating clustering accuracy and estimator properties, followed by applications to publicly available datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00245v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zahra AghahosseinaliShirazi, Pedro A. Rangel, Camila P. E. de Souza</dc:creator>
    </item>
    <item>
      <title>Interval Estimation of Coefficients in Penalized Regression Models of Insurance Data</title>
      <link>https://arxiv.org/abs/2410.01008</link>
      <description>arXiv:2410.01008v4 Announce Type: replace 
Abstract: The Tweedie exponential dispersion family is a popular choice among many to model insurance losses that consist of zero-inflated semicontinuous data. In such data, it is often important to obtain credibility (inference) of the most important features that describe the endogenous variables. Post-selection inference is the standard procedure in statistics to obtain confidence intervals of model parameters after performing a feature extraction procedure. For a linear model, the lasso estimate often has non-negligible estimation bias for large coefficients corresponding to exogenous variables. To have valid inference on those coefficients, it is necessary to correct the bias of the lasso estimate. Traditional statistical methods, such as hypothesis testing or standard confidence interval construction might lead to incorrect conclusions during post-selection, as they are generally too optimistic. Here we discuss a few methodologies for constructing confidence intervals of the coefficients after feature selection in the Generalized Linear Model (GLM) family with application to insurance data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01008v4</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alokesh Manna, Zijian Huang, Dipak K. Dey, Yuwen Gu, Robin He</dc:creator>
    </item>
    <item>
      <title>Nonparametric directional variogram estimation in the presence of outlier blocks</title>
      <link>https://arxiv.org/abs/2412.01464</link>
      <description>arXiv:2412.01464v2 Announce Type: replace 
Abstract: This paper proposes robust estimators of the variogram, a statistical tool that is commonly used in geostatistics to capture the spatial dependence structure of data. The new estimators are based on the highly robust minimum covariance determinant estimator and estimate the directional variogram for several lags jointly. Simulations and breakdown considerations confirm the good robustness properties of the new estimators. While Genton's estimator based on the robust estimation of the variance of pairwise sums and differences performs well in case of isolated outliers, the new estimators based on robust estimation of multivariate variance and covariance matrices perform superior to the established alternatives in the presence of outlier blocks in the data. The methods are illustrated by an application to satellite data, where outlier blocks may occur because of e.g. clouds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01464v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jana Gierse, Roland Fried</dc:creator>
    </item>
    <item>
      <title>Combining BART and Principal Stratification to estimate the effect of intermediate variables on primary outcomes with application to estimating the effect of family planning on employment in Nigeria and Senegal</title>
      <link>https://arxiv.org/abs/2412.16320</link>
      <description>arXiv:2412.16320v2 Announce Type: replace 
Abstract: There is interest in learning about the causal effects of family planning (FP) on empowerment-related outcomes. Data related to this question are available from studies in which FP programs increase access to FP, but such interventions do not necessarily result in uptake of FP. In addition, women impacted by such programs may differ systematically from target populations of interest in ways that alter the effect of FP. To assess the causal effect of FP on empowerment-related outcomes, we developed a 2-step approach. We use principal stratification and Bayesian Additive Regression Trees (BART) to non-parametrically estimate the effect in the source population among women affected by a FP program. We generalize the results to a broader population by taking the expectation of conditional average treatment effects from the selective sample over the covariate distribution in the target population. To estimate (uncertainty in) the covariate distribution from survey data with a complex sampling design, we use a Bayesian bootstrap (BB). We apply the approach to estimate the causal effect of modern contraceptive use on employment among urban women in Nigeria and Senegal and find strong effects and effect heterogeneity. Sensitivity analyses suggest robustness to violations of assumptions for internal and external validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16320v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Godoy Garraza, Ilene Speizer, Leontine Alkema</dc:creator>
    </item>
    <item>
      <title>DPGLM: A Semiparametric Bayesian GLM with Inhomogeneous Normalized Random Measures</title>
      <link>https://arxiv.org/abs/2502.17827</link>
      <description>arXiv:2502.17827v3 Announce Type: replace 
Abstract: We introduce a novel varying-weight dependent Dirichlet process (DDP) model that extends a recently developed semi-parametric generalized linear model (SPGLM) by adding a nonparametric Bayesian prior on the baseline distribution of the GLM. We show that the resulting model takes the form of an inhomogeneous completely random measure that arises from exponential tilting of a normalized completely random measure. Building on familiar posterior sampling methods for mixtures with respect to normalized random measures, we introduce posterior simulation in the resulting model. We validate the proposed methodology through extensive simulation studies and illustrate its application using data from a speech intelligibility study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17827v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Entejar Alam, Paul J. Rathouz, Peter Mueller</dc:creator>
    </item>
    <item>
      <title>Sparklen: A Statistical Learning Toolkit for High-Dimensional Hawkes Processes in Python</title>
      <link>https://arxiv.org/abs/2502.18979</link>
      <description>arXiv:2502.18979v2 Announce Type: replace 
Abstract: This paper introduces Sparklen, a statistical learning toolkit for Hawkes processes in Python, designed to bring together efficiency and ease of use. The purpose of this package is to provide the Python community with a complete suite of cutting-edge tools specifically tailored for the study of exponential Hawkes processes, with a particular focus on highdimensional framework. It includes state-of-the-art estimation tools with built-in support for incorporating regularization techniques, and novel classification methods. To enhance computational performance, Sparklen leverages a high-performance C++ core for intensive tasks. This dual-language approach makes Sparklen a powerful solution for computationally demanding real-world applications. Here, we present its implementation framework and provide illustrative examples, demonstrating its capabilities and practical usage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18979v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Romain Edmond Lacoste (LAMA)</dc:creator>
    </item>
    <item>
      <title>Compositional Outcomes and Environmental Mixtures: the Dirichlet Bayesian Weighted Quantile Sum Regression</title>
      <link>https://arxiv.org/abs/2503.21428</link>
      <description>arXiv:2503.21428v2 Announce Type: replace 
Abstract: Environmental mixture approaches do not accommodate compositional outcomes, consisting of vectors constrained onto the unit simplex. This limitation poses challenges in effectively evaluating the associations between multiple concurrent environmental exposures and their respective impacts on this type of outcomes. As a result, there is a pressing need for the development of analytical methods that can more accurately assess the complexity of these relationships. Here, we extend the Bayesian weighted quantile sum regression (BWQS) framework for jointly modeling compositional outcomes and environmental mixtures using a Dirichlet distribution with a multinomial logit link function. The proposed approach, named Dirichlet-BWQS (DBWQS), allows for the simultaneous estimation of mixture weights associated with each exposure mixture component as well as the association between the overall exposure mixture index and each outcome proportion. We assess the performance of DBWQS regression on extensive simulated data and a real scenario where we investigate the associations between environmental chemical mixtures and DNA methylation-derived placental cell composition, using publicly available data (GSE75248). We also compare our findings with results considering environmental mixtures and each outcome component. Finally, we developed an R package "xbwqs" where we made our proposed method publicly available (https://github.com/hasdk/xbwqs).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21428v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hachem Saddiki, Joshua L. Warren, Corina Lesseur, Elena Colicino</dc:creator>
    </item>
    <item>
      <title>Compress Then Test: Powerful Kernel Testing in Near-linear Time</title>
      <link>https://arxiv.org/abs/2301.05974</link>
      <description>arXiv:2301.05974v3 Announce Type: replace-cross 
Abstract: Kernel two-sample testing provides a powerful framework for distinguishing any pair of distributions based on $n$ sample points. However, existing kernel tests either run in $n^2$ time or sacrifice undue power to improve runtime. To address these shortcomings, we introduce Compress Then Test (CTT), a new framework for high-powered kernel testing based on sample compression. CTT cheaply approximates an expensive test by compressing each $n$ point sample into a small but provably high-fidelity coreset. For standard kernels and subexponential distributions, CTT inherits the statistical behavior of a quadratic-time test -- recovering the same optimal detection boundary -- while running in near-linear time. We couple these advances with cheaper permutation testing, justified by new power analyses; improved time-vs.-quality guarantees for low-rank approximation; and a fast aggregation procedure for identifying especially discriminating kernels. In our experiments with real and simulated data, CTT and its extensions provide 20--200x speed-ups over state-of-the-art approximate MMD tests with no loss of power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.05974v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carles Domingo-Enrich, Raaz Dwivedi, Lester Mackey</dc:creator>
    </item>
    <item>
      <title>Semiparametric Triple Difference Estimators</title>
      <link>https://arxiv.org/abs/2502.19788</link>
      <description>arXiv:2502.19788v2 Announce Type: replace-cross 
Abstract: The triple difference causal inference framework is an extension of the well-known difference-in-differences framework. It relaxes the parallel trends assumption of the difference-in-differences framework through leveraging data from an auxiliary domain. Despite being commonly applied in empirical research, the triple difference framework has received relatively limited attention in the statistics literature. Specifically, investigating the intricacies of identification and the design of robust and efficient estimators for this framework has remained largely unexplored. This work aims to address these gaps in the literature. From the identification standpoint, we present outcome regression and weighting methods to identify the average treatment effect on the treated in both panel data and repeated cross-section settings. For the latter, we relax the commonly made assumption of time-invariant covariates. From the estimation perspective, we consider semiparametric estimators for the triple difference framework in both panel data and repeated cross-sections settings. These estimators are based upon the cross-fitting technique, and flexible machine learning tools can be used to estimate the nuisance components. We demonstrate that our proposed estimators are doubly robust, and we characterize the conditions under which they are consistent and asymptotically normal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19788v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 31 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sina Akbari, Negar Kiyavash, AmirEmad Ghassami</dc:creator>
    </item>
  </channel>
</rss>
