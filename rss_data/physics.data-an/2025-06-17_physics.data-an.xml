<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Jun 2025 04:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Using krypton-83m to determine the neutrino-mass bias caused by a non-constant electric potential of the KATRIN source</title>
      <link>https://arxiv.org/abs/2506.13829</link>
      <description>arXiv:2506.13829v1 Announce Type: cross 
Abstract: Precision spectroscopy of the electron spectrum of the tritium $\beta$ decay near the kinematic endpoint is a direct method to determine the effective electron antineutrino mass. The KArlsruhe TRItium Neutrino (KATRIN) experiment aims to determine this quantity with a sensitivity of better than $0.3\,\mathrm{eV}$ ($90\,\%$~C.L.). An inhomogeneous electric potential in the tritium source of KATRIN leads to a distortion of the $\beta$ spectrum, which directly impacts the neutrino-mass observable. This effect can be quantified through precision spectroscopy of the conversion electrons of co-circulated metastable $^{83\mathrm{m}}Kr$. This work perturbatively describes the effect of the source potential on the recorded spectra, and thus establishes the leading-order observables of this effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13829v1</guid>
      <category>physics.ins-det</category>
      <category>nucl-ex</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moritz Machatschek</dc:creator>
    </item>
    <item>
      <title>Detection, attribution, and modeling of climate change: key open issues</title>
      <link>https://arxiv.org/abs/2506.13994</link>
      <description>arXiv:2506.13994v1 Announce Type: cross 
Abstract: The CMIP global climate models (GCMs) assess that nearly 100% of global surface warming observed between 1850-1900 and 2011-2020 is attributable to anthropogenic drivers like greenhouse gas emissions. These models also generate future climate projections based on shared socioeconomic pathways (SSPs), aiding in risk assessment and the development of costly Net-Zero climate mitigation strategies. Yet, the CMIP GCMs face significant scientific challenges in attributing and modeling climate change, particularly in capturing natural climate variability over multiple timescales throughout the Holocene. Other key concerns include the reliability of global surface temperature records, the accuracy of solar irradiance models, and the robustness of climate sensitivity estimates. Global warming estimates may be overstated due to uncorrected non-climatic biases, and the GCMs may significantly underestimate solar and astronomical influences on climate variations. The equilibrium climate sensitivity (ECS) to radiative forcing could be lower than commonly assumed; empirical findings suggest ECS values lower than 3 K and possibly even closer to 1.1 +/- 0.4 K. Empirical models incorporating natural variability suggest that the 21st-century global warming may remain moderate, even under SSP scenarios that do not necessitate Net-Zero emission policies. These findings raise important questions regarding the necessity and urgency of implementing aggressive climate mitigation strategies. While GCMs remain essential tools for climate research and policymaking, their scientific limitations underscore the need for more refined modeling approaches to ensure accurate future climate assessments. Addressing uncertainties related to climate change detection, natural variability, solar influences, and climate sensitivity to radiative forcing will enhance predictions and better inform sustainable climate strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13994v1</guid>
      <category>physics.soc-ph</category>
      <category>physics.ao-ph</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.gr.2025.05.001</arxiv:DOI>
      <arxiv:journal_reference>Gondwana Research, 2025</arxiv:journal_reference>
      <dc:creator>Nicola Scafetta</dc:creator>
    </item>
    <item>
      <title>Evolutionary chemical learning in dimerization networks</title>
      <link>https://arxiv.org/abs/2506.14006</link>
      <description>arXiv:2506.14006v1 Announce Type: cross 
Abstract: We present a novel framework for chemical learning based on Competitive Dimerization Networks (CDNs) - systems in which multiple molecular species, e.g. proteins or DNA/RNA oligomers, reversibly bind to form dimers. We show that these networks can be trained in vitro through directed evolution, enabling the implementation of complex learning tasks such as multiclass classification without digital hardware or explicit parameter tuning. Each molecular species functions analogously to a neuron, with binding affinities acting as tunable synaptic weights. A training protocol involving mutation, selection, and amplification of DNA-based components allows CDNs to robustly discriminate among noisy input patterns. The resulting classifiers exhibit strong output contrast and high mutual information between input and output, especially when guided by a contrast-enhancing loss function. Comparative analysis with in silico gradient descent training reveals closely correlated performance. These results establish CDNs as a promising platform for analog physical computation, bridging synthetic biology and machine learning, and advancing the development of adaptive, energy-efficient molecular computing systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14006v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>nlin.AO</category>
      <category>physics.data-an</category>
      <category>q-bio.MN</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexei V. Tkachenko, Bortolo Matteo Mognetti, Sergei Maslov</dc:creator>
    </item>
    <item>
      <title>The phenomenological renormalization group in neuronal models near criticality</title>
      <link>https://arxiv.org/abs/2506.14053</link>
      <description>arXiv:2506.14053v1 Announce Type: cross 
Abstract: The phenomenological renormalization group (PRG) has been applied to the study of scaleinvariant phenomena in neuronal data, providing evidence for critical phenomena in the brain. However, it remains unclear how reliably these observed signatures indicate genuine critical behavior, as it is not well established how close to criticality a system must be for them to emerge. Here, we rely on neuronal models with known critical points to investigate under which conditions the PRG procedure yields consistent results. We discuss how the time-binning step of data preprocessing can crucially affect the final results, and propose a data-driven method to adapt the time bin in order to circumvent this issue. Under these conditions, the PRG method only detects scaling behavior in neuronal models within a very narrow range of the critical point, lending credence to the conclusions drawn from PRG results in experimental data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14053v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>nlin.AO</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaio F. R. Nascimento, Daniel M. Castro, Gustavo G. Cambrainha, Mauro Copelli</dc:creator>
    </item>
    <item>
      <title>Machine learning approaches for automatic cleaning of investigative drilling data</title>
      <link>https://arxiv.org/abs/2506.14289</link>
      <description>arXiv:2506.14289v1 Announce Type: cross 
Abstract: Investigative drilling (ID) is an innovative measurement while drilling (MWD) technique that has been implemented in various site investigation projects across Australia. While the automated drilling feature of ID substantially reduces noise within drilling data streams, data cleaning remains essential for removing anomalies to enable accurate strata classification and prediction of soil and rock properties. This study employed three machine learning algorithms--IsoForest, one-class SVM, and DBSCAN--to automate the data cleaning process for ID data in rock drilling scenarios. Two data cleaning contexts were examined: (1) removing anomalies in rock drilling data, and (2) removing both anomalies and soil drilling data in mixed rock drilling data. The analysis revealed that all three machine learning algorithms outperformed traditional statistical methods (the 3-sigma rule and IQR method) in both data cleaning tasks, achieving a good balance between true positive rate and false positive rate, though hyperparameter tuning was required for one-class SVM and DBSCAN. Among them, IsoForest was proven to be the best-performing algorithm, capable of removing anomalies effectively without the need for hyperparameter adjustment. Furthermore, IsoForest, combined with two-cluster K-means, successfully eliminated both soil drilling data and anomalies while preserving almost all the normal data. The automatic data cleaning strategy proposed in this paper has the potential to reduce laborious manual data cleaning efforts and thereby facilitate the development of large-scale, high-quality datasets for machine learning studies capable of revealing complex relationships between drilling data and rock properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14289v1</guid>
      <category>physics.geo-ph</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fei Huang (Flinders University), Hongyu Qin (Flinders University), Masoud Manafi (Civil Group), Ben Juett (Civil Group), Ben Evans (Civil Group)</dc:creator>
    </item>
    <item>
      <title>Review of Machine Learning for Real-Time Analysis at the Large Hadron Collider experiments ALICE, ATLAS, CMS and LHCb</title>
      <link>https://arxiv.org/abs/2506.14578</link>
      <description>arXiv:2506.14578v1 Announce Type: cross 
Abstract: The field of high energy physics (HEP) has seen a marked increase in the use of machine learning (ML) techniques in recent years. The proliferation of applications has revolutionised many aspects of the data processing pipeline at collider experiments including the Large Hadron Collider (LHC). In this whitepaper, we discuss the increasingly crucial role that ML plays in real-time analysis (RTA) at the LHC, namely in the context of the unique challenges posed by the trigger systems of the large LHC experiments. We describe a small selection of the ML applications in use at the large LHC experiments to demonstrate the breadth of use-cases. We continue by emphasising the importance of collaboration and engagement between the HEP community and industry, highlighting commonalities and synergies between the two. The mutual benefits are showcased in several interdisciplinary examples of RTA from industrial contexts. This whitepaper, compiled by the SMARTHEP network, does not provide an exhaustive review of ML at the LHC but rather offers a high-level overview of specific real-time use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14578v1</guid>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Boggia, Carlos Cocha, Fotis Giasemis, Joachim Hansen, Patin Inkaew, Kaare Endrup Iversen, Pratik Jawahar, Henrique Pineiro Monteagudo, Micol Olocco, Sten Astrand, Martino Borsato, Leon Bozianu, Steven Schramm, the SMARTHEP Network</dc:creator>
    </item>
    <item>
      <title>SETI@home: Data Acquisition and Front-End Processing</title>
      <link>https://arxiv.org/abs/2506.14718</link>
      <description>arXiv:2506.14718v1 Announce Type: cross 
Abstract: SETI@home is a radio Search for Extraterrestrial Intelligence (SETI) project, looking for technosignatures in data recorded at multiple observatories from 1998 to 2020. Most radio SETI projects analyze data using dedicated processing hardware. SETI@home uses a different approach: time-domain data is distributed over the Internet to $\gt 10^{5}$ volunteered home computers, which analyze it. The large amount of computing power this affords ($\sim 10^{15}$ floating-point operations per second (FPOP/s)) allows us to increase the sensitivity and generality of our search in three ways. We use coherent integration, a technique in which data is transformed so that the power of drifting signals is confined to a single discrete Fourier transform (DFT) bin. We perform this coherent search over 123 000 Doppler drift rates in the range ($\pm$100 Hz s$^{-1}$). Second, we search for a variety of signal types, such as pulsed signals and arbitrary repeated waveforms. The analysis uses a range of DFT sizes, with frequency resolutions ranging from 0.075 Hz to 1221 Hz. The front end of SETI@home produces a set of detections that exceed thresholds in power and goodness of fit. We accumulated $\sim 1.2\times 10^{10}$ such detections. The back end of SETI@home takes these detections, identifies and removes radio frequency interference (RFI), and looks for groups of detections that are consistent with extraterrestrial origin and that persist over long timescales. This paper describes the front end of SETI@home and provides parameters for the primary data source, the Arecibo Observatory; the back end and its results are described in a companion paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14718v1</guid>
      <category>astro-ph.IM</category>
      <category>cs.DC</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric J. Korpela (Space Sciences Laboratory, University of California, Berkeley), David P. Anderson (Space Sciences Laboratory, University of California, Berkeley), Jeff Cobb (Space Sciences Laboratory, University of California, Berkeley), Matt Lebofsky (Space Sciences Laboratory, University of California, Berkeley), Wei Liu (Space Sciences Laboratory, University of California, Berkeley), Dan Werthimer (Space Sciences Laboratory, University of California, Berkeley)</dc:creator>
    </item>
    <item>
      <title>SETI@home: Data Analysis and Findings</title>
      <link>https://arxiv.org/abs/2506.14737</link>
      <description>arXiv:2506.14737v1 Announce Type: cross 
Abstract: SETI@home is a radio Search for Extraterrestrial Intelligence (SETI) project that looks for technosignatures in data recorded at the Arecibo Observatory. The data were collected over a period of 14 years and cover almost the entire sky visible to the telescope. The first stage of data analysis found billions of detections: brief excesses of continuous or pulsed narrowband power. The second stage removed detections that were likely radio frequency interference (RFI), then identified and ranked signal candidates: groups of detections, possibly spread over the 14 years, that plausibly originate from a single cosmic source. We manually examined the top-ranking signal candidates and selected a few hundred. In the third and final stage we are reobserving the corresponding sky locations and frequency ranges using the Five-hundred-meter Aperture Spherical Telescope (FAST) radio telescope. This paper covers SETI@home's second stage of data analysis. We describe the algorithms used to remove RFI and to identify and rank signal candidates. To guide the development of these algorithms, we used artificial candidate birdies that model persistent ET signals with a range of power, bandwidth, and planetary motion parameters. This approach also allowed us to estimate the sensitivity of our detection system to these signals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14737v1</guid>
      <category>astro-ph.IM</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David P. Anderson (Space Sciences Laboratory, University of California, Berkeley), Eric J. Korpela (Space Sciences Laboratory, University of California, Berkeley), Dan Werthimer (Space Sciences Laboratory, University of California, Berkeley), Jeff Cobb (Space Sciences Laboratory, University of California, Berkeley), Bruce Allen (Max Planck Institut f\"ur Gravitationsphysik)</dc:creator>
    </item>
    <item>
      <title>Large Language Models -- the Future of Fundamental Physics?</title>
      <link>https://arxiv.org/abs/2506.14757</link>
      <description>arXiv:2506.14757v1 Announce Type: cross 
Abstract: For many fundamental physics applications, transformers, as the state of the art in learning complex correlations, benefit from pretraining on quasi-out-of-domain data. The obvious question is whether we can exploit Large Language Models, requiring proper out-of-domain transfer learning. We show how the Qwen2.5 LLM can be used to analyze and generate SKA data, specifically 3D maps of the cosmological large-scale structure for a large part of the observable Universe. We combine the LLM with connector networks and show, for cosmological parameter regression and lightcone generation, that this Lightcone LLM (L3M) with Qwen2.5 weights outperforms standard initialization and compares favorably with dedicated networks of matching size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14757v1</guid>
      <category>astro-ph.CO</category>
      <category>astro-ph.IM</category>
      <category>hep-ph</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caroline Heneka, Florian Nieser, Ayodele Ore, Tilman Plehn, Daniel Schiller</dc:creator>
    </item>
  </channel>
</rss>
