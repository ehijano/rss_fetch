<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 28 Apr 2025 04:00:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>EcoServe: Enabling Cost-effective LLM Serving with Proactive Intra- and Inter-Instance Orchestration</title>
      <link>https://arxiv.org/abs/2504.18154</link>
      <description>arXiv:2504.18154v1 Announce Type: new 
Abstract: Existing LLM serving strategies can be categorized based on whether prefill and decode phases are disaggregated: non-disaggregated (NoDG) or fully disaggregated (FuDG). However, the NoDG strategy leads to strong prefill-decode interference and the FuDG strategy highly relies on high-performance interconnects, making them less cost-effective.
  We introduce EcoServe, a system that enables cost-effective LLM serving on clusters with commodity interconnects. EcoServe is built on the partially disaggregated (PaDG) strategy, applying temporal disaggregation and rolling activation for proactive intra- and inter-instance scheduling. It first disaggregates the prefill and decode phases along the time dimension within a single instance to mitigate inter-phase interference and enhance throughput. Next, it coordinates multiple instances and cyclically activates them to ensure the continuous availability of prefill processing, thereby improving latency. Thus, EcoServe's basic serving unit is the macro instance, within which multiple instances collaborate. It further integrates an adaptive scheduling algorithm to route requests in a macro instance and a mitosis scaling approach to enable fine-grained capacity scaling. Beyond delivering high goodput, EcoServe excels in load balancing, hardware cost, parallelism compatibility, and even engineering simplicity compared to existing solutions.
  When serving 30B- and 70B-scale models on a production-level cluster with 32 NVIDIA L20 GPUs using commodity Ethernet, EcoServe averagely improves goodput by 82.49%, 86.17%, 122.76%, and 126.96% over four representative NoDG and FuDG systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18154v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiangsu Du, Hongbin Zhang, Taosheng Wei, Zhenyi Zheng, Kaiyi Wu, Zhiguang Chen, Yutong Lu</dc:creator>
    </item>
    <item>
      <title>Dynamic Memory Management on GPUs with SYCL</title>
      <link>https://arxiv.org/abs/2504.18211</link>
      <description>arXiv:2504.18211v1 Announce Type: new 
Abstract: Dynamic memory allocation is not traditionally available in kernels running on GPUs. This work aims to build on Ouroboros, an efficient dynamic memory management library for CUDA applications, by porting the code to SYCL, a cross-platform accelerator API. Since SYCL can be compiled to a CUDA backend, it is possible to compare the performance of the SYCL implementation with that of the original CUDA implementation, as well as test it on non-CUDA platforms such as Intel's Xe graphics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18211v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Russell K. Standish</dc:creator>
    </item>
    <item>
      <title>Fast Multichannel Topology Discovery in Cognitive Radio Networks</title>
      <link>https://arxiv.org/abs/2504.17818</link>
      <description>arXiv:2504.17818v1 Announce Type: cross 
Abstract: In Cognitive Radio Networks (CRNs), secondary users (SUs) must efficiently discover each other across multiple communication channels while avoiding interference from primary users (PUs). Traditional multichannel rendezvous algorithms primarily focus on enabling pairs of SUs to find common channels without explicitly considering the underlying network topology. In this paper, we extend the rendezvous framework to explicitly incorporate network topology, introducing the \emph{multichannel topology discovery problem}. We propose a novel \emph{pseudo-random sweep algorithm with forward replacement}, designed to minimize correlation between consecutive unsuccessful rendezvous attempts, thereby significantly reducing the expected time-to-discovery (ETTD). Additionally, we introduce a \emph{threshold-based stick-together strategy} that dynamically synchronizes user hopping sequences based on partially known information, further enhancing discovery efficiency. Extensive simulation results validate our theoretical analysis, demonstrating that the proposed algorithms substantially outperform conventional (sequential) sweep methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17818v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yung-Li Wang, Yiwei Liu, Cheng-Shang Chang</dc:creator>
    </item>
    <item>
      <title>Optimal Secure Coded Distributed Computation over all Fields</title>
      <link>https://arxiv.org/abs/2504.18038</link>
      <description>arXiv:2504.18038v1 Announce Type: cross 
Abstract: We construct optimal secure coded distributed schemes that extend the known optimal constructions over fields of characteristic 0 to all fields. A serendipitous result is that we can encode \emph{all} functions over finite fields with a recovery threshold proportional to the complexity (tensor rank or multiplicative); this is due to the well-known result that all functions over a finite field can be represented as multivariate polynomials (or symmetric tensors). We get that a tensor of order $\ell$ (or a multivariate polynomial of degree $\ell$) can be computed in the faulty network of $N$ nodes setting within a factor of $\ell$ and an additive term depending on the genus of a code with $N$ rational points and distance covering the number of faulty servers; in particular, we present a coding scheme for general matrix multiplication of two $m \times m $ matrices with a recovery threshold of $2 m^{\omega } -1+g$ where $\omega $ is the exponent of matrix multiplication which is optimal for coding schemes using AG codes. Moreover, we give sufficient conditions for which the Hadamard-Shur product of general linear codes gives a similar recovery threshold, which we call \textit{log-additive codes}. Finally, we show that evaluation codes with a \textit{curve degree} function (first defined in [Ben-Sasson et al. (STOC '13)]) that have well-behaved zero sets are log-additive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18038v1</guid>
      <category>cs.IT</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>cs.SC</category>
      <category>math.AG</category>
      <category>math.IT</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pedro Soto</dc:creator>
    </item>
    <item>
      <title>Accelerating Particle-in-Cell Monte Carlo Simulations with MPI, OpenMP/OpenACC and Asynchronous Multi-GPU Programming</title>
      <link>https://arxiv.org/abs/2404.10270</link>
      <description>arXiv:2404.10270v4 Announce Type: replace 
Abstract: As fusion energy devices advance, plasma simulations are crucial for reactor design. Our work extends BIT1 hybrid parallelization by integrating MPI with OpenMP and OpenACC, focusing on asynchronous multi-GPU programming. Results show significant performance gains: 16 MPI ranks plus OpenMP threads reduced runtime by 53% on a petascale EuroHPC supercomputer, while OpenACC multicore achieved a 58% reduction. At 64 MPI ranks, OpenACC outperformed OpenMP, improving the particle mover function by 24%. On MareNostrum 5, OpenACC async(n) delivered strong performance, but OpenMP asynchronous multi-GPU approach proved more effective at extreme scaling, maintaining efficiency up to 400 GPUs. Speedup and parallel efficiency (PE) studies revealed OpenMP asynchronous multi-GPU achieving 8.77x speedup (54.81% PE), surpassing OpenACC (8.14x speedup, 50.87% PE). While PE declined at high node counts due to communication overhead, asynchronous execution mitigated scalability bottlenecks. OpenMP nowait and depend clauses improved GPU performance via efficient data transfer and task management. Using NVIDIA Nsight tools, we confirmed BIT1 efficiency for large-scale plasma simulations. OpenMP asynchronous multi-GPU implementation delivered exceptional performance in portability, high throughput, and GPU utilization, positioning BIT1 for exascale supercomputing and advancing fusion energy research. MareNostrum 5 brings us closer to achieving exascale performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10270v4</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jocs.2025.102590</arxiv:DOI>
      <dc:creator>Jeremy J. Williams, Felix Liu, Jordy Trilaksono, David Tskhakaya, Stefan Costea, Leon Kos, Ales Podolnik, Jakub Hromadka, Pratibha Hegde, Marta Garcia-Gasulla, Valentin Seitz, Frank Jenko, Erwin Laure, Stefano Markidis</dc:creator>
    </item>
    <item>
      <title>Adaptive Heuristics for Scheduling DNN Inferencing on Edge and Cloud for Personalized UAV Fleets</title>
      <link>https://arxiv.org/abs/2412.20860</link>
      <description>arXiv:2412.20860v2 Announce Type: replace 
Abstract: Drone fleets with onboard cameras coupled with computer vision and DNN inferencing models can support diverse applications. One such novel domain is for one or more buddy drones to assist Visually Impaired People (VIPs) lead an active lifestyle. Video inferencing tasks from such drones can help both navigate the drone and provide situation awareness to the VIP, and hence have strict execution deadlines. We propose a deadline-driven heuristic, DEMS-A, to schedule diverse DNN tasks generated continuously to perform inferencing over video segments generated by multiple drones linked to an edge, with the option to execute on the cloud. We use strategies like task dropping, work stealing and migration, and dynamic adaptation to cloud variability, to guarantee a Quality of Service (QoS), i.e. maximize the utility and the number of tasks completed. We also introduce an additional Quality of Experience (QoE) metric useful to the assistive drone domain, which values the frequency of success for task types to ensure the responsiveness and reliability of the VIP application. We extend our DEMS solution to GEMS to solve this. We evaluate these strategies, using (i) an emulated setup of a fleet of over 80 drones supporting over 25 VIPs, with real DNN models executing on pre-recorded drone video streams, using Jetson Nano edges and AWS Lambda cloud functions, and (ii) a real-world setup of a Tello drone and a Jetson Orin Nano edge generating drone commands to follow a VIP in real-time. Our strategies present a task completion rate of up to 88%, up to 2.7x higher QoS utility compared to the baselines, a further 16% higher QoS utility while adapting to network variability, and up to 75% higher QoE utility. Our practical validation exhibits task completion of up to 87% for GEMS and 33% higher total utility of GEMS compared to edge-only.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20860v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Suman Raj, Radhika Mittal, Harshil Gupta, Yogesh Simmhan</dc:creator>
    </item>
    <item>
      <title>The Geometry of Fixed-Magnetization Spin Systems at Low Temperature</title>
      <link>https://arxiv.org/abs/2411.03643</link>
      <description>arXiv:2411.03643v2 Announce Type: replace-cross 
Abstract: Spin systems are fundamental models of statistical physics that provide insight into collective behavior across scientific domains. Their interest to computer science stems in part from the deep connection between the phase transitions they exhibit and the computational complexity of sampling from the probability distributions they describe. Our focus is on the geometry of spin configurations, motivated by applications to programmable matter and computational biology. Rigorous results in this vein are scarce because the natural setting of these applications is the low-temperature, fixed-magnetization regime. Recent progress in this regime is largely limited to spin systems under which magnetization concentrates, which enables the analysis to be reduced to that of the simpler, variable-magnetization case. More complicated models, like those that arise in applications, do not share this property.
  We study the geometry of spin configurations on the triangular lattice under the Generalized Potts Model (GPM), which generalizes many fundamental models of statistical physics, including the Ising, Potts, clock, and Blume--Capel models. Moreover, it specializes to models used to program active matter to solve tasks like compression and separation, and it is closely related to the Cellular Potts Model, widely used in computational models of biological processes. Our main result shows that, under the fixed-magnetization GPM at low temperature, spins of different types are typically partitioned into regions of mostly one type, separated by boundaries that have nearly minimal perimeter. The proof uses techniques from Pirogov--Sinai theory to extend a classic Peierls argument for the fixed-magnetization Ising model, and introduces a new approach for comparing the partition functions of fixed- and variable-magnetization models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03643v2</guid>
      <category>math-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.DC</category>
      <category>math.MP</category>
      <category>math.PR</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacob Calvert, Shunhao Oh, Dana Randall</dc:creator>
    </item>
    <item>
      <title>Optimizing ML Concurrent Computation and Communication with GPU DMA Engines</title>
      <link>https://arxiv.org/abs/2412.14335</link>
      <description>arXiv:2412.14335v2 Announce Type: replace-cross 
Abstract: Concurrent computation and communication (C3) is a pervasive paradigm in ML and other domains, making its performance optimization crucial. In this paper, we carefully characterize C3 in ML on GPUs, which are most widely deployed for ML training and inference. We observe that while C3 leads to performance uplifts, the uplifts are far lower than ideal speedups (serial computation and communication versus maximum of computation or communication; all times from isolated executions). That is, C3 on average achieves only 21% of ideal speedup. This is so, due to known challenges of compute and memory interference between concurrent GPU kernels (that is, sharing of GPU's compute units, caches and HBM).
  To attain better performance for C3, first, we evaluate dual strategies of schedule prioritization and careful resource partitioning of compute units on GPUs to push performance attained with C3 (on average 42% of ideal speedup). We also provide heuristics that can guide a runtime while employing these strategies. To further enhance C3 performance, we propose to mitigate C3 interference by offloading communication tasks to the GPU's DMA engines. To this end, we build concurrent communication collectives (ConCCL) proof-of-concepts that harness DMA engines for communication. We show how ConCCL considerably closes the gap between realized and ideal speedup for C3 (on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall, our work makes a strong case for GPU DMA engine advancements to better support C3 on GPUs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14335v2</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anirudha Agrawal, Shaizeen Aga, Suchita Pati, Mahzabeen Islam</dc:creator>
    </item>
    <item>
      <title>Fully Scalable MPC Algorithms for Euclidean k-Center</title>
      <link>https://arxiv.org/abs/2504.16382</link>
      <description>arXiv:2504.16382v2 Announce Type: replace-cross 
Abstract: The $k$-center problem is a fundamental optimization problem with numerous applications in machine learning, data analysis, data mining, and communication networks. The $k$-center problem has been extensively studied in the classical sequential setting for several decades, and more recently there have been some efforts in understanding the problem in parallel computing, on the Massively Parallel Computation (MPC) model. For now, we have a good understanding of $k$-center in the case where each local MPC machine has sufficient local memory to store some representatives from each cluster, that is, when one has $\Omega(k)$ local memory per machine. While this setting covers the case of small values of $k$, for a large number of clusters these algorithms require undesirably large local memory, making them poorly scalable. The case of large $k$ has been considered only recently for the fully scalable low-local-memory MPC model for the Euclidean instances of the $k$-center problem. However, the earlier works have been considering only the constant dimensional Euclidean space, required a super-constant number of rounds, and produced only $k(1+o(1))$ centers whose cost is a super-constant approximation of $k$-center.
  In this work, we significantly improve upon the earlier results for the $k$-center problem for the fully scalable low-local-memory MPC model. In the low dimensional Euclidean case in $\mathbb{R}^d$, we present the first constant-round fully scalable MPC algorithm for $(2+\varepsilon)$-approximation. We push the ratio further to $(1 + \varepsilon)$-approximation albeit using slightly more $(1 + \varepsilon)k$ centers. All these results naturally extends to slightly super-constant values of $d$. In the high-dimensional regime, we provide the first fully scalable MPC algorithm that in a constant number of rounds achieves an $O(\log n/ \log \log n)$-approximation for $k$-center.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16382v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Artur Czumaj, Guichen Gao, Mohsen Ghaffari, Shaofeng H. -C. Jiang</dc:creator>
    </item>
  </channel>
</rss>
