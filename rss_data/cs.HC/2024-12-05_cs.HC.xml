<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Dec 2024 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>FathomGPT: A Natural Language Interface for Interactively Exploring Ocean Science Data</title>
      <link>https://arxiv.org/abs/2412.02784</link>
      <description>arXiv:2412.02784v1 Announce Type: new 
Abstract: We introduce FathomGPT, an open source system for the interactive investigation of ocean science data via a natural language interface. FathomGPT was developed in close collaboration with marine scientists to enable researchers to explore and analyze the FathomNet image database. FathomGPT provides a custom information retrieval pipeline that leverages OpenAI's large language models to enable: the creation of complex queries to retrieve images, taxonomic information, and scientific measurements; mapping common names and morphological features to scientific names; generating interactive charts on demand; and searching by image or specified patterns within an image. In designing FathomGPT, particular emphasis was placed on enhancing the user's experience by facilitating free-form exploration and optimizing response times. We present an architectural overview and implementation details of FathomGPT, along with a series of ablation studies that demonstrate the effectiveness of our approach to name resolution, fine tuning, and prompt modification. We also present usage scenarios of interactive data exploration sessions and document feedback from ocean scientists and machine learning experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02784v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3654777.3676462</arxiv:DOI>
      <arxiv:journal_reference>UIST 2024: Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology</arxiv:journal_reference>
      <dc:creator>Nabin Khanal, Chun Meng Yu, Jui-Cheng Chiu, Anav Chaudhary, Ziyue Zhang, Kakani Katija, Angus G. Forbes</dc:creator>
    </item>
    <item>
      <title>OriStitch: A Machine Embroidery Workflow to Turn Existing Fabrics into Self-Folding 3D Textiles</title>
      <link>https://arxiv.org/abs/2412.02891</link>
      <description>arXiv:2412.02891v1 Announce Type: new 
Abstract: OriStitch is a computational fabrication workflow to turn existing flat fabrics into self-folding 3D structures. Users turn fabrics into self-folding sheets by machine embroidering functional threads in specific patterns on fabrics, and then apply heat to deform the structure into a target 3D structure. OriStitch is compatible with a range of existing materials (e.g., leather, woven fabric, and denim).
  We present the design of specific embroidered hinges that fully close under exposure to heat. We discuss the stitch pattern design, thread and fabric selection, and heating conditions. To allow users to create 3D textiles using our hinges, we create a tool to convert 3D meshes to 2D stitch patterns automatically, as well as an end-to-end fabrication and actuation workflow. To validate this workflow, we designed and fabricated a cap (303 hinges), a handbag (338 hinges), and a cover for an organically shaped vase (140 hinges).
  In technical evaluation, we found that our tool successfully converted 23/28 models (textures and volumetric objects) found in related papers. We also demonstrate the folding performance across different materials (suede leather, cork, Neoprene, and felt).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02891v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zekun Chang, Yuta Noma, Shuo Feng, Xinyi Yang, Kazuhiro Shinoda, Tung D. Ta, Koji Yatani, Tomoyuki Yokota, Takao Someya, Yoshihiro Kawahara, Koya Narumi, Francois Guimbretiere, Thijs Roumen</dc:creator>
    </item>
    <item>
      <title>A Survey of Wireless Sensing Security from a Role-Based View: Victim, Weapon, and Shield</title>
      <link>https://arxiv.org/abs/2412.03064</link>
      <description>arXiv:2412.03064v1 Announce Type: new 
Abstract: Wireless sensing technology has become prevalent in healthcare, smart homes, and autonomous driving due to its non-contact operation, penetration capabilities, and cost-effectiveness. As its applications expand, the technology faces mounting security challenges: sensing systems can be attack targets, signals can be weaponized, or signals can function as security shields. Despite these security concerns significantly impacting the technology's development, a systematic review remains lacking. This paper presents the first comprehensive survey of wireless sensing security through a role-based perspective. Analyzing over 200 publications from 2020-2024, we propose a novel classification framework that systematically categorizes existing research into three main classes: (1) wireless systems as victims of attacks, (2) wireless signals as weapons for attacks, and (3) wireless signals as shields for security applications. This role-based classification method is not only intuitive and easy to understand, but also reflects the essential connection between wireless signals and security issues. Through systematic literature review and quantitative analysis, this paper outlines a panoramic view of wireless sensing security, revealing key technological trends and innovation opportunities, thereby helping to promote the development of this field. Project page: \url{https://github.com/Intelligent-Perception-Lab/Awesome-WS-Security}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03064v1</guid>
      <category>cs.HC</category>
      <category>cs.CR</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruixu Geng, Jianyang Wang, Yuqin Yuan, Fengquan Zhan, Tianyu Zhang, Rui Zhang, Pengcheng Huang, Dongheng Zhang, Jinbo Chen, Yang Hu, Yan Chen</dc:creator>
    </item>
    <item>
      <title>ObjectFinder: Open-Vocabulary Assistive System for Interactive Object Search by Blind People</title>
      <link>https://arxiv.org/abs/2412.03118</link>
      <description>arXiv:2412.03118v1 Announce Type: new 
Abstract: Assistive technology can be leveraged by blind people when searching for objects in their daily lives. We created ObjectFinder, an open-vocabulary interactive object-search prototype, which combines object detection with scene description and navigation. It enables blind persons to detect and navigate to objects of their choice. Our approach used co-design for the development of the prototype. We further conducted need-finding interviews to better understand challenges in object search, followed by a study with the ObjectFinder prototype in a laboratory setting simulating a living room and an office, with eight blind users. Additionally, we compared the prototype with BeMyEyes and Lookout for object search. We found that most participants felt more independent with ObjectFinder and preferred it over the baselines when deployed on more efficient hardware, as it enhances mental mapping and allows for active target definition. Moreover, we identified factors for future directions for the development of object-search systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03118v1</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiping Liu, Jiaming Zhang, Angela Sch\"on, Karin M\"uller, Junwei Zheng, Kailun Yang, Kathrin Gerling, Rainer Stiefelhagen</dc:creator>
    </item>
    <item>
      <title>Channel Reflection: Knowledge-Driven Data Augmentation for EEG-Based Brain-Computer Interfaces</title>
      <link>https://arxiv.org/abs/2412.03224</link>
      <description>arXiv:2412.03224v1 Announce Type: new 
Abstract: A brain-computer interface (BCI) enables direct communication between the human brain and external devices. Electroencephalography (EEG) based BCIs are currently the most popular for able-bodied users. To increase user-friendliness, usually a small amount of user-specific EEG data are used for calibration, which may not be enough to develop a pure data-driven decoding model. To cope with this typical calibration data shortage challenge in EEG-based BCIs, this paper proposes a parameter-free channel reflection (CR) data augmentation approach that incorporates prior knowledge on the channel distributions of different BCI paradigms in data augmentation. Experiments on eight public EEG datasets across four different BCI paradigms (motor imagery, steady-state visual evoked potential, P300, and seizure classifications) using different decoding algorithms demonstrated that: 1) CR is effective, i.e., it can noticeably improve the classification accuracy; 2) CR is robust, i.e., it consistently outperforms existing data augmentation approaches in the literature; and, 3) CR is flexible, i.e., it can be combined with other data augmentation approaches to further increase the performance. We suggest that data augmentation approaches like CR should be an essential step in EEG-based BCIs. Our code is available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03224v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.neunet.2024.106351</arxiv:DOI>
      <arxiv:journal_reference>Neural Networks, 176:106351, 2024</arxiv:journal_reference>
      <dc:creator>Ziwei Wang, Siyang Li, Jingwei Luo, Jiajing Liu, Dongrui Wu</dc:creator>
    </item>
    <item>
      <title>SPICE: Smart Projection Interface for Cooking Enhancement</title>
      <link>https://arxiv.org/abs/2412.03551</link>
      <description>arXiv:2412.03551v1 Announce Type: new 
Abstract: Tangible User Interfaces (TUI) for human--computer interaction (HCI) provide the user with physical representations of digital information with the aim to overcome the limitations of screen-based interfaces. Although many compelling demonstrations of TUIs exist in the literature, there is a lack of research on TUIs intended for daily two-handed tasks and processes, such as cooking. In response to this gap, we propose SPICE (Smart Projection Interface for Cooking Enhancement). SPICE investigates TUIs in a kitchen setting, aiming to transform the recipe following experience from simply text-based to tangibly interactive. SPICE includes a tracking system, an agent-based software, and vision large language models to create and interpret a kitchen environment where recipe information is projected directly onto the cooking surface. We conducted a comparative usability study of SPICE and text-based recipe following with 30 participants, assessing the task difficulty, total duration, and efficiency, as well as user confidence and taste perception. The results indicate that SPICE allowed participants to perform the recipe with less stops and in shorter time while also improving self-reported efficiency, confidence, and taste. Despite this, participants self-reported no change in overall difficulty, which is a direction for future research. Overall, the SPICE project demonstrates the potential of using TUIs to improve everyday activities, paving the way for future research in HCI and new computing interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03551v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <category>cs.MM</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Vera Prohaska, Eduardo Castell\'o Ferrer</dc:creator>
    </item>
    <item>
      <title>emg2pose: A Large and Diverse Benchmark for Surface Electromyographic Hand Pose Estimation</title>
      <link>https://arxiv.org/abs/2412.02725</link>
      <description>arXiv:2412.02725v1 Announce Type: cross 
Abstract: Hands are the primary means through which humans interact with the world. Reliable and always-available hand pose inference could yield new and intuitive control schemes for human-computer interactions, particularly in virtual and augmented reality. Computer vision is effective but requires one or multiple cameras and can struggle with occlusions, limited field of view, and poor lighting. Wearable wrist-based surface electromyography (sEMG) presents a promising alternative as an always-available modality sensing muscle activities that drive hand motion. However, sEMG signals are strongly dependent on user anatomy and sensor placement, and existing sEMG models have required hundreds of users and device placements to effectively generalize. To facilitate progress on sEMG pose inference, we introduce the emg2pose benchmark, the largest publicly available dataset of high-quality hand pose labels and wrist sEMG recordings. emg2pose contains 2kHz, 16 channel sEMG and pose labels from a 26-camera motion capture rig for 193 users, 370 hours, and 29 stages with diverse gestures - a scale comparable to vision-based hand pose datasets. We provide competitive baselines and challenging tasks evaluating real-world generalization scenarios: held-out users, sensor placements, and stages. emg2pose provides the machine learning community a platform for exploring complex generalization problems, holding potential to significantly enhance the development of sEMG-based human-computer interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02725v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sasha Salter, Richard Warren, Collin Schlager, Adrian Spurr, Shangchen Han, Rohin Bhasin, Yujun Cai, Peter Walkington, Anuoluwapo Bolarinwa, Robert Wang, Nathan Danielson, Josh Merel, Eftychios Pnevmatikakis, Jesse Marshall</dc:creator>
    </item>
    <item>
      <title>Grand Challenges on Immersive Technologies for Cultural Heritage</title>
      <link>https://arxiv.org/abs/2412.02853</link>
      <description>arXiv:2412.02853v1 Announce Type: cross 
Abstract: Cultural heritage, a testament to human history and civilization, has gained increasing recognition for its significance in preservation and dissemination. The integration of immersive technologies has transformed how cultural heritage is presented, enabling audiences to engage with it in more vivid, intuitive, and interactive ways. However, the adoption of these technologies also brings a range of challenges and potential risks. This paper presents a systematic review, with an in-depth analysis of 177 selected papers. We comprehensively examine and categorize current applications, technological approaches, and user devices in immersive cultural heritage presentations, while also highlighting the associated risks and challenges. Furthermore, we identify areas for future research in the immersive presentation of cultural heritage. Our goal is to provide a comprehensive reference for researchers and practitioners, enhancing understanding of the technological applications, risks, and challenges in this field, and encouraging further innovation and development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02853v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanbing Wang, Junyan Du, Yue Li, Lie Zhang, Xiang Li</dc:creator>
    </item>
    <item>
      <title>CLAS: A Machine Learning Enhanced Framework for Exploring Large 3D Design Datasets</title>
      <link>https://arxiv.org/abs/2412.02996</link>
      <description>arXiv:2412.02996v1 Announce Type: cross 
Abstract: Three-dimensional (3D) objects have wide applications. Despite the growing interest in 3D modeling in academia and industries, designing and/or creating 3D objects from scratch remains time-consuming and challenging. With the development of generative artificial intelligence (AI), designers discover a new way to create images for ideation. However, generative AIs are less useful in creating 3D objects with satisfying qualities. To allow 3D designers to access a wide range of 3D objects for creative activities based on their specific demands, we propose a machine learning (ML) enhanced framework CLAS - named after the four-step of capture, label, associate, and search - to enable fully automatic retrieval of 3D objects based on user specifications leveraging the existing datasets of 3D objects. CLAS provides an effective and efficient method for any person or organization to benefit from their existing but not utilized 3D datasets. In addition, CLAS may also be used to produce high-quality 3D object synthesis datasets for training and evaluating 3D generative models. As a proof of concept, we created and showcased a search system with a web user interface (UI) for retrieving 6,778 3D objects of chairs in the ShapeNet dataset powered by CLAS. In a close-set retrieval setting, our retrieval method achieves a mean reciprocal rank (MRR) of 0.58, top 1 accuracy of 42.27%, and top 10 accuracy of 89.64%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02996v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>XiuYu Zhang, Xiaolei Ye, Jui-Che Chang, Yue Fang</dc:creator>
    </item>
    <item>
      <title>Collecting Qualitative Data at Scale with Large Language Models: A Case Study</title>
      <link>https://arxiv.org/abs/2309.10187</link>
      <description>arXiv:2309.10187v3 Announce Type: replace 
Abstract: Chatbots have shown promise as tools to scale qualitative data collection. Recent advances in Large Language Models (LLMs) could accelerate this process by allowing researchers to easily deploy sophisticated interviewing chatbots. We test this assumption by conducting a large-scale user study (n=399) evaluating 3 different chatbots, two of which are LLM-based and a baseline which employs hard-coded questions. We evaluate the results with respect to participant engagement and experience, established metrics of chatbot quality grounded in theories of effective communication, and a novel scale evaluating "richness" or the extent to which responses capture the complexity and specificity of the social context under study. We find that, while the chatbots were able to elicit high-quality responses based on established evaluation metrics, the responses rarely capture participants' specific motives or personalized examples, and thus perform poorly with respect to richness. We further find low inter-rater reliability between LLMs and humans in the assessment of both quality and richness metrics. Our study offers a cautionary tale for scaling and evaluating qualitative research with LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10187v3</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Cuevas, Jennifer V. Scurrell, Eva M. Brown, Jason Entenmann, Madeleine I. G. Daepp</dc:creator>
    </item>
    <item>
      <title>Internet of Mirrors for Connected Healthcare and Beauty: A Prospective Vision</title>
      <link>https://arxiv.org/abs/2311.14734</link>
      <description>arXiv:2311.14734v2 Announce Type: replace 
Abstract: With the shift towards smart objects and automated services in many industries, the health and beauty industries are also becoming increasingly involved in AI-driven smart systems. There is a rising market demand for personalised services and a need for unified platforms in many sectors, specifically the cosmetics and healthcare industries. Alongside this rising demand, there are two major gaps when considering the integration of autonomous systems within these sectors. Firstly, the existing smart systems in the cosmetics industry are limited to single-purpose products and the employed technologies are not widespread enough to support the growing consumer demand for personalisation. Secondly, despite the rise of smart devices in healthcare, the current state-of-the-art services do not fulfil the accessibility demands and holistic nature of healthcare. To bridge these gaps, we propose integrating autonomous systems with health and beauty services through a unified visual platform coined as the Internet-of-Mirrors (IoM), an interconnected system of smart mirrors with sensing and communication capabilities where the smart mirror functions as an immersive visual dashboard to provide personalised services for health and beauty consultations and routines. We aim to present an overview of current state-of-the-art technologies that will enable the development of the IoM as well as provide a practical vision of this system with innovative scenarios to give a forward-looking vision for assistive technologies. We also discuss the missing capabilities and challenges the development of the IoM would face and outline future research directions that will support the realisation of our proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.14734v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.iot.2024.101415</arxiv:DOI>
      <arxiv:journal_reference>Internet of Things, Volume 28, 2024, 101415, ISSN 2542-6605,</arxiv:journal_reference>
      <dc:creator>Haneen Fatima, Muhammad Ali Imran, Ahmad Taha, Lina Mohjazi</dc:creator>
    </item>
    <item>
      <title>Exploring Communication Dynamics: Eye-tracking Analysis in Pair Programming of Computer Science Education</title>
      <link>https://arxiv.org/abs/2403.19560</link>
      <description>arXiv:2403.19560v3 Announce Type: replace 
Abstract: Pair programming is widely recognized as an effective educational tool in computer science that promotes collaborative learning and mirrors real-world work dynamics. However, communication breakdowns within pairs significantly challenge this learning process. In this study, we use eye-tracking data recorded during pair programming sessions to study communication dynamics between various pair programming roles across different student, expert, and mixed group cohorts containing 19 participants. By combining eye-tracking data analysis with focus group interviews and questionnaires, we provide insights into communication's multifaceted nature in pair programming. Our findings highlight distinct eye-tracking patterns indicating changes in communication skills across group compositions, with participants prioritizing code exploration over communication, especially during challenging tasks. Further, students showed a preference for pairing with experts, emphasizing the importance of understanding group formation in pair programming scenarios. These insights emphasize the importance of understanding group dynamics and enhancing communication skills through pair programming for successful outcomes in computer science education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19560v3</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3649902.3653942</arxiv:DOI>
      <dc:creator>Wunmin Jang, Hong Gao, Tilman Michaeli, Enkelejda Kasneci</dc:creator>
    </item>
    <item>
      <title>User Experience Evaluation of AR Assisted Industrial Maintenance and Support Applications</title>
      <link>https://arxiv.org/abs/2410.17348</link>
      <description>arXiv:2410.17348v2 Announce Type: replace 
Abstract: The paper introduces an innovative approach to industrial maintenance leveraging augmented reality (AR) technology, focusing on enhancing the user experience and efficiency. The shift from traditional to proactive maintenance strategies underscores the significance of maintenance in industrial systems. The proposed solution integrates AR interfaces, particularly through Head-Mounted Display (HMD) devices, to provide expert personnel-aided decision support for maintenance technicians, with the association of Artificial Intelligence (AI) solutions. The study explores the user experience aspect of AR interfaces in a simulated industrial environment, aiming to improve the maintenance processes' intuitiveness and effectiveness. Evaluation metrics such as the NASA Task Load Index (NASA-TLX) and the System Usability Scale (SUS) are employed to assess the usability, performance, and workload implications of the AR maintenance system. Additionally, the paper discusses the technical implementation, methodology, and results of experiments conducted to evaluate the effectiveness of the proposed solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17348v2</guid>
      <category>cs.HC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akos Nagy, Yannis Spyridis, Gregory J Mills, Vasileios Argyriou</dc:creator>
    </item>
    <item>
      <title>Exploiting the Uncoordinated Privacy Protections of Eye Tracking and VR Motion Data for Unauthorized User Identification</title>
      <link>https://arxiv.org/abs/2411.12766</link>
      <description>arXiv:2411.12766v2 Announce Type: replace 
Abstract: Virtual reality (VR) devices use a variety of sensors to capture a rich body of user-generated data. This data can be misused by malicious parties to covertly infer information about the user. Privacy-enhancing techniques that seek to reduce the amount of personally identifying information in sensor data are typically developed for a subset of data streams that are available on the platform, without consideration for the auxiliary information that may be readily available from other sensors. In this paper, we evaluate whether body motion data can be used to circumvent the privacy protections applied to eye tracking data to enable user identification on a VR platform, and vice versa. We empirically show that eye tracking, headset tracking, and hand tracking data are not only informative for inferring user identity on their own, but contain complementary information that can increase the rate of successful user identification. Most importantly, we demonstrate that applying privacy protections to only a subset of the data available in VR can create an opportunity for an adversary to bypass those privacy protections by using other unprotected data streams that are available on the platform, performing a user identification attack as accurately as though a privacy mechanism was never applied. These results highlight a new privacy consideration at the intersection between eye tracking and VR, and emphasizes the need for privacy-enhancing techniques that address multiple technologies comprehensively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12766v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samantha Aziz, Oleg Komogortsev</dc:creator>
    </item>
    <item>
      <title>Towards Understanding the Impact of Guidance in Data Visualization Systems for Domain Experts</title>
      <link>https://arxiv.org/abs/2412.01024</link>
      <description>arXiv:2412.01024v3 Announce Type: replace 
Abstract: Guided data visualization systems are highly useful for domain experts to highlight important trends in their large-scale and complex datasets. However, more work is needed to understand the impact of guidance on interpreting data visualizations as well as on the resulting use of visualizations when communicating insights. We conducted two user studies with domain experts and found that experts benefit from a guided coarse-to-fine structure when using data visualization systems, as this is the same structure in which they communicate findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01024v3</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sherry Qiu, Holly Rushmeier, Kim R. M. Blenman</dc:creator>
    </item>
    <item>
      <title>OpenDriver: An Open-Road Driver State Detection Dataset</title>
      <link>https://arxiv.org/abs/2304.04203</link>
      <description>arXiv:2304.04203v3 Announce Type: replace-cross 
Abstract: Among numerous studies for driver state detection, wearable physiological measurements offer a practical method for real-time monitoring. However, there are few driver physiological datasets in open-road scenarios, and the existing datasets suffer from issues such as poor signal quality, small sample sizes, and short data collection periods. Therefore, in this paper, a large-scale multimodal driving dataset, OpenDriver, for driver state detection is developed. The OpenDriver encompasses a total of 3,278 driving trips, with a signal collection duration spanning approximately 4,600 hours. Two modalities of driving signals are enrolled in OpenDriver: electrocardiogram (ECG) signals and six-axis motion data of the steering wheel from a motion measurement unit (IMU), which were recorded from 81 drivers and their vehicles. Furthermore, three challenging tasks are involved in our work, namely ECG signal quality assessment, individual biometric identification based on ECG signals, and physiological signal analysis in complex driving environments. To facilitate research in these tasks, corresponding benchmarks have also been introduced. First, a noisy augmentation strategy is applied to generate a larger-scale ECG signal dataset with realistic noise simulation for quality assessment. Second, an end-to-end contrastive learning framework is employed for individual biometric identification. Finally, a comprehensive analysis of drivers' HRV features under different driving conditions is conducted. Each benchmark provides evaluation metrics and reference results. The OpenDriver dataset will be publicly available at https://github.com/bdne/OpenDriver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.04203v3</guid>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Delong Liu, Shichao Li, Tianyi Shi, Zhu Meng, Guanyu Chen, Yadong Huang, Jin Dong, Zhicheng Zhao</dc:creator>
    </item>
    <item>
      <title>Prediction-Powered Ranking of Large Language Models</title>
      <link>https://arxiv.org/abs/2402.17826</link>
      <description>arXiv:2402.17826v3 Announce Type: replace-cross 
Abstract: Large language models are often ranked according to their level of alignment with human preferences -- a model is better than other models if its outputs are more frequently preferred by humans. One of the popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs. However, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a common practice to gather pairwise comparisons by a strong large language model -- a model strongly aligned with human preferences. Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings. In this work, we develop a statistical framework to bridge this gap. Given a (small) set of pairwise comparisons by humans and a large set of pairwise comparisons by a model, our framework provides a rank-set -- a set of possible ranking positions -- for each of the models under comparison. Moreover, it guarantees that, with a probability greater than or equal to a user-specified value, the rank-sets cover the true ranking consistent with the distribution of human pairwise preferences asymptotically. Using pairwise comparisons made by humans in the LMSYS Chatbot Arena platform and pairwise comparisons made by three strong large language models, we empirically demonstrate the effectivity of our framework and show that the rank-sets constructed using only pairwise comparisons by the strong large language models are often inconsistent with (the distribution of) human pairwise preferences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17826v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivi Chatzi, Eleni Straitouri, Suhas Thejaswi, Manuel Gomez Rodriguez</dc:creator>
    </item>
    <item>
      <title>Controlling Counterfactual Harm in Decision Support Systems Based on Prediction Sets</title>
      <link>https://arxiv.org/abs/2406.06671</link>
      <description>arXiv:2406.06671v2 Announce Type: replace-cross 
Abstract: Decision support systems based on prediction sets help humans solve multiclass classification tasks by narrowing down the set of potential label values to a subset of them, namely a prediction set, and asking them to always predict label values from the prediction sets. While this type of systems have been proven to be effective at improving the average accuracy of the predictions made by humans, by restricting human agency, they may cause harm$\unicode{x2014}$a human who has succeeded at predicting the ground-truth label of an instance on their own may have failed had they used these systems. In this paper, our goal is to control how frequently a decision support system based on prediction sets may cause harm, by design. To this end, we start by characterizing the above notion of harm using the theoretical framework of structural causal models. Then, we show that, under a natural, albeit unverifiable, monotonicity assumption, we can estimate how frequently a system may cause harm using only predictions made by humans on their own. Further, we also show that, under a weaker monotonicity assumption, which can be verified experimentally, we can bound how frequently a system may cause harm again using only predictions made by humans on their own. Building upon these assumptions, we introduce a computational framework to design decision support systems based on prediction sets that are guaranteed to cause harm less frequently than a user-specified value using conformal risk control. We validate our framework using real human predictions from two different human subject studies and show that, in decision support systems based on prediction sets, there is a trade-off between accuracy and counterfactual harm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06671v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eleni Straitouri, Suhas Thejaswi, Manuel Gomez Rodriguez</dc:creator>
    </item>
    <item>
      <title>Knowledge Mechanisms in Large Language Models: A Survey and Perspective</title>
      <link>https://arxiv.org/abs/2407.15017</link>
      <description>arXiv:2407.15017v4 Announce Type: replace-cross 
Abstract: Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial for advancing towards trustworthy AGI. This paper reviews knowledge mechanism analysis from a novel taxonomy including knowledge utilization and evolution. Knowledge utilization delves into the mechanism of memorization, comprehension and application, and creation. Knowledge evolution focuses on the dynamic progression of knowledge within individual and group LLMs. Moreover, we discuss what knowledge LLMs have learned, the reasons for the fragility of parametric knowledge, and the potential dark knowledge (hypothesis) that will be challenging to address. We hope this work can help understand knowledge in LLMs and provide insights for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15017v4</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengru Wang, Yunzhi Yao, Ziwen Xu, Shuofei Qiao, Shumin Deng, Peng Wang, Xiang Chen, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang</dc:creator>
    </item>
    <item>
      <title>DaVE -- A Curated Database of Visualization Examples</title>
      <link>https://arxiv.org/abs/2408.03188</link>
      <description>arXiv:2408.03188v2 Announce Type: replace-cross 
Abstract: Visualization, from simple line plots to complex high-dimensional visual analysis systems, has established itself throughout numerous domains to explore, analyze, and evaluate data. Applying such visualizations in the context of simulation science where High-Performance Computing (HPC) produces ever-growing amounts of data that is more complex, potentially multidimensional, and multimodal, takes up resources and a high level of technological experience often not available to domain experts. In this work, we present DaVE -- a curated database of visualization examples, which aims to provide state-of-the-art and advanced visualization methods that arise in the context of HPC applications. Based on domain- or data-specific descriptors entered by the user, DaVE provides a list of appropriate visualization techniques, each accompanied by descriptions, examples, references, and resources. Sample code, adaptable container templates, and recipes for easy integration in HPC applications can be downloaded for easy access to high-fidelity visualizations. While the database is currently filled with a limited number of entries based on a broad evaluation of needs and challenges of current HPC users, DaVE is designed to be easily extended by experts from both the visualization and HPC communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03188v2</guid>
      <category>cs.DC</category>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/VIS55277.2024.00010</arxiv:DOI>
      <dc:creator>Jens Koenen, Marvin Petersen, Christoph Garth, Tim Gerrits</dc:creator>
    </item>
    <item>
      <title>EgoPressure: A Dataset for Hand Pressure and Pose Estimation in Egocentric Vision</title>
      <link>https://arxiv.org/abs/2409.02224</link>
      <description>arXiv:2409.02224v2 Announce Type: replace-cross 
Abstract: Touch contact and pressure are essential for understanding how humans interact with and manipulate objects, insights which can significantly benefit applications in mixed reality and robotics. However, estimating these interactions from an egocentric camera perspective is challenging, largely due to the lack of comprehensive datasets that provide both accurate hand poses on contacting surfaces and detailed annotations of pressure information. In this paper, we introduce EgoPressure, a novel egocentric dataset that captures detailed touch contact and pressure interactions. EgoPressure provides high-resolution pressure intensity annotations for each contact point and includes accurate hand pose meshes obtained through our proposed multi-view, sequence-based optimization method processing data from an 8-camera capture rig. Our dataset comprises 5 hours of recorded interactions from 21 participants captured simultaneously by one head-mounted and seven stationary Kinect cameras, which acquire RGB images and depth maps at 30 Hz. To support future research and benchmarking, we present several baseline models for estimating applied pressure on external surfaces from RGB images, with and without hand pose information. We further explore the joint estimation of the hand mesh and applied pressure. Our experiments demonstrate that pressure and hand pose are complementary for understanding hand-object interactions. ng of hand-object interactions in AR/VR and robotics research. Project page: \url{https://yiming-zhao.github.io/EgoPressure/}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02224v2</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiming Zhao, Taein Kwon, Paul Streli, Marc Pollefeys, Christian Holz</dc:creator>
    </item>
    <item>
      <title>Mediating Modes of Thought: LLM's for design scripting</title>
      <link>https://arxiv.org/abs/2411.14485</link>
      <description>arXiv:2411.14485v2 Announce Type: replace-cross 
Abstract: Architects adopt visual scripting and parametric design tools to explore more expansive design spaces (Coates, 2010), refine their thinking about the geometric logic of their design (Woodbury, 2010), and overcome conventional software limitations (Burry, 2011). Despite two decades of effort to make design scripting more accessible, a disconnect between a designer's free ways of thinking and the rigidity of algorithms remains (Burry, 2011). Recent developments in Large Language Models (LLMs) suggest this might soon change, as LLMs encode a general understanding of human context and exhibit the capacity to produce geometric logic. This project speculates that if LLMs can effectively mediate between user intent and algorithms, they become a powerful tool to make scripting in design more widespread and fun. We explore if such systems can interpret natural language prompts to assemble geometric operations relevant to computational design scripting. In the system, multiple layers of LLM agents are configured with specific context to infer the user intent and construct a sequential logic. Given a user's high-level text prompt, a geometric description is created, distilled into a sequence of logic operations, and mapped to software-specific commands. The completed script is constructed in the user's visual programming interface. The system succeeds in generating complete visual scripts up to a certain complexity but fails beyond this complexity threshold. It shows how LLMs can make design scripting much more aligned with human creativity and thought. Future research should explore conversational interactions, expand to multimodal inputs and outputs, and assess the performance of these tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14485v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moritz Rietschel, Fang Guo, Kyle Steinfeld</dc:creator>
    </item>
  </channel>
</rss>
