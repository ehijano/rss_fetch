<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Jan 2025 03:45:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Understanding the Hamiltonian Monte Carlo through its Physics Fundamentals and Examples</title>
      <link>https://arxiv.org/abs/2501.13932</link>
      <description>arXiv:2501.13932v1 Announce Type: new 
Abstract: The Hamiltonian Monte Carlo (HMC) algorithm is a powerful Markov Chain Monte Carlo (MCMC) method that uses Hamiltonian dynamics to generate samples from a target distribution. To fully exploit its potential, we must understand how Hamiltonian dynamics work and why they can be used in a MCMC algorithm. This work elucidates the Monte Carlo Hamiltonian, providing comprehensive explanations of the underlying physical concepts. It is intended for readers with a solid foundation in mathematics who may lack familiarity with specific physical concepts, such as those related to Hamiltonian dynamics. Additionally, we provide Python code for the HMC algorithm, examples and comparisons with the Random Walk Metropolis-Hastings (RWMH) and t-walk algorithms to highlight HMC's strengths and weaknesses when applied to Bayesian Inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13932v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abraham Granados, Isa\'ias Ba\~nales</dc:creator>
    </item>
    <item>
      <title>Normalization and selecting non-differentially expressed genes improve machine learning modelling of cross-platform transcriptomic data</title>
      <link>https://arxiv.org/abs/2501.14248</link>
      <description>arXiv:2501.14248v1 Announce Type: cross 
Abstract: Normalization is a critical step in quantitative analyses of biological processes. Recent works show that cross-platform integration and normalization enable machine learning (ML) training on RNA microarray and RNA-seq data, but no independent datasets were used in their studies. Therefore, it is unclear how to improve ML modelling performance on independent RNA array and RNA-seq based datasets. Inspired by the house-keeping genes that are commonly used in experimental biology, this study tests the hypothesis that non-differentially expressed genes (NDEG) may improve normalization of transcriptomic data and subsequently cross-platform modelling performance of ML models. Microarray and RNA-seq datasets of the TCGA breast cancer were used as independent training and test datasets, respectively, to classify the molecular subtypes of breast cancer. NDEG (p&gt;0.85) and differentially expressed genes (DEG, p&lt;0.05) were selected based on the p values of ANOVA analysis and used for subsequent data normalization and classification, respectively. Models trained based on data from one platform were used for testing on the other platform. Our data show that NDEG and DEG gene selection could effectively improve the model classification performance. Normalization methods based on parametric statistical analysis were inferior to those based on nonparametric statistics. In this study, the LOG_QN and LOG_QNZ normalization methods combined with the neural network classification model seem to achieve better performance. Therefore, NDEG-based normalization appears useful for cross-platform testing on completely independent datasets. However, more studies are required to examine whether NDEG-based normalization can improve ML classification performance in other datasets and other omic data types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14248v1</guid>
      <category>q-bio.QM</category>
      <category>q-bio.GN</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fei Deng (Department of Chemical Biology, Ernest Mario School of Pharmacy, Rutgers University, Piscataway, NJ), Catherine H Feng (Department of Chemical Biology, Ernest Mario School of Pharmacy, Rutgers University, Piscataway, NJ, Harvard University, Cambridge, MA), Nan Gao (Department of Biological Sciences, School of Arts &amp; Sciences, Rutgers University, Newark, NJ, Department of Pharmacology, Physiology, and Neuroscience, New Jersey Medical School, Rutgers University, Newark, NJ), Lanjing Zhang (Department of Chemical Biology, Ernest Mario School of Pharmacy, Rutgers University, Piscataway, NJ, Rutgers Cancer Institute of New Jersey, New Brunswick, NJ)</dc:creator>
    </item>
    <item>
      <title>coverforest: Conformal Predictions with Random Forest in Python</title>
      <link>https://arxiv.org/abs/2501.14570</link>
      <description>arXiv:2501.14570v1 Announce Type: cross 
Abstract: Conformal prediction provides a framework for uncertainty quantification, specifically in the forms of prediction intervals and sets with distribution-free guaranteed coverage. While recent cross-conformal techniques such as CV+ and Jackknife+-after-bootstrap achieve better data efficiency than traditional split conformal methods, they incur substantial computational costs due to required pairwise comparisons between training and test samples' out-of-bag scores. Observing that these methods naturally extend from ensemble models, particularly random forests, we leverage existing optimized random forest implementations to enable efficient cross-conformal predictions.
  We present coverforest, a Python package that implements efficient conformal prediction methods specifically optimized for random forests. coverforest supports both regression and classification tasks through various conformal prediction methods, including split conformal, CV+, Jackknife+-after-bootstrap, and adaptive prediction sets. Our package leverages parallel computing and Cython optimizations to speed up out-of-bag calculations. Our experiments demonstrate that coverforest's predictions achieve the desired level of coverage. In addition, its training and prediction times can be faster than an existing implementation by 2--9 times. The source code for the coverforest is hosted on GitHub at https://github.com/donlapark/coverforest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14570v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Panisara Meehinkong, Donlapark Ponnoprat</dc:creator>
    </item>
    <item>
      <title>Concentration of discrepancy-based approximate Bayesian computation via Rademacher complexity</title>
      <link>https://arxiv.org/abs/2206.06991</link>
      <description>arXiv:2206.06991v5 Announce Type: replace-cross 
Abstract: There has been increasing interest on summary-free solutions for approximate Bayesian computation (ABC) which replace distances among summaries with discrepancies between the empirical distributions of the observed data and the synthetic samples generated under the proposed parameter values. The success of these strategies has motivated theoretical studies on the limiting properties of the induced posteriors. However, there is still the lack of a theoretical framework for summary-free ABC that (i) is unified, instead of discrepancy-specific, (ii) does not require to constrain the analysis to data generating processes and statistical models meeting specific regularity conditions, but rather facilitates the derivation of limiting properties that hold uniformly, and (iii) relies on verifiable assumptions that provide explicit concentration bounds clarifying which factors govern the limiting behavior of the ABC posterior. We address this gap via a novel theoretical framework that introduces the concept of Rademacher complexity in the analysis of the limiting properties for discrepancy-based ABC posteriors, including in non-i.i.d. and misspecified settings. This yields a unified theory that relies on constructive arguments and provides more informative asymptotic results and uniform concentration bounds, even in settings not covered by current studies. These advancements are obtained by relating the asymptotic properties of summary-free ABC posteriors to the behavior of the Rademacher complexity associated with the chosen discrepancy in the family of integral probability semimetrics (IPS). The IPS class extends summary-based distances, and includes the Wasserstein distance and maximum mean discrepancy, among others. As clarified in specialized theoretical analyses of popular IPS discrepancies and via illustrative simulations, this perspective improves the understanding of summary-free ABC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.06991v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sirio Legramanti, Daniele Durante, Pierre Alquier</dc:creator>
    </item>
    <item>
      <title>Stratified distance space improves the efficiency of sequential samplers for approximate Bayesian computation</title>
      <link>https://arxiv.org/abs/2401.00324</link>
      <description>arXiv:2401.00324v2 Announce Type: replace-cross 
Abstract: Approximate Bayesian computation (ABC) methods are standard tools for inferring parameters of complex models when the likelihood function is analytically intractable. A popular approach to improving the poor acceptance rate of the basic rejection sampling ABC algorithm is to use sequential Monte Carlo (ABC SMC) to produce a sequence of proposal distributions adapting towards the posterior, instead of generating values from the prior distribution of the model parameters. Proposal distribution for the subsequent iteration is typically obtained from a weighted set of samples, often called particles, of the current iteration of this sequence. Current methods for constructing these proposal distributions treat all the particles equivalently, regardless of the corresponding value generated by the sampler, which may lead to inefficiency when propagating the information across iterations of the algorithm. To improve sampler efficiency, we introduce a modified approach called stratified distance ABC SMC. Our algorithm stratifies particles based on their distance between the corresponding synthetic and observed data, and then constructs distinct proposal distributions for all the strata. Taking into account the distribution of distances across the particle space leads to substantially improved acceptance rate of the rejection sampling. We further show that efficiency can be gained by introducing a novel stopping rule for the sequential process based on the stratified posterior samples and demonstrate these advances by several examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00324v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henri Pesonen, Jukka Corander</dc:creator>
    </item>
  </channel>
</rss>
