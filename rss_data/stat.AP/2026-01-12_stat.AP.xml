<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 Jan 2026 03:30:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Representing asymmetric relationships by h-plots. Discovering the archetypal patterns of cross-journal citation relationships</title>
      <link>https://arxiv.org/abs/2601.05400</link>
      <description>arXiv:2601.05400v1 Announce Type: new 
Abstract: This work approaches the multidimensional scaling problem from a novel angle. We introduce a scalable method based on the h-plot, which inherently accommodates asymmetric proximity data. Instead of embedding the objects themselves, the method embeds the variables that define the proximity to or from each object. It is straightforward to implement, and the quality of the resulting representation can be easily evaluated. The methodology is illustrated by visualizing the asymmetric relationships between the citing and cited profiles of journals on a common map. Two profiles that are far apart (or close together) in the h-plot, as measured by Euclidean distance, are different (or similar), respectively. This representation allows archetypoid analysis (ADA) to be calculated. ADA is used to find archetypal journals (or extreme cases). We can represent the dataset as convex combinations of these archetypal journals, making the results easy to interpret, even for non-experts. Comparisons with other methodologies are carried out, showing the good performance of our proposal. Code and data are available for reproducibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05400v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aleix Alcacer, Irene Epifanio</dc:creator>
    </item>
    <item>
      <title>A latent factor approach to hyperspectral time series data for multivariate genomic prediction of grain yield in wheat</title>
      <link>https://arxiv.org/abs/2601.05842</link>
      <description>arXiv:2601.05842v1 Announce Type: new 
Abstract: High-dimensional time series phenotypic data is becoming increasingly common within plant breeding programmes. However, analysing and integrating such data for genetic analysis and genomic prediction remains difficult. Here we show how factor analysis with Procrustes rotation on the genetic correlation matrix of hyperspectral secondary phenotype data can help in extracting relevant features for within-trial prediction. We use a subset of Centro Internacional de Mejoramiento de Ma\'iz y Trigo (CIMMYT) elite yield wheat trial of 2014-2015, consisting of 1,033 genotypes. These were measured across three irrigation treatments at several timepoints during the season, using manned airplane flights with hyperspectral sensors capturing 62 bands in the spectrum of 385-850 nm. We perform multivariate genomic prediction using latent variables to improve within-trial genomic predictive ability (PA) of wheat grain yield within three distinct watering treatments. By integrating latent variables of the hyperspectral data in a multivariate genomic prediction model, we are able to achieve an absolute gain of .1 to .3 (on the correlation scale) in PA compared to univariate genomic prediction. Furthermore, we show which timepoints within a trial are important and how these relate to plant growth stages. This paper showcases how domain knowledge and data-driven approaches can be combined to increase PA and gain new insights from sensor data of high-throughput phenotyping platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05842v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jonathan F. Kunst, Killian A. C. Melsen, Willem Kruijer, Jos\'e Crossa, Chris Maliepaard, Fred A. van Eeuwijk, Carel F. W. Peeters</dc:creator>
    </item>
    <item>
      <title>Neural Methods for Multiple Systems Estimation Models</title>
      <link>https://arxiv.org/abs/2601.05859</link>
      <description>arXiv:2601.05859v1 Announce Type: new 
Abstract: Estimating the size of hidden populations using Multiple Systems Estimation (MSE) is a critical task in quantitative sociology; however, practical application is often hindered by imperfect administrative data and computational constraints. Real-world datasets frequently suffer from censoring and missingness due to privacy concerns, while standard inference methods, such as Maximum Likelihood Estimation (MLE) and Markov chain Monte Carlo (MCMC), can become computationally intractable or fail to converge when data are sparse. To address these limitations, we propose a novel simulation-based Bayesian inference framework utilizing Neural Bayes Estimators (NBE) and Neural Posterior Estimators (NPE). These neural methods are amortized: once trained, they provide instantaneous, computationally efficient posterior estimates, making them ideal for use in secure research environments where computational resources are limited. Through extensive simulation studies, we demonstrate that neural estimators achieve accuracy comparable to MCMC while being orders of magnitude faster and robust to the convergence failures that plague traditional samplers in sparse settings. We demonstrate our method on two real-world cases estimating the prevalence of modern slavery in the UK and female drug use in North East England.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05859v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Joseph Marsh, Nathan A. Judd, Lax Chan, Rowland G. Seymour</dc:creator>
    </item>
    <item>
      <title>Rotational Kinematics in the Globular Cluster System of M31: Insights from Bayesian Inference</title>
      <link>https://arxiv.org/abs/2601.05380</link>
      <description>arXiv:2601.05380v1 Announce Type: cross 
Abstract: As ancient stellar systems, globular clusters (GCs) offer valuable insights into the dynamical histories of large galaxies. Previous studies of GC populations in the inner and outer regions of the Andromeda Galaxy (M31) have revealed intriguing subpopulations with distinct kinematic properties. Here, we build upon earlier studies by employing Bayesian modelling to investigate the kinematics of the combined inner and outer GC populations of M31. Given the heterogeneous nature of the data, we examine subpopulations defined by GCs' metallicity and by associations with substructure, in order to characterise possible relationships between the inner and outer GC populations. We find that lower-metallicity GCs and those linked to substructures exhibit a common, more rapid rotation, whose alignment is distinct from that of higher-metallicity and non-substructure GCs. Furthermore, the higher-metallicity GCs rotate in alignment with Andromeda's stellar disk. These pronounced kinematic differences reinforce the idea that different subgroups of GCs were accreted to M31 at distinct epochs, shedding light on the complex assembly history of the galaxy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05380v1</guid>
      <category>astro-ph.GA</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator> Yuan (Cher),  Li, Brendon J. Brewer, Geraint F. Lewis, Dougal Mackey</dc:creator>
    </item>
    <item>
      <title>Uncertainty Analysis of Experimental Parameters for Reducing Warpage in Injection Molding</title>
      <link>https://arxiv.org/abs/2601.05396</link>
      <description>arXiv:2601.05396v1 Announce Type: cross 
Abstract: Injection molding is a critical manufacturing process, but controlling warpage remains a major challenge due to complex thermomechanical interactions. Simulation-based optimization is widely used to address this, yet traditional methods often overlook the uncertainty in model parameters. In this paper, we propose a data-driven framework to minimize warpage and quantify the uncertainty of optimal process settings. We employ polynomial regression models as surrogates for the injection molding simulations of a box-shaped part. By adopting a Bayesian framework, we estimate the posterior distribution of the regression coefficients. This approach allows us to generate a distribution of optimal decisions rather than a single point estimate, providing a measure of solution robustness. Furthermore, we develop a Monte Carlo-based boundary analysis method. This method constructs confidence bands for the zero-level sets of the response surfaces, helping to visualize the regions where warpage transitions between convex and concave profiles. We apply this framework to optimize four key process parameters: mold temperature, injection speed, packing pressure, and packing time. The results show that our approach finds stable process settings and clearly marks the boundaries of defects in the parameter space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05396v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yezhuo Li, Fan Zhang, Dhanashree Shinde, Qiong Zhang, Sai Pradeep, Srikanth Pilla, Gang Li</dc:creator>
    </item>
    <item>
      <title>Efficient Inference for Noisy LLM-as-a-Judge Evaluation</title>
      <link>https://arxiv.org/abs/2601.05420</link>
      <description>arXiv:2601.05420v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used as automatic evaluators of generative AI outputs, a paradigm often referred to as "LLM-as-a-judge." In practice, LLM judges are imperfect predictions for the underlying truth and can exhibit systematic, non-random errors. Two main approaches have recently been proposed to address this issue: (i) direct measurementerror correction based on misclassification models such as Rogan-Gladen-style estimators, and (ii) surrogate-outcome approaches such as prediction-powered inference (PPI), which correct bias by calibrating prediction residuals on a small set of gold-standard human labels. In this paper, we systematically study the performance of these two approaches for estimating mean parameters (e.g., average benchmark scores or pairwise win rates). Leveraging tools from semiparametric efficiency theory, we unify the two classes of estimators by deriving explicit forms of efficient influence function (EIF)-based efficient estimators and characterize conditions under which PPI-style estimators attain strictly smaller asymptotic variance than measurement-error corrections. We verify our theoretical results in simulations and demonstrate the methods on real-data examples. We provide an implementation of the benchmarked methods and comparison utilities at https://github.com/yiqunchen/debias-llm-as-a-judge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05420v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiqun T Chen, Sizhu Lu, Sijia Li, Moran Guo, Shengyi Li</dc:creator>
    </item>
    <item>
      <title>Buffered AUC maximization for scoring systems via mixed-integer optimization</title>
      <link>https://arxiv.org/abs/2601.05544</link>
      <description>arXiv:2601.05544v1 Announce Type: cross 
Abstract: A scoring system is a linear classifier composed of a small number of explanatory variables, each assigned a small integer coefficient. This system is highly interpretable and allows predictions to be made with simple manual calculations without the need for a calculator. Several previous studies have used mixed-integer optimization (MIO) techniques to develop scoring systems for binary classification; however, they have not focused on directly maximizing AUC (i.e., area under the receiver operating characteristic curve), even though AUC is recognized as an essential evaluation metric for scoring systems. Our goal herein is to establish an effective MIO framework for constructing scoring systems that directly maximize the buffered AUC (bAUC) as the tightest concave lower bound on AUC. Our optimization model is formulated as a mixed-integer linear optimization (MILO) problem that maximizes bAUC subject to a group sparsity constraint for limiting the number of questions in the scoring system. Computational experiments using publicly available real-world datasets demonstrate that our MILO method can build scoring systems with superior AUC values compared to the baseline methods based on regularization and stepwise regression. This research contributes to the advancement of MIO techniques for developing highly interpretable classification models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05544v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moe Shiina, Shunnosuke Ikeda, Yuichi Takano</dc:creator>
    </item>
    <item>
      <title>Multi-task Modeling for Engineering Applications with Sparse Data</title>
      <link>https://arxiv.org/abs/2601.05910</link>
      <description>arXiv:2601.05910v1 Announce Type: cross 
Abstract: Modern engineering and scientific workflows often require simultaneous predictions across related tasks and fidelity levels, where high-fidelity data is scarce and expensive, while low-fidelity data is more abundant. This paper introduces an Multi-Task Gaussian Processes (MTGP) framework tailored for engineering systems characterized by multi-source, multi-fidelity data, addressing challenges of data sparsity and varying task correlations. The proposed framework leverages inter-task relationships across outputs and fidelity levels to improve predictive performance and reduce computational costs. The framework is validated across three representative scenarios: Forrester function benchmark, 3D ellipsoidal void modeling, and friction-stir welding. By quantifying and leveraging inter-task relationships, the proposed MTGP framework offers a robust and scalable solution for predictive modeling in domains with significant computational and experimental costs, supporting informed decision-making and efficient resource utilization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05910v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yigitcan Comlek, R. Murali Krishnan, Sandipp Krishnan Ravi, Amin Moghaddas, Rafael Giorjao, Michael Eff, Anirban Samaddar, Nesar S. Ramachandra, Sandeep Madireddy, Liping Wang</dc:creator>
    </item>
    <item>
      <title>Detecting Stochasticity in Discrete Signals via Nonparametric Excursion Theorem</title>
      <link>https://arxiv.org/abs/2601.06009</link>
      <description>arXiv:2601.06009v1 Announce Type: cross 
Abstract: We develop a practical framework for distinguishing diffusive stochastic processes from deterministic signals using only a single discrete time series. Our approach is based on classical excursion and crossing theorems for continuous semimartingales, which correlates number $N_\varepsilon$ of excursions of magnitude at least $\varepsilon$ with the quadratic variation $[X]_T$ of the process. The scaling law holds universally for all continuous semimartingales with finite quadratic variation, including general Ito diffusions with nonlinear or state-dependent volatility, but fails sharply for deterministic systems -- thereby providing a theoretically-certfied method of distinguishing between these dynamics, as opposed to the subjective entropy or recurrence based state of the art methods. We construct a robust data-driven diffusion test. The method compares the empirical excursion counts against the theoretical expectation. The resulting ratio $K(\varepsilon)=N_{\varepsilon}^{\mathrm{emp}}/N_{\varepsilon}^{\mathrm{theory}}$ is then summarized by a log-log slope deviation measuring the $\varepsilon^{-2}$ law that provides a classification into diffusion-like or not. We demonstrate the method on canonical stochastic systems, some periodic and chaotic maps and systems with additive white noise, as well as the stochastic Duffing system. The approach is nonparametric, model-free, and relies only on the universal small-scale structure of continuous semimartingales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06009v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sunia Tanweer, Firas A. Khasawneh</dc:creator>
    </item>
    <item>
      <title>Cooperative Differential GNSS Positioning: Estimators and Bounds</title>
      <link>https://arxiv.org/abs/2601.06012</link>
      <description>arXiv:2601.06012v1 Announce Type: cross 
Abstract: In Differential GNSS (DGNSS) positioning, differencing measurements between a user and a reference station suppresses common-mode errors but also introduces reference-station noise, which fundamentally limits accuracy. This limitation is minor for high-grade stations but becomes significant when using reference infrastructure of mixed quality. This paper investigates how large-scale user cooperation can mitigate the impact of reference-station noise in conventional (non-cooperative) DGNSS systems. We develop a unified estimation framework for cooperative DGNSS (C-DGNSS) and cooperative real-time kinematic (C-RTK) positioning, and derive parameterized expressions for their Fisher information matrices as functions of network size, satellite geometry, and reference-station noise. This formulation enables theoretical analysis of estimation performance, identifying regimes where cooperation asymptotically restores the accuracy of DGNSS with an ideal (noise-free) reference. Simulations validate these theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06012v1</guid>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Helena Calatrava, Daniel Medina, Pau Closas</dc:creator>
    </item>
    <item>
      <title>Modelling between- and within-season trajectories in elite athletic performance data</title>
      <link>https://arxiv.org/abs/2405.17214</link>
      <description>arXiv:2405.17214v2 Announce Type: replace 
Abstract: Athletic performance follows a typical pattern of improvement and decline during a career. This pattern is also often observed within-seasons, as an athlete aims for their performance to peak at key events such as the Olympic Games or World Championships. A Bayesian hierarchical model is developed to analyse the evolution of athletic sporting performance throughout an athlete's career and separate these effects whilst allowing for confounding factors such as environmental conditions. Our model works in continuous time and estimates both $g(t)$, the average performance level of the population at age $t$, and $f_i(t)$, the difference of the $i$-th athlete from this average. We further decompose $f_i(t)$ into a season-to-season trajectory and a within-season trajectory, which is modelled by a restricted Bernstein polynomial. The model is fitted using an adaptive Metropolis-within-Gibbs algorithm with a carefully chosen blocking scheme. The model allows us to understand seasonal patterns in athlete performance, how these differ between athletes, and provides individual fitted and trend performance trajectories. The properties of the model are illustrated using a simulation study and an application to 100 metres and 200 metres freestyle swimming for both female and male athletes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17214v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>M. Spyropoulou, J. G. Hopker, J. E. Griffin</dc:creator>
    </item>
    <item>
      <title>Heterogeneous gene network estimation for single-cell transcriptomic data via a joint regularized deep neural network</title>
      <link>https://arxiv.org/abs/2503.06389</link>
      <description>arXiv:2503.06389v2 Announce Type: replace 
Abstract: Estimation of intracellular gene networks has been a critical component of single-cell transcriptomic data analysis, which can provide crucial insights into the complex interplay between genes, facilitating the discovery of the biological basis of human life at single-cell resolution. Despite notable achievements, existing methodologies often falter in their practicality, primarily due to their narrow focus on simplistic linear relationships and inadequate handling of cellular heterogeneity. To bridge these gaps, we propose a joint regularized deep neural network method incorporating Mahalanobis distance-based K-means clustering (JRDNN-KM) to estimate multiple networks for various cell subgroups simultaneously, accounting for both unknown cellular heterogeneity and zero inflation, and, more importantly, complex nonlinear relationships among genes. We introduce an innovative selection layer for network construction, along with hidden layers that include both shared and subgroup-specific neurons, to capture common patterns and subgroup-specific variations across networks. Applied to real single-cell transcriptomic data from multiple tissues and species, JRDNN-KM demonstrates higher accuracy and biological interpretability in network estimation, and more accurately identifies cell subgroups compared to current state-of-the-art methods.Building on network construction, we further find hub genes with important biological implications and modules with statistical enrichment of biological processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06389v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyuan Yang, Tao Li, Tianyi Wang, Shuangge Ma, Mengyun Wu</dc:creator>
    </item>
    <item>
      <title>Forecasting Extreme Day and Night Heat in Paris</title>
      <link>https://arxiv.org/abs/2508.12886</link>
      <description>arXiv:2508.12886v4 Announce Type: replace 
Abstract: As a form of ``small AI'', quantile statistical learning is used to forecast diurnal and nocturnal Q(.90) air temperatures for Paris, France from late spring to late summer months of 2020. The data are provided by the Paris-Montsouris weather station. Rather than trying to directly anticipate the onset and cessation of reported heat waves, Q(.90) values are estimated because the 90th percentile requires that the higher temperatures be relatively rare and extreme. Predictors include eight routinely available indicators of weather conditions, lagged by 14 days; the temperature forecasts are produced two weeks in advance. Conformal prediction regions capture forecasting uncertainty with provably valid properties. For both diurnal and nocturnal temperatures, forecasting accuracy is promising, and sound measures of uncertainty are provided. Benefits for policy and practice follow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12886v4</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Richard Berk</dc:creator>
    </item>
    <item>
      <title>Day-Ahead Electricity Price Forecasting Using Merit-Order Curves Time Series</title>
      <link>https://arxiv.org/abs/2512.17758</link>
      <description>arXiv:2512.17758v2 Announce Type: replace 
Abstract: We introduce a general, simple, and computationally efficient framework for predicting day-ahead supply and demand merit-order curves, from which both point and probabilistic electricity price forecasts can be derived. We conduct a rigorous empirical comparison of price forecasting performance between the proposed curve-based model, i.e., derived from predicted merit-order curves, and state-of-the-art price-based models that directly forecast the clearing price, using data from the Italian day-ahead market over the 2023-2024 period. Our results show that the proposed curve-based approach significantly improves both point and probabilistic price forecasting accuracy relative to price-based approaches, with average gains of approximately 5%, and improvements of up to 10% during mid-day hours, when prices occasionally drop due to high renewable generation and low demand.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17758v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guillaume Koechlin, Filippo Bovera, Piercesare Secchi</dc:creator>
    </item>
    <item>
      <title>A non-parametric approach for estimating consumer valuation distributions using second price auctions</title>
      <link>https://arxiv.org/abs/2312.07882</link>
      <description>arXiv:2312.07882v2 Announce Type: replace-cross 
Abstract: We focus on online second price auctions, where bids are made sequentially, and the winning bidder pays the maximum of the second-highest bid and a seller specified reserve price. For many such auctions, the seller does not see all the bids or the total number of bidders accessing the auction, and only observes the current selling prices throughout the course of the auction. We develop a novel non-parametric approach to estimate the underlying consumer valuation distribution based on this data. Previous non-parametric approaches in the literature only use the final selling price and assume knowledge of the total number of bidders. The resulting estimate, in particular, can be used by the seller to compute the optimal profit-maximizing price for the product. Our approach is free of tuning parameters, and we demonstrate its computational and statistical efficiency in a variety of simulation settings, and also on an Xbox 7-day auction dataset on eBay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07882v2</guid>
      <category>stat.ME</category>
      <category>cs.GT</category>
      <category>stat.AP</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourav Mukherjee, Ziqian Yang, Rohit K Patra, Kshitij Khare</dc:creator>
    </item>
    <item>
      <title>Multilayer networks characterize human-mobility patterns by industry sector for the 2021 Texas winter storm</title>
      <link>https://arxiv.org/abs/2509.03642</link>
      <description>arXiv:2509.03642v2 Announce Type: replace-cross 
Abstract: Understanding human mobility during disastrous events is crucial for emergency planning and disaster management. We develop a methodology to construct time-varying, multilayer networks where edges encode observed movements between spatial regions (census tracts) and network layers encode movement categories by industry sectors (e.g., schools, hospitals). Using the 2021 Texas winter storm as a case study, we find that people markedly reduced movements to ambulatory healthcare services, restaurants, and schools, but prioritized movements to grocery stores and gas stations. Additionally, we study the predictability of nodes' in- and out-degrees in the multilayer networks, which encode movements into and out of census tracts. Inward movements prove harder to predict than outward movements, especially during the storm. Our findings on the reduction, prioritization, and predictability of sector-specific movements aim to support mobility-related decisions during future extreme weather events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03642v2</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Melissa Butler, Alisha Khan, Francis Osei Tutu Afrifa, Yingjie Hu, Dane Taylor</dc:creator>
    </item>
    <item>
      <title>Data-Driven Approach to Capitation Reform in Rwanda</title>
      <link>https://arxiv.org/abs/2510.21851</link>
      <description>arXiv:2510.21851v2 Announce Type: replace-cross 
Abstract: As part of Rwanda's transition toward universal health coverage, the national Community-Based Health Insurance (CBHI) scheme is moving from retrospective fee-for-service reimbursements to prospective capitation payments for public primary healthcare providers. This work outlines a data-driven approach to designing, calibrating, and monitoring the capitation model using individual-level claims data from the Intelligent Health Benefits System (IHBS). We introduce a transparent, interpretable formula for allocating payments to Health Centers and their affiliated Health Posts. The formula is based on catchment population, service utilization patterns, and patient inflows, with parameters estimated via regression models calibrated on national claims data. Repeated validation exercises show the payment scheme closely aligns with historical spending while promoting fairness and adaptability across diverse facilities. In addition to payment design, the same dataset enables actionable behavioral insights. We highlight the use case of monitoring antibiotic prescribing patterns, particularly in pediatric care, to flag potential overuse and guideline deviations. Together, these capabilities lay the groundwork for a learning health financing system: one that connects digital infrastructure, resource allocation, and service quality to support continuous improvement and evidence-informed policy reform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21851v2</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Babaniyi Olaniyi, Ina Kalisa, Ana Fern\'andez del R\'io, Jean Marie Vianney Hakizayezu, Enric Jan\'e, Eniola Olaleye, Juan Francisco Garamendi, Ivan Nazarov, Aditya Rastogi, Mateo Diaz-Quiroz, \'Africa Peri\'a\~nez, Regis Hitimana</dc:creator>
    </item>
  </channel>
</rss>
