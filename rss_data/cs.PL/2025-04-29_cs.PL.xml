<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Apr 2025 01:48:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>An Interactive Debugger for Rust Trait Errors</title>
      <link>https://arxiv.org/abs/2504.18704</link>
      <description>arXiv:2504.18704v1 Announce Type: new 
Abstract: Compiler diagnostics for type inference failures are notoriously bad, and type classes only make the problem worse. By introducing a complex search process during inference, type classes can lead to wholly inscrutable or useless errors. We describe a system, Argus, for interactively visualizing type class inferences to help programmers debug inference failures, applied specifically to Rust's trait system. The core insight of Argus is to avoid the traditional model of compiler diagnostics as one-size-fits-all, instead providing the programmer with different views on the search tree corresponding to different debugging goals. Argus carefully uses defaults to improve debugging productivity, including interface design (e.g., not showing full paths of types by default) and heuristics (e.g., sorting obligations based on the expected complexity of fixing them). We evaluated Argus in a user study where $N = 25$ participants debugged type inference failures in realistic Rust programs, finding that participants using Argus correctly localized $2.2\times$ as many faults and localized $3.3\times$ faster compared to not using Argus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18704v1</guid>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3729302</arxiv:DOI>
      <dc:creator>Gavin Gray (Brown University), Will Crichton (Brown University), Shriram Krishnamurthi (Brown University)</dc:creator>
    </item>
    <item>
      <title>The Algebra of Patterns (Extended Version)</title>
      <link>https://arxiv.org/abs/2504.18920</link>
      <description>arXiv:2504.18920v1 Announce Type: new 
Abstract: Pattern matching is a popular feature in functional, imperative and object-oriented programming languages. Language designers should therefore invest effort in a good design for pattern matching. Most languages choose a first-match semantics for pattern matching; that is, clauses are tried in the order in which they appear in the program until the first one matches. As a consequence, the order in which the clauses appear cannot be arbitrarily changed, which results in a less declarative programming model. The declarative alternative to this is an order-independent semantics for pattern matching, which is not implemented in most programming languages since it requires more verbose patterns. The reason for this verbosity is that the syntax of patterns is usually not expressive enough to express the complement of a pattern. In this paper, we show a principled way to make order-independent pattern matching practical. Our solution consists of two parts: First, we introduce a boolean algebra of patterns which can express the complement of a pattern. Second, we introduce default clauses to pattern matches. These default clauses capture the essential idea of a fallthrough case without sacrificing the property of order-independence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18920v1</guid>
      <category>cs.PL</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Binder, Lean Ermantraut</dc:creator>
    </item>
    <item>
      <title>GPU accelerated program synthesis: Enumerate semantics, not syntax!</title>
      <link>https://arxiv.org/abs/2504.18943</link>
      <description>arXiv:2504.18943v1 Announce Type: new 
Abstract: Program synthesis is an umbrella term for generating programs and logical formulae from specifications. With the remarkable performance improvements that GPUs enable for deep learning, a natural question arose: can we also implement a search-based program synthesiser on GPUs to achieve similar performance improvements? In this article we discuss our insights on this question, based on recent works~. The goal is to build a synthesiser running on GPUs which takes as input positive and negative example traces and returns a logical formula accepting the positive and rejecting the negative traces. With GPU-friendly programming techniques -- using the semantics of formulae to minimise data movement and reduce data-dependent branching -- our synthesiser scales to significantly larger synthesis problems, and operates much faster than the previous CPU-based state-of-the-art. We believe the insights that make our approach GPU-friendly have wide potential for enhancing the performance of other formal methods (FM) workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18943v1</guid>
      <category>cs.PL</category>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Martin Berger, Nathana\"el Fijalkow, Mojtaba Valizadeh</dc:creator>
    </item>
    <item>
      <title>Automatic Goal Clone Detection in Rocq</title>
      <link>https://arxiv.org/abs/2504.19129</link>
      <description>arXiv:2504.19129v1 Announce Type: new 
Abstract: Proof engineering in Rocq is a labor-intensive process, and as proof developments grow in size, redundancy and maintainability become challenges. One such redundancy is goal cloning, i.e., proving {\alpha}-equivalent goals multiple times, leading to wasted effort and bloated proof scripts. In this paper, we introduce clone-finder, a novel technique for detecting goal clones in Rocq proofs. By leveraging the formal notion of {\alpha}-equivalence for Gallina terms, clone-finder systematically identifies duplicated proof goals across large Rocq codebases. We evaluate clone-finder on 40 real-world Rocq projects from the CoqGym dataset. Our results reveal that each project contains an average of 27.73 instances of goal clone. We observed that the clones can be categorized as either exact goal duplication, generalization, or {\alpha}-equivalent goals with different proofs, each signifying varying levels duplicate effort. Our findings highlight significant untapped potential for proof reuse in Rocq-based formal verification projects, paving the way for future improvements in automated proof engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19129v1</guid>
      <category>cs.PL</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.ECOOP.2025.23</arxiv:DOI>
      <dc:creator>Ali Ghanbari</dc:creator>
    </item>
    <item>
      <title>Rulebook: bringing co-routines to reinforcement learning environments</title>
      <link>https://arxiv.org/abs/2504.19625</link>
      <description>arXiv:2504.19625v1 Announce Type: new 
Abstract: Reinforcement learning (RL) algorithms, due to their reliance on external systems to learn from, require digital environments (e.g., simulators) with very simple interfaces, which in turn constrain significantly the implementation of such environments. In particular, these environments are implemented either as separate processes or as state machines, leading to synchronization and communication overheads in the first case, and to unstructured programming in the second.
  We propose a new domain-specific, co-routine-based, compiled language, called Rulebook, designed to automatically generate the state machine required to interact with machine learning (ML) algorithms and similar applications, with no performance overhead. Rulebook allows users to express programs without needing to be aware of the specific interface required by the ML components. By decoupling the execution model of the program from the syntactical encoding of the program, and thus without the need for manual state management, Rulebook allows to create larger and more sophisticated environments at a lower development cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19625v1</guid>
      <category>cs.PL</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Massimo Fioravanti, Samuele Pasini, Giovanni Agosta</dc:creator>
    </item>
    <item>
      <title>A Formal Framework for Naturally Specifying and Verifying Sequential Algorithms</title>
      <link>https://arxiv.org/abs/2504.19852</link>
      <description>arXiv:2504.19852v1 Announce Type: new 
Abstract: Current approaches for formal verification of algorithms face important limitations. For specification, they cannot express algorithms naturally and concisely, especially for algorithms with states and flexible control flow. For verification, formal proof based on Hoare logic cannot reflect the logical structure of natural proof. To address these challenges, we introduce a formal framework for naturally specifying and verifying sequential algorithms in Coq. We use the state relation monad to integrate Coq's expressive type system with the flexible control flow of imperative languages. It supports nondeterministic operations and customizable program states, enabling specifying algorithms at an appropriate level of abstraction. For verification, we build a Hoare logic for the monad and propose a novel two-stage proof approach that separates natural logical reasoning from mechanical composition. It reflects the logical structure of natural proof, enhancing modularity and readability. We evaluate the framework by formalizing the Depth-First Search (DFS) algorithm and verifying the Knuth-Morris-Pratt (KMP) algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19852v1</guid>
      <category>cs.PL</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengxi Yang, Shushu Wu, Qinxiang Cao</dc:creator>
    </item>
    <item>
      <title>Transparent Transformations for Timing Side-Channel Analysis</title>
      <link>https://arxiv.org/abs/2501.04183</link>
      <description>arXiv:2501.04183v2 Announce Type: replace 
Abstract: Side-channel analysis frameworks often lift programs into an intermediate representation (IR) before analyzing them. IRs are subject to transformations, which we call ex ante (XA) transformations, that improve the efficiency and accuracy of the analysis, thereby directly impacting the validity of analysis results. This paper explores the impact of XA transformations in the setting of timing-based side-channel analysis frameworks, focusing on the constant-time policy that many cryptographic libraries adopt.
  We identify two properties of XA transformations that directly impact the soundness and precision of side-channel analysis. The first property, preservation, requires that the transformation does not introduce leakage and is required to avoid false positives, i.e., for precision. The second one, reflection, requires that the transformation does not remove leakage and is required to avoid false negatives, i.e., for soundness. The combination of preservation and reflection, which we call transparency, is essential for sound and precise transformations. While preservation has been studied in the context of secure compilation, there is little work on reflection.
  This paper initiates the study of reflection and transparency. We formalize their definitions, propose a general method for proving that a transformation is transparent, and apply our method to different standard transformations. Subsequently, we present a transparent decompiler that combines seven XA transformations and prove that it is sound and precise. Additionally, we review the transparency of five existing tools and unveil violations in popular side-channel analysis frameworks and decompilers. Finally, we extend our approach to the setting of speculative side-channels. We redefine transparency of transformations with respect to Spectre attacks, and reevaluate the XA transformations for transparency w.r.t. Spectre.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04183v2</guid>
      <category>cs.PL</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Santiago Arranz-Olmos, Gilles Barthe, Lionel Blatter, S\"oren van der Wall, Zhiyuan Zhang</dc:creator>
    </item>
    <item>
      <title>A Lightweight Method for Generating Multi-Tier JIT Compilation Virtual Machine in a Meta-Tracing Compiler Framework</title>
      <link>https://arxiv.org/abs/2504.17460</link>
      <description>arXiv:2504.17460v2 Announce Type: replace 
Abstract: Meta-compiler frameworks, such as RPython and Graal/Truffle, generate high-performance virtual machines (VMs) from interpreter definitions. Although they generate VMs with high-quality just-in-time (JIT) compilers, they still lack an important feature that dedicated VMs (i.e., VMs that are developed for specific languages) have, namely \emph{multi-tier compilation}. Multi-tier compilation uses light-weight compilers at early stages and highly-optimizing compilers at later stages in order to balance between compilation overheads and code quality.
  We propose a novel approach to enabling multi-tier compilation in the VMs generated by a meta-compiler framework. Instead of extending the JIT compiler backend of the framework, our approach drives an existing (heavyweight) compiler backend in the framework to quickly generate unoptimized native code by merely embedding directives and compile-time operations into interpreter definitions.
  As a validation of the approach, we developed 2SOM, a Simple Object Machine with a two-tier JIT compiler based on RPython. 2SOM first applies the tier-1 threaded code generator that is generated by our proposed technique, then, to the loops that exceed a threshold, applies the tier-2 tracing JIT compiler that is generated by the original RPython framework. Our performance evaluation that runs a program with a realistic workload showed that 2SOM improved, when compared against an RPython-based VM, warm-up performance by 15\%, with merely a 5\% reduction in peak performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17460v2</guid>
      <category>cs.PL</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yusuke Izawa, Hidehiko Masuhara, Carl Friedrich Bolz-Tereick</dc:creator>
    </item>
    <item>
      <title>Dependency-Aware Compilation for Surface Code Quantum Architectures</title>
      <link>https://arxiv.org/abs/2311.18042</link>
      <description>arXiv:2311.18042v3 Announce Type: replace-cross 
Abstract: Practical applications of quantum computing depend on fault-tolerant devices with error correction. Today, the most promising approach is a class of error-correcting codes called surface codes. We study the problem of compiling quantum circuits for quantum computers implementing surface codes. Optimal or near-optimal compilation is critical for both efficiency and correctness. The compilation problem requires (1) mapping circuit qubits to the device qubits and (2) routing execution paths between interacting qubits. We solve this problem efficiently and near-optimally with a novel algorithm that exploits the dependency structure of circuit operations to formulate discrete optimization problems that can be approximated via simulated annealing, a classic and simple algorithm. Our extensive evaluation shows that our approach is powerful and flexible for compiling realistic workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18042v3</guid>
      <category>quant-ph</category>
      <category>cs.PL</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abtin Molavi, Amanda Xu, Swamit Tannu, Aws Albarghouthi</dc:creator>
    </item>
    <item>
      <title>Safe and usable kernel extensions with Rex</title>
      <link>https://arxiv.org/abs/2502.18832</link>
      <description>arXiv:2502.18832v2 Announce Type: replace-cross 
Abstract: Safe kernel extensions have gained significant traction, evolving from simple packet filters to large, complex programs that customize storage, networking, and scheduling. Existing kernel extension mechanisms like eBPF rely on in-kernel verifiers to ensure safety of kernel extensions by static verification using symbolic execution. We identify significant usability issues -- safe extensions being rejected by the verifier -- due to the language-verifier gap, a mismatch between developers' expectation of program safety provided by a contract with the programming language, and the verifier's expectation.
  We present Rex, a new kernel extension framework that closes the language-verifier gap and improves the usability of kernel extensions in terms of programming experience and maintainability. Rex builds upon language-based safety to provide safety properties desired by kernel extensions, along with a lightweight extralingual runtime for properties that are unsuitable for static analysis, including safe exception handling, stack safety, and termination. With Rex, kernel extensions are written in safe Rust and interact with the kernel via a safe interface provided by Rex's kernel crate. No separate static verification is needed. Rex addresses usability issues of eBPF kernel extensions without compromising performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18832v2</guid>
      <category>cs.OS</category>
      <category>cs.PL</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinghao Jia, Ruowen Qin, Milo Craun, Egor Lukiyanov, Ayush Bansal, Michael V. Le, Hubertus Franke, Hani Jamjoom, Tianyin Xu, Dan Williams</dc:creator>
    </item>
    <item>
      <title>Tilus: A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM Serving</title>
      <link>https://arxiv.org/abs/2504.12984</link>
      <description>arXiv:2504.12984v2 Announce Type: replace-cross 
Abstract: Serving Large Language Models (LLMs) is critical for AI-powered applications but demands substantial computational resources, particularly in memory bandwidth and computational throughput. Low-precision computation has emerged as a key technique to improve efficiency while reducing resource consumption. Existing approaches for generating low-precision kernels are limited to weight bit widths that are powers of two and suffer from suboptimal performance due to high-level GPU programming abstractions. These abstractions restrict critical optimizations, such as fine-grained register management and optimized memory access patterns, which are essential for efficient low-precision computations. In this paper, we introduce a virtual machine (VM) designed for General-Purpose GPU (GPGPU) computing, enabling support for low-precision data types with arbitrary bit widths while maintaining GPU programmability. The proposed VM features a thread-block-level programming model, a hierarchical memory space, a novel algebraic layout system, and extensive support for diverse low-precision data types. VM programs are compiled into highly efficient GPU programs with automatic vectorization and instruction selection. Extensive experiments demonstrate that our VM efficiently supports a full spectrum of low-precision data types, and outperforms state-of-the-art low-precision kernels on their supported types. Compared to existing compilers like Triton and Ladder, as well as hand-optimized kernels such as QuantLLM and Marlin, our VM achieves performance improvements of 1.75x, 2.61x, 1.29x and 1.03x, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.12984v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yaoyao Ding, Bohan Hou, Xiao Zhang, Allan Lin, Tianqi Chen, Cody Yu Hao, Yida Wang, Gennady Pekhimenko</dc:creator>
    </item>
  </channel>
</rss>
