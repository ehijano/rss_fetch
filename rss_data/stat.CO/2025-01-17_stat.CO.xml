<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Jan 2025 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Exploiting Higher-Order Statistics for Robust Probabilistic Rounding Error Analysis</title>
      <link>https://arxiv.org/abs/2404.12556</link>
      <description>arXiv:2404.12556v2 Announce Type: replace 
Abstract: Modern computer hardware supports low- and mixed-precision arithmetic for enhanced computational efficiency. In practical predictive modeling, however, it becomes vital to quantify the uncertainty due to rounding along with other sources of uncertainty (such as measurement, sampling, and numerical discretization) to ensure efficiency gains do not compromise accuracy. Higham and Mary [1] showed that modeling rounding errors as zero-mean independent random variables yields a problem size-dependent constant, $\tilde{\gamma}_n \propto \sqrt{n}$, which scales more slowly than in traditional deterministic analysis. We propose a novel variance-informed probabilistic rounding error analysis, modeling rounding errors as bounded, independent, and identically distributed (i.i.d.) random variables. This yields a new constant $\hat{\gamma}_n$, dependent on the mean, variance, and bounds of the rounding error distribution. We rigorously show that $\hat{\gamma}_n \propto \sqrt{n}$ using statistical properties of rounding errors, without ad-hoc assumptions, as in Higham and Mary. This new constant increases gradually with problem size and can improve the rounding error estimates for large arithmetic operations performed at low precision by up to six orders of magnitude. We conduct numerical experiments on random vector dot products, matrix-vector multiplication, a linear system solution, and a stochastic boundary value problem. We show that quantifying rounding uncertainty along with traditional sources (numerical discretization, sampling, parameters) enables a more efficient allocation of computational resources, thereby balancing computational efficiency with predictive accuracy. This study is a step towards a comprehensive mixed-precision approach that improves model reliability and enables budgeting of computational resources in predictive modeling and decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12556v2</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahil Bhola, Karthik Duraisamy</dc:creator>
    </item>
    <item>
      <title>ouladFormat R package: Preparing the Open University Learning Analytics Dataset for analysis</title>
      <link>https://arxiv.org/abs/2501.08366</link>
      <description>arXiv:2501.08366v2 Announce Type: replace 
Abstract: Analysing educational data sets is fundamental to many fields of research focusing on improving student learning. However, large educational data sets are complex and can involve intensive preprocessing. These obstacles can be overcome through the development of educational tools which simplifies the preprocessing stages of analysis. The Open University Learning Analytics Dataset (OULAD), available online, contains data from 32,593 students across 22 module presentations at the Open University. This paper introduces the R software package ouladFormat; which loads and formats the OULAD for data analysis. The paper summarizes the ouladFormat R package and explains the different functions within the package. In addition, two case studies are provided which discuss how the OULAD and ouladFormat R package could be used when preparing for an educational study, and in the early identification of at-risk students. The package increases the accessibility of the OULAD for researchers, practitioners, and educators, and supports reproducibility and comparability of educational studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08366v2</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Howard</dc:creator>
    </item>
    <item>
      <title>An efficient likelihood-free Bayesian inference method based on sequential neural posterior estimation</title>
      <link>https://arxiv.org/abs/2311.12530</link>
      <description>arXiv:2311.12530v4 Announce Type: replace-cross 
Abstract: Sequential neural posterior estimation (SNPE) techniques have been recently proposed for dealing with simulation-based models with intractable likelihoods. Unlike approximate Bayesian computation, SNPE techniques learn the posterior from sequential simulation using neural network-based conditional density estimators by minimizing a specific loss function. The SNPE method proposed by Lueckmann et al. (2017) used a calibration kernel to boost the sample weights around the observed data, resulting in a concentrated loss function. However, the use of calibration kernels may increase the variances of both the empirical loss and its gradient, making the training inefficient. To improve the stability of SNPE, this paper proposes to use an adaptive calibration kernel and several variance reduction techniques. The proposed method greatly speeds up the process of training and provides a better approximation of the posterior than the original SNPE method and some existing competitors as confirmed by numerical experiments. We also managed to demonstrate the superiority of the proposed method for a high-dimensional model with a real-world dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.12530v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifei Xiong, Xiliang Yang, Sanguo Zhang, Zhijian He</dc:creator>
    </item>
    <item>
      <title>Functional Singular Value Decomposition</title>
      <link>https://arxiv.org/abs/2410.03619</link>
      <description>arXiv:2410.03619v3 Announce Type: replace-cross 
Abstract: Heterogeneous functional data commonly arise in time series and longitudinal studies. To uncover the statistical structures of such data, we propose Functional Singular Value Decomposition (FSVD), a unified framework encompassing various tasks for the analysis of functional data with potential heterogeneity. We establish the mathematical foundation of FSVD by proving its existence and providing its fundamental properties. We then develop an implementation approach for noisy and irregularly observed functional data based on a novel alternating minimization scheme and provide theoretical guarantees for its convergence and estimation accuracy. The FSVD framework also introduces the concepts of intrinsic basis functions and intrinsic basis vectors, representing two fundamental structural aspects of random functions. These concepts enable FSVD to provide new and improved solutions to tasks including functional principal component analysis, factor models, functional clustering, functional linear regression, and functional completion, while effectively handling heterogeneity and irregular temporal sampling. Through extensive simulations, we demonstrate that FSVD-based methods consistently outperform existing methods across these tasks. To showcase the value of FSVD in real-world datasets, we apply it to extract temporal patterns from a COVID-19 case count dataset and perform data completion on an electronic health record dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03619v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jianbin Tan, Pixu Shi, Anru R. Zhang</dc:creator>
    </item>
  </channel>
</rss>
