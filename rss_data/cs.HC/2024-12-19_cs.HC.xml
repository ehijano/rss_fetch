<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Dec 2024 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A comprehensive review of assistive technologies for children with dyslexia</title>
      <link>https://arxiv.org/abs/2412.13241</link>
      <description>arXiv:2412.13241v1 Announce Type: new 
Abstract: Dyslexia is a learning disability that primarily disrupts one's ability to read, write and spell. About 15-20% of the world population is affected by dyslexia, which signals it is a concerning subject. A systematic literature review was conducted on this research to evaluate trending interventions, common accessibility features, and potential barriers to existing assistive devices and to identify any research gaps. Papers published over the period (2015-2023) of 8 years were reviewed, and the data was meticulously analyzed. This research shows that technological-based interventions are leading interventions, especially with the use of mobile apps and augmentative realities. More innovative technologies like virtual reality, NLP, haptic, and tangible user interfaces are emerging to provide unique solutions addressing the user's needs. Non-computing devices are less effective compared to modern digital solutions however, they provide a promising alternative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13241v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sansrit Paudel, Subek Acharya</dc:creator>
    </item>
    <item>
      <title>Nods of Agreement: Webcam-Driven Avatars Improve Meeting Outcomes and Avatar Satisfaction Over Audio-Driven or Static Avatars in All-Avatar Work Videoconferencing</title>
      <link>https://arxiv.org/abs/2412.13265</link>
      <description>arXiv:2412.13265v1 Announce Type: new 
Abstract: Avatars are edging into mainstream videoconferencing, but evaluation of how avatar animation modalities contribute to work meeting outcomes has been limited. We report a within-group videoconferencing experiment in which 68 employees of a global technology company, in 16 groups, used the same stylized avatars in three modalities (static picture, audio-animation, and webcam-animation) to complete collaborative decision-making tasks. Quantitatively, for meeting outcomes, webcam-animated avatars improved meeting effectiveness over the picture modality and were also reported to be more comfortable and inclusive than both other modalities. In terms of avatar satisfaction, there was a similar preference for webcam animation as compared to both other modalities. Our qualitative analysis shows participants expressing a preference for the holistic motion of webcam animation, and that meaningful movement outweighs realism for meeting outcomes, as evidenced through a systematic overview of ten thematic factors. We discuss implications for research and commercial deployment and conclude that webcam-animated avatars are a plausible alternative to video in work meetings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13265v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fang Ma, Ju Zhang, Lev Tankelevitch, Payod Panda, Torang Asadi, Charlie Hewitt, Lohit Petikam, Marco Gillies, Xueni Pan, Sean Rintel, Marta Wilczkowiak</dc:creator>
    </item>
    <item>
      <title>What Human-Horse Interactions may Teach us About Effective Human-AI Interactions</title>
      <link>https://arxiv.org/abs/2412.13405</link>
      <description>arXiv:2412.13405v1 Announce Type: new 
Abstract: This article explores human-horse interactions as a metaphor for understanding and designing effective human-AI partnerships. Drawing on the long history of human collaboration with horses, we propose that AI, like horses, should complement rather than replace human capabilities. We move beyond traditional benchmarks such as the Turing test, which emphasize AI's ability to mimic human intelligence, and instead advocate for a symbiotic relationship where distinct intelligences enhance each other. We analyze key elements of human-horse relationships: trust, communication, and mutual adaptability, to highlight essential principles for human-AI collaboration. Trust is critical in both partnerships, built through predictability and shared understanding, while communication and feedback loops foster mutual adaptability. We further discuss the importance of taming and habituation in shaping these interactions, likening it to how humans train AI to perform reliably and ethically in real-world settings. The article also addresses the asymmetry of responsibility, where humans ultimately bear the greater burden of oversight and ethical judgment. Finally, we emphasize that long-term commitment and continuous learning are vital in both human-horse and human-AI relationships, as ongoing interaction refines the partnership and increases mutual adaptability. By drawing on these insights from human-horse interactions, we offer a vision for designing AI systems that are trustworthy, adaptable, and capable of fostering symbiotic human-AI partnerships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13405v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Hossein Jarrahi, Stanley Ahalt</dc:creator>
    </item>
    <item>
      <title>The Role of Task Complexity in Reducing AI Plagiarism: A Study of Generative AI Tools</title>
      <link>https://arxiv.org/abs/2412.13412</link>
      <description>arXiv:2412.13412v1 Announce Type: new 
Abstract: This study investigates whether assessments fostering higher-order thinking skills can reduce plagiarism involving generative AI tools. Participants completed three tasks of varying complexity in four groups: control, e-textbook, Google, and ChatGPT. Findings show that AI plagiarism decreases as task complexity increases, with higher-order tasks resulting in lower similarity scores and AI plagiarism percentages. The study also highlights the distinction between similarity scores and AI plagiarism, recommending both for effective plagiarism detection. Results suggest that assessments promoting higher-order thinking are a viable strategy for minimizing AI-driven plagiarism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13412v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sacip Toker, Mahir Akgun</dc:creator>
    </item>
    <item>
      <title>Evaluating the Effect of Pretesting with Conversational AI on Retention of Needed Information</title>
      <link>https://arxiv.org/abs/2412.13487</link>
      <description>arXiv:2412.13487v1 Announce Type: new 
Abstract: This study explores the role of pretesting when integrated with conversational AI tools, specifically ChatGPT, in enhancing learning outcomes. Drawing on existing research, which demonstrates the benefits of pretesting in memory activation and retention, this experiment extends these insights into the context of digital learning environments. A randomized true experimental study was utilized. Participants were divided into two groups: one engaged in pretesting before using ChatGPT for a problem-solving task involving chi-square analysis, while the control group accessed ChatGPT immediately. The results indicate that the pretest group significantly outperformed the no-pretest group in a subsequent test, which suggests that pretesting enhances the retention of complex material. This study contributes to the field by demonstrating that pretesting can augment the learning process in technology-assisted environments by preparing the memory and promoting active engagement with the material. The findings also suggest that learning strategies like pretesting retain their relevance in the context of rapidly evolving AI technologies. Further research and practical implications are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13487v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahir Akgun, Sacip Toker</dc:creator>
    </item>
    <item>
      <title>Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data Presentation</title>
      <link>https://arxiv.org/abs/2412.13509</link>
      <description>arXiv:2412.13509v1 Announce Type: new 
Abstract: Understanding sensor data can be challenging for non-experts because of the complexity and unique semantic meanings of sensor modalities. This calls for intuitive and effective methods to present sensor information. However, creating intuitive sensor data visualizations presents three key challenges: the variability of sensor readings, gaps in domain comprehension, and the dynamic nature of sensor data. To address these issues, we develop Vivar, a novel AR system that integrates multi-modal sensor data and presents 3D volumetric content for visualization. In particular, we introduce a cross-modal embedding approach that maps sensor data into a pre-trained visual embedding space through barycentric interpolation. This allows for accurate and continuous integration of multi-modal sensor information. Vivar also incorporates sensor-aware AR scene generation using foundation models and 3D Gaussian Splatting (3DGS) without requiring domain expertise. In addition, Vivar leverages latent reuse and caching strategies to accelerate 2D and AR content generation. Our extensive experiments demonstrate that our system achieves 11$\times$ latency reduction without compromising quality. A user study involving over 485 participants, including domain experts, demonstrates Vivar's effectiveness in accuracy, consistency, and real-world applicability, paving the way for more intuitive sensor data visualization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13509v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunqi Guo, Kaiyuan Hou, Heming Fu, Hongkai Chen, Zhenyu Yan, Guoliang Xing, Xiaofan Jiang</dc:creator>
    </item>
    <item>
      <title>ScamGPT-J: Inside the Scammer's Mind, A Generative AI-Based Approach Toward Combating Messaging Scams</title>
      <link>https://arxiv.org/abs/2412.13528</link>
      <description>arXiv:2412.13528v1 Announce Type: new 
Abstract: The increase in global cellphone usage has led to a spike in instant messaging scams, causing extensive socio-economic damage with yearly losses exceeding half a trillion US dollars. These scams pose a challenge to the integrity of justice systems worldwide due to their international nature, which complicates legal action. Scams often exploit emotional vulnerabilities, making detection difficult for many. To address this, we introduce ScamGPT-J, a large language model that replicates scammer tactics. Unlike traditional methods that simply detect and block scammers, ScamGPT-J helps users recognize scam interactions by simulating scammer responses in real-time. If a user receives a message that closely matches a ScamGPT-J simulated response, it signals a potential scam, thus helping users identify and avoid scams more effectively. The model's effectiveness is evaluated through technical congruence with scam dialogues and user engagement. Our results show that ScamGPT-J can significantly aid in protecting against messaging scams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13528v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xue Wen Tan, Kenneth See, Stanley Kok</dc:creator>
    </item>
    <item>
      <title>Revisiting Interactions of Multiple Driver States in Heterogenous Population and Cognitive Tasks</title>
      <link>https://arxiv.org/abs/2412.13574</link>
      <description>arXiv:2412.13574v1 Announce Type: new 
Abstract: In real-world driving scenarios, multiple states occur simultaneously due to individual differences and environmental factors, complicating the analysis and estimation of driver states. Previous studies, limited by experimental design and analytical methods, may not be able to disentangle the relationships among multiple driver states and environmental factors. This paper introduces the Double Machine Learning (DML) analysis method to the field of driver state analysis to tackle this challenge. To train and test the DML model, a driving simulator experiment with 42 participants was conducted. All participants drove SAE level-3 vehicles and conducted three types of cognitive tasks in a 3-hour driving experiment. Drivers' subjective cognitive load and drowsiness levels were collected throughout the experiment. Then, we isolated individual and environmental factors affecting driver state variations and the factors affecting drivers' physiological and eye-tracking metrics when they are under specific states. The results show that our approach successfully decoupled and inferred the complex causal relationships between multiple types of drowsiness and cognitive load. Additionally, we identified key physiological and eye-tracking indicators in the presence of multiple driver states and under the influence of a single state, excluding the influence of other driver states, environmental factors, and individual characteristics. Our causal inference analytical framework can offer new insights for subsequent analysis of drivers' states. Further, the updated causal relation graph based on the DML analysis can provide theoretical bases for driver state monitoring based on physiological and eye-tracking measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13574v1</guid>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiyao Wang, Ange Wang, Song Yan, Dengbo He, Kaishun Wu</dc:creator>
    </item>
    <item>
      <title>NeckCare: Preventing Tech Neck using Hearable-based Multimodal Sensing</title>
      <link>https://arxiv.org/abs/2412.13579</link>
      <description>arXiv:2412.13579v1 Announce Type: new 
Abstract: Tech neck is a modern epidemic caused by prolonged device usage and it can lead to significant neck strain and discomfort. This paper addresses the challenge of detecting and preventing tech neck syndrome using non-invasive ubiquitous sensing techniques. We present NeckCare, a novel system leveraging hearable sensors, including IMUs and microphones, to monitor tech neck postures and estimate distance form screen in real-time. By analyzing pitch, displacement, and acoustic ranging data from 15 participants, we achieve posture classification accuracy of 96% using IMU data alone and 99% when combined with audio data. Our distance estimation technique is millimeter-level accurate even in noisy conditions. NeckCare provides immediate feedback to users, promoting healthier posture and reducing neck strain. Future work will explore personalizing alerts, predicting muscle strain, integrating neck exercise detection and enhancing digital eye strain prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13579v1</guid>
      <category>cs.HC</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bhawana Chhaglani, Alan Seefeldt</dc:creator>
    </item>
    <item>
      <title>From Risk to Readiness: VR-Based Safety Training for Industrial Hazards</title>
      <link>https://arxiv.org/abs/2412.13725</link>
      <description>arXiv:2412.13725v1 Announce Type: new 
Abstract: This study explores the application of Virtual Reality (VR) as a tool for safety training in high-risk industrial settings, specifically focusing on the IPLOM refinery, Busalla (Italy). As industries increasingly adopt digital tools to enhance safety and operational efficiency, VR provides a risk-free, immersive environment for training operators in emergency protocols. This project developed a VR simulation using Unreal Engine and Meta Quest headsets to mirror refinery conditions, including equipment handling, emergency response procedures, and spatial navigation. Integrated tools, such as multi-gas detectors and evacuation drills, allow users to practice real-world tasks virtually, enhancing procedural knowledge and spatial awareness. The simulation's design allows for future integration with Augmented Reality (AR) to enable real-time equipment monitoring and data overlays, enhancing on-site decision-making. Feedback from initial testing shows high user satisfaction and increased confidence in emergency response skills, indicating the effectiveness of VR in safety training. This VR approach offers a scalable, adaptable model for refining industrial training, reducing physical risks and costs associated with traditional drills, and setting a foundation for the use of immersive technologies in other high-risk sectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13725v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gianni Vercelli, Saverio Iacono, Luca Martini, Michele Zardetto, Daniele Zolezzi</dc:creator>
    </item>
    <item>
      <title>Understanding and Evaluating Trust in Generative AI and Large Language Models for Spreadsheets</title>
      <link>https://arxiv.org/abs/2412.14062</link>
      <description>arXiv:2412.14062v1 Announce Type: new 
Abstract: Generative AI and Large Language Models (LLMs) hold promise for automating spreadsheet formula creation. However, due to hallucinations, bias and variable user skill, outputs obtained from generative AI cannot be assumed to be accurate or trustworthy. To address these challenges, a trustworthiness framework is proposed based on evaluating the transparency and dependability of the formula. The transparency of the formula is explored through explainability (understanding the formula's reasoning) and visibility (inspecting the underlying algorithms). The dependability of the generated formula is evaluated in terms of reliability (consistency and accuracy) and ethical considerations (bias and fairness). The paper also examines the drivers to these metrics in the form of hallucinations, training data bias and poorly constructed prompts. Finally, examples of mistrust in technology are considered and the consequences explored.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14062v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the European Spreadsheets Risks Interest Group 2024</arxiv:journal_reference>
      <dc:creator>Simon Thorne</dc:creator>
    </item>
    <item>
      <title>Quantitative Predictive Monitoring and Control for Safe Human-Machine Interaction</title>
      <link>https://arxiv.org/abs/2412.13365</link>
      <description>arXiv:2412.13365v1 Announce Type: cross 
Abstract: There is a growing trend toward AI systems interacting with humans to revolutionize a range of application domains such as healthcare and transportation. However, unsafe human-machine interaction can lead to catastrophic failures. We propose a novel approach that predicts future states by accounting for the uncertainty of human interaction, monitors whether predictions satisfy or violate safety requirements, and adapts control actions based on the predictive monitoring results. Specifically, we develop a new quantitative predictive monitor based on Signal Temporal Logic with Uncertainty (STL-U) to compute a robustness degree interval, which indicates the extent to which a sequence of uncertain predictions satisfies or violates an STL-U requirement. We also develop a new loss function to guide the uncertainty calibration of Bayesian deep learning and a new adaptive control method, both of which leverage STL-U quantitative predictive monitoring results. We apply the proposed approach to two case studies: Type 1 Diabetes management and semi-autonomous driving. Experiments show that the proposed approach improves safety and effectiveness in both case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13365v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyang Dong, Meiyi Ma, Josephine Lamp, Sebastian Elbaum, Matthew B. Dwyer, Lu Feng</dc:creator>
    </item>
    <item>
      <title>An Exploratory Study of ML Sketches and Visual Code Assistants</title>
      <link>https://arxiv.org/abs/2412.13386</link>
      <description>arXiv:2412.13386v1 Announce Type: cross 
Abstract: This paper explores the integration of Visual Code Assistants in Integrated Development Environments (IDEs). In Software Engineering, whiteboard sketching is often the initial step before coding, serving as a crucial collaboration tool for developers. Previous studies have investigated patterns in SE sketches and how they are used in practice, yet methods for directly using these sketches for code generation remain limited. The emergence of visually-equipped large language models presents an opportunity to bridge this gap, which is the focus of our research. In this paper, we built a first prototype of a Visual Code Assistant to get user feedback regarding in-IDE sketch-to-code tools. We conduct an experiment with 19 data scientists, most of whom regularly sketch as part of their job. We investigate developers' mental models by analyzing patterns commonly observed in their sketches when developing an ML workflow. Analysis indicates that diagrams were the preferred organizational component (52.6%), often accompanied by lists (42.1%) and numbered points (36.8%). Our tool converts their sketches into a Python notebook by querying an LLM. We use an LLM-as-judge setup to score the quality of the generated code, finding that even brief sketching can effectively generate useful code outlines. We also find a positive correlation between sketch time and the quality of the generated code. We conclude the study by conducting extensive interviews to assess the tool's usefulness, explore potential use cases, and understand developers' needs. As noted by participants, promising applications for these assistants include education, prototyping, and collaborative settings. Our findings signal promise for the next generation of Code Assistants to integrate visual information, both to improve code generation and to better leverage developers' existing sketching practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13386v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)</arxiv:journal_reference>
      <dc:creator>Lu\'is F. Gomes, Vincent J. Hellendoorn, Jonathan Aldrich, Rui Abreu</dc:creator>
    </item>
    <item>
      <title>Toward an Insider Threat Education Platform: A Theoretical Literature Review</title>
      <link>https://arxiv.org/abs/2412.13446</link>
      <description>arXiv:2412.13446v1 Announce Type: cross 
Abstract: Insider threats (InTs) within organizations are small in number but have a disproportionate ability to damage systems, information, and infrastructure. Existing InT research studies the problem from psychological, technical, and educational perspectives. Proposed theories include research on psychological indicators, machine learning, user behavioral log analysis, and educational methods to teach employees recognition and mitigation techniques. Because InTs are a human problem, training methods that address InT detection from a behavioral perspective are critical. While numerous technological and psychological theories exist on detection, prevention, and mitigation, few training methods prioritize psychological indicators. This literature review studied peer-reviewed, InT research organized by subtopic and extracted critical theories from psychological, technical, and educational disciplines. In doing so, this is the first study to comprehensively organize research across all three approaches in a manner which properly informs the development of an InT education platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13446v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haywood Gelman, John D. Hastings, David Kenley, Eleanor Loiacono</dc:creator>
    </item>
    <item>
      <title>GUI Agents: A Survey</title>
      <link>https://arxiv.org/abs/2412.13501</link>
      <description>arXiv:2412.13501v1 Announce Type: cross 
Abstract: Graphical User Interface (GUI) agents, powered by Large Foundation Models, have emerged as a transformative approach to automating human-computer interaction. These agents autonomously interact with digital systems or software applications via GUIs, emulating human actions such as clicking, typing, and navigating visual elements across diverse platforms. Motivated by the growing interest and fundamental importance of GUI agents, we provide a comprehensive survey that categorizes their benchmarks, evaluation metrics, architectures, and training methods. We propose a unified framework that delineates their perception, reasoning, planning, and acting capabilities. Furthermore, we identify important open challenges and discuss key future directions. Finally, this work serves as a basis for practitioners and researchers to gain an intuitive understanding of current progress, techniques, benchmarks, and critical open problems that remain to be addressed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13501v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, Xintong Li, Jing Shi, Hongjie Chen, Viet Dac Lai, Zhouhang Xie, Sungchul Kim, Ruiyi Zhang, Tong Yu, Mehrab Tanjim, Nesreen K. Ahmed, Puneet Mathur, Seunghyun Yoon, Lina Yao, Branislav Kveton, Thien Huu Nguyen, Trung Bui, Tianyi Zhou, Ryan A. Rossi, Franck Dernoncourt</dc:creator>
    </item>
    <item>
      <title>TelePhantom: A User-Friendly Teleoperation System with Virtual Assistance for Enhanced Effectiveness</title>
      <link>https://arxiv.org/abs/2412.13548</link>
      <description>arXiv:2412.13548v1 Announce Type: cross 
Abstract: Dexterous manipulation is a critical area of robotics. In this field, teleoperation faces three key challenges: user-friendliness for novices, safety assurance, and transferability across different platforms. While collecting real robot dexterous manipulation data by teleoperation to train robots has shown impressive results on diverse tasks, due to the morphological differences between human and robot hands, it is not only hard for new users to understand the action mapping but also raises potential safety concerns during operation. To address these limitations, we introduce TelePhantom. This teleoperation system offers real-time visual feedback on robot actions based on human user inputs, with a total hardware cost of less than $1,000. TelePhantom allows the user to see a virtual robot that represents the outcome of the user's next movement. By enabling flexible switching between command visualization and actual execution, this system helps new users learn how to demonstrate quickly and safely. We demonstrate its superiority over other teleoperation systems across five tasks, emphasize its ease of use, and highlight its ease of deployment across diverse input sensors and robotic platforms. We will release our code and a deployment document on our website: https://telephantom.github.io/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13548v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingxiang Guo, Jiayu Luo, Zhenyu Wei, Yiwen Hou, Zhixuan Xu, Xiaoyi Lin, Chongkai Gao, Lin Shao</dc:creator>
    </item>
    <item>
      <title>Unified Understanding of Environment, Task, and Human for Human-Robot Interaction in Real-World Environments</title>
      <link>https://arxiv.org/abs/2412.13726</link>
      <description>arXiv:2412.13726v1 Announce Type: cross 
Abstract: To facilitate human--robot interaction (HRI) tasks in real-world scenarios, service robots must adapt to dynamic environments and understand the required tasks while effectively communicating with humans. To accomplish HRI in practice, we propose a novel indoor dynamic map, task understanding system, and response generation system. The indoor dynamic map optimizes robot behavior by managing an occupancy grid map and dynamic information, such as furniture and humans, in separate layers. The task understanding system targets tasks that require multiple actions, such as serving ordered items. Task representations that predefine the flow of necessary actions are applied to achieve highly accurate understanding. The response generation system is executed in parallel with task understanding to facilitate smooth HRI by informing humans of the subsequent actions of the robot. In this study, we focused on waiter duties in a restaurant setting as a representative application of HRI in a dynamic environment. We developed an HRI system that could perform tasks such as serving food and cleaning up while communicating with customers. In experiments conducted in a simulated restaurant environment, the proposed HRI system successfully communicated with customers and served ordered food with 90\% accuracy. In a questionnaire administered after the experiment, the HRI system of the robot received 4.2 points out of 5. These outcomes indicated the effectiveness of the proposed method and HRI system in executing waiter tasks in real-world environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13726v1</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/RO-MAN60168.2024.10731235</arxiv:DOI>
      <dc:creator>Yuga Yano, Akinobu Mizutani, Yukiya Fukuda, Daiju Kanaoka, Tomohiro Ono, Hakaru Tamukoh</dc:creator>
    </item>
    <item>
      <title>TH\"OR-MAGNI Act: Actions for Human Motion Modeling in Robot-Shared Industrial Spaces</title>
      <link>https://arxiv.org/abs/2412.13729</link>
      <description>arXiv:2412.13729v1 Announce Type: cross 
Abstract: Accurate human activity and trajectory prediction are crucial for ensuring safe and reliable human-robot interactions in dynamic environments, such as industrial settings, with mobile robots. Datasets with fine-grained action labels for moving people in industrial environments with mobile robots are scarce, as most existing datasets focus on social navigation in public spaces. This paper introduces the TH\"OR-MAGNI Act dataset, a substantial extension of the TH\"OR-MAGNI dataset, which captures participant movements alongside robots in diverse semantic and spatial contexts. TH\"OR-MAGNI Act provides 8.3 hours of manually labeled participant actions derived from egocentric videos recorded via eye-tracking glasses. These actions, aligned with the provided TH\"OR-MAGNI motion cues, follow a long-tailed distribution with diversified acceleration, velocity, and navigation distance profiles. We demonstrate the utility of TH\"OR-MAGNI Act for two tasks: action-conditioned trajectory prediction and joint action and trajectory prediction. We propose two efficient transformer-based models that outperform the baselines to address these tasks. These results underscore the potential of TH\"OR-MAGNI Act to develop predictive models for enhanced human-robot interaction in complex environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13729v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tiago Rodrigues de Almeida, Tim Schreiter, Andrey Rudenko, Luigi Palmieiri, Johannes A. Stork, Achim J. Lilienthal</dc:creator>
    </item>
    <item>
      <title>User-Generated Content and Editors in Games: A Comprehensive Survey</title>
      <link>https://arxiv.org/abs/2412.13743</link>
      <description>arXiv:2412.13743v1 Announce Type: cross 
Abstract: User-Generated Content (UGC) refers to any form of content, such as posts and images, created by users rather than by professionals. In recent years, UGC has become an essential part of the evolving video game industry, influencing both game culture and community dynamics. The ability for users to actively contribute to the games they engage with has shifted the landscape of gaming from a one-directional entertainment experience into a collaborative, user-driven ecosystem. Therefore, this growing trend highlights the urgent need for summarizing the current UGC development in game industry. Our conference paper has systematically classified the existing UGC in games and the UGC editors separately into four types. However, the previous survey lacks the depth and precision necessary to capture the wide-ranging and increasingly complex nature of UGC. To this end, as an extension of previous work, this paper presents a refined and expanded classification of UGC and UGC editors within video games, offering a more robust and comprehensive framework with representative cases that better reflects the diversity and nuances of contemporary user-generated contributions. Moreover, we provide our insights on the future of UGC, involving game culture, game genre and user creative tendencies, artificial intelligence, its potential ethical considerations, and relationship between games, users and communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13743v1</guid>
      <category>cs.MM</category>
      <category>cs.HC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuyue Liu, Haihan Duan, Wei Cai</dc:creator>
    </item>
    <item>
      <title>AI Perceptions Across Cultures: Similarities and Differences in Expectations, Risks, Benefits, Tradeoffs, and Value in Germany and China</title>
      <link>https://arxiv.org/abs/2412.13841</link>
      <description>arXiv:2412.13841v1 Announce Type: cross 
Abstract: As artificial intelligence (AI) continues to advance, understanding public perceptions -- including biases, risks, and benefits -- is critical for guiding research priorities, shaping public discourse, and informing policy. This study explores public mental models of AI using micro scenarios to assess reactions to 71 statements about AI's potential future impacts. Drawing on cross-cultural samples from Germany (N=52) and China (N=60), we identify significant differences in expectations, evaluations, and risk-utility tradeoffs. German participants tended toward more cautious assessments, whereas Chinese participants expressed greater optimism regarding AI's societal benefits. Chinese participants exhibited relatively balanced risk-benefit tradeoffs ($\beta=-0.463$ for risk and $\beta=+0.484$ for benefit, $r^2=.630$). In contrast, German participants showed a stronger emphasis on AI benefits and less on risks ($\beta=-0.337$ for risk and $\beta=+0.715$ for benefit, $r^2=.839$). Visual cognitive maps illustrate these contrasts, offering new perspectives on how cultural contexts shape AI acceptance. Our findings underline key factors influencing public perception and provide actionable insights for fostering equitable and culturally sensitive integration of AI technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13841v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Brauner, Felix Glawe, Gian Luca Liehner, Luisa Vervier, Martina Ziefle</dc:creator>
    </item>
    <item>
      <title>Cognition Chain for Explainable Psychological Stress Detection on Social Media</title>
      <link>https://arxiv.org/abs/2412.14009</link>
      <description>arXiv:2412.14009v1 Announce Type: cross 
Abstract: Stress is a pervasive global health issue that can lead to severe mental health problems. Early detection offers timely intervention and prevention of stress-related disorders. The current early detection models perform "black box" inference suffering from limited explainability and trust which blocks the real-world clinical application. Thanks to the generative properties introduced by the Large Language Models (LLMs), the decision and the prediction from such models are semi-interpretable through the corresponding description. However, the existing LLMs are mostly trained for general purposes without the guidance of psychological cognitive theory. To this end, we first highlight the importance of prior theory with the observation of performance boosted by the chain-of-thoughts tailored for stress detection. This method termed Cognition Chain explicates the generation of stress through a step-by-step cognitive perspective based on cognitive appraisal theory with a progress pipeline: Stimulus $\rightarrow$ Evaluation $\rightarrow$ Reaction $\rightarrow$ Stress State, guiding LLMs to provide comprehensive reasoning explanations. We further study the benefits brought by the proposed Cognition Chain format by utilising it as a synthetic dataset generation template for LLMs instruction-tuning and introduce CogInstruct, an instruction-tuning dataset for stress detection. This dataset is developed using a three-stage self-reflective annotation pipeline that enables LLMs to autonomously generate and refine instructional data. By instruction-tuning Llama3 with CogInstruct, we develop CogLLM, an explainable stress detection model. Evaluations demonstrate that CogLLM achieves outstanding performance while enhancing explainability. Our work contributes a novel approach by integrating cognitive theories into LLM reasoning processes, offering a promising direction for future explainable AI research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14009v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Wang, Boyan Gao, Yi Dai, Lei Cao, Liang Zhao, Yibo Yang, David Clifton</dc:creator>
    </item>
    <item>
      <title>Future Research Avenues for Artificial Intelligence in Digital Gaming: An Exploratory Report</title>
      <link>https://arxiv.org/abs/2412.14085</link>
      <description>arXiv:2412.14085v1 Announce Type: cross 
Abstract: Video games are a natural and synergistic application domain for artificial intelligence (AI) systems, offering both the potential to enhance player experience and immersion, as well as providing valuable benchmarks and virtual environments to advance AI technologies in general. This report presents a high-level overview of five promising research pathways for applying state-of-the-art AI methods, particularly deep learning, to digital gaming within the context of the current research landscape. The objective of this work is to outline a curated, non-exhaustive list of encouraging research directions at the intersection of AI and video games that may serve to inspire more rigorous and comprehensive research efforts in the future. We discuss (i) investigating large language models as core engines for game agent modelling, (ii) using neural cellular automata for procedural game content generation, (iii) accelerating computationally expensive in-game simulations via deep surrogate modelling, (iv) leveraging self-supervised learning to obtain useful video game state embeddings, and (v) training generative models of interactive worlds using unlabelled video data. We also briefly address current technical challenges associated with the integration of advanced deep learning systems into video game development, and indicate key areas where further progress is likely to be beneficial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14085v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Markus Dablander</dc:creator>
    </item>
    <item>
      <title>I Know Your Feelings Before You Do: Predicting Future Affective Reactions in Human-Computer Dialogue</title>
      <link>https://arxiv.org/abs/2303.00146</link>
      <description>arXiv:2303.00146v4 Announce Type: replace 
Abstract: Current Spoken Dialogue Systems (SDSs) often serve as passive listeners that respond only after receiving user speech. To achieve human-like dialogue, we propose a novel future prediction architecture that allows an SDS to anticipate future affective reactions based on its current behaviors before the user speaks. In this work, we investigate two scenarios: speech and laughter. In speech, we propose to predict the user's future emotion based on its temporal relationship with the system's current emotion and its causal relationship with the system's current Dialogue Act (DA). In laughter, we propose to predict the occurrence and type of the user's laughter using the system's laughter behaviors in the current turn. Preliminary analysis of human-robot dialogue demonstrated synchronicity in the emotions and laughter displayed by the human and robot, as well as DA-emotion causality in their dialogue. This verifies that our architecture can contribute to the development of an anticipatory SDS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.00146v4</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanchao Li, Koji Inoue, Leimin Tian, Changzeng Fu, Carlos Ishi, Hiroshi Ishiguro, Tatsuya Kawahara, Catherine Lai</dc:creator>
    </item>
    <item>
      <title>Case Law Grounding: Using Precedents to Align Decision-Making for Humans and AI</title>
      <link>https://arxiv.org/abs/2310.07019</link>
      <description>arXiv:2310.07019v3 Announce Type: replace 
Abstract: Communities and groups often need to make decisions grounded by social norms and preferences, such as when moderating content or providing judgments for aligning AI systems. Prevailing approaches to provide this grounding have primarily centered around constructing high-level guidelines and criteria, similar to legal ``constitutions''. However, it can be challenging to specify social norms and preferences consistently and accurately through constitutions alone. In this work, we take inspiration from legal systems and introduce ``case law grounding'' (CLG) -- a novel approach for grounding decision-making that uses past cases and decisions (precedents) to ground future decisions in a way that can be utilized by human-led processes or implemented through prompting large language models (LLMs). We evaluate how accurately CLG grounds decisions with five groups and communities spread across two decision task domains, comparing against a traditional constitutional grounding approach, and find that in 4 out of 5 groups, decisions produced with CLG were significantly more accurately aligned to ground truth: 16.0--23.3 %-points higher accuracy using the human-led process, and 20.8--32.9 %-points higher when prompting LLMs. We also evaluate the impact of different configurations of CLG, such as the case retrieval window size and whether to enforce binding decisions based on selected precedents, showing support for using binding decisions and preferring larger retrieval windows. Finally, we discuss the limitations of our case-based approach as well as how it may be best used to augment existing constitutional approaches when it comes to aligning human and AI decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07019v3</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Quan Ze Chen, Amy X. Zhang</dc:creator>
    </item>
    <item>
      <title>A Multi-Scale Cognitive Interaction Model of Instrument Operations at the Linac Coherent Light Source</title>
      <link>https://arxiv.org/abs/2408.04734</link>
      <description>arXiv:2408.04734v4 Announce Type: replace 
Abstract: The Linac Coherent Light Source (LCLS) is the world's first x-ray free electron laser. It is a scientific user facility operated by the SLAC National Accelerator Laboratory, at Stanford, for the U.S. Department of Energy. As beam time at LCLS is extremely valuable and limited, experimental efficiency -- getting the most high quality data in the least time -- is critical. Our overall project employs cognitive engineering methodologies with the goal of improving experimental efficiency and increasing scientific productivity at LCLS by refining experimental interfaces and workflows, simplifying tasks, reducing errors, and improving operator safety and stress. Here we describe a multi-agent, multi-scale computational cognitive interaction model of instrument operations at LCLS. Our model simulates aspects of human cognition at multiple cognitive and temporal scales, ranging from seconds to hours, and among agents playing multiple roles, including instrument operator, real time data analyst, and experiment manager. The model can roughly predict impacts stemming from proposed changes to operational interfaces and workflows. Example results demonstrate the model's potential in guiding modifications to improve operational efficiency. We discuss the implications of our effort for cognitive engineering in complex experimental settings, and outline future directions for research. The model is open source and supplementary videos provide extensive detail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04734v4</guid>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <category>hep-ex</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Segal, Wan-Lin Hu, Paul H. Fuoss, Frank E. Ritter, Jeff Shrager</dc:creator>
    </item>
    <item>
      <title>Towards Understanding the Impact of Guidance in Data Visualization Systems for Domain Experts</title>
      <link>https://arxiv.org/abs/2412.01024</link>
      <description>arXiv:2412.01024v4 Announce Type: replace 
Abstract: Guided data visualization systems are highly useful for domain experts to highlight important trends in their large-scale and complex datasets. However, more work is needed to understand the impact of guidance on interpreting data visualizations as well as on the resulting use of visualizations when communicating insights. We conducted two user studies with domain experts and found that experts benefit from a guided coarse-to-fine structure when using data visualization systems, as this is the same structure in which they communicate findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01024v4</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sherry Qiu, Holly Rushmeier, Kim R. M. Blenman</dc:creator>
    </item>
    <item>
      <title>LLM Whisperer: An Inconspicuous Attack to Bias LLM Responses</title>
      <link>https://arxiv.org/abs/2406.04755</link>
      <description>arXiv:2406.04755v3 Announce Type: replace-cross 
Abstract: Writing effective prompts for large language models (LLM) can be unintuitive and burdensome. In response, services that optimize or suggest prompts have emerged. While such services can reduce user effort, they also introduce a risk: the prompt provider can subtly manipulate prompts to produce heavily biased LLM responses. In this work, we show that subtle synonym replacements in prompts can increase the likelihood (by a difference up to 78%) that LLMs mention a target concept (e.g., a brand, political party, nation). We substantiate our observations through a user study, showing our adversarially perturbed prompts 1) are indistinguishable from unaltered prompts by humans, 2) push LLMs to recommend target concepts more often, and 3) make users more likely to notice target concepts, all without arousing suspicion. The practicality of this attack has the potential to undermine user autonomy. Among other measures, we recommend implementing warnings against using prompts from untrusted parties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04755v3</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Weiran Lin, Anna Gerchanovsky, Omer Akgul, Lujo Bauer, Matt Fredrikson, Zifan Wang</dc:creator>
    </item>
  </channel>
</rss>
