<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Oct 2024 04:03:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Joint Probability Estimation of Many Binary Outcomes via Localized Adversarial Lasso</title>
      <link>https://arxiv.org/abs/2410.15166</link>
      <description>arXiv:2410.15166v1 Announce Type: new 
Abstract: In this work we consider estimating the probability of many (possibly dependent) binary outcomes which is at the core of many applications, e.g., multi-level treatments in causal inference, demands for bundle of products, etc. Without further conditions, the probability distribution of an M dimensional binary vector is characterized by exponentially in M coefficients which can lead to a high-dimensional problem even without the presence of covariates. Understanding the (in)dependence structure allows us to substantially improve the estimation as it allows for an effective factorization of the probability distribution. In order to estimate the probability distribution of a M dimensional binary vector, we leverage a Bahadur representation that connects the sparsity of its coefficients with independence across the components. We propose to use regularized and adversarial regularized estimators to obtain an adaptive estimator with respect to the dependence structure which allows for rates of convergence to depend on this intrinsic (lower) dimension. These estimators are needed to handle several challenges within this setting, including estimating nuisance parameters, estimating covariates, and nonseparable moment conditions. Our main results consider the presence of (low dimensional) covariates for which we propose a locally penalized estimator. We provide pointwise rates of convergence addressing several issues in the theoretical analyses as we strive for making a computationally tractable formulation. We apply our results in the estimation of causal effects with multiple binary treatments and show how our estimators can improve the finite sample performance when compared with non-adaptive estimators that try to estimate all the probabilities directly. We also provide simulations that are consistent with our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15166v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Belloni, Yan Chen, Matthew Harding</dc:creator>
    </item>
    <item>
      <title>Polyspectral Mean Estimation of General Nonlinear Processes</title>
      <link>https://arxiv.org/abs/2410.15187</link>
      <description>arXiv:2410.15187v1 Announce Type: new 
Abstract: Higher-order spectra (or polyspectra), defined as the Fourier Transform of a stationary process' autocumulants, are useful in the analysis of nonlinear and non Gaussian processes. Polyspectral means are weighted averages over Fourier frequencies of the polyspectra, and estimators can be constructed from analogous weighted averages of the higher-order periodogram (a statistic computed from the data sample's discrete Fourier Transform). We derive the asymptotic distribution of a class of polyspectral mean estimators, obtaining an exact expression for the limit distribution that depends on both the given weighting function as well as on higher-order spectra. Secondly, we use bispectral means to define a new test of the linear process hypothesis. Simulations document the finite sample properties of the asymptotic results. Two applications illustrate our results' utility: we test the linear process hypothesis for a Sunspot time series, and for the Gross Domestic Product we conduct a clustering exercise based on bispectral means with different weight functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15187v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhrubajyoti Ghosh, Tucker McElroy, Soumendra Lahiri</dc:creator>
    </item>
    <item>
      <title>Volatility estimation from a view point of entropy</title>
      <link>https://arxiv.org/abs/2410.15307</link>
      <description>arXiv:2410.15307v1 Announce Type: new 
Abstract: In the present paper, we first revisit the volatility estimation approach proposed by N. Kunitomo and S. Sato, and second, we show that the volatility estimator proposed by P. Malliavin and M.E. Mancino can be understood in a unified way by the approach. Third, we introduce an alternative estimator that might overcome the inconsistency caused by the microstructure noise of the initial observation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15307v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jir\^o Akahori, Ryuya Namba, Atsuhito Watanabe</dc:creator>
    </item>
    <item>
      <title>Quantiles and Quantile Regression on Riemannian Manifolds: a measure-transportation-based approach</title>
      <link>https://arxiv.org/abs/2410.15711</link>
      <description>arXiv:2410.15711v1 Announce Type: new 
Abstract: Increased attention has been given recently to the statistical analysis of variables with values on nonlinear manifolds. A natural but nontrivial problem in that context is the definition of quantile concepts. We are proposing a solution for compact Riemannian manifolds without boundaries; typical examples are polyspheres, hyperspheres, and toro\"{\i}dal manifolds equipped with their Riemannian metrics. Our concept of quantile function comes along with a concept of distribution function and, in the empirical case, ranks and signs. The absence of a canonical ordering is offset by resorting to the data-driven ordering induced by optimal transports. Theoretical properties, such as the uniform convergence of the empirical distribution and conditional (and unconditional) quantile functions and distribution-freeness of ranks and signs, are established. Statistical inference applications, from goodness-of-fit to distribution-free rank-based testing, are without number. Of particular importance is the case of quantile regression with directional or toro\"{\i}dal multiple output, which is given special attention in this paper. Extensive simulations are carried out to illustrate these novel concepts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15711v1</guid>
      <category>math.ST</category>
      <category>math.GT</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Hallin, Hang Liu</dc:creator>
    </item>
    <item>
      <title>The mutual arrangement of Wright-Fisher diffusion path measures and its impact on parameter estimation</title>
      <link>https://arxiv.org/abs/2410.15955</link>
      <description>arXiv:2410.15955v1 Announce Type: new 
Abstract: The Wright-Fisher diffusion is a fundamentally important model of evolution encompassing genetic drift, mutation, and natural selection. Suppose you want to infer the parameters associated with these processes from an observed sample path. Then to write down the likelihood one first needs to know the mutual arrangement of two path measures under different parametrizations; that is, whether they are absolutely continuous, equivalent, singular, and so on. In this paper we give a complete answer to this question by finding the separating times for the diffusion - the stopping time before which one measure is absolutely continuous with respect to the other and after which the pair is mutually singular. In one dimension this extends a classical result of Dawson on the local equivalence between neutral and non-neutral Wright-Fisher diffusion measures. Along the way we also develop new zero-one type laws for the diffusion on its approach to, and emergence from, the boundary. As an application we derive an explicit expression for the joint maximum likelihood estimator of the mutation and selection parameters and show that its convergence properties are closely related to the separating time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15955v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>q-bio.PE</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul A. Jenkins</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bayesian networks are typically faithful in the total variation metric</title>
      <link>https://arxiv.org/abs/2410.16004</link>
      <description>arXiv:2410.16004v1 Announce Type: new 
Abstract: We show that for a given DAG $G$, among all observational distributions of Bayesian networks over $G$ with arbitrary outcome spaces, the faithful distributions are `typical': they constitute a dense, open set with respect to the total variation metric. As a consequence, the set of faithful distributions is non-empty, and the unfaithful distributions are nowhere dense. We extend this result to the space of Bayesian networks, where the properties hold for Bayesian networks instead of distributions of Bayesian networks. As special cases, we show that these results also hold for the faithful parameters of the subclasses of linear Gaussian -- and discrete Bayesian networks, giving a topological analogue of the measure-zero results of Spirtes et al. (1993) and Meek (1995). Finally, we extend our topological results and the measure-zero results of Spirtes et al. and Meek to Bayesian networks with latent variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16004v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philip Boeken, Patrick Forr\'e, Joris M. Mooij</dc:creator>
    </item>
    <item>
      <title>High-dimensional prediction for count response via sparse exponential weights</title>
      <link>https://arxiv.org/abs/2410.15381</link>
      <description>arXiv:2410.15381v1 Announce Type: cross 
Abstract: Count data is prevalent in various fields like ecology, medical research, and genomics. In high-dimensional settings, where the number of features exceeds the sample size, feature selection becomes essential. While frequentist methods like Lasso have advanced in handling high-dimensional count data, Bayesian approaches remain under-explored with no theoretical results on prediction performance. This paper introduces a novel probabilistic machine learning framework for high-dimensional count data prediction. We propose a pseudo-Bayesian method that integrates a scaled Student prior to promote sparsity and uses an exponential weight aggregation procedure. A key contribution is a novel risk measure tailored to count data prediction, with theoretical guarantees for prediction risk using PAC-Bayesian bounds. Our results include non-asymptotic oracle inequalities, demonstrating rate-optimal prediction error without prior knowledge of sparsity. We implement this approach efficiently using Langevin Monte Carlo method. Simulations and a real data application highlight the strong performance of our method compared to the Lasso in various settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15381v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>The Tien Mai</dc:creator>
    </item>
    <item>
      <title>Simultaneous Inference in Multiple Matrix-Variate Graphs for High-Dimensional Neural Recordings</title>
      <link>https://arxiv.org/abs/2410.15530</link>
      <description>arXiv:2410.15530v1 Announce Type: cross 
Abstract: As large-scale neural recordings become common, many neuroscientific investigations are focused on identifying functional connectivity from spatio-temporal measurements in two or more brain areas across multiple sessions. Spatial-temporal data in neural recordings can be represented as matrix-variate data, with time as the first dimension and space as the second. In this paper, we exploit the multiple matrix-variate Gaussian Graphical model to encode the common underlying spatial functional connectivity across multiple sessions of neural recordings. By effectively integrating information across multiple graphs, we develop a novel inferential framework that allows simultaneous testing to detect meaningful connectivity for a target edge subset of arbitrary size. Our test statistics are based on a group penalized regression approach and a high-dimensional Gaussian approximation technique. The validity of simultaneous testing is demonstrated theoretically under mild assumptions on sample size and non-stationary autoregressive temporal dependence. Our test is nearly optimal in achieving the testable region boundary. Additionally, our method involves only convex optimization and parametric bootstrap, making it computationally attractive. We demonstrate the efficacy of the new method through both simulations and an experimental study involving multiple local field potential (LFP) recordings in the Prefrontal Cortex (PFC) and visual area V4 during a memory-guided saccade task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15530v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zongge Liu, Heejong Bong, Zhao Ren, Matthew A. Smith, Robert E. Kass</dc:creator>
    </item>
    <item>
      <title>Distributionally Robust Instrumental Variables Estimation</title>
      <link>https://arxiv.org/abs/2410.15634</link>
      <description>arXiv:2410.15634v1 Announce Type: cross 
Abstract: Instrumental variables (IV) estimation is a fundamental method in econometrics and statistics for estimating causal effects in the presence of unobserved confounding. However, challenges such as untestable model assumptions and poor finite sample properties have undermined its reliability in practice. Viewing common issues in IV estimation as distributional uncertainties, we propose DRIVE, a distributionally robust framework of the classical IV estimation method. When the ambiguity set is based on a Wasserstein distance, DRIVE minimizes a square root ridge regularized variant of the two stage least squares (TSLS) objective. We develop a novel asymptotic theory for this regularized regression estimator based on the square root ridge, showing that it achieves consistency without requiring the regularization parameter to vanish. This result follows from a fundamental property of the square root ridge, which we call ``delayed shrinkage''. This novel property, which also holds for a class of generalized method of moments (GMM) estimators, ensures that the estimator is robust to distributional uncertainties that persist in large samples. We further derive the asymptotic distribution of Wasserstein DRIVE and propose data-driven procedures to select the regularization parameter based on theoretical results. Simulation studies confirm the superior finite sample performance of Wasserstein DRIVE. Thanks to its regularization and robustness properties, Wasserstein DRIVE could be preferable in practice, particularly when the practitioner is uncertain about model assumptions or distributional shifts in data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15634v1</guid>
      <category>econ.EM</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaonan Qu, Yongchan Kwon</dc:creator>
    </item>
    <item>
      <title>A note on the sparse Hanson-Wright inequality</title>
      <link>https://arxiv.org/abs/2410.15652</link>
      <description>arXiv:2410.15652v1 Announce Type: cross 
Abstract: We obtain Hanson-Wright inequalities for the quadratic form of a random vector with independent sparse random variables. Specifically, we consider cases where the components of the random vector are sparse $\alpha$-sub-exponential random variables with $\alpha&gt;0$. Our proof relies on a novel combinatorial approach to estimate the moments of the random quadratic form.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15652v1</guid>
      <category>math.PR</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiyun He, Ke Wang, Yizhe Zhu</dc:creator>
    </item>
    <item>
      <title>On the VC dimension of deep group convolutional neural networks</title>
      <link>https://arxiv.org/abs/2410.15800</link>
      <description>arXiv:2410.15800v1 Announce Type: cross 
Abstract: We study the generalization capabilities of Group Convolutional Neural Networks (GCNNs) with ReLU activation function by deriving upper and lower bounds for their Vapnik-Chervonenkis (VC) dimension. Specifically, we analyze how factors such as the number of layers, weights, and input dimension affect the VC dimension. We further compare the derived bounds to those known for other types of neural networks. Our findings extend previous results on the VC dimension of continuous GCNNs with two layers, thereby providing new insights into the generalization properties of GCNNs, particularly regarding the dependence on the input resolution of the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15800v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Sepliarskaia, Sophie Langer, Johannes Schmidt-Hieber</dc:creator>
    </item>
    <item>
      <title>On the Geometry of Regularization in Adversarial Training: High-Dimensional Asymptotics and Generalization Bounds</title>
      <link>https://arxiv.org/abs/2410.16073</link>
      <description>arXiv:2410.16073v1 Announce Type: cross 
Abstract: Regularization, whether explicit in terms of a penalty in the loss or implicit in the choice of algorithm, is a cornerstone of modern machine learning. Indeed, controlling the complexity of the model class is particularly important when data is scarce, noisy or contaminated, as it translates a statistical belief on the underlying structure of the data. This work investigates the question of how to choose the regularization norm $\lVert \cdot \rVert$ in the context of high-dimensional adversarial training for binary classification. To this end, we first derive an exact asymptotic description of the robust, regularized empirical risk minimizer for various types of adversarial attacks and regularization norms (including non-$\ell_p$ norms). We complement this analysis with a uniform convergence analysis, deriving bounds on the Rademacher Complexity for this class of problems. Leveraging our theoretical results, we quantitatively characterize the relationship between perturbation size and the optimal choice of $\lVert \cdot \rVert$, confirming the intuition that, in the data scarce regime, the type of regularization becomes increasingly important for adversarial training as perturbations grow in size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16073v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Vilucchio, Nikolaos Tsilivis, Bruno Loureiro, Julia Kempe</dc:creator>
    </item>
    <item>
      <title>Implicit Regularization for Tubal Tensor Factorizations via Gradient Descent</title>
      <link>https://arxiv.org/abs/2410.16247</link>
      <description>arXiv:2410.16247v1 Announce Type: cross 
Abstract: We provide a rigorous analysis of implicit regularization in an overparametrized tensor factorization problem beyond the lazy training regime. For matrix factorization problems, this phenomenon has been studied in a number of works. A particular challenge has been to design universal initialization strategies which provably lead to implicit regularization in gradient-descent methods. At the same time, it has been argued by Cohen et. al. 2016 that more general classes of neural networks can be captured by considering tensor factorizations. However, in the tensor case, implicit regularization has only been rigorously established for gradient flow or in the lazy training regime. In this paper, we prove the first tensor result of its kind for gradient descent rather than gradient flow. We focus on the tubal tensor product and the associated notion of low tubal rank, encouraged by the relevance of this model for image data. We establish that gradient descent in an overparametrized tensor factorization model with a small random initialization exhibits an implicit bias towards solutions of low tubal rank. Our theoretical findings are illustrated in an extensive set of numerical simulations show-casing the dynamics predicted by our theory as well as the crucial role of using a small random initialization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16247v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Santhosh Karnik, Anna Veselovska, Mark Iwen, Felix Krahmer</dc:creator>
    </item>
    <item>
      <title>Complete Asymptotic Expansions for the Normalizing Constants of High-Dimensional Matrix Bingham and Matrix Langevin Distributions</title>
      <link>https://arxiv.org/abs/2402.08663</link>
      <description>arXiv:2402.08663v3 Announce Type: replace 
Abstract: For positive integers $d$ and $p$ such that $d \ge p$, let $\mathbb{R}^{d \times p}$ denote the set of $d \times p$ real matrices, $I_p$ be the identity matrix of order $p$, and $V_{d,p} = \{x \in \mathbb{R}^{d \times p} \mid x'x = I_p\}$ be the Stiefel manifold in $\mathbb{R}^{d \times p}$. Complete asymptotic expansions as $d \to \infty$ are obtained for the normalizing constants of the matrix Bingham and matrix Langevin probability distributions on $V_{d,p}$. The accuracy of each truncated expansion is strictly increasing in $d$; also, for sufficiently large $d$, the accuracy is strictly increasing in $m$, the number of terms in the truncated expansion. Lower bounds are obtained for the truncated expansions when the matrix parameters of the matrix Bingham distribution are positive definite and when the matrix parameter of the matrix Langevin distribution is of full rank. These results are applied to obtain the rates of convergence of the asymptotic expansions as both $d \to \infty$ and $p \to \infty$. Values of $d$ and $p$ arising in numerous data sets are used to illustrate the rate of convergence of the truncated approximations as $d$ or $m$ increases. These results extend recently-obtained asymptotic expansions for the normalizing constants of the high-dimensional Bingham distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08663v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.3842/SIGMA.2024.094</arxiv:DOI>
      <arxiv:journal_reference>SIGMA 20 (2024), 094, 22 pages</arxiv:journal_reference>
      <dc:creator>Armine Bagyan, Donald Richards</dc:creator>
    </item>
    <item>
      <title>Randomization-based confidence sets for the local average treatment effect</title>
      <link>https://arxiv.org/abs/2404.18786</link>
      <description>arXiv:2404.18786v2 Announce Type: replace 
Abstract: We consider the problem of generating confidence sets in randomized experiments with noncompliance. We show that a refinement of a randomization-based procedure proposed by Imbens and Rosenbaum (2005) has desirable properties. Namely, we show that using a studentized Anderson-Rubin-type statistic as a test statistic yields confidence intervals that are finite-sample exact under treatment effect homogeneity, and remain asymptotically valid for the Local Average Treatment Effect when the treatment effect is heterogeneous. We provide a uniform analysis of this procedure. An algorithm is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18786v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>P. M. Aronow, Haoge Chang, Patrick Lopatto</dc:creator>
    </item>
    <item>
      <title>Generalized van Trees inequality: Local minimax bounds for non-smooth functionals and irregular statistical models</title>
      <link>https://arxiv.org/abs/2405.06437</link>
      <description>arXiv:2405.06437v2 Announce Type: replace 
Abstract: In a decision-theoretic framework, the minimax lower bound provides the worst-case performance of estimators relative to a given class of statistical models. For parametric and semiparametric models, the H\'{a}jek--Le Cam local asymptotic minimax (LAM) theorem provides the sharp local asymptotic lower bound. Despite its relative generality, this result comes with limitations as it only applies to the estimation of differentiable functionals under regular statistical models. On the other hand, minimax lower bound techniques such as Fano's or Assoud's are applicable in more general settings but are not sharp enough to imply the LAM theorem. To address this gap, we provide new non-asymptotic minimax lower bounds under minimal regularity assumptions, which imply sharp asymptotic constants. The proposed lower bounds do not require the differentiability of functionals or regularity of statistical models, extending the efficiency theory to broader situations where standard results fail. The use of the new lower bounds is illustrated through the local minimax lower bound constants for estimating the density at a point and directionally differentiable parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06437v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenta Takatsu, Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>Continuous Testing</title>
      <link>https://arxiv.org/abs/2409.05654</link>
      <description>arXiv:2409.05654v5 Announce Type: replace 
Abstract: Testing has developed into the fundamental statistical framework for falsifying hypotheses. Unfortunately, tests are binary in nature: a test either rejects a hypothesis or not. Such binary decisions do not reflect the reality of many scientific studies, which often aim to present the evidence against a hypothesis and do not necessarily intend to establish a definitive conclusion. We propose the continuous generalization of a test, which we use to measure the evidence against a hypothesis. Such a continuous test can be viewed as a continuous and non-randomized interpretation of the classical 'randomized test'. This offers the benefits of a randomized test, without the downsides of external randomization. Another interpretation is as a literal measure, which measures the amount of binary tests that reject the hypothesis. Our work completes the bridge between classical tests and the recently proposed $e$-values: $e$-values bounded to $[0, 1/\alpha]$ are continuously interpreted size $\alpha$ randomized tests. Taking $\alpha$ to 0 yields the regular $e$-value: a 'level 0' continuous test. Moreover, we generalize the traditional notion of power by using generalized means. This produces a unified framework that contains both classical Neyman-Pearson optimal testing and log-optimal $e$-values, as well as a continuum of other options. The traditional $p$-value appears as the reciprocal of generally invalid continuous test. In an illustration in a Gaussian location model, we find that optimal continuous tests are of a beautifully simple form.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05654v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick W. Koning</dc:creator>
    </item>
    <item>
      <title>General linear hypothesis testing of high-dimensional mean vectors with unequal covariance matrices based on random integration</title>
      <link>https://arxiv.org/abs/2410.14120</link>
      <description>arXiv:2410.14120v2 Announce Type: replace 
Abstract: This paper is devoted to the study of the general linear hypothesis testing (GLHT) problem of multi-sample high-dimensional mean vectors. For the GLHT problem, we introduce a test statistic based on $L^2$-norm and random integration method, and deduce the asymptotic distribution of the statistic under given conditions. Finally, the potential advantages of our test statistics are verified by numerical simulation studies and examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14120v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingxiang Cao, Yelong Qiu, Junyong Park</dc:creator>
    </item>
    <item>
      <title>Concentration of the Langevin Algorithm's Stationary Distribution</title>
      <link>https://arxiv.org/abs/2212.12629</link>
      <description>arXiv:2212.12629v2 Announce Type: replace-cross 
Abstract: A canonical algorithm for log-concave sampling is the Langevin Algorithm, aka the Langevin Diffusion run with some discretization stepsize $\eta &gt; 0$. This discretization leads the Langevin Algorithm to have a stationary distribution $\pi_{\eta}$ which differs from the stationary distribution $\pi$ of the Langevin Diffusion, and it is an important challenge to understand whether the well-known properties of $\pi$ extend to $\pi_{\eta}$. In particular, while concentration properties such as isoperimetry and rapidly decaying tails are classically known for $\pi$, the analogous properties for $\pi_{\eta}$ are open questions with algorithmic implications. This note provides a first step in this direction by establishing concentration results for $\pi_{\eta}$ that mirror classical results for $\pi$. Specifically, we show that for any nontrivial stepsize $\eta &gt; 0$, $\pi_{\eta}$ is sub-exponential (respectively, sub-Gaussian) when the potential is convex (respectively, strongly convex). Moreover, the concentration bounds we show are essentially tight. We also show that these concentration bounds extend to all iterates along the trajectory of the Langevin Algorithm, and to inexact implementations which use sub-Gaussian estimates of the gradient.
  Key to our analysis is the use of a rotation-invariant moment generating function (aka Bessel function) to study the stationary dynamics of the Langevin Algorithm. This technique may be of independent interest because it enables directly analyzing the discrete-time stationary distribution $\pi_{\eta}$ without going through the continuous-time stationary distribution $\pi$ as an intermediary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.12629v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jason M. Altschuler, Kunal Talwar</dc:creator>
    </item>
    <item>
      <title>Maximum Likelihood Estimation under the Emax Model: Existence, Geometry and Efficiency</title>
      <link>https://arxiv.org/abs/2401.00354</link>
      <description>arXiv:2401.00354v2 Announce Type: replace-cross 
Abstract: This study focuses on the estimation of the Emax dose-response model, a widely utilized framework in clinical trials, agriculture, and environmental experiments. Existing challenges in obtaining maximum likelihood estimates (MLE) for model parameters are often ascribed to computational issues but, in reality, stem from the absence of a MLE. Our contribution provides a new understanding and control of all the experimental situations that practitioners might face, guiding them in the estimation process. We derive the exact MLE for a three-point experimental design and we identify the two scenarios where the MLE fails. To address these challenges, we propose utilizing Firth's modified score, providing its analytical expression as a function of the experimental design. Through a simulation study, we demonstrate that, in one of the problematic cases, the Firth modification yields a finite estimate. For the remaining case, we introduce a design-augmentation strategy akin to a hypothesis test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00354v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giacomo Aletti, Nancy Flournoy, Caterina May, Chiara Tommasi</dc:creator>
    </item>
    <item>
      <title>Homaloidal Polynomials and Gaussian Models of Maximum Likelihood Degree One</title>
      <link>https://arxiv.org/abs/2402.06090</link>
      <description>arXiv:2402.06090v3 Announce Type: replace-cross 
Abstract: We study the Gaussian statistical models whose log-likelihood function has a unique complex critical point, i.e., has maximum likelihood degree one. We exploit the connection developed by Am\'endola et. al. between the models having maximum likelihood degree one and homaloidal polynomials. We study the spanning tree generating function of a graph and show this polynomial is homaloidal when the graph is chordal. When the graph is a cycle on $n$ vertices, $n \geq 4$, we prove the polynomial is not homaloidal, and show that the maximum likelihood degree of the resulting model is the $n$th Eulerian number. These results support our conjecture that the spanning tree generating function is a homaloidal polynomial if and only if the graph is chordal. We also provide an algebraic formulation for the defining equations of these models. Using existing results, we provide a computational study on constructing new families of homaloidal polynomials. In the end, we analyze the symmetric determinantal representation of such polynomials and provide an upper bound on the size of the matrices involved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06090v3</guid>
      <category>math.AG</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shelby Cox, Pratik Misra, Pardis Semnani</dc:creator>
    </item>
    <item>
      <title>Gradient-flow adaptive importance sampling for Bayesian leave one out cross-validation with application to sigmoidal classification models</title>
      <link>https://arxiv.org/abs/2402.08151</link>
      <description>arXiv:2402.08151v2 Announce Type: replace-cross 
Abstract: We introduce gradient-flow-guided adaptive importance sampling (IS) transformations for stabilizing Monte-Carlo approximations of leave-one-out (LOO) cross-validated predictions for Bayesian models. After defining two variational problems, we derive corresponding simple nonlinear transformations that utilize gradient information to shift a model's pre-trained full-data posterior closer to the target LOO posterior predictive distributions. In doing so, the transformations stabilize importance weights. The resulting Monte Carlo integrals depend on Jacobian determinants with respect to the model Hessian. We derive closed-form exact formulae for these Jacobian determinants in the cases of logistic regression and shallow ReLU-activated artificial neural networks, and provide a simple approximation that sidesteps the need to compute full Hessian matrices and their spectra. We test the methodology on an $n\ll p$ dataset that is known to produce unstable LOO IS weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08151v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua C Chang, Xiangting Li, Shixin Xu, Hao-Ren Yao, Julia Porcino, Carson Chow</dc:creator>
    </item>
    <item>
      <title>Adversarial Consistency and the Uniqueness of the Adversarial Bayes Classifier</title>
      <link>https://arxiv.org/abs/2404.17358</link>
      <description>arXiv:2404.17358v3 Announce Type: replace-cross 
Abstract: Minimizing an adversarial surrogate risk is a common technique for learning robust classifiers. Prior work showed that convex surrogate losses are not statistically consistent in the adversarial context -- or in other words, a minimizing sequence of the adversarial surrogate risk will not necessarily minimize the adversarial classification error. We connect the consistency of adversarial surrogate losses to properties of minimizers to the adversarial classification risk, known as adversarial Bayes classifiers. Specifically, under reasonable distributional assumptions, a convex surrogate loss is statistically consistent for adversarial learning iff the adversarial Bayes classifier satisfies a certain notion of uniqueness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17358v3</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Natalie S. Frank</dc:creator>
    </item>
    <item>
      <title>A Concentration Inequality for Maximum Mean Discrepancy (MMD)-based Statistics and Its Application in Generative Models</title>
      <link>https://arxiv.org/abs/2405.14051</link>
      <description>arXiv:2405.14051v2 Announce Type: replace-cross 
Abstract: Maximum Mean Discrepancy (MMD) is a probability metric that has found numerous applications in machine learning. In this work, we focus on its application in generative models, including the minimum MMD estimator, Generative Moment Matching Network (GMMN), and Generative Adversarial Network (GAN). In these cases, MMD is part of an objective function in a minimization or min-max optimization problem. Even if its empirical performance is competitive, the consistency and convergence rate analysis of the corresponding MMD-based estimators has yet to be carried out.
  We propose a uniform concentration inequality for a class of Maximum Mean Discrepancy (MMD)-based estimators, that is, a maximum deviation bound of empirical MMD values over a collection of generated distributions and adversarially learned kernels. Here, our inequality serves as an efficient tool in the theoretical analysis for MMD-based generative models. As elaborating examples, we applied our main result to provide the generalization error bounds for the MMD-based estimators in the context of the minimum MMD estimator and MMD GAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14051v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yijin Ni, Xiaoming Huo</dc:creator>
    </item>
    <item>
      <title>Matrix perturbation bounds via contour bootstrapping</title>
      <link>https://arxiv.org/abs/2407.05230</link>
      <description>arXiv:2407.05230v4 Announce Type: replace-cross 
Abstract: Matrix perturbation bounds play an essential role in the design and analysis of spectral algorithms. In this paper, we use a "contour bootstrapping" argument to derive several new perturbation bounds. As applications, we discuss new bounds on the error occurring when one uses matrix sparsification to speed up the computation of spectral parameters. Another potential application is the estimation of the trade-off in computing with privacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05230v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Phuc Tran, Van Vu</dc:creator>
    </item>
    <item>
      <title>Permutation groups, partition lattices and block structures</title>
      <link>https://arxiv.org/abs/2409.10461</link>
      <description>arXiv:2409.10461v2 Announce Type: replace-cross 
Abstract: Let $G$ be a transitive permutation group on $\Omega$. The $G$-invariant partitions form a sublattice of the lattice of all partitions of $\Omega$, having the further property that all its elements are uniform (that is, have all parts of the same size). If, in addition, all the equivalence relations defining the partitions commute, then the relations form an \emph{orthogonal block structure}, a concept from statistics; in this case the lattice is modular. If it is distributive, then we have a \emph{poset block structure}, whose automorphism group is a \emph{generalised wreath product}. We examine permutation groups with these properties, which we call the \emph{OB property} and \emph{PB property} respectively, and in particular investigate when direct and wreath products of groups with these properties also have these properties.
  A famous theorem on permutation groups asserts that a transitive imprimitive group $G$ is embeddable in the wreath product of two factors obtained from the group (the group induced on a block by its setwise stabiliser, and the group induced on the set of blocks by~$G$). We extend this theorem to groups with the PB property, embeddng them into generalised wreath products. We show that the map from posets to generalised wreath products preserves intersections and inclusions.
  We have included background and historical material on these concepts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10461v2</guid>
      <category>math.GR</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marina Anagnostopoulou-Merkouri, R. A. Bailey, Peter J. Cameron</dc:creator>
    </item>
    <item>
      <title>Functional Singular Value Decomposition</title>
      <link>https://arxiv.org/abs/2410.03619</link>
      <description>arXiv:2410.03619v2 Announce Type: replace-cross 
Abstract: Heterogeneous functional data are commonly seen in time series and longitudinal data analysis. To capture the statistical structures of such data, we propose the framework of Functional Singular Value Decomposition (FSVD), a unified framework with structure-adaptive interpretability for the analysis of heterogeneous functional data. We establish the mathematical foundation of FSVD by proving its existence and providing its fundamental properties using operator theory. We then develop an implementation approach for noisy and irregularly observed functional data based on a novel joint kernel ridge regression scheme and provide theoretical guarantees for its convergence and estimation accuracy. The framework of FSVD also introduces the concepts of intrinsic basis functions and intrinsic basis vectors, which represent two fundamental statistical structures for random functions and connect FSVD to various tasks including functional principal component analysis, factor models, functional clustering, and functional completion. We compare the performance of FSVD with existing methods in several tasks through extensive simulation studies. To demonstrate the value of FSVD in real-world datasets, we apply it to extract temporal patterns from a COVID-19 case count dataset and perform data completion on an electronic health record dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03619v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jianbin Tan, Pixu Shi, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Modelling 1/f Noise in TRNGs via Fractional Brownian Motion</title>
      <link>https://arxiv.org/abs/2410.14205</link>
      <description>arXiv:2410.14205v2 Announce Type: replace-cross 
Abstract: Building upon the foundational work of atomic clock physicists Barnes and Allan, this paper presents a highly scalable and numerically exact framework for modeling \(1/f\) noise in oscillatory True Random Number Generators (TRNGs) and assessing their cryptographic security. By employing Fractional Brownian Motion, the framework constructs Gaussian non-stationary processes that represent these noise spectra accurately and in a mathematically sound way. Furthermore, it establishes several critical properties, including optimal bounds on the achievable generation rate of cryptographically secure bits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14205v2</guid>
      <category>cs.CR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maciej Skorski</dc:creator>
    </item>
  </channel>
</rss>
