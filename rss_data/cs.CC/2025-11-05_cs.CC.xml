<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Nov 2025 05:00:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Complexity of counting points on curves and the factor $P_1(T)$ of the zeta function of surfaces</title>
      <link>https://arxiv.org/abs/2511.02262</link>
      <description>arXiv:2511.02262v1 Announce Type: new 
Abstract: This article concerns the computational complexity of a fundamental problem in number theory: counting points on curves and surfaces over finite fields. There is no subexponential-time algorithm known and it is unclear if it can be $\mathrm{NP}$-hard.
  Given a curve, we present the first efficient Arthur-Merlin protocol to certify its point-count, its Jacobian group structure, and its Hasse-Weil zeta function. We extend this result to a smooth projective surface to certify the factor $P_{1}(T)$, corresponding to the first Betti number, of the zeta function; by using the counting oracle. We give the first algorithm to compute $P_{1}(T)$ that is poly($\log q$)-time if the degree $D$ of the input surface is fixed; and in quantum poly($D\log q$)-time in general.
  Our technique in the curve case, is to sample hash functions using the Weil and Riemann-Roch bounds, to certify the group order of its Jacobian. For higher dimension varieties, we first reduce to the case of a surface, which is fibred as a Lefschetz pencil of hyperplane sections over $\mathbb{P}^{1}$. The formalism of vanishing cycles, and the inherent big monodromy, enable us to prove an effective version of Deligne's `theoreme du pgcd' using the hard-Lefschetz theorem and an equidistribution result due to Katz. These reduce our investigations to that of computing the zeta function of a curve, defined over a finite field extension $\mathbb{F}_{Q}/\mathbb{F}_{q}$ of poly-bounded degree. This explicitization of the theory yields the first nontrivial upper bounds on the computational complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02262v1</guid>
      <category>cs.CC</category>
      <category>math.AG</category>
      <category>math.NT</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diptajit Roy, Nitin Saxena, Madhavan Venkatesh</dc:creator>
    </item>
    <item>
      <title>Spectral Certificates and Sum-of-Squares Lower Bounds for Semirandom Hamiltonians</title>
      <link>https://arxiv.org/abs/2511.02264</link>
      <description>arXiv:2511.02264v1 Announce Type: new 
Abstract: The $k$-$\mathsf{XOR}$ problem is one of the most well-studied problems in classical complexity. We study a natural quantum analogue of $k$-$\mathsf{XOR}$, the problem of computing the ground energy of a certain subclass of structured local Hamiltonians, signed sums of $k$-local Pauli operators, which we refer to as $k$-$\mathsf{XOR}$ Hamiltonians. As an exhibition of the connection between this model and classical $k$-$\mathsf{XOR}$, we extend results on refuting $k$-$\mathsf{XOR}$ instances to the Hamiltonian setting by crafting a quantum variant of the Kikuchi matrix for CSP refutation, instead capturing ground energy optimization. As our main result, we show an $n^{O(\ell)}$-time classical spectral algorithm certifying ground energy at most $\frac{1}{2} + \varepsilon$ in (1) semirandom Hamiltonian $k$-$\mathsf{XOR}$ instances or (2) sums of Gaussian-signed $k$-local Paulis both with $O(n) \cdot \left(\frac{n}{\ell}\right)^{k/2-1} \log n /\varepsilon^4$ local terms, a tradeoff known as the refutation threshold. Additionally, we give evidence this tradeoff is tight in the semirandom regime via non-commutative Sum-of-Squares lower bounds embedding classical $k$-$\mathsf{XOR}$ instances as entirely classical Hamiltonians.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02264v1</guid>
      <category>cs.CC</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas Kocurek</dc:creator>
    </item>
    <item>
      <title>Relaxed vs. Full Local Decodability with Few Queries: Equivalence and Separations for Linear Codes</title>
      <link>https://arxiv.org/abs/2511.02633</link>
      <description>arXiv:2511.02633v1 Announce Type: new 
Abstract: A locally decodable code (LDC) $C \colon \{0,1\}^k \to \{0,1\}^n$ is an error-correcting code that allows one to recover any bit of the original message with good probability while only reading a small number of bits from a corrupted codeword. A relaxed locally decodable code (RLDC) is a weaker notion where the decoder is additionally allowed to abort and output a special symbol $\bot$ if it detects an error. For a large constant number of queries $q$, there is a large gap between the blocklength $n$ of the best $q$-query LDC and the best $q$-query RLDC. Existing constructions of RLDCs achieve polynomial length $n = k^{1 + O(1/q)}$, while the best-known $q$-LDCs only achieve subexponential length $n = 2^{k^{o(1)}}$. On the other hand, for $q = 2$, it is known that RLDCs and LDCs are equivalent. We thus ask the question: what is the smallest $q$ such that there exists a $q$-RLDC that is not a $q$-LDC?
  In this work, we show that any linear $3$-query RLDC is in fact a $3$-LDC, i.e., linear RLDCs and LDCs are equivalent at $3$ queries. More generally, we show for any constant $q$, there is a soundness error threshold $s(q)$ such that any linear $q$-RLDC with soundness error below this threshold must be a $q$-LDC. This implies that linear RLDCs cannot have "strong soundness" -- a stricter condition satisfied by linear LDCs that says the soundness error is proportional to the fraction of errors in the corrupted codeword -- unless they are simply LDCs.
  In addition, we give simple constructions of linear $15$-query RLDCs that are not $q$-LDCs for any constant $q$, showing that for $q = 15$, linear RLDCs and LDCs are not equivalent.
  We also prove nearly identical results for locally correctable codes and their corresponding relaxed counterpart.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02633v1</guid>
      <category>cs.CC</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elena Grigorescu, Vinayak M. Kumar, Peter Manohar, Geoffrey Mon</dc:creator>
    </item>
    <item>
      <title>Fast Approximation Algorithm for Non-Monotone DR-submodular Maximization under Size Constraint</title>
      <link>https://arxiv.org/abs/2511.02254</link>
      <description>arXiv:2511.02254v1 Announce Type: cross 
Abstract: This work studies the non-monotone DR-submodular Maximization over a ground set of $n$ subject to a size constraint $k$. We propose two approximation algorithms for solving this problem named FastDrSub and FastDrSub++. FastDrSub offers an approximation ratio of $0.044$ with query complexity of $O(n \log(k))$. The second one, FastDrSub++, improves upon it with a ratio of $1/4-\epsilon$ within query complexity of $(n \log k)$ for an input parameter $\epsilon &gt;0$. Therefore, our proposed algorithms are the first constant-ratio approximation algorithms for the problem with the low complexity of $O(n \log(k))$.
  Additionally, both algorithms are experimentally evaluated and compared against existing state-of-the-art methods, demonstrating their effectiveness in solving the Revenue Maximization problem with DR-submodular objective function. The experimental results show that our proposed algorithms significantly outperform existing approaches in terms of both query complexity and solution quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02254v1</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tan D. Tran, Canh V. Pham</dc:creator>
    </item>
    <item>
      <title>Non-commutative linear logic fragments with sub-context-free complexity</title>
      <link>https://arxiv.org/abs/2511.02348</link>
      <description>arXiv:2511.02348v1 Announce Type: cross 
Abstract: We present new descriptive complexity characterisations of classes REG (regular languages), LCFL (linear context-free languages) and CFL (context-free languages) as restrictions on inference rules, size of formulae and permitted connectives in the Lambek calculus; fragments of the intuitionistic non-commutative linear logic with direction-sensitive implication connectives. Our identification of the Lambek calculus fragments with proof complexity REG and LCFL is the first result of its kind. We further show the CFL complexity of one of the strictly `weakest' possible variants of the logic, admitting only a single inference rule. The proof thereof, moreover, is based on a direct translation between type-logical and formal grammar and structural induction on provable sequents; a simpler and more intuitive method than those employed in prior works. We thereby establish a clear conceptual utility of the Cut-elimination theorem for comparing formal grammar and sequent calculus, and identify the exact analogue of the Greibach Normal Form in Lambek grammar. We believe the result presented herein constitutes a first step toward a more extensive and richer characterisation of the interaction between computation and logic, as well as a finer-grained complexity separation of various sequent calculi.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02348v1</guid>
      <category>cs.LO</category>
      <category>cs.CC</category>
      <category>cs.FL</category>
      <category>math.LO</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yusaku Nishimiya, Masaya Taniguchi</dc:creator>
    </item>
    <item>
      <title>Arithmetic Circuits and Neural Networks for Regular Matroids</title>
      <link>https://arxiv.org/abs/2511.02406</link>
      <description>arXiv:2511.02406v1 Announce Type: cross 
Abstract: We prove that there exist uniform $(+,\times,/)$-circuits of size $O(n^3)$ to compute the basis generating polynomial of regular matroids on $n$ elements. By tropicalization, this implies that there exist uniform $(\max,+,-)$-circuits and ReLU neural networks of the same size for weighted basis maximization of regular matroids. As a consequence in linear programming theory, we obtain a first example where taking the difference of two extended formulations can be more efficient than the best known individual extended formulation of size $O(n^6)$ by Aprile and Fiorini. Such differences have recently been introduced as virtual extended formulations. The proof of our main result relies on a fine-tuned version of Seymour's decomposition of regular matroids which allows us to identify and maintain graphic substructures to which we can apply a local version of the star-mesh transformation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02406v1</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Hertrich, Stefan Kober, Georg Loho</dc:creator>
    </item>
    <item>
      <title>Recursively Enumerably Representable Classes and Computable Versions of the Fundamental Theorem of Statistical Learning</title>
      <link>https://arxiv.org/abs/2511.02644</link>
      <description>arXiv:2511.02644v1 Announce Type: cross 
Abstract: We study computable probably approximately correct (CPAC) learning, where learners are required to be computable functions. It had been previously observed that the Fundamental Theorem of Statistical Learning, which characterizes PAC learnability by finiteness of the Vapnik-Chervonenkis (VC-)dimension, no longer holds in this framework. Recent works recovered analogs of the Fundamental Theorem in the computable setting, for instance by introducing an effective VC-dimension. Guided by this, we investigate the connection between CPAC learning and recursively enumerable representable (RER) classes, whose members can be algorithmically listed. Our results show that the effective VC-dimensions can take arbitrary values above the traditional one, even for RER classes, which creates a whole family of (non-)examples for various notions of CPAC learning. Yet the two dimensions coincide for classes satisfying sufficiently strong notions of CPAC learning. We then observe that CPAC learnability can also be characterized via containment of RER classes that realize the same samples. Furthermore, it is shown that CPAC learnable classes satisfying a unique identification property are necessarily RER. Finally, we establish that agnostic learnability can be guaranteed for RER classes, by considering the relaxed notion of nonuniform CPAC learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02644v1</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>math.LO</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David Kattermann, Lothar Sebastian Krapp</dc:creator>
    </item>
    <item>
      <title>Tensor rank and dimension expanders</title>
      <link>https://arxiv.org/abs/2511.02670</link>
      <description>arXiv:2511.02670v1 Announce Type: cross 
Abstract: We prove a lower bound on the rank of tensors constructed from families of linear maps that `expand' the dimension of every subspace. Such families, called {\em dimension expanders} have been studied for many years with several known explicit constructions. Using these constructions we show that one can construct an explicit $[D]\times [n] \times [n]$-tensor with rank at least $(2 - \epsilon)n$, with $D$ a constant depending on $\epsilon$. Our results extend to border rank over the real or complex numbers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02670v1</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeev Dvir</dc:creator>
    </item>
    <item>
      <title>On the Parameterized Complexity of Grundy Domination and Zero Forcing Problems</title>
      <link>https://arxiv.org/abs/2508.18104</link>
      <description>arXiv:2508.18104v2 Announce Type: replace 
Abstract: We consider two different problem families that deal with domination in graphs. On the one hand, we focus on dominating sequences. In such a sequence, every vertex dominates some vertex of the graph that was not dominated by any earlier vertex in the sequence. The problem of finding the longest dominating sequence is known as $\mathsf{Grundy~Domination}$. Depending on whether the closed or the open neighborhoods are used for domination, there are three other versions of this problem. We show that all four problem variants are $\mathsf{W[1]}$-complete when parameterized by the solution size. On the other hand, we consider the family of zero forcing problems which form the parameterized duals of the Grundy domination problems. In these problems, one looks for the smallest set of vertices initially colored blue such that certain color change rules are able to color all other vertices blue. Bhyravarapu et al. [IWOCA 2025] showed that one of these problems, known as $\mathsf{Zero~Forcing~Set}$, is in $\mathsf{FPT}$ when parameterized by the treewidth or the solution size. We extend their treewidth result to the other three variants of zero forcing and their respective Grundy domination problems. Our algorithm also implies an $\mathsf{FPT}$ algorithm for $\mathsf{Grundy~Domination}$ when parameterized by the number of vertices that are not in the dominating sequence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18104v2</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert Scheffler</dc:creator>
    </item>
    <item>
      <title>Realizable Circuit Complexity: Embedding Computation in Space-Time</title>
      <link>https://arxiv.org/abs/2509.19161</link>
      <description>arXiv:2509.19161v2 Announce Type: replace 
Abstract: Classical circuit complexity characterizes parallel computation in purely combinatorial terms, ignoring the physical constraints that govern real hardware. The standard classes $\mathbf{NC}$, $\mathbf{AC}$, and $\mathbf{TC}$ treat unlimited fan-in, free interconnection, and polynomial gate counts as feasible -- assumptions that conflict with geometric, energetic, and thermodynamic realities. We introduce the family of \textit{realizable circuit classes} $\mathbf{RC}_d$, which model computation embedded in physical $d$-dimensional space. Each circuit in $\mathbf{RC}_d$ obeys conservative realizability laws: volume scales as $\mathcal{O}(t^d)$, cross-boundary information flux is bounded by $\mathcal{O}(t^{d-1})$ per unit time, and growth occurs through local, physically constructible edits. These bounds apply to all causal systems, classical or quantum. Within this framework, we show that algorithms with runtime $\omega(n^{d/(d-1)})$ cannot scale to inputs of maximal entropy, and that any $d$-dimensional parallel implementation offers at most a polynomial speed-up of degree $(d-1)$ over its optimal sequential counterpart. In the limit $d\to\infty$, $\mathbf{RC}_\infty(\mathrm{polylog})=\mathbf{NC}$, recovering classical parallelism as a non-physical idealization. By unifying geometry, causality, and information flow, $\mathbf{RC}_d$ extends circuit complexity into the physical domain, revealing universal scaling laws for computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19161v2</guid>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Prada, Ankur Mali</dc:creator>
    </item>
    <item>
      <title>Compression of Voxelized Vector Field Data by Boxes is Hard</title>
      <link>https://arxiv.org/abs/2510.20801</link>
      <description>arXiv:2510.20801v2 Announce Type: replace 
Abstract: Voxelized vector field data consists of a vector field over a high dimensional lattice. The lattice consists of integer coordinates called voxels. The voxelized vector field assigns a vector at each voxel. This data type encompasses images, tensors, and voxel data.
  Assume there is a nice energy function on the vector field. We consider the problem of lossy compression of voxelized vector field data in Shannon's rate-distortion framework. This means the data is compressed and then decompressed up to an error bound on the energy distortion at each voxel.
  Our first result is that under general conditions, lossy compression of voxelized vector fields is undecidable to compute. This is caused by having an infinite number of Euclidean vectors. We formulate this problem instead in terms of clustering the finite number of indices of a voxelized vector field by boxes. We call this problem the $(k,D)$-RectLossyVVFCompression problem.
  We show four main results about the $(k,D)$-RectLossyVVFCompression problem. The first is that it is decidable. The second is that decompression for this problem is polynomial time tractable. This means that the only obstruction to a tractable solution of the $(k,D)$-RectLossyVVFCompression problem lies in the compression stage. This is shown by the two hardness results about the compression stage. We show that the compression stage is NP-Hard to compute exactly and that it is even APX-Hard to approximate for $k,D\geq 2$.
  Assuming $P\neq NP$, this shows that when $k,D \geq 2$ there can be no exact polynomial time algorithm nor can there even be a PTAS approximation algorithm for the $(k,D)$-RectLossyVVFCompression problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20801v2</guid>
      <category>cs.CC</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Zhang</dc:creator>
    </item>
  </channel>
</rss>
