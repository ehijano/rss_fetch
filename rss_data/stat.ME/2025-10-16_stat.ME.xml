<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Oct 2025 01:51:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Escaping Neal's Funnel: a multi-stage sampling method for hierarchical models</title>
      <link>https://arxiv.org/abs/2510.12917</link>
      <description>arXiv:2510.12917v1 Announce Type: new 
Abstract: Neal's funnel refers to an exponential tapering in probability densities common to Bayesian hierarchical models. Usual sampling methods, such as Markov Chain Monte Carlo, struggle to efficiently sample the funnel. Reparameterizing the model or analytically marginalizing local parameters are common techniques to remedy sampling pathologies in distributions exhibiting Neal's funnel. In this paper, we show that the challenges of Neal's funnel can be avoided by performing the hierarchical analysis, well, hierarchically. That is, instead of sampling all parameters of the hierarchical model jointly, we break the sampling into multiple stages. The first stage samples a generalized (higher-dimensional) hierarchical model which is parameterized to lessen the sharpness of the funnel. The next stage samples from the estimated density of the first stage, but under a constraint which restricts the sampling to recover the marginal distributions on the hyper-parameters of the original (lower-dimensional) hierarchical model. A normalizing flow can be used to represent the distribution from the first stage, such that it can easily be sampled from for the second stage of the analysis. This technique is useful when effective reparameterizations are computationally expensive to calculate, or a generalized hierarchical model already exists from which it is easy to sample.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12917v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aiden Gundersen, Neil J. Cornish</dc:creator>
    </item>
    <item>
      <title>Learning Shared and Source-specific Subspaces across Multiple Data Sources for Functional Data</title>
      <link>https://arxiv.org/abs/2510.13010</link>
      <description>arXiv:2510.13010v1 Announce Type: new 
Abstract: In the era of big data, integrating multi-source functional data to extract a subspace that captures the shared subspace across sources has attracted considerable attention. In practice, data collection procedures often follow source-specific protocols. Directly averaging sample covariance operators across sources implicitly assumes homogeneity, which may bias the recovery of both shared and source-specific variation patterns. To address this issue, we propose a projection-based data integration method that explicitly separates the shared and source-specific subspaces. The method first estimates source-specific projection operators via smoothing to accommodate the nonparametric nature of functional data. The shared subspace is then isolated by examining the eigenvalues of the averaged projection operator across all sources. If a source-specific subspace is of interest, we re-project the associated source-specific covariance estimator onto the subspace orthogonal to the estimated shared subspace, and estimate the source-specific subspace from the resulting projection. We further establish the asymptotic properties of both the shared and source-specific subspace estimators. Extensive simulation studies demonstrate the effectiveness of the proposed method across a wide range of settings. Finally, we illustrate its practical utility with an example of air pollutant data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13010v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chi Zhang, Peijun Sang, Yingli Qin</dc:creator>
    </item>
    <item>
      <title>The $\phi$-PCA Framework: A Unified and Efficiency-Preserving Approach with Robust Variants</title>
      <link>https://arxiv.org/abs/2510.13159</link>
      <description>arXiv:2510.13159v1 Announce Type: new 
Abstract: Principal component analysis (PCA) is a fundamental tool in multivariate statistics, yet its sensitivity to outliers and limitations in distributed environments restrict its effectiveness in modern large-scale applications. To address these challenges, we introduce the $\phi$-PCA framework which provides a unified formulation of robust and distributed PCA. The class of $\phi$-PCA methods retains the asymptotic efficiency of standard PCA, while aggregating multiple local estimates using a proper $\phi$ function enhances ordering-robustness, leading to more accurate eigensubspace estimation under contamination. Notably, the harmonic mean PCA (HM-PCA), corresponding to the choice $\phi(u)=u^{-1}$, achieves optimal ordering-robustness and is recommended for practical use. Theoretical results further show that robustness increases with the number of partitions, a phenomenon seldom explored in the literature on robust or distributed PCA. Altogether, the partition-aggregation principle underlying $\phi$-PCA offers a general strategy for developing robust and efficiency-preserving methodologies applicable to both robust and distributed data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13159v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hung Hung, Zhi-Yu Jou, Su-Yun Huang, Shinto Eguchi</dc:creator>
    </item>
    <item>
      <title>Edgington's Method for Random-Effects Meta-Analysis Part II: Prediction</title>
      <link>https://arxiv.org/abs/2510.13216</link>
      <description>arXiv:2510.13216v1 Announce Type: new 
Abstract: Statistical inference about the average effect in random-effects meta-analysis has been considered insufficient in the presence of substantial between-study heterogeneity. Predictive distributions are well-suited for quantifying heterogeneity since they are interpretable on the effect scale and provide clinically relevant information about future events. We construct predictive distributions accounting for uncertainty through confidence distributions from Edgington's $p$-value combination method and the generalized heterogeneity statistic. Simulation results suggest that 95% prediction intervals typically achieve nominal coverage when more than three studies are available and effectively reflect skewness in effect estimates in scenarios with 20 or less studies. Formulations that ignore uncertainty in heterogeneity estimation typically fail to achieve correct coverage, underscoring the need for this adjustment in random-effects meta-analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13216v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Kronthaler, Leonhard Held</dc:creator>
    </item>
    <item>
      <title>Scalable Bayesian inference for high-dimensional mixed-type multivariate spatial data</title>
      <link>https://arxiv.org/abs/2510.13233</link>
      <description>arXiv:2510.13233v1 Announce Type: new 
Abstract: Spatial generalized linear mixed-effects methods are popularly used to model spatially indexed univariate responses. However, with modern technology, it is common to observe vector-valued mixed-type responses, e.g., a combination of binary, count, or continuous types, at each location. Methods that allow joint modeling of such mixed-type multivariate spatial responses are rare. Using latent multivariate Gaussian processes (GPs), we present a class of Bayesian spatial methods that can be employed for any combination of exponential family responses. Since multivariate GP-based methods can suffer from computational bottlenecks when the number of spatial locations is high, we further employ a computationally efficient Vecchia approximation for fast posterior inference and prediction. Key theoretical properties of the proposed model, such as identifiability and the structure of the induced covariance, are established. Our approach employs a Markov chain Monte Carlo-based inference method that utilizes elliptical slice sampling in a blocked Metropolis-within-Gibbs sampling framework. We illustrate the efficacy of the proposed method through simulation studies and a real-data application on joint modeling of wildfire counts and burnt areas across the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13233v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arghya Mukherjee, Arnab Hazra, Dootika Vats</dc:creator>
    </item>
    <item>
      <title>postcard: An R Package for Marginal Effect Estimation with or without Prognostic Score Adjustment</title>
      <link>https://arxiv.org/abs/2510.13347</link>
      <description>arXiv:2510.13347v1 Announce Type: new 
Abstract: Covariate adjustment is a widely used technique in randomized clinical trials (RCTs) for improving the efficiency of treatment effect estimators. By adjusting for predictive baseline covariates, variance can be reduced, enhancing statistical precision and study power. Rosenblum and van der Laan [2010] use the framework of generalized linear models (GLMs) in a plug-in analysis to show efficiency gains using covariate adjustment for marginal effect estimation. Recently the use of prognostic scores as adjustment covariates has gained popularity. Schuler et al. [2022] introduce and validate the method for continuous endpoints using linear models. Building on this work H{\o}jbjerre-Frandsen et al. [2025] extends the method proposed by Schuler et al. [2022] to be used in combination with the GLM plug-in procedure [Rosenblum and van der Laan, 2010]. This method achieves semi-parametric efficiency under assumptions of additive treatment effects on the link scale. Additionally, H{\o}jbjerre-Frandsen et al. [2025] provide a formula for power approximation which is valid even under model misspecification, enabling realistic sample size estimation. This article introduces an R package, which implements the GLM plug-in method with or without PrOgnoSTic CovARiate aDjustment, postcard. The package has two core features: (1) estimating marginal effects and the variance hereof (with or without prognostic adjustment) and (2) approximating statistical power. Functionalities also include integration of the Discrete Super Learner for constructing prognostic scores and simulation capabilities for exploring the methods in practice. Through examples and simulations, we demonstrate postcard as a practical toolkit for statisticians.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13347v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mathias Lerbech Jeppesen, Emilie H{\o}jbjerre-Frandsen</dc:creator>
    </item>
    <item>
      <title>A Flexible Partially Linear Single Index Proportional Hazards Regression Model for Multivariate Survival Data</title>
      <link>https://arxiv.org/abs/2510.13377</link>
      <description>arXiv:2510.13377v1 Announce Type: new 
Abstract: We address the problem of survival regression modelling with multivariate responses and nonlinear covariate effects. Our model extends the proportional hazards model by introducing several weakly-parametric elements: the marginal baseline hazard functions are expressed as piecewise constants, association is modelled with copulas, and nonlinear covariate effects are handled by a single-index structure using a spline. The model permits a full likelihood approach to inference, making it possible to obtain individual-level survival or hazard function estimates. Performance of the new model is evaluated through simulation studies and application to the Busselton health study data. The results suggest that the proposed method can capture nonlinear covariate effects well, and that there is benefit to modeling the association between the correlated responses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13377v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Na Lei, Mark A. Wolters, Wenqing He</dc:creator>
    </item>
    <item>
      <title>Understanding and Using the Relative Importance Measures Based on Orthonormality Transformation</title>
      <link>https://arxiv.org/abs/2510.13389</link>
      <description>arXiv:2510.13389v1 Announce Type: new 
Abstract: A class of relative importance measures based on orthonormality transformation (OTMs), has been found to effectively approximate the General Dominance index (GD). In particular, Johnson's Relative Weight (RW) has been deemed the most successful OTM in the literature. Nevertheless, the theoretical foundation of the OTMs remains unclear. To further understand the OTMs, we provide a generalized framework that breaks down the OTM into two functional steps: orthogonalization and reallocation. To assess the impact of each step on the performance of OTMs, we conduct extensive Monte Carlo simulations under various predictors' correlation structures and response variable distributions. Our findings reveal that Johnson's minimal transformation consistently outperforms other common orthogonalization methods. We also summarize the performance of reallocation methods under four scenarios of predictors' correlation structures in terms of the first principal component and the variance inflation factor (VIF). This analysis provides guidelines for selecting appropriate reallocation methods in different scenarios, illustrated with real-world dataset examples. Our research offers a deeper understanding of OTMs and provides valuable insights for practitioners seeking to accurately measure variable importance in various modeling contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13389v1</guid>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tien-En Chang, Argon Chen</dc:creator>
    </item>
    <item>
      <title>Non-asymptotic goodness-of-fit tests and model selection in valued stochastic blockmodels</title>
      <link>https://arxiv.org/abs/2510.13636</link>
      <description>arXiv:2510.13636v1 Announce Type: new 
Abstract: A valued stochastic blockmodel (SBM) is a general way to view networked data in which nodes are grouped into blocks and links between them are measured by counts or labels. This family allows for varying dyad sampling schemes, thereby including the classical, Poisson, and labeled SBMs, as well as those in which some edge observations are censored. This paper addresses the question of testing goodness-of-fit of such non-Bernoulli SBMs, focusing in particular on finite-sample tests. We derive explicit Markov bases moves necessary to generate samples from reference distributions and define goodness-of-fit statistics for determining model fit, comparable to those in the literature for related model families.
  For the labeled SBM, which includes in particular the censored-edge model, we study the asymptotic behavior of said statistics. One of the main purposes of testing goodness-of-fit of an SBM is to determine whether block membership of the nodes influences network formation. Power and Type 1 error rates are verified on simulated data. Additionally, we discuss the use of asymptotic results in selecting the number of blocks under the latent-block modeling assumption. The method derived for Poisson SBM is applied to ecological networks of host-parasite interactions. Our data analysis conclusions differ in selecting the number of blocks for the species from previous results in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13636v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>F\'elix Almendra-Hern\'andez, Miles Bakenhus, Vishesh Karwa, Mitsunori Ogawa, Sonja Petrovi\'c</dc:creator>
    </item>
    <item>
      <title>Exact Coordinate Descent for High-Dimensional Regularized Huber Regression</title>
      <link>https://arxiv.org/abs/2510.13715</link>
      <description>arXiv:2510.13715v1 Announce Type: new 
Abstract: We develop an exact coordinate descent algorithm for high-dimensional regularized Huber regression. In contrast to composite gradient descent methods, our algorithm fully exploits the advantages of coordinate descent when the underlying model is sparse. Moreover, unlike existing second-order approximation methods previously introduced in the literature, it remains effective even when the Hessian becomes ill-conditioned due to high correlations among covariates drawn from heavy-tailed distributions. The key idea is that, for each coordinate, marginal increments arise only from inlier observations, while the derivatives remain monotonically increasing over a grid constructed from the partial residuals. Building on conventional coordinate descent strategies, we further propose variable screening rules that selectively determine which variables to update at each iteration, thereby accelerating convergence. To the best of our knowledge, this is the first work to develop a first-order coordinate descent algorithm for penalized Huber loss minimization. We bound the nonasymptotic convergence rate of the proposed algorithm by extending arguments developed for the Lasso and formally characterize the operation of the proposed screening rule. Extensive simulation studies under heavy-tailed and highly-correlated predictors, together with a real data application, demonstrate both the practical efficiency of the method and the benefits of the computational enhancements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13715v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Younghoon Kim, Po-Ling Loh, Sumanta Basu</dc:creator>
    </item>
    <item>
      <title>Beyond Returns: A Candlestick-Based Approach to Spot Covariance Estimation</title>
      <link>https://arxiv.org/abs/2510.12911</link>
      <description>arXiv:2510.12911v1 Announce Type: cross 
Abstract: Spot covariance estimation is commonly based on high-frequency open-to-close return data over short time windows, but such approaches face a trade-off between statistical accuracy and localization. In this paper, I introduce a new estimation framework using high-frequency candlestick data, which include open, high, low, and close prices, effectively addressing this trade-off. By exploiting the information contained in candlesticks, the proposed method improves estimation accuracy relative to benchmarks while preserving local structure. I further develop a test for spot covariance inference based on candlesticks that demonstrates reasonable size control and a notable increase in power, particularly in small samples. Motivated by recent work in the finance literature, I empirically test the market neutrality of the iShares Bitcoin Trust ETF (IBIT) using 1-minute candlestick data for the full year of 2024. The results show systematic deviations from market neutrality, especially in periods of market stress. An event study around FOMC announcements further illustrates the new method's ability to detect subtle shifts in response to relatively mild information events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12911v1</guid>
      <category>econ.EM</category>
      <category>q-fin.RM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yasin Simsek</dc:creator>
    </item>
    <item>
      <title>Trajectory-based real-time pedestrian crash prediction at intersections: A novel non-linear link function for block maxima led Bayesian GEV framework addressing heterogeneous traffic condition</title>
      <link>https://arxiv.org/abs/2510.12963</link>
      <description>arXiv:2510.12963v1 Announce Type: cross 
Abstract: This study develops a real-time framework for estimating pedestrian crash risk at signalized intersections under heterogeneous, non-lane-based traffic. Existing approaches often assume linear relationships between covariates and parameters, oversimplifying the complex, non-monotonic interactions among different road users. To overcome this, the framework introduces a non-linear link function within a Bayesian generalized extreme value (GEV) structure to capture traffic variability more accurately. The framework applies extreme value theory through the block maxima approach using post-encroachment time as a surrogate safety measure. A hierarchical Bayesian model incorporating both linear and non-linear link functions into GEV parameters is estimated using Markov Chain Monte Carlo simulation. It also introduces a behavior-normalized Modified Crash Risk (MRC) formula to account for pedestrians' habitual risk-taking behavior. Seven Bayesian hierarchical models were developed and compared using deviance information criterion. Models employing non-linear link functions for the location and scale parameters significantly outperformed their linear counterparts. The results revealed that pedestrian speed has a negative relationship with crash risk, while flow and speed of motorized vehicles, pedestrian flow, and non-motorized vehicles conflicting speed contribute positively. The MRC formulation reduced overestimation and provided crash predictions with 93% confidence. The integration of non-linear link functions enhances model flexibility, capturing the non-linear nature of traffic extremes. The proposed MRC metric aligns crash risk estimates with real-world pedestrian behavior in mixed-traffic environments. This framework offers a practical analytical tool for traffic engineers and planners to design adaptive signal control and pedestrian safety interventions before crashes occur.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12963v1</guid>
      <category>stat.AP</category>
      <category>physics.soc-ph</category>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parvez Anowar, Nazmul Haque, Md Asif Raihan, Md Hadiuzzaman</dc:creator>
    </item>
    <item>
      <title>Simplicial Gaussian Models: Representation and Inference</title>
      <link>https://arxiv.org/abs/2510.12983</link>
      <description>arXiv:2510.12983v1 Announce Type: cross 
Abstract: Probabilistic graphical models (PGMs) are powerful tools for representing statistical dependencies through graphs in high-dimensional systems. However, they are limited to pairwise interactions. In this work, we propose the simplicial Gaussian model (SGM), which extends Gaussian PGM to simplicial complexes. SGM jointly models random variables supported on vertices, edges, and triangles, within a single parametrized Gaussian distribution. Our model builds upon discrete Hodge theory and incorporates uncertainty at every topological level through independent random components. Motivated by applications, we focus on the marginal edge-level distribution while treating node- and triangle-level variables as latent. We then develop a maximum-likelihood inference algorithm to recover the parameters of the full SGM and the induced conditional dependence structure. Numerical experiments on synthetic simplicial complexes with varying size and sparsity confirm the effectiveness of our algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12983v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Marinucci, Gabriele D'Acunto, Paolo Di Lorenzo, Sergio Barbarossa</dc:creator>
    </item>
    <item>
      <title>DeepCausalMMM: A Deep Learning Framework for Marketing Mix Modeling with Causal Inference</title>
      <link>https://arxiv.org/abs/2510.13087</link>
      <description>arXiv:2510.13087v1 Announce Type: cross 
Abstract: Marketing Mix Modeling (MMM) is a statistical technique used to estimate the impact of marketing activities on business outcomes such as sales, revenue, or customer visits. Traditional MMM approaches often rely on linear regression or Bayesian hierarchical models that assume independence between marketing channels and struggle to capture complex temporal dynamics and non-linear saturation effects [@Hanssens2005; @Ng2021Bayesian].
  DeepCausalMMM is a Python package that addresses these limitations by combining deep learning, causal inference, and advanced marketing science. The package uses Gated Recurrent Units (GRUs) to automatically learn temporal patterns such as adstock (carryover effects) and lag, while simultaneously learning statistical dependencies and potential causal structures between marketing channels through Directed Acyclic Graph (DAG) learning [@Zheng2018NOTEARS; @Gong2024CausalMMM]. Additionally, it implements Hill equation-based saturation curves to model diminishing returns and optimize budget allocation.
  Key innovations include: (1) a data-driven design where hyperparameters and transformations (e.g., adstock decay, saturation curves) are learned or estimated from data with sensible defaults, rather than requiring fixed heuristics or manual specification, (2) multi-region modeling with both shared and region-specific parameters, (3) robust statistical methods including Huber loss and advanced regularization, (4) comprehensive response curve analysis for understanding channel saturation, and (5) an extensive visualization suite with 14+ interactive dashboards for business insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13087v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Puttaparthi Tirumala</dc:creator>
    </item>
    <item>
      <title>Nonparametric Identification of Spatial Treatment Effect Boundaries: Evidence from Bank Branch Consolidation</title>
      <link>https://arxiv.org/abs/2510.13148</link>
      <description>arXiv:2510.13148v1 Announce Type: cross 
Abstract: I develop a nonparametric framework for identifying spatial boundaries of treatment effects without imposing parametric functional form restrictions. The method employs local linear regression with data-driven bandwidth selection to flexibly estimate spatial decay patterns and detect treatment effect boundaries. Monte Carlo simulations demonstrate that the nonparametric approach exhibits lower bias and correctly identifies the absence of boundaries when none exist, unlike parametric methods that may impose spurious spatial patterns. I apply this framework to bank branch openings during 2015--2020, matching 5,743 new branches to 5.9 million mortgage applications across 14,209 census tracts. The analysis reveals that branch proximity significantly affects loan application volume (8.5\% decline per 10 miles) but not approval rates, consistent with branches stimulating demand through local presence while credit decisions remain centralized. Examining branch survival during the digital transformation era (2010--2023), I find a non-monotonic relationship with area income: high-income areas experience more closures despite conventional wisdom. This counterintuitive pattern reflects strategic consolidation of redundant branches in over-banked wealthy urban areas rather than discrimination against poor neighborhoods. Controlling for branch density, urbanization, and competition, the direct income effect diminishes substantially, with branch density emerging as the primary determinant of survival. These findings demonstrate the necessity of flexible nonparametric methods for detecting complex spatial patterns that parametric models would miss, and challenge simplistic narratives about banking deserts by revealing the organizational complexity underlying spatial consolidation decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13148v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Model-assisted estimation for MRV: How to boost the economics of SOC sequestration projects without compromising on scientific integrity</title>
      <link>https://arxiv.org/abs/2510.13609</link>
      <description>arXiv:2510.13609v1 Announce Type: cross 
Abstract: Soil organic carbon (SOC) sequestration projects require unbiased, precise and cost-effective Monitoring, Reporting, and Verification (MRV) systems that balance sampling costs against uncertainty deductions imposed by regulatory frameworks. Design-based estimators guarantee unbiasedness but cannot exploit auxiliary data. Model-based approaches (VCS Methodology VT0014 v1.0 (2025)) can improve precision but require independent validation for each project. Model-assisted estimation offers a robust compromise, combining model predictions with probability sampling to retain design-based guarantees while improving precision. We evaluate the scientific integrity and efficiency of the simple regression estimator (SRE), a well-known model-assisted estimator, via an extensive simulation study. Our simulations span diverse SOC stock variances, sample sizes, and model performances. We assess three core properties: empirical bias, empirical confidence interval coverage, and precision gain relative to the design-based Horvitz-Thompson estimator (HTE). Results show negligible bias and valid coverage probabilities for n &gt; 40, regardless of SOC stock variance. Below this threshold, variance approximations and normality assumptions yield unreliable uncertainty estimates. With correlated ancillary variables (r^2 = 0.3), SRE achieves 30% precision gains over HTE. With uncorrelated variables, no gains are observed, but performance converges to HTE for n &gt;= 40. Model-assisted estimation can enhance project economics without compromising scientific rigor. Regulators should permit such estimators while mandating minimum sample size thresholds. Project proponents should routinely employ such estimators when correlated ancillary variables exist. The industry should prioritize the retrieval of high-quality, project-specific covariates to maximize precision gains and thereby the project economics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13609v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ahmad Awad, Erik Scharw\"achter</dc:creator>
    </item>
    <item>
      <title>Hierarchical Bayesian Modeling of Dengue in Recife, Brazil (2015-2024): The Role of Spatial Granularity and Data Quality for Epidemiological Risk Mapping</title>
      <link>https://arxiv.org/abs/2510.13672</link>
      <description>arXiv:2510.13672v1 Announce Type: cross 
Abstract: Dengue remains one of Brazil's major epidemiological challenges, marked by strong intra-urban inequalities and the influence of climatic and socio-environmental factors. This study analyzed confirmed dengue cases in Recife from 2015 to 2024 using a Bayesian hierarchical spatio-temporal model implemented in R-INLA, combining a BYM2 spatial structure with an RW1 temporal component. Covariates included population density, household size, income, drainage channels, lagged precipitation, and mean temperature. Population density and household size had positive effects on dengue risk, while income and channel presence were protective. Lagged precipitation increased risk, and higher temperatures showed an inverse association, suggesting thermal thresholds for vector activity. The model achieved good fit (DIC=65817; WAIC=64506) and stable convergence, with moderate residual spatial autocorrelation (phi=0.06) and a smooth temporal trend between 2016 and 2019. Spatio-temporal estimates revealed persistent high-risk clusters in northern and western Recife, overlapping with areas of higher density and social vulnerability. Beyond reproducing historical patterns, the Bayesian model supports probabilistic forecasting and early warning systems. Compared with classical models (GLM, SAR, GWR, GTWR), INLA explicitly integrates uncertainty and spatial-temporal dependence, offering credible interval inference for decision-making in urban health management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13672v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc\'ilio Ferreira dos Santos, Andreza dos Santos Rodrigues de Melo</dc:creator>
    </item>
    <item>
      <title>Multilevel Control Functional</title>
      <link>https://arxiv.org/abs/2305.12996</link>
      <description>arXiv:2305.12996v3 Announce Type: replace 
Abstract: Control variates are variance reduction techniques for Monte Carlo estimators. They play a critical role in improving Monte Carlo estimators in scientific and machine learning applications that involve computationally expensive integrals. We introduce multilevel control functionals (MLCFs), a novel and widely applicable extension of control variates that combines non-parametric Stein-based control variates with multi-fidelity methods. We show that when the integrand and the density are smooth, and when the dimensionality is not very high, MLCFs enjoy a faster convergence rate. We provide both theoretical analysis and empirical assessments on differential equation examples, including Bayesian inference for ecological models, to demonstrate the effectiveness of our proposed approach. Furthermore, we extend MLCFs for variational inference, and demonstrate improved performance empirically through Bayesian neural network examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.12996v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaiyu Li, Yiming Yang, Xiaoyuan Cheng, Yi He, Zhuo Sun</dc:creator>
    </item>
    <item>
      <title>Quantifying Uncertainty: All We Need is the Bootstrap?</title>
      <link>https://arxiv.org/abs/2403.20182</link>
      <description>arXiv:2403.20182v3 Announce Type: replace 
Abstract: A critical literature review and comprehensive simulation study is used to show that (a) non-parametric bootstrap is a viable alternative to commonly taught and used methods in basic estimation tasks (mean, variance, quartiles, correlation) and (b), contrary to recommendations in most related work, double bootstrap performs better than BCa. Quantifying uncertainty through standard errors, confidence intervals, hypothesis tests, and related measures is a fundamental aspect of statistical practice. However, these techniques involve a variety of methods, mathematical formulas, and underlying concepts, which can be complex. Could the non-parametric bootstrap, known for its simplicity and general applicability, serve as a universal alternative? This paper addresses this question through a review of the existing literature and a simulation analysis of one- and two-sided confidence intervals across varying sample sizes, confidence levels, data-generating processes, and statistical functionals. Results show that the double bootstrap consistently performs best and is a promising alternative to traditional methods used for common statistical tasks. These results suggest that the bootstrap, particularly the double bootstrap, could simplify statistical education and practice without compromising effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20182v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ur\v{s}a Zrim\v{s}ek, Erik \v{S}trumbelj</dc:creator>
    </item>
    <item>
      <title>Steady Continuous Monitoring is (Just Barely) Impossible for Tests of Unbounded Length</title>
      <link>https://arxiv.org/abs/2408.02821</link>
      <description>arXiv:2408.02821v2 Announce Type: replace 
Abstract: AB testing evaluates the difference between a control and a treatment in a statistically rigorous manner. Continuous monitoring allows statistical evaluation of an AB test as it proceeds. One goal of continuous monitoring is early stopping -- confirming a statistically significant difference between control and treatment as soon as possible. Another goal is to maintain some statistical capability to discover significant differences later in the test if they cannot be confirmed earlier. These goals are in conflict -- looser requirements for early stopping leave us with more stringent ones for later. This paper shows that it is impossible to maintain a constant requirement for significance for tests that have no a priori stopping time, but we can come arbitrarily close to that goal by using tests that require repeated significant results to con rm statistically significant differences between treatment and control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02821v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Bax, Alex Shtoff</dc:creator>
    </item>
    <item>
      <title>Autoregressive models for panel data causal inference with application to state-level opioid policies</title>
      <link>https://arxiv.org/abs/2408.09012</link>
      <description>arXiv:2408.09012v2 Announce Type: replace 
Abstract: Motivated by the study of state opioid policies, we propose a novel approach that uses autoregressive models for causal effect estimation in settings with panel data and staggered treatment adoption. Specifically, we seek to estimate the impact of key opioid-related policies by quantifying the effects of must access prescription drug monitoring programs (PDMPs), naloxone access laws (NALs), and medical marijuana laws on opioid prescribing. Existing methods, such as differences-in-differences and synthetic controls, are challenging to apply in these types of dynamic policy landscapes where multiple policies are implemented over time and sample sizes are small. Autoregressive models are an alternative strategy that have been used to estimate policy effects in similar settings, but until this paper have lacked formal justification. We outline a set of assumptions that tie these models to causal effects, and we study biases of estimates based on this approach when key causal assumptions are violated. In a set of simulation studies that mirror the structure of our application, we show that our proposed estimators frequently outperform existing estimators. In short, we justify the use of autoregressive models to evaluate the effectiveness of four state policies in combating the opioid crisis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09012v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph Antonelli, Max Rubinstein, Denis Agniel, Rosanna Smart, Elizabeth Stuart, Matthew Cefalu, Terry Schell, Joshua Eagan, Elizabeth Stone, Max Griswold, Beth Ann Griffin</dc:creator>
    </item>
    <item>
      <title>Modeling Dependence in Omics Association Analysis via Structured Co-Expression Networks to Improve Power and Replicability</title>
      <link>https://arxiv.org/abs/2504.20431</link>
      <description>arXiv:2504.20431v2 Announce Type: replace 
Abstract: Accounting for dependence among high-dimensional variables in omics data analysis is critical to obtain accurate and reliable statistical inference. Although latent, omics variables often exhibit structured correlation/co-expression patterns. However, there are few methods explicitly accounting for such structured dependence in the statistical analysis of omics data (e.g., differential expression analysis). To address this methodological gap, we propose a Co-expression network multivariate Regression (CoReg), which integrates co-expression network structure into multivariate regression analysis to precisely account for the inter-correlations (dependence) among omics variables. We show in simulations that CoReg substantially improves the accuracy of statistical inference and replicability across studies. These findings suggest that CoReg provides an alternative approach for omics data association analysis with dependence adjustment, analogous to the role of mixed-effects models in handling repeated measures in lower-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20431v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hwiyoung Lee, Yezhi Pan, Shuo Chen</dc:creator>
    </item>
    <item>
      <title>Bayesian adaptive randomization in the I-SPY2 sequential multiple assignment randomized trial</title>
      <link>https://arxiv.org/abs/2505.16047</link>
      <description>arXiv:2505.16047v2 Announce Type: replace 
Abstract: The I-SPY2 phase 2 clinical trial is a long-running platform trial
  that evaluates neoadjuvant treatments for locally advanced breast
  cancer, assigning subjects to novel agents using response-adaptive
  randomization. Recently, I-SPY2 was reconfigured as a sequential
  multiple assignment randomized trial (SMART), with up to three
  stages of therapy. At the first stage, a subject is assigned to a
  tumor-subtype-specific therapy. If the subject fails to show a
  satisfactory response to the initial therapy, the subject is
  assigned to a second subtype-specific therapy, and receives a third,
  rescue therapy if response is still not achieved. The I-SPY2 SMART
  thus evaluates entire treatment regimes that recommend therapies at
  every stage if needed. The transition of I-SPY2 to a SMART required
  development of a response-adaptive randomization scheme that updates
  randomization probabilities at each stage, aligned with the goal of
  maximizing the number of subjects who achieve a pathological
  complete response (pCR). We present the details of our novel
  approach, which uses Thompson sampling to update randomization
  probabilities based on the posterior probability that treatments are
  part of the optimal regime. Empirical studies that demonstrate that
  it improves within-trial regime-specific pCR rates and recommends
  optimal regimes at rates similar to uniform, nonadaptive
  randomization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16047v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Norwood, Christina Yau, Denise Wolf, Philip Beineke, Andrew Chapple, Anastasios Tsiatis, Marie Davidian</dc:creator>
    </item>
    <item>
      <title>Coefficient Shape Transfer Learning for Functional Linear Regression</title>
      <link>https://arxiv.org/abs/2506.11367</link>
      <description>arXiv:2506.11367v4 Announce Type: replace 
Abstract: The shapes of functions provide highly interpretable summaries of their trajectories. This article develops a novel transfer learning methodology to tackle the challenge of data scarcity in functional linear models. The methodology incorporates samples from the target model (target domain) alongside those from auxiliary models (source domains), transferring knowledge of coefficient shape from the source domains to the target domain. This shape-based transfer learning framework enhances robustness and generalizability: by being invariant to covariate scaling and signal strength, it ensures reliable knowledge transfer even when data from different sources differ in magnitude, and by formalizing the notion of coefficient shape homogeneity, it extends beyond traditional coefficient-equality assumptions to incorporate information from a broader range of source domains. We rigorously analyze the convergence rates of the proposed estimator and examine the minimax optimality. Our findings show that the degree of improvement depends not only on the similarity of coefficient shapes between the target and source domains, but also on coefficient magnitudes and the spectral decay rates of the functional covariates covariance operators. To address situations where only a subset of auxiliary models is informative for the target model, we further develop a data-driven procedure for identifying such informative sources. The effectiveness of the proposed methodology is demonstrated through comprehensive simulation studies and an application to occupation time analysis using physical activity data from the U.S. National Health and Nutrition Examination Survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11367v4</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuhao Jiao, Ian W. Mckeague</dc:creator>
    </item>
    <item>
      <title>Effect Identification and Unit Categorization in the Multi-Score Regression Discontinuity Design with Application to LED Manufacturing</title>
      <link>https://arxiv.org/abs/2508.15692</link>
      <description>arXiv:2508.15692v2 Announce Type: replace 
Abstract: RDD (Regression discontinuity design) is a widely used framework for identifying and estimating causal effects at the cutoff of a single running variable. In practice, however, decision-making often involves multiple thresholds and criteria, especially in production systems. Standard MRD (multi-score RDD) methods address this complexity by reducing the problem to a one-dimensional design. This simplification allows existing approaches to be used to identify and estimate causal effects, but it can introduce non-compliance by misclassifying units relative to the original cutoff rules. We develop theoretical tools to detect and reduce "fuzziness" when estimating the cutoff effect for units that comply with individual subrules of a multi-rule system. In particular, we propose a formal definition and categorization of unit behavior types under multi-dimensional cutoff rules, extending standard classifications of compliers, alwaystakers, and nevertakers, and incorporating defiers and indecisive units. We further identify conditions under which cutoff effects for compliers can be estimated in multiple dimensions, and establish when identification remains valid after excluding nevertakers and alwaystakers. In addition, we examine how decomposing complex Boolean cutoff rules (such as AND- and OR-type rules) into simpler components affects the classification of units into behavioral types and improves estimation by making it possible to identify and remove non-compliant units more accurately. We validate our framework using both semi-synthetic simulations calibrated to production data and real-world data from opto-electronic semiconductor manufacturing. The empirical results demonstrate that our approach has practical value in refining production policies and reduces estimation variance. This underscores the usefulness of the MRD framework in manufacturing contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15692v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Alexander Schwarz, Oliver Schacht, Sven Klaassen, Johannes Oberpriller, Martin Spindler</dc:creator>
    </item>
    <item>
      <title>Deep learning based doubly robust test for Granger causality</title>
      <link>https://arxiv.org/abs/2509.15798</link>
      <description>arXiv:2509.15798v2 Announce Type: replace 
Abstract: Granger causality is popular for analyzing time series data in many applications from natural science to social science including genomics, neuroscience, economics, and finance. Consequently, the Granger causality test has become one of the main concerns of the econometrician for decades. Taking advantage of the theoretical breakthroughs in deep learning in recent years, we propose a doubly robust Granger causality test (DRGCT). Our method offers several key advantages. The first and most direct benefit is for the users, DRGCT allows them to handle large lag orders while alleviating the curse of dimensionality that traditional nonlinear Granger causality tests usually face. Second, introducing a doubly robust test statistic for time series based on neural networks that achieves a parametric convergence rate not only suggests a new paradigm for nonparametric inference in econometrics, but also broadens the application scope of deep learning. Third, a multiplier bootstrap method, combined with the doubly robust approach, provides an efficient way to obtain critical values, effectively reducing computational time and avoiding redundant calculations. We prove that the test asymptotically controls the type I error, while achieving power approaches one, and validate the effectiveness of our test through numerical simulations. In real data analysis, we apply DRGCT to revisit the price-volume relationship problem in the stock markets of America, China, and Japan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15798v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongchang Hui, Chijin Liu, Xiaojun Song</dc:creator>
    </item>
    <item>
      <title>Identification and Estimation of Joint Potential Outcome Distributions from a Single Study</title>
      <link>https://arxiv.org/abs/2509.20506</link>
      <description>arXiv:2509.20506v3 Announce Type: replace 
Abstract: Most causal inference methods focus on estimating marginal average treatment effects, but many important causal estimands depend on the joint distribution of potential outcomes, including the probability of causation and proportions benefiting from or harmed by treatment. Wu et al (2025) recently established nonparametric identification of this joint distribution for categorical outcomes under binary treatment by leveraging variation across multiple studies. We demonstrate that their multi-study framework can be implemented within a single study by using a baseline covariate that is associated with untreated potential outcomes but does not modify treatment effects conditional on those outcomes. This reframing substantially broadens the practical applicability of their results, as it eliminates the need for multiple independent datasets and gives analysts control over covariate selection to satisfy key identifying assumptions. We provide complete identification and estimation theory for the single-study setting, including a Neyman-orthogonal estimator for cases where the conditional independence assumption only holds after adjusting for covariates. However, we argue that only in unusual settings would it be even theoretically possible for the identifying assumptions to hold exactly, making sensitivity analysis particularly important. We validate the estimator in a simulation and apply it to data from a large field experiment assessing the effect of mailings on voter turnout.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20506v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zach Shahn, David Madigan</dc:creator>
    </item>
    <item>
      <title>Interventional Processes for Causal Uncertainty Quantification</title>
      <link>https://arxiv.org/abs/2410.14483</link>
      <description>arXiv:2410.14483v2 Announce Type: replace-cross 
Abstract: Reliable uncertainty quantification for causal effects is crucial in various applications, but remains difficult in nonparametric models, particularly for continuous treatments. We introduce IMPspec, a Gaussian process (GP) framework for modeling uncertainty over interventional causal functions under continuous treatments, which can be represented using reproducing Kernel Hilbert Spaces (RKHSs). By using principled function class expansions and a spectral representation of RKHS features, IMPspec yields tractable training and inference, a spectral algorithm to calibrate posterior credible intervals, and avoids the underfitting and variance collapse pathologies of earlier GP-on-RKHS methods. Across synthetic benchmarks and an application in healthcare, IMPspec delivers state-of-the-art performance in causal uncertainty quantification and downstream causal Bayesian optimization tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14483v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugh Dance, Peter Orbanz, Arthur Gretton</dc:creator>
    </item>
    <item>
      <title>Accounting for Missing Data in Public Health Research Using a Synthesis of Statistical and Mathematical Models</title>
      <link>https://arxiv.org/abs/2503.02789</link>
      <description>arXiv:2503.02789v3 Announce Type: replace-cross 
Abstract: Introduction: Accounting for missing data by imputing or weighting conditional on covariates relies on the variable with missingness being observed at least some of the time for all unique covariate values. This requirement is referred to as positivity and positivity violations can result in bias. Here, we review a novel approach to addressing positivity violations in the context of systolic blood pressure. Methods: To illustrate the proposed approach, we estimate the mean systolic blood pressure among children and adolescents aged 2-17 years old in the United States using data from the 2017-2018 National Health and Nutrition Examination Survey (NHANES). As blood pressure was not measured for those aged 2-7, there exists a positivity violation by design. Using a recently proposed synthesis of statistical and mathematical models, we integrate external information with NHANES to address our motivating question. Results: With the synthesis model, the estimated mean systolic blood pressure was 100.5 (95% confidence interval: 99.9, 101.0), which is notably lower than either a complete-case analysis or extrapolation from a statistical model. The synthesis results were supported by a diagnostic comparing the performance of the mathematical model in the positive region. Discussion: Positivity violations pose a threat to quantitative medical research, and standard approaches to addressing nonpositivity rely on restrictive untestable assumptions. Using a synthesis model, like the one detailed here, offers a viable alternative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02789v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul N Zivich, Bonnie E Shook-Sa, Stephen R Cole, Eric T Lofgren, Jessie K Edwards</dc:creator>
    </item>
    <item>
      <title>Conditional Distribution Compression via the Kernel Conditional Mean Embedding</title>
      <link>https://arxiv.org/abs/2504.10139</link>
      <description>arXiv:2504.10139v3 Announce Type: replace-cross 
Abstract: Existing distribution compression methods, like Kernel Herding (KH), were originally developed for unlabelled data. However, no existing approach directly compresses the conditional distribution of labelled data. To address this gap, we first introduce the Average Maximum Conditional Mean Discrepancy (AMCMD), a natural metric for comparing conditional distributions. We then derive a consistent estimator for the AMCMD and establish its rate of convergence. Next, we make a key observation: in the context of distribution compression, the cost of constructing a compressed set targeting the AMCMD can be reduced from $\mathcal{O}(n^3)$ to $\mathcal{O}(n)$. Building on this, we extend the idea of KH to develop Average Conditional Kernel Herding (ACKH), a linear-time greedy algorithm that constructs a compressed set targeting the AMCMD. To better understand the advantages of directly compressing the conditional distribution rather than doing so via the joint distribution, we introduce Joint Kernel Herding (JKH), a straightforward adaptation of KH designed to compress the joint distribution of labelled data. While herding methods provide a simple and interpretable selection process, they rely on a greedy heuristic. To explore alternative optimisation strategies, we propose Joint Kernel Inducing Points (JKIP) and Average Conditional Kernel Inducing Points (ACKIP), which jointly optimise the compressed set while maintaining linear complexity. Experiments show that directly preserving conditional distributions with ACKIP outperforms both joint distribution compression (via JKH and JKIP) and the greedy selection used in ACKH. Moreover, we see that JKIP consistently outperforms JKH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10139v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominic Broadbent, Nick Whiteley, Robert Allison, Tom Lovett</dc:creator>
    </item>
    <item>
      <title>Discretion in the Loop: Human Expertise in Algorithm-Assisted College Advising</title>
      <link>https://arxiv.org/abs/2505.13325</link>
      <description>arXiv:2505.13325v3 Announce Type: replace-cross 
Abstract: In higher education, many institutions use algorithmic alerts to flag at-risk students and deliver advising at scale. While much research has focused on evaluating algorithmic predictions, relatively little is known about how discretionary interventions by human experts shape outcomes in algorithm-assisted settings. We study this question using rich quantitative and qualitative data from a randomized controlled trial of an algorithm-assisted advising program at Georgia State University. Taking a mixed-methods approach, we examine whether and how advisors use context unavailable to an algorithm to guide interventions and influence student success. We develop a causal graphical framework for human expertise in the interventional setting, extending prior work on discretion in purely predictive settings. We then test a necessary condition for discretionary expertise using structured advisor logs and student outcomes data, identifying several interventions that meet the criterion for statistical significance. Accordingly, we estimate that 2 out of 3 interventions taken by advisors in the treatment arm were plausibly ``expertly targeted'' to students using non-algorithmic context. Systematic qualitative analysis of advisor notes corroborates these findings, showing a pattern of advisors incorporating diverse forms of contextual information--such as personal circumstances, financial issues, and student engagement--into their decisions. Finally, we document heterogeneity in advising styles, finding that one style elicits more holistic information about students and is associated with improved graduation rates. Our results offer theoretical and practical insight into the real-world effectiveness of algorithm-supported college advising, and underscore the importance of accounting for human expertise in the design, evaluation, and implementation of algorithmic decision systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13325v3</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kara Schechtman, Benjamin Brandon, Jenise Stafford, Hannah Li, Lydia T. Liu</dc:creator>
    </item>
    <item>
      <title>Robust estimation of heterogeneous treatment effects in randomized trials leveraging external data</title>
      <link>https://arxiv.org/abs/2507.03681</link>
      <description>arXiv:2507.03681v2 Announce Type: replace-cross 
Abstract: Randomized trials are typically designed to detect average treatment effects but often lack the statistical power to uncover individual-level treatment effect heterogeneity, limiting their value for personalized decision-making. To address this, we propose the QR-learner, a model-agnostic learner that estimates conditional average treatment effects (CATE) within the trial population by leveraging external data from other trials or observational studies. The proposed method is robust: it can reduce the mean squared error relative to a trial-only CATE learner, and is guaranteed to recover the true CATE even when the external data are not aligned with the trial. Moreover, we introduce a procedure that combines the QR-learner with a trial-only CATE learner and show that it asymptotically matches or exceeds both component learners in terms of mean squared error. We examine the performance of our approach in simulation studies and apply the methods to a real-world dataset, demonstrating improvements in both CATE estimation and statistical power for detecting heterogeneous effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03681v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rickard Karlsson, Piersilvio De Bartolomeis, Issa J. Dahabreh, Jesse H. Krijthe</dc:creator>
    </item>
    <item>
      <title>A coupling-based approach to f-divergences diagnostics for Markov chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2510.07559</link>
      <description>arXiv:2510.07559v2 Announce Type: replace-cross 
Abstract: A long-standing gap exists between the theoretical analysis of Markov chain Monte Carlo convergence, which is often based on statistical divergences, and the diagnostics used in practice. We introduce the first general convergence diagnostics for Markov chain Monte Carlo based on any f-divergence, allowing users to directly monitor, among others, the Kullback--Leibler and the $\chi^2$ divergences as well as the Hellinger and the total variation distances. Our first key contribution is a coupling-based `weight harmonization' scheme that produces a direct, computable, and consistent weighting of interacting Markov chains with respect to their target distribution. The second key contribution is to show how such consistent weightings of empirical measures can be used to provide upper bounds to f-divergences in general. We prove that these bounds are guaranteed to tighten over time and converge to zero as the chains approach stationarity, providing a concrete diagnostic. Numerical experiments demonstrate that our method is a practical and competitive diagnostic tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07559v2</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrien Corenflos, Hai-Dang Dau</dc:creator>
    </item>
  </channel>
</rss>
