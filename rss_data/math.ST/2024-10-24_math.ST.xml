<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Oct 2024 01:45:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Study of inaccuracy measures of record values</title>
      <link>https://arxiv.org/abs/2410.17658</link>
      <description>arXiv:2410.17658v1 Announce Type: new 
Abstract: In this paper, we investigate inaccuracy measures based on record values, focusing on the relationship between the distribution of the n-th upper and lower k-record values and the parent distribution. We extend the classical Kerridge inaccuracy measure, originally developed for comparing two distributions, to record values and derive expressions for both upper and lower record values. In addition, we explore various other inaccuracybased measures, such as cumulative residual inaccuracy, cumulative past inaccuracy, and extropy inaccuracy measures, and their applications in characterizing symmetric distributions. We compute these measures through illustrative examples for several well-known lifetime distributions, including the exponential, Pareto, and Weibull distributions. Our findings provide insights into how inaccuracy varies with record order and distribution parameters, contributing to a deeper understanding of information-theoretic measures applied to records.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17658v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Santosh Kumar Chaudhary, Nitin Gupta</dc:creator>
    </item>
    <item>
      <title>Asymptotics for parametric martingale posteriors</title>
      <link>https://arxiv.org/abs/2410.17692</link>
      <description>arXiv:2410.17692v1 Announce Type: new 
Abstract: The martingale posterior framework is a generalization of Bayesian inference where one elicits a sequence of one-step ahead predictive densities instead of the likelihood and prior. Posterior sampling then involves the imputation of unseen observables, and can then be carried out in an expedient and parallelizable manner using predictive resampling without requiring Markov chain Monte Carlo. Recent work has investigated the use of plug-in parametric predictive densities, combined with stochastic gradient descent, to specify a parametric martingale posterior. This paper investigates the asymptotic properties of this class of parametric martingale posteriors. In particular, two central limit theorems based on martingale limit theory are introduced and applied. The first is a predictive central limit theorem, which enables a significant acceleration of the predictive resampling scheme through a hybrid sampling algorithm based on a normal approximation. The second is a Bernstein-von Mises result, which is novel for martingale posteriors, and provides methodological guidance on attaining desirable frequentist properties. We demonstrate the utility of the theoretical results in simulations and a real data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17692v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edwin Fong, Andrew Yiu</dc:creator>
    </item>
    <item>
      <title>Limit Laws for Gromov-Wasserstein Alignment with Applications to Testing Graph Isomorphisms</title>
      <link>https://arxiv.org/abs/2410.18006</link>
      <description>arXiv:2410.18006v1 Announce Type: new 
Abstract: The Gromov-Wasserstein (GW) distance enables comparing metric measure spaces based solely on their internal structure, making it invariant to isomorphic transformations. This property is particularly useful for comparing datasets that naturally admit isomorphic representations, such as unlabelled graphs or objects embedded in space. However, apart from the recently derived empirical convergence rates for the quadratic GW problem, a statistical theory for valid estimation and inference remains largely obscure. Pushing the frontier of statistical GW further, this work derives the first limit laws for the empirical GW distance across several settings of interest: (i)~discrete, (ii)~semi-discrete, and (iii)~general distributions under moment constraints under the entropically regularized GW distance. The derivations rely on a novel stability analysis of the GW functional in the marginal distributions. The limit laws then follow by an adaptation of the functional delta method. As asymptotic normality fails to hold in most cases, we establish the consistency of an efficient estimation procedure for the limiting law in the discrete case, bypassing the need for computationally intensive resampling methods. We apply these findings to testing whether collections of unlabelled graphs are generated from distributions that are isomorphic to each other.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18006v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Rioux, Ziv Goldfeld, Kengo Kato</dc:creator>
    </item>
    <item>
      <title>Computing Optimal Regularizers for Online Linear Optimization</title>
      <link>https://arxiv.org/abs/2410.17336</link>
      <description>arXiv:2410.17336v1 Announce Type: cross 
Abstract: Follow-the-Regularized-Leader (FTRL) algorithms are a popular class of learning algorithms for online linear optimization (OLO) that guarantee sub-linear regret, but the choice of regularizer can significantly impact dimension-dependent factors in the regret bound. We present an algorithm that takes as input convex and symmetric action sets and loss sets for a specific OLO instance, and outputs a regularizer such that running FTRL with this regularizer guarantees regret within a universal constant factor of the best possible regret bound. In particular, for any choice of (convex, symmetric) action set and loss set we prove that there exists an instantiation of FTRL which achieves regret within a constant factor of the best possible learning algorithm, strengthening the universality result of Srebro et al., 2011.
  Our algorithm requires preprocessing time and space exponential in the dimension $d$ of the OLO instance, but can be run efficiently online assuming a membership and linear optimization oracle for the action and loss sets, respectively (and is fully polynomial time for the case of constant dimension $d$). We complement this with a lower bound showing that even deciding whether a given regularizer is $\alpha$-strongly-convex with respect to a given norm is NP-hard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17336v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khashayar Gatmiry, Jon Schneider, Stefanie Jegelka</dc:creator>
    </item>
    <item>
      <title>Concatenation of Markov processes</title>
      <link>https://arxiv.org/abs/2410.17384</link>
      <description>arXiv:2410.17384v1 Announce Type: cross 
Abstract: We investigate the concatenation of Markov processes. Our primary concern is to utilize processes constructed in this manner for Monte Carlo integration. To enable this using conventional methods, it is essential to demonstrate the Markov property and invariance with respect to a given target distribution. We provide mild sufficient conditions for this. Our main result is the identification of the generator of the concatenation of Markov processes. This result provides the theoretical foundation for Monte Carlo methods based on this construction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17384v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sascha Holl</dc:creator>
    </item>
    <item>
      <title>Sacred and Profane: from the Involutive Theory of MCMC to Helpful Hamiltonian Hacks</title>
      <link>https://arxiv.org/abs/2410.17398</link>
      <description>arXiv:2410.17398v1 Announce Type: cross 
Abstract: In the first edition of this Handbook, two remarkable chapters consider seemingly distinct yet deeply connected subjects ...</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17398v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan E. Glatt-Holtz, Andrew J. Holbrook, Justin A. Krometis, Cecilia F. Mondaini, Ami Sheth</dc:creator>
    </item>
    <item>
      <title>Extremal Eigenvalues of Random Kernel Matrices with Polynomial Scaling</title>
      <link>https://arxiv.org/abs/2410.17515</link>
      <description>arXiv:2410.17515v1 Announce Type: cross 
Abstract: We study the spectral norm of random kernel matrices with polynomial scaling, where the number of samples scales polynomially with the data dimension. In this regime, Lu and Yau (2022) proved that the empirical spectral distribution converges to the additive free convolution of a semicircle law and a Marcenko-Pastur law. We demonstrate that the random kernel matrix can be decomposed into a "bulk" part and a low-rank part. The spectral norm of the "bulk" part almost surely converges to the edge of the limiting spectrum.
  In the special case where the random kernel matrices correspond to the inner products of random tensors, the empirical spectral distribution converges to the Marcenko-Pastur law. We prove that the largest and smallest eigenvalues converge to the corresponding spectral edges of the Marcenko-Pastur law.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17515v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Kogan, Sagnik Nandy, Jiaoyang Huang</dc:creator>
    </item>
    <item>
      <title>On the lumpability of tree-valued Markov chains</title>
      <link>https://arxiv.org/abs/2410.17919</link>
      <description>arXiv:2410.17919v1 Announce Type: cross 
Abstract: Phylogenetic trees constitute an interesting class of objects for stochastic processes due to the non-standard nature of the space they inhabit. In particular, many statistical applications require the construction of Markov processes on the space of trees, whose cardinality grows superexponentially with the number of leaves considered. We investigate whether certain lower-dimensional projections of tree space preserve the Markov property in tree-valued Markov processes. We study exact lumpability of tree shapes and $\varepsilon$-lumpability of clades, exploiting the combinatorial structure of the SPR graph to obtain bounds on the lumping error under the random walk and Metropolis-Hastings processes. Finally, we show how to use these results in empirical investigation, leveraging exact and $\varepsilon$-lumpability to improve Monte Carlo estimation of tree-related quantities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17919v1</guid>
      <category>math.PR</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rodrigo B. Alves, Yuri F. Saporito, Luiz M. Carvalho</dc:creator>
    </item>
    <item>
      <title>Estimating the Spectral Moments of the Kernel Integral Operator from Finite Sample Matrices</title>
      <link>https://arxiv.org/abs/2410.17998</link>
      <description>arXiv:2410.17998v2 Announce Type: cross 
Abstract: Analyzing the structure of sampled features from an input data distribution is challenging when constrained by limited measurements in both the number of inputs and features. Traditional approaches often rely on the eigenvalue spectrum of the sample covariance matrix derived from finite measurement matrices; however, these spectra are sensitive to the size of the measurement matrix, leading to biased insights. In this paper, we introduce a novel algorithm that provides unbiased estimates of the spectral moments of the kernel integral operator in the limit of infinite inputs and features from finitely sampled measurement matrices. Our method, based on dynamic programming, is efficient and capable of estimating the moments of the operator spectrum. We demonstrate the accuracy of our estimator on radial basis function (RBF) kernels, highlighting its consistency with the theoretical spectra. Furthermore, we showcase the practical utility and robustness of our method in understanding the geometry of learned representations in neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17998v2</guid>
      <category>cs.LG</category>
      <category>math.SP</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chanwoo Chun, SueYeon Chung, Daniel D. Lee</dc:creator>
    </item>
    <item>
      <title>Deep Nonparametric Inference for Conditional Hazard Function</title>
      <link>https://arxiv.org/abs/2410.18021</link>
      <description>arXiv:2410.18021v1 Announce Type: cross 
Abstract: We propose a novel deep learning approach to nonparametric statistical inference for the conditional hazard function of survival time with right-censored data. We use a deep neural network (DNN) to approximate the logarithm of a conditional hazard function given covariates and obtain a DNN likelihood-based estimator of the conditional hazard function. Such an estimation approach renders model flexibility and hence relaxes structural and functional assumptions on conditional hazard or survival functions. We establish the nonasymptotic error bound and functional asymptotic normality of the proposed estimator. Subsequently, we develop new one-sample tests for goodness-of-fit evaluation and two-sample tests for treatment comparison. Both simulation studies and real application analysis show superior performances of the proposed estimators and tests in comparison with existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18021v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wen Su, Kin-Yat Liu, Guosheng Yin, Jian Huang, Xingqiu Zhao</dc:creator>
    </item>
    <item>
      <title>Bayes optimal learning in high-dimensional linear regression with network side information</title>
      <link>https://arxiv.org/abs/2306.05679</link>
      <description>arXiv:2306.05679v4 Announce Type: replace 
Abstract: Supervised learning problems with side information in the form of a network arise frequently in applications in genomics, proteomics and neuroscience. For example, in genetic applications, the network side information can accurately capture background biological information on the intricate relations among the relevant genes. In this paper, we initiate a study of Bayes optimal learning in high-dimensional linear regression with network side information. To this end, we first introduce a simple generative model (called the Reg-Graph model) which posits a joint distribution for the supervised data and the observed network through a common set of latent parameters. Next, we introduce an iterative algorithm based on Approximate Message Passing (AMP) which is provably Bayes optimal under very general conditions. In addition, we characterize the limiting mutual information between the latent signal and the data observed, and thus precisely quantify the statistical impact of the network side information. Finally, supporting numerical experiments suggest that the introduced algorithm has excellent performance in finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.05679v4</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sagnik Nandy, Subhabrata Sen</dc:creator>
    </item>
    <item>
      <title>A provable initialization and robust clustering method for general mixture models</title>
      <link>https://arxiv.org/abs/2401.05574</link>
      <description>arXiv:2401.05574v3 Announce Type: replace 
Abstract: Clustering is a fundamental tool in statistical machine learning in the presence of heterogeneous data. Most recent results focus primarily on optimal mislabeling guarantees when data are distributed around centroids with sub-Gaussian errors. Yet, the restrictive sub-Gaussian model is often invalid in practice since various real-world applications exhibit heavy tail distributions around the centroids or suffer from possible adversarial attacks that call for robust clustering with a robust data-driven initialization. In this paper, we present initialization and subsequent clustering methods that provably guarantee near-optimal mislabeling for general mixture models when the number of clusters and data dimensions are finite. We first introduce a hybrid clustering technique with a novel multivariate trimmed mean type centroid estimate to produce mislabeling guarantees under a weak initialization condition for general error distributions around the centroids. A matching lower bound is derived, up to factors depending on the number of clusters. In addition, our approach also produces similar mislabeling guarantees even in the presence of adversarial outliers. Our results reduce to the sub-Gaussian case in finite dimensions when errors follow sub-Gaussian distributions. To solve the problem thoroughly, we also present novel data-driven robust initialization techniques and show that, with probabilities approaching one, these initial centroid estimates are sufficiently good for the subsequent clustering algorithm to achieve the optimal mislabeling rates. Furthermore, we demonstrate that the Lloyd algorithm is suboptimal for more than two clusters even when errors are Gaussian and for two clusters when error distributions have heavy tails. Both simulated data and real data examples further support our robust initialization procedure and clustering algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.05574v3</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soham Jana, Jianqing Fan, Sanjeev Kulkarni</dc:creator>
    </item>
    <item>
      <title>Online Differentially Private Synthetic Data Generation</title>
      <link>https://arxiv.org/abs/2402.08012</link>
      <description>arXiv:2402.08012v3 Announce Type: replace 
Abstract: We present a polynomial-time algorithm for online differentially private synthetic data generation. For a data stream within the hypercube $[0,1]^d$ and an infinite time horizon, we develop an online algorithm that generates a differentially private synthetic dataset at each time $t$. This algorithm achieves a near-optimal accuracy bound of $O(\log(t)t^{-1/d})$ for $d\geq 2$ and $O(\log^{4.5}(t)t^{-1})$ for $d=1$ in the 1-Wasserstein distance. This result extends the previous work on the continual release model for counting queries to Lipschitz queries. Compared to the offline case, where the entire dataset is available at once, our approach requires only an extra polylog factor in the accuracy bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08012v3</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiyun He, Roman Vershynin, Yizhe Zhu</dc:creator>
    </item>
    <item>
      <title>Time Series, Persistent Homology and Chirality</title>
      <link>https://arxiv.org/abs/1909.09846</link>
      <description>arXiv:1909.09846v2 Announce Type: replace-cross 
Abstract: We investigate the point process of persistent diagram for Brownian motions with drift, obtaining some of its basic characteristics. Further we introduce and study the refinement of the persistent homology, assigning to each bar its chirality.</description>
      <guid isPermaLink="false">oai:arXiv.org:1909.09846v2</guid>
      <category>math.PR</category>
      <category>math.AT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuliy Baryshnikov</dc:creator>
    </item>
    <item>
      <title>Variational Causal Inference</title>
      <link>https://arxiv.org/abs/2209.05935</link>
      <description>arXiv:2209.05935v3 Announce Type: replace-cross 
Abstract: Estimating an individual's potential outcomes under counterfactual treatments is a challenging task for traditional causal inference and supervised learning approaches when the outcome is high-dimensional (e.g. gene expressions, impulse responses, human faces) and covariates are relatively limited. In this case, to construct one's outcome under a counterfactual treatment, it is crucial to leverage individual information contained in its observed factual outcome on top of the covariates. We propose a deep variational Bayesian framework that rigorously integrates two main sources of information for outcome construction under a counterfactual treatment: one source is the individual features embedded in the high-dimensional factual outcome; the other source is the response distribution of similar subjects (subjects with the same covariates) that factually received this treatment of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.05935v3</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>q-bio.GN</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yulun Wu, Layne C. Price, Zichen Wang, Vassilis N. Ioannidis, Robert A. Barton, George Karypis</dc:creator>
    </item>
    <item>
      <title>Geodesic slice sampling on the sphere</title>
      <link>https://arxiv.org/abs/2301.08056</link>
      <description>arXiv:2301.08056v3 Announce Type: replace-cross 
Abstract: Probability measures on the sphere form an important class of statistical models and are used, for example, in modeling directional data or shapes. Due to their widespread use, but also as an algorithmic building block, efficient sampling of distributions on the sphere is highly desirable. We propose a shrinkage based and an idealized geodesic slice sampling Markov chain, designed to generate approximate samples from distributions on the sphere. In particular, the shrinkage-based version of the algorithm can be implemented such that it runs efficiently in any dimension and has no tuning parameters. We verify reversibility and prove that under weak regularity conditions geodesic slice sampling is uniformly ergodic. Numerical experiments show that the proposed slice samplers achieve excellent mixing on challenging targets including the Bingham distribution and mixtures of von Mises-Fisher distributions. In these settings our approach outperforms standard samplers such as random-walk Metropolis-Hastings and Hamiltonian Monte Carlo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.08056v3</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Habeck, Mareike Hasenpflug, Shantanu Kodgirwar, Daniel Rudolf</dc:creator>
    </item>
    <item>
      <title>Exact Thresholds for Noisy Non-Adaptive Group Testing</title>
      <link>https://arxiv.org/abs/2401.04884</link>
      <description>arXiv:2401.04884v2 Announce Type: replace-cross 
Abstract: In recent years, the mathematical limits and algorithmic bounds for probabilistic group testing have become increasingly well-understood, with exact asymptotic thresholds now being known in general scaling regimes for the noiseless setting. In the noisy setting where each test outcome is flipped with constant probability, there have been similar developments, but the overall understanding has lagged significantly behind the noiseless setting. In this paper, we substantially narrow this gap by deriving exact asymptotic thresholds for the noisy setting under two widely-studied random test designs: i.i.d. Bernoulli and near-constant tests-per-item. These thresholds are established by combining components of an existing information-theoretic threshold decoder with a novel analysis of maximum-likelihood decoding (upper bounds), and deriving a novel set of impossibility results by analyzing certain failure events for optimal maximum-likelihood decoding (lower bounds).</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.04884v2</guid>
      <category>cs.IT</category>
      <category>cs.DM</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junren Chen, Jonathan Scarlett</dc:creator>
    </item>
    <item>
      <title>Bayesian High-dimensional Linear Regression with Sparse Projection-posterior</title>
      <link>https://arxiv.org/abs/2410.16577</link>
      <description>arXiv:2410.16577v2 Announce Type: replace-cross 
Abstract: We consider a novel Bayesian approach to estimation, uncertainty quantification, and variable selection for a high-dimensional linear regression model under sparsity. The number of predictors can be nearly exponentially large relative to the sample size. We put a conjugate normal prior initially disregarding sparsity, but for making an inference, instead of the original multivariate normal posterior, we use the posterior distribution induced by a map transforming the vector of regression coefficients to a sparse vector obtained by minimizing the sum of squares of deviations plus a suitably scaled $\ell_1$-penalty on the vector. We show that the resulting sparse projection-posterior distribution contracts around the true value of the parameter at the optimal rate adapted to the sparsity of the vector. We show that the true sparsity structure gets a large sparse projection-posterior probability. We further show that an appropriately recentred credible ball has the correct asymptotic frequentist coverage. Finally, we describe how the computational burden can be distributed to many machines, each dealing with only a small fraction of the whole dataset. We conduct a comprehensive simulation study under a variety of settings and found that the proposed method performs well for finite sample sizes. We also apply the method to several real datasets, including the ADNI data, and compare its performance with the state-of-the-art methods. We implemented the method in the \texttt{R} package called \texttt{sparseProj}, and all computations have been carried out using this package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16577v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Samhita Pal, Subhashis Ghoshal</dc:creator>
    </item>
  </channel>
</rss>
