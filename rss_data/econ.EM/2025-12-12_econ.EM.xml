<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Dec 2025 05:03:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Microfoundations and the Causal Interpretation of Price-Exposure Designs</title>
      <link>https://arxiv.org/abs/2512.10076</link>
      <description>arXiv:2512.10076v1 Announce Type: new 
Abstract: This paper studies regional exposure designs that use commodity prices as instruments to study local effects of aggregate shocks. Unlike standard shift share designs that leverage differential exposure to many shocks, the price exposure relies on exogenous variation from a single shock, leading to challenges for both identification and inference. We motivate the design using a multi sector labor model. Under the model and a potential outcomes framework, we characterize the 2SLS and TWFE estimands as weighted averages of region and sector specific effects plus contamination terms driven by the covariance structure of prices and by general-equilibrium output responses. We derive conditions under which these estimands have a clear causal interpretation and provide simple sensitivity analysis procedures for violations. Finally, we show that standard inference procedures suffer from an overrejection problem in price-exposure designs. We derive a new standard error estimator and show its desirable finite-sample properties through Monte Carlo simulations. In an application to gold mining and homicides in the Amazon, the price exposure standard errors are roughly twice as large as conventional clustered standard errors, making the main effect statistically insignificant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10076v1</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luca Moreno-Louzada, Guilherme Figueira, Pedro Picchetti</dc:creator>
    </item>
    <item>
      <title>Inference for Batched Adaptive Experiments</title>
      <link>https://arxiv.org/abs/2512.10156</link>
      <description>arXiv:2512.10156v1 Announce Type: new 
Abstract: The advantages of adaptive experiments have led to their rapid adoption in economics, other fields, as well as among practitioners. However, adaptive experiments pose challenges for causal inference. This note suggests a BOLS (batched ordinary least squares) test statistic for inference of treatment effects in adaptive experiments. The statistic provides a precision-equalizing aggregation of per-period treatment-control differences under heteroskedasticity. The combined test statistic is a normalized average of heteroskedastic per-period z-statistics and can be used to construct asymptotically valid confidence intervals. We provide simulation results comparing rejection rates in the typical case with few treatment periods and few (or many) observations per batch.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10156v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Kemper, Davud Rostam-Afschar</dc:creator>
    </item>
    <item>
      <title>AI-Enhanced TOE Framework for Sustainable Industrial Performance in Fragile and Transforming Economies: Evidence from Yemen and Saudi Arabia</title>
      <link>https://arxiv.org/abs/2512.10333</link>
      <description>arXiv:2512.10333v1 Announce Type: new 
Abstract: Using an integrated framework rooted in the TOE model enhanced with AI, this study looks at ways to improve industrial performance and environmental sustainability in fragile and rapidly transforming contexts such as those found in Yemen and Saudi Arabia. Data for the research are field-based and were obtained from a total of 600 SMEs operating in both countries. Based on the questionnaires' responses by 294 managers, results from the partial least squares structural equation modeling (PLS-SEM) have indicated significant positive effects of AI-TOE on environmental performance (beta = 0.487) and manufacturing performance (beta = 0.759). Results indicate that AI acts as a transformative force, though its impact differs based on the maturity of infrastructure and organizational readiness. The Saudi SMEs gain from their institutional support and advanced technologies, while those in Yemen are dependent on the low-cost adoption of AI and organizational flexibility to accept structural challenges. PLS-SEM analysis of the study showed that integrating AI into the TOE dimensions accelerates operational efficiency in order to support environmental performance. Industrial performance was found to be a very important mediator in this relationship. This study responds to the call for digital transformation literature by providing an actionable framework of AI adoption in resource-constrained environments. These findings offer insights that might guide policymakers and organizations toward more resilient and sustainable operational strategies. These findings provide valuable guidance for engineering managers within the context of negotiating digital transformation and sustainability trade-offs in fragile and resource-constrained contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10333v1</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaima Farhana, Dong Yua, Amirhossein Karamoozianc, Ali Al-shawafid, Amar N. Alsheavif</dc:creator>
    </item>
    <item>
      <title>Optimal Embeddedness and Governance in Biotech Venture Capital Syndicates</title>
      <link>https://arxiv.org/abs/2512.10568</link>
      <description>arXiv:2512.10568v1 Announce Type: new 
Abstract: The biotech venture market faces intense capital demands and regulatory scrutiny, yet academic research on VC networks remains rooted in software and consumer-tech contexts. This dissertation investigates how repeated co-investment ties and domain-expertise homophily influence a venture's exit likelihood, timing, and route amid the sector's pronounced technological and market uncertainty. Using a novel panel of 11,680 biotechnology start-ups from the United States, Canada, and Europe (2010-2024), we apply pooled logit, Cox proportional-hazards, multinomial logit, and Fine-Gray competing-risk models. Our findings show that both average prior co-investment and investor homophily exhibit robust inverted-U relationships with exit outcomes. Moderate familiarity and scientific overlap maximize exit probability, while either sparse or excessive embedding reduces success. Governance mechanisms also play a crucial role: participation of a pharmaceutical corporate VC or a highly independent board flattens the negative effects of over-embedding, enabling syndicates to sustain exit momentum at higher levels of familiarity or homogeneity. Furthermore, the optimal degree of embeddedness is route-specific: IPOs require deeper coordination than trade sales, while acquisitions peak earlier and are less sensitive to homophily. These findings refine network-embeddedness theory in the life-science context, identify governance contingencies, and offer practitioners quantitative metrics to balance trust, expertise, and oversight in biotech financing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10568v1</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxin Hu, Nektarios Oraiopoulos</dc:creator>
    </item>
    <item>
      <title>Learning Time-Varying Correlation Networks with FDR Control via Time-Varying P-values</title>
      <link>https://arxiv.org/abs/2512.10467</link>
      <description>arXiv:2512.10467v1 Announce Type: cross 
Abstract: This paper presents a systematic framework for controlling false discovery rate in learning time-varying correlation networks from high-dimensional, non-linear, non-Gaussian and non-stationary time series with an increasing number of potential abrupt change points in means. We propose a bootstrap-assisted approach to derive dependent and time-varying P-values from a robust estimate of time-varying correlation functions, which are not sensitive to change points. Our procedure is based on a new high-dimensional Gaussian approximation result for the uniform approximation of P-values across time and different coordinates. Moreover, we establish theoretically guaranteed Benjamini--Hochberg and Benjamini--Yekutieli procedures for the dependent and time-varying P-values, which can achieve uniform false discovery rate control. The proposed methods are supported by rigorous mathematical proofs and simulation studies. We also illustrate the real-world application of our framework using both brain electroencephalogram and financial time series data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10467v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bufan Li, Lujia Bai, Weichi Wu</dc:creator>
    </item>
    <item>
      <title>Assessing the Effects of Monetary Shocks on Macroeconomic Stars: A SMUC-IV Framework</title>
      <link>https://arxiv.org/abs/2510.05802</link>
      <description>arXiv:2510.05802v2 Announce Type: replace 
Abstract: This paper proposes a structural multivariate unobserved components model with external instrument (SMUC-IV) to investigate the effects of monetary policy shocks on key U.S. macroeconomic "stars"-namely, the level of potential output, the growth rate of potential output, trend inflation, and the neutral interest rate. A key feature of our approach is the use of an external instrument to identify monetary policy shocks within the multivariate unob- served components modeling framework. We develop an MCMC estimation method to facilitate posterior inference within our proposed SMUC-IV frame- work. In addition, we propose an marginal likelihood estimator to enable model comparison across alternative specifications. Our empirical analysis shows that contractionary monetary policy shocks have significant negative effects on the macroeconomic stars, highlighting the nonzero long-run effects of transitory monetary policy shocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05802v2</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bowen Fu, Chenghan Hou, Jan Pr\"user</dc:creator>
    </item>
    <item>
      <title>Principal component analysis in econometrics: a selective inference perspective</title>
      <link>https://arxiv.org/abs/2511.10419</link>
      <description>arXiv:2511.10419v2 Announce Type: replace 
Abstract: We study the long-standing problem of determining the number of principal components in econometric applications from a selective inference perspective. We consider i.i.d. observations from a $p$-dimensional random vector with $p&lt;n$ and define the ``true'' dimensionality as the rank of the population covariance matrix. Building on the sequential testing viewpoint, we propose a data-driven procedure that estimates $\rank(\Sigma_X)$ using a statistic that depends on the eigenvalues of the sample covariance matrix. While the test statistic shares the functional form of its fixed design counterpart Choi et al. (2017), our analysis departs from the non-stochastic setting by treating the design as random and by avoiding parametric Gaussian assumptions. Under a locally defined null hypothesis, we establish asymptotically exact type~I error controls in the sequential testing procedure, with simulation results indicating empirical validity of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10419v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasuyuki Matsumura, Chisato Tachibana</dc:creator>
    </item>
    <item>
      <title>Prior-Free Blackwell</title>
      <link>https://arxiv.org/abs/2510.08709</link>
      <description>arXiv:2510.08709v3 Announce Type: replace-cross 
Abstract: This paper develops a prior-free model of data-driven decision making in which the decision maker observes the entire distribution of signals generated by a known experiment under an unknown distribution of the state variable and evaluates actions according to their worst-case payoff over the set of state distributions consistent with that observation. We show how our model applies to partial identification in econometrics and propose a ranking of experiments in which E is robustly more informative than E' if the value of the decision maker's problem after observing E is always at least as high as the value of the decision maker's problem after observing E'. This comparison, which is strictly weaker than Blackwell's classical order, holds if and only if the null space of E is contained in the null space of E'.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08709v3</guid>
      <category>econ.TH</category>
      <category>econ.EM</category>
      <pubDate>Fri, 12 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maxwell Rosenthal</dc:creator>
    </item>
  </channel>
</rss>
