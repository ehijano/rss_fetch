<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 31 Dec 2024 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Forecasting Malaria in Indian States: A Time Series Approach with R Shiny Integration</title>
      <link>https://arxiv.org/abs/2412.20121</link>
      <description>arXiv:2412.20121v1 Announce Type: new 
Abstract: Malaria remains a significant public health challenge in many regions, necessitating robust predictive models to aid in its management and prevention. This study focuses on developing and evaluating time series models for forecasting malaria cases across eight Indian states: Jharkhand, Chhattisgarh, Maharashtra, Meghalaya, Mizoram, Odisha, Tripura, and Uttar Pradesh. We employed various modeling approaches, including polynomial regression with seasonal components, log-transformed polynomial regression, lagged difference models, and ARIMA models, to capture the temporal dynamics of malaria incidence. Comprehensive model fitting, residual analysis, and performance evaluation using metrics such as Root Mean Squared Error (RMSE) and Mean Absolute Percentage Error (MAPE) indicated that the log-transformed polynomial regression model consistently outperformed other models in terms of accuracy and robustness across all states. Rolling forecast validation further confirmed the superior predictive capability of the log-transformed model over time. Additionally, an interactive R Shiny tool was developed to facilitate the use of these predictive models by researchers and public health officials. This tool allows users to input data, select modeling approaches, and visualize predictions and performance metrics, providing a practical tool for real-time malaria forecasting and decision-making support. Our findings highlight the critical role of appropriate modeling techniques in malaria prediction and offer valuable resources for enhancing malaria surveillance and response efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20121v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sujit K. Ghosh, Usha Ananthakumar, Praveen D. Chougale, Adithya B. Somaraj</dc:creator>
    </item>
    <item>
      <title>Maximizing Predictive Performance for Small Subgroups: Functionally Adaptive Interaction Regularization (FAIR)</title>
      <link>https://arxiv.org/abs/2412.20190</link>
      <description>arXiv:2412.20190v1 Announce Type: new 
Abstract: In many healthcare settings, it is both critical to consider fairness when building analytical applications but also uniquely unacceptable to lower model performance for one group to match that of another (e.g. fairness cannot be achieved by lowering the diagnostic ability of a model for one group to match that of another and lose overall diagnostic power). Therefore a modeler needs to maximize model performance across groups as much as possible, often while maintaining a model's interpretability, which is a challenge for a number of reasons. In this paper we therefore suggest a new modeling framework, FAIR, to maximize performance across imbalanced groups, based on existing linear regression approaches already commonly used in healthcare settings. We propose a full linear interaction model between groups and all other covariates, paired with a weighting of samples by group size and independent regularization penalties for each group. This efficient approach overcomes many of the limitations in current approaches and manages to balance learning from other groups with tailoring prediction to the small focal group(s). FAIR has an added advantage in that it still allows for model interpretability in research and clinical settings. We demonstrate its usefulness with numerical and health data experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20190v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Smolyak, Courtney Paulson, Margr\'et V. Bjarnad\'ottir</dc:creator>
    </item>
    <item>
      <title>Predicting Customer Lifetime Value Using Recurrent Neural Net</title>
      <link>https://arxiv.org/abs/2412.20295</link>
      <description>arXiv:2412.20295v1 Announce Type: new 
Abstract: This paper introduces a recurrent neural network approach for predicting user lifetime value in Software as a Service (SaaS) applications. The approach accounts for three connected time dimensions. These dimensions are the user cohort (the date the user joined), user age-in-system (the time since the user joined the service) and the calendar date the user is an age-in-system (i.e., contemporaneous information).The recurrent neural networks use a multi-cell architecture, where each cell resembles a long short-term memory neural network. The approach is applied to predicting both acquisition (new users) and rolling (existing user) lifetime values for a variety of time horizons. It is found to significantly improve median absolute percent error versus light gradient boost models and Buy Until You Die models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20295v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huigang Chen, Edwin Ng, Gavin Steininger, Slawek Smyl</dc:creator>
    </item>
    <item>
      <title>A Dynamic Spillover Effect Investigation on Cryptocurrency Market Before and After Pandemic</title>
      <link>https://arxiv.org/abs/2412.19983</link>
      <description>arXiv:2412.19983v1 Announce Type: cross 
Abstract: This paper distinguishes between risk resonance and risk diversification relationships in the cryptocurrency market based on the newly developed asymmetric breakpoint approach, and analyzes the risk propagation mechanism among cryptocurrencies under extreme events. In addition, through the lens of node association and network structure, this paper explores the dynamic evolutionary relationship of cryptocurrency risk association before and after the epidemic. In addition, the driving mechanism of the cryptocurrency risk movement is analyzed in a depth with the epidemic indicators. The findings show that the effect of propagation of risk among cryptocurrencies becomes more significant under the influence of the new crown outbreak. At the same time, the increase in the number of confirmed cases exacerbated the risk spillover effect among cryptocurrencies, while the risk resonance effect that exists between the crude oil market and the cryptocurrency market amplified the extent of the outbreak's impact on cryptocurrencies. However, other financial markets are relatively independent of the cryptocurrency market. This study proposes a strategy to deal with the spread of cryptocurrency risks from the perspective of a public health crisis, providing a useful reference basis for improving the regulatory mechanism of cryptocurrencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19983v1</guid>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenjie Lan</dc:creator>
    </item>
    <item>
      <title>Stronger together? The homophily trap in networks</title>
      <link>https://arxiv.org/abs/2412.20158</link>
      <description>arXiv:2412.20158v1 Announce Type: cross 
Abstract: While homophily -- the tendency to link with similar others -- may nurture a sense of belonging and shared values, it can also hinder diversity and widen inequalities. Here, we unravel this trade-off analytically, revealing homophily traps for minority groups: scenarios where increased homophilic interaction among minorities negatively affects their structural opportunities within a network. We demonstrate that homophily traps arise when minority size falls below 25% of a network, at which point homophily comes at the expense of lower structural visibility for the minority group. Our work reveals that social groups require a critical size to benefit from homophily without incurring structural costs, providing insights into core processes underlying the emergence of group inequality in networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20158v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcos Oliveira, Leonie Neuhauser, Fariba Karimi</dc:creator>
    </item>
    <item>
      <title>Estimation of conditional inequality measures</title>
      <link>https://arxiv.org/abs/2412.20228</link>
      <description>arXiv:2412.20228v1 Announce Type: cross 
Abstract: Classical inequality measures such as the Gini index are often used to describe the sparsity of the distribution of a certain feature in a population. It is sometimes also used to compare the inequalities between some subpopulations, conditioned on certain values of the covariates. The concept of measuring inequality in subpopulation was described in the literature and it is strongly related to the decomposition of the Gini index. In this paper, the idea of conditional inequality measures is extended to the case where covariates are continuous. Curves of conditional inequality measures are introduced, especially, the curves of the conditional quantile versions of the Zenga and $D$ indices are considered. Various methods of their estimation based on quantile regression are presented. An approach using isotonic regression is used to prevent quantile crossing in quantile regression. The accuracy of the estimators considered is compared in simulation studies. Furthermore, an analysis of the growth in salary inequalities with respect to employee age is included to demonstrate the potential of conditional inequality measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20228v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alicja Jokiel-Rokita, Sylwester Pi\k{a}tek, Rafa{\l} Topolnicki</dc:creator>
    </item>
    <item>
      <title>Exploring the Magnitude-Shape Plot Framework for Anomaly Detection in Crowded Video Scenes</title>
      <link>https://arxiv.org/abs/2412.20363</link>
      <description>arXiv:2412.20363v1 Announce Type: cross 
Abstract: Detecting anomalies in crowded video scenes is critical for public safety, enabling timely identification of potential threats. This study explores video anomaly detection within a Functional Data Analysis framework, focusing on the application of the Magnitude-Shape (MS) Plot. Autoencoders are used to learn and reconstruct normal behavioral patterns from anomaly-free training data, resulting in low reconstruction errors for normal frames and higher errors for frames with potential anomalies. The reconstruction error matrix for each frame is treated as multivariate functional data, with the MS-Plot applied to analyze both magnitude and shape deviations, enhancing the accuracy of anomaly detection. Using its capacity to evaluate the magnitude and shape of deviations, the MS-Plot offers a statistically principled and interpretable framework for anomaly detection. The proposed methodology is evaluated on two widely used benchmark datasets, UCSD Ped2 and CUHK Avenue, demonstrating promising performance. It performs better than traditional univariate functional detectors (e.g., FBPlot, TVDMSS, Extremal Depth, and Outliergram) and several state-of-the-art methods. These results highlight the potential of the MS-Plot-based framework for effective anomaly detection in crowded video scenes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20363v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zuzheng Wang, Fouzi Harrou, Ying Sun, Marc G Genton</dc:creator>
    </item>
    <item>
      <title>On the Missing Factor in Some Concentration Inequalities for Martingales</title>
      <link>https://arxiv.org/abs/2412.20542</link>
      <description>arXiv:2412.20542v1 Announce Type: cross 
Abstract: In this note, we improve some concentration inequalities for martingales with bounded increments. These results recover the missing factor in Freedman-style inequalities and are near optimal. We also provide minor refinements of concentration inequalities for functions of independent random variables. These proofs use techniques from the works of Bentkus and Pinelis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20542v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>Testing and Improving the Robustness of Amortized Bayesian Inference for Cognitive Models</title>
      <link>https://arxiv.org/abs/2412.20586</link>
      <description>arXiv:2412.20586v1 Announce Type: cross 
Abstract: Contaminant observations and outliers often cause problems when estimating the parameters of cognitive models, which are statistical models representing cognitive processes. In this study, we test and improve the robustness of parameter estimation using amortized Bayesian inference (ABI) with neural networks. To this end, we conduct systematic analyses on a toy example and analyze both synthetic and real data using a popular cognitive model, the Drift Diffusion Models (DDM). First, we study the sensitivity of ABI to contaminants with tools from robust statistics: the empirical influence function and the breakdown point. Next, we propose a data augmentation or noise injection approach that incorporates a contamination distribution into the data-generating process during training. We examine several candidate distributions and evaluate their performance and cost in terms of accuracy and efficiency loss relative to a standard estimator. Introducing contaminants from a Cauchy distribution during training considerably increases the robustness of the neural density estimator as measured by bounded influence functions and a much higher breakdown point. Overall, the proposed method is straightforward and practical to implement and has a broad applicability in fields where outlier detection or removal is challenging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20586v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yufei Wu, Stefan Radev, Francis Tuerlinckx</dc:creator>
    </item>
    <item>
      <title>Econometric Analysis of Pandemic Disruption and Recovery Trajectory in the U.S. Rail Freight Industry</title>
      <link>https://arxiv.org/abs/2412.20669</link>
      <description>arXiv:2412.20669v1 Announce Type: cross 
Abstract: To measure the impacts on U.S. rail and intermodal freight by economic disruptions of the 2007-09 Great Recession and the COVID-19 pandemic, this paper uses time series analysis with the AutoRegressive Integrated Moving Average (ARIMA) family of models and covariates to model intermodal and commodity-specific rail freight volumes based on pre-disruption data. A framework to construct scenarios and select parameters and variables is demonstrated. By comparing actual freight volumes during the disruptions against three counterfactual scenarios, Trend Continuation, Covariate-adapted Trend Continuation, and Full Covariate-adapted Prediction, the characteristics and differences in magnitude and timing between the two disruptions and their effects across nine freight components are examined.
  Results show the disruption impacts differ from measurement by simple comparison with pre-disruption levels or year-on-year comparison depending on the structural trend and seasonal pattern. Recovery Pace Plots are introduced to support comparison in recovery speeds across freight components. Accounting for economic variables helps improve model fitness. It also enables evaluation of the change in association between freight volumes and covariates, where intermodal freight was found to respond more slowly during the pandemic, potentially due to supply constraint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20669v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Max T. M. Ng, Hani S. Mahmassani, Joseph L. Schofer</dc:creator>
    </item>
    <item>
      <title>An Observation-Driven State-Space Model for Claims Size Modeling</title>
      <link>https://arxiv.org/abs/2412.21099</link>
      <description>arXiv:2412.21099v1 Announce Type: cross 
Abstract: State-space models are popular models in econometrics. Recently, these models have gained some popularity in the actuarial literature. The best known state-space models are of Kalman-filter type. These models are so-called parameter-driven because the observations do not impact the state-space dynamics. A second less well-known class of state-space models are so-called observation-driven state-space models where the state-space dynamics is also impacted by the actual observations. A typical example is the Poisson-Gamma observation-driven state-space model for counts data. This Poisson-Gamma model is fully analytically tractable. The goal of this paper is to develop a Gamma- Gamma observation-driven state-space model for claim size modeling. We provide fully tractable versions of Gamma-Gamma observation-driven state-space models, and these versions extend the work of Smith and Miller (1986) by allowing for a fully flexible variance behavior. Additionally, we demonstrate that the proposed model aligns with evolutionary credibility, a methodology in insurance that dynamically adjusts premium rates over time using evolving data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.21099v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jae Youn Ahn, Himchan Jeong, Mario V. W\"uthrich</dc:creator>
    </item>
    <item>
      <title>Causal Hangover Effects</title>
      <link>https://arxiv.org/abs/2412.21181</link>
      <description>arXiv:2412.21181v1 Announce Type: cross 
Abstract: It's not unreasonable to think that in-game sporting performance can be affected partly by what takes place off the court. We can't observe what happens between games directly. Instead, we proxy for the possibility of athletes partying by looking at play following games in party cities. We are interested to see if teams exhibit a decline in performance the day following a game in a city with active nightlife; we call this a "hangover effect". Part of the question is determining a reasonable way to measure levels of nightlife, and correspondingly which cities are notorious for it; we colloquially refer to such cities as "party cities". To carry out this study, we exploit data on bookmaker spreads: the expected score differential between two teams after conditioning on observable performance in past games and expectations about the upcoming game. We expect a team to meet the spread half the time, since this is one of the easiest ways for bookmakers to guarantee a profit. We construct a model which attempts to estimate the causal effect of visiting a "party city" on subsequent day performance as measured by the odds of beating the spread. In particular, we only consider the hangover effect on games played back-to-back within 24 hours of each other. To the extent that odds of beating the spread against next day opponent is uncorrelated with playing in a party city the day before, which should be the case under an efficient betting market, we have identification in our variable of interest. We find that visiting a city with active nightlife the day prior to a game does have a statistically significant negative effect on a team's likelihood of meeting bookmakers' expectations for both NBA and MLB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.21181v1</guid>
      <category>econ.EM</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andreas Santucci, Eric Lax</dc:creator>
    </item>
    <item>
      <title>Bayesian Time Varying Coefficient Model with Applications to Marketing Mix Modeling</title>
      <link>https://arxiv.org/abs/2106.03322</link>
      <description>arXiv:2106.03322v4 Announce Type: replace 
Abstract: Both Bayesian and varying coefficient models are very useful tools in practice as they can be used to model parameter heterogeneity in a generalizable way. Motivated by the need of enhancing Marketing Mix Modeling at Uber, we propose a Bayesian Time Varying Coefficient model, equipped with a hierarchical Bayesian structure. This model is different from other time varying coefficient models in the sense that the coefficients are weighted over a set of local latent variables following certain probabilistic distributions. Stochastic Variational Inference is used to approximate the posteriors of latent variables and dynamic coefficients. The proposed model also helps address many challenges faced by traditional MMM approaches. We used simulations as well as real world marketing datasets to demonstrate our model superior performance in terms of both accuracy and interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.03322v4</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edwin Ng, Zhishi Wang, Athena Dai</dc:creator>
    </item>
    <item>
      <title>Robust semi-parametric signal detection in particle physics with classifiers decorrelated via optimal transport</title>
      <link>https://arxiv.org/abs/2409.06399</link>
      <description>arXiv:2409.06399v2 Announce Type: replace 
Abstract: Searches of new signals in particle physics are usually done by training a supervised classifier to separate a signal model from the known Standard Model physics (also called the background model). However, even when the signal model is correct, systematic errors in the background model can influence supervised classifiers and might adversely affect the signal detection procedure. To tackle this problem, one approach is to use the (possibly misspecified) classifier only to perform a preliminary signal-enrichment step and then to carry out a bump hunt on the signal-rich sample using only the real experimental data. For this procedure to work, we need a classifier constrained to be decorrelated with one or more protected variables used for the signal detection step. We do this by considering an optimal transport map of the classifier output that makes it independent of the protected variable(s) for the background. We then fit a semi-parametric mixture model to the distribution of the protected variable after making cuts on the transformed classifier to detect the presence of a signal. We compare and contrast this decorrelation method with previous approaches, show that the decorrelation procedure is robust to moderate background misspecification, and analyse the power of the signal detection test as a function of the cut on the classifier.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06399v2</guid>
      <category>stat.AP</category>
      <category>hep-ex</category>
      <category>hep-ph</category>
      <category>stat.ML</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Purvasha Chakravarti, Lucas Kania, Olaf Behnke, Mikael Kuusela, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Low-Rank Expectile Representations of a Data Matrix, with Application to Diurnal Heart Rates</title>
      <link>https://arxiv.org/abs/2412.04765</link>
      <description>arXiv:2412.04765v2 Announce Type: replace 
Abstract: Low-rank matrix factorization is a powerful tool for understanding the structure of 2-way data, and is usually accomplished by minimizing a sum of squares criterion. Expectile analysis generalizes squared-error loss by introducing asymmetry, allowing tail behavior to be elicited. Here we present a framework for low-rank expectile analysis of a data matrix that incorporates both additive and multiplicative effects, utilizing expectile loss, and accommodating arbitrary patterns of missing data. The representation can be fit with gradient-descent. Simulation studies demonstrate the accuracy of the structure recovery. Using diurnal heart rate data indexed by person-days versus minutes within a day, we find divergent behavior for lower versus upper expectiles, with the lower expectiles being much more stable within subjects across days, while the upper expectiles are much more variable, even within subjects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04765v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuge Ouyang, Yunxuan Tang, Benjamin Osafo Agyare</dc:creator>
    </item>
    <item>
      <title>A General Framework of Brain Region Detection And Genetic Variants Selection in Imaging Genetics</title>
      <link>https://arxiv.org/abs/2412.19735</link>
      <description>arXiv:2412.19735v2 Announce Type: replace 
Abstract: Imaging genetics is a growing field that employs structural or functional neuroimaging techniques to study individuals with genetic risk variants potentially linked to specific illnesses. This area presents considerable challenges to statisticians due to the heterogeneous information and different data forms it involves. In addition, both imaging and genetic data are typically high-dimensional, creating a "big data squared" problem. Moreover, brain imaging data contains extensive spatial information. Simply vectorizing tensor images and treating voxels as independent features can lead to computational issues and disregard spatial structure. This paper presents a novel statistical method for imaging genetics modeling while addressing all these challenges. We explore a Canonical Correlation Analysis based linear model for the joint modeling of brain imaging, genetic information, and clinical phenotype, enabling the simultaneous detection of significant brain regions and selection of important genetic variants associated with the phenotype outcome. Scalable algorithms are developed to tackle the "big data squared" issue. We apply the proposed method to explore the reaction speed, an indicator of cognitive functions, and its associations with brain MRI and genetic factors using the UK Biobank database. Our study reveals a notable connection between the caudate nucleus region of brain and specific significant SNPs, along with their respective regulated genes, and the reaction speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19735v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siqiang Su, Zhenghao Li, Long Feng, Ting Li</dc:creator>
    </item>
    <item>
      <title>Estimating probabilities of multivariate failure sets based on pairwise tail dependence coefficients</title>
      <link>https://arxiv.org/abs/2210.12618</link>
      <description>arXiv:2210.12618v2 Announce Type: replace-cross 
Abstract: Estimating the probability of extreme events involving multiple risk factors is a critical challenge in fields such as finance and climate science. This paper proposes a semi-parametric approach to estimate the probability that a multivariate random vector falls into an extreme failure set, based on the information in the tail pairwise dependence matrix (TPDM) only. The TPDM provides a partial summary of tail dependence for all pairs of components of the random vector. We propose an efficient algorithm to obtain approximate completely positive decompositions of the TPDM, enabling the construction of a max-linear model whose TPDM approximates that of the original random vector. We also provide conditions under which the approximation turns out to be exact. Based on the decompositions, we can construct max-linear random vectors to estimate failure probabilities, exploiting its computational simplicity. The algorithm allows to obtain multiple decompositions efficiently. Finally, we apply our framework to estimate probabilities of extreme events for real-world datasets, including industry portfolio returns and maximal wind speeds, demonstrating its practical utility for risk assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.12618v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Kiriliouk, Chen Zhou</dc:creator>
    </item>
    <item>
      <title>Local linear smoothing for regression surfaces on the simplex using Dirichlet kernels</title>
      <link>https://arxiv.org/abs/2408.07209</link>
      <description>arXiv:2408.07209v2 Announce Type: replace-cross 
Abstract: This paper introduces a local linear smoother for regression surfaces on the simplex. The estimator solves a least-squares regression problem weighted by a locally adaptive Dirichlet kernel, ensuring good boundary properties. Asymptotic results for the bias, variance, mean squared error, and mean integrated squared error are derived, generalizing the univariate results of Chen [Ann. Inst. Statist. Math., 54(2) (2002), pp. 312-323]. A simulation study shows that the proposed local linear estimator with Dirichlet kernel outperforms its only direct competitor in the literature, the Nadaraya-Watson estimator with Dirichlet kernel due to Bouzebda, Nezzal and Elhattab [AIMS Math., 9(9) (2024), pp. 26195-26282].</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07209v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Genest, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>Assessing the Optimistic Bias in the Natural Inflow Forecasts: A Call for Model Monitoring in Brazil</title>
      <link>https://arxiv.org/abs/2410.13763</link>
      <description>arXiv:2410.13763v2 Announce Type: replace-cross 
Abstract: Hydroelectricity accounted for roughly 66% of the total generation in Brazil in 2023 and addressed most of the intermittency of wind and solar generation. Thus, one of the most important steps in the operation planning of this country is the forecast of the natural inflow energy (NIE) time series, an approximation of the energetic value of the water inflows. To manage water resources over time, the Brazilian system operator performs long-term forecasts for the NIE to assess the water values through long-term hydrothermal planning models, which are then used to define the short-term merit order in day-ahead scheduling. Therefore, monitoring optimistic bias in NIE forecasts is crucial to prevent an optimistic view of future system conditions and subsequent riskier storage policies. In this article, we investigate and showcase strong evidence of an optimistic bias in the official NIE forecasts, with predicted values consistently exceeding the observations over the past 12 years in the two main subsystems (Southeast and Northeast). Rolling window out-of-sample tests conducted with real data demonstrate that the official forecast model exhibits a statistically significant bias of 6%, 13%, 18%, and 23% for 1, 6, 12, and 24 steps ahead in the Southeast subsystem, and 19%, 57%, 80%, and 108% in the Northeast.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13763v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arthur Brigatto, Alexandre Street, Cristiano Fernandes, Davi Valladao, Guilherme Bodin, Joaquim Dias Garcia</dc:creator>
    </item>
    <item>
      <title>Gaussian Mixture Models Based Augmentation Enhances GNN Generalization</title>
      <link>https://arxiv.org/abs/2411.08638</link>
      <description>arXiv:2411.08638v2 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) have shown great promise in tasks like node and graph classification, but they often struggle to generalize, particularly to unseen or out-of-distribution (OOD) data. These challenges are exacerbated when training data is limited in size or diversity. To address these issues, we introduce a theoretical framework using Rademacher complexity to compute a regret bound on the generalization error and then characterize the effect of data augmentation. This framework informs the design of GMM-GDA, an efficient graph data augmentation (GDA) algorithm leveraging the capability of Gaussian Mixture Models (GMMs) to approximate any distribution. Our approach not only outperforms existing augmentation techniques in terms of generalization but also offers improved time complexity, making it highly suitable for real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08638v2</guid>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yassine Abbahaddou, Fragkiskos D. Malliaros, Johannes F. Lutzeyer, Amine Mohamed Aboussalah, Michalis Vazirgiannis</dc:creator>
    </item>
    <item>
      <title>Privacy-Preserving Customer Support: A Framework for Secure and Scalable Interactions</title>
      <link>https://arxiv.org/abs/2412.07687</link>
      <description>arXiv:2412.07687v2 Announce Type: replace-cross 
Abstract: The growing reliance on artificial intelligence (AI) in customer support has significantly improved operational efficiency and user experience. However, traditional machine learning (ML) approaches, which require extensive local training on sensitive datasets, pose substantial privacy risks and compliance challenges with regulations like the General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA). Existing privacy-preserving techniques, such as anonymization, differential privacy, and federated learning, address some concerns but face limitations in utility, scalability, and complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning (PP-ZSL) framework, a novel approach leveraging large language models (LLMs) in a zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminates the need for local training on sensitive data by utilizing pre-trained LLMs to generate responses directly. The framework incorporates real-time data anonymization to redact or mask sensitive information, retrieval-augmented generation (RAG) for domain-specific query resolution, and robust post-processing to ensure compliance with regulatory standards. This combination reduces privacy risks, simplifies compliance, and enhances scalability and operational efficiency. Empirical analysis demonstrates that the PP-ZSL framework provides accurate, privacy-compliant responses while significantly lowering the costs and complexities of deploying AI-driven customer support systems. The study highlights potential applications across industries, including financial services, healthcare, e-commerce, legal support, telecommunications, and government services. By addressing the dual challenges of privacy and performance, this framework establishes a foundation for secure, efficient, and regulatory-compliant AI applications in customer interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07687v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anant Prakash Awasthi, Girdhar Gopal Agarwal, Chandraketu Singh, Rakshit Varma, Sanchit Sharma</dc:creator>
    </item>
  </channel>
</rss>
