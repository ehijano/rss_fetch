<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Sep 2025 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Regularized Riccati Recursion for Interior-Point Optimal Control</title>
      <link>https://arxiv.org/abs/2509.16370</link>
      <description>arXiv:2509.16370v1 Announce Type: new 
Abstract: We derive a closed-form extension of Riccati's recursion for solving regularized LQR problems. We also show how this can be used to solve general constrained, non-convex, discrete-time optimal control problems via a regularized interior point method, while guaranteeing that each step is a descent direction of an Augmented Barrier-Lagrangian merit function. We also provide MIT-licensed implementations of our method in C++ and JAX.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16370v1</guid>
      <category>math.OC</category>
      <category>cs.MS</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jo\~ao Sousa-Pinto, Dominique Orban</dc:creator>
    </item>
    <item>
      <title>Overfitting in Adaptive Robust Optimization</title>
      <link>https://arxiv.org/abs/2509.16451</link>
      <description>arXiv:2509.16451v1 Announce Type: new 
Abstract: Adaptive robust optimization (ARO) extends static robust optimization by allowing decisions to depend on the realized uncertainty - weakly dominating static solutions within the modeled uncertainty set. However, ARO makes previous constraints that were independent of uncertainty now dependent, making it vulnerable to additional infeasibilities when realizations fall outside the uncertainty set. This phenomenon of adaptive policies being brittle is analogous to overfitting in machine learning. To mitigate against this, we propose assigning constraint-specific uncertainty set sizes, with harder constraints given stronger probabilistic guarantees. Interpreted through the overfitting lens, this acts as regularization: tighter guarantees shrink adaptive coefficients to ensure stability, while looser ones preserve useful flexibility. This view motivates a principled approach to designing uncertainty sets that balances robustness and adaptivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16451v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karl Zhu, Dimitris Bertsimas</dc:creator>
    </item>
    <item>
      <title>Bayesian distributionally robust variational inequalities: regularization and quantification</title>
      <link>https://arxiv.org/abs/2509.16537</link>
      <description>arXiv:2509.16537v1 Announce Type: new 
Abstract: We propose a Bayesian distributionally robust variational inequality (DRVI) framework that models the data-generating distribution through a finite mixture family, which allows us to study the DRVI on a tractable finite-dimensional parametric ambiguity set. To address distributional uncertainty, we construct a data-driven ambiguity set with posterior coverage guarantees via Bayesian inference. We also employ a regularization approach to ensure numerical stability. We prove the existence of solutions to the Bayesian DRVI and the asymptotic convergence to a true solution as sample size grows to infinity and the regularization parameter goes to zero. Moreover, we derive quantitative stability bounds and finite-sample guarantees under data scarcity and contamination. Numerical experiments on a distributionally robust multi-portfolio Nash equilibrium problem validate our theoretical results and demonstrate the robustness and reliability of Bayesian DRVI solutions in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16537v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wentao Ma, Zhiping Chen, Xiaojun Chen</dc:creator>
    </item>
    <item>
      <title>An Improved Yau-Yau Algorithm for High Dimensional Nonlinear Filtering Problems</title>
      <link>https://arxiv.org/abs/2509.16896</link>
      <description>arXiv:2509.16896v1 Announce Type: new 
Abstract: Nonlinear state estimation under noisy observations is rapidly intractable as system dimension increases. We introduce an improved Yau-Yau filtering framework that breaks the curse of dimensionality and extends real-time nonlinear filtering to systems with up to thousands of state dimensions, achieving high-accuracy estimates in just a few seconds with rigorous theoretical error guarantees. This new approach integrates quasi-Monte Carlo low-discrepancy sampling, a novel offline-online update, high-order multi-scale kernel approximations, fully log-domain likelihood computation, and a local resampling-restart mechanism, all realized with CPU/GPU-parallel computation. Theoretical analysis guarantees local truncation error \(O(\Delta t^2 + D^*(n))\) and global error \(O(\Delta t + D^*(n)/\Delta t)\), where \(\Delta t\) is the time step and \(D^*(n)\) the star-discrepancy. Numerical experiments, spanning large-scale nonlinear cubic sensors up to 1000 dimensions, highly nonlinear small-scale problems, and linear Gaussian benchmarks, demonstrate sub-quadratic runtime scaling, sub-linear error growth, and excellent performance that surpasses the extended and unscented Kalman filters (EKF, UKF) and the particle filter (PF) under strong nonlinearity, while matching or exceeding the optimal Kalman-Bucy filter in linear regimes. By breaking the curse of dimensionality, our method enables accurate, real-time, high-dimensional nonlinear filtering, opening broad opportunities for applications in science and engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16896v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shing-Tung Yau, Yi-Shuai Niu</dc:creator>
    </item>
    <item>
      <title>Differential Privacy for Euclidean Jordan Algebra with Applications to Private Symmetric Cone Programming</title>
      <link>https://arxiv.org/abs/2509.16915</link>
      <description>arXiv:2509.16915v1 Announce Type: new 
Abstract: In this paper, we study differentially private mechanisms for functions whose outputs lie in a Euclidean Jordan algebra. Euclidean Jordan algebras capture many important mathematical structures and form the foundation of linear programming, second-order cone programming, and semidefinite programming. Our main contribution is a generic Gaussian mechanism for such functions, with sensitivity measured in $\ell_2$, $\ell_1$, and $\ell_\infty$ norms. Notably, this framework includes the important case where the function outputs are symmetric matrices, and sensitivity is measured in the Frobenius, nuclear, or spectral norm. We further derive private algorithms for solving symmetric cone programs under various settings, using a combination of the multiplicative weights update method and our generic Gaussian mechanism. As an application, we present differentially private algorithms for semidefinite programming, resolving a major open question posed by [Hsu, Roth, Roughgarden, and Ullman, ICALP 2014].</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16915v1</guid>
      <category>math.OC</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhao Song, Jianfei Xue, Lichen Zhang</dc:creator>
    </item>
    <item>
      <title>Hessian-guided Perturbed Wasserstein Gradient Flows for Escaping Saddle Points</title>
      <link>https://arxiv.org/abs/2509.16974</link>
      <description>arXiv:2509.16974v1 Announce Type: new 
Abstract: Wasserstein gradient flow (WGF) is a common method to perform optimization over the space of probability measures. While WGF is guaranteed to converge to a first-order stationary point, for nonconvex functionals the converged solution does not necessarily satisfy the second-order optimality condition; i.e., it could converge to a saddle point. In this work, we propose a new algorithm for probability measure optimization, perturbed Wasserstein gradient flow (PWGF), that achieves second-order optimality for general nonconvex objectives. PWGF enhances WGF by injecting noisy perturbations near saddle points via a Gaussian process-based scheme. By pushing the measure forward along a random vector field generated from a Gaussian process, PWGF helps the solution escape saddle points efficiently by perturbing the solution towards the smallest eigenvalue direction of the Wasserstein Hessian. We theoretically derive the computational complexity for PWGF to achieve a second-order stationary point. Furthermore, we prove that PWGF converges to a global optimum in polynomial time for strictly benign objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16974v1</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naoya Yamamoto, Juno Kim, Taiji Suzuki</dc:creator>
    </item>
    <item>
      <title>Guardian maps for continuous-time systems: A Lie-algebraic approach</title>
      <link>https://arxiv.org/abs/2509.17016</link>
      <description>arXiv:2509.17016v1 Announce Type: new 
Abstract: Guardian maps are scalar maps that vanish when a matrix or polynomial is on the verge of stability. Several guardian maps have been proposed in the literature for Hurwitz stability based on the Kronecker sum, the second lower Schl\"aflian matrix, and the bialternate sum. It is natural to ask if there is a unifying principle for all these
  maps. Here, we introduce the Lie-algebraic notion of a guardian representation, and show that all the examples above are instances of this unifying idea. We also show that the bialternate sum coincides with the second additive compound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17016v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eyal Bar-Shalom, Alexander Ovseevich, Michael Margaliot</dc:creator>
    </item>
    <item>
      <title>Reachability-based Approach to Point-to-Point Steering Problem</title>
      <link>https://arxiv.org/abs/2509.17076</link>
      <description>arXiv:2509.17076v1 Announce Type: new 
Abstract: This paper presents a reachability-based approach to finite-time transition problem of nonlinear systems between two stationary points (i.e., the point-to-point steering problem). When the target state is reachable, we prove that a solution can always be constructed by concatenation of two Pontraygin extremals. This allows to formulate the problem as a two-point boundary value problem (TPBVP) of extremals, where the solution existence to the formulated TPBVP is equivalent to that of the original problem. The theoretical developments are applied to curves with prescribed curvature bounds in R3, thereby extending the recent works on Dubins car to dimension three. We prove that to construct a curvature-bounded path in R3 with prescribed length and boundary conditions, it suffices to consider the trajectories that are concatenations of CSC, CCC, their subsegments, and H, where C denotes a circular arc with maximum curvature, S a straight line segment, and H a certain class of helicoidal arcs with constant curvature. Numerical demonstrations are conducted on a nonlinear dynamics example, and on curvature-bounded paths in R2 and R3.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17076v1</guid>
      <category>math.OC</category>
      <category>math.DS</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juho Bae, Han-Lim Choi</dc:creator>
    </item>
    <item>
      <title>Clarke Differentials and the Envelope Theorem in Dynamic Programming</title>
      <link>https://arxiv.org/abs/2509.17103</link>
      <description>arXiv:2509.17103v1 Announce Type: new 
Abstract: In this paper, we consider a deterministic dynamic programming model, and derive the envelope theorem using the Clarke differential. Compared with past research, the requirements for our result are weaker, and do not include differentiability, convexity, and boundedness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17103v1</guid>
      <category>math.OC</category>
      <category>econ.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuhki Hosoya</dc:creator>
    </item>
    <item>
      <title>Optimized self-adaptive PID speed control for autonomous vehicles</title>
      <link>https://arxiv.org/abs/2509.17214</link>
      <description>arXiv:2509.17214v1 Announce Type: new 
Abstract: The main control tasks in autonomous vehicles are steering (lateral) and speed (longitudinal) control. PID controllers are widely used in the industry because of their simplicity and good performance, but they are difficult to tune and need additional adaptation to control nonlinear systems with varying parameters. In this paper, the longitudinal control task is addressed by implementing adaptive PID control using two different approaches: Genetic Algorithms (GA-PID) and then Neural Networks (NN-PID) respectively. The vehicle nonlinear longitudinal dynamics are modeled using Powertrain blockset library. Finally, simulations are performed to assess and compare the performance of the two controllers subject to external disturbances. Code can be found here: https://github.com/yassinekebbati/Self-adaptive-PID</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17214v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ICCMA54375.2021.9646218</arxiv:DOI>
      <arxiv:journal_reference>2021 9th International Conference on Control, Mechatronics and Automation (ICCMA)</arxiv:journal_reference>
      <dc:creator>Yassine Kebbati, Naima Ait-Oufroukh, Vincent Vigneron, Dalil Ichalal, Dominique Gruyer</dc:creator>
    </item>
    <item>
      <title>Optimized adaptive MPC for lateral control of autonomous vehicles</title>
      <link>https://arxiv.org/abs/2509.17215</link>
      <description>arXiv:2509.17215v1 Announce Type: new 
Abstract: Autonomous vehicles are the upcoming solution to most transportation problems such as safety, comfort and efficiency. The steering control is one of the main important tasks in achieving autonomous driving. Model predictive control (MPC) is among the fittest controllers for this task due to its optimal performance and ability to handle constraints. This paper proposes an adaptive MPC controller (AMPC) for the path tracking task, and an improved PSO algorithm for optimising the AMPC parameters. Parameter adaption is realised online using a lookup table approach. The propose AMPC performance is assessed and compared with the classic MPC and the Pure Pursuit controller through simulations. Code can be found here: https://github.com/yassinekebbati/Optimized_adaptive_MPC</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17215v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.23919/ICAC50006.2021.9594131</arxiv:DOI>
      <arxiv:journal_reference>2021 26th International Conference on Automation and Computing (ICAC)</arxiv:journal_reference>
      <dc:creator>Yassine Kebbati, Vicen\c{c} Puig, Naima Ait-Oufroukh, Vincent Vigneron, Dalil Ichalal</dc:creator>
    </item>
    <item>
      <title>Team problems and stochastic programming</title>
      <link>https://arxiv.org/abs/2509.17222</link>
      <description>arXiv:2509.17222v1 Announce Type: new 
Abstract: The paper bridges two vast areas of research: stochastic team decision problems and convex stochastic programming. New methods developed in the latter are applied to the study of fundamental problems in the former. The main results are concerned with the Lagrangian relaxation of informational and material constraints in convex stochastic team problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17222v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Igor V. Evstigneev, Mohammad J. Vanaei, Mikhail V. Zhitlukhin</dc:creator>
    </item>
    <item>
      <title>Minimization of Nonsmooth Weakly Convex Function over Prox-regular Set for Robust Low-rank Matrix Recovery</title>
      <link>https://arxiv.org/abs/2509.17549</link>
      <description>arXiv:2509.17549v1 Announce Type: new 
Abstract: We propose a prox-regular-type low-rank constrained nonconvex nonsmooth optimization model for Robust Low-Rank Matrix Recovery (RLRMR), i.e., estimate problem of low-rank matrix from an observed signal corrupted by outliers. For RLRMR, the $\ell_{1}$-norm has been utilized as a convex loss to detect outliers as well as to keep tractability of optimization models. Nevertheless, the $\ell_{1}$-norm is not necessarily an ideal robust loss because the $\ell_{1}$-norm tends to overpenalize entries corrupted by outliers of large magnitude. In contrast, the proposed model can employ a weakly convex function as a more robust loss, against outliers, than the $\ell_{1}$-norm. For the proposed model, we present (i) a projected variable smoothing-type algorithm applicable for the minimization of a nonsmooth weakly convex function over a prox-regular set, and (ii) a convergence analysis of the proposed algorithm in terms of stationary point. Numerical experiments demonstrate the effectiveness of the proposed model compared with the existing models that employ the $\ell_{1}$-norm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17549v1</guid>
      <category>math.OC</category>
      <category>eess.SP</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keita Kume, Isao Yamada</dc:creator>
    </item>
    <item>
      <title>Distributed Functional Optimization and Learning on Banach Spaces: Generic Frameworks</title>
      <link>https://arxiv.org/abs/2509.17554</link>
      <description>arXiv:2509.17554v1 Announce Type: new 
Abstract: In this paper, we establish a distributed functional optimization (DFO) theory based on time-varying multi-agent networks. The vast majority of existing distributed optimization theories are developed based on Euclidean decision variables. However, for many scenarios in machine learning and statistical learning, such as reproducing kernel spaces or probability measure spaces that use functions or probability measures as fundamental variables, the development of existing distributed optimization theories exhibit obvious theoretical and technical deficiencies. This paper addresses these issues by developing a novel general DFO theory on Banach spaces, allowing functional learning problems in the aforementioned scenarios to be incorporated into our framework for resolution. We study both convex and nonconvex DFO problems and rigorously establish a comprehensive convergence theory of distributed functional mirror descent and distributed functional gradient descent algorithm to solve them. Satisfactory convergence rates are fully derived. The work has provided generic analyzing frameworks for distributed optimization. The established theory is shown to have crucial application value in the kernel-based distributed learning theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17554v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhan Yu, Zhongjie Shi, Deming Yuan, Daniel W. C. Ho</dc:creator>
    </item>
    <item>
      <title>Distributed Stochastic Optimization under Heavy-Tailed Noise: A Federated Mirror Descent Approach with High Probability Convergence</title>
      <link>https://arxiv.org/abs/2509.17659</link>
      <description>arXiv:2509.17659v1 Announce Type: new 
Abstract: We study the distributed stochastic optimization (DSO) problem under a heavy-tailed noise condition by utilizing a multi-agent system. Despite the extensive research on DSO algorithms used to solve DSO problems under light-tailed noise conditions (such as Gaussian noise), there is a significant lack of study of DSO algorithms in the context of heavy-tailed random noise. Classical DSO approaches in a heavy-tailed setting may present poor convergence behaviors. Therefore, developing DSO methods in the context of heavy-tailed noises is of importance. This work follows this path and we consider the setting that the gradient noises associated with each agent can be heavy-tailed, potentially having unbounded variance. We propose a clipped federated stochastic mirror descent algorithm to solve the DSO problem. We rigorously present a convergence theory and show that, under appropriate rules on the stepsize and the clipping parameter associated with the local noisy gradient influenced by the heavy-tailed noise, the algorithm is able to achieve satisfactory high probability convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17659v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhan Yu, Lan Liao, Deming Yuan, Daniel W. C. Ho, Ding-Xuan Zhou</dc:creator>
    </item>
    <item>
      <title>An Optimal Control Interpretation of Augmented Distributed Optimization Algorithms</title>
      <link>https://arxiv.org/abs/2509.17785</link>
      <description>arXiv:2509.17785v1 Announce Type: new 
Abstract: Distributed optimization algorithms are used in a wide variety of problems involving complex network systems where the goal is for a set of agents in the network to solve a network-wide optimization problem via distributed update rules. In many applications, such as communication networks and power systems, transient performance of the algorithms is just as critical as convergence, as the algorithms link to physical processes which must behave well. Primal-dual algorithms have a long history in solving distributed optimization problems, with augmented Lagrangian methods leading to important classes of widely used algorithms, which have been observed in simulations to improve transient performance. Here we show that such algorithms can be seen as being the optimal solution to an appropriately formulated optimal control problem, i.e., a cost functional associated with the transient behavior of the algorithm is minimized, penalizing deviations from optimality during algorithm transients. This is shown for broad classes of algorithm dynamics, including the more involved setting where inequality constraints are present. The results presented improve our understanding of the performance of distributed optimization algorithms and can be used as a basis for improved formulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17785v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liam Hallinan, Ioannis Lestas</dc:creator>
    </item>
    <item>
      <title>Cooperative missile guidance design using Distributed Nonlinear Dynamic Inversion</title>
      <link>https://arxiv.org/abs/2509.18022</link>
      <description>arXiv:2509.18022v1 Announce Type: new 
Abstract: This paper presents a new cooperative guidance algorithm based on Distributed Nonlinear Dynamic Inversion (DNDI) to demonstrate a coordinated missile attack.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18022v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sabyasachi Mondal, Bhaskar Biswas, Venkatraman Renganathan</dc:creator>
    </item>
    <item>
      <title>Deep Learning as the Disciplined Construction of Tame Objects</title>
      <link>https://arxiv.org/abs/2509.18025</link>
      <description>arXiv:2509.18025v1 Announce Type: new 
Abstract: One can see deep-learning models as compositions of functions within the so-called tame geometry. In this expository note, we give an overview of some topics at the interface of tame geometry (also known as o-minimality), optimization theory, and deep learning theory and practice. To do so, we gradually introduce the concepts and tools used to build convergence guarantees for stochastic gradient descent in a general nonsmooth nonconvex, but tame, setting. This illustrates some ways in which tame geometry is a natural mathematical framework for the study of AI systems, especially within Deep Learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18025v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.LO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Gilles Bareilles, Allen Gehret, Johannes Aspman, Jana Lep\v{s}ov\'a, Jakub Mare\v{c}ek</dc:creator>
    </item>
    <item>
      <title>Pareto-Optimal Linear Programming</title>
      <link>https://arxiv.org/abs/2509.18073</link>
      <description>arXiv:2509.18073v1 Announce Type: new 
Abstract: Pareto-optimality plays a central role in evaluating the efficiency of solutions to allocation problems, such as house allocation, school choice, and kidney exchange. We introduce a general linear programming problem subject to Pareto-optimality conditions, which we call Max-Pareto. Using the novel result that Pareto-optimal bipartite matchings are fractionally Pareto-optimal, we prove that Max-Pareto is $\mathcal{NP}$-complete. We propose a bilinear programming formulation of Max-Pareto, and evaluate its computational performance on the problem of finding Pareto-optimal allocations of highest welfare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18073v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bart van Rossum, Twan Dollevoet</dc:creator>
    </item>
    <item>
      <title>Optimal Sizing of Community Photovoltaic and Battery Energy Storage Systems with Second-Life Batteries in Peer-to-Peer Energy Communities</title>
      <link>https://arxiv.org/abs/2509.18082</link>
      <description>arXiv:2509.18082v1 Announce Type: new 
Abstract: This article presents a mixed-integer second-order cone programming model to determine the optimal sizing of a community-shared photovoltaic and battery energy storage system (PV-BESS) within a peer-to-peer (P2P) energy trading framework. The model accounts for heterogeneous users who may already own individual PV or PV-BESS systems and aims to enhance the overall energy autonomy of the energy community. A key feature of the model is the explicit comparison between first-life (FL) and second-life (SL) battery technologies, incorporating their respective degradation dynamics into investment and operational decisions, and the technical feasibility by considering constraints of a low-voltage distribution network. The proposed formulation is tested on the reduced equivalent of the IEEE European low-voltage network. Results show that the most influential factors in the adoption of a shared BESS are: (i) the market cost of battery technologies, (ii) electricity tariffs, particularly purchase prices, and (iii) the degradation characteristics of the chosen technology. Secondary factors, such as DER penetration among users and the community's peak demand, have a lesser impact. The analysis further suggests that SL batteries could become a cost-effective alternative to FL technologies if their degradation performance improves or their capital cost is significantly reduced.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18082v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>J\'ulia Monar, Fernando Garc\'ia-Mu\~noz, Natalia Jorquera Bravo, Joaqu\'in Aballay Araya, Vicente Castro Burgos</dc:creator>
    </item>
    <item>
      <title>Reconstructing Physics-Informed Machine Learning for Traffic Flow Modeling: a Multi-Gradient Descent and Pareto Learning Approach</title>
      <link>https://arxiv.org/abs/2505.13241</link>
      <description>arXiv:2505.13241v3 Announce Type: cross 
Abstract: Physics-informed machine learning (PIML) is crucial in modern traffic flow modeling because it combines the benefits of both physics-based and data-driven approaches. In conventional PIML, physical information is typically incorporated by constructing a hybrid loss function that combines data-driven loss and physics loss through linear scalarization. The goal is to find a trade-off between these two objectives to improve the accuracy of model predictions. However, from a mathematical perspective, linear scalarization is limited to identifying only the convex region of the Pareto front, as it treats data-driven and physics losses as separate objectives. Given that most PIML loss functions are non-convex, linear scalarization restricts the achievable trade-off solutions. Moreover, tuning the weighting coefficients for the two loss components can be both time-consuming and computationally challenging. To address these limitations, this paper introduces a paradigm shift in PIML by reformulating the training process as a multi-objective optimization problem, treating data-driven loss and physics loss independently. We apply several multi-gradient descent algorithms (MGDAs), including traditional multi-gradient descent (TMGD) and dual cone gradient descent (DCGD), to explore the Pareto front in this multi-objective setting. These methods are evaluated on both macroscopic and microscopic traffic flow models. In the macroscopic case, MGDAs achieved comparable performance to traditional linear scalarization methods. Notably, in the microscopic case, MGDAs significantly outperformed their scalarization-based counterparts, demonstrating the advantages of a multi-objective optimization approach in complex PIML scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13241v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.trc.2025.105344</arxiv:DOI>
      <arxiv:journal_reference>Transportation Research Part C: Emerging Technologies, Volume 180, November 2025, 105344</arxiv:journal_reference>
      <dc:creator>Yuan-Zheng Lei, Yaobang Gong, Dianwei Chen, Yao Cheng, Xianfeng Terry Yang</dc:creator>
    </item>
    <item>
      <title>Predictive Machine Learning to Increase the Throughput of Container Yards</title>
      <link>https://arxiv.org/abs/2509.16207</link>
      <description>arXiv:2509.16207v1 Announce Type: cross 
Abstract: This study seeks to improve the throughput rates for shipping container terminals. In the United States, shipping ports link the domestic economy to global markets and are vital to sustain supply chain flow and economic stability. Maritime shipping accounts for nearly half of the U.S.'s annual international trade, two thirds of which are represented by container shipping. Previous studies highlighted the capability of automation in enhancing container processing; however, unlike in European and East Asian ports, full automation is limited in U.S. ports due to legal protections for human labor. Consequently, there is a need for alternative methods that deliver automation level efficiencies while maintaining the terms of cooperative agreements. This paper proposes an Intelligent Planning System (IPS) that applies the concept of Pareto Optimization to container yards through a mixed integer linear programming (MILP) based recursive appointment system. The results show an improvement from baseline for both daily terminal throughput volumes and processing times. The generated IPS can be employed to provide recommendations for container positioning and truck pickup appointments to optimize container yard layout and flow resulting in reduced realtime congestion and predictively mitigated future congestion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16207v1</guid>
      <category>cs.CE</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Austin Ford Cooper</dc:creator>
    </item>
    <item>
      <title>"It Was a Magical Box": Understanding Practitioner Workflows and Needs in Optimization</title>
      <link>https://arxiv.org/abs/2509.16402</link>
      <description>arXiv:2509.16402v1 Announce Type: cross 
Abstract: Optimization underpins decision-making in domains from healthcare to logistics, yet for many practitioners it remains a "magical box": powerful but opaque, difficult to use, and reliant on specialized expertise. While prior work has extensively studied machine learning workflows, the everyday practices of optimization model developers (OMDs) have received little attention. We conducted semi-structured interviews with 15 OMDs across diverse domains to examine how optimization is done in practice. Our findings reveal a highly iterative workflow spanning six stages: problem elicitation, data processing, model development, implementation, validation, and deployment. Importantly, we find that optimization practice is not only about algorithms that deliver better decisions, but is equally shaped by data and dialogue - the ongoing communication with stakeholders that enables problem framing, trust, and adoption. We discuss opportunities for future tooling that foregrounds data and dialogue alongside decision-making, opening new directions for human-centered optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16402v1</guid>
      <category>cs.HC</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Connor Lawless, Jakob Schoeffer, Madeleine Udell</dc:creator>
    </item>
    <item>
      <title>Safe Guaranteed Dynamics Exploration with Probabilistic Models</title>
      <link>https://arxiv.org/abs/2509.16650</link>
      <description>arXiv:2509.16650v1 Announce Type: cross 
Abstract: Ensuring both optimality and safety is critical for the real-world deployment of agents, but becomes particularly challenging when the system dynamics are unknown. To address this problem, we introduce a notion of maximum safe dynamics learning via sufficient exploration in the space of safe policies. We propose a $\textit{pessimistically}$ safe framework that $\textit{optimistically}$ explores informative states and, despite not reaching them due to model uncertainty, ensures continuous online learning of dynamics. The framework achieves first-of-its-kind results: learning the dynamics model sufficiently $-$ up to an arbitrary small tolerance (subject to noise) $-$ in a finite time, while ensuring provably safe operation throughout with high probability and without requiring resets. Building on this, we propose an algorithm to maximize rewards while learning the dynamics $\textit{only to the extent needed}$ to achieve close-to-optimal performance. Unlike typical reinforcement learning (RL) methods, our approach operates online in a non-episodic setting and ensures safety throughout the learning process. We demonstrate the effectiveness of our approach in challenging domains such as autonomous car racing and drone navigation under aerodynamic effects $-$ scenarios where safety is critical and accurate modeling is difficult.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16650v1</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manish Prajapat, Johannes K\"ohler, Melanie N. Zeilinger, Andreas Krause</dc:creator>
    </item>
    <item>
      <title>Data-Driven Two-Stage IRS-Aided Sumrate Maximization with Inexact Precoding</title>
      <link>https://arxiv.org/abs/2509.16776</link>
      <description>arXiv:2509.16776v1 Announce Type: cross 
Abstract: We propose iZoSGA, a data-driven learning algorithm for joint passive long-term intelligent reflective surface (IRS)-aided beamforming and active short-term precoding in wireless networks. iZoSGA is based on a zeroth-order stochastic quasigradient ascent methodology designed for tackling two-stage nonconvex stochastic programs with continuous uncertainty and objective functions with "black-box" terms, and where second-stage optimization is inexact. As such, iZoSGA utilizes inexact precoding oracles, enabling practical implementation when short-term (e.g., WMMSE-based) beamforming is solved approximately. The proposed method is agnostic to channel models or statistics, and applies to arbitrary IRS/network configurations. We prove non-asymptotic convergence of iZoSGA to a neighborhood of a stationary solution of the original exact problem under minimal assumptions. Our numerics confirm the efficacy iZoSGA in several "inexact regimes", enabling passive yet fully effective IRS operation in diverse and realistic IRS-aided scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16776v1</guid>
      <category>eess.SP</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hassaan Hashmi, Spyridon Pougkakiotis, Dionysis Kalogerias</dc:creator>
    </item>
    <item>
      <title>The Complexity of Finding Local Optima in Contrastive Learning</title>
      <link>https://arxiv.org/abs/2509.16898</link>
      <description>arXiv:2509.16898v1 Announce Type: cross 
Abstract: Contrastive learning is a powerful technique for discovering meaningful data representations by optimizing objectives based on $\textit{contrastive information}$, often given as a set of weighted triplets $\{(x_i, y_i^+, z_{i}^-)\}_{i = 1}^m$ indicating that an "anchor" $x_i$ is more similar to a "positive" example $y_i$ than to a "negative" example $z_i$. The goal is to find representations (e.g., embeddings in $\mathbb{R}^d$ or a tree metric) where anchors are placed closer to positive than to negative examples. While finding $\textit{global}$ optima of contrastive objectives is $\mathsf{NP}$-hard, the complexity of finding $\textit{local}$ optima -- representations that do not improve by local search algorithms such as gradient-based methods -- remains open. Our work settles the complexity of finding local optima in various contrastive learning problems by proving $\mathsf{PLS}$-hardness in discrete settings (e.g., maximize satisfied triplets) and $\mathsf{CLS}$-hardness in continuous settings (e.g., minimize Triplet Loss), where $\mathsf{PLS}$ (Polynomial Local Search) and $\mathsf{CLS}$ (Continuous Local Search) are well-studied complexity classes capturing local search dynamics in discrete and continuous optimization, respectively. Our results imply that no polynomial time algorithm (local search or otherwise) can find a local optimum for various contrastive learning problems, unless $\mathsf{PLS}\subseteq\mathsf{P}$ (or $\mathsf{CLS}\subseteq \mathsf{P}$ for continuous problems). Even in the unlikely scenario that $\mathsf{PLS}\subseteq\mathsf{P}$ (or $\mathsf{CLS}\subseteq \mathsf{P}$), our reductions imply that there exist instances where local search algorithms need exponential time to reach a local optimum, even for $d=1$ (embeddings on a line).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16898v1</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingming Yan, Yiyuan Luo, Vaggos Chatziafratis, Ioannis Panageas, Parnian Shahkar, Stelios Stavroulakis</dc:creator>
    </item>
    <item>
      <title>Geometric Interpolation of Rigid Body Motions</title>
      <link>https://arxiv.org/abs/2509.16966</link>
      <description>arXiv:2509.16966v1 Announce Type: cross 
Abstract: The problem of interpolating a rigid body motion is to find a spatial trajectory between a prescribed initial and terminal pose. Two variants of this interpolation problem are addressed. The first is to find a solution that satisfies initial conditions on the k-1 derivatives of the rigid body twist. This is called the kth-order initial value trajectory interpolation problem (k-IV-TIP). The second is to find a solution that satisfies conditions on the rigid body twist and its k-1 derivatives at the initial and terminal pose. This is called the kth-order boundary value trajectory interpolation problem (k-BV-TIP). Solutions to the k-IV-TIP for k=1,...,4, i.e. the initial twist and up to the 4th time derivative are prescribed. Further, a solution to the 1-IV-TBP is presented, i.e. the initial and terminal twist are prescribed. The latter is a novel cubic interpolation between two spatial configurations with given initial and terminal twist. This interpolation is automatically identical to the minimum acceleration curve when the twists are set to zero. The general approach to derive higher-order solutions is presented. Numerical results are shown for two examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16966v1</guid>
      <category>cs.RO</category>
      <category>cs.NA</category>
      <category>math.DG</category>
      <category>math.GR</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-45705-0_31</arxiv:DOI>
      <arxiv:journal_reference>In: Okada, M. (eds) Advances in Mechanism and Machine Science. IFToMM WC 2023. Mechanisms and Machine Science, vol 147. Springer, Cham</arxiv:journal_reference>
      <dc:creator>Andreas Mueller</dc:creator>
    </item>
    <item>
      <title>Communication over LQG Control Systems: A Convex Optimization Approach to Capacity</title>
      <link>https://arxiv.org/abs/2509.17002</link>
      <description>arXiv:2509.17002v1 Announce Type: cross 
Abstract: We study communication over control systems, where a controller-encoder selects inputs to a dynamical system in order to simultaneously regulate the system and convey a message to an observer that has access to the system's output measurements. This setup reflects implicit communication, as the controller embeds a message in the control signal. The capacity of a control system is the maximal reliable rate of the embedded message subject to a closed-loop control-cost constraint. We focus on linear quadratic Gaussian (LQG) control systems, in which the dynamical system is given by a state-space model with Gaussian noise, and the control cost is a quadratic function of the system inputs and system states. Our main result is a convex optimization upper bound on the capacity of LQG systems. In the case of scalar systems, we prove that the upper bound yields the exact LQG system capacity. The upper bound also recovers all known results, including LQG control, feedback capacity of Gaussian channels with memory, and the LQG system capacity with a state-feedback. For vector LQG control systems, we provide a sufficient condition for tightness of the upper bound, based on the Riccati equation. Numerical simulations indicate the upper bound tightness in all tested examples, suggesting that the upper bound may be equal to the LQG system capacity in the vector case as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17002v1</guid>
      <category>cs.IT</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aharon Rips, Oron Sabag</dc:creator>
    </item>
    <item>
      <title>Smooth hyperbolicity cones are second-order cone representable</title>
      <link>https://arxiv.org/abs/2509.17121</link>
      <description>arXiv:2509.17121v1 Announce Type: cross 
Abstract: Netzer and Sanyal proved that every smooth hyperbolicity cone is a spectrahedral shadow. We generalize and sharpen this result at the same time, by showing that every Nash-smooth hyperbolicity cone is even second-order cone representable (socr). The result is proved as a consequence of another theorem, according to which every compact convex semialgebraic set is socr, provided that its boundary is Nash-smooth of strict positive curvature. The proof uses the technique of tensor evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17121v1</guid>
      <category>math.AG</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Claus Scheiderer</dc:creator>
    </item>
    <item>
      <title>Neural Network and ANFIS based auto-adaptive MPC for path tracking in autonomous vehicles</title>
      <link>https://arxiv.org/abs/2509.17213</link>
      <description>arXiv:2509.17213v1 Announce Type: cross 
Abstract: Self-driving cars operate in constantly changing environments and are exposed to a variety of uncertainties and disturbances. These factors render classical controllers ineffective, especially for lateral control. Therefore, an adaptive MPC controller is designed in this paper for the path tracking task, tuned by an improved particle swarm optimization algorithm. Online parameter adaptation is performed using Neural Networks and ANFIS. The designed controller showed promising results compared to standard MPC in triple lane change and trajectory tracking scenarios. Code can be found here: https://github.com/yassinekebbati/NN_MPC-vs-ANFIS_MPC</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17213v1</guid>
      <category>cs.RO</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ICNSC52481.2021.9702227</arxiv:DOI>
      <arxiv:journal_reference>2021 IEEE International Conference on Networking, Sensing and Control (ICNSC)</arxiv:journal_reference>
      <dc:creator>Yassine Kebbati, Naima Ait-Oufroukh, Vincent Vigneron, Dalil Ichala</dc:creator>
    </item>
    <item>
      <title>Learning and Optimization with 3D Orientations</title>
      <link>https://arxiv.org/abs/2509.17274</link>
      <description>arXiv:2509.17274v1 Announce Type: cross 
Abstract: There exist numerous ways of representing 3D orientations. Each representation has both limitations and unique features. Choosing the best representation for one task is often a difficult chore, and there exist conflicting opinions on which representation is better suited for a set of family of tasks. Even worse, when dealing with scenarios where we need to learn or optimize functions with orientations as inputs and/or outputs, the set of possibilities (representations, loss functions, etc.) is even larger and it is not easy to decide what is best for each scenario. In this paper, we attempt to a) present clearly, concisely and with unified notation all available representations, and "tricks" related to 3D orientations (including Lie Group algebra), and b) benchmark them in representative scenarios. The first part feels like it is missing from the robotics literature as one has to read many different textbooks and papers in order have a concise and clear understanding of all possibilities, while the benchmark is necessary in order to come up with recommendations based on empirical evidence. More precisely, we experiment with the following settings that attempt to cover most widely used scenarios in robotics: 1) direct optimization, 2) imitation/supervised learning with a neural network controller, 3) reinforcement learning, and 4) trajectory optimization using differential dynamic programming. We finally provide guidelines depending on the scenario, and make available a reference implementation of all the orientation math described.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17274v1</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandros Ntagkas, Constantinos Tsakonas, Chairi Kiourt, Konstantinos Chatzilygeroudis</dc:creator>
    </item>
    <item>
      <title>Trajectory Encryption Cooperative Salvo Guidance</title>
      <link>https://arxiv.org/abs/2509.17341</link>
      <description>arXiv:2509.17341v1 Announce Type: cross 
Abstract: This paper introduces the concept of trajectory encryption in cooperative simultaneous target interception, wherein heterogeneity in guidance principles across a team of unmanned autonomous systems is leveraged as a strategic design feature. By employing a mix of heterogeneous time-to-go formulations leading to a cooperative guidance strategy, the swarm of vehicles is able to generate diverse trajectory families. This diversity expands the feasible solution space for simultaneous target interception, enhances robustness under disturbances, and enables flexible time-to-go adjustments without predictable detouring. From an adversarial perspective, heterogeneity obscures the collective interception intent by preventing straightforward prediction of swarm dynamics, effectively acting as an encryption layer in the trajectory domain. Simulations demonstrate that the swarm of heterogeneous vehicles is able to intercept a moving target simultaneously from a diverse set of initial engagement configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17341v1</guid>
      <category>eess.SY</category>
      <category>cs.MA</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lohitvel Gopikannan, Shashi Ranjan Kumar, Abhinav Sinha</dc:creator>
    </item>
    <item>
      <title>On the vanishing viscosity limit of Hamilton-Jacobi equations with nearly optimal discount</title>
      <link>https://arxiv.org/abs/2509.17402</link>
      <description>arXiv:2509.17402v1 Announce Type: cross 
Abstract: In this paper, we establish the convergence of solutions to the viscous Hamilton-Jacobi equation (with a Tonelli Hamiltonian): \[
  \lambda u +H(x, du)=\varepsilon(\lambda)\Delta u,\quad \lambda&gt;0 \]
  as $\lambda\rightarrow 0_+$, once the modulus $\varepsilon(\lambda)$ satisfies $\varlimsup_{\lambda\rightarrow 0_+}\varepsilon(\lambda)/\lambda=0$. Such an exponent of $\varepsilon(\lambda)$ is nearly optimal in the convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17402v1</guid>
      <category>math.AP</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zibo Wang, Jianlu Zhang</dc:creator>
    </item>
    <item>
      <title>Distributionally Robust Safety Verification of Neural Networks via Worst-Case CVaR</title>
      <link>https://arxiv.org/abs/2509.17413</link>
      <description>arXiv:2509.17413v1 Announce Type: cross 
Abstract: Ensuring the safety of neural networks under input uncertainty is a fundamental challenge in safety-critical applications. This paper builds on and expands Fazlyab's quadratic-constraint (QC) and semidefinite-programming (SDP) framework for neural network verification to a distributionally robust and tail-risk-aware setting by integrating worst-case Conditional Value-at-Risk (WC-CVaR) over a moment-based ambiguity set with fixed mean and covariance. The resulting conditions remain SDP-checkable and explicitly account for tail risk. This integration broadens input-uncertainty geometry-covering ellipsoids, polytopes, and hyperplanes-and extends applicability to safety-critical domains where tail-event severity matters. Applications to closed-loop reachability of control systems and classification are demonstrated through numerical experiments, illustrating how the risk level $\varepsilon$ trades conservatism for tolerance to tail events-while preserving the computational structure of prior QC/SDP methods for neural network verification and robustness analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17413v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masako Kishida</dc:creator>
    </item>
    <item>
      <title>Infinite Euclidean Distance Discriminants</title>
      <link>https://arxiv.org/abs/2509.17456</link>
      <description>arXiv:2509.17456v1 Announce Type: cross 
Abstract: We study infinite Euclidean distance discriminants of algebraic varieties, defined as the loci of data points whose fibers under the second projection from the Euclidean distance correspondence are positive-dimensional. In particular, these discriminants contain all data points with infinitely many critical points for the nearest-point problem. We present computer code that computes the infinite Euclidean distance discriminant, and use it to present numerous varieties with nonempty such discriminants. Moreover, we prove that for any data point, the fiber under the second projection is contained in a finite union of hyperspheres centered at that point. For curves, we include a complete characterization; their infinite Euclidean distance discriminants turn out to be affine linear spaces. Finally, we introduce and characterize skew-tube surfaces in three-dimensional space. By construction, these have a one-dimensional infinite Euclidean distance discriminant. We further demonstrate that many skew-tubes have significantly lower Euclidean distance degrees than generic surfaces of the same degree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17456v1</guid>
      <category>math.AG</category>
      <category>math.DG</category>
      <category>math.MG</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felix Rydell, Emil Horobet</dc:creator>
    </item>
    <item>
      <title>Impossibility Results of Card-Based Protocols via Mathematical Optimization</title>
      <link>https://arxiv.org/abs/2509.17595</link>
      <description>arXiv:2509.17595v1 Announce Type: cross 
Abstract: This paper introduces mathematical optimization as a new method for proving impossibility proofs in the field of card-based cryptography. While previous impossibility proofs were often limited to cases involving a small number of cards, this new approach establishes results that hold for a large number of cards. The research focuses on single-cut full-open (SCFO) protocols, which consist of performing one random cut and then revealing all cards. The main contribution is that for any three-variable Boolean function, no new SCFO protocols exist beyond those already known, under the condition that all additional cards have the same color. The significance of this work is that it provides a new framework for impossibility proofs and delivers a proof that is valid for any number of cards, as long as all additional cards have the same color.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17595v1</guid>
      <category>cs.CR</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunnosuke Ikeda, Kazumasa Shinagawa</dc:creator>
    </item>
    <item>
      <title>EigenSafe: A Spectral Framework for Learning-Based Stochastic Safety Filtering</title>
      <link>https://arxiv.org/abs/2509.17750</link>
      <description>arXiv:2509.17750v1 Announce Type: cross 
Abstract: We present EigenSafe, an operator-theoretic framework for learning-enabled safety-critical control for stochastic systems. In many robotic systems where dynamics are best modeled as stochastic systems due to factors such as sensing noise and environmental disturbances, it is challenging for conventional methods such as Hamilton-Jacobi reachability and control barrier functions to provide a holistic measure of safety. We derive a linear operator governing the dynamic programming principle for safety probability, and find that its dominant eigenpair provides information about safety for both individual states and the overall closed-loop system. The proposed learning framework, called EigenSafe, jointly learns this dominant eigenpair and a safe backup policy in an offline manner. The learned eigenfunction is then used to construct a safety filter that detects potentially unsafe situations and falls back to the backup policy. The framework is validated in three simulated stochastic safety-critical control tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17750v1</guid>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Inkyu Jang, Jonghae Park, Chams E. Mballo, Sihyun Cho, Claire J. Tomlin, H. Jin Kim</dc:creator>
    </item>
    <item>
      <title>Stabilizability and lower spectral radius for linear switched systems with singular matrices</title>
      <link>https://arxiv.org/abs/2509.17799</link>
      <description>arXiv:2509.17799v1 Announce Type: cross 
Abstract: We investigate the stabilizability of linear discrete-time switched systems with singular matrices, focusing on the spectral radius in this context. A new lower bound of the stabilizability radius is proposed, which is applicable to any matrix set. Based on this lower bound, more relationships between the stabilizability radius and joint spectral subradius are established. Detailed analysis of the stabilizability radius of a special kind of two-dimensional switched system, consisting of a singular matrix and a rotation matrix, is presented. The Hausdorff dimensions of the parameter sets such that the stabilizability radius of these systems equals a constant are also presented. Other properties of switched systems with singular matrices are also discussed along with examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17799v1</guid>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carl P. Dettmann, Chenmiao Zhang</dc:creator>
    </item>
    <item>
      <title>Global Optimization via Softmin Energy Minimization</title>
      <link>https://arxiv.org/abs/2509.17815</link>
      <description>arXiv:2509.17815v1 Announce Type: cross 
Abstract: Global optimization, particularly for non-convex functions with multiple local minima, poses significant challenges for traditional gradient-based methods. While metaheuristic approaches offer empirical effectiveness, they often lack theoretical convergence guarantees and may disregard available gradient information. This paper introduces a novel gradient-based swarm particle optimization method designed to efficiently escape local minima and locate global optima. Our approach leverages a "Soft-min Energy" interacting function, $J_\beta(\mathbf{x})$, which provides a smooth, differentiable approximation of the minimum function value within a particle swarm. We define a stochastic gradient flow in the particle space, incorporating a Brownian motion term for exploration and a time-dependent parameter $\beta$ to control smoothness, similar to temperature annealing. We theoretically demonstrate that for strongly convex functions, our dynamics converges to a stationary point where at least one particle reaches the global minimum, with other particles exhibiting exploratory behavior. Furthermore, we show that our method facilitates faster transitions between local minima by reducing effective potential barriers with respect to Simulated Annealing. More specifically, we estimate the hitting times of unexplored potential wells for our model in the small noise regime and show that they compare favorably with the ones of overdamped Langevin. Numerical experiments on benchmark functions, including double wells and the Ackley function, validate our theoretical findings and demonstrate better performance over the well-known Simulated Annealing method in terms of escaping local minima and achieving faster convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17815v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Agazzi, Vittorio Carlei, Marco Romito, Samuele Saviozzi</dc:creator>
    </item>
    <item>
      <title>Quantum Portfolio Optimization: An Extensive Benchmark</title>
      <link>https://arxiv.org/abs/2509.17876</link>
      <description>arXiv:2509.17876v1 Announce Type: cross 
Abstract: Recently, several researchers proposed portfolio optimization as a potential use case for quantum optimization. However, the literature is lacking an extensive benchmark quantifying the potential of quantum computers for portfolio optimization. In this work, we fill this gap. We provide a computational study, comparing quantum approaches against state-of-the-art classical methods on a meaningful, real-world instance set. In particular, we compare quantum annealing and the quantum approximate optimization algorithm against classical mixed-integer programming, simulated annealing, steepest descent local search, tabu search and a problem-tailored heuristics. We consider a variant of portfolio optimization which we show to be particular difficult for classical solvers in practice. Our benchmark comprises 250 instances with up to 1,000 assets from actual stock data. The results show that all instances can be solved to proven optimality by mixed-integer programming in the order of seconds. Moreover, the problem-tailored heuristic consistently outperforms quantum approaches in terms of solution quality for fixed runtime. Thus, we conclude that there is only very limited room for a potential quantum advantage in portfolio optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17876v1</guid>
      <category>quant-ph</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Stopfer, Friedrich Wagner</dc:creator>
    </item>
    <item>
      <title>Control Disturbance Rejection in Neural ODEs</title>
      <link>https://arxiv.org/abs/2509.18034</link>
      <description>arXiv:2509.18034v1 Announce Type: cross 
Abstract: In this paper, we propose an iterative training algorithm for Neural ODEs that provides models resilient to control (parameter) disturbances. The method builds on our earlier work Tuning without Forgetting-and similarly introduces training points sequentially, and updates the parameters on new data within the space of parameters that do not decrease performance on the previously learned training points-with the key difference that, inspired by the concept of flat minima, we solve a minimax problem for a non-convex non-concave functional over an infinite-dimensional control space. We develop a projected gradient descent algorithm on the space of parameters that admits the structure of an infinite-dimensional Banach subspace. We show through simulations that this formulation enables the model to effectively learn new data points and gain robustness against control disturbance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18034v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erkan Bayram, Mohamed-Ali Belabbas, Tamer Ba\c{s}ar</dc:creator>
    </item>
    <item>
      <title>Theoretical analysis of the randomized subspace regularized Newton method for non-convex optimization</title>
      <link>https://arxiv.org/abs/2209.04170</link>
      <description>arXiv:2209.04170v3 Announce Type: replace 
Abstract: While there already exist randomized subspace Newton methods that restrict the search direction to a random subspace for a convex function, we propose a randomized subspace regularized Newton method for a non-convex function {and more generally we investigate thoroughly the local convergence rate of the randomized subspace Newton method}.
  In our proposed algorithm using a modified Hessian of the function restricted to some random subspace, with high probability, the function value decreases even when the objective function is non-convex.
  In this paper, we show that our method has global convergence under appropriate assumptions and its convergence rate is the same as that of the full regularized Newton method. %We also prove that Furthermore, we can obtain a local linear convergence rate under some additional assumptions, and prove that this rate is the best we can hope, in general, when using random subspace. We furthermore prove that if the Hessian at the local optimum is rank defficient then superlienar convergence holds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.04170v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Terunari Fuji, Pierre-Louis Poirion, Akiko Takeda</dc:creator>
    </item>
    <item>
      <title>LieDetect: Detection of representation orbits of compact Lie groups from point clouds</title>
      <link>https://arxiv.org/abs/2309.03086</link>
      <description>arXiv:2309.03086v3 Announce Type: replace 
Abstract: We suggest a new algorithm to estimate representations of compact Lie groups from finite samples of their orbits. Different from other reported techniques, our method allows the retrieval of the precise representation type as a direct sum of irreducible representations. Moreover, the knowledge of the representation type permits the reconstruction of its orbit, which is useful for identifying the Lie group that generates the action, from a finite list of candidates. Our algorithm is general for any compact Lie group, but only instantiations for SO(2), T^d, SU(2), and SO(3) are considered. Theoretical guarantees of robustness in terms of Hausdorff and Wasserstein distances are derived. Our tools are drawn from geometric measure theory, computational geometry, and optimization on matrix manifolds. The algorithm is tested for synthetic data up to dimension 32, as well as real-life applications in image analysis, harmonic analysis, density estimation, equivariant neural networks, chemical conformational spaces, and classical mechanics systems, achieving very accurate results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.03086v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.RT</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10208-025-09728-4</arxiv:DOI>
      <arxiv:journal_reference>Foundations of Computational Mathematics (2025)</arxiv:journal_reference>
      <dc:creator>Henrique Ennes, Rapha\"el Tinarrage</dc:creator>
    </item>
    <item>
      <title>Minimizing transients via the Kreiss system norm</title>
      <link>https://arxiv.org/abs/2309.04718</link>
      <description>arXiv:2309.04718v2 Announce Type: replace 
Abstract: We introduce system norms which assess transient behavior of stable Linear Time-Invariant (LTI) systems. This allows us to address undesired responses to initial conditions, finite resource consumption signals, or persistent perturbations. We then consider the challenging problem of minimizing these norms in closed loop using structured linear feedback. The computed controllers mitigate transients in a linearized closed loop, with the potential side effect of enlarging the region of stability of the underlying non-linear controlled system. In applications this helps to prevent transition to undesired nonlinear regimes, limit cycles or chaotic behavior. The success of our approach is certified a posteriori using Lyapunov-like techniques and simulations, as we demonstrate through a variety of applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.04718v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierre Apkarian, Dominikus Noll</dc:creator>
    </item>
    <item>
      <title>Stochastic Bregman Proximal Gradient Method Revisited: Kernel Conditioning and Painless Variance Reduction</title>
      <link>https://arxiv.org/abs/2401.03155</link>
      <description>arXiv:2401.03155v4 Announce Type: replace 
Abstract: We investigate stochastic Bregman proximal gradient (SBPG) methods for minimizing a finite-sum nonconvex function $\Psi(x):=\frac{1}{n}\sum_{i=1}^nf_i(x)+\phi(x)$, where $\phi$ is convex and nonsmooth, while $f_i$, instead of gradient global Lipschitz continuity, satisfies a smooth-adaptability condition w.r.t. some kernel $h$. Standard acceleration techniques for stochastic algorithms (momentum, shuffling, variance reduction) depend on bounding stochastic errors by gradient differences that are further controlled via Lipschitz property. Lacking this, existing SBPG results are limited to vanilla stochastic approximation that cannot yield the optimal $O(\sqrt{n})$ complexity dependence on $n$. Moreover, existing works report complexities under various nonstandard stationarity measures that largely deviate from the standard minimal limiting Fr\'echet subdifferential $\mathrm{dist}(0,\partial\Psi(\cdot))$. Our analysis reveals that these popular stationarity measures are often much smaller than $\mathrm{dist}(0,\partial\Psi(\cdot))$, leading to overstated solution quality and non-stationary output. To resolve these issues, we design a new gradient mapping $\mathcal{D}_{\phi,h}^\lambda (\cdot)$ by BPG residuals in dual space and a new kernel-conditioning (KC) regularity, under which the mismatch between $\|\mathcal{D}_{\phi,h}^\lambda (\cdot)\|$ and $\mathrm{dist}(0,\partial\Psi(\cdot))$ is provably $O(1)$ and instance-free. Moreover, KC-regularity guarantees Lipschitz-like bounds for gradient differences, providing general analysis tools for momentum, shuffling, and variance reduction under smooth-adaptability. We illustrate this point on variance reduced SBPG methods and establish an $O(\sqrt{n})$ complexity for $\|\mathcal{D}_{\phi,h}^\lambda (\cdot)\|$, providing instance-free (worst-case) complexity under $\mathrm{dist}(0,\partial\Psi(\cdot))$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03155v4</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyu Zhang</dc:creator>
    </item>
    <item>
      <title>Data-Driven Discovery of PDEs via the Adjoint Method</title>
      <link>https://arxiv.org/abs/2401.17177</link>
      <description>arXiv:2401.17177v5 Announce Type: replace 
Abstract: In this work, we present an adjoint-based method for discovering the underlying governing partial differential equations (PDEs) given data. The idea is to consider a parameterized PDE in a general form and formulate a PDE-constrained optimization problem aimed at minimizing the error of the PDE solution from data. Using variational calculus, we obtain an evolution equation for the Lagrange multipliers (adjoint equations) allowing us to compute the gradient of the objective function with respect to the parameters of PDEs given data in a straightforward manner. In particular, we consider a family of parameterized PDEs encompassing linear, nonlinear, and spatial derivative candidate terms, and elegantly derive the corresponding adjoint equations. We show the efficacy of the proposed approach in identifying the form of the PDE up to machine accuracy, enabling the accurate discovery of PDEs from data. We also compare its performance with the famous PDE Functional Identification of Nonlinear Dynamics method known as PDE-FIND (Rudy et al., 2017), on both smooth and noisy data sets. Even though the proposed adjoint method relies on forward/backward solvers, it outperforms PDE-FIND for large data sets thanks to the analytic expressions for gradients of the cost function with respect to each PDE parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17177v5</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohsen Sadr, Tony Tohme, Kamal Youcef-Toumi</dc:creator>
    </item>
    <item>
      <title>Dynamic Facility Location under Cumulative Customer Demand</title>
      <link>https://arxiv.org/abs/2405.02439</link>
      <description>arXiv:2405.02439v3 Announce Type: replace 
Abstract: Dynamic facility location problems aim at placing one or more valuable resources over a planning horizon to meet customer demand. Existing literature commonly assumes that customer demand quantities are defined independently for each time period. In many planning contexts, however, unmet demand carries over to future time periods. Unmet demand at some time periods may therefore affect decisions of subsequent time periods. This work studies a novel location problem, where the decision maker places facilities over time to capture cumulative customer demand. We propose two mixed-integer programming formulations for this problem, and show that one of them has a tighter continuous relaxation and allows the representation of more general customer demand behaviour. We characterize the computational complexity for this problem, and analyze which problem characteristics result in NP-hardness. We then propose an exact branch-and-Benders-cut method, and show that this method is approximately five times faster, on average, than solving the tighter formulation directly in our computational experiments. Our results also quantify the benefit of accounting for cumulative customer demand within the optimization framework, since the corresponding planning solutions perform much better than those obtained by ignoring cumulative demand or employing myopic heuristics. We also draw managerial insights on the quality of service perceived by customers when the provider places facilities under cumulative customer demand.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02439v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Warley Almeida Silva, Margarida Carvalho, Sanjay Dominik Jena</dc:creator>
    </item>
    <item>
      <title>Chordal-NMF with Riemannian Multiplicative Update</title>
      <link>https://arxiv.org/abs/2405.12823</link>
      <description>arXiv:2405.12823v4 Announce Type: replace 
Abstract: Nonnegative Matrix Factorization (NMF) is the problem of approximating a given nonnegative matrix M through the product of two nonnegative low-rank matrices W and H. Traditionally NMF is tackled by optimizing a specific objective function evaluating the quality of the approximation. This assessment is often done based on the Frobenius norm (F-norm). In this work, we argue that the F-norm, as the ``point-to-point'' distance, may not always be appropriate. Viewing from the perspective of cone, NMF may not naturally align with F-norm. So, a ray-to-ray chordal distance is proposed as an alternative way of measuring the quality of the approximation. As this measure corresponds to the Euclidean distance on the sphere, it motivates the use of manifold optimization techniques. We apply Riemannian optimization technique to solve chordal-NMF by casting it on a manifold. Unlike works on Riemannian optimization that require the manifold to be smooth, the nonnegativity in chordal-NMF defines a non-differentiable manifold. We propose a Riemannian Multiplicative Update (RMU), and showcase the effectiveness of the chordal-NMF on synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12823v4</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Flavia Esposito, Andersen Ang</dc:creator>
    </item>
    <item>
      <title>Bilevel optimization with sustainability perspective: a survey on applications</title>
      <link>https://arxiv.org/abs/2406.07184</link>
      <description>arXiv:2406.07184v3 Announce Type: replace 
Abstract: Bilevel optimization, a well-established field for modeling hierarchical decision-making problems, has recently intersected with sustainability studies and practices, resulting in a series of works focusing on bilevel optimization problems involving multiple decision makers with diverse economic, environmental, and social objectives. This survey offers a comprehensive overview of sustainable bilevel optimization applications. First, we introduce the main concepts related to the nature of bilevel optimization problems and present some typical mathematical formulations for bilevel pricing problems that cover many of the collected applications. Then, we review the most relevant works published in sustainable bilevel optimization, giving a classification based on the application domains and their association with well-known operations research problems, while briefly discussing the proposed solution methodologies. We survey applications on transportation and logistics, production planning and manufacturing, water, waste, and agriculture management, supply chains, and disaster prevention and response. Finally, we outline a list of open questions and opportunities for future research in this domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07184v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ejor.2025.08.051</arxiv:DOI>
      <dc:creator>Giulia Caselli, Manuel Iori, Ivana Ljubi\'c</dc:creator>
    </item>
    <item>
      <title>Vibrational Control of Complex Networks</title>
      <link>https://arxiv.org/abs/2408.08263</link>
      <description>arXiv:2408.08263v2 Announce Type: replace 
Abstract: The stability of complex networks, from power grids to biological systems, is crucial for their proper functioning. It is thus important to control such systems to maintain or restore their stability. Traditional approaches rely on real-time state measurements for feedback control, but this can be challenging in many real-world systems, such as the brain, due to their complex and dynamic nature. This paper utilizes vibrational control -- an open-loop strategy -- to regulate network stability. Unlike conventional methods targeting network nodes, our approach focuses on manipulating network edges through vibrational inputs. We establish sufficient graph-theoretic conditions for vibration-induced functional modifications of network edges and stabilization of network systems as a whole. Additionally, we provide methods for designing effective vibrational control inputs and validate our theoretical findings through numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08263v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuzhen Qin, Fabio Pasqualetti, Danielle S. Bassett, Marcel van Gerven</dc:creator>
    </item>
    <item>
      <title>An inexact golden ratio primal-dual algorithm with linesearch step for a saddle point problem</title>
      <link>https://arxiv.org/abs/2408.08519</link>
      <description>arXiv:2408.08519v2 Announce Type: replace 
Abstract: In this paper, we propose an inexact golden ratio primal-dual algorithm with linesearch step(IP-GRPDAL) for solving the saddle point problems, where two subproblems can be approximately solved by applying the notations of inexact extended proximal operators with matrix norm. Our proposed IP-GRPDAL method allows for larger stepsizes by replacing the extrapolation step with a convex combination step. Each iteration of the linesearch requires to update only the dual variable, and hence it is quite cheap. In addition, we prove convergence of the proposed algorithm and show an O(1/N) ergodic convergence rate for our algorithm, where N represents the number of iterations. When one of the component functions is strongly convex, the accelerated O(1/N2) convergence rate results are established by choosing adaptively some algorithmic parameters. Furthermore, when both component functions are strongy convex, the linear convergence rate results are achieved. Numerical simulation results on the sparse recovery and image deblurring problems illustrate the feasibility and efficiency of our inexact algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08519v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changjie Fang, Jinxiu Liu, Jingtao Qiu, Shenglan Chen</dc:creator>
    </item>
    <item>
      <title>Advancing Strategic Planning and Dynamic Control of Complex Projects</title>
      <link>https://arxiv.org/abs/2408.12422</link>
      <description>arXiv:2408.12422v3 Announce Type: replace 
Abstract: Strategic project planning and dynamic control are essential to ensure that complex projects are both prepared and executed best-fit-for-common-purpose, guided by three interrelated strategies: (1) Agreeing First, (2) Acting Feasibly, and (3) Adapting Flexibly. When these strategies become too complex for humans to fully conceive and manage, effective computer-aided decision support becomes crucial. However, standard simulation-driven evaluation and a-posteriori decision-making are typically single-sided and technically focused focus, rather than applying a combined simulation-and-optimisation approach that a-priori integrates stakeholder interests and their mitigation behavior. Moreover, current planning and control methodologies often lack robust stochastic representations and associative multi objective optimisation methods that capture the full socio-technical complexity while maximizing the potential within reach. This paper introduces Odycon (Open Design and Dynamic Control), a new purpose-driven project management methodology that provides an actionable solution to these challenges. It presents a generic mathematical framework for project planning and control that integrates stakeholder preferences (human domain) with system performances (physical domain), enabling more effective planning and dynamic control. Odycon integrates standard Monte Carlo simulation (MCS) with the novel Integrative Maximisation of Aggregated Preferences (IMAP) optimisation method to develop a best-fit strategic plan and the most effective mitigation control strategies. Its value is demonstrated through applications in offshore wind installation and highway infrastructure projects, showcasing advances in associative design and decision-making, and aiming for a best-fit-for-common-purpose synthesis across different complex project phases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12422v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>L. G. Teuber, H. J. van Heukelum, A. R. M. Wolfert</dc:creator>
    </item>
    <item>
      <title>Measure-to-measure interpolation using Transformers</title>
      <link>https://arxiv.org/abs/2411.04551</link>
      <description>arXiv:2411.04551v2 Announce Type: replace 
Abstract: Transformers are deep neural network architectures that underpin the recent successes of large language models. Unlike more classical architectures that can be viewed as point-to-point maps, a Transformer acts as a measure-to-measure map implemented as specific interacting particle system on the unit sphere: the input is the empirical measure of tokens in a prompt and its evolution is governed by the continuity equation. In fact, Transformers are not limited to empirical measures and can in principle process any input measure. As the nature of data processed by Transformers is expanding rapidly, it is important to investigate their expressive power as maps from an arbitrary measure to another arbitrary measure. To that end, we provide an explicit choice of parameters that allows a single Transformer to match $N$ arbitrary input measures to $N$ arbitrary target measures, under the minimal assumption that every pair of input-target measures can be matched by some transport map.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04551v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Borjan Geshkovski, Philippe Rigollet, Dom\`enec Ruiz-Balet</dc:creator>
    </item>
    <item>
      <title>Two-timescale EXTRA for Distributed Smooth Non-convex Optimization</title>
      <link>https://arxiv.org/abs/2411.19483</link>
      <description>arXiv:2411.19483v3 Announce Type: replace 
Abstract: In this paper, we study distributed optimization with smooth non-convex local objectives. We propose a novel variant of the well-known EXact firsT-ordeR Algorithm (EXTRA), called Two-timescale EXTRA, by introducing two distinct step-sizes. Leveraging the two-timescale strategy, we construct a Lyapunov function and establish the sub-linear convergence of Two-timescale EXTRA to a consensual first-order stationary point. Additionally, we introduce an off-line sequential method for algorithm parameter selection, and the numerical results support the theoretical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19483v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zeyu Peng, Farhad Farokhi, Ye Pu</dc:creator>
    </item>
    <item>
      <title>Monge-Kantorovich's duality for separable Baire measures in completely regular Hausdorff spaces</title>
      <link>https://arxiv.org/abs/2503.03929</link>
      <description>arXiv:2503.03929v5 Announce Type: replace 
Abstract: We generalize the classical Monge-Kantorovich duality--typically established for tight (Radon) probability measures--to separable Baire probability measures, which are strictly more general than tight measures on completely regular Hausdorff spaces. Within this broader framework, we also demonstrate the existence of solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03929v5</guid>
      <category>math.OC</category>
      <category>math.FA</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammed Bachir</dc:creator>
    </item>
    <item>
      <title>Were Residual Penalty and Neural Operators All We Needed for Solving Optimal Control Problems?</title>
      <link>https://arxiv.org/abs/2506.04742</link>
      <description>arXiv:2506.04742v2 Announce Type: replace 
Abstract: Neural networks have been used to solve optimal control problems, typically by training neural networks using a combined loss function that considers data, differential equation residuals, and objective costs. We show that including cost functions in the training process is unnecessary, advocating for a simpler architecture and streamlined approach by decoupling the optimal control problem from the training process. Thus, our work shows that a simple neural operator architecture, such as DeepONet, coupled with an unconstrained optimization routine, can solve multiple optimal control problems with a single physics-informed training phase and a subsequent optimization phase. We achieve this by adding a penalty term based on the differential equation residual to the cost function and computing gradients with respect to the control using automatic differentiation through the trained neural operator within an iterative optimization routine. Our results show acceptable accuracy for practical applications and potential computational savings for more complex and higher-dimensional problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04742v2</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oliver G. S. Lundqvist, Fabricio Oliveira</dc:creator>
    </item>
    <item>
      <title>Acceleration via silver step-size on Riemannian manifolds with applications to Wasserstein space</title>
      <link>https://arxiv.org/abs/2506.06160</link>
      <description>arXiv:2506.06160v2 Announce Type: replace 
Abstract: There is extensive literature on accelerating first-order optimization methods in a Euclidean setting. Under which conditions such acceleration is feasible in Riemannian optimization problems is an active area of research. Motivated by the recent success of dynamic stepsize methods in the Euclidean setting, we undertake a study of such algorithms in the Riemannian setting. We provide the new class of algorithms determined by the choice of vector transport that allows the dynamic stepsize acceleration on Riemannian manifolds for the function classes associated with the corresponding vector transport. As a core application, we show our algorithm recovers the standard Wasserstein gradient descent on 2-Wasserstein space, and as a result provides the first provable accelerated gradient method in Wasserstein space. In addition, we validate the numerical strength of the algorithm for standard benchmark tasks on the space of symmetric positive definite matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06160v2</guid>
      <category>math.OC</category>
      <category>math.DG</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiyoung Park, Abhishek Roy, Jonathan W. Siegel, Anirban Bhattacharya</dc:creator>
    </item>
    <item>
      <title>The Generalized Matrix Separation Problem: Algorithms</title>
      <link>https://arxiv.org/abs/2507.17069</link>
      <description>arXiv:2507.17069v2 Announce Type: replace 
Abstract: When given a generalized matrix separation problem, which aims to recover a low rank matrix $L_0$ and a sparse matrix $S_0$ from $M_0=L_0+HS_0$, the work \cite{CW25} proposes a novel convex optimization problem whose objective function is the sum of the $\ell_1$-norm and nuclear norm. In this paper we detail the iterative algorithms and its associated computations for solving this convex optimization problem. We present various efficient implementation strategies, with attention to practical cases where $H$ is circulant, separable, or block structured. Notably, we propose a preconditioning technique that drastically improved the performance of our algorithms in terms of efficiency, accuracy, and robustness. While this paper serves as an illustrative algorithm implementation manual, we also provide theoretical guarantee for our preconditioning strategy. Numerical results demonstrate the effectiveness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17069v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuemei Chen, Owen Deen</dc:creator>
    </item>
    <item>
      <title>Convergence Analysis of the PAGE Stochastic Algorithm for Weakly Convex Finite-Sum Optimization</title>
      <link>https://arxiv.org/abs/2509.00737</link>
      <description>arXiv:2509.00737v2 Announce Type: replace 
Abstract: PAGE, a stochastic algorithm introduced by Li et al. [2021], was designed to find stationary points of averages of smooth nonconvex functions. In this work, we study PAGE in the broad framework of $\tau$-weakly convex functions, which provides a continuous interpolation between the general nonconvex $L$-smooth case ($\tau = L$) and the convex case ($\tau = 0$). We establish new convergence rates for PAGE, showing that its complexity improves as $\tau$ decreases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00737v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laurent Condat, Peter Richt\'arik</dc:creator>
    </item>
    <item>
      <title>The Aubin Property for Generalized Equations over $C^2$-cone Reducible Sets</title>
      <link>https://arxiv.org/abs/2509.14194</link>
      <description>arXiv:2509.14194v2 Announce Type: replace 
Abstract: This paper establishes the equivalence of the Aubin property and the strong regularity for generalized equations over $C^2$-cone reducible sets. This result resolves a long-standing question in variational analysis and extends the well-known equivalence theorem for polyhedral sets to a significantly broader class of non-polyhedral cases. Our proof strategy departs from traditional variational techniques, integrating insights from convex geometry with powerful tools from algebraic topology. A cornerstone of our analysis is a new fundamental lemma concerning the local structure of the normal cone map for arbitrary closed convex sets, which reveals how the dimension of normal cones varies in the neighborhood of a boundary point. This geometric insight is the key to applying degree theory, allowing us to prove that a crucial function associated with the problem has a topological index of $\pm1$. This, via a homological version of the inverse mapping theorem, implies that the function is a local homeomorphism, which in turn yields the strong regularity of the original solution map. This result unifies and extends several existing stability results for problems such as conventional nonlinear programming, nonlinear second-order cone programming, and nonlinear semidefinite programming under a single general framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14194v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaming Ma, Defeng Sun</dc:creator>
    </item>
    <item>
      <title>On extremal points for some vectorial total variation seminorms</title>
      <link>https://arxiv.org/abs/2404.12831</link>
      <description>arXiv:2404.12831v2 Announce Type: replace-cross 
Abstract: We consider the set of extremal points of the generalized unit ball induced by gradient total variation seminorms for vector-valued functions on bounded Euclidean domains. These are central to the understanding of sparse solutions and sparse optimization algorithms for variational problems posed among such functions. For cases in which either the domain or the target are one dimensional or the sum of the total variations of each component is used, we prove that these extremals consist of piecewise constant functions with two regions. For definitions involving more involved matrix norms and in particular spectral norms, we produce families of examples to show that the resulting set of extremal points is larger and includes piecewise constant functions with more than two regions. We also consider the total deformation induced by the symmetrized gradient, for which minimization with linear constraints appears in problems of determination of limit loads in a number of continuum mechanical models involving plasticity. For this case, we show piecewise infinitesimally rigid functions with two pieces to be extremal under mild assumptions. Finally, as an example which is not piecewise constant, we prove that unit radial vector fields are extremal for the Frobenius total variation in the plane.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12831v2</guid>
      <category>math.FA</category>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kristian Bredies, Jos\'e A. Iglesias, Daniel Walter</dc:creator>
    </item>
    <item>
      <title>Expressive Power of Graph Neural Networks for (Mixed-Integer) Quadratic Programs</title>
      <link>https://arxiv.org/abs/2406.05938</link>
      <description>arXiv:2406.05938v2 Announce Type: replace-cross 
Abstract: Quadratic programming (QP) is the most widely applied category of problems in nonlinear programming. Many applications require real-time/fast solutions, though not necessarily with high precision. Existing methods either involve matrix decomposition or use the preconditioned conjugate gradient method. For relatively large instances, these methods cannot achieve the real-time requirement unless there is an effective preconditioner. Recently, graph neural networks (GNNs) opened new possibilities for QP. Some promising empirical studies of applying GNNs for QP tasks show that GNNs can capture key characteristics of an optimization instance and provide adaptive guidance accordingly to crucial configurations during the solving process, or directly provide an approximate solution. However, the theoretical understanding of GNNs in this context remains limited. Specifically, it is unclear what GNNs can and cannot achieve for QP tasks in theory. This work addresses this gap in the context of linearly constrained QP tasks. In the continuous setting, we prove that message-passing GNNs can universally represent fundamental properties of convex quadratic programs, including feasibility, optimal objective values, and optimal solutions. In the more challenging mixed-integer setting, while GNNs are not universal approximators, we identify a subclass of QP problems that GNNs can reliably represent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05938v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziang Chen, Xiaohan Chen, Jialin Liu, Xinshang Wang, Wotao Yin</dc:creator>
    </item>
    <item>
      <title>On the vanishing of eigenfunctions of the Laplacian on tori</title>
      <link>https://arxiv.org/abs/2406.19925</link>
      <description>arXiv:2406.19925v3 Announce Type: replace-cross 
Abstract: Consider an eigenfunction of the Laplacian on a torus. How small can its $L^2$-norm be on small balls? We provide partial answers to this question by exploiting the distribution of integer points on spheres, basic properties of polynomials, and Nazarov--Tur\'an type estimates for exponential polynomials. Applications to quantum limits and control theory are given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19925v3</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre Germain, Iv\'an Moyano, Hui Zhu</dc:creator>
    </item>
    <item>
      <title>Alternating Subspace Method for Sparse Recovery of Signals</title>
      <link>https://arxiv.org/abs/2407.07436</link>
      <description>arXiv:2407.07436v2 Announce Type: replace-cross 
Abstract: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.
  Numerous renowned algorithms for tackling the compressed sensing problem employ an alternating strategy, which typically involves data matching in one module and denoising in another. Based on an in-depth analysis of the connection between the message passing and operator splitting, we present a novel approach, the Alternating Subspace Method (ASM), which integrates the principles of the greedy methods (e.g., the orthogonal matching pursuit type methods) and the splitting methods (e.g., the approximate message passing type methods). Essentially, ASM enhances the splitting method by achieving fidelity in a subspace-restricted fashion. We reveal that such confining strategy still yields a consistent fixed point iteration and establish its local geometric convergence on the LASSO problem. Numerical experiments on the LASSO, channel estimation, and dynamic compressed sensing problems demonstrate its high convergence rate and its capacity to incorporate different prior distributions. Overall, the proposed method is promising in efficiency, accuracy and flexibility, which has the potential to be competitive in different sparse recovery applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07436v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xu Zhu, Yufei Ma, Xiaoguang Li, Tiejun Li</dc:creator>
    </item>
    <item>
      <title>Neural Networks and (Virtual) Extended Formulations</title>
      <link>https://arxiv.org/abs/2411.03006</link>
      <description>arXiv:2411.03006v3 Announce Type: replace-cross 
Abstract: Neural networks with piecewise linear activation functions, such as rectified linear units (ReLU) or maxout, are among the most fundamental models in modern machine learning. We make a step towards proving lower bounds on the size of such neural networks by linking their representative capabilities to the notion of the extension complexity $\mathrm{xc}(P)$ of a polytope $P$. This is a well-studied quantity in combinatorial optimization and polyhedral geometry describing the number of inequalities needed to model $P$ as a linear program. We show that $\mathrm{xc}(P)$ is a lower bound on the size of any monotone or input-convex neural network that solves the linear optimization problem over $P$. This implies exponential lower bounds on such neural networks for a variety of problems, including the polynomially solvable maximum weight matching problem.
  In an attempt to prove similar bounds also for general neural networks, we introduce the notion of virtual extension complexity $\mathrm{vxc}(P)$, which generalizes $\mathrm{xc}(P)$ and describes the number of inequalities needed to represent the linear optimization problem over $P$ as a difference of two linear programs. We prove that $\mathrm{vxc}(P)$ is a lower bound on the size of any neural network that optimizes over $P$. While it remains an open question to derive useful lower bounds on $\mathrm{vxc}(P)$, we argue that this quantity deserves to be studied independently from neural networks by proving that one can efficiently optimize over a polytope $P$ using a small virtual extended formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03006v3</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Hertrich, Georg Loho</dc:creator>
    </item>
    <item>
      <title>Covariance-Based Device Activity Detection with Massive MIMO for Near-Field Correlated Channels</title>
      <link>https://arxiv.org/abs/2411.05492</link>
      <description>arXiv:2411.05492v2 Announce Type: replace-cross 
Abstract: This paper studies the device activity detection problem in a massive multiple-input multiple-output (MIMO) system for near-field communications (NFC). In this system, active devices transmit their signature sequences to the base station (BS), which detects the active devices based on the received signal. In this paper, we model the near-field channels as correlated Rician fading channels and formulate the device activity detection problem as a maximum likelihood estimation (MLE) problem. Compared to the traditional uncorrelated channel model, the correlation of channels complicates both algorithm design and theoretical analysis of the MLE problem. On the algorithmic side, we present the classical exact coordinate descent (CD) algorithm for solving the MLE problem, which suffers from numerical instability when applied to correlated channels. We propose a computationally efficient inexact CD algorithm by approximating the objective function, which approximately solves the one-dimensional subproblem and improves both computational efficiency and numerical stability. Additionally, we analyze the detection performance of the MLE problem under correlated channels by comparing it with the case of uncorrelated channels. The analysis shows that when the overall number of devices $N$ is large or the signature sequence length $L$ is small, the detection performance of MLE under correlated channels tends to be better than that under uncorrelated channels. Conversely, when $N$ is small or $L$ is large, MLE performs better under uncorrelated channels than under correlated ones. Finally, we study the MLE model in the joint device activity and data detection context. Simulation results demonstrate the computational performance of the presented algorithms and verify the correctness of the analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05492v2</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TWC.2025.3610347</arxiv:DOI>
      <dc:creator>Ziyue Wang, Yang Li, Ya-Feng Liu, Junjie Ma</dc:creator>
    </item>
    <item>
      <title>OptiChat: Bridging Optimization Models and Practitioners with Large Language Models</title>
      <link>https://arxiv.org/abs/2501.08406</link>
      <description>arXiv:2501.08406v2 Announce Type: replace-cross 
Abstract: Optimization models have been applied to solve a wide variety of decision-making problems. These models are usually developed by optimization experts but are used by practitioners without optimization expertise in various application domains. As a result, practitioners often struggle to interact with and draw useful conclusions from optimization models independently. To fill this gap, we introduce OptiChat, a natural language dialogue system designed to help practitioners interpret model formulation, diagnose infeasibility, analyze sensitivity, retrieve information, evaluate modifications, and provide counterfactual explanations. By augmenting large language models (LLMs) with functional calls and code generation tailored for optimization models, we enable seamless interaction and minimize the risk of hallucinations in OptiChat. We develop a new dataset to evaluate OptiChat's performance in explaining optimization models. Experiments demonstrate that OptiChat effectively bridges the gap between optimization models and practitioners, delivering autonomous, accurate, and instant responses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08406v2</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Chen, Gonzalo Esteban Constante-Flores, Krishna Sri Ipsit Mantri, Sai Madhukiran Kompalli, Akshdeep Singh Ahluwalia, Can Li</dc:creator>
    </item>
    <item>
      <title>Markov decision processes: on the convergence of the Monte-Carlo first visit algorithm</title>
      <link>https://arxiv.org/abs/2501.08800</link>
      <description>arXiv:2501.08800v2 Announce Type: replace-cross 
Abstract: We consider the Monte-Carlo first visit algorithm, of which the goal is to find the optimal control in a Markov decision process with finite state space and finite number of possible actions. We show its convergence when the discount factor is smaller than $1/2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08800v2</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sylvain Delattre, Nicolas Fournier</dc:creator>
    </item>
    <item>
      <title>Solving General QUBOs with Warm-Start QAOA via a Reduction to Max-Cut</title>
      <link>https://arxiv.org/abs/2504.06253</link>
      <description>arXiv:2504.06253v2 Announce Type: replace-cross 
Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is a quantum algorithm that finds approximate solutions to problems in combinatorial optimization, especially those that can be formulated as a Quadratic Unconstrained Binary Optimization (QUBO) problem. In prior work, researchers have considered various ways of "warm-starting" QAOA by constructing an initial quantum state using classically-obtained solutions or information; these warm-starts typically cause QAOA to yield better approximation ratios at much lower circuit depths. For the Max-Cut problem, one warm-start approaches constructs the initial state using the high-dimensional vectors that are output from an SDP relaxation of the corresponding Max-Cut problem. This work leverages these semidefinite warmstarts for a broader class of problem instances by using a standard reduction that transforms any QUBO instance into a Max-Cut instance. We empirically compare this approach to a "QUBO-relaxation" approach that relaxes the QUBO directly. Our results consider a variety of QUBO instances ranging from randomly generated QUBOs to QUBOs corresponding to specific problems such as the traveling salesman problem, maximum independent set, and portfolio optimization. We find that the best choice of warmstart approach is strongly dependent on the problem type.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06253v2</guid>
      <category>quant-ph</category>
      <category>cs.DM</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bikrant Bhattacharyya, Michael Capriotti, Reuben Tate</dc:creator>
    </item>
    <item>
      <title>Virtual Contraction Approach to Decentralized Adaptive Stabilization of Nonlinear Time-Delayed Networks</title>
      <link>https://arxiv.org/abs/2504.10855</link>
      <description>arXiv:2504.10855v2 Announce Type: replace-cross 
Abstract: In this paper, we exploit a diagonally dominant structure for the decentralized stabilization of unknown nonlinear time-delayed networks. To this end, we first introduce a novel generalization of virtual contraction analysis to diagonally dominant time-delayed control systems. We then show that nonlinear time-delayed networks can be stabilized using diagonal high-gains, provided that the input matrices satisfy certain generalized (column/row) diagonally dominant conditions. To enable stabilization of unknown networks, we further propose a distributed adaptive tuning rule for each individual gain function, guaranteeing that all closed-loop trajectories converge to the origin while the gains converge to finite values. The effectiveness of the proposed decentralized adaptive control is illustrated through a case study on epidemic spreading control in SIS networks with transmission delays.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10855v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yu Kawano, Zhiyong Sun</dc:creator>
    </item>
    <item>
      <title>Towards Quantifying the Hessian Structure of Neural Networks</title>
      <link>https://arxiv.org/abs/2505.02809</link>
      <description>arXiv:2505.02809v2 Announce Type: replace-cross 
Abstract: Empirical studies reported that the Hessian matrix of neural networks (NNs) exhibits a near-block-diagonal structure, yet its theoretical foundation remains unclear. In this work, we reveal that the reported Hessian structure comes from a mixture of two forces: a ``static force'' rooted in the architecture design, and a ''dynamic force'' arisen from training. We then provide a rigorous theoretical analysis of ''static force'' at random initialization. We study linear models and 1-hidden-layer networks for classification tasks with $C$ classes. By leveraging random matrix theory, we compare the limit distributions of the diagonal and off-diagonal Hessian blocks and find that the block-diagonal structure arises as $C$ becomes large. Our findings reveal that $C$ is one primary driver of the near-block-diagonal structure. These results may shed new light on the Hessian structure of large language models (LLMs), which typically operate with a large $C$ exceeding $10^4$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02809v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhaorui Dong, Yushun Zhang, Jianfeng Yao, Ruoyu Sun</dc:creator>
    </item>
    <item>
      <title>Wasserstein Convergence of Score-based Generative Models under Semiconvexity and Discontinuous Gradients</title>
      <link>https://arxiv.org/abs/2505.03432</link>
      <description>arXiv:2505.03432v2 Announce Type: replace-cross 
Abstract: Score-based Generative Models (SGMs) approximate a data distribution by perturbing it with Gaussian noise and subsequently denoising it via a learned reverse diffusion process. These models excel at modeling complex data distributions and generating diverse samples, achieving state-of-the-art performance across domains such as computer vision, audio generation, reinforcement learning, and computational biology. Despite their empirical success, existing Wasserstein-2 convergence analysis typically assume strong regularity conditions-such as smoothness or strict log-concavity of the data distribution-that are rarely satisfied in practice. In this work, we establish the first non-asymptotic Wasserstein-2 convergence guarantees for SGMs targeting semiconvex distributions with potentially discontinuous gradients. Our upper bounds are explicit and sharp in key parameters, achieving optimal dependence of $O(\sqrt{d})$ on the data dimension $d$ and convergence rate of order one. The framework accommodates a wide class of practically relevant distributions, including symmetric modified half-normal distributions, Gaussian mixtures, double-well potentials, and elastic net potentials. By leveraging semiconvexity without requiring smoothness assumptions on the potential such as differentiability, our results substantially broaden the theoretical foundations of SGMs, bridging the gap between empirical success and rigorous guarantees in non-smooth, complex data regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03432v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefano Bruno, Sotirios Sabanis</dc:creator>
    </item>
    <item>
      <title>On the $O(\frac{\sqrt{d}}{K^{1/4}})$ Convergence Rate of AdamW Measured by $\ell_1$ Norm</title>
      <link>https://arxiv.org/abs/2505.11840</link>
      <description>arXiv:2505.11840v2 Announce Type: replace-cross 
Abstract: As the default optimizer for training large language models, AdamW has achieved remarkable success in deep learning. However, its convergence behavior is not theoretically well-understood. This paper establishes the convergence rate $\frac{1}{K}\sum_{k=1}^KE\left[||\nabla f(x^k)||_1\right]\leq O(\frac{\sqrt{d}C}{K^{1/4}})$ for AdamW measured by $\ell_1$ norm, where $K$ represents the iteration number, $d$ denotes the model dimension, and $C$ matches the constant in the optimal convergence rate of SGD. Theoretically, we have $||\nabla f(x)||_2\ll ||\nabla f(x)||_1\leq \sqrt{d}||\nabla f(x)||_2$ for any high-dimensional vector $x$ and $E\left[||\nabla f(x)||_1\right]\geq\sqrt{\frac{2d}{\pi}}E\left[||\nabla f(x)||_2\right]$ when each element of $\nabla f(x)$ is generated from Gaussian distribution $\mathcal N(0,1)$. Empirically, our experimental results on real-world deep learning tasks reveal $||\nabla f(x)||_1=\varTheta(\sqrt{d})||\nabla f(x)||_2$. Both support that our convergence rate can be considered to be analogous to the optimal $\frac{1}{K}\sum_{k=1}^KE\left[||\nabla f(x^k)||_2\right]\leq O(\frac{C}{K^{1/4}})$ convergence rate of SGD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11840v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huan Li, Yiming Dong, Zhouchen Lin</dc:creator>
    </item>
    <item>
      <title>A note on pliability and the openness of the multiexponential map in Carnot groups</title>
      <link>https://arxiv.org/abs/2507.13049</link>
      <description>arXiv:2507.13049v2 Announce Type: replace-cross 
Abstract: In recent years, several notions of non-rigidity of horizontal vectors in Carnot groups have been proposed, motivated, in particular, by the characterization of monotone sets and Whitney extension properties. In this note we compare some of these notions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13049v2</guid>
      <category>math.MG</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fr\'ed\'eric Jean (OC, UMA), Mario Sigalotti (LJLL), Alessandro Socionovo (LJLL)</dc:creator>
    </item>
    <item>
      <title>Search-Optimized Quantization in Biomedical Ontology Alignment</title>
      <link>https://arxiv.org/abs/2507.13742</link>
      <description>arXiv:2507.13742v2 Announce Type: replace-cross 
Abstract: In the fast-moving world of AI, as organizations and researchers develop more advanced models, they face challenges due to their sheer size and computational demands. Deploying such models on edge devices or in resource-constrained environments adds further challenges related to energy consumption, memory usage and latency. To address these challenges, emerging trends are shaping the future of efficient model optimization techniques. From this premise, by employing supervised state-of-the-art transformer-based models, this research introduces a systematic method for ontology alignment, grounded in cosine-based semantic similarity between a biomedical layman vocabulary and the Unified Medical Language System (UMLS) Metathesaurus. It leverages Microsoft Olive to search for target optimizations among different Execution Providers (EPs) using the ONNX Runtime backend, followed by an assembled process of dynamic quantization employing Intel Neural Compressor and IPEX (Intel Extension for PyTorch). Through our optimization process, we conduct extensive assessments on the two tasks from the DEFT 2020 Evaluation Campaign, achieving a new state-of-the-art in both. We retain performance metrics intact, while attaining an average inference speed-up of 20x and reducing memory usage by approximately 70%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13742v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3389/frai.2025.1662984</arxiv:DOI>
      <arxiv:journal_reference>Front. Artif. Intell. 8:1662984 (2025)</arxiv:journal_reference>
      <dc:creator>Oussama Bouaggad, Natalia Grabar</dc:creator>
    </item>
    <item>
      <title>Does the draw matter in an incomplete round-robin tournament? The case of the UEFA Champions League</title>
      <link>https://arxiv.org/abs/2507.15320</link>
      <description>arXiv:2507.15320v2 Announce Type: replace-cross 
Abstract: A fundamental reform has been introduced in the 2024/25 season of club competitions organised by the Union of European Football Associations (UEFA): the well-established group stage has been replaced by an incomplete round-robin format. In this format, the 36 teams are ranked in a single league table, but play against only a subset of the competitors. While this innovative change has highlighted that the incomplete round-robin tournament is a reasonable alternative to the standard design of allocating the teams into round-robin groups, the characteristics of the new format remain unexplored. Our paper contributes to this topic by using simulations to compare the uncertainty generated by the draw in the old format with that in the new format of the UEFA Champions League. We develop a method to break down the impact of the 2024/25 reform into various components for each team. The new format is found to decrease the overall effect of the draw. However, this reduction can mainly be attributed to the inaccurate seeding system used by UEFA. If the teams are seeded based on their actual strengths, the impact of the draw is about the same in a tournament with an incomplete round-robin league or a group stage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15320v2</guid>
      <category>stat.AP</category>
      <category>math.OC</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o, Andr\'as Gyimesi, Dries Goossens, Karel Devriesere, Roel Lambers, Frits Spieksma</dc:creator>
    </item>
    <item>
      <title>Max-Min and 1-Bounded Space Algorithms for the Bin Packing Problem</title>
      <link>https://arxiv.org/abs/2508.18718</link>
      <description>arXiv:2508.18718v3 Announce Type: replace-cross 
Abstract: In the (1-dimensional) bin packing problem, we are asked to pack all the given items into bins, each of capacity one, so that the number of non-empty bins is minimized. Zhu~[Chaos, Solitons \&amp; Fractals 2016] proposed an approximation algorithm $MM$ that sorts the item sequence in a non-increasing order by size at the beginning, and then repeatedly packs, into the current single open bin, first as many of the largest items in the remaining sequence as possible and then as many of the smallest items in the remaining sequence as possible. In this paper we prove that the asymptotic approximation ratio of $MM$ is at most 1.5. Next, focusing on the fact that $MM$ is at the intersection of two algorithm classes, max-min algorithms and 1-bounded space algorithms, we comprehensively analyze the theoretical performance bounds of each subclass derived from the two classes. Our results include a lower bound of 1.25 for the intersection of the two classes. Furthermore, we extend the theoretical analysis over algorithm classes to the cardinality constrained bin packing problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18718v3</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiroshi Fujiwara, Rina Atsumi, Hiroaki Yamamoto</dc:creator>
    </item>
    <item>
      <title>Optimizing Paths for Adaptive Fly-Scan Microscopy: An Extended Version</title>
      <link>https://arxiv.org/abs/2509.01869</link>
      <description>arXiv:2509.01869v2 Announce Type: replace-cross 
Abstract: In x-ray microscopy, traditional raster-scanning techniques are used to acquire a microscopic image in a series of step-scans. Alternatively, scanning the x-ray probe along a continuous path, called a fly-scan, reduces scan time and increases scan efficiency. However, not all regions of an image are equally important. Currently used fly-scan methods do not adapt to the characteristics of the sample during the scan, often wasting time in uniform, uninteresting regions. One approach to avoid unnecessary scanning in uniform regions for raster step-scans is to use deep learning techniques to select a shorter optimal scan path instead of a traditional raster scan path, followed by reconstructing the entire image from the partially scanned data. However, this approach heavily depends on the quality of the initial sampling, requires a large dataset for training, and incurs high computational costs. We propose leveraging the fly-scan method along an optimal scanning path, focusing on regions of interest (ROIs) and using image completion techniques to reconstruct details in non-scanned areas. This approach further shortens the scanning process and potentially decreases x-ray exposure dose while maintaining high-quality and detailed information in critical regions. To achieve this, we introduce a multi-iteration fly-scan framework that adapts to the scanned image. Specifically, in each iteration, we define two key functions: (1) a score function to generate initial anchor points and identify potential ROIs, and (2) an objective function to optimize the anchor points for convergence to an optimal set. Using these anchor points, we compute the shortest scanning path between optimized anchor points, perform the fly-scan, and subsequently apply image completion based on the acquired information in preparation for the next scan iteration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01869v2</guid>
      <category>eess.IV</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Lu, Thomas F. Lynn, Ming Du, Zichao Di, Sven Leyffer</dc:creator>
    </item>
  </channel>
</rss>
