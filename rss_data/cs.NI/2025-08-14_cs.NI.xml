<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Aug 2025 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Agentic TinyML for Intent-aware Handover in 6G Wireless Networks</title>
      <link>https://arxiv.org/abs/2508.09147</link>
      <description>arXiv:2508.09147v1 Announce Type: new 
Abstract: As 6G networks evolve into increasingly AI-driven, user-centric ecosystems, traditional reactive handover mechanisms demonstrate limitations, especially in mobile edge computing and autonomous agent-based service scenarios. This manuscript introduces WAAN, a cross-layer framework that enables intent-aware and proactive handovers by embedding lightweight TinyML agents as autonomous, negotiation-capable entities across heterogeneous edge nodes that contribute to intent propagation and network adaptation. To ensure continuity across mobility-induced disruptions, WAAN incorporates semi-stable rendezvous points that serve as coordination anchors for context transfer and state preservation. The framework's operational capabilities are demonstrated through a multimodal environmental control case study, highlighting its effectiveness in maintaining user experience under mobility. Finally, the article discusses key challenges and future opportunities associated with the deployment and evolution of WAAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09147v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alaa Saleh, Roberto Morabito, Sasu Tarkoma, Anders Lindgren, Susanna Pirttikangas, Lauri Lov\'en</dc:creator>
    </item>
    <item>
      <title>Semantic-Aware LLM Orchestration for Proactive Resource Management in Predictive Digital Twin Vehicular Networks</title>
      <link>https://arxiv.org/abs/2508.09149</link>
      <description>arXiv:2508.09149v1 Announce Type: new 
Abstract: Next-generation automotive applications require vehicular edge computing (VEC), but current management systems are essentially fixed and reactive. They are suboptimal in extremely dynamic vehicular environments because they are constrained to static optimization objectives and base their decisions on the current network states. This paper presents a novel Semantic-Aware Proactive LLM Orchestration (SP-LLM) framework to address these issues. Our method transforms the traditional Digital Twin (DT) into a Predictive Digital Twin (pDT) that predicts important network parameters such as task arrivals, vehicle mobility, and channel quality. A Large Language Model (LLM) that serves as a cognitive orchestrator is at the heart of our framework. It makes proactive, forward-looking decisions about task offloading and resource allocation by utilizing the pDT's forecasts. The LLM's ability to decipher high-level semantic commands given in natural language is crucial because it enables it to dynamically modify its optimization policy to match evolving strategic objectives, like giving emergency services priority or optimizing energy efficiency. We show through extensive simulations that SP-LLM performs significantly better in terms of scalability, robustness in volatile conditions, and adaptability than state-of-the-art reactive and MARL-based approaches. More intelligent, autonomous, and goal-driven vehicular networks will be possible due to our framework's outstanding capacity to convert human intent into optimal network behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09149v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seyed Hossein Ahmadpanah</dc:creator>
    </item>
    <item>
      <title>Enabling On-demand Guaranteed QoS for Real Time Video Streaming from Vehicles in 5G Advanced with CAPIF &amp; NEF APIs</title>
      <link>https://arxiv.org/abs/2508.09150</link>
      <description>arXiv:2508.09150v1 Announce Type: new 
Abstract: This paper presents the design and implementation of a Proof of Concept (PoC) that demonstrates how 5G Advanced Network Functions can be integrated with the Common API Framework (CAPIF) to support enhanced connectivity for automotive applications. The PoC shows the continuous monitoring of the mobile network performance and the on-demand and dynamic adaptation of Quality of Service (QoS) for selected 5G User Equipment (UE) video streaming traffic flows using standard 3GPP Network Exposure Function (NEF) APIs exposed via CAPIF. Moreover, traffic flows are redirected to the edge to improve latency and optimize network resource utilization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09150v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pietro Piscione, Leonardo Lossi, Maziar Nekovee, Chathura Galkandage, Phil O Connor, Simon Davies</dc:creator>
    </item>
    <item>
      <title>Physiological Signal-Driven QoE Optimization for Wireless Virtual Reality Transmission</title>
      <link>https://arxiv.org/abs/2508.09151</link>
      <description>arXiv:2508.09151v1 Announce Type: new 
Abstract: Abrupt resolution changes in virtual reality (VR) streaming can significantly impair the quality-of-experience (QoE) of users, particularly during transitions from high to low resolutions. Existing QoE models and transmission schemes inadequately address the perceptual impact of these shifts. To bridge this gap, this article proposes, for the first time, an innovative physiological signal-driven QoE modeling and optimization framework that fully leverages users' electroencephalogram (EEG), electrocardiogram (ECG), and skin activity signals. This framework precisely captures the temporal dynamics of physiological responses and resolution changes in VR streaming, enabling accurate quantification of resolution upgrades' benefits and downgrades' impacts. Integrated the proposed QoE framework into the radio access network (RAN) via a deep reinforcement learning (DRL) framework, adaptive transmission strategies have been implemented to allocate radio resources dynamically, which mitigates short-term channel fluctuations and adjusts frame resolution in response to channel variations caused by user mobility. By prioritizing long-term resolution while minimizing abrupt transitions, the proposed solution achieves an 88.7\% improvement in resolution and an 81.0\% reduction in handover over the baseline. Experimental results demonstrate the effectiveness of this physiological signal-driven strategy, underscoring the promise of edge AI in immersive media services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09151v1</guid>
      <category>cs.NI</category>
      <category>cs.MA</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chang Wu, Yuang Chen, Yiyuan Chen, Fengqian Guo, Xiaowei Qin, Hancheng Lu</dc:creator>
    </item>
    <item>
      <title>5G Core Fault Detection and Root Cause Analysis using Machine Learning and Generative AI</title>
      <link>https://arxiv.org/abs/2508.09152</link>
      <description>arXiv:2508.09152v1 Announce Type: new 
Abstract: With the advent of 5G networks and technologies, ensuring the integrity and performance of packet core traffic is paramount. During network analysis, test files such as Packet Capture (PCAP) files and log files will contain errors if present in the system that must be resolved for better overall network performance, such as connectivity strength and handover quality. Current methods require numerous person-hours to sort out testing results and find the faults. This paper presents a novel AI/ML-driven Fault Analysis (FA) Engine designed to classify successful and faulty frames in PCAP files, specifically within the 5G packet core. The FA engine analyses network traffic using natural language processing techniques to identify anomalies and inefficiencies, significantly reducing the effort time required and increasing efficiency. The FA Engine also suggests steps to fix the issue using Generative AI via a Large Language Model (LLM) trained on several 5G packet core documents. The engine explains the details of the error from the domain perspective using documents such as the 3GPP standards and user documents regarding the internal conditions of the tests. Test results on the ML models show high classification accuracy on the test dataset when trained with 80-20 splits for the successful and failed PCAP files. Future scopes include extending the AI engine to incorporate 4G network traffic and other forms of network data, such as log text files and multimodal systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09152v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph H. R. Isaac, Harish Saradagam, Nallamothu Pardhasaradhi</dc:creator>
    </item>
    <item>
      <title>Agoran: An Agentic Open Marketplace for 6G RAN Automation</title>
      <link>https://arxiv.org/abs/2508.09159</link>
      <description>arXiv:2508.09159v1 Announce Type: new 
Abstract: Next-generation mobile networks must reconcile the often-conflicting goals of multiple service owners. However, today's network slice controllers remain rigid, policy-bound, and unaware of the business context. We introduce Agoran Service and Resource Broker (SRB), an agentic marketplace that brings stakeholders directly into the operational loop. Inspired by the ancient Greek agora, Agoran distributes authority across three autonomous AI branches: a Legislative branch that answers compliance queries using retrieval-augmented Large Language Models (LLMs); an Executive branch that maintains real-time situational awareness through a watcher-updated vector database; and a Judicial branch that evaluates each agent message with a rule-based Trust Score, while arbitrating LLMs detect malicious behavior and apply real-time incentives to restore trust. Stakeholder-side Negotiation Agents and the SRB-side Mediator Agent negotiate feasible, Pareto-optimal offers produced by a multi-objective optimizer, reaching a consensus intent in a single round, which is then deployed to Open and AI RAN controllers. Deployed on a private 5G testbed and evaluated with realistic traces of vehicle mobility, Agoran achieved significant gains: (i) a 37% increase in throughput of eMBB slices, (ii) a 73% reduction in latency of URLLC slices, and concurrently (iii) an end-to-end 8.3% saving in PRB usage compared to a static baseline. An 1B-parameter Llama model, fine-tuned for five minutes on 100 GPT-4 dialogues, recovers approximately 80% of GPT-4.1's decision quality, while operating within 6 GiB of memory and converging in only 1.3 seconds. These results establish Agoran as a concrete, standards-aligned path toward ultra-flexible, stakeholder-centric 6G networks. A live demo is presented https://www.youtube.com/watch?v=h7vEyMu2f5w\&amp;ab_channel=BubbleRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09159v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Chatzistefanidis, Navid Nikaein, Andrea Leone, Ali Maatouk, Leandros Tassioulas, Roberto Morabito, Ioannis Pitsiorlas, Marios Kountouris</dc:creator>
    </item>
    <item>
      <title>WPTrack: A Wi-Fi and Pressure Insole Fusion System for Single Target Tracking</title>
      <link>https://arxiv.org/abs/2508.09166</link>
      <description>arXiv:2508.09166v1 Announce Type: new 
Abstract: As the Internet of Things (IoT) continues to evolve, indoor location has become a critical element for enabling smart homes, behavioral monitoring, and elderly care. Existing WiFi-based human tracking solutions typically require specialized equipment or multiple Wi-Fi links, a limitation in most indoor settings where only a single pair of Wi-Fi devices is usually available. However, despite efforts to implement human tracking using one Wi-Fi link, significant challenges remain, such as difficulties in acquiring initial positions and blind spots in DFS estimation of tangent direction. To address these challenges, this paper proposes WPTrack, the first Wi-Fi and Pressure Insoles Fusion System for Single Target Tracking. WPTrack collects Channel State Information (CSI) from a single Wi-Fi link and pressure data from 90 insole sensors. The phase difference and Doppler velocity are computed from the CSI, while the pressure sensor data is used to calculate walking velocity. Then, we propose the CSI-pressure fusion model, integrating CSI and pressure data to accurately determine initial positions and facilitate precise human tracking. The simulation results show that the initial position localization accuracy ranges from 0.02 cm to 42.55 cm. The trajectory tracking results obtained from experimental data collected in a real-world environment closely align with the actual trajectory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09166v1</guid>
      <category>cs.NI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Guo, Shunsei Yamagishi, Lei Jing</dc:creator>
    </item>
    <item>
      <title>webMCP: Efficient AI-Native Client-Side Interaction for Agent-Ready Web Design</title>
      <link>https://arxiv.org/abs/2508.09171</link>
      <description>arXiv:2508.09171v1 Announce Type: new 
Abstract: Current AI agents create significant barriers for users by requiring extensive processing to understand web pages, making AI-assisted web interaction slow and expensive. This paper introduces webMCP (Web Machine Context &amp; Procedure), a client-side standard that embeds structured interaction metadata directly into web pages, enabling more efficient human-AI collaboration on existing websites. webMCP transforms how AI agents understand web interfaces by providing explicit mappings between page elements and user actions. Instead of processing entire HTML documents, agents can access pre-structured interaction data, dramatically reducing computational overhead while maintaining task accuracy. A comprehensive evaluation across 1,890 real API calls spanning online shopping, authentication, and content management scenarios demonstrates webMCP reduces processing requirements by 67.6% while maintaining 97.9% task success rates compared to 98.8% for traditional approaches. Users experience significantly lower costs (34-63% reduction) and faster response times across diverse web interactions. Statistical analysis confirms these improvements are highly significant across multiple AI models. An independent WordPress deployment study validates practical applicability, showing consistent improvements across real-world content management workflows. webMCP requires no server-side modifications, making it deployable across millions of existing websites without technical barriers. These results establish webMCP as a viable solution for making AI web assistance more accessible and sustainable, addressing the critical gap between user interaction needs and AI computational requirements in production environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09171v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>D. Perera</dc:creator>
    </item>
    <item>
      <title>Camel: Energy-Aware LLM Inference on Resource-Constrained Devices</title>
      <link>https://arxiv.org/abs/2508.09173</link>
      <description>arXiv:2508.09173v1 Announce Type: new 
Abstract: Most Large Language Models (LLMs) are currently deployed in the cloud, with users relying on internet connectivity for access. However, this paradigm faces challenges such as network latency, privacy concerns, and bandwidth limits. Thus, deploying LLMs on edge devices has become an important research focus. In edge inference, request latency is critical as high latency can impair real-time tasks. At the same time, edge devices usually have limited battery capacity, making energy consumption another major concern. Balancing energy consumption and inference latency is essential. To address this, we propose an LLM inference energy management framework that optimizes GPU frequency and batch size to balance latency and energy consumption. By effectively managing the exploration-exploitation dilemma in configuration search, the framework finds the optimal settings. The framework was implemented on the NVIDIA Jetson AGX Orin platform, and a series of experimental validations were conducted. Results demonstrate that, compared to the default configuration, our framework reduces energy delay product (EDP) by 12.4%-29.9%, achieving a better balance between energy consumption and latency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09173v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Xu, Long Peng, Shezheng Song, Xiaodong Liu, Ma Jun, Shasha Li, Jie Yu, Xiaoguang Mao</dc:creator>
    </item>
    <item>
      <title>HiSTM: Hierarchical Spatiotemporal Mamba for Cellular Traffic Forecasting</title>
      <link>https://arxiv.org/abs/2508.09184</link>
      <description>arXiv:2508.09184v1 Announce Type: new 
Abstract: Cellular traffic forecasting is essential for network planning, resource allocation, or load-balancing traffic across cells. However, accurate forecasting is difficult due to intricate spatial and temporal patterns that exist due to the mobility of users. Existing AI-based traffic forecasting models often trade-off accuracy and computational efficiency. We present Hierarchical SpatioTemporal Mamba (HiSTM), which combines a dual spatial encoder with a Mamba-based temporal module and attention mechanism. HiSTM employs selective state space methods to capture spatial and temporal patterns in network traffic. In our evaluation, we use a real-world dataset to compare HiSTM against several baselines, showing a 29.4% MAE improvement over the STN baseline while using 94% fewer parameters. We show that the HiSTM generalizes well across different datasets and improves in accuracy over longer time-horizons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09184v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zineddine Bettouche, Khalid Ali, Andreas Fischer, Andreas Kassler</dc:creator>
    </item>
    <item>
      <title>MX-AI: Agentic Observability and Control Platform for Open and AI-RAN</title>
      <link>https://arxiv.org/abs/2508.09197</link>
      <description>arXiv:2508.09197v1 Announce Type: new 
Abstract: Future 6G radio access networks (RANs) will be artificial intelligence (AI)-native: observed, reasoned about, and re-configured by autonomous agents cooperating across the cloud-edge continuum. We introduce MX-AI, the first end-to-end agentic system that (i) instruments a live 5G Open RAN testbed based on OpenAirInterface (OAI) and FlexRIC, (ii) deploys a graph of Large-Language-Model (LLM)-powered agents inside the Service Management and Orchestration (SMO) layer, and (iii) exposes both observability and control functions for 6G RAN resources through natural-language intents. On 50 realistic operational queries, MX-AI attains a mean answer quality of 4.1/5.0 and 100 % decision-action accuracy, while incurring only 8.8 seconds end-to-end latency when backed by GPT-4.1. Thus, it matches human-expert performance, validating its practicality in real settings. We publicly release the agent graph, prompts, and evaluation harness to accelerate open research on AI-native RANs. A live demo is presented here: https://www.youtube.com/watch?v=CEIya7988Ug&amp;t=285s&amp;ab_channel=BubbleRAN</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09197v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Chatzistefanidis, Andrea Leone, Ali Yaghoubian, Mikel Irazabal, Sehad Nassim, Lina Bariah, Merouane Debbah, Navid Nikaein</dc:creator>
    </item>
    <item>
      <title>CoMoE: Collaborative Optimization of Expert Aggregation and Offloading for MoE-based LLMs at Edge</title>
      <link>https://arxiv.org/abs/2508.09208</link>
      <description>arXiv:2508.09208v1 Announce Type: new 
Abstract: The proliferation of large language models (LLMs) has driven the adoption of Mixture-of-Experts (MoE) architectures as a promising solution to scale model capacity while controlling computational costs. However, deploying MoE models in resource-constrained mobile edge computing environments presents significant challenges due to their large memory footprint and dynamic expert activation patterns. To address these challenges, we propose a novel dynamic resource-aware collaborative optimization framework that jointly optimizes expert aggregation granularity and offloading strategies based on real-time device resource states, network conditions, and input characteristics in mobile edge environments, denoted as CoMoE. In CoMoE, we first systematically analyze existing expert aggregation techniques, including expert parameter merging,knowledge distillation,and parameter sharing decomposition, identifying their limitations in dynamic mobile environments.We then investigate expert offloading strategies encompassing expert prediction and prefetching, expert caching and scheduling, and multi-tier storage architectures, revealing the interdependencies between routing decisions and offloading performance.The CoMoE incorporates adaptive scheduling mechanisms that respond to user mobility and varying network conditions, enabling efficient MoE deployment across heterogeneous edge devices. Extensive experiments on real mobile edge testbeds demonstrate that CoMoE achieves approximately 70% reduction in memory usage compared to baseline methods, 10.5% lower inference latency than existing expert offloading techniques, while maintaining model performance stability. For large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE reduces memory requirements from 15.6GB to 4.7GB, enabling deployment on resource-constrained mobile edge devices that previously could only support much smaller models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09208v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muqing Li, Ning Li, Xin Yuan, Wenchao Xu, Quan Chen, Song Guo, Haijun Zhang</dc:creator>
    </item>
    <item>
      <title>Cluster Topology-Driven Placement of Experts Reduces Network Traffic in MoE Inference</title>
      <link>https://arxiv.org/abs/2508.09229</link>
      <description>arXiv:2508.09229v1 Announce Type: new 
Abstract: Efficient deployment of a pre-trained LLM to a cluster with multiple servers is a critical step for providing fast responses to users' queries. The recent success of Mixture-of-Experts (MoE) LLMs raises the question of how to deploy them efficiently, considering their underlying structure. During the inference in MoE LLMs, only a small part of the experts is selected to process a given token. Moreover, in practice, the experts' load is highly imbalanced. For efficient deployment, one has to distribute the model across a large number of servers using a model placement algorithm. Thus, to improve cluster utilization, the model placement algorithm has to take into account the network topology. This work focuses on the efficient topology-aware placement of the pre-trained MoE LLMs in the inference stage. We propose an integer linear program (ILP) that determines the optimal placement of experts, minimizing the expected number of transmissions. Due to the internal structure, this optimization problem can be solved with a standard ILP solver. We demonstrate that ILP-based placement strategy yields lower network traffic than competitors for small-scale (DeepSeekMoE~16B) and large-scale (DeepSeek-R1~671B) models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09229v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Danil Sivtsov, Aleksandr Katrutsa, Ivan Oseledets</dc:creator>
    </item>
    <item>
      <title>NEFMind: Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation</title>
      <link>https://arxiv.org/abs/2508.09240</link>
      <description>arXiv:2508.09240v1 Announce Type: new 
Abstract: The use of Service-Based Architecture in modern telecommunications has exponentially increased Network Functions (NFs) and Application Programming Interfaces (APIs), creating substantial operational complexities in service discovery and management. We introduce \textit{NEFMind}, a framework leveraging parameter-efficient fine-tuning of open-source Large Language Models (LLMs) to address these challenges. It integrates three core components: synthetic dataset generation from Network Exposure Function (NEF) API specifications, model optimization through Quantized-Low-Rank Adaptation, and performance evaluation via GPT-4 Ref Score and BertScore metrics. Targeting 5G Service-Based Architecture APIs, our approach achieves 85% reduction in communication overhead compared to manual discovery methods. Experimental validation using the open-source Phi-2 model demonstrates exceptional API call identification performance at 98-100% accuracy. The fine-tuned Phi-2 model delivers performance comparable to significantly larger models like GPT-4 while maintaining computational efficiency for telecommunications infrastructure deployment. These findings validate domain-specific, parameter-efficient LLM strategies for managing complex API ecosystems in next-generation telecommunications networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09240v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zainab Khan, Ahmed Hussain, Mukesh Thakur, Arto Hellas, Panos Papadimitratos</dc:creator>
    </item>
    <item>
      <title>On-Device Multimodal Federated Learning for Efficient Jamming Detection</title>
      <link>https://arxiv.org/abs/2508.09369</link>
      <description>arXiv:2508.09369v1 Announce Type: new 
Abstract: Wireless networks face severe vulnerabilities from jamming attacks, which can significantly disrupt communication. Existing detection approaches are often unimodal, rely on centralized processing, and demand substantial computational resources, hindering scalability, efficiency, and deployment feasibility. To address these challenges, we introduce a multimodal Federated Learning (FL) framework for on-device jamming detection and classification that integrates spectrograms with cross-layer network Key Performance Indicators (KPIs) through a lightweight dual-encoder architecture equipped with a fusion module and a multimodal projection head. This design enables privacy-preserving training and inference by ensuring that only model parameters are exchanged, while raw data remains on the device. The framework is implemented and evaluated on a wireless experimental testbed using, to the best of our knowledge, the first over-the-air multimodal dataset with synchronized benign and three distinct jamming scenarios. Results show that our approach surpasses state-of-the-art unimodal baselines by up to 15% in detection accuracy, achieves convergence with 60% fewer communication rounds, and maintains low resource usage. Its benefits are most evident under heterogeneous data distributions across devices, where it exhibits strong robustness and reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09369v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis Panitsas, Iason Ofeidis, Leandros Tassiulas</dc:creator>
    </item>
    <item>
      <title>Metrics for Assessing Changes in Flow-based Networks</title>
      <link>https://arxiv.org/abs/2508.09573</link>
      <description>arXiv:2508.09573v1 Announce Type: new 
Abstract: This paper addresses the challenges of evaluating network performance in the presence of fluctuating traffic patterns, with a particular focus on the impact of peak data rates on network resources. We introduce a set of metrics to quantify network load and measure the impact of individual flows on the overall network state. By analyzing link and flow data through percentile values and sample distributions, and introducing the Utilization Score metric, the research provides insights into resource utilization under varying network conditions. Furthermore, we employ a modified Shapley value-based approach to measure the influence of individual flows on the network, offering a better understanding of their contribution to network performance. The paper reviews and compares 11 metrics across various network scenarios, evaluating their practical relevance for research and development. Our evaluation demonstrates that these metrics effectively capture changes in network state induced by specific flows, with three of them offering a broad range of valuable insights while remaining relatively easy to maintain. Moreover, the methodology described in this paper serves as a framework for future research, with the potential to expand and refine the set of metrics used to evaluate flow impact on network performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09573v1</guid>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Micha{\l} Rzepka, Piotr Cho{\l}da</dc:creator>
    </item>
    <item>
      <title>Energy-efficient PON-based Backhaul Connectivity for a VLC-enabled Indoor Fog Computing Environment</title>
      <link>https://arxiv.org/abs/2508.09582</link>
      <description>arXiv:2508.09582v1 Announce Type: new 
Abstract: In this paper, we consider the use of visible light communication (VLC) to provide connectivity to indoor fog computing resources and propose an energy-efficient passive optical network (PON)-based backhaul architecture to support the VLC system. We develop a mixed-integer linear programming (MILP) model to optimize the allocation of computing resources over the proposed architecture, aiming to minimize processing and networking power consumption. We evaluate the performance of the proposed architecture under varying workload demands and user distributions. Comparative analysis against a backhaul architecture that is based on the state-of-the-art spine-and-leaf (S&amp;L) network design demonstrates total power savings of up to 82%. Further comparison with centralized cloud processing shows improvements in energy efficiency of up to 93%. Additionally, we examine the improvements in energy efficiency obtained by splitting tasks among multiple processing nodes and propose enhancements to the architecture including dynamic bandwidth allocation, increased wavelength bandwidth and improved connectivity within rooms to alleviate networking bottlenecks. Furthermore, we introduce an inter-building architecture that leverages resources from neighboring buildings to support high-demand scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09582v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wafaa B. M. Fadlelmula, Sanaa Hamid Mohamed, Taisir E. H. El-Gorashi, Jaafar M. H. Elmirghani</dc:creator>
    </item>
    <item>
      <title>Duty-Cycling is Not Enough in Constrained IoT Networking: Revealing the Energy Savings of Dynamic Clock Scaling</title>
      <link>https://arxiv.org/abs/2508.09620</link>
      <description>arXiv:2508.09620v1 Announce Type: new 
Abstract: Minimizing energy consumption of low-power wireless nodes is a persistent challenge from the constrained Internet of Things (IoT). In this paper, we start from the observation that constrained IoT devices have largely different hardware (im-)balances than full-scale machines. We find that the performance gap between MCU and network throughput on constrained devices enables minimal energy delay product (EDP) for IoT networking at largely reduced clock frequencies. We analyze the potentials by integrating dynamic voltage and frequency scaling (DVFS) into the RIOT IoT operating system and show that the DVFS reconfiguration overhead stays below the energy saved for a single, downscaled MAC operation. Backed by these findings, we systematically investigate how DVFS further improves energy-efficiency for common networking tasks -- in addition to duty-cycling. We measure IoT communication scenarios between real-world systems and analyze two MAC operating modes -- CSMA/CA and time slotting -- in combination with different CoAP transactions, payload sizes, as well as DTLS transport encryption. Our experiments reveal energy savings between 24% and 52% for MAC operations and up to 37% for encrypted CoAP communication. These results shall encourage research and system design work to integrate DVFS in future IoT devices for performing tasks at their optimal frequencies and thereby significantly extending battery lifetimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09620v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michel Rottleuthner, Thomas C. Schmidt, Matthias W\"ahlisch</dc:creator>
    </item>
    <item>
      <title>Anomaly Detection for IoT Global Connectivity</title>
      <link>https://arxiv.org/abs/2508.09660</link>
      <description>arXiv:2508.09660v1 Announce Type: new 
Abstract: Internet of Things (IoT) application providers rely on Mobile Network Operators (MNOs) and roaming infrastructures to deliver their services globally. In this complex ecosystem, where the end-to-end communication path traverses multiple entities, it has become increasingly challenging to guarantee communication availability and reliability. Further, most platform operators use a reactive approach to communication issues, responding to user complaints only after incidents have become severe, compromising service quality. This paper presents our experience in the design and deployment of ANCHOR -- an unsupervised anomaly detection solution for the IoT connectivity service of a large global roaming platform. ANCHOR assists engineers by filtering vast amounts of data to identify potential problematic clients (i.e., those with connectivity issues affecting several of their IoT devices), enabling proactive issue resolution before the service is critically impacted. We first describe the IoT service, infrastructure, and network visibility of the IoT connectivity provider we operate. Second, we describe the main challenges and operational requirements for designing an unsupervised anomaly detection solution on this platform. Following these guidelines, we propose different statistical rules, and machine- and deep-learning models for IoT verticals anomaly detection based on passive signaling traffic. We describe the steps we followed working with the operational teams on the design and evaluation of our solution on the operational platform, and report an evaluation on operational IoT customers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09660v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jesus Oma\~na Iglesias, Carlos Segura Perales, Stefan Gei{\ss}ler, Diego Perino, Andra Lutu</dc:creator>
    </item>
    <item>
      <title>Route Planning and Online Routing for Quantum Key Distribution Networks</title>
      <link>https://arxiv.org/abs/2508.09735</link>
      <description>arXiv:2508.09735v1 Announce Type: new 
Abstract: Quantum Key Distribution (QKD) networks harness the principles of quantum physics in order to securely transmit cryptographic key material, providing physical guarantees. These networks require traditional management and operational components, such as routing information through the network elements. However, due to the limitations on capacity and the particularities of information handling in these networks, traditional shortest paths algorithms for routing perform poorly on both route planning and online routing, which is counterintuitive. Moreover, due to the scarce resources in such networks, often the expressed demand cannot be met by any assignment of routes. To address both the route planning problem and the need for fair automated suggestions in infeasible cases, we propose to model this problem as a Quadratic Programming (QP) problem. For the online routing problem, we showcase that the shortest (available) paths routing strategy performs poorly in the online setting. Furthermore, we prove that the widest shortest path routing strategy has a competitive ratio greater or equal than $\frac{1}{2}$, efficiently addressing both routing modes in QKD networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09735v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jorge L\'opez, Charalampos Chatzinakis, Marc Cartigny</dc:creator>
    </item>
    <item>
      <title>The Paradigm of Massive Wireless Human Sensing: Concept, Architecture and Challenges</title>
      <link>https://arxiv.org/abs/2508.09756</link>
      <description>arXiv:2508.09756v1 Announce Type: new 
Abstract: This article is a position paper which introduces the paradigm of ``Massive Wireless Human Sensing'', i.e. an infrastructure for wireless human sensing based on a plethora of heterogeneous wireless communication signals. More specifically, we aim to exploit signal diversity in the time, frequency, and space domains using opportunistically both device-free and device-based wireless sensing approaches, with the objective of enhancing human sensing capabilities in terms of accuracy and service availability over different environments. The enabling element of this concept is the massive wireless human sensing edge device, that is, an embedded system acting as a multi-technology and multi-approach RF receiver with feature extraction functionality, located within the monitoring area or at its borders. In this framework, architecture solutions and challenges are discussed to lead the future development of this new paradigm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09756v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/IOTM.001.2400108</arxiv:DOI>
      <arxiv:journal_reference>EEE Internet of Things Magazine, vol. 8, no. 3, pp. 134-141, May 2025</arxiv:journal_reference>
      <dc:creator>Mauro De Sanctis</dc:creator>
    </item>
    <item>
      <title>An (m,k)-firm Elevation Policy to Increase the Robustness of Time-Driven Schedules in 5G Time-Sensitive Networks</title>
      <link>https://arxiv.org/abs/2508.09769</link>
      <description>arXiv:2508.09769v1 Announce Type: new 
Abstract: Current standardization efforts are advancing the integration of 5G and Time-Sensitive Networking (TSN) to facilitate the deployment of safety-critical industrial applications that require real-time communication. However, there remains a fundamental disconnect between the probabilistic 5G delay characteristics and the often idealistic delay models used to synthesize 5G-TSN network configurations. For time-driven schedules in particular, any delay outlier unforeseen during schedule synthesis can jeopardize the robustness of their real-time guarantees. To address this challenge, we present the (m,k)-firm Elevation Policy to uphold a base level of weakly hard real-time guarantees during unstable network conditions that do not match the expected delay characteristics. It augments the primary time-driven schedule with a dynamic priority-driven scheme to elevate the priority of m out of k consecutive frames if they are delayed. Our evaluations demonstrate that weakly hard real-time guarantees are essential to uphold the quality of control within a networked control system. At the same time, only a small overhead is imposed when the primary schedule can provide stronger quality of service guarantees. Our (m,k)-firm Elevation Policy thereby yields a robust but light-weight fallback mechanism to serve applications with meaningful guarantees during unstable network conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09769v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Egger, Robin Laidig, Heiko Geppert, Lucas Haug, Jona Herrmann, Frank D\"urr, Christian Becker</dc:creator>
    </item>
    <item>
      <title>A First Look at Starlink In-Flight Performance: An Intercontinental Empirical Study</title>
      <link>https://arxiv.org/abs/2508.09839</link>
      <description>arXiv:2508.09839v1 Announce Type: new 
Abstract: Starlink delivers Internet services to users across terrestrial, maritime, and aviation domains. The prior works have studied its performance at fixed sites and in-motion vehicles, while an in-depth analysis of in-flight performance remains absent. With major airlines now offering Starlink Internet onboard, there is a growing need to evaluate and improve its performance for aviation users. This paper addresses this shortcoming by conducting in-flight measurements over the Baltic Sea and the Pacific Ocean. Our measurement results show that a single user device experiences median throughputs of 64 Mbps and 24 Mbps for the downlink and uplink, respectively. The median uplink throughput is approximately 33 Mbps when the aircraft maintains an altitude above 17,000 feet. However, a significant reduction in uplink performance is observed during the aircraft descent phase, with the median throughput dropping to around 20 Mbps at lower altitudes. Round-trip time (RTT) is highly dependent on the location of the ground station being pinged and the use of inter-satellite links (ISLs). We dive deeper into 5.5 hours of ping measurements collected over the Pacific Ocean and investigate factors influencing RTT, hypothesizing that ISLs routing, data queuing at satellites, and feeder link congestion contribute to deviations from theoretical values. For comparative analysis, we evaluate the Starlink ground terminal and in-flight connectivity performance from the perspectives of a residential user and an airline passenger, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09839v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Asad Ullah, Luca Borgianni, Heikki Kokkinen, Antti Anttonen, Stefano Giordano</dc:creator>
    </item>
    <item>
      <title>RadioMamba: Breaking the Accuracy-Efficiency Trade-off in Radio Map Construction via a Hybrid Mamba-UNet</title>
      <link>https://arxiv.org/abs/2508.09140</link>
      <description>arXiv:2508.09140v1 Announce Type: cross 
Abstract: Radio map (RM) has recently attracted much attention since it can provide real-time and accurate spatial channel information for 6G services and applications. However, current deep learning-based methods for RM construction exhibit well known accuracy-efficiency trade-off. In this paper, we introduce RadioMamba, a hybrid Mamba-UNet architecture for RM construction to address the trade-off. Generally, accurate RM construction requires modeling long-range spatial dependencies, reflecting the global nature of wave propagation physics. RadioMamba utilizes a Mamba-Convolutional block where the Mamba branch captures these global dependencies with linear complexity, while a parallel convolutional branch extracts local features. This hybrid design generates feature representations that capture both global context and local detail. Experiments show that RadioMamba achieves higher accuracy than existing methods, including diffusion models, while operating nearly 20 times faster and using only 2.9\% of the model parameters. By improving both accuracy and efficiency, RadioMamba presents a viable approach for real-time intelligent optimization in next generation wireless systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09140v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Honggang Jia (Sherman), Nan Cheng (Sherman), Xiucheng Wang (Sherman), Conghao Zhou (Sherman), Ruijin Sun (Sherman),  Xuemin (Sherman),  Shen</dc:creator>
    </item>
    <item>
      <title>To Theoretically Understand Transformer-Based In-Context Learning for Optimizing CSMA</title>
      <link>https://arxiv.org/abs/2508.09146</link>
      <description>arXiv:2508.09146v1 Announce Type: cross 
Abstract: The binary exponential backoff scheme is widely used in WiFi 7 and still incurs poor throughput performance under dynamic channel environments. Recent model-based approaches (e.g., non-persistent and $p$-persistent CSMA) simply optimize backoff strategies under a known and fixed node density, still leading to a large throughput loss due to inaccurate node density estimation. This paper is the first to propose LLM transformer-based in-context learning (ICL) theory for optimizing channel access. We design a transformer-based ICL optimizer to pre-collect collision-threshold data examples and a query collision case. They are constructed as a prompt as the input for the transformer to learn the pattern, which then generates a predicted contention window threshold (CWT). To train the transformer for effective ICL, we develop an efficient algorithm and guarantee a near-optimal CWT prediction within limited training steps. As it may be hard to gather perfect data examples for ICL in practice, we further extend to allow erroneous data input in the prompt. We prove that our optimizer maintains minimal prediction and throughput deviations from the optimal values. Experimental results on NS-3 further demonstrate our approach's fast convergence and near-optimal throughput over existing model-based and DRL-based approaches under unknown node densities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09146v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shugang Hao, Hongbo Li, Lingjie Duan</dc:creator>
    </item>
    <item>
      <title>VeriPHY: Physical Layer Signal Authentication for Wireless Communication in 5G Environments</title>
      <link>https://arxiv.org/abs/2508.09213</link>
      <description>arXiv:2508.09213v1 Announce Type: cross 
Abstract: Physical layer authentication (PLA) uses inherent characteristics of the communication medium to provide secure and efficient authentication in wireless networks, bypassing the need for traditional cryptographic methods. With advancements in deep learning, PLA has become a widely adopted technique for its accuracy and reliability. In this paper, we introduce VeriPHY, a novel deep learning-based PLA solution for 5G networks, which enables unique device identification by embedding signatures within wireless I/Q transmissions using steganography. VeriPHY continuously generates pseudo-random signatures by sampling from Gaussian Mixture Models whose distribution is carefully varied to ensure signature uniqueness and stealthiness over time, and then embeds the newly generated signatures over I/Q samples transmitted by users to the 5G gNB. Utilizing deep neural networks, VeriPHY identifies and authenticates users based on these embedded signatures. VeriPHY achieves high precision, identifying unique signatures between 93% and 100% with low false positive rates and an inference time of 28 ms when signatures are updated every 20 ms. Additionally, we also demonstrate a stealth generation mode where signatures are generated in a way that makes them virtually indistinguishable from unaltered 5G signals while maintaining over 93% detection accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09213v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Clifton Paul Robinson, Salvatore D'Oro, Tommaso Melodia</dc:creator>
    </item>
    <item>
      <title>Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks</title>
      <link>https://arxiv.org/abs/2508.09532</link>
      <description>arXiv:2508.09532v1 Announce Type: cross 
Abstract: Federated fine-tuning has emerged as a promising approach for adapting foundation models (FMs) to diverse downstream tasks in edge environments. In Internet of Vehicles (IoV) systems, enabling efficient and low-latency multi-task adaptation is particularly challenging due to client mobility, heterogeneous resources, and intermittent connectivity. This paper proposes a hierarchical federated fine-tuning framework that coordinates roadside units (RSUs) and vehicles to support resource-aware and mobility-resilient learning across dynamic IoV scenarios. Leveraging Low-Rank Adaptation (LoRA), we introduce a decentralized, energy-aware rank adaptation mechanism formulated as a constrained multi-armed bandit problem. A novel UCB-DUAL algorithm is developed to enable adaptive exploration under per-task energy budgets, achieving provable sublinear regret. To evaluate our method, we construct a large-scale IoV simulator based on real-world trajectories, capturing dynamic participation, RSU handoffs, and communication variability. Extensive experiments show that our approach achieves the best accuracy-efficiency trade-off among all baselines, reducing latency by over 24\% and improving average accuracy by more than 2.5\%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09532v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bokeng Zheng, Jianqiang Zhong, Jiayi Liu, Xiaoxi Zhang</dc:creator>
    </item>
    <item>
      <title>Closing the HPC-Cloud Convergence Gap: Multi-Tenant Slingshot RDMA for Kubernetes</title>
      <link>https://arxiv.org/abs/2508.09663</link>
      <description>arXiv:2508.09663v1 Announce Type: cross 
Abstract: Converged HPC-Cloud computing is an emerging computing paradigm that aims to support increasingly complex and multi-tenant scientific workflows. These systems require reconciliation of the isolation requirements of native cloud workloads and the performance demands of HPC applications. In this context, networking hardware is a critical boundary component: it is the conduit for high-throughput, low-latency communication and enables isolation across tenants. HPE Slingshot is a high-speed network interconnect that provides up to 200 Gbps of throughput per port and targets high-performance computing (HPC) systems. The Slingshot host software, including hardware drivers and network middleware libraries, is designed to meet HPC deployments, which predominantly use single-tenant access modes. Hence, the Slingshot stack is not suited for secure use in multi-tenant deployments, such as converged HPC-Cloud deployments. In this paper, we design and implement an extension to the Slingshot stack targeting converged deployments on the basis of Kubernetes. Our integration provides secure, container-granular, and multi-tenant access to Slingshot RDMA networking capabilities at minimal overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09663v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp A. Friese, Ahmed Eleliemy, Utz-Uwe Haus, Martin Schulz</dc:creator>
    </item>
    <item>
      <title>3GPP NR V2X Mode 2d: Analysis of Distributed Scheduling for Groupcast using ns-3 5G LENA Simulator</title>
      <link>https://arxiv.org/abs/2508.09708</link>
      <description>arXiv:2508.09708v1 Announce Type: cross 
Abstract: Vehicle-to-everything (V2X) communication is a key technology for enabling intelligent transportation systems (ITS) that can improve road safety, traffic efficiency, and environmental sustainability. Among the various V2X applications, platooning is one of the most promising ones, as it allows a group of vehicles to travel closely together at high speeds, reducing fuel consumption and emissions. However, it poses significant challenges for wireless communication, such as high reliability and low latency. In this paper, we evaluate the benefits of group scheduling, also referred to as Mode 2d, which is based on a distributed and scheduled resource allocation scheme that allows the group of cars to select resources from a configured pool without network assistance. We evaluated the scheme through simulations, and the results show that this approach can meet the reliability, low latency, and data rate requirements for platooning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09708v1</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Fehrenbach, Luis Omar Ortiz Abrego, Cornelius Hellge, Thomas Schierl, J\"org Ott</dc:creator>
    </item>
    <item>
      <title>Hummingbird: Fast, Flexible, and Fair Inter-Domain Bandwidth Reservations</title>
      <link>https://arxiv.org/abs/2308.09959</link>
      <description>arXiv:2308.09959v4 Announce Type: replace 
Abstract: The current Internet lacks quality-of-service guarantees for real-time applications like video calls and gaming, cloud-based systems, financial transactions, telesurgery, and other remote applications that benefit from reliable communication. To address this problem, this paper introduces Hummingbird: a novel, lightweight bandwidth-reservation system that provides fine-grained inter-domain reservations for end hosts and introduces several improvements over previous designs.
  Hummingbird enables flexible and composable reservations with end-to-end guarantees, and addresses an often overlooked, but crucial, aspect of bandwidth reservation systems: incentivization of network providers. Hummingbird represents bandwidth reservations as tradeable assets which allows markets to emerge that ensure fair and efficient resource allocation and encourage deployment by remunerating providers. This incentivization is facilitated by decoupling reservations from network identities, which enables novel control-plane mechanisms and allows us to design a control plane based on smart contracts.
  Hummingbird also provides an efficient reservation data plane which streamlines the processing on routers and thus simplifies the implementation, deployment, and traffic policing while maintaining robust security properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.09959v4</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3718958.3750495</arxiv:DOI>
      <dc:creator>Karl W\"ust, Giacomo Giuliari, Markus Legner, Jean-Pierre Smith, Marc Wyss, Jules Bachmann, Juan A. Garcia-Pardo, Adrian Perrig</dc:creator>
    </item>
    <item>
      <title>Binary VPN Traffic Detection Using Wavelet Features and Machine Learning</title>
      <link>https://arxiv.org/abs/2502.13804</link>
      <description>arXiv:2502.13804v3 Announce Type: replace 
Abstract: Encrypted traffic classification faces growing challenges as encryption renders traditional deep packet inspection ineffective. This study addresses binary VPN detection, distinguishing VPN-encrypted from non-VPN traffic using wavelet transform-based features across multiple machine learning models. Unlike previous studies focused on application-level classification within encrypted traffic, we specifically evaluate the fundamental task of VPN identification regardless of application type. We analyze the impact of wavelet decomposition levels and dataset filtering on classification performance across significantly imbalanced data, where filtering reduces some traffic categories by up to 95%. Our results demonstrate that Random Forest (RF) achieves superior performance with an F1-score of 99%, maintaining robust accuracy even after significant dataset filtering. Neural Networks (NN) show comparable effectiveness with an F1-score of 98% when trained on wavelet level 12, while Support Vector Machines (SVM) exhibit notable sensitivity to dataset reduction, with F1-scores dropping from 90% to 85% after filtering. Comparing wavelet decomposition at levels 5 and 12, we observe improved classification performance at level 12, particularly for variable traffic types, though the marginal gains may not justify the additional computational overhead. These findings establish RF as the most reliable model for VPN traffic classification while highlighting key performance tradeoffs in feature extraction and preprocessing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13804v3</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yasameen Sajid Razooqi, Adrian Pekar</dc:creator>
    </item>
    <item>
      <title>Generative Active Adaptation for Drifting and Imbalanced Network Intrusion Detection</title>
      <link>https://arxiv.org/abs/2503.03022</link>
      <description>arXiv:2503.03022v2 Announce Type: replace 
Abstract: Machine learning has shown promise in network intrusion detection systems, yet its performance often degrades due to concept drift and imbalanced data. These challenges are compounded by the labor-intensive process of labeling network traffic, especially when dealing with evolving and rare attack types, which makes preparing the right data for adaptation difficult. To address these issues, we propose a generative active adaptation framework that minimizes labeling effort while enhancing model robustness. Our approach employs density-aware dataset prior selection to identify the most informative samples for annotation, and leverages deep generative models to conditionally synthesize diverse samples, thereby augmenting the training set and mitigating the effects of concept drift. We evaluate our end-to-end framework \NetGuard on both simulated IDS data and a real-world ISP dataset, demonstrating significant improvements in intrusion detection performance. Our method boosts the overall F1-score from 0.60 (without adaptation) to 0.86. Rare attacks such as Infiltration, Web Attack, and FTP-BruteForce, which originally achieved F1 scores of 0.001, 0.04, and 0.00, improve to 0.30, 0.50, and 0.71, respectively, with generative active adaptation in the CIC-IDS 2018 dataset. Our framework effectively enhances rare attack detection while reducing labeling costs, making it a scalable and practical solution for intrusion detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03022v2</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ragini Gupta, Shinan Liu, Ruixiao Zhang, Xinyue Hu, Xiaoyang Wang, Hadjer Benkraouda, Pranav Kommaraju, Nick Feamster, Klara Nahrstedt</dc:creator>
    </item>
    <item>
      <title>Confidence Driven Classification of Application Types in the Presence of Background Network</title>
      <link>https://arxiv.org/abs/2508.03891</link>
      <description>arXiv:2508.03891v3 Announce Type: replace 
Abstract: Accurately classifying the application types of network traffic using deep learning models has recently gained popularity. However, we find that these classifiers do not perform well on real-world traffic data due to the presence of non-application-specific generic background traffic originating from advertisements, analytics, shared APIs, and trackers. Unfortunately, state-of-the-art application classifiers overlook such traffic in curated datasets and only classify relevant application traffic. To address this issue, when we label and train using an additional class for background traffic, it leads to additional confusion between application and background traffic, as the latter is heterogeneous and encompasses all traffic that is not relevant to the application sessions. To avoid falsely classifying background traffic as one of the relevant application types, a reliable confidence measure is warranted, such that we can refrain from classifying uncertain samples. Therefore, we design a Gaussian Mixture Model-based classification framework that improves the indication of the deep learning classifier's confidence to allow more reliable classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03891v3</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eun Hun Choi, Jasleen Kaur, Vladas Pipiras, Nelson Gomes Rodrigues Antunes, Brendan Massey</dc:creator>
    </item>
    <item>
      <title>STAC: Leveraging Spatio-Temporal Data Associations For Efficient Cross-Camera Streaming and Analytics</title>
      <link>https://arxiv.org/abs/2401.15288</link>
      <description>arXiv:2401.15288v2 Announce Type: replace-cross 
Abstract: In IoT based distributed network of cameras, real-time multi-camera video analytics is challenged by high bandwidth demands and redundant visual data, creating a fundamental tension where reducing data saves network overhead but can degrade model performance, and vice versa. We present STAC, a cross-cameras surveillance system that leverages spatio-temporal associations for efficient object tracking under constrained network conditions. STAC integrates multi-resolution feature learning, ensuring robustness under variable networked system level optimizations such as frame filtering, FFmpeg-based compression, and Region-of-Interest (RoI) masking, to eliminate redundant content across distributed video streams while preserving downstream model accuracy for object identification and tracking. Evaluated on NVIDIA's AICity Challenge dataset, STAC achieves a 76\% improvement in tracking accuracy and an 8.6x reduction in inference latency over a standard multi-object multi-camera tracking baseline (using YOLOv4 and DeepSORT). Furthermore, 29\% of redundant frames are filtered, significantly reducing data volume without compromising inference quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15288v2</guid>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <category>cs.NI</category>
      <pubDate>Thu, 14 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ragini Gupta, Lingzhi Zhao, Jiaxi Li, Volodymyr Vakhniuk, Claudiu Danilov, Josh Eckhardt, Keyshla Bernard, Klara Nahrstedt</dc:creator>
    </item>
  </channel>
</rss>
