<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 Aug 2024 04:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 21 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Impossible temperatures are not as rare as you think</title>
      <link>https://arxiv.org/abs/2408.10251</link>
      <description>arXiv:2408.10251v1 Announce Type: new 
Abstract: The last decade has seen numerous record-shattering heatwaves in all corners of the globe. In the aftermath of these devastating events, there is interest in identifying worst-case thresholds or upper bounds that quantify just how hot temperatures can become. Generalized Extreme Value theory provides a data-driven estimate of extreme thresholds; however, upper bounds may be exceeded by future events, which undermines attribution and planning for heatwave impacts. Here, we show how the occurrence and relative probability of observed events that exceed a priori upper bound estimates, so-called "impossible" temperatures, has changed over time. We find that many unprecedented events are actually within data-driven upper bounds, but only when using modern spatial statistical methods. Furthermore, there are clear connections between anthropogenic forcing and the "impossibility" of the most extreme temperatures. Robust understanding of heatwave thresholds provides critical information about future record-breaking events and how their extremity relates to historical measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10251v1</guid>
      <category>physics.data-an</category>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mark D. Risser, Likun Zhang, Michael F. Wehner</dc:creator>
    </item>
    <item>
      <title>Diversity and stylization of the contemporary user-generated visual arts in the complexity-entropy plane</title>
      <link>https://arxiv.org/abs/2408.10356</link>
      <description>arXiv:2408.10356v1 Announce Type: cross 
Abstract: The advent of computational and numerical methods in recent times has provided new avenues for analyzing art historiographical narratives and tracing the evolution of art styles therein. Here, we investigate an evolutionary process underpinning the emergence and stylization of contemporary user-generated visual art styles using the complexity-entropy (C-H) plane, which quantifies local structures in paintings. Informatizing 149,780 images curated in DeviantArt and Behance platforms from 2010 to 2020, we analyze the relationship between local information of the C-H space and multi-level image features generated by a deep neural network and a feature extraction algorithm. The results reveal significant statistical relationships between the C-H information of visual artistic styles and the dissimilarities of the multi-level image features over time within groups of artworks. By disclosing a particular C-H region where the diversity of image representations is noticeably manifested, our analyses reveal an empirical condition of emerging styles that are both novel in the C-H plane and characterized by greater stylistic diversity. Our research shows that visual art analyses combined with physics-inspired methodologies and machine learning, can provide macroscopic insights into quantitatively mapping relevant characteristics of an evolutionary process underpinning the creative stylization of uncharted visual arts of given groups and time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10356v1</guid>
      <category>cs.CV</category>
      <category>physics.data-an</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seunghwan Kim, Byunghwee Lee, Wonjae Lee</dc:creator>
    </item>
    <item>
      <title>New horizon in the statistical physics of earthquakes: Dragon-king theory and dragon-king earthquakes</title>
      <link>https://arxiv.org/abs/2408.10857</link>
      <description>arXiv:2408.10857v1 Announce Type: cross 
Abstract: A systematic quantitative investigation into whether the mechanisms of large earthquakes are unique could significantly deepen our understanding of fault rupture and seismicity patterns. This research holds the potential to advance our ability to predict large earthquakes and enhance the effectiveness of disaster prevention and mitigation strategies. In 2009, one of us introduced the dragon-king theory, offering a quantitative framework for identifying and testing extreme outliers-referred to as dragon-king events-that are endogenously generated. This theory provides valuable tools for explaining, predicting, and managing the risks associated with these rare but highly impactful events. The present paper discusses the feasibility of applying this theory to seismology, proposing that dragon-king earthquake events can be identified as outliers to the Gutenberg-Richter law. It also examines several seismological mechanisms that may contribute to the occurrence of these extraordinary events. Although applying the dragon-king theory to seismology presents practical challenges, it offers the potential to significantly enrich statistical seismology. By reexamining the classification of earthquake rupture types through a statistical testing lens and integrating these insights with underlying physical mechanisms, this approach can greatly enhance the analytical tools and depth of research in the field of statistical seismology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10857v1</guid>
      <category>physics.geo-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Li, Didier Sornette, Zhongliang Wu, Hangwei Li</dc:creator>
    </item>
    <item>
      <title>mdendro: An R package for extended agglomerative hierarchical clustering</title>
      <link>https://arxiv.org/abs/2309.13333</link>
      <description>arXiv:2309.13333v3 Announce Type: replace-cross 
Abstract: "mdendro" is an R package that provides a comprehensive collection of linkage methods for agglomerative hierarchical clustering on a matrix of proximity data (distances or similarities), returning a multifurcated dendrogram or multidendrogram. Multidendrograms can group more than two clusters at the same time, solving the nonuniqueness problem that arises when there are ties in the data. This problem causes that different binary dendrograms are possible depending both on the order of the input data and on the criterion used to break ties. Weighted and unweighted versions of the most common linkage methods are included in the package, which also implements two parametric linkage methods. In addition, package "mdendro" provides five descriptive measures to analyze the resulting dendrograms: cophenetic correlation coefficient, space distortion ratio, agglomeration coefficient, chaining coefficient and tree balance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13333v3</guid>
      <category>cs.IR</category>
      <category>physics.data-an</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alberto Fern\'andez, Sergio G\'omez</dc:creator>
    </item>
    <item>
      <title>Synergy as the failure of distributivity</title>
      <link>https://arxiv.org/abs/2404.03455</link>
      <description>arXiv:2404.03455v3 Announce Type: replace-cross 
Abstract: The concept of emergence, or synergy in its simplest form, is widely used but lacks a rigorous definition. Our work connects information and set theory to uncover the mathematical nature of synergy as the failure of distributivity. It resolves the persistent self-contradiction of information decomposition theory and reinstates it as a primary route toward a rigorous definition of emergence. Our results suggest that non-distributive variants of set theory may be used to describe emergent physical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03455v3</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>physics.bio-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ivan A. Sevostianov, Ofer Feinerman</dc:creator>
    </item>
    <item>
      <title>Model orthogonalization and Bayesian forecast mixing via Principal Component Analysis</title>
      <link>https://arxiv.org/abs/2405.10839</link>
      <description>arXiv:2405.10839v2 Announce Type: replace-cross 
Abstract: One can improve predictability in the unknown domain by combining forecasts of imperfect complex computational models using a Bayesian statistical machine learning framework. In many cases, however, the models used in the mixing process are similar. In addition to contaminating the model space, the existence of such similar, or even redundant, models during the multimodeling process can result in misinterpretation of results and deterioration of predictive performance. In this work we describe a method based on the Principal Component Analysis that eliminates model redundancy. We show that by adding model orthogonalization to the proposed Bayesian Model Combination framework, one can arrive at better prediction accuracy and reach excellent uncertainty quantification performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10839v2</guid>
      <category>nucl-th</category>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pablo Giuliani, Kyle Godbey, Vojtech Kejzlar, Witold Nazarewicz</dc:creator>
    </item>
  </channel>
</rss>
