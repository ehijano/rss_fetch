<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Nov 2024 03:48:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Analyzing the AI Nudification Application Ecosystem</title>
      <link>https://arxiv.org/abs/2411.09751</link>
      <description>arXiv:2411.09751v1 Announce Type: new 
Abstract: Given a source image of a clothed person (an image subject), AI-based nudification applications can produce nude (undressed) images of that person. Moreover, not only do such applications exist, but there is ample evidence of the use of such applications in the real world and without the consent of an image subject. Still, despite the growing awareness of the existence of such applications and their potential to violate the rights of image subjects and cause downstream harms, there has been no systematic study of the nudification application ecosystem across multiple applications. We conduct such a study here, focusing on 20 popular and easy-to-find nudification websites. We study the positioning of these web applications (e.g., finding that most sites explicitly target the nudification of women, not all people), the features that they advertise (e.g., ranging from undressing-in-place to the rendering of image subjects in sexual positions, as well as differing user-privacy options), and their underlying monetization infrastructure (e.g., credit cards and cryptocurrencies). We believe this work will empower future, data-informed conversations -- within the scientific, technical, and policy communities -- on how to better protect individuals' rights and minimize harm in the face of modern (and future) AI-based nudification applications. Content warning: This paper includes descriptions of web applications that can be used to create synthetic non-consensual explicit AI-created imagery (SNEACI). This paper also includes an artistic rendering of a user interface for such an application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09751v1</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cassidy Gibson, Daniel Olszewski, Natalie Grace Brigham, Anna Crowder, Kevin R. B. Butler, Patrick Traynor, Elissa M. Redmiles, Tadayoshi Kohno</dc:creator>
    </item>
    <item>
      <title>AI-Driven Human-Autonomy Teaming in Tactical Operations: Proposed Framework, Challenges, and Future Directions</title>
      <link>https://arxiv.org/abs/2411.09788</link>
      <description>arXiv:2411.09788v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) techniques, particularly machine learning techniques, are rapidly transforming tactical operations by augmenting human decision-making capabilities. This paper explores AI-driven Human-Autonomy Teaming (HAT) as a transformative approach, focusing on how it empowers human decision-making in complex environments. While trust and explainability continue to pose significant challenges, our exploration focuses on the potential of AI-driven HAT to transform tactical operations. By improving situational awareness and supporting more informed decision-making, AI-driven HAT can enhance the effectiveness and safety of such operations. To this end, we propose a comprehensive framework that addresses the key components of AI-driven HAT, including trust and transparency, optimal function allocation between humans and AI, situational awareness, and ethical considerations. The proposed framework can serve as a foundation for future research and development in the field. By identifying and discussing critical research challenges and knowledge gaps in this framework, our work aims to guide the advancement of AI-driven HAT for optimizing tactical operations. We emphasize the importance of developing scalable and ethical AI-driven HAT systems that ensure seamless human-machine collaboration, prioritize ethical considerations, enhance model transparency through Explainable AI (XAI) techniques, and effectively manage the cognitive load of human operators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09788v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Desta Haileselassie Hagos, Hassan El Alami, Danda B. Rawat</dc:creator>
    </item>
    <item>
      <title>LLM-Powered AI Tutors with Personas for d/Deaf and Hard-of-Hearing Online Learners</title>
      <link>https://arxiv.org/abs/2411.09873</link>
      <description>arXiv:2411.09873v1 Announce Type: new 
Abstract: Intelligent tutoring systems (ITS) using artificial intelligence (AI) technology have shown promise in supporting learners with diverse abilities; however, they often fail to meet the specific communication needs and cultural nuances needed by d/Deaf and Hard-of-Hearing (DHH) learners. As large language models (LLMs) provide new opportunities to incorporate personas to AI-based tutors and support dynamic interactive dialogue, this paper explores how DHH learners perceive LLM-powered ITS with different personas and identified design suggestions for improving the interaction. We developed an interface that allows DHH learners to interact with ChatGPT and three LLM-powered AI tutors with different experiences in DHH education while the learners watch an educational video. A user study with 16 DHH participants showed that they perceived conversations with the AI tutors who had DHH education experiences to be more human-like and trustworthy due to the tutors' cultural knowledge of DHH communities. Participants also suggested providing more transparency regarding the tutors' background information to clarify each AI tutor's position within the DHH community. We discuss design implications for more inclusive LLM-based systems, such as supports for the multimodality of sign language.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09873v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haocong Cheng, Si Chen, Christopher Perdriau, Yun Huang</dc:creator>
    </item>
    <item>
      <title>EEG Spectral Analysis in Gray Zone Between Healthy and Insomnia</title>
      <link>https://arxiv.org/abs/2411.09875</link>
      <description>arXiv:2411.09875v1 Announce Type: new 
Abstract: This study investigates the sleep characteristics and brain activity of individuals in the gray zone of insomnia, a population that experiences sleep disturbances yet does not fully meet the clinical criteria for chronic insomnia. Thirteen healthy participants and thirteen individuals from the gray zone were assessed using polysomnography and electroencephalogram to analyze both sleep architecture and neural activity. Although no significant differences in objective sleep quality or structure were found between the groups, gray zone individuals reported higher insomnia severity index scores, indicating subjective sleep difficulties. Electroencephalogram analysis revealed increased delta and alpha activity during the wake stage, suggesting lingering sleep inertia, while non-rapid eye movement stages 1 and 2 exhibited elevated beta and gamma activity, often associated with chronic insomnia. However, these high-frequency patterns were not observed in non-rapid eye movement stage 3 or rapid eye movement sleep, suggesting less severe disruptions compared to chronic insomnia. This study emphasizes that despite normal polysomnography findings, EEG patterns in gray zone individuals suggest a potential risk for chronic insomnia, highlighting the need for early identification and tailored intervention strategies to prevent progression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09875v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ha-Na Jo, Young-Seok Kweon, Seo-Hyun Lee</dc:creator>
    </item>
    <item>
      <title>A Multi-Label EEG Dataset for Mental Attention State Classification in Online Learning</title>
      <link>https://arxiv.org/abs/2411.09879</link>
      <description>arXiv:2411.09879v1 Announce Type: new 
Abstract: Attention is a vital cognitive process in the learning and memory environment, particularly in the context of online learning. Traditional methods for classifying attention states of online learners based on behavioral signals are prone to distortion, leading to increased interest in using electroencephalography (EEG) signals for authentic and accurate assessment. However, the field of attention state classification based on EEG signals in online learning faces challenges, including the scarcity of publicly available datasets, the lack of standardized data collection paradigms, and the requirement to consider the interplay between attention and other psychological states. In light of this, we present the Multi-label EEG dataset for classifying Mental Attention states (MEMA) in online learning. We meticulously designed a reliable and standard experimental paradigm with three attention states: neutral, relaxing, and concentrating, considering human physiological and psychological characteristics. This paradigm collected EEG signals from 20 subjects, each participating in 12 trials, resulting in 1,060 minutes of data. Emotional state labels, basic personal information, and personality traits were also collected to investigate the relationship between attention and other psychological states. Extensive quantitative and qualitative analysis, including a multi-label correlation study, validated the quality of the EEG attention data. The MEMA dataset and analysis provide valuable insights for advancing research on attention in online learning. The dataset is publicly available at \url{https://github.com/GuanjianLiu/MEMA}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09879v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huan Liu, Yuzhe Zhang, Guanjian Liu, Xinxin Du, Haochong Wang, Dalin Zhang</dc:creator>
    </item>
    <item>
      <title>A natural-language-based approach to intelligent data retrieval and representation for cloud BIM</title>
      <link>https://arxiv.org/abs/2411.09951</link>
      <description>arXiv:2411.09951v1 Announce Type: new 
Abstract: As the information from diverse disciplines continues to integrate during the whole life cycle of an Architecture, Engineering, and Construction (AEC) project, the BIM (Building Information Model/Modeling) becomes increasingly large. This condition will cause users difficulty in acquiring the information they truly desire on a mobile device with limited space for interaction. To improve the value of the big data of BIM, an approach to intelligent data retrieval and representation for cloud BIM applications based on natural language processing was proposed. First, strategies for data storage and query acceleration based on the popular cloud-based database were explored to handle the large amount of BIM data. Then, the concepts keyword and constraint were proposed to capture the key objects and their specifications in a natural-language-based sentence that expresses the requirements of the user. Keywords and constraints can be mapped to IFC entities or properties through the International Framework for Dictionaries (IFD). The relationship between the user's requirement and the IFC-based data model was established by path finding in a graph generated from the IFC schema, enabling data retrieval and analysis. Finally, the analyzed and summarized results of BIM data were represented based on the structure of the retrieved data. A prototype application was developed to validate the proposed approach on the data collected during the construction of the terminal of Kunming Airport, the largest single building in China. With this approach, users can significantly benefit from requesting for information and the value of BIM will be enhanced.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09951v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1111/mice.12151</arxiv:DOI>
      <arxiv:journal_reference>Computer Aided Civil and Infrastructure Engineering, 2016</arxiv:journal_reference>
      <dc:creator>Jia-Rui Lin, Zhen-Zhong Hu, Jian-Ping Zhang, Fang-Qiang Yu</dc:creator>
    </item>
    <item>
      <title>Steering AI-Driven Personalization of Scientific Text for General Audiences</title>
      <link>https://arxiv.org/abs/2411.09969</link>
      <description>arXiv:2411.09969v1 Announce Type: new 
Abstract: Digital media platforms (e.g., social media, science blogs) offer opportunities to communicate scientific content to general audiences at scale. However, these audiences vary in their scientific expertise, literacy levels, and personal backgrounds, making effective science communication challenging. To address this challenge, we designed TranSlider, an AI-powered tool that generates personalized translations of scientific text based on individual user profiles (e.g., hobbies, location, and education). Our tool features an interactive slider that allows users to steer the degree of personalization from 0 (weakly relatable) to 100 (strongly relatable), leveraging LLMs to generate the translations with given degrees. Through an exploratory study with 15 participants, we investigated both the utility of these AI-personalized translations and how interactive reading features influenced users' understanding and reading experiences. We found that participants who preferred higher degrees of personalization appreciated the relatable and contextual translations, while those who preferred lower degrees valued concise translations with subtle contextualization. Furthermore, participants reported the compounding effect of multiple translations on their understanding of scientific content. Given these findings, we discuss several implications of AI-personalized translation tools in facilitating communication in collaborative contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09969v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Taewook Kim, Dhruv Agarwal, Jordan Ackerman, Manaswi Saha</dc:creator>
    </item>
    <item>
      <title>DBenVis: A Visual Analytics System for Comparing DBMS Performance via Benchmark Programs</title>
      <link>https://arxiv.org/abs/2411.09997</link>
      <description>arXiv:2411.09997v1 Announce Type: new 
Abstract: Database benchmarking is an essential method for evaluating and comparing the performance characteristics of a database management system (DBMS). It helps researchers and developers to evaluate the efficacy of their optimizations or newly developed DBMS solutions. Also, companies can benefit by analyzing the performance of DBMS under specific workloads and leveraging the result to select the most suitable system for their needs. The proper interpretation of raw benchmark results requires effective visualization, which helps users gain meaningful insights. However, visualization of the results requires prior knowledge, and existing approaches often involve time-consuming manual tasks. This is due to the absence of a unified visual analytics system for benchmark results across diverse DBMSs. To address these challenges, we present DBenVis, an interactive visual analytics system that provides efficient and versatile benchmark results visualization. DBenVis is designed to support both online transaction processing (OLTP) and online analytic processing (OLAP) benchmarks. DBenVis provides an interactive comparison view, which enables users to perform an in-depth analysis of performance characteristics across various metrics among different DBMSs. Notably, we devise an interactive visual encoding idiom for the OLAP benchmark to represent a query execution plan as a tree. In the process of building a system, we propose novel techniques for parsing meaningful data from raw benchmark results and converting the query plan to a D3 hierarchical format. Through case studies conducted with domain experts, we demonstrate the efficacy and usability of DBenVis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09997v1</guid>
      <category>cs.HC</category>
      <category>cs.DB</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yoojin Choi, Juhee Han, Daehyun Kim</dc:creator>
    </item>
    <item>
      <title>AI and the Future of Work in Africa White Paper</title>
      <link>https://arxiv.org/abs/2411.10091</link>
      <description>arXiv:2411.10091v1 Announce Type: new 
Abstract: This white paper is the output of a multidisciplinary workshop in Nairobi (Nov 2023). Led by a cross-organisational team including Microsoft Research, NEPAD, Lelapa AI, and University of Oxford. The workshop brought together diverse thought-leaders from various sectors and backgrounds to discuss the implications of Generative AI for the future of work in Africa. Discussions centred around four key themes: Macroeconomic Impacts; Jobs, Skills and Labour Markets; Workers' Perspectives and Africa-Centris AI Platforms. The white paper provides an overview of the current state and trends of generative AI and its applications in different domains, as well as the challenges and risks associated with its adoption and regulation. It represents a diverse set of perspectives to create a set of insights and recommendations which aim to encourage debate and collaborative action towards creating a dignified future of work for everyone across Africa.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10091v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacki O'Neill, Vukosi Marivate, Barbara Glover, Winnie Karanu, Girmaw Abebe Tadesse, Akua Gyekye, Anne Makena, Wesley Rosslyn-Smith, Matthew Grollnek, Charity Wayua, Rehema Baguma, Angel Maduke, Sarah Spencer, Daniel Kandie, Dennis Ndege Maari, Natasha Mutangana, Maxamed Axmed, Nyambura Kamau, Muhammad Adamu, Frank Swaniker, Brian Gatuguti, Jonathan Donner, Mark Graham, Janet Mumo, Caroline Mbindyo, Charlette N'Guessan, Irene Githinji, Lesego Makhafola, Sean Kruger, Olivia Etyang, Mulang Onando, Joe Sevilla, Nanjira Sambuli, Martin Mbaya, Paul Breloff, Gideon M. Anapey, Tebogo L. Mogaleemang, Tiyani Nghonyama, Muthoni Wanyoike, Bhekani Mbuli, Lawrence Nderu, Wambui Nyabero, Uzma Alam, Kayode Olaleye, Caroline Njenga, Abigail Sellen, David Kairo, Rutendo Chabikwa, Najeeb G. Abdulhamid, Ketry Kubasu, Chinasa T. Okolo, Eugenia Akpo, Joel Budu, Issa Karambal, Joseph Berkoh, William Wasswa, Muchai Njagwi, Rob Burnet, Loise Ochanda, Hanlie de Bod, Elizabeth Ankrah, Selemani Kinyunyu, Mutembei Kariuki, Angel Maduke, Kizito Kiyimba, Farida Eleshin, Lillian Secelela Madeje, Catherine Muraga, Ida Nganga, Judy Gichoya, Tabbz Maina, Samuel Maina, Muchai Mercy, Millicent Ochieng, Stephanie Nyairo</dc:creator>
    </item>
    <item>
      <title>Tangi: a Tool to Create Tangible Artifacts for Sharing Insights from 360$^\circ$ Video</title>
      <link>https://arxiv.org/abs/2411.10192</link>
      <description>arXiv:2411.10192v1 Announce Type: new 
Abstract: Designers often engage with video to gain rich, temporal insights about the context of users, collaboratively analyzing it to gather ideas, challenge assumptions, and foster empathy. To capture the full visual context of users and their situations, designers are adopting 360$^\circ$ video, providing richer, more multi-layered insights. Unfortunately, the spherical nature of 360$^\circ$ video means designers cannot create tangible video artifacts such as storyboards for collaborative analysis. To overcome this limitation, we created Tangi, a web-based tool that converts 360$^\circ$ images into tangible 360$^\circ$ video artifacts, that enable designers to embody and share their insights. Our evaluation with nine experienced designers demonstrates that the artifacts Tangi creates enable tangible interactions found in collaborative workshops and introduce two new capabilities: spatial orientation within 360$^\circ$ environments and linking specific details to the broader 360$^\circ$ context. Since Tangi is an open-source tool, designers can immediately leverage 360$^\circ$ video in collaborative workshops.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10192v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Wo Meijer, Jacky Bourgeois, Tilman Dingler, Gerd Kortuem</dc:creator>
    </item>
    <item>
      <title>Exploring Augmented Table Setup and Lighting Customization in a Simulated Restaurant to Improve the User Experience</title>
      <link>https://arxiv.org/abs/2411.10230</link>
      <description>arXiv:2411.10230v1 Announce Type: new 
Abstract: This study explored a concept for using Augmented Reality (AR) glasses to customize augmented table setup and lighting in a restaurant. The aim was to provide insights into AR usage in restaurants and contribute to existing research by introducing an extendable and versatile concept for scholars and restaurateurs. A controlled laboratory study, using a within-subjects design, was conducted to investigate the effects of a customizable augmented table setup and lighting on user experience (UX), perceived waiting time, psychological ownership, and social acceptability. A simulated restaurant environment was created using a 360-degree image in Virtual Reality (VR). The study implemented default and customizable table setup and lighting. Results from a paired samples t-test showed a statistically significant effect of table setup and lighting on the pragmatic quality of UX, hedonic quality of UX, overall UX, valence, dominance, psychological ownership, and affect. Furthermore, table setup had a significant effect on arousal and perceived waiting time. Moreover, table setup significantly affected AR interaction, isolation, and safety acceptability, while lighting only affected AR interaction acceptability. Findings suggest that these investigated variables are worth considering for AR applications in a restaurant, especially when offering customizable augmented table setup and lighting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10230v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jana Motowilowa, Maurizio Vergari, Tanja Koji\'c, Maximilian Warsinke, Sebastian M\"oller, Jan-Niklas Voigt-Antons</dc:creator>
    </item>
    <item>
      <title>Generative AI in Multimodal User Interfaces: Trends, Challenges, and Cross-Platform Adaptability</title>
      <link>https://arxiv.org/abs/2411.10234</link>
      <description>arXiv:2411.10234v1 Announce Type: new 
Abstract: As the boundaries of human computer interaction expand, Generative AI emerges as a key driver in reshaping user interfaces, introducing new possibilities for personalized, multimodal and cross-platform interactions. This integration reflects a growing demand for more adaptive and intuitive user interfaces that can accommodate diverse input types such as text, voice and video, and deliver seamless experiences across devices. This paper explores the integration of generative AI in modern user interfaces, examining historical developments and focusing on multimodal interaction, cross-platform adaptability and dynamic personalization. A central theme is the interface dilemma, which addresses the challenge of designing effective interactions for multimodal large language models, assessing the trade-offs between graphical, voice-based and immersive interfaces. The paper further evaluates lightweight frameworks tailored for mobile platforms, spotlighting the role of mobile hardware in enabling scalable multimodal AI. Technical and ethical challenges, including context retention, privacy concerns and balancing cloud and on-device processing are thoroughly examined. Finally, the paper outlines future directions such as emotionally adaptive interfaces, predictive AI driven user interfaces and real-time collaborative systems, underscoring generative AI's potential to redefine adaptive user-centric interfaces across platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10234v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Bieniek, M. Rahouti, D. C. Verma</dc:creator>
    </item>
    <item>
      <title>Scaling up the Evaluation of Collaborative Problem Solving: Promises and Challenges of Coding Chat Data with ChatGPT</title>
      <link>https://arxiv.org/abs/2411.10246</link>
      <description>arXiv:2411.10246v1 Announce Type: new 
Abstract: Collaborative problem solving (CPS) is widely recognized as a critical 21st century skill. Efficiently coding communication data is a big challenge in scaling up research on assessing CPS. This paper reports the findings on using ChatGPT to directly code CPS chat data by benchmarking performance across multiple datasets and coding frameworks. We found that ChatGPT-based coding outperformed human coding in tasks where the discussions were characterized by colloquial languages but fell short in tasks where the discussions dealt with specialized scientific terminology and contexts. The findings offer practical guidelines for researchers to develop strategies for efficient and scalable analysis of communication data from CPS tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10246v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiangang Hao, Wenju Cui, Patrick Kyllonen, Emily Kerzabi, Lei Liu, Michael Flor</dc:creator>
    </item>
    <item>
      <title>From Score-Driven to Value-Sharing: Understanding Chinese Family Use of AI to Support Decision Making of College Applications</title>
      <link>https://arxiv.org/abs/2411.10280</link>
      <description>arXiv:2411.10280v1 Announce Type: new 
Abstract: This study investigates how 18-year-old students, parents, and experts in China utilize artificial intelligence (AI) tools to support decision-making in college applications during college entrance exam -- a highly competitive, score-driven, annual national exam. Through 32 interviews, we examine the use of Quark GaoKao, an AI tool that generates college application lists and acceptance probabilities based on exam scores, historical data, preferred locations, etc. Our findings show that AI tools are predominantly used by parents with limited involvement from students, and often focus on immediate exam results, failing to address long-term career goals. We also identify challenges such as misleading AI recommendations, and irresponsible use of AI by third-party consultant agencies. Finally, we offer design insights to better support multi-stakeholders' decision-making in families, especially in the Chinese context, and discuss how emerging AI tools create barriers for families with fewer resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10280v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Si Chen, Jingyi Xie, Ge Wang, Haizhou Wang, Haocong Cheng, Yun Huang</dc:creator>
    </item>
    <item>
      <title>Interactive Cycle Model -- The Linkage Combination among Automatic Speech Recognition, Large Language Models and Smart Glasses</title>
      <link>https://arxiv.org/abs/2411.10362</link>
      <description>arXiv:2411.10362v1 Announce Type: new 
Abstract: This research proposes the interaction loop model "ASR-LLM-Smart Glasses", which model combines automatic speech recognition, large language model and smart glasses to facilitate seamless human-computer interaction. And the methodology of this research involves decomposing the interaction process into different stages and elements. Speech is captured and processed by ASR, then analyzed and interpreted by LLM. The results are then transmitted to smart glasses for display. The feedback loop is complete when the user interacts with the displayed data. Mathematical formulas are used to quantify the performance of the model that revolves around core evaluation points: accuracy, coherence, and latency during ASR speech-to-text conversion. The research results are provided theoretically to test and evaluate the feasibility and performance of the model. Although such human-computer interaction products have not yet appeared in the industry, the performance indicators of this model in enhancing user experience in fields that rely on human-computer interaction have also verified its utility as a technology to promote human-computer interaction. In addition, this research pioneered the idea of integrating cutting-edge technologies such as generative pre-trained Transformer models into unique interaction models, LLM provides raw value through powerful evaluation techniques and innovative use, which provides a new perspective to evaluate and enhanced human-computer interaction.
  Keywords: Automatic speech recognition, Large Language Model, Smart glasses, Interaction mechanism</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10362v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Libo Wang</dc:creator>
    </item>
    <item>
      <title>Exploring the Future Metaverse: Research Models for User Experience, Business Readiness, and National Competitiveness</title>
      <link>https://arxiv.org/abs/2411.10408</link>
      <description>arXiv:2411.10408v1 Announce Type: new 
Abstract: This systematic literature review paper explores perspectives on the ideal metaverse from user experience, business, and national levels, considering both academic and industry viewpoints. The study examines the metaverse as a sociotechnical imaginary, enabled collectively by virtual reality (VR), augmented reality (AR), and mixed reality (MR) technologies. Through a systematic literature review, n=144 records were included and by employing grounded theory for analysis of data, we developed three research models, which can guide researchers in examining the metaverse as a sociotechnical future of information technology. Designers can apply the metaverse user experience maturity model to develop more user-friendly services, while business strategists can use the metaverse business readiness model to assess their firms' current state and prepare for transformation. Additionally, policymakers and policy analysts can utilize the metaverse national competitiveness model to track their countries' competitiveness during this paradigm shift. The synthesis of the results also led to the development of practical assessment tools derived from these models that can guide researchers</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10408v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amir Reza Asadi, Shiva Ghasemi</dc:creator>
    </item>
    <item>
      <title>AI-Driven Feedback Loops in Digital Technologies: Psychological Impacts on User Behaviour and Well-Being</title>
      <link>https://arxiv.org/abs/2411.09706</link>
      <description>arXiv:2411.09706v1 Announce Type: cross 
Abstract: The rapid spread of digital technologies has produced data-driven feedback loops, wearable devices, social media networks, and mobile applications that shape user behavior, motivation, and mental well-being. While these systems encourage self-improvement and the development of healthier habits through real-time feedback, they also create psychological risks such as technostress, addiction, and loss of autonomy. The present study also aims to investigate the positive and negative psychological consequences of feedback mechanisms on users' behaviour and well-being. Employing a descriptive survey method, the study collected data from 200 purposely selected users to assess changes in behaviour, motivation, and mental well-being related to health, social, and lifestyle applications. Results indicate that while feedback mechanisms facilitate goal attainment and social interconnection through streaks and badges, among other components, they also enhance anxiety, mental weariness, and loss of productivity due to actions that are considered feedback-seeking. Furthermore, test subjects reported that their actions are unconsciously shaped by app feedback, often at the expense of personal autonomy, while real-time feedback minimally influences professional or social interactions. The study shows that data-driven feedback loops deliver not only motivational benefits but also psychological challenges. To mitigate these risks, users should establish boundaries regarding their use of technology to prevent burnout and addiction, while developers need to refine feedback mechanisms to reduce cognitive load and foster more inclusive participation. Future research should focus on designing feedback mechanisms that promote well-being without compromising individual freedom or increasing social comparison.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09706v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Anthonette Adanyin</dc:creator>
    </item>
    <item>
      <title>Decoding Fatigue Levels of Pilots Using EEG Signals with Hybrid Deep Neural Networks</title>
      <link>https://arxiv.org/abs/2411.09707</link>
      <description>arXiv:2411.09707v1 Announce Type: cross 
Abstract: The detection of pilots' mental states is critical, as abnormal mental states have the potential to cause catastrophic accidents. This study demonstrates the feasibility of using deep learning techniques to classify different fatigue levels, specifically a normal state, low fatigue, and high fatigue. To the best of our knowledge, this is the first study to classify fatigue levels in pilots. Our approach employs the hybrid deep neural network comprising five convolutional blocks and one long short-term memory block to extract the significant features from electroencephalography signals. Ten pilots participated in the experiment, which was conducted in a simulated flight environment. Compared to four conventional models, our proposed model achieved a superior grand-average accuracy of 0.8801, outperforming other models by at least 0.0599 in classifying fatigue levels. In addition to successfully classifying fatigue levels, our model provided valuable feedback to subjects. Therefore, we anticipate that our study will make the significant contributions to the advancement of autonomous flight and driving technologies, leveraging artificial intelligence in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09707v1</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dae-Hyeok Lee, Sung-Jin Kim, Si-Hyun Kim</dc:creator>
    </item>
    <item>
      <title>Can EEG resting state data benefit data-driven approaches for motor-imagery decoding?</title>
      <link>https://arxiv.org/abs/2411.09789</link>
      <description>arXiv:2411.09789v1 Announce Type: cross 
Abstract: Resting-state EEG data in neuroscience research serve as reliable markers for user identification and reveal individual-specific traits. Despite this, the use of resting-state data in EEG classification models is limited. In this work, we propose a feature concatenation approach to enhance decoding models' generalization by integrating resting-state EEG, aiming to improve motor imagery BCI performance and develop a user-generalized model. Using feature concatenation, we combine the EEGNet model, a standard convolutional neural network for EEG signal classification, with functional connectivity measures derived from resting-state EEG data. The findings suggest that although grounded in neuroscience with data-driven learning, the concatenation approach has limited benefits for generalizing models in within-user and across-user scenarios. While an improvement in mean accuracy for within-user scenarios is observed on two datasets, concatenation doesn't benefit across-user scenarios when compared with random data concatenation. The findings indicate the necessity of further investigation on the model interpretability and the effect of random data concatenation on model robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09789v1</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rishan Mehta, Param Rajpura, Yogesh Kumar Meena</dc:creator>
    </item>
    <item>
      <title>Instruction-Guided Editing Controls for Images and Multimedia: A Survey in LLM era</title>
      <link>https://arxiv.org/abs/2411.09955</link>
      <description>arXiv:2411.09955v1 Announce Type: cross 
Abstract: The rapid advancement of large language models (LLMs) and multimodal learning has transformed digital content creation and manipulation. Traditional visual editing tools require significant expertise, limiting accessibility. Recent strides in instruction-based editing have enabled intuitive interaction with visual content, using natural language as a bridge between user intent and complex editing operations. This survey provides an overview of these techniques, focusing on how LLMs and multimodal models empower users to achieve precise visual modifications without deep technical knowledge. By synthesizing over 100 publications, we explore methods from generative adversarial networks to diffusion models, examining multimodal integration for fine-grained content control. We discuss practical applications across domains such as fashion, 3D scene manipulation, and video synthesis, highlighting increased accessibility and alignment with human intuition. Our survey compares existing literature, emphasizing LLM-empowered editing, and identifies key challenges to stimulate further research. We aim to democratize powerful visual editing across various industries, from entertainment to education. Interested readers are encouraged to access our repository at https://github.com/tamlhp/awesome-instruction-editing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09955v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.MM</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Thanh Tam Nguyen, Zhao Ren, Trinh Pham, Phi Le Nguyen, Hongzhi Yin, Quoc Viet Hung Nguyen</dc:creator>
    </item>
    <item>
      <title>Generative Agent Simulations of 1,000 People</title>
      <link>https://arxiv.org/abs/2411.10109</link>
      <description>arXiv:2411.10109v1 Announce Type: cross 
Abstract: The promise of human behavioral simulation--general-purpose computational agents that replicate human behavior across domains--could enable broad applications in policymaking and social science. We present a novel agent architecture that simulates the attitudes and behaviors of 1,052 real individuals--applying large language models to qualitative interviews about their lives, then measuring how well these agents replicate the attitudes and behaviors of the individuals that they represent. The generative agents replicate participants' responses on the General Social Survey 85% as accurately as participants replicate their own answers two weeks later, and perform comparably in predicting personality traits and outcomes in experimental replications. Our architecture reduces accuracy biases across racial and ideological groups compared to agents given demographic descriptions. This work provides a foundation for new tools that can help investigate individual and collective behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10109v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joon Sung Park, Carolyn Q. Zou, Aaron Shaw, Benjamin Mako Hill, Carrie Cai, Meredith Ringel Morris, Robb Willer, Percy Liang, Michael S. Bernstein</dc:creator>
    </item>
    <item>
      <title>Let people fail! Exploring the influence of explainable virtual and robotic agents in learning-by-doing tasks</title>
      <link>https://arxiv.org/abs/2411.10176</link>
      <description>arXiv:2411.10176v1 Announce Type: cross 
Abstract: Collaborative decision-making with artificial intelligence (AI) agents presents opportunities and challenges. While human-AI performance often surpasses that of individuals, the impact of such technology on human behavior remains insufficiently understood, primarily when AI agents can provide justifiable explanations for their suggestions. This study compares the effects of classic vs. partner-aware explanations on human behavior and performance during a learning-by-doing task. Three participant groups were involved: one interacting with a computer, another with a humanoid robot, and a third one without assistance. Results indicated that partner-aware explanations influenced participants differently based on the type of artificial agents involved. With the computer, participants enhanced their task completion times. At the same time, those interacting with the humanoid robot were more inclined to follow its suggestions, although they did not reduce their timing. Interestingly, participants autonomously performing the learning-by-doing task demonstrated superior knowledge acquisition than those assisted by explainable AI (XAI). These findings raise profound questions and have significant implications for automated tutoring and human-AI collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10176v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Matarese, Francesco Rea, Katharina J. Rohlfing, Alessandra Sciutti</dc:creator>
    </item>
    <item>
      <title>Advancing Building Energy Modeling with Large Language Models: Exploration and Case Studies</title>
      <link>https://arxiv.org/abs/2402.09579</link>
      <description>arXiv:2402.09579v2 Announce Type: replace 
Abstract: The rapid progression in artificial intelligence has facilitated the emergence of large language models like ChatGPT, offering potential applications extending into specialized engineering modeling, especially physics-based building energy modeling. This paper investigates the innovative integration of large language models with building energy modeling software, focusing specifically on the fusion of ChatGPT with EnergyPlus. A literature review is first conducted to reveal a growing trend of incorporating large language models in engineering modeling, albeit limited research on their application in building energy modeling. We underscore the potential of large language models in addressing building energy modeling challenges and outline potential applications including simulation input generation, simulation output analysis and visualization, conducting error analysis, co-simulation, simulation knowledge extraction and training, and simulation optimization. Three case studies reveal the transformative potential of large language models in automating and optimizing building energy modeling tasks, underscoring the pivotal role of artificial intelligence in advancing sustainable building practices and energy efficiency. The case studies demonstrate that selecting the right large language model techniques is essential to enhance performance and reduce engineering efforts. The findings advocate a multidisciplinary approach in future artificial intelligence research, with implications extending beyond building energy modeling to other specialized engineering modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09579v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.enbuild.2024.114788</arxiv:DOI>
      <dc:creator>Liang Zhang, Zhelun Chen, Vitaly Ford</dc:creator>
    </item>
    <item>
      <title>Exploring the Role of LLMs for Supporting Older Adults: Opportunities and Concerns</title>
      <link>https://arxiv.org/abs/2411.08123</link>
      <description>arXiv:2411.08123v2 Announce Type: replace 
Abstract: We explore some of the existing research in HCI around technology for older adults and examine the role of LLMs in enhancing it. We also discuss the digital divide and emphasize the need for inclusive technology design. At the same time, we also surface concerns regarding privacy, security, and the accuracy of information provided by LLMs, alongside the importance of user-centered design to make technology accessible and effective for the elderly. We show the transformative possibilities of LLM-supported interactions at the intersection of aging, technology, and human-computer interaction, advocating for further research and development in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08123v2</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sidharth Kaliappan, Abhay Sheel Anand, Koustuv Saha, Ravi Karkar</dc:creator>
    </item>
    <item>
      <title>Large Language Model-Based Interpretable Machine Learning Control in Building Energy Systems</title>
      <link>https://arxiv.org/abs/2402.09584</link>
      <description>arXiv:2402.09584v2 Announce Type: replace-cross 
Abstract: The potential of Machine Learning Control (MLC) in HVAC systems is hindered by its opaque nature and inference mechanisms, which is challenging for users and modelers to fully comprehend, ultimately leading to a lack of trust in MLC-based decision-making. To address this challenge, this paper investigates and explores Interpretable Machine Learning (IML), a branch of Machine Learning (ML) that enhances transparency and understanding of models and their inferences, to improve the credibility of MLC and its industrial application in HVAC systems. Specifically, we developed an innovative framework that combines the principles of Shapley values and the in-context learning feature of Large Language Models (LLMs). While the Shapley values are instrumental in dissecting the contributions of various features in ML models, LLM provides an in-depth understanding of the non-data-driven or rule-based elements in MLC; combining them, LLM further packages these insights into a coherent, human-understandable narrative. The paper presents a case study to demonstrate the feasibility of the developed IML framework for model predictive control-based precooling under demand response events in a virtual testbed. The results indicate that the developed framework generates and explains the control signals in accordance with the rule-based rationale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09584v2</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.enbuild.2024.114278</arxiv:DOI>
      <arxiv:journal_reference>Energy and Buildings, 313, 114278 (2024)</arxiv:journal_reference>
      <dc:creator>Liang Zhang, Zhelun Chen</dc:creator>
    </item>
    <item>
      <title>ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Images</title>
      <link>https://arxiv.org/abs/2403.09871</link>
      <description>arXiv:2403.09871v4 Announce Type: replace-cross 
Abstract: Designing egocentric 3D hand pose estimation systems that can perform reliably in complex, real-world scenarios is crucial for downstream applications. Previous approaches using RGB or NIR imagery struggle in challenging conditions: RGB methods are susceptible to lighting variations and obstructions like handwear, while NIR techniques can be disrupted by sunlight or interference from other NIR-equipped devices. To address these limitations, we present ThermoHands, the first benchmark focused on thermal image-based egocentric 3D hand pose estimation, demonstrating the potential of thermal imaging to achieve robust performance under these conditions. The benchmark includes a multi-view and multi-spectral dataset collected from 28 subjects performing hand-object and hand-virtual interactions under diverse scenarios, accurately annotated with 3D hand poses through an automated process. We introduce a new baseline method, TherFormer, utilizing dual transformer modules for effective egocentric 3D hand pose estimation in thermal imagery. Our experimental results highlight TherFormer's leading performance and affirm thermal imaging's effectiveness in enabling robust 3D hand pose estimation in adverse conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09871v4</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fangqiang Ding, Yunzhou Zhu, Xiangyu Wen, Gaowen Liu, Chris Xiaoxuan Lu</dc:creator>
    </item>
    <item>
      <title>Coniferest: a complete active anomaly detection framework</title>
      <link>https://arxiv.org/abs/2410.17142</link>
      <description>arXiv:2410.17142v2 Announce Type: replace-cross 
Abstract: We present coniferest, an open source generic purpose active anomaly detection framework written in Python. The package design and implemented algorithms are described. Currently, static outlier detection analysis is supported via the Isolation forest algorithm. Moreover, Active Anomaly Discovery (AAD) and Pineforest algorithms are available to tackle active anomaly detection problems. The algorithms and package performance are evaluated on a series of synthetic datasets. We also describe a few success cases which resulted from applying the package to real astronomical data in active anomaly detection tasks within the SNAD project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17142v2</guid>
      <category>astro-ph.IM</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>proceeding from Data Analytics and Management in Data Intensive Domains (DAMDID) 2024</arxiv:journal_reference>
      <dc:creator>M. V. Kornilov, V. S. Korolev, K. L. Malanchev, A. D. Lavrukhina, E. Russeil, T. A. Semenikhin, E. Gangler, E. E. O. Ishida, M. V. Pruzhinskaya, A. A. Volnova, S. Sreejith</dc:creator>
    </item>
    <item>
      <title>Provocation: Who benefits from "inclusion" in Generative AI?</title>
      <link>https://arxiv.org/abs/2411.09102</link>
      <description>arXiv:2411.09102v2 Announce Type: replace-cross 
Abstract: The demands for accurate and representative generative AI systems means there is an increased demand on participatory evaluation structures. While these participatory structures are paramount to to ensure non-dominant values, knowledge and material culture are also reflected in AI models and the media they generate, we argue that dominant structures of community participation in AI development and evaluation are not explicit enough about the benefits and harms that members of socially marginalized groups may experience as a result of their participation. Without explicit interrogation of these benefits by AI developers, as a community we may remain blind to the immensity of systemic change that is needed as well. To support this provocation, we present a speculative case study, developed from our own collective experiences as AI researchers. We use this speculative context to itemize the barriers that need to be overcome in order for the proposed benefits to marginalized communities to be realized, and harms mitigated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09102v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samantha Dalal, Siobhan Mackenzie Hall, Nari Johnson</dc:creator>
    </item>
  </channel>
</rss>
