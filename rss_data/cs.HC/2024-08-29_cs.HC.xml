<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 Aug 2024 04:00:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>From zero to figure hero. A checklist for designing scientific data visualizations</title>
      <link>https://arxiv.org/abs/2408.16007</link>
      <description>arXiv:2408.16007v1 Announce Type: new 
Abstract: Biological research spans scales and methodologies, generating complex data visualizations such as images, text, numbers, networks, and maps. With increasingly large and multimodal datasets, effective visualization is essential for efficiently conveying scientific insights. Despite this crucial role, biologist often lack training in data visualization and information design. This work addresses this gap by providing a framework for creating clear, accurate, and impactful visualizations of biological data. It is centered around a checklist that guides biologists through the process of developing publishable figures. The guide and checklist cover key aspects such as selecting appropriate display types, using color palettes effectively, and optimizing figure layouts to communicate complex data. Additionally, the work is supported by evidence from visualization research, ensuring that the checklist recommendations are grounded in established principles. By following this guide, biologists can enhance their visual data presentations, ultimately increasing the impact of their scientific findings on diverse audiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16007v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Helena Klara Jambor</dc:creator>
    </item>
    <item>
      <title>Our Stories, Our Data: Co-designing Visualizations with People with Intellectual and Developmental Disabilities</title>
      <link>https://arxiv.org/abs/2408.16072</link>
      <description>arXiv:2408.16072v1 Announce Type: new 
Abstract: Individuals with Intellectual and Developmental Disabilities (IDD) have unique needs and challenges when working with data. While visualization aims to make data more accessible to a broad audience, our understanding of how to design cognitively accessible visualizations remains limited. In this study, we engaged 20 participants with IDD as co-designers to explore how they approach and visualize data. Our preliminary investigation paired four participants as data pen-pals in a six-week online asynchronous participatory design workshop. In response to the observed conceptual, technological, and emotional struggles with data, we subsequently organized a two-day in-person co-design workshop with 16 participants to further understand relevant visualization authoring and sensemaking strategies. Reflecting on how participants engaged with and represented data, we propose two strategies for cognitively accessible data visualizations: transforming numbers into narratives and blending data design with everyday aesthetics. Our findings emphasize the importance of involving individuals with IDD in the design process, demonstrating their capacity for data analysis and expression, and underscoring the need for a narrative and tangible approach to accessible data visualization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16072v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3663548.3675615</arxiv:DOI>
      <dc:creator>Keke Wu, Ghulam Jilani Quadri, Arran Zeyu Wang, David Kwame Osei-Tutu, Emma Petersen, Varsha Koushik, Danielle Albers Szafir</dc:creator>
    </item>
    <item>
      <title>Causal Priors and Their Influence on Judgements of Causality in Visualized Data</title>
      <link>https://arxiv.org/abs/2408.16077</link>
      <description>arXiv:2408.16077v1 Announce Type: new 
Abstract: "Correlation does not imply causation" is a famous mantra in statistical and visual analysis. However, consumers of visualizations often draw causal conclusions when only correlations between variables are shown. In this paper, we investigate factors that contribute to causal relationships users perceive in visualizations. We collected a corpus of concept pairs from variables in widely used datasets and created visualizations that depict varying correlative associations using three typical statistical chart types. We conducted two MTurk studies on (1) preconceived notions on causal relations without charts, and (2) perceived causal relations with charts, for each concept pair. Our results indicate that people make assumptions about causal relationships between pairs of concepts even without seeing any visualized data. Moreover, our results suggest that these assumptions constitute causal priors that, in combination with visualized association, impact how data visualizations are interpreted. The results also suggest that causal priors may lead to over- or under-estimation in perceived causal relations in different circumstances, and that those priors can also impact users' confidence in their causal assessments. In addition, our results align with prior work, indicating that chart type may also affect causal inference. Using data from the studies, we develop a model to capture the interaction between causal priors and visualized associations as they combine to impact a user's perceived causal relations. In addition to reporting the study results and analyses, we provide an open dataset of causal priors for 56 specific concept pairs that can serve as a potential benchmark for future studies. We also suggest remaining challenges and heuristic-based guidelines to help designers improve visualization design choices to better support visual causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16077v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arran Zeyu Wang, David Borland, Tabitha C. Peck, Wenyuan Wang, David Gotz</dc:creator>
    </item>
    <item>
      <title>Beyond Correlation: Incorporating Counterfactual Guidance to Better Support Exploratory Visual Analysis</title>
      <link>https://arxiv.org/abs/2408.16078</link>
      <description>arXiv:2408.16078v1 Announce Type: new 
Abstract: Providing effective guidance for users has long been an important and challenging task for efficient exploratory visual analytics, especially when selecting variables for visualization in high-dimensional datasets. Correlation is the most widely applied metric for guidance in statistical and analytical tools, however a reliance on correlation may lead users towards false positives when interpreting causal relations in the data. In this work, inspired by prior insights on the benefits of counterfactual visualization in supporting visual causal inference, we propose a novel, simple, and efficient counterfactual guidance method to enhance causal inference performance in guided exploratory analytics based on insights and concerns gathered from expert interviews. Our technique aims to capitalize on the benefits of counterfactual approaches while reducing their complexity for users. We integrated counterfactual guidance into an exploratory visual analytics system, and using a synthetically generated ground-truth causal dataset, conducted a comparative user study and evaluated to what extent counterfactual guidance can help lead users to more precise visual causal inferences. The results suggest that counterfactual guidance improved visual causal inference performance, and also led to different exploratory behaviors compared to correlation-based guidance. Based on these findings, we offer future directions and challenges for incorporating counterfactual guidance to better support exploratory visual analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16078v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arran Zeyu Wang, David Borland, David Gotz</dc:creator>
    </item>
    <item>
      <title>Shape It Up: An Empirically Grounded Approach for Designing Shape Palettes</title>
      <link>https://arxiv.org/abs/2408.16079</link>
      <description>arXiv:2408.16079v1 Announce Type: new 
Abstract: Shape is commonly used to distinguish between categories in multi-class scatterplots. However, existing guidelines for choosing effective shape palettes rely largely on intuition and do not consider how these needs may change as the number of categories increases. Although shapes can be a finite number compared to colors, they can not be represented by a numerical space, making it difficult to propose a general guideline for shape choices or shed light on the design heuristics of designer-crafted shape palettes. This paper presents a series of four experiments evaluating the efficiency of 39 shapes across three tasks -- relative mean judgment tasks, expert choices, and data correlation estimation. Given how complex and tangled results are, rather than relying on conventional features for modeling, we built a model and introduced a corresponding design tool that offers recommendations for shape encodings. The perceptual effectiveness of shapes significantly varies across specific pairs, and certain shapes may enhance perceptual efficiency and accuracy. However, how performance varies does not map well to classical features of shape such as angles, fill, or convex hull. We developed a model based on pairwise relations between shapes measured in our experiments and the number of shapes required to intelligently recommend shape palettes for a given design. This tool provides designers with agency over shape selection while incorporating empirical elements of perceptual performance captured in our study. Our model advances the understanding of shape perception in visualization contexts and provides practical design guidelines for advanced shape usage in visualization design that optimize perceptual efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16079v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chin Tseng, Arran Zeyu Wang, Ghulam Jilani Quadri, Danielle Albers Szafir</dc:creator>
    </item>
    <item>
      <title>Striking the Right Balance: Systematic Assessment of Evaluation Method Distribution Across Contribution Types</title>
      <link>https://arxiv.org/abs/2408.16080</link>
      <description>arXiv:2408.16080v1 Announce Type: new 
Abstract: In the rapidly evolving field of information visualization, rigorous evaluation is essential for validating new techniques, understanding user interactions, and demonstrating the effectiveness and usability of visualizations. Faithful evaluations provide valuable insights into how users interact with and perceive the system, enabling designers to identify potential weaknesses and make informed decisions about design choices and improvements. However, an emerging trend of multiple evaluations within a single research raises critical questions about the sustainability, feasibility, and methodological rigor of such an approach. New researchers and students, influenced by this trend, may believe -- multiple evaluations are necessary for a study, regardless of the contribution types. However, the number of evaluations in a study should depend on its contributions and merits, not on the trend of including multiple evaluations to strengthen a paper. So, how many evaluations are enough? This is a situational question and cannot be formulaically determined. Our objective is to summarize current trends and patterns to assess the distribution of evaluation methods over different paper contribution types. In this paper, we identify this trend through a non-exhaustive literature survey of evaluation patterns in 214 papers in the two most recent years' VIS issues in IEEE TVCG from 2023 and 2024. We then discuss various evaluation strategy patterns in the information visualization field to guide practical choices and how this paper will open avenues for further discussion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16080v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feng Lin, Arran Zeyu Wang, Md Dilshadur Rahman, Danielle Albers Szafir, Ghulam Jilani Quadri</dc:creator>
    </item>
    <item>
      <title>Data Formulator 2: Iteratively Creating Rich Visualizations with AI</title>
      <link>https://arxiv.org/abs/2408.16119</link>
      <description>arXiv:2408.16119v1 Announce Type: new 
Abstract: To create rich visualizations, data analysts often need to iterate back and forth among data processing and chart specification to achieve their goals. To achieve this, analysts need not only proficiency in data transformation and visualization tools but also efforts to manage the branching history consisting of many different versions of data and charts. Recent LLM-powered AI systems have greatly improved visualization authoring experiences, for example by mitigating manual data transformation barriers via LLMs' code generation ability. However, these systems do not work well for iterative visualization authoring, because they often require analysts to provide, in a single turn, a text-only prompt that fully describes the complex visualization task to be performed, which is unrealistic to both users and models in many cases. In this paper, we present Data Formulator 2, an LLM-powered visualization system to address these challenges. With Data Formulator 2, users describe their visualization intent with blended UI and natural language inputs, and data transformation are delegated to AI. To support iteration, Data Formulator 2 lets users navigate their iteration history and reuse previous designs towards new ones so that they don't need to start from scratch every time. In a user study with eight participants, we observed that Data Formulator 2 allows participants to develop their own iteration strategies to complete challenging data exploration sessions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16119v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenglong Wang, Bongshin Lee, Steven Drucker, Dan Marshall, Jianfeng Gao</dc:creator>
    </item>
    <item>
      <title>DrowzEE-G-Mamba: Leveraging EEG and State Space Models for Driver Drowsiness Detection</title>
      <link>https://arxiv.org/abs/2408.16145</link>
      <description>arXiv:2408.16145v1 Announce Type: new 
Abstract: Driver drowsiness is identified as a critical factor in road accidents, necessitating robust detection systems to enhance road safety. This study proposes a driver drowsiness detection system, DrowzEE-G-Mamba, that combines Electroencephalography (EEG) with State Space Models (SSMs). EEG data, known for its sensitivity to alertness, is used to model driver state transitions between alert and drowsy. Compared to traditional methods, DrowzEE-G-Mamba achieves significantly improved detection rates and reduced false positives. Notably, it achieves a peak accuracy of 83.24% on the SEED-VIG dataset, surpassing existing techniques. The system maintains high accuracy across varying complexities, making it suitable for real-time applications with limited resources. This robustness is attributed to the combination of channel-split, channel-concatenation, and channel-shuffle operations within the architecture, optimizing information flow from EEG data. Additionally, the integration of convolutional layers and SSMs facilitates comprehensive analysis, capturing both local features and long-range dependencies in the EEG signals. These findings suggest the potential of DrowzEE-G-Mamba for enhancing road safety through accurate drowsiness detection. It also paves the way for developing powerful SSM-based AI algorithms in Brain-Computer Interface applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16145v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gourav Siddhad, Sayantan Dey, Partha Pratim Roy</dc:creator>
    </item>
    <item>
      <title>Passenger hazard perception based on EEG signals for highly automated driving vehicles</title>
      <link>https://arxiv.org/abs/2408.16315</link>
      <description>arXiv:2408.16315v1 Announce Type: new 
Abstract: Enhancing the safety of autonomous vehicles is crucial, especially given recent accidents involving automated systems. As passengers in these vehicles, humans' sensory perception and decision-making can be integrated with autonomous systems to improve safety. This study explores neural mechanisms in passenger-vehicle interactions, leading to the development of a Passenger Cognitive Model (PCM) and the Passenger EEG Decoding Strategy (PEDS). Central to PEDS is a novel Convolutional Recurrent Neural Network (CRNN) that captures spatial and temporal EEG data patterns. The CRNN, combined with stacking algorithms, achieves an accuracy of $85.0\% \pm 3.18\%$. Our findings highlight the predictive power of pre-event EEG data, enhancing the detection of hazardous scenarios and offering a network-driven framework for safer autonomous vehicles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16315v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashton Yu Xuan Tan, Yingkai Yang, Xiaofei Zhang, Bowen Li, Xiaorong Gao, Sifa Zheng, Jianqiang Wang, Xinyu Gu, Jun Li, Yang Zhao, Yuxin Zhang, Tania Stathaki</dc:creator>
    </item>
    <item>
      <title>Virtual Fieldwork in Immersive Environments using Game Engines</title>
      <link>https://arxiv.org/abs/2408.16346</link>
      <description>arXiv:2408.16346v1 Announce Type: new 
Abstract: Fieldwork still is the first and foremost source of insight in many disciplines of the geosciences. Virtual fieldwork is an approach meant to enable scientists trained in fieldwork to apply these skills to a virtual representation of outcrops that are inaccessible to humans e.g. due to being located on the seafloor. For this purpose we develop a virtual fieldwork software in the game engine and 3D creation tool Unreal Engine. This software is developed specifically for a large, spatially immersive environment as well as virtual reality using head-mounted displays. It contains multiple options for quantitative measurements of visualized 3D model data. We visualize three distinct real-world datasets gathered by different photogrammetric and bathymetric methods as use cases and gather initial feedback from domain experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16346v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Armin Bernstetter, Tom Kwasnitschka, Jens Karstens, Markus Schl\"uter, Isabella Peters</dc:creator>
    </item>
    <item>
      <title>Human and LLM-Based Voice Assistant Interaction: An Analytical Framework for User Verbal and Nonverbal Behaviors</title>
      <link>https://arxiv.org/abs/2408.16465</link>
      <description>arXiv:2408.16465v1 Announce Type: new 
Abstract: Recent progress in large language model (LLM) technology has significantly enhanced the interaction experience between humans and voice assistants (VAs). This project aims to explore a user's continuous interaction with LLM-based VA (LLM-VA) during a complex task. We recruited 12 participants to interact with an LLM-VA during a cooking task, selected for its complexity and the requirement for continuous interaction. We observed that users show both verbal and nonverbal behaviors, though they know that the LLM-VA can not capture those nonverbal signals. Despite the prevalence of nonverbal behavior in human-human communication, there is no established analytical methodology or framework for exploring it in human-VA interactions. After analyzing 3 hours and 39 minutes of video recordings, we developed an analytical framework with three dimensions: 1) behavior characteristics, including both verbal and nonverbal behaviors, 2) interaction stages--exploration, conflict, and integration--that illustrate the progression of user interactions, and 3) stage transition throughout the task. This analytical framework identifies key verbal and nonverbal behaviors that provide a foundation for future research and practical applications in optimizing human and LLM-VA interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16465v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Szeyi Chan, Shihan Fu, Jiachen Li, Bingsheng Yao, Smit Desai, Mirjana Prpa, Dakuo Wang</dc:creator>
    </item>
    <item>
      <title>Fostering Creative Visualisation Skills Through Data-Art Exhibitions</title>
      <link>https://arxiv.org/abs/2408.16479</link>
      <description>arXiv:2408.16479v1 Announce Type: new 
Abstract: Data-art exhibitions offer a unique and real-world setting to foster creative visualisation skills among students. They serve as real-world platform for students to display their work, bridging the gap between classroom learning and professional practice. Students must develop a technical solution, grasp the context, and produce work that is appropriate for public presentation. This scenario helps to encourage innovative thinking, engagement with the topic, and helps to enhance technical proficiency. We present our implementation of a data-art exhibition within a computing curriculum, for third-year degree-level students. Students create art-based visualisations from selected datasets and present their work in a public exhibition. We have used this initiative over the course of two academic years with different cohorts, and reflect on its impact on student learning and creativity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16479v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan C. Roberts</dc:creator>
    </item>
    <item>
      <title>VMC: A Grammar for Visualizing Statistical Model Checks</title>
      <link>https://arxiv.org/abs/2408.16702</link>
      <description>arXiv:2408.16702v1 Announce Type: new 
Abstract: Visualizations play a critical role in validating and improving statistical models. However, the design space of model check visualizations is not well understood, making it difficult for authors to explore and specify effective graphical model checks. VMC defines a model check visualization using four components: (1) samples of distributions of checkable quantities generated from the model, including predictive distributions for new data and distributions of model parameters; (2) transformations on observed data to facilitate comparison; (3) visual representations of distributions; and (4) layouts to facilitate comparing model samples and observed data. We contribute an implementation of VMC as an R package. We validate VMC by reproducing a set of canonical model check examples, and show how using VMC to generate model checks reduces the edit distance between visualizations relative to existing visualization toolkits. The findings of an interview study with three expert modelers who used VMC highlight challenges and opportunities for encouraging exploration of correct, effective model check visualizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16702v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyang Guo, Alex Kale, Matthew Kay, Jessica Hullman</dc:creator>
    </item>
    <item>
      <title>Life Histories of Taboo Knowledge Artifacts</title>
      <link>https://arxiv.org/abs/2408.16099</link>
      <description>arXiv:2408.16099v1 Announce Type: cross 
Abstract: Communicating about some vital topics -- such as sexuality and health -- is treated as taboo and subjected to censorship. How can we construct knowledge about these topics? Wikipedia is home to numerous high-quality knowledge artifacts about taboo topics like sexual organs and human reproduction. How did these artifacts come into being? How is their existence sustained? This mixed-methods comparative project builds on previous work on taboo topics in Wikipedia and draws from qualitative and quantitative approaches. We follow a sequential complementary design, developing a narrative articulation of the life of taboo articles, comparing them to nontaboo articles, and examining some of their quantifiable traits. We find that taboo knowledge artifacts develop through multiple successful collaboration styles and, unsurprisingly, that taboo subjects are the sites of conflict. We identify and describe six themes in the development of taboo knowledge artifacts. These artifacts need &lt;i&gt;resilient leadership&lt;/i&gt; and &lt;i&gt;engaged organizations&lt;/i&gt; to thrive under conditions of &lt;i&gt;limited identifiability&lt;/i&gt; and &lt;i&gt;disjointed sensemaking&lt;/i&gt;, while contributors simultaneously engage in &lt;i&gt;emergent governance&lt;/i&gt; and &lt;i&gt;imagining public audiences&lt;/i&gt;. Our observations have important implications for supporting public knowledge work on controversial subjects such as taboos and more generally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16099v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaylea Champion, Benjamin Mako Hill</dc:creator>
    </item>
    <item>
      <title>Guided Reasoning: A Non-Technical Introduction</title>
      <link>https://arxiv.org/abs/2408.16331</link>
      <description>arXiv:2408.16331v1 Announce Type: cross 
Abstract: We introduce the concept and a default implementation of Guided Reasoning. A multi-agent system is a Guided Reasoning system iff one agent (the guide) primarily interacts with other agents in order to improve reasoning quality. We describe Logikon's default implementation of Guided Reasoning in non-technical terms. This is a living document we'll gradually enrich with more detailed information and examples.
  Code: https://github.com/logikon-ai/logikon</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16331v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Gregor Betz</dc:creator>
    </item>
    <item>
      <title>Do Recommender Systems Promote Local Music? A Reproducibility Study Using Music Streaming Data</title>
      <link>https://arxiv.org/abs/2408.16430</link>
      <description>arXiv:2408.16430v1 Announce Type: cross 
Abstract: This paper examines the influence of recommender systems on local music representation, discussing prior findings from an empirical study on the LFM-2b public dataset. This prior study argued that different recommender systems exhibit algorithmic biases shifting music consumption either towards or against local content. However, LFM-2b users do not reflect the diverse audience of music streaming services. To assess the robustness of this study's conclusions, we conduct a comparative analysis using proprietary listening data from a global music streaming service, which we publicly release alongside this paper. We observe significant differences in local music consumption patterns between our dataset and LFM-2b, suggesting that caution should be exercised when drawing conclusions on local music based solely on LFM-2b. Moreover, we show that the algorithmic biases exhibited in the original work vary in our dataset, and that several unexplored model parameters can significantly influence these biases and affect the study's conclusion on both datasets. Finally, we discuss the complexity of accurately labeling local music, emphasizing the risk of misleading conclusions due to unreliable, biased, or incomplete labels. To encourage further research and ensure reproducibility, we have publicly shared our dataset and code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16430v1</guid>
      <category>cs.IR</category>
      <category>cs.DB</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristina Matrosova, Lilian Marey, Guillaume Salha-Galvan, Thomas Louail, Olivier Bodini, Manuel Moussallam</dc:creator>
    </item>
    <item>
      <title>Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming</title>
      <link>https://arxiv.org/abs/2408.16725</link>
      <description>arXiv:2408.16725v1 Announce Type: cross 
Abstract: Recent advances in language models have achieved significant progress. GPT-4o, as a new milestone, has enabled real-time conversations with humans, demonstrating near-human natural fluency. Such human-computer interaction necessitates models with the capability to perform reasoning directly with the audio modality and generate output in streaming. However, this remains beyond the reach of current academic models, as they typically depend on extra TTS systems for speech synthesis, resulting in undesirable latency. This paper introduces the Mini-Omni, an audio-based end-to-end conversational model, capable of real-time speech interaction. To achieve this capability, we propose a text-instructed speech generation method, along with batch-parallel strategies during inference to further boost the performance. Our method also helps to retain the original model's language capabilities with minimal degradation, enabling other works to establish real-time interaction capabilities. We call this training method "Any Model Can Talk". We also introduce the VoiceAssistant-400K dataset to fine-tune models optimized for speech output. To our best knowledge, Mini-Omni is the first fully end-to-end, open-source model for real-time speech interaction, offering valuable potential for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16725v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhifei Xie, Changqiao Wu</dc:creator>
    </item>
    <item>
      <title>Auricular Vagus Nerve Stimulation for Enhancing Remote Pilot Training and Operations</title>
      <link>https://arxiv.org/abs/2408.16755</link>
      <description>arXiv:2408.16755v1 Announce Type: cross 
Abstract: The rapid growth of the drone industry, particularly in the use of small unmanned aerial systems (sUAS) and unmanned aerial vehicles (UAVs), requires the development of advanced training protocols for remote pilots. Remote pilots must develop a combination of technical and cognitive skills to manage the complexities of modern drone operations. This paper explores the integration of neurotechnology, specifically auricular vagus nerve stimulation (aVNS), as a method to enhance remote pilot training and performance. The scientific literature shows aVNS can safely improve cognitive functions such as attention, learning, and memory. It has also been shown useful to manage stress responses. For safe and efficient sUAS/UAV operation, it is essential for pilots to maintain high levels of vigilance and decision-making under pressure. By modulating sympathetic stress and cortical arousal, aVNS can prime cognitive faculties before training, help maintain focus during training and improve stress recovery post-training. Furthermore, aVNS has demonstrated the potential to enhance multitasking and cognitive control. This may help remote pilots during complex sUAS operations by potentially reducing the risk of impulsive decision-making or cognitive errors. This paper advocates for the inclusion of aVNS in remote pilot training programs by proposing that it can provide significant benefits in improving cognitive readiness, skill and knowledge acquisition, as well as operational safety and efficiency. Future research should focus on optimizing aVNS protocols for drone pilots while assessing long-term benefits to industrial safety and workforce readiness in real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16755v1</guid>
      <category>q-bio.NC</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William J. Tyler</dc:creator>
    </item>
    <item>
      <title>Summaries, Highlights, and Action items: Design, implementation and evaluation of an LLM-powered meeting recap system</title>
      <link>https://arxiv.org/abs/2307.15793</link>
      <description>arXiv:2307.15793v2 Announce Type: replace 
Abstract: Meetings play a critical infrastructural role in the coordination of work. In recent years, due to shift to hybrid and remote work, more meetings are moving to online Computer Mediated Spaces. This has led to new problems (e.g. more time spent in less engaging meetings) and new opportunities (e.g. automated transcription/captioning and recap support). Recent advances in large language models (LLMs) for dialog summarization have the potential to improve the experience of meetings by reducing individuals' meeting load and increasing the clarity and alignment of meeting outputs. Despite this potential, they face technological limitation due to long transcripts and inability to capture diverse recap needs based on user's context. To address these gaps, we design, implement and evaluate in-context a meeting recap system. We first conceptualize two salient recap representations -- important highlights, and a structured, hierarchical minutes view. We develop a system to operationalize the representations with dialogue summarization as its building blocks. Finally, we evaluate the effectiveness of the system with seven users in the context of their work meetings. Our findings show promise in using LLM-based dialogue summarization for meeting recap and the need for both representations in different contexts. However, we find that LLM-based recap still lacks an understanding of whats personally relevant to participants, can miss important details, and mis-attributions can be detrimental to group dynamics. We identify collaboration opportunities such as a shared recap document that a high quality recap enables. We report on implications for designing AI systems to partner with users to learn and improve from natural interactions to overcome the limitations related to personal relevance and summarization quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15793v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sumit Asthana, Sagih Hilleli, Pengcheng He, Aaron Halfaker</dc:creator>
    </item>
    <item>
      <title>From Data Creator to Data Reuser: Distance Matters</title>
      <link>https://arxiv.org/abs/2402.07926</link>
      <description>arXiv:2402.07926v2 Announce Type: replace 
Abstract: Sharing research data is necessary, but not sufficient, for data reuse. Open science policies focus more heavily on data sharing than on reuse, yet both are complex, labor-intensive, expensive, and require infrastructure investments by multiple stakeholders. The value of data reuse lies in relationships between creators and reusers. By addressing knowledge exchange, rather than mere transactions between stakeholders, investments in data management and knowledge infrastructures can be made more wisely. Drawing upon empirical studies of data sharing and reuse, we develop the theoretical construct of distance between data creator and data reuser, identifying six distance dimensions that influence the ability to transfer knowledge effectively: domain, methods, collaboration, curation, purposes, and time and temporality. We address the social and socio-technical aspects of these dimensions, exploring ways in which they may decrease -- or increase -- distances between creators and reusers. Our theoretical framing of the distance between data creators and prospective reusers leads to recommendations to four categories of stakeholders on how to make data sharing and reuse more effective: data creators, data reusers, data archivists, and funding agencies. 'It takes a village' to share research data -- and a village to reuse data. Our aim is to provoke new research questions, new research, and new investments in effective and efficient circulation of research data; and to identify criteria for investments at each stage of data and research life cycles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07926v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.IR</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Christine L. Borgman, Paul T. Groth</dc:creator>
    </item>
    <item>
      <title>From Top-Right to User-Right: Perceptual Prioritization of Point-Feature Label Positions</title>
      <link>https://arxiv.org/abs/2407.11996</link>
      <description>arXiv:2407.11996v2 Announce Type: replace 
Abstract: In cartography, Geographic Information Systems (GIS), and visualization, the position of a label relative to its point feature is crucial for readability and user experience. Alongside other factors, the point-feature label placement (PFLP) is typically governed by the Position Priority Order (PPO), a systematic raking of potential label positions around a point feature according to predetermined priorities. While there is a broad consensus on factors such as avoiding label conflicts and ensuring clear label-to-feature associations, there is no agreement on PPO. Most PFLP techniques rely on traditional PPOs grounded in typographic and cartographic conventions established decades ago, which may no longer meet today's user expectations. In contrast, commercial products like Google Maps and Mapbox use non-traditional PPOs for unreported reasons. Our extensive user study introduces the Perceptual Position Priority Order (PerceptPPO), a user-validated PPO that significantly departs from traditional conventions. A key finding is that labels placed above point features are significantly preferred by users, contrary to the conventional top-right position. We also conducted a supplementary study on the preferred label density, an area scarcely explored in prior research. Finally, we performed a comparative user study assessing the perceived quality of PerceptPPO over existing PPOs, advocating its adoption in cartographic and GIS applications, as well as in other types of visualizations. Our research, supported by nearly 800 participants from 48 countries and over 45,500 pairwise comparisons, offers practical guidance for designers and application developers aiming to optimize user engagement and comprehension, paving the way for more intuitive and accessible visualizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11996v2</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Petr Bob\'ak, Ladislav \v{C}mol\'ik, Martin \v{C}ad\'ik</dc:creator>
    </item>
    <item>
      <title>Promises and challenges of generative artificial intelligence for human learning</title>
      <link>https://arxiv.org/abs/2408.12143</link>
      <description>arXiv:2408.12143v2 Announce Type: replace 
Abstract: Generative artificial intelligence (GenAI) holds the potential to transform the delivery, cultivation, and evaluation of human learning. This Perspective examines the integration of GenAI as a tool for human learning, addressing its promises and challenges from a holistic viewpoint that integrates insights from learning sciences, educational technology, and human-computer interaction. GenAI promises to enhance learning experiences by scaling personalised support, diversifying learning materials, enabling timely feedback, and innovating assessment methods. However, it also presents critical issues such as model imperfections, ethical dilemmas, and the disruption of traditional assessments. Cultivating AI literacy and adaptive skills is imperative for facilitating informed engagement with GenAI technologies. Rigorous research across learning contexts is essential to evaluate GenAI's impact on human cognition, metacognition, and creativity. Humanity must learn with and about GenAI, ensuring it becomes a powerful ally in the pursuit of knowledge and innovation, rather than a crutch that undermines our intellectual abilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12143v2</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lixiang Yan, Samuel Greiff, Ziwen Teuber, Dragan Ga\v{s}evi\'c</dc:creator>
    </item>
    <item>
      <title>Improving Ontology Requirements Engineering with OntoChat and Participatory Prompting</title>
      <link>https://arxiv.org/abs/2408.15256</link>
      <description>arXiv:2408.15256v2 Announce Type: replace 
Abstract: Past ontology requirements engineering (ORE) has primarily relied on manual methods, such as interviews and collaborative forums, to gather user requirements from domain experts, especially in large projects. Current OntoChat offers a framework for ORE that utilises large language models (LLMs) to streamline the process through four key functions: user story creation, competency question (CQ) extraction, CQ filtration and analysis, and ontology testing support. In OntoChat, users are expected to prompt the chatbot to generate user stories. However, preliminary evaluations revealed that they struggle to do this effectively. To address this issue, we experimented with a research method called participatory prompting, which involves researcher-mediated interactions to help users without deep knowledge of LLMs use the chatbot more effectively. This participatory prompting user study produces pre-defined prompt templates based on user queries, focusing on creating and refining personas, goals, scenarios, sample data, and data resources for user stories. These refined user stories will subsequently be converted into CQs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15256v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihang Zhao, Bohui Zhang, Xi Hu, Shuyin Ouyang, Jongmo Kim, Nitisha Jain, Jacopo de Berardinis, Albert Mero\~no-Pe\~nuela, Elena Simperl</dc:creator>
    </item>
    <item>
      <title>Super-intelligent society for the silver segment: Ethics in design</title>
      <link>https://arxiv.org/abs/2408.15618</link>
      <description>arXiv:2408.15618v2 Announce Type: replace 
Abstract: A super-intelligent AI- society should be based on inclusion, so that all members of society can equally benefit from the possibilities new technologies offer in everyday life. At present, the digital society is overwhelming many people, a large group of whom are older adults, whose quality of life has been undermined in many respects by their difficulties in using digital technology. However, this silver segment should be kept involved as active users of digital services and contribute to the functioning and development of a super-intelligent, AI-enabled society. The paper calls for action-oriented design thinking that considers the challenge to improve the quality of life, with an emphasis on ethical design and ethical impact assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15618v2</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaana Leikas, Rebekah Rousi, Hannu Vilpponen, Pertti Saariluoma</dc:creator>
    </item>
    <item>
      <title>Follow-up Attention: An Empirical Study of Developer and Neural Model Code Exploration</title>
      <link>https://arxiv.org/abs/2210.05506</link>
      <description>arXiv:2210.05506v2 Announce Type: replace-cross 
Abstract: Recent neural models of code, such as OpenAI Codex and AlphaCode, have demonstrated remarkable proficiency at code generation due to the underlying attention mechanism. However, it often remains unclear how the models actually process code, and to what extent their reasoning and the way their attention mechanism scans the code matches the patterns of developers. A poor understanding of the model reasoning process limits the way in which current neural models are leveraged today, so far mostly for their raw prediction. To fill this gap, this work studies how the processed attention signal of three open large language models - CodeGen, InCoder and GPT-J - agrees with how developers look at and explore code when each answers the same sensemaking questions about code. Furthermore, we contribute an open-source eye-tracking dataset comprising 92 manually-labeled sessions from 25 developers engaged in sensemaking tasks. We empirically evaluate five heuristics that do not use the attention and ten attention-based post-processing approaches of the attention signal of CodeGen against our ground truth of developers exploring code, including the novel concept of follow-up attention which exhibits the highest agreement between model and human attention. Our follow-up attention method can predict the next line a developer will look at with 47% accuracy. This outperforms the baseline prediction accuracy of 42.3%, which uses the session history of other developers to recommend the next line. These results demonstrate the potential of leveraging the attention signal of pre-trained models for effective code exploration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.05506v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Paltenghi, Rahul Pandita, Austin Z. Henley, Albert Ziegler</dc:creator>
    </item>
    <item>
      <title>PsychoGAT: A Novel Psychological Measurement Paradigm through Interactive Fiction Games with LLM Agents</title>
      <link>https://arxiv.org/abs/2402.12326</link>
      <description>arXiv:2402.12326v2 Announce Type: replace-cross 
Abstract: Psychological measurement is essential for mental health, self-understanding, and personal development. Traditional methods, such as self-report scales and psychologist interviews, often face challenges with engagement and accessibility. While game-based and LLM-based tools have been explored to improve user interest and automate assessment, they struggle to balance engagement with generalizability. In this work, we propose PsychoGAT (Psychological Game AgenTs) to achieve a generic gamification of psychological assessment. The main insight is that powerful LLMs can function both as adept psychologists and innovative game designers. By incorporating LLM agents into designated roles and carefully managing their interactions, PsychoGAT can transform any standardized scales into personalized and engaging interactive fiction games. To validate the proposed method, we conduct psychometric evaluations to assess its effectiveness and employ human evaluators to examine the generated content across various psychological constructs, including depression, cognitive distortions, and personality traits. Results demonstrate that PsychoGAT serves as an effective assessment tool, achieving statistically significant excellence in psychometric metrics such as reliability, convergent validity, and discriminant validity. Moreover, human evaluations confirm PsychoGAT's enhancements in content coherence, interactivity, interest, immersion, and satisfaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12326v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qisen Yang, Zekun Wang, Honghui Chen, Shenzhi Wang, Yifan Pu, Xin Gao, Wenhao Huang, Shiji Song, Gao Huang</dc:creator>
    </item>
    <item>
      <title>Helmsman of the Masses? Evaluate the Opinion Leadership of Large Language Models in the Werewolf Game</title>
      <link>https://arxiv.org/abs/2404.01602</link>
      <description>arXiv:2404.01602v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have exhibited memorable strategic behaviors in social deductive games. However, the significance of opinion leadership exhibited by LLM-based agents has been largely overlooked, which is crucial for practical applications in multi-agent and human-AI interaction settings. Opinion leaders are individuals who have a noticeable impact on the beliefs and behaviors of others within a social group. In this work, we employ the Werewolf game as a simulation platform to assess the opinion leadership of LLMs. The game includes the role of the Sheriff, tasked with summarizing arguments and recommending decision options, and therefore serves as a credible proxy for an opinion leader. We develop a framework integrating the Sheriff role and devise two novel metrics based on the critical characteristics of opinion leaders. The first metric measures the reliability of the opinion leader, and the second assesses the influence of the opinion leader on other players' decisions. We conduct extensive experiments to evaluate LLMs of different scales. In addition, we collect a Werewolf question-answering dataset (WWQA) to assess and enhance LLM's grasp of the game rules, and we also incorporate human participants for further analysis. The results suggest that the Werewolf game is a suitable test bed to evaluate the opinion leadership of LLMs, and few LLMs possess the capacity for opinion leadership.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01602v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Silin Du, Xiaowei Zhang</dc:creator>
    </item>
    <item>
      <title>BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General Role-Playing Language Model</title>
      <link>https://arxiv.org/abs/2408.10903</link>
      <description>arXiv:2408.10903v5 Announce Type: replace-cross 
Abstract: The rapid advancement of large language models (LLMs) has revolutionized role-playing, enabling the development of general role-playing models. However, current role-playing training has two significant issues: (I) Using a predefined role profile to prompt dialogue training for specific scenarios usually leads to inconsistencies and even conflicts between the dialogue and the profile, resulting in training biases. (II) The model learns to imitate the role based solely on the profile, neglecting profile-dialogue alignment at the sentence level. In this work, we propose a simple yet effective framework called BEYOND DIALOGUE, designed to overcome these hurdles. This framework innovatively introduces "beyond dialogue" tasks to align dialogue with profile traits based on each specific scenario, thereby eliminating biases during training. Furthermore, by adopting an innovative prompting mechanism that generates reasoning outcomes for training, the framework allows the model to achieve fine-grained alignment between profile and dialogue at the sentence level. The aforementioned methods are fully automated and low-cost. Additionally, the integration of automated dialogue and objective evaluation methods forms a comprehensive framework, paving the way for general role-playing. Experimental results demonstrate that our model excels in adhering to and reflecting various dimensions of role profiles, outperforming most proprietary general and specialized role-playing baselines. All code and datasets are available at https://github.com/yuyouyu32/BeyondDialogue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10903v5</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yeyong Yu, Runsheng Yu, Haojie Wei, Zhanqiu Zhang, Quan Qian</dc:creator>
    </item>
  </channel>
</rss>
