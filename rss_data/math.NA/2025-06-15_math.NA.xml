<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.NA</link>
    <description>math.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 16 Jun 2025 04:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Analysis of Floating-Point Matrix Multiplication Computed via Integer Arithmetic</title>
      <link>https://arxiv.org/abs/2506.11277</link>
      <description>arXiv:2506.11277v1 Announce Type: new 
Abstract: Ootomo, Ozaki, and Yokota [Int. J. High Perform. Comput. Appl., 38 (2024), p. 297-313] have proposed a strategy to recast a floating-point matrix multiplication in terms of integer matrix products. The factors A and B are split into integer slices, the product of these slices is computed exactly, and AB is approximated by accumulating these integer products in floating-point arithmetic. This technique is particularly well suited to mixed-precision matrix multiply-accumulate units with integer support, such as the NVIDIA tensor cores or the AMD matrix cores. The number of slices allows for performance-accuracy tradeoffs: more slices yield better accuracy but require more multiplications, which in turn reduce performance. We propose an inexpensive way to estimate the minimum number of multiplications needed to achieve a prescribed level of accuracy. Our error analysis shows that the algorithm may become inaccurate (or inefficient) if rows of A or columns of B are badly scaled. We perform a range of numerical experiments, both in simulation and on the latest NVIDIA GPUs, that confirm the analysis and illustrate strengths and weaknesses of the algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11277v1</guid>
      <category>math.NA</category>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahmad Abdelfattah, Jack Dongarra, Massimiliano Fasi, Mantas Mikaitis, Fran\c{c}oise Tisseur</dc:creator>
    </item>
    <item>
      <title>Regularization for time-dependent inverse problems: Geometry of Lebesgue-Bochner spaces and algorithms</title>
      <link>https://arxiv.org/abs/2506.11291</link>
      <description>arXiv:2506.11291v1 Announce Type: new 
Abstract: We consider time-dependent inverse problems in a mathematical setting using Lebesgue-Bochner spaces. Such problems arise when one aims to recover a function from given observations where the function or the data depend on time. Lebesgue-Bochner spaces allow to easily incorporate the different nature of time and space.
  In this manuscript, we present two different regularization methods in Lebesgue Bochner spaces:
  1. classical Tikhonov regularization in Banach spaces
  2. variational regularization by penalizing the time-derivative
  In the first case, we additionally investigate geometrical properties of Lebesgue Bochner spaces. In particular, we compute the duality mapping and show that these spaces are smooth of power type. With this we can implement Tikhononv regularization in Lebesgue-Bochner spaces using different regularities for time and space.
  We test both methods using the example of dynamic computerized tomography.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11291v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gesa Sarnighausen, Thorsten Hohage, Martin Burger, Andreas Hauptmann, Anne Wald</dc:creator>
    </item>
    <item>
      <title>On existence of a variational regularization parameter under Morozov's discrepancy principle</title>
      <link>https://arxiv.org/abs/2506.11397</link>
      <description>arXiv:2506.11397v1 Announce Type: new 
Abstract: Morozov's discrepancy principle is commonly adopted in Tikhonov regularization for choosing the regularization parameter. Nevertheless, for a general non-linear inverse problem, the discrepancy $\|F(x_{\alpha}^{\delta})-y^{\delta}\|_Y$ does not depend continuously on $\alpha$ and it is questionable whether there exists a regularization parameter $\alpha$ such that $\tau_1\delta\leq \|F(x_{\alpha}^{\delta})-y^{\delta}\|_Y\leq \tau_2 \delta$ $(1\le \tau_1&lt;\tau_2)$. In this paper, we prove the existence of $\alpha$ under Morozov's discrepancy principle if $\tau_2\ge (3+2\gamma)\tau_1$, where $\gamma&gt;0$ is a parameter in a tangential cone condition for the nonlinear operator $F$. Furthermore, we present results on the convergence of the regularized solutions under Morozov's discrepancy principle. Numerical results are reported on the efficiency of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11397v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liang Ding, Long Li, Weimin Han, Wei Wang</dc:creator>
    </item>
    <item>
      <title>Transformed Diffusion-Wave fPINNs: Enhancing Computing Efficiency for PINNs Solving Time-Fractional Diffusion-Wave Equations</title>
      <link>https://arxiv.org/abs/2506.11518</link>
      <description>arXiv:2506.11518v1 Announce Type: new 
Abstract: We propose transformed Diffsuion-Wave fractional Physics-Informed Neural Networks (tDWfPINNs) for efficiently solving time-fractional diffusion-wave equations with fractional order $\alpha\in(1,2)$. Conventional numerical methods for these equations often compromise the mesh-free advantage of Physics-Informed Neural Networks (PINNs) or impose high computational costs when computing fractional derivatives. The proposed method avoids first-order derivative calculations at quadrature points by introducing an integrand transformation technique, significantly reducing computational costs associated with fractional derivative evaluation while preserving accuracy. We conduct a comprehensive comparative analysis applying this integrand transformation in conjunction with both Monte Carlo integration and Gauss-Jacobi quadrature schemes across various time-fractional PDEs. Our results demonstrate that tDWfPINNs achieve superior computational efficiency without sacrificing accuracy. Furthermore, we incorporate the proposed approach into adaptive sampling approaches such as the residual-based adaptive distribution (RAD) for the time-fractional Burgers equation with order $\alpha\in(1,2)$, which exhibits complex solution dynamics. The experiments show that the Gauss-Jacobi method typically outperforms the Monte Carlo approach; however, careful consideration is required when selecting the number of quadrature points. Overall, the proposed tDWfPINNs offer a significant advancement in the numerical solution of time-fractional diffusion-wave equations, providing an accurate and scalable mesh-free alternative for challenging fractional models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11518v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Li, Zhengqi Zhang</dc:creator>
    </item>
    <item>
      <title>Error Analysis of Truncation Legendre Method for Solving Numerical Differentiation</title>
      <link>https://arxiv.org/abs/2506.11529</link>
      <description>arXiv:2506.11529v1 Announce Type: new 
Abstract: We study the problem of numerical differentiation of functions from weighted Wiener classes. We construct and analyze a truncation Legendre method to recover arbitrary order derivatives. The main focus is on obtaining error estimates in integral and uniform metrics. Unlike previous studies, which predominantly focused on first-order derivatives and specific functional spaces, we conduct a comprehensive analysis across a wide spectrum of function regularity parameters and various metrics for measuring errors. We establish precise error bounds for the truncation method in the metrics of C and L_q for 2 less than or equal to q less than or equal to infinity, and determine optimal truncation parameters as functions of the error level and smoothness parameters. Our results demonstrate that the truncation method achieves optimal convergence rates on weighted Wiener classes, requiring an optimal number of perturbed Fourier-Legendre coefficients for effective derivative recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11529v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maksym Kyselov</dc:creator>
    </item>
    <item>
      <title>Refined stability estimates for mixed problems by exploiting semi norm arguments</title>
      <link>https://arxiv.org/abs/2506.11566</link>
      <description>arXiv:2506.11566v1 Announce Type: new 
Abstract: Refined stability estimates are derived for classical mixed problems. The novel emphasis is on the importance of semi norms on data functionals, inspired by recent progress on pressure-robust discretizations for the incompressible Navier--Stokes equations. In fact, kernels of these semi norms are shown to be connected to physical regimes in applications and are related to some well-known consistency errors in classical discretizations of mixed problems. Consequently, significantly sharper stability estimates for solutions close to these physical regimes are obtained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11566v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Gauger, Alexander Linke, Christian Merdon</dc:creator>
    </item>
    <item>
      <title>Quasi-Monte Carlo hyperinterpolation</title>
      <link>https://arxiv.org/abs/2506.11622</link>
      <description>arXiv:2506.11622v1 Announce Type: new 
Abstract: This paper studies a generalization of hyperinterpolation over the high-dimensional unit cube. Hyperinterpolation of degree \( m \) serves as a discrete approximation of the \( L_2 \)-orthogonal projection of the same degree, using Fourier coefficients evaluated by a positive-weight quadrature rule that exactly integrates all polynomials of degree up to \( 2m \). Traditional hyperinterpolation methods often depend on exact quadrature assumptions, which can be impractical in high-dimensional contexts. We address the challenges and advancements in hyperinterpolation, bypassing the assumption of exactness for quadrature rules by replacing it with quasi-Monte Carlo (QMC) rules and propose a novel approximation scheme with an index set \( I \), which is referred to as QMC hyperinterpolation of range \( I \). In particular, we provide concrete construction algorithms for QMC hyperinterpolation with certain lattice rules. Consequently, we show that QMC hyperinterpolation achieves accuracy comparable to traditional hyperinterpolation while avoiding the curse of dimensionality.
  Furthermore, we introduce a Lasso-based approach to improve the robustness of QMC hyperinterpolation against noise from sampling processes. Numerical experiments validate the efficacy of our proposed methods, showing significant improvements in approximation accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11622v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Congpei An, Mou Cai, Takashi Goda</dc:creator>
    </item>
    <item>
      <title>Deep Symmetric Autoencoders from the Eckart-Young-Schmidt Perspective</title>
      <link>https://arxiv.org/abs/2506.11641</link>
      <description>arXiv:2506.11641v1 Announce Type: new 
Abstract: Deep autoencoders have become a fundamental tool in various machine learning applications, ranging from dimensionality reduction and reduced order modeling of partial differential equations to anomaly detection and neural machine translation. Despite their empirical success, a solid theoretical foundation for their expressiveness remains elusive, particularly when compared to classical projection-based techniques. In this work, we aim to take a step forward in this direction by presenting a comprehensive analysis of what we refer to as symmetric autoencoders, a broad class of deep learning architectures ubiquitous in the literature. Specifically, we introduce a formal distinction between different classes of symmetric architectures, analyzing their strengths and limitations from a mathematical perspective. For instance, we show that the reconstruction error of symmetric autoencoders with orthonormality constraints can be understood by leveraging the well-renowned Eckart-Young-Schmidt (EYS) theorem. As a byproduct of our analysis, we end up developing the EYS initialization strategy for symmetric autoencoders, which is based on an iterated application of the Singular Value Decomposition (SVD). To validate our findings, we conduct a series of numerical experiments where we benchmark our proposal against conventional deep autoencoders, discussing the importance of model design and initialization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11641v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simone Brivio, Nicola Rares Franco</dc:creator>
    </item>
    <item>
      <title>Fast parallel transient electromagnetic modeling using a uniform-in-time approximation to the exponential</title>
      <link>https://arxiv.org/abs/2506.11657</link>
      <description>arXiv:2506.11657v1 Announce Type: new 
Abstract: A new approach for the parallel forward modeling of transient electromagnetic (TEM) fields is presented. It is based on a family of uniform-in-time rational approximants to the matrix exponential that share a common denominator independent of the evaluation time points. The partial fraction decomposition of this family is exploited to devise a fast solver with high parallel efficiency. The number of shifted linear systems that need to be solved in parallel does not depend on the number of required time channels nor the spatial discretization. We also argue that similar parallel efficiency gains can be expected when solving the inverse TEM problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11657v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.geo-ph</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ralph-Uwe B\"orner, Stefan G\"uttel</dc:creator>
    </item>
    <item>
      <title>Automatic differentiation for Lax-Wendroff-type discretizations</title>
      <link>https://arxiv.org/abs/2506.11719</link>
      <description>arXiv:2506.11719v1 Announce Type: new 
Abstract: Lax-Wendroff methods combined with discontinuous Galerkin/flux reconstruction spatial discretization provide a high-order, single-stage, quadrature-free method for solving hyperbolic conservation laws. In this work, we introduce automatic differentiation (AD) in the element-local time average flux computation step (the predictor step) of Lax-Wendroff methods. The application of AD is similar for methods of any order and does not need positivity corrections during the predictor step. This contrasts with the approximate Lax-Wendroff procedure, which requires different finite difference formulas for different orders of the method and positivity corrections in the predictor step for fluxes that can only be computed on admissible states. The method is Jacobian-free and problem-independent, allowing direct application to any physical flux function. Numerical experiments demonstrate the order and positivity preservation of the method. Additionally, performance comparisons indicate that the wall-clock time of automatic differentiation is always on par with the approximate Lax-Wendroff method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11719v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arpit Babbar, Valentin Churavy, Michael Schlottke Lakemper, Hendrik Ranocha</dc:creator>
    </item>
    <item>
      <title>Data-driven approaches to inverse problems</title>
      <link>https://arxiv.org/abs/2506.11732</link>
      <description>arXiv:2506.11732v1 Announce Type: new 
Abstract: Inverse problems are concerned with the reconstruction of unknown physical quantities using indirect measurements and are fundamental across diverse fields such as medical imaging, remote sensing, and material sciences. These problems serve as critical tools for visualizing internal structures beyond what is visible to the naked eye, enabling quantification, diagnosis, prediction, and discovery. However, most inverse problems are ill-posed, necessitating robust mathematical treatment to yield meaningful solutions. While classical approaches provide mathematically rigorous and computationally stable solutions, they are constrained by the ability to accurately model solution properties and implement them efficiently.
  A more recent paradigm considers deriving solutions to inverse problems in a data-driven manner. Instead of relying on classical mathematical modeling, this approach utilizes highly over-parameterized models, typically deep neural networks, which are adapted to specific inverse problems using carefully selected training data. Current approaches that follow this new paradigm distinguish themselves through solution accuracy paired with computational efficiency that was previously inconceivable.
  These notes offer an introduction to this data-driven paradigm for inverse problems. The first part of these notes will provide an introduction to inverse problems, discuss classical solution strategies, and present some applications. The second part will delve into modern data-driven approaches, with a particular focus on adversarial regularization and provably convergent linear plug-and-play denoisers. Throughout the presentation of these methodologies, their theoretical properties will be discussed, and numerical examples will be provided. The lecture series will conclude with a discussion of open problems and future perspectives in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11732v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carola-Bibiane Sch\"onlieb, Zakhar Shumaylov</dc:creator>
    </item>
    <item>
      <title>Learning to Integrate</title>
      <link>https://arxiv.org/abs/2506.11801</link>
      <description>arXiv:2506.11801v1 Announce Type: new 
Abstract: This work deals with uncertainty quantification for a generic input distribution to some resource-intensive simulation, e.g., requiring the solution of a partial differential equation. While efficient numerical methods exist to compute integrals for high-dimensional Gaussian and other separable distributions based on sparse grids (SG), input data arising in practice often does not fall into this class. We therefore employ transport maps to transform complex distributions to multivatiate standard normals. In generative learning, a number of neural network architectures have been introduced that accomplish this task approximately. Examples are affine coupling flows (ACF) and ordinary differential equation-based networks such as conditional flow matching (CFM). To compute the expectation of a quantity of interest, we numerically integrate the composition of the inverse of the learned transport map with the simulation code output. As this map is integrated over a multivariate Gaussian distribution, SG techniques can be applied. Viewing the images of the SG quadrature nodes as learned quadrature nodes for a given complex distribution motivates our title. We demonstrate our method for monomials of total degrees for which the unmapped SG rules are exact. We also apply our approach to the stationary diffusion equation with coefficients modeled by exponentiated L\'evy random fields, using a Karhunen-Lo\`eve-like modal expansions with 9 and 25 modes. In a series of numerical experiments, we investigate errors due to learning accuracy, quadrature, statistical estimation, truncation of the modal series of the input random field, and training data size for three normalizing flows (ACF, conditional Flow Matching and Optimal transport Flow Matching) We discuss the mathematical assumptions on which our approach is based and demonstrate its shortcomings when these are violated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11801v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oliver G. Ernst, Hanno Gottschalk, Toni Kowalewitz, Patrick Kr\"uger</dc:creator>
    </item>
    <item>
      <title>Random Batch Methods for Discretized PDEs on Graphs</title>
      <link>https://arxiv.org/abs/2506.11809</link>
      <description>arXiv:2506.11809v1 Announce Type: new 
Abstract: Gas transport and other complex real-world challenges often require solving and controlling partial differential equations (PDEs) defined on graph structures, which typically demand substantial memory and computational resources. The Random Batch Method (RBM) offers significant relief from these demands by enabling the simulation of large-scale systems with reduced computational cost.
  In this paper, we analyze the application of RBM for solving PDEs on one-dimensional graphs, specifically concentrating on the heat equation. Our approach involves a two-step process: initially discretizing the PDE to transform it into a finite-dimensional problem, followed by the application of the RBM. We refer to this integrated approach as discretize+RBM. We establish the convergence of this method in expectation, under the appropriate selection and simultaneous reduction of the switching parameter in RBM and the discretization parameters. Moreover, we extend these findings to include the optimal control of the heat equation on graphs, enhancing the practical utility of our methodology. The efficacy and computational efficiency of our proposed solution are corroborated through numerical experiments that not only demonstrate convergence but also show significant reductions in computational costs.
  Our algorithm can be viewed as a randomized variant of domain decomposition, specifically adapted for PDEs defined on graph structures. It is sufficiently versatile to be applied to a wide range of linear PDEs -- not just the heat equation -- while maintaining comparable analytical guarantees and convergence properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11809v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mart\'in Hern\'andez, Enrique Zuazua</dc:creator>
    </item>
    <item>
      <title>Second-Order Linear Relaxation Schemes for Time-Fractional Phase-Field Models</title>
      <link>https://arxiv.org/abs/2506.11817</link>
      <description>arXiv:2506.11817v1 Announce Type: new 
Abstract: This work uses a linear relaxation method to develop efficient numerical schemes for the time-fractional Allen-Cahn and Cahn-Hilliard equations. The L1+-CN formula is used to discretize the fractional derivative, and an auxiliary variable is introduced to approximate the nonlinear term by solving an algebraic equation rather than a differential equation as in the invariant energy quadratization (IEQ) and scalar auxiliary variable (SAV) approaches. The proposed semi-discrete scheme is linear, second-order accurate in time, and the inconsistency between the auxiliary and the original variables does not deteriorate over time. Furthermore, we prove that the scheme is unconditionally energy stable. Numerical results demonstrate the effectiveness of the proposed scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11817v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hui Yu, Zhaoyang Wang, Ping Lin</dc:creator>
    </item>
    <item>
      <title>Fourth- and Higher-order Interface Tracking of Three or More Materials with Arbitrarily Complex Topology and Geometry</title>
      <link>https://arxiv.org/abs/2506.11897</link>
      <description>arXiv:2506.11897v1 Announce Type: new 
Abstract: For interface tracking of an arbitrary number of materials in two dimensions, we propose a multiphase cubic MARS method that (a) accurately and efficiently represents the topology and geometry of the interface via graphs, cycles, and cubic splines, (b) maintains a $(r,h)$-regularity condition of the interface so that the distance between any pair of adjacent markers is within a user-specified range that may vary according to the local curvature, (c) applies to multiple materials with arbitrarily complex topology and geometry, and (d) achieves fourth-, sixth-, and eighth-order accuracy both in time and in space. In particular, all possible types of junctions, which pose challenges to VOF methods and level-set methods, are handled with ease. The fourth- and higher-order convergence rates of the proposed method are proven under the MARS framework. Results of classic benchmark tests confirm the analysis and demonstrate the superior accuracy and efficiency of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11897v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Tan, Yixiao Qian, Zhiqi Li, Qinghai Zhang</dc:creator>
    </item>
    <item>
      <title>A visco-plastic constitutive model for accurate densification and shape predictions in powder metallurgy hot isostatic pressing</title>
      <link>https://arxiv.org/abs/2506.11946</link>
      <description>arXiv:2506.11946v1 Announce Type: new 
Abstract: Powder metallurgy hot isostatic pressing (PM-HIP) is an advanced manufacturing process that produces near net shape parts with high material utilization and uniform microstructures. Despite being used frequently to produce small-scale components, the application of PM-HIP to large-scale components is limited due to inadequate understanding of its complex mechanisms that cause unpredictable post-HIP shape distortions. A computational model can provide necessary information about the intermediate and final stages of the HIP process that can help understand it better and make accurate predictions. Generally, two types of computational models are employed for PM-HIP simulations, namely, plastic and visco-plastic models. Between these, the plastic model is preferred due to its cheaper calibration approach requiring less experimental data. However, the plastic model sometimes produces incorrect predictions when slight variations of the HIP conditions are encountered in practical situations. Therefore, this work presents a visco-plastic model that addresses these limitations of the plastic model. A novel modified calibration approach is employed for the visco-plastic model that utilizes less experimental data than existing approaches. With the new approach, the data requirement is same for both plastic and visco-plastic models. This also enables a quantitative comparison of plastic and visco-plastic models, which have been only qualitatively compared in the past. When calibrated with the same experimental data, both the models are found to produce similar results. The calibrated visco-plastic model is applied to several complex geometries, and the predictions are found to be in good agreement with experimental observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11946v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subrato Sarkar, Jason R Mayeur, KPK Ajjarapu, Fred A List III, Soumya Nag, Ryan R Dehoff</dc:creator>
    </item>
    <item>
      <title>Analysis of BDDC preconditioners for non-conforming polytopal hybrid discretisation methods</title>
      <link>https://arxiv.org/abs/2506.11956</link>
      <description>arXiv:2506.11956v1 Announce Type: new 
Abstract: In this work, we build on the discrete trace theory developed in [Badia, Droniou, Tushar, arXiv (2024)] to analyse the rate of convergence of the Balancing Domain Decomposition by Constraints (BDDC) preconditioner generated from non-conforming polytopal hybrid discretisation. We prove polylogarithmic condition number bounds for the preconditioner that are independent of the mesh parameter and number of subdomains, and hold on polytopal meshes. The analysis hinges on the continuity of a face truncation operator, which we prove in the fully discrete polytopal setting. To validate the theory, we present two numerical experiments: the first verifies the truncation estimate, and the second -- a weak scalability test -- verifies the robustness of the condition number bounds for BDDC when applied to second-order elliptic problems discretised using discontinuous skeletal methods, specifically Hybridizable Discontinuous Galerkin (HDG) and Hybrid High-Order (HHO) methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11956v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Santiago Badia, Jerome Droniou, Jordi Manyer, Jai Tushar</dc:creator>
    </item>
    <item>
      <title>Learning the Analytic Geometry of Transformations to Achieve Efficient Computation</title>
      <link>https://arxiv.org/abs/2506.11990</link>
      <description>arXiv:2506.11990v1 Announce Type: new 
Abstract: We propose a novel framework for fast integral operations by uncovering hidden geometries in the row and column structures of the underlying operators. This is accomplished through an iterative procedure that constructs adaptive hierarchical partition trees, revealing latent multiscale organization and exposing local low-rank structures within the data. Guided by this geometry, we employ two complementary techniques: (1) the \emph{butterfly algorithm}, which exploits the learned hierarchical low-rank structure; and (2) \emph{adaptive best tilings} in both space and frequency using all levels of the generalized Haar--Walsh wavelet packet tree. These techniques enable efficient matrix factorization and multiplication. Unlike classical approaches that rely on prior knowledge of the underlying geometry, our method is fully data-driven and applicable to matrices arising from irregular or unknown distributions. We demonstrate the effectiveness of our approach on matrices associated with acoustic heterogeneous potential operators and families of orthogonal polynomials. The resulting compressed representations reduce storage complexity from $\mathcal{O}(N^2)$ to $\mathcal{O}(N \log N)$, enabling fast computation and scalable implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11990v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pei-Chun Su, Ronald R. Coifman</dc:creator>
    </item>
    <item>
      <title>Physical Constraint Preserving Higher Order Finite Volume Schemes for Divergence-Free Astrophysical MHD and RMHD</title>
      <link>https://arxiv.org/abs/2506.11181</link>
      <description>arXiv:2506.11181v1 Announce Type: cross 
Abstract: Higher order finite volume schemes for magnetohydrodynamics (MHD) and relativistic magnetohydrodynamics (RMHD) are very valuable because they allow us to carry out astrophysical simulations with very high accuracy. However, astrophysical problems sometimes have unusually large Mach numbers, exceptionally high Lorentz factors and very strong magnetic fields. All these effects cause higher order codes to become brittle and prone to code crashes. In this paper we document physical constraint preserving (PCP) methods for treating numerical MHD and RMHD. While unnecessary for standard problems, for stringent astrophysical problems these methods show their value. We describe higher order methods that allow divergence-free evolution of the magnetic field. We present a novel two-dimensional Riemann solver. This two-dimensional Riemann solver plays a key role in the design of PCP schemes for MHD and RMHD. We present a very simple PCP formulation and show how it is amalgamated with the evolution of face-centered magnetic fields. The methods presented here are time-explicit and do not add much to the computational cost. We show that the methods meet their design accuracies and work well on problems that would otherwise be considered too extreme for typical higher order Godunov methods of the type used in computational astrophysics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11181v1</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.HE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3847/1538-4357/ade397</arxiv:DOI>
      <dc:creator>Dinshaw S. Balsara, Deepak Bhoriya, Chetan Singh, Harish Kumar, Roger K\"appeli, Federico Gatti</dc:creator>
    </item>
    <item>
      <title>Sensor Model Identification via Simultaneous Model Selection and State Variable Determination</title>
      <link>https://arxiv.org/abs/2506.11263</link>
      <description>arXiv:2506.11263v1 Announce Type: cross 
Abstract: We present a method for the unattended gray-box identification of sensor models commonly used by localization algorithms in the field of robotics. The objective is to determine the most likely sensor model for a time series of unknown measurement data, given an extendable catalog of predefined sensor models. Sensor model definitions may require states for rigid-body calibrations and dedicated reference frames to replicate a measurement based on the robot's localization state. A health metric is introduced, which verifies the outcome of the selection process in order to detect false positives and facilitate reliable decision-making. In a second stage, an initial guess for identified calibration states is generated, and the necessity of sensor world reference frames is evaluated. The identified sensor model with its parameter information is then used to parameterize and initialize a state estimation application, thus ensuring a more accurate and robust integration of new sensor elements. This method is helpful for inexperienced users who want to identify the source and type of a measurement, sensor calibrations, or sensor reference frames. It will also be important in the field of modular multi-agent scenarios and modularized robotic platforms that are augmented by sensor modalities during runtime. Overall, this work aims to provide a simplified integration of sensor modalities to downstream applications and circumvent common pitfalls in the usage and development of localization approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11263v1</guid>
      <category>cs.RO</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Brommer, Alessandro Fornasier, Jan Steinbrener, Stephan Weiss</dc:creator>
    </item>
    <item>
      <title>A fast mesh-free boundary integral method for two-phase flow with soluble surfactant</title>
      <link>https://arxiv.org/abs/2506.11282</link>
      <description>arXiv:2506.11282v1 Announce Type: cross 
Abstract: We present an accurate and efficient boundary integral (BI) method for simulating the deformation of drops and bubbles in Stokes flow with soluble surfactant. Soluble surfactant advects and diffuses in bulk fluids while adsorbing and desorbing at interfaces. Since the fluid velocity is coupled to the surfactant concentration, the advection-diffusion equation governing the bulk surfactant concentration $C$ is nonlinear, precluding the Green's function formulation necessary for a BI method. However, in the physically representative large P\'eclet number limit, an analytical reduction of the surfactant dynamics permits a Green's function formulation for $C$ as an Abel-type time-convolution integral at each Lagrangian interface point. A challenge in developing a practical numerical method based on this formulation is the fast evaluation of the time convolution, since the kernel depends on the time history of quantities at the interface, which is only found during the time-stepping process. To address this, we develop a novel, causal version of the Fast Multipole Method that reduces the computational cost from $O(P^2)$ for direct evaluation of the time convolution to $O(P \log_2^2 P)$ per surface grid point, where $P$ is the number of time steps. In the bulk phase, the resulting method is mesh-free and provides an accurate solution to the fully coupled moving interface problem with soluble surfactant. The approach extends naturally to a broader class of advection-diffusion problems in the high P\'eclet number regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11282v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samantha G. Evans, Michael Siegel, Johannes Tausch, Michael R. Booty</dc:creator>
    </item>
    <item>
      <title>Phase-Field Modeling and Energy-Stable Schemes for Osmotic Flow through Semi-Permeable</title>
      <link>https://arxiv.org/abs/2506.11374</link>
      <description>arXiv:2506.11374v1 Announce Type: cross 
Abstract: We present a thermodynamically consistent phase-field model for simulating fluid transport across semi-permeable membranes, with a particular focus on osmotic pressure effects. The model extends the classical Navier-Stokes-Cahn-Hilliard (NSCH) system by introducing an Allen-Cahn-type transmembrane flux governed by chemical potential imbalances, resulting in a strongly coupled system involving fluid motion, solute transport, and interface dynamics. To solve this system efficiently and accurately, we develop high-order, energy-stable numerical schemes. The local discontinuous Galerkin (LDG) method is employed for spatial discretization, offering high-order accuracy and geometric flexibility. For temporal integration, we first construct a first-order decoupled scheme with rigorous energy stability, and then improve temporal accuracy via a semi-implicit spectral deferred correction (SDC) method. Numerical experiments confirm the theoretical properties of the proposed scheme and demonstrate the influence of osmotic pressure and membrane permeability on droplet morphology at equilibrium. The framework offers a robust and versatile tool for modeling transmembrane fluid transport in both biological and industrial applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11374v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ruihan Guo, Jie Shen, Shixin Xu, Xianmin Xu</dc:creator>
    </item>
    <item>
      <title>Convergence of physics-informed neural networks modeling time-harmonic wave fields</title>
      <link>https://arxiv.org/abs/2506.11395</link>
      <description>arXiv:2506.11395v1 Announce Type: cross 
Abstract: Studying physics-informed neural networks (PINNs) for modeling partial differential equations to solve the acoustic wave field has produced promising results for simple geometries in two-dimensional domains. One option is to compute the time-harmonic wave field using the Helmholtz equation. Compared to existing numerical models, the physics-informed neural networks forward problem has to overcome several topics related to the convergence of the optimization toward the "true" solution. The topics reach from considering the physical dimensionality (from 2D to 3D), the modeling of realistic sources (from a self-similar source to a realistic confined point source), the modeling of sound-hard (Neumann) boundary conditions, and the modeling of the full wave field by considering the complex solution quantities. Within this contribution, we study 3D room acoustic cases at low frequency, varying the source definition and the number of boundary condition sets and using a complex speed of sound model to account for some degree of absorption. We assess the convergence behavior by looking at the loss landscape of the PINN architecture, the $L^2$ error compared to a finite element reference simulation for each network architecture and configuration. The convergence studies showed that at least six training points per wavelength are necessary for accurate training and subsequent predictions of the PINN. The developments are part of an initiative aiming to model the low-frequency behavior of room acoustics, including absorbers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11395v1</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefan Schoder, Aneta Furmanov\'a, Viktor Hru\v{s}ka</dc:creator>
    </item>
    <item>
      <title>A stochastic Galerkin method for optimal Dirichlet boundary control problems with uncertain data</title>
      <link>https://arxiv.org/abs/2506.11479</link>
      <description>arXiv:2506.11479v1 Announce Type: cross 
Abstract: The paper deals with a stochastic Galerkin approximation of elliptic Dirichlet boundary control problems with random input data. The expectation of a tracking cost functional with the deterministic constrained control is minimized. Error estimates are derived for the control variable in $L^2(\partial \mathcal D)$-norm and state variable in $L^2(\Omega\times\mathcal D)$-norm. To solve large linear systems, appropriate preconditioners are proposed for both unconstrained and constrained scenarios. To illustrate the validity and efficiency of the proposed approaches, some numerical experiments are performed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11479v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Winkler, Hamdullah Y\"ucel</dc:creator>
    </item>
    <item>
      <title>On scattering for NLS: rigidity properties and numerical simulations via the lens transform</title>
      <link>https://arxiv.org/abs/2506.11560</link>
      <description>arXiv:2506.11560v1 Announce Type: cross 
Abstract: We analyse the scattering operator associated with the defocusing nonlinear Schr{\"o}dinger equation which captures the evolution of solutions over an infinite time-interval under the nonlinear flow of this equation. The asymptotic nature of the scattering operator (involving unbounded time) makes its computation particularly challenging. We overcome this by exploiting the space-time compactification provided by the lens transform, marking the first use of this technique in numerical simulations. This results in a highly efficient and reliable methodology for computing the scattering operator in various regimes. In developing this approach we introduce and prove several new identities and theoretical properties of the scattering operator. We support our construction with several numerical experiments which we show to agree with known analytical properties of the scattering operator, and also address the case of long-range scattering for the one-dimensional cubic Schr{\"o}dinger equation. Our simulations permit us to further explore regimes beyond current analytical understanding, and lead us to formulate new conjectures concerning fixed and rotating points of the operator, as well as its existence in the long-range setting for both defocusing and focusing cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11560v1</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>R\'emi Carles (IRMAR), Georg Maierhofer (DAMTP)</dc:creator>
    </item>
    <item>
      <title>A DC-Reformulation for Gradient-$L^0$-Constrained Problems in Function Spaces</title>
      <link>https://arxiv.org/abs/2506.11917</link>
      <description>arXiv:2506.11917v1 Announce Type: cross 
Abstract: Cardinality constraints in optimization are commonly of $L^0$-type, and they lead to sparsely supported optimizers. An efficient way of dealing with these constraints algorithmically, when the objective functional is convex, is reformulating the constraint using the difference of suitable $L^1$- and largest-$K$-norms and subsequently solving a sequence of penalized subproblems in the difference-of-convex (DC) class. We extend this DC-reformulation approach to problems with $L^0$-type cardinality constraints on the support of the gradients, \ie, problems where sparsity of the gradient and thus piecewise constant functions are the target.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11917v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Bastian Dittrich, Evelyn Herberg, Roland Herzog, Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>Optimal trace norms for Helmholtz problems</title>
      <link>https://arxiv.org/abs/2506.11944</link>
      <description>arXiv:2506.11944v1 Announce Type: cross 
Abstract: The natural $H^1(\Omega)$ energy norm for Helmholtz problems is weighted with the wavenumber modulus $\sigma$ and induces natural weighted norms on the trace spaces $H^{\pm1/2}(\Gamma)$ by minimial extension to $\Omega\subset\mathbb R^n$. This paper presents a rigorous analysis for these trace norms with an explicit characterisation by weighted Sobolev-Slobodeckij norms and scaling estimates, highlighting their dependence on the geometry of the extension set $\Omega\subset\mathbb R^n$ and the weight $\sigma$. The analysis identifies conditions under which these trace norms are intrinsic to the isolated boundary component $\Gamma\subset\partial\Omega$ and provides $\sigma$-explicit estimates for trace inequalities in weighted spaces. In these natural wavenumber-weighted norms, the boundary integral operators allow improved continuity estimates that do \emph{not} deterioriate as $\sigma\to 0$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11944v1</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Benedikt Gr\"a{\ss}le</dc:creator>
    </item>
    <item>
      <title>Spectral Estimation with Free Decompression</title>
      <link>https://arxiv.org/abs/2506.11994</link>
      <description>arXiv:2506.11994v1 Announce Type: cross 
Abstract: Computing eigenvalues of very large matrices is a critical task in many machine learning applications, including the evaluation of log-determinants, the trace of matrix functions, and other important metrics. As datasets continue to grow in scale, the corresponding covariance and kernel matrices become increasingly large, often reaching magnitudes that make their direct formation impractical or impossible. Existing techniques typically rely on matrix-vector products, which can provide efficient approximations, if the matrix spectrum behaves well. However, in settings like distributed learning, or when the matrix is defined only indirectly, access to the full data set can be restricted to only very small sub-matrices of the original matrix. In these cases, the matrix of nominal interest is not even available as an implicit operator, meaning that even matrix-vector products may not be available. In such settings, the matrix is "impalpable," in the sense that we have access to only masked snapshots of it. We draw on principles from free probability theory to introduce a novel method of "free decompression" to estimate the spectrum of such matrices. Our method can be used to extrapolate from the empirical spectral densities of small submatrices to infer the eigenspectrum of extremely large (impalpable) matrices (that we cannot form or even evaluate with full matrix-vector products). We demonstrate the effectiveness of this approach through a series of examples, comparing its performance against known limiting distributions from random matrix theory in synthetic settings, as well as applying it to submatrices of real-world datasets, matching them with their full empirical eigenspectra.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11994v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siavash Ameli, Chris van der Heide, Liam Hodgkinson, Michael W. Mahoney</dc:creator>
    </item>
    <item>
      <title>A new insight on augmented Lagrangian method with applications in machine learning</title>
      <link>https://arxiv.org/abs/2108.11125</link>
      <description>arXiv:2108.11125v4 Announce Type: replace 
Abstract: By exploiting double-penalty terms for the primal subproblem, we develop a novel relaxed augmented Lagrangian method for solving a family of convex optimization problems subject to equality or inequality constraints. The method is then extended to solve a general multi-block separable convex optimization problem, and two related primal-dual hybrid gradient algorithms are also discussed. Convergence results about the sublinear and linear convergence rates are established by variational characterizations for both the saddle-point of the problem and the first-order optimality conditions of involved subproblems. A large number of experiments on testing the linear support vector machine problem and the robust principal component analysis problem arising from machine learning indicate that our proposed algorithms perform much better than several state-of-the-art algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.11125v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Journal of Scientific Computing, 99, 53, (2024)</arxiv:journal_reference>
      <dc:creator>Jianchao Bai, Linyuan Jia, Zheng Peng</dc:creator>
    </item>
    <item>
      <title>Randomized Preconditioned Solvers for Strong Constraint 4D-Var Data Assimilation</title>
      <link>https://arxiv.org/abs/2401.15758</link>
      <description>arXiv:2401.15758v3 Announce Type: replace 
Abstract: The Strong Constraint 4D Variational (SC-4DVAR) data assimilation method is widely used in climate and weather applications. SC-4DVAR involves solving a minimization problem to compute the maximum a posteriori estimate, which we tackle using the Gauss-Newton method. The computation of the descent direction is expensive since it involves the solution of a large-scale and potentially ill-conditioned linear system, solved using the preconditioned conjugate gradient (PCG) method. To address this cost, we efficiently construct scalable preconditioners using three different randomization techniques, which all rely on a certain low-rank structure involving the Gauss-Newton Hessian. The proposed techniques come with theoretical guarantees on the condition number, and at the same time, are amenable to parallelization. We also develop an adaptive approach to estimate the sketch size and choose between the reuse or recomputation of the preconditioner. We demonstrate the performance and effectiveness of our methodology on two representative model problems--the Burgers and barotropic vorticity equation--showing a drastic reduction in both the number of PCG iterations and the number of Gauss-Newton Hessian products after including the preconditioner construction cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15758v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amit N. Subrahmanya, Vishwas Rao, Arvind K. Saibaba</dc:creator>
    </item>
    <item>
      <title>A gradient-based and determinant-free framework for fully Bayesian Gaussian process regression</title>
      <link>https://arxiv.org/abs/2412.20884</link>
      <description>arXiv:2412.20884v2 Announce Type: replace 
Abstract: Gaussian Process Regression (GPR) is widely used for inferring functions from noisy data. GPR crucially relies on the choice of a kernel, which might be specified in terms of a collection of hyperparameters that must be chosen or learned. Fully Bayesian GPR seeks to infer these kernel hyperparameters in a Bayesian sense, and the key computational challenge in sampling from their posterior distribution is the need for frequent determinant evaluations of large kernel matrices. This paper introduces a gradient-based, determinant-free approach for fully Bayesian GPR that combines a Gaussian integration trick for avoiding the determinant with Hamiltonian Monte Carlo (HMC) sampling. Our framework permits a matrix-free formulation and reduces the difficulty of dealing with hyperparameter gradients to a simple automatic differentiation. Our implementation is highly flexible and leverages GPU acceleration with linear-scaling memory footprint. Numerical experiments demonstrate the method's ability to scale gracefully to both high-dimensional hyperparameter spaces and large kernel matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20884v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>P. Michael Kielstra, Michael Lindsey</dc:creator>
    </item>
    <item>
      <title>Modelling Mosquito Population Dynamics using PINN-derived Empirical Parameters</title>
      <link>https://arxiv.org/abs/2412.07514</link>
      <description>arXiv:2412.07514v3 Announce Type: replace-cross 
Abstract: Vector-borne diseases continue to pose a significant health threat globally with more than 3 billion people at risk each year. Despite some limitations, mechanistic dynamic models are a popular approach to representing biological processes using ordinary differential equations where the parameters describe the different development and survival rates. Recent advances in population modelling have seen the combination of these mechanistic models with machine learning. One approach is physics-informed neural networks (PINNs) whereby the machine learning framework embeds physical, biological, or chemical laws into neural networks trained on observed or measured data. This enables forward simulations, predicting system behaviour from given parameters and inputs, and inverse modelling, improving parameterisation of existing parameters and estimating unknown or latent variables. In this paper, we focus on improving the parameterisation of biological processes in mechanistic models using PINNs to determine inverse parameters. In comparing mechanistic and PINN models, our experiments offer important insights into the strengths and weaknesses of both approaches but demonstrated that the PINN approach generally outperforms the dynamic model. For a deeper understanding of the performance of PINN models, a final validation was used to investigate how modifications to PINN architectures affect the performance of the framework. By varying only a single component at a time and keeping all other factors constant, we are able to observe the effect of each change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07514v3</guid>
      <category>physics.bio-ph</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Branislava Lalic, Dinh Viet Cuong, Mina Petric, Vladimir Pavlovic, Ana Firanj Sremac, Mark Roantree</dc:creator>
    </item>
    <item>
      <title>Transformers for Learning on Noisy and Task-Level Manifolds: Approximation and Generalization Insights</title>
      <link>https://arxiv.org/abs/2505.03205</link>
      <description>arXiv:2505.03205v2 Announce Type: replace-cross 
Abstract: Transformers serve as the foundational architecture for large language and video generation models, such as GPT, BERT, SORA and their successors. Empirical studies have demonstrated that real-world data and learning tasks exhibit low-dimensional structures, along with some noise or measurement error. The performance of transformers tends to depend on the intrinsic dimension of the data/tasks, though theoretical understandings remain largely unexplored for transformers. This work establishes a theoretical foundation by analyzing the performance of transformers for regression tasks involving noisy input data on a manifold. Specifically, the input data are in a tubular neighborhood of a manifold, while the ground truth function depends on the projection of the noisy data onto the manifold. We prove approximation and generalization errors which crucially depend on the intrinsic dimension of the manifold. Our results demonstrate that transformers can leverage low-complexity structures in learning task even when the input data are perturbed by high-dimensional noise. Our novel proof technique constructs representations of basic arithmetic operations by transformers, which may hold independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03205v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaiming Shen, Alex Havrilla, Rongjie Lai, Alexander Cloninger, Wenjing Liao</dc:creator>
    </item>
    <item>
      <title>PDESpectralRefiner: Achieving More Accurate Long Rollouts with Spectral Adjustment</title>
      <link>https://arxiv.org/abs/2506.10711</link>
      <description>arXiv:2506.10711v2 Announce Type: replace-cross 
Abstract: Generating accurate and stable long rollouts is a notorious challenge for time-dependent PDEs (Partial Differential Equations). Recently, motivated by the importance of high-frequency accuracy, a refiner model called PDERefiner utilizes diffusion models to refine outputs for every time step, since the denoising process could increase the correctness of modeling high frequency part. For 1-D Kuramoto-Sivashinsky equation, refiner models can degrade the amplitude of high frequency part better than not doing refinement process. However, for some other cases, the spectrum might be more complicated. For example, for a harder PDE like Navior-Stokes equation, diffusion models could over-degrade the higher frequency part. This motivates us to release the constraint that each frequency weighs the same. We enhance our refiner model with doing adjustments on spectral space, which recovers Blurring diffusion models. We developed a new v-prediction technique for Blurring diffusion models, recovering the MSE training objective on the first refinement step. We show that in this case, for different model backbones, such as U-Net and neural operators, the outputs of PDE-SpectralRefiner are more accurate for both one-step MSE loss and rollout loss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10711v2</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Li Luo</dc:creator>
    </item>
  </channel>
</rss>
