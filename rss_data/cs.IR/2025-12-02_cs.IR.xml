<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.IR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.IR</link>
    <description>cs.IR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.IR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Dec 2025 02:37:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Enhancing Talent Search Ranking with Role-Aware Expert Mixtures and LLM-based Fine-Grained Job Descriptions</title>
      <link>https://arxiv.org/abs/2512.00004</link>
      <description>arXiv:2512.00004v1 Announce Type: new 
Abstract: Talent search is a cornerstone of modern recruitment systems, yet existing approaches often struggle to capture nuanced job-specific preferences, model recruiter behavior at a fine-grained level, and mitigate noise from subjective human judgments. We present a novel framework that enhances talent search effectiveness and delivers substantial business value through two key innovations: (i) leveraging LLMs to extract fine-grained recruitment signals from job descriptions and historical hiring data, and (ii) employing a role-aware multi-gate MoE network to capture behavioral differences across recruiter roles. To further reduce noise, we introduce a multi-task learning module that jointly optimizes click-through rate (CTR), conversion rate (CVR), and resume matching relevance. Experiments on real-world recruitment data and online A/B testing show relative AUC gains of 1.70% (CTR) and 5.97% (CVR), and a 17.29% lift in click-through conversion rate. These improvements reduce dependence on external sourcing channels, enabling an estimated annual cost saving of millions of CNY.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00004v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jihang Li, Bing Xu, Zulong Chen, Chuanfei Xu, Minping Chen, Suyu Liu, Ying Zhou, Zeyi Wen</dc:creator>
    </item>
    <item>
      <title>Use of Retrieval-Augmented Large Language Model Agent for Long-Form COVID-19 Fact-Checking</title>
      <link>https://arxiv.org/abs/2512.00007</link>
      <description>arXiv:2512.00007v1 Announce Type: new 
Abstract: The COVID-19 infodemic calls for scalable fact-checking solutions that handle long-form misinformation with accuracy and reliability. This study presents SAFE (system for accurate fact extraction and evaluation), an agent system that combines large language models with retrieval-augmented generation (RAG) to improve automated fact-checking of long-form COVID-19 misinformation. SAFE includes two agents - one for claim extraction and another for claim verification using LOTR-RAG, which leverages a 130,000-document COVID-19 research corpus. An enhanced variant, SAFE (LOTR-RAG + SRAG), incorporates Self-RAG to refine retrieval via query rewriting. We evaluated both systems on 50 fake news articles (2-17 pages) containing 246 annotated claims (M = 4.922, SD = 3.186), labeled as true (14.1%), partly true (14.4%), false (27.0%), partly false (2.2%), and misleading (21.0%) by public health professionals. SAFE systems significantly outperformed baseline LLMs in all metrics (p &lt; 0.001). For consistency (0-1 scale), SAFE (LOTR-RAG) scored 0.629, exceeding both SAFE (+SRAG) (0.577) and the baseline (0.279). In subjective evaluations (0-4 Likert scale), SAFE (LOTR-RAG) also achieved the highest average ratings in usefulness (3.640), clearness (3.800), and authenticity (3.526). Adding SRAG slightly reduced overall performance, except for a minor gain in clearness. SAFE demonstrates robust improvements in long-form COVID-19 fact-checking by addressing LLM limitations in consistency and explainability. The core LOTR-RAG design proved more effective than its SRAG-augmented variant, offering a strong foundation for scalable misinformation mitigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00007v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jingyi Huang, Yuyi Yang, Mengmeng Ji, Charles Alba, Sheng Zhang, Ruopeng An</dc:creator>
    </item>
    <item>
      <title>Evolving Paradigms in Task-Based Search and Learning: A Comparative Analysis of Traditional Search Engine with LLM-Enhanced Conversational Search System</title>
      <link>https://arxiv.org/abs/2512.00313</link>
      <description>arXiv:2512.00313v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are rapidly reshaping information retrieval by enabling interactive, generative, and inference-driven search. While traditional keyword-based search remains central to web and academic information access, it often struggles to support multi-step reasoning and exploratory learning tasks. LLM-powered search interfaces, such as ChatGPT and Claude, introduce new capabilities that may influence how users formulate queries, navigate information, and construct knowledge. However, empirical understanding of these effects is still limited. This study compares search behavior and learning outcomes in two environments: a standard search engine and an LLM-powered search system. We investigate (1) how search strategies, query formulation, and evaluation behaviors differ across systems, and (2) how LLM use affects comprehension, knowledge integration, and critical thinking during search-based learning tasks. Findings offer insight into how generative AI shapes information-seeking processes and contribute to ongoing discussions in information retrieval, human-AI interaction, and technology-supported learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00313v1</guid>
      <category>cs.IR</category>
      <category>cs.HC</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhitong Guan, Yi Wang</dc:creator>
    </item>
    <item>
      <title>Breaking It Down: Domain-Aware Semantic Segmentation for Retrieval Augmented Generation</title>
      <link>https://arxiv.org/abs/2512.00367</link>
      <description>arXiv:2512.00367v1 Announce Type: new 
Abstract: Document chunking is a crucial component of Retrieval-Augmented Generation (RAG), as it directly affects the retrieval of relevant and precise context. Conventional fixed-length and recursive splitters often produce arbitrary, incoherent segments that fail to preserve semantic structure. Although semantic chunking has gained traction, its influence on generation quality remains underexplored. This paper introduces two efficient semantic chunking methods, Projected Similarity Chunking (PSC) and Metric Fusion Chunking (MFC), trained on PubMed data using three different embedding models. We further present an evaluation framework that measures the effect of chunking on both retrieval and generation by augmenting PubMedQA with full-text PubMed Central articles. Our results show substantial retrieval improvements (24x with PSC) in MRR and higher Hits@k on PubMedQA. We provide a comprehensive analysis, including statistical significance and response-time comparisons with common chunking libraries. Despite being trained on a single domain, PSC and MFC also generalize well, achieving strong out-of-domain generation performance across multiple datasets. Overall, our findings confirm that our semantic chunkers, especially PSC, consistently deliver superior performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00367v1</guid>
      <category>cs.IR</category>
      <category>cs.CL</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aparajitha Allamraju, Maitreya Prafulla Chitale, Hiranmai Sri Adibhatla, Rahul Mishra, Manish Shrivastava</dc:creator>
    </item>
    <item>
      <title>PEOAT: Personalization-Guided Evolutionary Question Assembly for One-Shot Adaptive Testing</title>
      <link>https://arxiv.org/abs/2512.00439</link>
      <description>arXiv:2512.00439v1 Announce Type: new 
Abstract: With the rapid advancement of intelligent education, Computerized Adaptive Testing (CAT) has attracted increasing attention by integrating educational psychology with deep learning technologies. Unlike traditional paper-and-pencil testing, CAT aims to efficiently and accurately assess examinee abilities by adaptively selecting the most suitable items during the assessment process. However, its real-time and sequential nature presents limitations in practical scenarios, particularly in large-scale assessments where interaction costs are high, or in sensitive domains such as psychological evaluations where minimizing noise and interference is essential. These challenges constrain the applicability of conventional CAT methods in time-sensitive or resourceconstrained environments. To this end, we first introduce a novel task called one-shot adaptive testing (OAT), which aims to select a fixed set of optimal items for each test-taker in a one-time selection. Meanwhile, we propose PEOAT, a Personalization-guided Evolutionary question assembly framework for One-shot Adaptive Testing from the perspective of combinatorial optimization. Specifically, we began by designing a personalization-aware initialization strategy that integrates differences between examinee ability and exercise difficulty, using multi-strategy sampling to construct a diverse and informative initial population. Building on this, we proposed a cognitive-enhanced evolutionary framework incorporating schema-preserving crossover and cognitively guided mutation to enable efficient exploration through informative signals. To maintain diversity without compromising fitness, we further introduced a diversity-aware environmental selection mechanism. The effectiveness of PEOAT is validated through extensive experiments on two datasets, complemented by case studies that uncovered valuable insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00439v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoshan Yu, Ziwei Huang, Shangshang Yang, Ziwen Wang, Haiping Ma, Xingyi Zhang</dc:creator>
    </item>
    <item>
      <title>DLRREC: Denoising Latent Representations via Multi-Modal Knowledge Fusion in Deep Recommender Systems</title>
      <link>https://arxiv.org/abs/2512.00596</link>
      <description>arXiv:2512.00596v1 Announce Type: new 
Abstract: Modern recommender systems struggle to effectively utilize the rich, yet high-dimensional and noisy, multi-modal features generated by Large Language Models (LLMs). Treating these features as static inputs decouples them from the core recommendation task. We address this limitation with a novel framework built on a key insight: deeply fusing multi-modal and collaborative knowledge for representation denoising. Our unified architecture introduces two primary technical innovations. First, we integrate dimensionality reduction directly into the recommendation model, enabling end-to-end co-training that makes the reduction process aware of the final ranking objective. Second, we introduce a contrastive learning objective that explicitly incorporates the collaborative filtering signal into the latent space. This synergistic process refines raw LLM embeddings, filtering noise while amplifying task-relevant signals. Extensive experiments confirm our method's superior discriminative power, proving that this integrated fusion and denoising strategy is critical for achieving state-of-the-art performance. Our work provides a foundational paradigm for effectively harnessing LLMs in recommender systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00596v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiahao Tian, Zhenkai Wang</dc:creator>
    </item>
    <item>
      <title>ProEx: A Unified Framework Leveraging Large Language Model with Profile Extrapolation for Recommendation</title>
      <link>https://arxiv.org/abs/2512.00679</link>
      <description>arXiv:2512.00679v1 Announce Type: new 
Abstract: The powerful text understanding and generation capabilities of large language models (LLMs) have brought new vitality to general recommendation with implicit feedback. One possible strategy involves generating a unique user (or item) profile from historical interaction data, which is then mapped to a semantic representation in the language space. However, a single-instance profile may be insufficient to comprehensively capture the complex intentions behind a user's interacted items. Moreover, due to the inherent instability of LLMs, a biased or misinterpreted profile could even undermine the original recommendation performance. Consequently, an intuitive solution is to generate multiple profiles for each user (or item), each reflecting a distinct aspect of their characteristics. In light of this, we propose a unified recommendation framework with multi-faceted profile extrapolation (ProEx) in this paper. By leveraging chain-of-thought reasoning, we construct multiple distinct profiles for each user and item. These new profiles are subsequently mapped into semantic vectors, extrapolating from the position of the original profile to explore a broader region of the language space. Subsequently, we introduce the concept of environments, where each environment represents a possible linear combination of all profiles. The differences across environments are minimized to reveal the inherent invariance of user preferences. We apply ProEx to three discriminative methods and three generative methods, and conduct extensive experiments on three datasets. The experimental results demonstrate that ProEx significantly enhances the performance of these base recommendation models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00679v1</guid>
      <category>cs.IR</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Zhang, Yiwen Zhang, Yu Wang, Tong Chen, Hongzhi Yin</dc:creator>
    </item>
    <item>
      <title>SHRAG: AFrameworkfor Combining Human-Inspired Search with RAG</title>
      <link>https://arxiv.org/abs/2512.00772</link>
      <description>arXiv:2512.00772v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) is gaining recognition as one of the key technological axes for next generation information retrieval, owing to its ability to mitigate the hallucination phenomenon in Large Language
  Models (LLMs)and effectively incorporate up-to-date information. However, specialized expertise is necessary to
  construct ahigh-quality retrieval system independently; moreover, RAGdemonstratesrelativelyslowerprocessing
  speeds compared to conventional pure retrieval systems because it involves both retrieval and generation stages.
  Accordingly, this study proposes SHRAG, a novel framework designed to facilitate the seamless integration of
  Information Retrieval and RAG while simultaneously securing precise retrieval performance. SHRAG utilizes a
  Large Language Model as a Query Strategist to automatically transform unstructured natural language queries
  into logically structured search queries, subsequently performing Boolean retrieval to emulate the search process
  of an expert human searcher. Furthermore, it incorporates multilingual query expansion and a multilingual
  embedding model, enabling it to perform efficient cross-lingual question answering within the multilingual
  dataset environment of the ScienceON Challenge. Experimental results demonstrate that the proposed method,
  combining logical retrieval capabilities and generative reasoning, can significantly enhance the accuracy and
  reliability of RAG systems. Furthermore, SHRAG movesbeyondconventionaldocument-centric retrieval methods,
  presenting the potential for a new search paradigm capable of providing direct and reliable responses to queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00772v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.DL</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hyunseok Ryu, Wonjune Shin, Hyun Park</dc:creator>
    </item>
    <item>
      <title>Optimizing Generative Ranking Relevance via Reinforcement Learning in Xiaohongshu Search</title>
      <link>https://arxiv.org/abs/2512.00968</link>
      <description>arXiv:2512.00968v1 Announce Type: new 
Abstract: Ranking relevance is a fundamental task in search engines, aiming to identify the items most relevant to a given user query. Traditional relevance models typically produce scalar scores or directly predict relevance labels, limiting both interpretability and the modeling of complex relevance signals. Inspired by recent advances in Chain-of-Thought (CoT) reasoning for complex tasks, we investigate whether explicit reasoning can enhance both interpretability and performance in relevance modeling. However, existing reasoning-based Generative Relevance Models (GRMs) primarily rely on supervised fine-tuning on large amounts of human-annotated or synthetic CoT data, which often leads to limited generalization. Moreover, domain-agnostic, free-form reasoning tends to be overly generic and insufficiently grounded, limiting its potential to handle the diverse and ambiguous cases prevalent in open-domain search. In this work, we formulate relevance modeling in Xiaohongshu search as a reasoning task and introduce a Reinforcement Learning (RL)-based training framework to enhance the grounded reasoning capabilities of GRMs. Specifically, we incorporate practical business-specific relevance criteria into the multi-step reasoning prompt design and propose Stepwise Advantage Masking (SAM), a lightweight process-supervision strategy which facilitates effective learning of these criteria through improved credit assignment. To enable industrial deployment, we further distill the large-scale RL-tuned model to a lightweight version suitable for real-world search systems. Extensive experiments on industrial datasets, along with online A/B tests, demonstrate the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00968v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyang Zeng, Heming Jing, Jindong Chen, Xiangli Li, Hongyu Liu, Yixuan He, Zhengyu Li, Yige Sun, Zheyong Xie, Yuqing Yang, Shaosheng Cao, Jun Fan, Yi Wu, Yao Hu</dc:creator>
    </item>
    <item>
      <title>Conversion rate prediction in online advertising: modeling techniques, performance evaluation and future directions</title>
      <link>https://arxiv.org/abs/2512.01171</link>
      <description>arXiv:2512.01171v1 Announce Type: new 
Abstract: Conversion and conversion rate (CVR) prediction play a critical role in efficient advertising decision-making. In past decades, although researchers have developed plenty of models for CVR prediction, the methodological evolution and relationships between different techniques have been precluded. In this paper, we conduct a comprehensive literature review on CVR prediction in online advertising, and classify state-of-the-art CVR prediction models into six categories with respect to the underlying techniques and elaborate on connections between these techniques. For each category of models, we present the framework of underlying techniques, their advantages and disadvantages, and discuss how they are utilized for CVR prediction. Moreover, we summarize the performance of various CVR prediction models on public and proprietary datasets. Finally, we identify research trends, major challenges, and promising future directions. We observe that results of performance evaluation reported in prior studies are not unanimous; semantics-enriched, attribution-enhanced, debiased CVR prediction and jointly modeling CTR and CVR prediction would be promising directions to explore in the future. This review is expected to provide valuable references and insights for future researchers and practitioners in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01171v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ipm.2025.104450.</arxiv:DOI>
      <arxiv:journal_reference>Information Processing &amp; Management, 63(2), 104450 (2026)</arxiv:journal_reference>
      <dc:creator>Tao Xue, Yanwu Yang, Panyu Zhai</dc:creator>
    </item>
    <item>
      <title>Toward a benchmark for CTR prediction in online advertising: datasets, evaluation protocols and perspectives</title>
      <link>https://arxiv.org/abs/2512.01179</link>
      <description>arXiv:2512.01179v1 Announce Type: new 
Abstract: This research designs a unified architecture of CTR prediction benchmark (Bench-CTR) platform that offers flexible interfaces with datasets and components of a wide range of CTR prediction models. Moreover, we construct a comprehensive system of evaluation protocols encompassing real-world and synthetic datasets, a taxonomy of metrics, standardized procedures and experimental guidelines for calibrating the performance of CTR prediction models. Furthermore, we implement the proposed benchmark platform and conduct a comparative study to evaluate a wide range of state-of-the-art models from traditional multivariate statistical to modern large language model (LLM)-based approaches on three public datasets and two synthetic datasets. Experimental results reveal that, (1) high-order models largely outperform low-order models, though such advantage varies in terms of metrics and on different datasets; (2) LLM-based models demonstrate a remarkable data efficiency, i.e., achieving the comparable performance to other models while using only 2% of the training data; (3) the performance of CTR prediction models has achieved significant improvements from 2015 to 2016, then reached a stage with slow progress, which is consistent across various datasets. This benchmark is expected to facilitate model development and evaluation and enhance practitioners' understanding of the underlying mechanisms of models in the area of CTR prediction. Code is available at https://github.com/NuriaNinja/Bench-CTR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01179v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10660-025-10061-9</arxiv:DOI>
      <arxiv:journal_reference>Electronic Commerce Research, forthcoming (2025)</arxiv:journal_reference>
      <dc:creator>Shan Gao, Yanwu Yang</dc:creator>
    </item>
    <item>
      <title>Structured Spectral Reasoning for Frequency-Adaptive Multimodal Recommendation</title>
      <link>https://arxiv.org/abs/2512.01372</link>
      <description>arXiv:2512.01372v1 Announce Type: new 
Abstract: Multimodal recommendation aims to integrate collaborative signals with heterogeneous content such as visual and textual information, but remains challenged by modality-specific noise, semantic inconsistency, and unstable propagation over user-item graphs. These issues are often exacerbated by naive fusion or shallow modeling strategies, leading to degraded generalization and poor robustness. While recent work has explored the frequency domain as a lens to separate stable from noisy signals, most methods rely on static filtering or reweighting, lacking the ability to reason over spectral structure or adapt to modality-specific reliability. To address these challenges, we propose a Structured Spectral Reasoning (SSR) framework for frequency-aware multimodal recommendation. Our method follows a four-stage pipeline: (i) Decompose graph-based multimodal signals into spectral bands via graph-guided transformations to isolate semantic granularity; (ii) Modulate band-level reliability with spectral band masking, a training-time masking with a prediction-consistency objective that suppresses brittle frequency components; (iii) Fuse complementary frequency cues using hyperspectral reasoning with low-rank cross-band interaction; and (iv) Align modality-specific spectral features via contrastive regularization to promote semantic and structural consistency. Experiments on three real-world benchmarks show consistent gains over strong baselines, particularly under sparse and cold-start settings. Additional analyses indicate that structured spectral modeling improves robustness and provides clearer diagnostics of how different bands contribute to performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01372v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Yang, Rui Zhong, Yiqun Chen, Chi Lu, Peng Jiang</dc:creator>
    </item>
    <item>
      <title>Evidence-Guided Schema Normalization for Temporal Tabular Reasoning</title>
      <link>https://arxiv.org/abs/2512.00329</link>
      <description>arXiv:2512.00329v1 Announce Type: cross 
Abstract: Temporal reasoning over evolving semi-structured tables poses a challenge to current QA systems. We propose a SQL-based approach that involves (1) generating a 3NF schema from Wikipedia infoboxes, (2) generating SQL queries, and (3) query execution. Our central finding challenges model scaling assumptions: the quality of schema design has a greater impact on QA precision than model capacity. We establish three evidence-based principles: normalization that preserves context, semantic naming that reduces ambiguity, and consistent temporal anchoring. Our best configuration (Gemini 2.5 Flash schema + Gemini-2.0-Flash queries) achieves 80.39 EM, a 16.8\% improvement over the baseline (68.89 EM).</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00329v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ashish Thanga, Vibhu Dixit, Abhilash Shankarampeta, Vivek Gupta</dc:creator>
    </item>
    <item>
      <title>The Information Theory of Similarity</title>
      <link>https://arxiv.org/abs/2512.00378</link>
      <description>arXiv:2512.00378v1 Announce Type: cross 
Abstract: We establish a precise mathematical equivalence between witness-based similarity systems (REWA) and Shannon's information theory. We prove that witness overlap is mutual information, that REWA bit complexity bounds arise from channel capacity limitations, and that ranking-preserving encodings obey rate-distortion constraints. This unification reveals that fifty years of similarity search research -- from Bloom filters to locality-sensitive hashing to neural retrieval -- implicitly developed information theory for relational data. We derive fundamental lower bounds showing that REWA's $O(\Delta^{-2} \log N)$ complexity is optimal: no encoding scheme can preserve similarity rankings with fewer bits. The framework establishes that semantic similarity has physical units (bits of mutual information), search is communication (query transmission over a noisy channel), and retrieval systems face fundamental capacity limits analogous to Shannon's channel coding theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00378v1</guid>
      <category>cs.IT</category>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikit Phadke</dc:creator>
    </item>
    <item>
      <title>Mitigating the Threshold Priming Effect in Large Language Model-Based Relevance Judgments via Personality Infusing</title>
      <link>https://arxiv.org/abs/2512.00390</link>
      <description>arXiv:2512.00390v1 Announce Type: cross 
Abstract: Recent research has explored LLMs as scalable tools for relevance labeling, but studies indicate they are susceptible to priming effects, where prior relevance judgments influence later ones. Although psychological theories link personality traits to such biases, it is unclear whether simulated personalities in LLMs exhibit similar effects. We investigate how Big Five personality profiles in LLMs influence priming in relevance labeling, using multiple LLMs on TREC 2021 and 2022 Deep Learning Track datasets. Our results show that certain profiles, such as High Openness and Low Neuroticism, consistently reduce priming susceptibility. Additionally, the most effective personality in mitigating priming may vary across models and task types. Based on these findings, we propose personality prompting as a method to mitigate threshold priming, connecting psychological evidence with LLM-based evaluation practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00390v1</guid>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nuo Chen, Hanpei Fang, Jiqun Liu, Wilson Wei, Tetsuya Sakai, Xiao-Ming Wu</dc:creator>
    </item>
    <item>
      <title>Cross-Domain Federated Semantic Communication with Global Representation Alignment and Domain-Aware Aggregation</title>
      <link>https://arxiv.org/abs/2512.00711</link>
      <description>arXiv:2512.00711v1 Announce Type: cross 
Abstract: Semantic communication can significantly improve bandwidth utilization in wireless systems by exploiting the meaning behind raw data. However, the advancements achieved through semantic communication are closely dependent on the development of deep learning (DL) models for joint source-channel coding (JSCC) encoder/decoder techniques, which require a large amount of data for training. To address this data-intensive nature of DL models, federated learning (FL) has been proposed to train a model in a distributed manner, where the server broadcasts the DL model to clients in the network for training with their local data. However, the conventional FL approaches suffer from catastrophic degradation when client data are from different domains. In contrast, in this paper, a novel FL framework is proposed to address this domain shift by constructing the global representation, which aligns with the local features of the clients to preserve the semantics of different data domains. In addition, the dominance problem of client domains with a large number of samples is identified and, then, addressed with a domain-aware aggregation approach. This work is the first to consider the domain shift in training the semantic communication system for the image reconstruction task. Finally, simulation results demonstrate that the proposed approach outperforms the model-contrastive FL (MOON) framework by 0.5 for PSNR values under three domains at an SNR of 1 dB, and this gap continues to widen as the channel quality improves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00711v1</guid>
      <category>cs.IT</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.IR</category>
      <category>math.IT</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Loc X. Nguyen, Ji Su Yoon, Huy Q. Le, Yu Qiao, Avi Deb Raha, Eui-Nam Huh, Walid Saad, Dusit Niyato, Zhu Han, Choong Seon Hong</dc:creator>
    </item>
    <item>
      <title>Upcycled and Merged MoE Reward Model for Mitigating Reward Hacking</title>
      <link>https://arxiv.org/abs/2512.00724</link>
      <description>arXiv:2512.00724v1 Announce Type: cross 
Abstract: Reward models play a critical role in Reinforcement Learning from Human Feedback (RLHF) by assessing the consistency between generated outputs and human preferences. However, conventional reward models are prone to reward hacking or over-optimization, where the policy exploits shortcut patterns to obtain high reward scores that do not reflect true human preference. Although Mixture-of-Experts (MoE)-based reward models can enhance discriminative capability, they typically introduce substantial computational overhead. To address these challenges, we propose an upcycle and merge MoE reward modeling approach. We first upcycle a dense reward model into a MoE architecture, where a shared expert captures general knowledge, while normal experts specialize in instruction-specific patterns. We then apply routing-weight normalization and merge experts back into a dense model through a learnable weight-averaging mechanism, preserving performance gains while significantly reducing inference cost. Experimental results demonstrate that our method effectively mitigates reward hacking across various model scales. Our work highlights the potential of upcycle and merge MoE structures for improving both robustness and efficiency of RLHF reward models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00724v1</guid>
      <category>cs.LG</category>
      <category>cs.IR</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lingling Fu</dc:creator>
    </item>
    <item>
      <title>Text Mining Analysis of Symptom Patterns in Medical Chatbot Conversations</title>
      <link>https://arxiv.org/abs/2512.00768</link>
      <description>arXiv:2512.00768v1 Announce Type: cross 
Abstract: The fast growth of digital health systems has led to a need to better comprehend how they interpret and represent patient-reported symptoms. Chatbots have been used in healthcare to provide clinical support and enhance the user experience, making it possible to provide meaningful clinical patterns from text-based data through chatbots. The proposed research utilises several different natural language processing methods to study the occurrences of symptom descriptions in medicine as well as analyse the patterns that emerge through these conversations within medical bots. Through the use of the Medical Conversations to Disease Dataset which contains 960 multi-turn dialogues divided into 24 Clinical Conditions, a standardised representation of conversations between patient and bot is created for further analysis by computational means. The multi-method approach uses a variety of tools, including Latent Dirichlet Allocation (LDA) to identify latent symptom themes, K-Means to group symptom descriptions by similarity, Transformer-based Named Entity Recognition (NER) to extract medical concepts, and the Apriori algorithm to discover frequent symptom pairs. Findings from the analysis indicate a coherent structure of clinically relevant topics, moderate levels of clustering cohesiveness and several high confidence rates on the relationships between symptoms like fever headache and rash itchiness. The results support the notion that conversational medical data can be a valuable diagnostic signal for early symptom interpretation, assist in strengthening decision support and improve how users interact with tele-health technology. By demonstrating a method for converting unstructured free-flowing dialogue into actionable knowledge regarding symptoms this work provides an extensible framework to further enhance future performance, dependability and clinical utility of selecting medical chatbots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00768v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hamed Razavi</dc:creator>
    </item>
    <item>
      <title>MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications</title>
      <link>https://arxiv.org/abs/2512.01710</link>
      <description>arXiv:2512.01710v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions. Human communication, however, relies on multiple forms of memory, from recalling past conversations to adapting to personal traits and situational context. This paper introduces the Mixed Memory-Augmented Generation (MMAG) pattern, a framework that organizes memory for LLM-based agents into five interacting layers: conversational, long-term user, episodic and event-linked, sensory and context-aware, and short-term working memory. Drawing inspiration from cognitive psychology, we map these layers to technical components and outline strategies for coordination, prioritization, and conflict resolution. We demonstrate the approach through its implementation in the Heero conversational agent, where encrypted long-term bios and conversational history already improve engagement and retention. We further discuss implementation concerns around storage, retrieval, privacy, and latency, and highlight open challenges. MMAG provides a foundation for building memory-rich language agents that are more coherent, proactive, and aligned with human needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01710v1</guid>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefano Zeppieri</dc:creator>
    </item>
    <item>
      <title>LEXA: Legal Case Retrieval via Graph Contrastive Learning with Contextualised LLM Embeddings</title>
      <link>https://arxiv.org/abs/2405.11791</link>
      <description>arXiv:2405.11791v2 Announce Type: replace 
Abstract: Legal case retrieval (LCR) is a specialised information retrieval task aimed at identifying relevant cases given a query case. LCR holds pivotal significance in facilitating legal practitioners to locate legal precedents. Existing LCR methods predominantly rely on traditional lexical models or language models; however, they typically overlook the domain-specific structural information embedded in legal documents. Our previous work CaseGNN successfully harnesses text-attributed graphs and graph neural networks to incorporate structural legal information. Nonetheless, three key challenges remain in enhancing the representational capacity of CaseGNN: (1) The under-utilisation of rich edge information in text-attributed case graph (TACG). (2) The insufficiency of training signals for graph contrastive learning. (3) The lack of contextualised legal information in node and edge features. In this paper, the LEXA model, an extension of CaseGNN, is proposed to overcome these limitations by jointly leveraging rich edge information, enhanced training signals, and contextualised embeddings derived from large language models (LLMs). Specifically, an edge-updated graph attention layer (EUGAT) is proposed to comprehensively update node and edge features during graph modelling, resulting in a full utilisation of structural information of legal cases. Moreover, LEXA incorporates a novel graph contrastive learning objective with graph augmentation to provide additional training signals, thereby strengthening the model's legal comprehension capabilities. What's more, LLMs are employed to generate node and edge features for TACG. Extensive experiments on two benchmark datasets demonstrate that LEXA not only significantly improves CaseGNN but also achieves supreme performance compared to state-of-the-art LCR methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11791v2</guid>
      <category>cs.IR</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanran Tang, Ruihong Qiu, Yilun Liu, Xue Li, Zi Huang</dc:creator>
    </item>
    <item>
      <title>CART: A Generative Cross-Modal Retrieval Framework with Coarse-To-Fine Semantic Modeling</title>
      <link>https://arxiv.org/abs/2406.17507</link>
      <description>arXiv:2406.17507v3 Announce Type: replace 
Abstract: Cross-modal retrieval aims to search for instances, which are semantically related to the query through the interaction of different modal data. Traditional solutions utilize a single-tower or dual-tower framework to explicitly compute the score between queries and candidates, which is challenged by training cost and inference latency with large-scale data. Inspired by the remarkable performance and efficiency of generative models, we propose a generative cross-modal retrieval framework (CART) based on coarse-to-fine semantic modeling, which assigns identifiers to each candidate and treats the generating identifier as the retrieval target. Specifically, we explore an effective coarse-to-fine scheme, combining K-Means and RQ-VAE to discretize multimodal data into token sequences that support autoregressive generation. Further, considering the lack of explicit interaction between queries and candidates, we propose a feature fusion strategy to align their semantics. Extensive experiments demonstrate the effectiveness of the strategies in the CART, achieving excellent results in both retrieval performance and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17507v3</guid>
      <category>cs.IR</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minghui Fang, Shengpeng Ji, Jialong Zuo, Hai Huang, Yan Xia, Jieming Zhu, Xize Cheng, Xiaoda Yang, Wenrui Liu, Gang Wang, Zhenhua Dong, Zhou Zhao</dc:creator>
    </item>
    <item>
      <title>Faster and Memory-Efficient Training of Sequential Recommendation Models for Large Catalogs</title>
      <link>https://arxiv.org/abs/2509.09682</link>
      <description>arXiv:2509.09682v4 Announce Type: replace 
Abstract: Sequential recommendations (SR) with transformer-based architectures are widely adopted in real-world applications, where SR models require frequent retraining to adapt to ever-changing user preferences. However, training transformer-based SR models often encounters a high computational cost associated with scoring extensive item catalogs, often exceeding thousands of items. This occurs mainly due to the use of cross-entropy loss, where peak memory scales proportionally to catalog size, batch size, and sequence length. Recognizing this, practitioners in the field of recommendation systems typically address memory consumption by integrating the cross-entropy (CE) loss with negative sampling, thereby reducing the explicit memory demands of the final layer. However, a small number of negative samples would degrade model performance, and as we demonstrate in our work, increasing the number of negative samples and the batch size further improves the model's performance, but rapidly starts to exceed industrial GPUs' size (~40Gb).
  In this work, we introduce the CCE- method, which offers a GPU-efficient implementation of the CE loss with negative sampling. Our method accelerates training by up to two times while reducing memory consumption by more than 10 times. Leveraging the memory savings afforded by using CCE- for model training, it becomes feasible to enhance its accuracy on datasets with a large item catalog compared to those trained with original PyTorch-implemented loss functions. Finally, we perform an analysis of key memory-related hyperparameters and highlight the necessity of a delicate balance among these factors. We demonstrate that scaling both the number of negative samples and batch size leads to better results rather than maximizing only one of them. To facilitate further adoption of CCE-, we release a Triton kernel that efficiently implements the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09682v4</guid>
      <category>cs.IR</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maxim Zhelnin, Dmitry Redko, Volkov Daniil, Anna Volodkevich, Petr Sokerin, Valeriy Shevchenko, Egor Shvetsov, Alexey Vasilev, Darya Denisova, Ruslan Izmailov, Alexey Zaytsev</dc:creator>
    </item>
    <item>
      <title>Optimizing Product Deduplication in E-Commerce with Multimodal Embeddings</title>
      <link>https://arxiv.org/abs/2509.15858</link>
      <description>arXiv:2509.15858v2 Announce Type: replace 
Abstract: In large scale e-commerce marketplaces, duplicate product listings frequently cause consumer confusion and operational inefficiencies, degrading trust on the platform and increasing costs. Traditional keyword-based search methodologies falter in accurately identifying duplicates due to their reliance on exact textual matches, neglecting semantic similarities inherent in product titles. To address these challenges, we introduce a scalable, multimodal product deduplication designed specifically for the e-commerce domain. Our approach employs a domain-specific text model grounded in BERT architecture in conjunction with MaskedAutoEncoders for image representations. Both of these architectures are augmented with dimensionality reduction techniques to produce compact 128-dimensional embeddings without significant information loss. Complementing this, we also developed a novel decider model that leverages both text and image vectors. By integrating these feature extraction mechanisms with Milvus, an optimized vector database, our system can facilitate efficient and high-precision similarity searches across extensive product catalogs exceeding 200 million items with just 100GB of system RAM consumption. Empirical evaluations demonstrate that our matching system achieves a macro-average F1 score of 0.90, outperforming third-party solutions which attain an F1 score of 0.83. Our findings show the potential of combining domain-specific adaptations with state-of-the-art machine learning techniques to mitigate duplicate listings in large-scale e-commerce environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15858v2</guid>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aysenur Kulunk, Berk Taskin, M. Furkan Eseoglu, H. Bahadir Sahin</dc:creator>
    </item>
    <item>
      <title>HiFIRec Towards High-Frequency yet Low-Intention Behaviors for Multi-Behavior Recommendation</title>
      <link>https://arxiv.org/abs/2509.25755</link>
      <description>arXiv:2509.25755v2 Announce Type: replace 
Abstract: Multi behavior recommendation leverages multiple types of user-item interactions to address data sparsity and cold-start issues,providing personalized services in domains such as healthcare and ecommerce.Most existing methods utilize graph neural networks to model user intention in a unified manner,which inadequately considers the heterogeneity across different behaviors.Especially,high frequency yet low intention behaviors may implicitly contain noisy signals,and frequent patterns that are plausible while misleading,thereby hindering the learning of user intentions.To this end,this paper proposes a novel multi-behavior recommendation method,HiFIRec,that corrects the effect of high-frequency yet low-intention behaviors by differential behavior modeling.To revise the noisy signals,we hierarchically suppress it across layers by extracting neighborhood information through layer-wise neighborhood aggregation and further capturing user intentions through adaptive cross layer feature fusion.To correct plausible frequent patterns,we propose an intensity-aware non-sampling strategy that dynamically adjusts the weights of negative samples.Extensive experiments on two benchmarks show that HiFIRec relatively improves HR@10 by 4.21%-6.81% over several state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25755v2</guid>
      <category>cs.IR</category>
      <category>cs.SI</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruiqi Luo, Ran Jin, Kaixi Hu, Xiaohui Tao, Lin Li</dc:creator>
    </item>
    <item>
      <title>Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2510.20150</link>
      <description>arXiv:2510.20150v3 Announce Type: replace 
Abstract: Large language models (LLMs) are reshaping the recommender system paradigm by enabling users to express preferences and receive recommendations through conversations. Yet, aligning LLMs to the recommendation task remains challenging: pretrained LLMs often generate out-of-catalog items, violate required output formats, and their ranking quality degrades sharply toward the end of the generated list. To this end, we propose ConvRec-R1, a two-stage framework for end-to-end training of LLM-based conversational recommender systems. In Stage 1, we construct a behavioral-cloning dataset with a Remap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded demonstrations from powerful blackbox LLMs to warm-start the RL training. In Stage 2, we propose Rank-GRPO, a principled extension of group relative policy optimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats each rank in the recommendation list as the unit instead of token (too fine-grained) or sequence (too coarse), redefining rewards to remove non-causal credit assignment and introducing a rank-level importance ratio based on the geometric mean of rank-wise token probabilities to stabilize policy updates. Experiments on the public Reddit-v2 dataset show that ConvRec-R1 converges faster and achieves higher Recall and NDCG than GRPO-style baselines. Code and datasets are released at https://github.com/yaochenzhu/Rank-GRPO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20150v3</guid>
      <category>cs.IR</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yaochen Zhu, Harald Steck, Dawen Liang, Yinhan He, Vito Ostuni, Jundong Li, Nathan Kallus</dc:creator>
    </item>
    <item>
      <title>Turning Noise into Value: Uncovering Service Preferences from Ambiguous Interaction in E-commerce</title>
      <link>https://arxiv.org/abs/2511.05808</link>
      <description>arXiv:2511.05808v2 Announce Type: replace 
Abstract: In e-commerce service recommendation, utilizing auxiliary behaviors to alleviate data sparsity often relies on the flawed assumption that auxiliary behaviors that fail to trigger target actions are negative samples. This approach is fundamentally flawed as it ignores false negatives where users actually harbor latent intent or interest but have not yet converted due to external factors. Consequently, existing methods suffer from sample selection bias and a severe distribution shift between the auxiliary and target behaviors, leading to the erroneous suppression of potential user needs. To address these challenges, we propose a Noise-to-Value Adapter (NoVa), an e-commerce service recommendation framework that re-examines the problem through the lens of positive-unlabeled learning. Instead of treating ambiguous auxiliary behaviors as definite negatives, NoVa aims to uncover high-quality preferences from noise via two key mechanisms. First, to bridge the distribution gap, we employ adversarial feature alignment. This module aligns the auxiliary behavior distribution with the target space to identify high-confidence false negatives, which are instances that statistically resemble confirmed target behaviors and thus represent latent conversion intents. Second, to mitigate label noise caused by accidental clicks or random browsing, we introduce a semantic consistency constraint. This mechanism implements semantic-aware filtering based on the content similarity of services, acting as a bias correction step to filter out low-confidence interactions that lack semantic relevance to historical user preferences. Extensive experiments on three real-world datasets demonstrate that NoVa outperforms state-of-the-art baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05808v2</guid>
      <category>cs.IR</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cheng Li, Yong Xu, Suhua Tang, Wenqiang Lin, Xin He, Jinde Cao</dc:creator>
    </item>
    <item>
      <title>Heterogeneous Multi-treatment Uplift Modeling for Trade-off Optimization in Short-Video Recommendation</title>
      <link>https://arxiv.org/abs/2511.18997</link>
      <description>arXiv:2511.18997v2 Announce Type: replace 
Abstract: The rapid proliferation of short videos on social media platforms presents unique challenges and opportunities for recommendation systems. Users exhibit diverse preferences, and the responses resulting from different strategies often conflict with one another, potentially exhibiting inverse correlations between metrics such as watch time and video view counts. Existing uplift models face limitations in handling the heterogeneous multi-treatment scenarios of short-video recommendations, often failing to effectively capture both the synergistic and individual causal effects of different strategies. Furthermore, traditional fixed-weight approaches for balancing these responses lack personalization and can result in biased decision-making. To address these issues, we propose a novel Heterogeneous Multi-treatment Uplift Modeling (HMUM) framework for trade-off optimization in short-video recommendations. HMUM comprises an Offline Hybrid Uplift Modeling (HUM) module, which captures the synergistic and individual effects of multiple strategies, and an Online Dynamic Decision-Making (DDM) module, which estimates the value weights of different user responses in real-time for personalized decision-making. Evaluated on two public datasets, an industrial dataset, and through online A/B experiments on the Kuaishou platform, our model demonstrated superior offline performance and significant improvements in key metrics. It is now fully deployed on the platform, benefiting hundreds of millions of users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18997v2</guid>
      <category>cs.IR</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenhao Zhai, Chang Meng, Xueliang Wang, Shuchang Liu, Xiaolong Hu, Shisong Tang, Xiaoqiang Feng, Xiu Li</dc:creator>
    </item>
    <item>
      <title>Causal Feature Selection Method for Contextual Multi-Armed Bandits in Recommender System</title>
      <link>https://arxiv.org/abs/2409.13888</link>
      <description>arXiv:2409.13888v2 Announce Type: replace-cross 
Abstract: Effective feature selection is essential for optimizing contextual multi-armed bandits (CMABs) in large-scale online systems, where suboptimal features can degrade rewards, interpretability, and efficiency. Traditional feature selection often prioritizes outcome correlation, neglecting the crucial role of heterogeneous treatment effects (HTE) across arms in CMAB decision-making. This paper introduces two novel, model-free filter methods, Heterogeneous Incremental Effect (HIE) and Heterogeneous Distribution Divergence (HDD), specifically designed to identify features driving HTE. HIE quantifies a feature's value based on its ability to induce changes in the optimal arm, while HDD measures its impact on reward distribution divergence across arms. These methods are computationally efficient, robust to model mis-specification, and adaptable to various feature types, making them suitable for rapid screening in dynamic environments where retraining complex models is infeasible. We validate HIE and HDD on synthetic data with known ground truth and in a large-scale commercial recommender system, demonstrating their consistent ability to identify influential HTE features and thereby enhance CMAB performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13888v2</guid>
      <category>cs.LG</category>
      <category>cs.IR</category>
      <category>stat.ML</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenyu Zhao, Yexi Jiang</dc:creator>
    </item>
    <item>
      <title>MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding</title>
      <link>https://arxiv.org/abs/2508.11999</link>
      <description>arXiv:2508.11999v4 Announce Type: replace-cross 
Abstract: With the rapid advancement of e-commerce, exploring general representations rather than task-specific ones has attracted increasing research attention. For product understanding, although existing discriminative dual-flow architectures drive progress in this field, they inherently struggle to model the many-to-one alignment between multiple images and texts of products. Therefore, we argue that generative Multimodal Large Language Models (MLLMs) hold significant potential for improving product representation learning. Nevertheless, achieving this goal still remains non-trivial due to several key challenges: the lack of multimodal and aspect-aware modeling modules in typical LLMs; the common presence of background noise in product images; and the absence of a standard benchmark for evaluation. To address these issues, we propose the first generative MLLM-based model named MOON for product representation learning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for targeted modeling of multimodal and aspect-specific product content; (2) effectively detects core semantic regions in product images to mitigate the distraction and interference caused by background noise; and (3) introduces the specialized negative sampling strategy to increase the difficulty and diversity of negative samples. In addition, we release a large-scale multimodal benchmark MBE for various product understanding tasks. Experimentally, our model demonstrates competitive zero-shot performance on both our benchmark and the public dataset, showcasing strong generalization across various downstream tasks, including cross-modal retrieval, product classification, and attribute prediction. Furthermore, the case study and visualization illustrate the effectiveness of MOON for product understanding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11999v4</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daoze Zhang, Chenghan Fu, Zhanheng Nie, Jianyu Liu, Wanxian Guan, Yuan Gao, Jun Song, Pengjie Wang, Jian Xu, Bo Zheng</dc:creator>
    </item>
    <item>
      <title>From Topology to Retrieval: Decoding Embedding Spaces with Unified Signatures</title>
      <link>https://arxiv.org/abs/2511.22150</link>
      <description>arXiv:2511.22150v2 Announce Type: replace-cross 
Abstract: Studying how embeddings are organized in space not only enhances model interpretability but also uncovers factors that drive downstream task performance. In this paper, we present a comprehensive analysis of topological and geometric measures across a wide set of text embedding models and datasets. We find a high degree of redundancy among these measures and observe that individual metrics often fail to sufficiently differentiate embedding spaces. Building on these insights, we introduce Unified Topological Signatures (UTS), a holistic framework for characterizing embedding spaces. We show that UTS can predict model-specific properties and reveal similarities driven by model architecture. Further, we demonstrate the utility of our method by linking topological structure to ranking effectiveness and accurately predicting document retrievability. We find that a holistic, multi-attribute perspective is essential to understanding and leveraging the geometry of text embeddings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22150v2</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Rottach, William Rudman, Bastian Rieck, Harrisen Scells, Carsten Eickhoff</dc:creator>
    </item>
  </channel>
</rss>
