<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 17 Feb 2025 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Novel Hybrid Approach to Contraceptive Demand Forecasting: Integrating Point Predictions with Probabilistic Distributions</title>
      <link>https://arxiv.org/abs/2502.09685</link>
      <description>arXiv:2502.09685v1 Announce Type: cross 
Abstract: Accurate demand forecasting is vital for ensuring reliable access to contraceptive products, supporting key processes like procurement, inventory, and distribution. However, forecasting contraceptive demand in developing countries presents challenges, including incomplete data, poor data quality, and the need to account for multiple geographical and product factors. Current methods often rely on simple forecasting techniques, which fail to capture demand uncertainties arising from these factors, warranting expert involvement. Our study aims to improve contraceptive demand forecasting by combining probabilistic forecasting methods with expert knowledge. We developed a hybrid model that combines point forecasts from domain-specific model with probabilistic distributions from statistical and machine learning approaches, enabling human input to fine-tune and enhance the system-generated forecasts. This approach helps address the uncertainties in demand and is particularly useful in resource-limited settings. We evaluate different forecasting methods, including time series, Bayesian, machine learning, and foundational time series methods alongside our new hybrid approach. By comparing these methods, we provide insights into their strengths, weaknesses, and computational requirements. Our research fills a gap in forecasting contraceptive demand and offers a practical framework that combines algorithmic and human expertise. Our proposed model can also be generalized to other humanitarian contexts with similar data patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09685v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harsha Chamara Hewage, Bahman Rostami-Tabar, Aris Syntetos, Federico Liberatore, Glenn Milano</dc:creator>
    </item>
    <item>
      <title>Assessing the Accuracy of Multisource Register-based Official Statistics for Multinomial Outcomes</title>
      <link>https://arxiv.org/abs/2502.10182</link>
      <description>arXiv:2502.10182v1 Announce Type: cross 
Abstract: The emergence of new data sources and statistical methods is driving an update in the traditional official statistics paradigm. As an example, the Italian National Institute of Statistics (ISTAT) is undergoing a significant modernisation of the data production process, transitioning from a statistical paradigm based on single sources (census, sample surveys, or administrative data) to an integrated system of statistical registers. The latter results from an integration process of administrative and survey data based on different statistical methods, and, as such, prone to different sources of error. This work discusses and validates a global measure of error assessment for such multisource register-based statistics. Focusing on two important sources of uncertainty (sampling and modelling), we provide an analytical solution that well approximates the global error of mass-imputation procedures for multi-category type of outcomes, assuming a multinomial logistic model. Among other advantages, the proposed measure results in an interpretable, computationally feasible, and flexible approach, while allowing for unplanned on-the-fly statistics on totals to be supported by accuracy estimates. An application to education data from the Base Register of Individuals from ISTAT's integrated system of statistical registers is presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10182v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nina Deliu, Piero Demetrio Falorsi, Stefano Falorsi, Diego Chianella, Giorgio Alleva</dc:creator>
    </item>
    <item>
      <title>Robust variance estimators in application to segmentation of measurement data distorted by impulsive and non-Gaussian noise</title>
      <link>https://arxiv.org/abs/2502.10275</link>
      <description>arXiv:2502.10275v1 Announce Type: cross 
Abstract: The paper algorithmizes the problem of regime change point identification for data measured in a system exhibiting impulsive behaviors. This is a fundamental challenge for annotation of measurement data relevant, e.g., for designing data-driven autonomous systems. The contribution consists in the formulation of an offline robust methodology based on the classical approach for structural break detection. The problem of data segmentation is considered in the context of scale change, which physically can be translated into the occurrence of a critical event that reorganizes the system structure. The main advantage of our approach is that it does not require the existence of a variance of the data distribution. The efficiency has been evaluated for simulated data from two distributions and for real-world datasets measured in financial, mechanical, and medical systems. Simulation studies show that in the most challenging case, the error in estimating regime change is 20 times smaller for robust approach compared to the classical one.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10275v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Justyna Witulska, Anna Zaleska, Natalia Kremzer-Osiadacz, Agnieszka Wy{\l}oma\'nska, Ireneusz Jab{\l}o\'nski</dc:creator>
    </item>
    <item>
      <title>Targeted Quality Measurement of Health Care Providers</title>
      <link>https://arxiv.org/abs/2105.02379</link>
      <description>arXiv:2105.02379v4 Announce Type: replace 
Abstract: Assessing the quality of cancer care administered by US health providers poses numerous challenges due to meaningful heterogeneity in patient populations. Patients undergoing oncology treatment exhibit substantial variation in disease presentation among other crucial characteristics. In this paper, we present a framework for institutional quality measurement that addresses this patient heterogeneity. Our framework follows recent advancements in health outcomes research, conceptualizing quality measurement as a causal inference problem. This approach enables us to use flexible covariate profiles to target specific patient populations of interest. We use different clinically relevant covariate profiles and evaluate methods for case-mix adjustments. These adjustments integrate weighting and regression modeling approaches in a progressive manner in order to reduce model extrapolation and allow for provider effect modification. We evaluate these methods in an extensive simulation study, comparing their performance in terms of point estimates and estimated rankings. We highlight the practical utility of weighting methods that can generate stable weights when covariate overlap is limited and alert investigators when case-mix adjustments are infeasible without some form of extrapolation that goes beyond the support of the observed data. In our study of cancer-care outcomes, we assess the performance of oncology practices for different profiles that correspond to important types of patients who may receive cancer care. We describe how the methods examined may be particularly important for high-stakes quality measurement, such as public reporting or performance-based payments. These methods have the potential to help inform individual patient health care decisions and contribute to progress toward more personalized quality measurement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.02379v4</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yige Li, Nancy L. Keating, Mary Beth Landrum, Jose R. Zubizarreta</dc:creator>
    </item>
    <item>
      <title>Handling multivariable missing data in causal mediation analysis</title>
      <link>https://arxiv.org/abs/2403.17396</link>
      <description>arXiv:2403.17396v2 Announce Type: replace 
Abstract: Mediation analysis is commonly used in epidemiological research, but guidance is lacking on how multivariable missing data should be dealt with in these analyses. Multiple imputation (MI) is a widely used approach, but questions remain regarding impact of missingness mechanism, how to ensure imputation model compatibility and approaches to variance estimation. To address these gaps, we conducted a simulation study based on the Victorian Adolescent Health Cohort Study. We considered six missingness mechanisms, involving varying assumptions regarding the influence of outcome and/or mediator on missingness in key variables. We compared the performance of complete-case analysis, seven MI approaches, differing in how the imputation model was tailored, and a "substantive model compatible" MI approach. We evaluated both the MI-Boot (MI, then bootstrap) and Boot-MI (bootstrap, then MI) approaches to variance estimation. Results showed that when the mediator and/or outcome influenced their own missingness, there was large bias in effect estimates, while for other mechanisms appropriate MI approaches yielded approximately unbiased estimates. Beyond incorporating all analysis variables in the imputation model, how MI was tailored for compatibility with mediation analysis did not greatly impact point estimation bias. BootMI returned variance estimates with smaller bias than MIBoot, especially in the presence of incompatibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17396v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>S. Ghazaleh Dashti, Katherine J. Lee, Julie A. Simpson, John B. Carlin, Margarita Moreno-Betancur</dc:creator>
    </item>
    <item>
      <title>Bootstrapping Fisher Market Equilibrium and First-Price Pacing Equilibrium</title>
      <link>https://arxiv.org/abs/2402.02303</link>
      <description>arXiv:2402.02303v3 Announce Type: replace-cross 
Abstract: The linear Fisher market (LFM) is a basic equilibrium model from economics, which also has applications in fair and efficient resource allocation. First-price pacing equilibrium (FPPE) is a model capturing budget-management mechanisms in first-price auctions. In certain practical settings such as advertising auctions, there is an interest in performing statistical inference over these models. A popular methodology for general statistical inference is the bootstrap procedure. Yet, for LFM and FPPE there is no existing theory for the valid application of bootstrap procedures. In this paper, we introduce and devise several statistically valid bootstrap inference procedures for LFM and FPPE. The most challenging part is to bootstrap general FPPE, which reduces to bootstrapping constrained M-estimators, a largely unexplored problem. We devise a bootstrap procedure for FPPE under mild degeneracy conditions by using the powerful tool of epi-convergence theory. Experiments with synthetic and semi-real data verify our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02303v3</guid>
      <category>math.ST</category>
      <category>cs.GT</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luofeng Liao, Christian Kroer</dc:creator>
    </item>
    <item>
      <title>A non-parametric U-statistic testing approach for multi-arm clinical trials with multivariate longitudinal data</title>
      <link>https://arxiv.org/abs/2408.10149</link>
      <description>arXiv:2408.10149v2 Announce Type: replace-cross 
Abstract: Randomized clinical trials (RCTs) often involve multiple longitudinal primary outcomes to comprehensively assess treatment efficacy. The Longitudinal Rank-Sum Test (LRST), a robust U-statistics-based, non-parametric, rank-based method, effectively controls Type I error and enhances statistical power by leveraging the temporal structure of the data without relying on distributional assumptions. However, the LRST is limited to two-arm comparisons. To address the need for comparing multiple doses against a control group in many RCTs, we extend the LRST to a multi-arm setting. This novel multi-arm LRST provides a flexible and powerful approach for evaluating treatment efficacy across multiple arms and outcomes, with a strong capability for detecting the most effective dose in multi-arm trials. Extensive simulations demonstrate that this method maintains excellent Type I error control while providing greater power compared to the two-arm LRST with multiplicity adjustments. Application to the Bapineuzumab (Bapi) 301 trial further validates the multi-arm LRST's practical utility and robustness, confirming its efficacy in complex clinical trial analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10149v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhrubajyoti Ghosh, Sheng Luo</dc:creator>
    </item>
    <item>
      <title>Error-controlled non-additive interaction discovery in machine learning models</title>
      <link>https://arxiv.org/abs/2408.17016</link>
      <description>arXiv:2408.17016v2 Announce Type: replace-cross 
Abstract: Machine learning (ML) models are powerful tools for detecting complex patterns within data, yet their "black box" nature limits their interpretability, hindering their use in critical domains like healthcare and finance. To address this challenge, interpretable ML methods have been developed to explain how features influence model predictions. However, these methods often focus on univariate feature importance, overlooking the complex interactions between features that ML models are capable of capturing. Recognizing this limitation, recent efforts have aimed to extend these methods to discover feature interactions, but existing approaches struggle with robustness and error control, especially under data perturbations. In this study, we introduce Diamond, a novel method for trustworthy feature interaction discovery. Diamond uniquely integrates the model-X knockoffs framework to control the false discovery rate (FDR), ensuring that the proportion of falsely discovered interactions remains low. A key innovation in Diamond is its non-additivity distillation procedure, which refines existing interaction importance measures to distill non-additive interaction effects, ensuring that FDR control is maintained. This approach addresses the limitations of off-the-shelf interaction measures, which, when used naively, can lead to inaccurate discoveries. Diamond's applicability spans a wide range of ML models, including deep neural networks, transformer models, tree-based models, and factorization-based models. Our empirical evaluations on both simulated and real datasets across various biomedical studies demonstrate Diamond's utility in enabling more reliable data-driven scientific discoveries. This method represents a significant step forward in the deployment of ML models for scientific innovation and hypothesis generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17016v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Winston Chen, Yifan Jiang, William Stafford Noble, Yang Young Lu</dc:creator>
    </item>
    <item>
      <title>Partial membership models for soft clustering of multivariate football player performance data</title>
      <link>https://arxiv.org/abs/2409.01874</link>
      <description>arXiv:2409.01874v2 Announce Type: replace-cross 
Abstract: The standard mixture modeling framework has been widely used to study heterogeneous populations, by modeling them as being composed of a finite number of homogeneous sub-populations. However, the standard mixture model assumes that each data point belongs to one and only one mixture component, or cluster, but when data points have fractional membership in multiple clusters this assumption is unrealistic. It is in fact conceptually very different to represent an observation as partly belonging to multiple groups instead of belonging to one group with uncertainty. For this purpose, various soft clustering approaches, or individual-level mixture models, have been developed. In this context, Heller et al (2008) formulated the Bayesian partial membership model (PM) as an alternative structure for individual-level mixtures, which also captures partial membership in the form of attribute-specific mixtures. Our work proposes using the PM for soft clustering of count data arising in football performance analysis and compares the results with those achieved with the mixed membership model and finite mixture model. Learning and inference are carried out using Markov chain Monte Carlo methods. The method is applied on Serie A football player data from the 2022/2023 football season, to estimate the positions on the field where the players tend to play, in addition to their primary position, based on their playing style. The application of partial membership model to football data could have practical implications for coaches, talent scouts, team managers and analysts. These stakeholders can utilize the findings to make informed decisions related to team strategy, talent acquisition, and statistical research, ultimately enhancing performance and understanding in the field of football.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01874v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emiliano Seri, Roberto Rocci, Thomas Brendan Murphy</dc:creator>
    </item>
    <item>
      <title>Consumer Segmentation and Participation Drivers in Community-Supported Agriculture: A Choice Experiment and PLS-SEM Approach</title>
      <link>https://arxiv.org/abs/2411.00010</link>
      <description>arXiv:2411.00010v2 Announce Type: replace-cross 
Abstract: As the global food system faces increasing challenges from sustainability, climate change, and food security issues, alternative food networks like Community-Supported Agriculture (CSA) play an essential role in fostering stronger connections between consumers and producers. However, understanding consumer engagement with CSA is fragmented, particularly in Japan where CSA participation is still emerging. This study aims to identify potential CSA participants in Japan and validate existing theories on CSA participation through a quantitative analysis of 2,484 Japanese consumers. Using choice experiments, Latent Class Analysis, and Partial Least Squares Structural Equation Modeling, we identified five distinct consumer segments. The "Sustainable Food Seekers" group showed the highest positive utility for CSA, driven primarily by "Food Education and Learning Opportunities" and "Contribution to Environmental and Social Issues." These factors were consistently significant across all segments, suggesting that many Japanese consumers value CSA for its educational and environmental benefits. In contrast, factors related to "Variety of Ingredients" were less influential in determining participation intentions. The findings suggest that promoting CSA in Japan may be most effective by emphasizing its role in environmental and social impact, rather than focusing solely on product attributes like organic certification, which is readily available in supermarkets. This reflects a key distinction between CSA adoption in Japan and in other cultural contexts, where access to organic produce is a primary driver. For "Sustainable Food Seekers," CSA offers a way to contribute to broader societal goals rather than just securing organic products.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00010v2</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sota Takagi, Miki Saijo, Takumi Ohashi</dc:creator>
    </item>
    <item>
      <title>Spherical Double K-Means: a co-clustering approach for text data analysis</title>
      <link>https://arxiv.org/abs/2501.04562</link>
      <description>arXiv:2501.04562v2 Announce Type: replace-cross 
Abstract: In text analysis, Spherical K-means (SKM) is a specialized k-means clustering algorithm widely utilized for grouping documents represented in high-dimensional, sparse term-document matrices, often normalized using techniques like TF-IDF. Researchers frequently seek to cluster not only documents but also the terms associated with them into coherent groups. To address this dual clustering requirement, we introduce Spherical Double K-Means (SDKM), a novel methodology that simultaneously clusters documents and terms. This approach offers several advantages: first, by integrating the clustering of documents and terms, SDKM provides deeper insights into the relationships between content and vocabulary, enabling more effective topic identification and keyword extraction. Additionally, the two-level clustering assists in understanding both overarching themes and specific terminologies within document clusters, enhancing interpretability. SDKM effectively handles the high dimensionality and sparsity inherent in text data by utilizing cosine similarity, leading to improved computational efficiency. Moreover, the method captures dynamic changes in thematic content over time, making it well-suited for applications in rapidly evolving fields. Ultimately, SDKM presents a comprehensive framework for advancing text mining efforts, facilitating the uncovering of nuanced patterns and structures that are critical for robust data analysis. We apply SDKM to the corpus of US presidential inaugural addresses, spanning from George Washington in 1789 to Joe Biden in 2021. Our analysis reveals distinct clusters of words and documents that correspond to significant historical themes and periods, showcasing the method's ability to facilitate a deeper understanding of the data. Our findings demonstrate the efficacy of SDKM in uncovering underlying patterns in textual data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04562v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilaria Bombelli, Domenica Fioredistella Iezzi, Emiliano Seri, Maurizio Vichi</dc:creator>
    </item>
  </channel>
</rss>
