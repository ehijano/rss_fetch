<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Nov 2025 02:40:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Mechanizing a Proof-Relevant Logical Relation for Timed Message-Passing Protocols</title>
      <link>https://arxiv.org/abs/2511.19521</link>
      <description>arXiv:2511.19521v1 Announce Type: new 
Abstract: Semantic typing has become a powerful tool for program verification, applying the technique of logical relations as not only a proof method, but also a device for prescribing program behavior. In recent work, Yao et al. scaled semantic typing to the verification of timed message-passing protocols, which are prevalent in, e.g., IoT and real-time systems applications. The appeal of semantic typing in this context is precisely because of its ability to support typed and untyped program components alike -- including physical objects -- which caters to the heterogeneity of these applications. Another demand inherent to these applications is timing: constraining the time or time window within which a message exchange must happen. Yao et al. equipped their logical relation not only with temporal predicates, but also with computable trajectories, to supply the evidence that an inhabitant can step from one time point to another one. While Yao et al. provide the formalization for such a verification tool, it lacks a mechanization. Mechanizing the system would not only provide a machine proof for it, but also facilitate scalability for future extensions and applications.
  This paper tackles the challenge of mechanizing the resulting proof-relevant logical relation in a proof assistant. allowing trajectories to be interleaved, partitioned, and concatenated, while the intended equality on trajectories is the equality of their graphs when seen as processes indexed by time. Unfortunately, proof assistants based on intensional type theory only have modest support for such equations, forcing a prolific use of transports. This paper reports on the process of mechanizing Yao et al.'s results, comprising the logical relation, the algebra of computable trajectories with supporting lemmas, and the fundamental theorem of the logical relation, in the Rocq theorem prover.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19521v1</guid>
      <category>cs.PL</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tesla Zhang, Asher Kornfeld, Rui Li, Sonya Simkin, Yue Yao, Stephanie Balzer</dc:creator>
    </item>
    <item>
      <title>Understanding Accelerator Compilers via Performance Profiling</title>
      <link>https://arxiv.org/abs/2511.19764</link>
      <description>arXiv:2511.19764v1 Announce Type: new 
Abstract: Accelerator design languages (ADLs), high-level languages that compile to hardware units, help domain experts quickly design efficient application-specific hardware. ADL compilers optimize datapaths and convert software-like control flow constructs into control paths. Such compilers are necessarily complex and often unpredictable: they must bridge the wide semantic gap between high-level semantics and cycle-level schedules, and they typically rely on advanced heuristics to optimize circuits. The resulting performance can be difficult to control, requiring guesswork to find and resolve performance problems in the generated hardware. We conjecture that ADL compilers will never be perfect: some performance unpredictability is endemic to the problem they solve.
  In lieu of compiler perfection, we argue for compiler understanding tools that give ADL programmers insight into how the compiler's decisions affect performance. We introduce Petal, a cycle-level Petal for the Calyx intermediate language (IL). Petal instruments the Calyx code with probes and then analyzes the trace from a register-transfer-level simulation. It maps the events in the trace back to high-level control constructs in the Calyx code to track the clock cycles when each construct was active. Using case studies, we demonstrate that Petal's cycle-level profiles can identify performance problems in existing accelerator designs. We show that these insights can also guide developers toward optimizations that the compiler was unable to perform automatically, including a reduction by 46.9\% of total cycles for one application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19764v1</guid>
      <category>cs.PL</category>
      <category>cs.AR</category>
      <category>cs.SE</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ayaka Yorihiro, Griffin Berlstein, Pedro Pontes Garc\'ia, Kevin Laeufer, Adrian Sampson</dc:creator>
    </item>
    <item>
      <title>The Ghosts of Empires: Extracting Modularity from Interleaving-Based Proofs (Extended Version)</title>
      <link>https://arxiv.org/abs/2511.20369</link>
      <description>arXiv:2511.20369v1 Announce Type: new 
Abstract: Implementation bugs threaten the soundness of algorithmic software verifiers. Generating correctness certificates for correct programs allows for efficient independent validation of verification results, and thus helps to reveal such bugs. Automatic generation of small, compact correctness proofs for concurrent programs is challenging, as the correctness arguments may depend on the particular interleaving, which can lead to exponential explosion. We present an approach that converts an interleaving-based correctness proof, as generated by many algorithmic verifiers, into a thread-modular correctness proof in the style of Owicki and Gries. We automatically synthesize ghost variables that capture the relevant interleaving information, and abstract away irrelevant details. Our evaluation shows that the approach is efficient in practice and generates compact proofs, compared to a baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20369v1</guid>
      <category>cs.PL</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3776684</arxiv:DOI>
      <dc:creator>Frank Sch\"ussele, Matthias Zumkeller, Miriam Lagunes-Rochin, Dominik Klumpp</dc:creator>
    </item>
    <item>
      <title>DiverseClaire: Simulating Students to Improve Introductory Programming Course Materials for All CS1 Learners</title>
      <link>https://arxiv.org/abs/2511.14198</link>
      <description>arXiv:2511.14198v1 Announce Type: cross 
Abstract: Although CS programs are booming, introductory courses like CS1 still adopt a one-size-fits-all formats that can exacerbate cognitive load and discourage learners with autism, ADHD, dyslexia and other neurological conditions. These call for compassionate pedagogies and Universal Design For Learning (UDL) to create learning environments and materials where cognitive diversity is welcomed. To address this, we introduce DiverseClaire a pilot study, which simulates students including neurodiverse profiles using LLMs and diverse personas. By leveraging Bloom's Taxonomy and UDL, DiverseClaire compared UDL-transformed lecture slides with traditional formats. To evaluate DiverseClaire controlled experiments, we used the evaluation metric the average score. The findings revealed that the simulated neurodiverse students struggled with learning due to lecture slides that were in inaccessible formats. These results highlight the need to provide course materials in multiple formats for diverse learner preferences. Data from our pilot study will be made available to assist future CS1 instructors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14198v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.PL</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wendy Wong, Yuchao Jiang, Yuekang Li</dc:creator>
    </item>
    <item>
      <title>CrypTorch: PyTorch-based Auto-tuning Compiler for Machine Learning with Multi-party Computation</title>
      <link>https://arxiv.org/abs/2511.19711</link>
      <description>arXiv:2511.19711v1 Announce Type: cross 
Abstract: Machine learning (ML) involves private data and proprietary model parameters. MPC-based ML allows multiple parties to collaboratively run an ML workload without sharing their private data or model parameters using multi-party computing (MPC). Because MPC cannot natively run ML operations such as Softmax or GELU, existing frameworks use different approximations. Our study shows that, on a well-optimized framework, these approximations often become the dominating bottleneck. Popular approximations are often insufficiently accurate or unnecessarily slow, and these issues are hard to identify and fix in existing frameworks. To tackle this issue, we propose a compiler for MPC-based ML, CrypTorch. CrypTorch disentangles these approximations with the rest of the MPC runtime, allows easily adding new approximations through its programming interface, and automatically selects approximations to maximize both performance and accuracy. Built as an extension to PyTorch 2's compiler, we show that CrypTorch's auto-tuning alone provides 1.20--1.7$\times$ immediate speedup without sacrificing accuracy, and 1.31--1.8$\times$ speedup when some accuracy degradation is allowed, compared to our well-optimized baseline. Combined with better engineering and adoption of state-of-the-art practices, the entire framework brings 3.22--8.6$\times$ end-to-end speedup compared to the popular framework, CrypTen.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19711v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jinyu Liu, Gang Tan, Kiwan Maeng</dc:creator>
    </item>
    <item>
      <title>QiMeng-CRUX: Narrowing the Gap between Natural Language and Verilog via Core Refined Understanding eXpression</title>
      <link>https://arxiv.org/abs/2511.20099</link>
      <description>arXiv:2511.20099v2 Announce Type: cross 
Abstract: Large language models (LLMs) have shown promising capabilities in hardware description language (HDL) generation. However, existing approaches often rely on free-form natural language descriptions that are often ambiguous, redundant, and unstructured, which poses significant challenges for downstream Verilog code generation. We treat hardware code generation as a complex transformation from an open-ended natural language space to a domain-specific, highly constrained target space. To bridge this gap, we introduce Core Refined Understanding eXpression (CRUX), a structured intermediate space that captures the essential semantics of user intent while organizing the expression for precise Verilog code generation. We further design a two-stage training framework, comprising Joint Expression Modeling and Dual-Space Optimization, to enhance the quality of both CRUX and Verilog code. Experiments across multiple Verilog generation benchmarks demonstrate that our model, CRUX-V, achieves state-of-the-art performance among general models, particularly under challenging design tasks. Furthermore, the CRUX space proves transferable and beneficial when used as input prompts for other code models, highlighting its effectiveness in narrowing the gap between free-form natural language descriptions and precise Verilog generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20099v2</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <category>cs.PL</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Huang, Rui Zhang, Jiaming Guo, Yang Zhang, Di Huang, Shuyao Cheng, Pengwei Jin, Chongxiao Li, Zidong Du, Xing Hu, Qi Guo, Yunji Chen</dc:creator>
    </item>
    <item>
      <title>Separating the Wheat from the Chaff: Understanding (In-)Completeness of Proof Mechanisms for Separation Logic with Inductive Definitions</title>
      <link>https://arxiv.org/abs/2511.20193</link>
      <description>arXiv:2511.20193v1 Announce Type: cross 
Abstract: For over two decades Separation Logic has been arguably the most popular framework for reasoning about heap-manipulating programs, as well as reasoning about shared resources and permissions. Separation Logic is often extended to include inductively-defined predicates, interpreted as least fixpoints, forming Separation Logic with Inductive Definitions (SLID). Many theoretical and practical advances have been made in developing automated proof mechanisms for SLID, but these mechanisms are imperfect, and a deeper understanding of their failures is desired. As expressive as Separation Logic is, it is not surprising that it is incomplete, and in fact, it contains several sources of incompleteness that defy automated reasoning.
  In this paper we study these sources of incompleteness and how they relate to failures of proof mechanisms. We place SLID within a larger logic, that we call Weak Separation Logic (WSL). We prove that unlike SLID, WSL is complete for a non-trivial fragment of quantified entailments with background theories and inductive definitions, via a reduction to first-order logic (FOL). Moreover, we show that the ubiquitous fold/unfold proof mechanism is sound and complete for theory-free, quantifier-free WSL entailments with inductive definitions. Through this, we understand proof failures as stemming from nonstandard models present in WSL, but not allowed in SLID. These rogue models are typically infinite, and we use the formalism of symbolic structures to represent and automatically find them.
  We present a prototype tool that implements the FOL encoding of WSL and test it on an existing benchmark, which contains over 700 quantified entailment problems with inductive definitions. Our tool is able to find counter-models to many of the examples, and we provide a partial taxonomy of the rogue models, shedding some light on real-world proof failures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20193v1</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3776671</arxiv:DOI>
      <dc:creator>Neta Elad, Adithya Murali, Sharon Shoham</dc:creator>
    </item>
    <item>
      <title>Translating Large-Scale C Repositories to Idiomatic Rust</title>
      <link>https://arxiv.org/abs/2511.20617</link>
      <description>arXiv:2511.20617v1 Announce Type: cross 
Abstract: Existing C to Rust translation techniques fail to balance quality and scalability: transpilation-based approaches scale to large projects but produce code with poor safety, idiomaticity, and readability. In contrast, LLM-based techniques are prohibitively expensive due to their reliance on frontier models (without which they cannot reliably generate compilable translations), thus limiting scalability. This paper proposes Rustine, a fully automated pipeline for effective and efficient repository-level C to idiomatic safe Rust translation. Evaluating on a diverse set of 23 C programs, ranging from 27 to 13,200 lines of code, Rustine can generate fully compilable Rust code for all and achieve 87% functional equivalence (passing 1,063,099 assertions out of 1,221,192 in test suites with average function and line coverage of 74.7% and 72.2%). Compared to six prior repository-level C to Rust translation techniques, the translations by Rustine are overall safer (fewer raw pointers, pointer arithmetic, and unsafe constructs), more idiomatic (fewer Rust linter violations), and more readable. When the translations cannot pass all tests to fulfill functional equivalence, human developers were able to complete the task in 4.5 hours, on average, using Rustine as debugging support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20617v1</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saman Dehghan (University of Illinois at Urbana-Champaign, USA), Tianran Sun (Shanghai Jiao Tong University, China), Tianxiang Wu (University of Illinois at Urbana-Champaign, USA), Zihan Li (University of Illinois at Urbana-Champaign, USA), Reyhaneh Jabbarvand (University of Illinois at Urbana-Champaign, USA)</dc:creator>
    </item>
    <item>
      <title>Soteria: Efficient Symbolic Execution as a Functional Library</title>
      <link>https://arxiv.org/abs/2511.08729</link>
      <description>arXiv:2511.08729v4 Announce Type: replace 
Abstract: Symbolic execution (SE) tools often rely on intermediate languages (ILs) to support multiple programming languages, promising reusability and efficiency. In practice, this approach introduces trade-offs between performance, accuracy, and language feature support. We argue that building SE engines \emph{directly} for each source language is both simpler and more effective. We present Soteria, a lightweight OCaml library for writing SE engines in a functional style, without compromising on performance, accuracy or feature support. Soteria enables developers to construct SE engines that operate directly over source-language semantics, offering \emph{configurability}, compositional reasoning, and ease of implementation. Using Soteria, we develop Soteria$^{\text{Rust}}$, the \emph{first} Rust SE engine supporting Tree Borrows (the intricate aliasing model of Rust), and Soteria$^{\text{C}}$, a compositional SE engine for C. Both tools are competitive with or outperform state-of-the-art tools such as Kani, Pulse, CBMC and Gillian-C in performance and the number of bugs detected. We formalise the theoretical foundations of Soteria and prove its soundness, demonstrating that sound, efficient, accurate, and expressive SE can be achieved without the compromises of ILs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08729v4</guid>
      <category>cs.PL</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sacha-\'Elie Ayoun, Opale Sj\"ostedt, Azalea Raad</dc:creator>
    </item>
    <item>
      <title>Lazy Linearity for a Core Functional Language</title>
      <link>https://arxiv.org/abs/2511.10361</link>
      <description>arXiv:2511.10361v2 Announce Type: replace 
Abstract: Traditionally, in linearly typed languages, consuming a linear resource is synonymous with its syntactic occurrence in the program. However, under the lens of non-strict evaluation, linearity can be further understood semantically, where a syntactic occurrence of a resource does not necessarily entail using that resource when the program is executed. While this distinction has been largely unexplored, it turns out to be inescapable in Haskell's optimising compiler, which heavily rewrites the source program in ways that break syntactic linearity but preserve the program's semantics. We introduce Linear Core, a novel system which accepts the lazy semantics of linearity statically and is suitable for lazy languages such as the Core intermediate language of the Glasgow Haskell Compiler. We prove that Linear Core is sound, guaranteeing linear resource usage, and that multiple optimising transformations preserve linearity in Linear Core while failing to do so in Core. We have implemented Linear Core as a compiler plugin to validate the system against linearity-heavy libraries, including linear-base.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10361v2</guid>
      <category>cs.PL</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3776711</arxiv:DOI>
      <dc:creator>Rodrigo Mesquita, Bernardo Toninho</dc:creator>
    </item>
    <item>
      <title>Data Race Detection by Digest-Driven Abstract Interpretation (Extended Version)</title>
      <link>https://arxiv.org/abs/2511.11055</link>
      <description>arXiv:2511.11055v2 Announce Type: replace 
Abstract: Sound static analysis can prove the absence of data races by establishing that no two conflicting memory accesses can occur at the same time. We repurpose the concept of digests -- summaries of computational histories originally introduced to bring tunable concurrency-sensitivity to thread-modular value analysis by abstract interpretation, extending this idea to race detection: We use digests to capture the conditions under which conflicting accesses may not happen in parallel. To formalize this, we give a definition of data races in the thread-modular local trace semantics and show how exclusion criteria for potential conflicts can be expressed as digests. We report on our implementation of digest-driven data race detection in the static analyzer Goblint, and evaluate it on the SV-COMP benchmark suite. Combining the lockset digest with digests reasoning on thread ids and thread joins increases the number of correctly solved tasks by more than a factor of five compared to lockset reasoning alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11055v2</guid>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Schwarz, Julian Erhard</dc:creator>
    </item>
    <item>
      <title>Probabilistic Concurrent Reasoning in Outcome Logic: Independence, Conditioning, and Invariants</title>
      <link>https://arxiv.org/abs/2411.11662</link>
      <description>arXiv:2411.11662v3 Announce Type: replace-cross 
Abstract: Although randomization has long been used in distributed computing, formal methods for reasoning about probabilistic concurrent programs have lagged behind. No existing program logics can express specifications about the full distributions of outcomes resulting from programs that are both probabilistic and concurrent. To address this, we introduce Probabilistic Concurrent Outcome Logic (pcOL), which incorporates ideas from concurrent and probabilistic separation logics into Outcome Logic to introduce new compositional reasoning principles. At its core, pcOL reinterprets the rules of Concurrent Separation Logic in a setting where separation models probabilistic independence, so as to compositionally describe joint distributions over variables in concurrent threads. Reasoning about outcomes also proves crucial, as case analysis is often necessary to derive precise information about threads that rely on randomized shared state. We demonstrate pcOL on a variety of examples, including to prove almost sure termination of unbounded loops.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11662v3</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3776651</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the ACM on Programming Languages (PACMPL). Volume 10, Issue POPL, Article 9 (January 2026)</arxiv:journal_reference>
      <dc:creator>Noam Zilberstein, Alexandra Silva, Joseph Tassarotti</dc:creator>
    </item>
    <item>
      <title>Hadamard-Pi: Equational Quantum Programming</title>
      <link>https://arxiv.org/abs/2506.06835</link>
      <description>arXiv:2506.06835v2 Announce Type: replace-cross 
Abstract: Quantum computing offers advantages over classical computation, yet the precise features that set the two apart remain unclear. In the standard quantum circuit model, adding a 1-qubit basis-changing gate -- commonly chosen to be the Hadamard gate -- to a universal set of classical reversible gates yields computationally universal quantum computation. However, the computational behaviours enabled by this addition are not fully characterised. We give such a characterisation by introducing a small quantum programming language extending the universal classical reversible programming language $\Pi$ with a single primitive corresponding to the Hadamard gate. The language comes equipped with a sound and complete categorical semantics that is specified by a purely equational theory. Completeness is shown by means of a novel finite presentation, and a corresponding synthesis algorithm, for the groups of orthogonal matrices with entries in the ring $\mathbb{Z}[\tfrac{1}{\sqrt{2}}]$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06835v2</guid>
      <category>quant-ph</category>
      <category>cs.PL</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3776647</arxiv:DOI>
      <arxiv:journal_reference>Proc. ACM Program. Lang. 10, POPL, Article 5 (January 2026), 27 pages</arxiv:journal_reference>
      <dc:creator>Wang Fang, Chris Heunen, Robin Kaarsgaard</dc:creator>
    </item>
    <item>
      <title>Weighted Automata for Exact Inference in Discrete Probabilistic Programs</title>
      <link>https://arxiv.org/abs/2509.15074</link>
      <description>arXiv:2509.15074v2 Announce Type: replace-cross 
Abstract: In probabilistic programming, the inference problem asks to determine a program's posterior distribution conditioned on its "observe" instructions. Inference is challenging, especially when exact rather than approximate results are required. Inspired by recent work on probability generating functions (PGFs), we propose encoding distributions on $\mathbb{N}^k$ as weighted automata over a commutative alphabet with $k$ symbols. Based on this, we map the semantics of various imperative programming statements to automata-theoretic constructions. For a rich class of programs, this results in an effective translation from prior to posterior distribution, both encoded as automata. We prove that our approach is sound with respect to a standard operational program semantics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15074v2</guid>
      <category>cs.FL</category>
      <category>cs.PL</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dominik Gei{\ss}ler, Tobias Winkler</dc:creator>
    </item>
  </channel>
</rss>
