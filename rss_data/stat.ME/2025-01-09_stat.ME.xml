<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 09 Jan 2025 05:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Bridging Impulse Control of Piecewise Deterministic Markov Processes and Markov Decision Processes: Frameworks, Extensions, and Open Challenges</title>
      <link>https://arxiv.org/abs/2501.04120</link>
      <description>arXiv:2501.04120v1 Announce Type: new 
Abstract: Control theory plays a pivotal role in understanding and optimizing the behavior of complex dynamical systems across various scientific and engineering disciplines. Two key frameworks that have emerged for modeling and solving control problems in stochastic systems are piecewise deterministic Markov processes (PDMPs) and Markov decision processes (MDPs). Each framework has its unique strengths, and their intersection offers promising opportunities for tackling a broad class of problems, particularly in the context of impulse controls and decision-making in complex systems.
  The relationship between PDMPs and MDPs is a natural subject of exploration, as embedding impulse control problems for PDMPs into the MDP framework could open new avenues for their analysis and resolution. Specifically, this integration would allow leveraging the computational and theoretical tools developed for MDPs to address the challenges inherent in PDMPs. On the other hand, PDMPs can offer a versatile and simple paradigm to model continuous time problems that are often described as discrete-time MDPs parametrized by complex transition kernels. This transformation has the potential to bridge the gap between the two frameworks, enabling solutions to previously intractable problems and expanding the scope of both fields. This paper presents a comprehensive review of two research domains, illustrated through a recurring medical example. The example is revisited and progressively formalized within the framework of thevarious concepts and objects introduced</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04120v1</guid>
      <category>stat.ME</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alice Cleynen, Beno\^ite de Saporta, Orlane Rossini, R\'egis Sabbadin, Am\'elie Vernay</dc:creator>
    </item>
    <item>
      <title>Estimating Propensities of Selection for Big Datasets via Data Integration</title>
      <link>https://arxiv.org/abs/2501.04185</link>
      <description>arXiv:2501.04185v1 Announce Type: new 
Abstract: Big data presents potential but unresolved value as a source for analysis and inference. However,selection bias, present in many of these datasets, needs to be accounted for so that appropriate inferences can be made on the target population. One way of approaching the selection bias issue is to first estimate the propensity of inclusion in the big dataset for each member of the big dataset, and then to apply these propensities in an inverse probability weighting approach to produce population estimates. In this paper, we provide details of a new variant of existing propensity score estimation methods that takes advantage of the ability to integrate the big data with a probability sample. We compare the ability of this method to produce efficient inferences for the target population with several alternative methods through an empirical study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04185v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lyndon Ang, Robert Clark, Bronwyn Loong, Anders Holmberg</dc:creator>
    </item>
    <item>
      <title>Modeling Hypergraphs with Diversity and Heterogeneous Popularity</title>
      <link>https://arxiv.org/abs/2501.04251</link>
      <description>arXiv:2501.04251v1 Announce Type: new 
Abstract: While relations among individuals make an important part of data with scientific and business interests, existing statistical modeling of relational data has mainly been focusing on dyadic relations, i.e., those between two individuals. This article addresses the less studied, though commonly encountered, polyadic relations that can involve more than two individuals. In particular, we propose a new latent space model for hypergraphs using determinantal point processes, which is driven by the diversity within hyperedges and each node's popularity. This model mechanism is in contrast to existing hypergraph models, which are predominantly driven by similarity rather than diversity. Additionally, the proposed model accommodates broad types of hypergraphs, with no restriction on the cardinality and multiplicity of hyperedges, which previous models often have. Consistency and asymptotic normality of the maximum likelihood estimates of the model parameters have been established. The proof is challenging, owing to the special configuration of the parameter space. Further, we apply the projected accelerated gradient descent algorithm to obtain the parameter estimates, and we show its effectiveness in simulation studies. We also demonstrate an application of the proposed model on the What's Cooking data and present the embedding of food ingredients learned from cooking recipes using the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04251v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xianshi Yu, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>Inside Out: Externalizing Assumptions in Data Analysis as Validation Checks</title>
      <link>https://arxiv.org/abs/2501.04296</link>
      <description>arXiv:2501.04296v1 Announce Type: new 
Abstract: In data analysis, unexpected results often prompt researchers to revisit their procedures to identify potential issues. While some researchers may struggle to identify the root causes, experienced researchers can often quickly diagnose problems by checking a few key assumptions. These checked assumptions, or expectations, are typically informal, difficult to trace, and rarely discussed in publications. In this paper, we introduce the term *analysis validation checks* to formalize and externalize these informal assumptions. We then introduce a procedure to identify a subset of checks that best predict the occurrence of unexpected outcomes, based on simulations of the original data. The checks are evaluated in terms of accuracy, determined by binary classification metrics, and independence, which measures the shared information among checks. We demonstrate this approach with a toy example using step count data and a generalized linear model example examining the effect of particulate matter air pollution on daily mortality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04296v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>H. Sherry Zhang, Roger D. Peng</dc:creator>
    </item>
    <item>
      <title>Neural Parameter Estimation with Incomplete Data</title>
      <link>https://arxiv.org/abs/2501.04330</link>
      <description>arXiv:2501.04330v1 Announce Type: new 
Abstract: Advancements in artificial intelligence (AI) and deep learning have led to neural networks being used to generate lightning-speed answers to complex questions, to paint like Monet, or to write like Proust. Leveraging their computational speed and flexibility, neural networks are also being used to facilitate fast, likelihood-free statistical inference. However, it is not straightforward to use neural networks with data that for various reasons are incomplete, which precludes their use in many applications. A recently proposed approach to remedy this issue inputs an appropriately padded data vector and a vector that encodes the missingness pattern to a neural network. While computationally efficient, this "masking" approach can result in statistically inefficient inferences. Here, we propose an alternative approach that is based on the Monte Carlo expectation-maximization (EM) algorithm. Our EM approach is likelihood-free, substantially faster than the conventional EM algorithm as it does not require numerical optimization at each iteration, and more statistically efficient than the masking approach. This research represents a prototype problem that indicates how improvements could be made in AI by introducing Bayesian statistical thinking. We compare the two approaches to missingness using simulated incomplete data from two models: a spatial Gaussian process model, and a spatial Potts model. The utility of the methodology is shown on Arctic sea-ice data and cryptocurrency data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04330v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Sainsbury-Dale, Andrew Zammit-Mangion, Noel Cressie, Rapha\"el Huser</dc:creator>
    </item>
    <item>
      <title>An algorithm for a constrained P-spline</title>
      <link>https://arxiv.org/abs/2501.04335</link>
      <description>arXiv:2501.04335v1 Announce Type: new 
Abstract: Regression splines are largely used to investigate and predict data behavior, attracting the interest of mathematicians for their beautiful numerical properties, and of statisticians for their versatility with respect to the applications. Several penalized spline regression models are available in the literature, and the most commonly used ones in real-world applications are P-splines, which enjoy the advantages of penalized models while being easy to generalize across different functional spaces and higher degree order, because of their discrete penalty term. To face the different requirements imposed by the nature of the problem or the physical meaning of the expected values, the P-spline definition is often modified by additional hypotheses, often translated into constraints on the solution or its derivatives. In this framework, our work is motivated by the aim of getting approximation models that fall within pre-established thresholds. Specifically, starting from a set of observed data, we consider a P-spline constrained between some prefixed bounds. In our paper, we just consider 0 as lower bound, although our approach applies to more general cases. We propose to get nonnegativity by imposing lower bounds on selected sample points. The spline can be computed through a sequence of linearly constrained problems. We suggest a strategy to dynamically select the sample points, to avoid extremely dense sampling, and therefore try to reduce as much as possible the computational burden. We show through some computational experiments the reliability of our approach and the accuracy of the results compared to some state-of-the-art models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04335v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rosanna Campagna, Serena Crisci, Gabriele Santin, Gerardo Toraldo, Marco Viola</dc:creator>
    </item>
    <item>
      <title>ART: Distribution-Free and Model-Agnostic Changepoint Detection with Finite-Sample Guarantees</title>
      <link>https://arxiv.org/abs/2501.04475</link>
      <description>arXiv:2501.04475v1 Announce Type: new 
Abstract: We introduce ART, a distribution-free and model-agnostic framework for changepoint detection that provides finite-sample guarantees. ART transforms independent observations into real-valued scores via a symmetric function, ensuring exchangeability in the absence of changepoints. These scores are then ranked and aggregated to detect distributional changes. The resulting test offers exact Type-I error control, agnostic to specific distributional or model assumptions. Moreover, ART seamlessly extends to multi-scale settings, enabling robust multiple changepoint estimation and post-detection inference with finite-sample error rate control. By locally ranking the scores and performing aggregations across multiple prespecified intervals, ART identifies changepoint intervals and refines subsequent inference while maintaining its distribution-free and model-agnostic nature. This adaptability makes ART as a reliable and versatile tool for modern changepoint analysis, particularly in high-dimensional data contexts and applications leveraging machine learning methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04475v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaolong Cui, Haoyu Geng, Guanghui Wang, Zhaojun Wang, Changliang Zou</dc:creator>
    </item>
    <item>
      <title>Spherical Double K-Means: a co-clustering approach for text data analysis</title>
      <link>https://arxiv.org/abs/2501.04562</link>
      <description>arXiv:2501.04562v1 Announce Type: new 
Abstract: In this study, we introduce the Spherical Double K-Means (SDKM) clustering method for text data. A novel approach for simultaneous clustering of terms and documents. Using the strengths of k-means, double k-means, and spherical k-means, SDKM addresses the challenges of high dimensionality, noise, and sparsity inherent in text analysis. We address the choice of the number of clusters, both for the words and documents, using the cluster validity index pseudo-F, and verify the reliability of the method through simulation studies.
  We apply SDKM to the corpus of US presidential inaugural addresses, spanning from George Washington in 1789 to Joe Biden in 2021. Our analysis reveals distinct clusters of words and documents that correspond to significant historical themes and periods, showcasing the method's ability to facilitate a deeper understanding of the data. Our findings demonstrate the efficacy of SDKM in uncovering underlying patterns in textual data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04562v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emiliano Seri, Ilaria Bombelli, Domenica Fioredistella Iezzi, Maurizio Vichi</dc:creator>
    </item>
    <item>
      <title>Modeling temporal dependence in a sequence of spatial random partitions driven by spanning tree: an application to mosquito-borne diseases</title>
      <link>https://arxiv.org/abs/2501.04601</link>
      <description>arXiv:2501.04601v1 Announce Type: new 
Abstract: Spatially constrained clustering is an important field of research, particularly when it involves changes over time. Partitioning a map is not simple since there is a vast number of possible partitions within the search space. In spatio-temporal clustering, this task becomes even more difficult, as we must consider sequences of partitions. Motivated by these challenges, we introduce a Bayesian model for time-dependent sequences of spatial random partitions by proposing a prior distribution based on product partition models that correlates partitions. Additionally, we employ random spanning trees to facilitate the exploration of the partition search space and to guarantee spatially constrained clustering. This work is motivated by a relevant applied problem: identifying spatial and temporal patterns of mosquito-borne diseases. Given the overdispersion present in this type of data, we introduce a spatio-temporal Poisson mixture model in which mean and dispersion parameters vary according to spatio-temporal covariates. The proposed model is applied to analyze the number of dengue cases reported weekly from 2018 to 2023 in the Southeast region of Brazil. We also evaluate model performance using simulated data. Overall, the proposed model has proven to be a competitive approach for analyzing the temporal evolution of spatial clustering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04601v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jessica Pavani, Rosangela Helena Loschi, Fernando Andres Quintana</dc:creator>
    </item>
    <item>
      <title>Doubly Robust and Efficient Calibration of Prediction Sets for Censored Time-to-Event Outcomes</title>
      <link>https://arxiv.org/abs/2501.04615</link>
      <description>arXiv:2501.04615v1 Announce Type: new 
Abstract: Our objective is to construct well-calibrated prediction sets for a time-to-event outcome subject to right-censoring with guaranteed coverage. Our approach is inspired by modern conformal inference literature, in that, unlike classical frameworks, we obviate the need for a well-specified parametric or semi-parametric survival model to accomplish our goal. In contrast to existing conformal prediction methods for survival data, which restrict censoring to be of Type I, whereby potential censoring times are assumed to be fully observed on all units in both training and validation samples, we consider the more common right-censoring setting in which either only the censoring time or only the event time of primary interest is directly observed, whichever comes first. Under a standard conditional independence assumption between the potential survival and censoring times given covariates, we propose and analyze two methods to construct valid and efficient lower predictive bounds for the survival time of a future observation. The proposed methods build upon modern semiparametric efficiency theory for censored data, in that the first approach incorporates inverse-probability-of-censoring weighting (IPCW), while the second approach is based on augmented-inverse-probability-of-censoring weighting (AIPCW). For both methods, we formally establish asymptotic coverage guarantees, and demonstrate both via theory and empirical experiments that AIPCW substantially improves efficiency over IPCW in the sense that its coverage error bound is of second-order mixed bias type, that is \emph{doubly robust}, and therefore guaranteed to be asymptotically negligible relative to the coverage error of IPCW.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04615v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rebecca Farina, Arun Kumar Kuchibhotla, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Multiple testing in multi-stream sequential change detection</title>
      <link>https://arxiv.org/abs/2501.04130</link>
      <description>arXiv:2501.04130v1 Announce Type: cross 
Abstract: Multi-stream sequential change detection involves simultaneously monitoring many streams of data and trying to detect when their distributions change, if at all. Here, we theoretically study multiple testing issues that arise from detecting changes in many streams. We point out that any algorithm with finite average run length (ARL) must have a trivial worst-case false detection rate (FDR), family-wise error rate (FWER), and per-family error rate (PFER); thus, any attempt to control these Type I error metrics is fundamentally in conflict with the desire for a finite ARL. One of our contributions is to define a new class of metrics which can be controlled, called error over patience (EOP). We propose algorithms that combine the recent e-detector framework (which generalizes the Shiryaev-Roberts and CUSUM methods) with the recent e-Benjamini-Hochberg procedure and e-Bonferroni procedures. We prove that these algorithms control the EOP at any desired level under very general dependence structures on the data within and across the streams. In fact, we prove a more general error control that holds uniformly over all stopping times and provides a smooth trade-off between the conflicting metrics. Additionally, if finiteness of the ARL is forfeited, we show that our algorithms control the Type I error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04130v1</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjit Dandapanthula, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Totally Concave Regression</title>
      <link>https://arxiv.org/abs/2501.04360</link>
      <description>arXiv:2501.04360v1 Announce Type: cross 
Abstract: Shape constraints offer compelling advantages in nonparametric regression by enabling the estimation of regression functions under realistic assumptions, devoid of tuning parameters. However, most existing shape-constrained nonparametric regression methods, except additive models, impose too few restrictions on the regression functions. This often leads to suboptimal performance, such as overfitting, in multivariate contexts due to the curse of dimensionality. On the other hand, additive shape-constrained models are sometimes too restrictive because they fail to capture interactions among the covariates. In this paper, we introduce a novel approach for multivariate shape-constrained nonparametric regression, which allows interactions without suffering from the curse of dimensionality. Our approach is based on the notion of total concavity originally due to T. Popoviciu and recently described in Gal [24]. We discuss the characterization and computation of the least squares estimator over the class of totally concave functions and derive rates of convergence under standard assumptions. The rates of convergence depend on the number of covariates only logarithmically, and the estimator, therefore, is guaranteed to avoid the curse of dimensionality to some extent. We demonstrate that total concavity can be justified for many real-world examples and validate the efficacy of our approach through empirical studies on various real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04360v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dohyeong Ki, Adityanand Guntuboyina</dc:creator>
    </item>
    <item>
      <title>Choosing the Right Norm for Change Point Detection in Functional Data</title>
      <link>https://arxiv.org/abs/2501.04476</link>
      <description>arXiv:2501.04476v1 Announce Type: cross 
Abstract: We consider the problem of detecting a change point in a sequence of mean functions from a functional time series. We propose an $L^1$ norm based methodology and establish its theoretical validity both for classical and for relevant hypotheses. We compare the proposed method with currently available methodology that is based on the $L^2$ and supremum norms. Additionally we investigate the asymptotic behaviour under the alternative for all three methods and showcase both theoretically and empirically that the $L^1$ norm achieves the best performance in a broad range of scenarios. We also propose a power enhancement component that improves the performance of the $L^1$ test against sparse alternatives. Finally we apply the proposed methodology to both synthetic and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04476v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Bastian</dc:creator>
    </item>
    <item>
      <title>Some models are useful, but for how long?: A decision theoretic approach to choosing when to refit large-scale prediction models</title>
      <link>https://arxiv.org/abs/2405.13926</link>
      <description>arXiv:2405.13926v2 Announce Type: replace 
Abstract: Large-scale prediction models using tools from artificial intelligence (AI) or machine learning (ML) are increasingly common across a variety of industries and scientific domains. Despite their effectiveness, training AI and ML tools at scale can cost tens or hundreds of thousands of dollars (or more); and even after a model is trained, substantial resources must be invested to keep models up-to-date. This paper presents a decision-theoretic framework for deciding when to refit an AI/ML model when the goal is to perform unbiased statistical inference using partially AI/ML-generated data. Drawing on portfolio optimization theory, we treat the decision of {\it recalibrating} a model or statistical inference versus {\it refitting} the model as a choice between ``investing'' in one of two ``assets.'' One asset, recalibrating the model based on another model, is quick and relatively inexpensive but bears uncertainty from sampling and may not be robust to model drift. The other asset, {\it refitting} the model, is costly but removes the drift concern (though not statistical uncertainty from sampling). We present a framework for balancing these two potential investments while preserving statistical validity. We evaluate the framework using simulation and data on electricity usage and predicting flu trends.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13926v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kentaro Hoffman, Stephen Salerno, Jeff Leek, Tyler McCormick</dc:creator>
    </item>
    <item>
      <title>A Direct Importance Sampling-based Framework for Rare Event Uncertainty Quantification in Non-Gaussian Spaces</title>
      <link>https://arxiv.org/abs/2405.14149</link>
      <description>arXiv:2405.14149v2 Announce Type: replace 
Abstract: This work introduces a novel framework for precisely and efficiently estimating rare event probabilities in complex, high-dimensional non-Gaussian spaces, building on our foundational Approximate Sampling Target with Post-processing Adjustment (ASTPA) approach. An unnormalized sampling target is first constructed and sampled, relaxing the optimal importance sampling distribution and appropriately designed for non-Gaussian spaces. Post-sampling, its normalizing constant is estimated using a stable inverse importance sampling procedure, employing an importance sampling density based on the already available samples. The sought probability is then computed based on the estimates evaluated in these two stages. The proposed estimator is theoretically analyzed, proving its unbiasedness and deriving its analytical coefficient of variation. To sample the constructed target, we resort to our developed Quasi-Newton mass preconditioned Hamiltonian MCMC (QNp-HMCMC) and we prove that it converges to the correct stationary target distribution. To avoid the challenging task of tuning the trajectory length in complex spaces, QNp-HMCMC is effectively utilized in this work with a single-step integration. We thus show the equivalence of QNp-HMCMC with single-step implementation to a unique and efficient preconditioned Metropolis-adjusted Langevin algorithm (MALA). An optimization approach is also leveraged to initiate QNp-HMCMC effectively, and the implementation of the developed framework in bounded spaces is eventually discussed. A series of diverse problems involving high dimensionality (several hundred inputs), strong nonlinearity, and non-Gaussianity is presented, showcasing the capabilities and efficiency of the suggested framework and demonstrating its advantages compared to relevant state-of-the-art sampling methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14149v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elsayed Eshra, Konstantinos G. Papakonstantinou, Hamed Nikbakht</dc:creator>
    </item>
    <item>
      <title>Asymptotic Inference for Multi-Stage Stationary Treatment Policy with Variable Selection</title>
      <link>https://arxiv.org/abs/2301.12553</link>
      <description>arXiv:2301.12553v3 Announce Type: replace-cross 
Abstract: Dynamic treatment regimes or policies are a sequence of decision functions over multiple stages that are tailored to individual features. One important class of treatment policies in practice, namely multi-stage stationary treatment policies, prescribes treatment assignment probabilities using the same decision function across stages, where the decision is based on the same set of features consisting of time-evolving variables (e.g., routinely collected disease biomarkers). Although there has been extensive literature on constructing valid inference for the value function associated with dynamic treatment policies, little work has focused on the policies themselves, especially in the presence of high-dimensional feature variables. We aim to fill the gap in this work. Specifically, we first estimate the multi-stage stationary treatment policy using an augmented inverse probability weighted estimator for the value function to increase asymptotic efficiency, and further apply a penalty to select important feature variables. We then construct one-step improvements of the policy parameter estimators for valid inference. Theoretically, we show that the improved estimators are asymptotically normal, even if nuisance parameters are estimated at a slow convergence rate and the dimension of the feature variables increases with the sample size. Our numerical studies demonstrate that the proposed method estimates a sparse policy with a near-optimal value function and conducts valid inference for the policy parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.12553v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daiqi Gao, Yufeng Liu, Donglin Zeng</dc:creator>
    </item>
    <item>
      <title>Simulating Relational Event Histories: Why and How</title>
      <link>https://arxiv.org/abs/2403.19329</link>
      <description>arXiv:2403.19329v4 Announce Type: replace-cross 
Abstract: Many important social phenomena are characterized by repeated interactions among individuals over time such as email exchanges in an organization or face-to-face interactions in a classroom. To understand the underlying mechanisms of social interaction dynamics, statistical simulation techniques of longitudinal network data on a fine temporal granularity are crucially important. This paper makes two contributions to the field. First, we present statistical frameworks to simulate relational event networks under dyadic and actor-oriented relational event models which are implemented in a new R package 'remulate'. Second, we explain how the simulation framework can be used to address challenging problems in temporal social network analysis, such as model fit assessment, theory building, network intervention planning, making predictions, understanding the impact of network structures, to name a few. This is shown in three extensive case studies. In the first study, it is elaborated why simulation-based techniques are crucial for relational event model assessment which is illustrated for a network of criminal gangs. In the second study, it is shown how simulation techniques are important when building and extending theories about social phenomena which is illustrated via optimal distinctiveness theory. In the third study, we demonstrate how simulation techniques contribute to a better understanding of the longevity and the potential effect sizes of network interventions. Through these case studies and software, researchers will be able to better understand social interaction dynamics using relational event data from real-life networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19329v4</guid>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rumana Lakdawala, Joris Mulder, Roger Leenders</dc:creator>
    </item>
  </channel>
</rss>
