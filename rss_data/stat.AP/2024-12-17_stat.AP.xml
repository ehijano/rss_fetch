<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Dec 2024 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>CESAR: A Convolutional Echo State AutoencodeR for High-Resolution Wind Forecasting</title>
      <link>https://arxiv.org/abs/2412.10578</link>
      <description>arXiv:2412.10578v1 Announce Type: new 
Abstract: An accurate and timely assessment of wind speed and energy output allows an efficient planning and management of this resource on the power grid. Wind energy, especially at high resolution, calls for the development of nonlinear statistical models able to capture complex dependencies in space and time. This work introduces a Convolutional Echo State AutoencodeR (CESAR), a spatio-temporal, neural network-based model which first extracts the spatial features with a deep convolutional autoencoder, and then models their dynamics with an echo state network. We also propose a two-step approach to also allow for computationally affordable inference, while also performing uncertainty quantification. We focus on a high-resolution simulation in Riyadh (Saudi Arabia), an area where wind farm planning is currently ongoing, and show how CESAR is able to provide improved forecasting of wind speed and power for proposed building sites by up to 17% against the best alternative methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10578v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Bonas, Paolo Giani, Paola Crippa, Stefano Castruccio</dc:creator>
    </item>
    <item>
      <title>Statistical Problems in the Diagnosis of Shaken Baby Syndrome/Abusive Head Trauma: Limitations to Algorithms and the Need for Reliable Data</title>
      <link>https://arxiv.org/abs/2412.10648</link>
      <description>arXiv:2412.10648v1 Announce Type: new 
Abstract: The medical and legal controversy surrounding the diagnosis of Shaken Baby Syndrome/Abusive Head Trauma (SBS/AHT) raises critical questions about its scientific foundation and reliability. This article argues that SBS/AHT can only be understood by studying the statistical challenges with the data. Current health records are insufficient because there is a lack of ground truth, reliance on circular reasoning, contextual bias, heterogeneity across institutions, and integration of legal decisions into medical assessments. There exists no comprehensive source of legal data. Thus, current data is insufficient to reliably distinguish SBS/AHT from other medical conditions or accidental injuries. A privately-collected medico-legal dataset that has the relevant contextual information, but is limited by being a convenience sample, is used to show how a data analysis might be performed with higher-quality data. There is a need for systematic data collection of the additional contextual information used by physicians and pathologists to make determinations of abuse. Furthermore, because of the legal nature of the diagnosis, i.e., its accuracy, repeatability, and reproducibility, must be tested. Better data and evaluating the scientific validity of SBS/AHT are essential to protect vulnerable children while ensuring fairness and accuracy in legal proceedings involving allegations of abuse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10648v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Cuellar</dc:creator>
    </item>
    <item>
      <title>Efficient Bayesian inversion for simultaneous estimation of geometry and spatial field using the Karhunen-Lo\`eve expansion</title>
      <link>https://arxiv.org/abs/2412.11610</link>
      <description>arXiv:2412.11610v1 Announce Type: new 
Abstract: Detection of abrupt spatial changes in physical properties representing unique geometric features such as buried objects, cavities, and fractures is an important problem in geophysics and many engineering disciplines. In this context, simultaneous spatial field and geometry estimation methods that explicitly parameterize the background spatial field and the geometry of the embedded anomalies are of great interest. This paper introduces an advanced inversion procedure for simultaneous estimation using the domain independence property of the Karhunen-Lo\`eve (K-L) expansion. Previous methods pursuing this strategy face significant computational challenges. The associated integral eigenvalue problem (IEVP) needs to be solved repeatedly on evolving domains, and the shape derivatives in gradient-based algorithms require costly computations of the Moore-Penrose inverse. Leveraging the domain independence property of the K-L expansion, the proposed method avoids both of these bottlenecks, and the IEVP is solved only once on a fixed bounding domain. Comparative studies demonstrate that our approach yields two orders of magnitude improvement in K-L expansion gradient computation time. Inversion studies on one-dimensional and two-dimensional seepage flow problems highlight the benefits of incorporating geometry parameters along with spatial field parameters. The proposed method captures abrupt changes in hydraulic conductivity with a lower number of parameters and provides accurate estimates of boundary and spatial-field uncertainties, outperforming spatial-field-only estimation methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11610v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuya Shibata, Michael Conrad Koch, Iason Papaioannou, Kazunori Fujisawa</dc:creator>
    </item>
    <item>
      <title>Conformal Prediction for Astronomy Data with Measurement Error</title>
      <link>https://arxiv.org/abs/2412.10544</link>
      <description>arXiv:2412.10544v1 Announce Type: cross 
Abstract: Astronomers often deal with data where the covariates and the dependent variable are measured with heteroscedastic non-Gaussian error. For instance, while TESS and Kepler datasets provide a wealth of information, addressing the challenges of measurement errors and systematic biases is critical for extracting reliable scientific insights and improving machine learning models' performance. Although techniques have been developed for estimating regression parameters for these data, few techniques exist to construct prediction intervals with finite sample coverage guarantees. To address this issue, we tailor the conformal prediction approach to our application. We empirically demonstrate that this method gives finite sample control over Type I error probabilities under a variety of assumptions on the measurement errors in the observed data. Further, we demonstrate how the conformal prediction method could be used for constructing prediction intervals for unobserved exoplanet masses using established broken power-law relationships between masses and radii found in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10544v1</guid>
      <category>astro-ph.IM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naomi Giertych, Jonathan P Williams, Sujit Ghosh</dc:creator>
    </item>
    <item>
      <title>Cardiovascular Disease Detection By Leveraging Semi-Supervised Learning</title>
      <link>https://arxiv.org/abs/2412.10567</link>
      <description>arXiv:2412.10567v1 Announce Type: cross 
Abstract: Cardiovascular disease (CVD) persists as a primary cause of death on a global scale, which requires more effective and timely detection methods. Traditional supervised learning approaches for CVD detection rely heavily on large-labeled datasets, which are often difficult to obtain. This paper employs semi-supervised learning models to boost efficiency and accuracy of CVD detection when there are few labeled samples. By leveraging both labeled and vast amounts of unlabeled data, our approach demonstrates improvements in prediction performance, while reducing the dependency on labeled data. Experimental results in a publicly available dataset show that semi-supervised models outperform traditional supervised learning techniques, providing an intriguing approach for the initial identification of cardiovascular disease within clinical environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10567v1</guid>
      <category>q-bio.QM</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaohan Chen, Zheyan Liu, Huili Zheng, Qimin Zhang, Yiru Gong</dc:creator>
    </item>
    <item>
      <title>Balancing Accuracy and Costs in Cross-Temporal Hierarchies: Investigating Decision-Based and Validation-Based Reconciliation</title>
      <link>https://arxiv.org/abs/2412.11153</link>
      <description>arXiv:2412.11153v1 Announce Type: cross 
Abstract: Wind power forecasting is essential for managing daily operations at wind farms and enabling market operators to manage power uncertainty effectively in demand planning. This paper explores advanced cross-temporal forecasting models and their potential to enhance forecasting accuracy. First, we propose a novel approach that leverages validation errors, rather than traditional in-sample errors, for covariance matrix estimation and forecast reconciliation. Second, we introduce decision-based aggregation levels for forecasting and reconciliation where certain horizons are based on the required decisions in practice. Third, we evaluate the forecasting performance of the models not only on their ability to minimize errors but also on their effectiveness in reducing decision costs, such as penalties in ancillary services. Our results show that statistical-based hierarchies tend to adopt less conservative forecasts and reduce revenue losses. On the other hand, decision-based reconciliation offers a more balanced compromise between accuracy and decision cost, making them attractive for practical use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11153v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahdi Abolghasemi, Daniele Girolimetto, Tommaso Di Fonzo</dc:creator>
    </item>
    <item>
      <title>Missing data imputation for noisy time-series data and applications in healthcare</title>
      <link>https://arxiv.org/abs/2412.11164</link>
      <description>arXiv:2412.11164v1 Announce Type: cross 
Abstract: Healthcare time series data is vital for monitoring patient activity but often contains noise and missing values due to various reasons such as sensor errors or data interruptions. Imputation, i.e., filling in the missing values, is a common way to deal with this issue. In this study, we compare imputation methods, including Multiple Imputation with Random Forest (MICE-RF) and advanced deep learning approaches (SAITS, BRITS, Transformer) for noisy, missing time series data in terms of MAE, F1-score, AUC, and MCC, across missing data rates (10 % - 80 %). Our results show that MICE-RF can effectively impute missing data compared to deep learning methods and the improvement in classification of data imputed indicates that imputation can have denoising effects. Therefore, using an imputation algorithm on time series with missing data can, at the same time, offer denoising effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11164v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lien P. Le, Xuan-Hien Nguyen Thi, Thu Nguyen, Michael A. Riegler, P{\aa}l Halvorsen, Binh T. Nguyen</dc:creator>
    </item>
    <item>
      <title>P3LS: Point Process Partial Least Squares</title>
      <link>https://arxiv.org/abs/2412.11267</link>
      <description>arXiv:2412.11267v1 Announce Type: cross 
Abstract: Many studies collect data that can be considered as a realization of a point process. Included are medical imaging data where photon counts are recorded by a gamma camera from patients being injected with a gamma emitting tracer. It is of interest to develop analytic methods that can help with diagnosis as well as in the training of inexpert radiologists. Partial least squares (PLS) is a popular analytic approach that combines features from linear modeling as well as dimension reduction to provide parsimonious prediction and classification. However, existing PLS methodologies do not include the analysis of point process predictors. In this article, we introduce point process PLS (P3LS) for analyzing latent time-varying intensity functions from collections of inhomogeneous point processes. A novel estimation procedure for $P^3LS$ is developed that utilizes the properties of log-Gaussian Cox processes, and its empirical properties are examined in simulation studies. The method is used to analyze kidney functionality in patients with renal disease in order to aid in the diagnosis of kidney obstruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11267v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jamshid Namdari, Robert T Krafty, Amita K Manatunga</dc:creator>
    </item>
    <item>
      <title>Spatial Cross-Recurrence Quantification Analysis for Multi-Platform Contact Tracing and Epidemiology Research</title>
      <link>https://arxiv.org/abs/2412.11326</link>
      <description>arXiv:2412.11326v1 Announce Type: cross 
Abstract: Contact tracing is an essential tool in slowing and containing outbreaks of contagious diseases. Current contact tracing methods range from interviews with public health personnel to Bluetooth pings from smartphones. While all methods offer various benefits, it is difficult for different methods to integrate with one another. Additionally, for contact tracing mobile applications, data privacy is a concern to many as GPS data from users is saved to either a central server or the user's device. The current paper describes a method called spatial cross-recurrence quantification analysis (SpaRQ) that can combine and analyze contact tracing data, regardless of how it has been obtained, and generate a risk profile for the user without storing GPS data. Furthermore, the plots from SpaRQ can be used to investigate the nature of the infectious agent, such as how long it can remain viable in air or on surfaces after an infected person has passed, the chance of infection based on exposure time, and what type of exposure is maximally infective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11326v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>K. J. Patten</dc:creator>
    </item>
    <item>
      <title>Chopin: An Open Source R-language Tool to Support Spatial Analysis on Parallelizable Infrastructure</title>
      <link>https://arxiv.org/abs/2412.11355</link>
      <description>arXiv:2412.11355v1 Announce Type: cross 
Abstract: An increasing volume of studies utilize geocomputation methods in large spatial data. There is a bottleneck in scalable computation for general scientific use as the existing solutions require high-performance computing domain knowledge and are tailored for specific use cases. This study presents an R package `chopin` to reduce the technical burden for parallelization in geocomputation. Supporting popular spatial analysis packages in R, `chopin` leverages parallel computing by partitioning data that are involved in a computation task. The partitioning is implemented at regular grids, data hierarchies, and multiple file inputs with flexible input types for interoperability between different packages and efficiency. This approach makes the geospatial covariate calculation to the scale of the available processing power in a wide range of computing assets from laptop computers to high-performance computing infrastructure. Testing use cases in environmental exposure assessment demonstrated that the package reduced the execution time by order of processing units used. The work is expected to provide broader research communities using geospatial data with an efficient tool to process large scale data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11355v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Insang Song, Kyle P. Messier</dc:creator>
    </item>
    <item>
      <title>But Can You Use It? Design Recommendations for Differentially Private Interactive Systems</title>
      <link>https://arxiv.org/abs/2412.11794</link>
      <description>arXiv:2412.11794v1 Announce Type: cross 
Abstract: Accessing data collected by federal statistical agencies is essential for public policy research and improving evidence-based decision making, such as evaluating the effectiveness of social programs, understanding demographic shifts, or addressing public health challenges. Differentially private interactive systems, or validation servers, can form a crucial part of the data-sharing infrastructure. They may allow researchers to query targeted statistics, providing flexible, efficient access to specific insights, reducing the need for broad data releases and supporting timely, focused research. However, they have not yet been practically implemented. While substantial theoretical work has been conducted on the privacy and accuracy guarantees of differentially private mechanisms, prior efforts have not considered usability as an explicit goal of interactive systems. This work outlines and considers the barriers to developing differentially private interactive systems for informing public policy and offers an alternative way forward. We propose balancing three design considerations: privacy assurance, statistical utility, and system usability, we develop recommendations for making differentially private interactive systems work in practice, we present an example architecture based on these recommendations, and we provide an outline of how to conduct the necessary user-testing. Our work seeks to move the practical development of differentially private interactive systems forward to better aid public policy making and spark future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11794v1</guid>
      <category>cs.HC</category>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liudas Panavas, Joshua Snoke, Erika Tyagi, Claire McKay Bowen, Aaron R. Williams</dc:creator>
    </item>
    <item>
      <title>Bayesian Model Calibration and Sensitivity Analysis for Oscillating Biological Experiments</title>
      <link>https://arxiv.org/abs/2110.10604</link>
      <description>arXiv:2110.10604v4 Announce Type: replace 
Abstract: Understanding the oscillating behaviors that govern organisms' internal biological processes requires interdisciplinary efforts combining both biological and computer experiments, as the latter can complement the former by simulating perturbed conditions with higher resolution. Harmonizing the two types of experiment, however, poses significant statistical challenges due to identifiability issues, numerical instability, and ill behavior in high dimension. This article devises a new Bayesian calibration framework for oscillating biochemical models. The proposed Bayesian model is estimated relying on an advanced Markov chain Monte Carlo (MCMC) technique which can efficiently infer the parameter values that match the simulated and observed oscillatory processes. Also proposed is an approach to sensitivity analysis based on the intervention posterior. This approach measures the influence of individual parameters on the target process by using the obtained MCMC samples as a computational tool. The proposed framework is illustrated with circadian oscillations observed in a filamentous fungus, Neurospora crassa.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.10604v4</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youngdeok Hwang, Hang J. Kim, Won Chang, Christian Hong, Steven N. MacEachern</dc:creator>
    </item>
    <item>
      <title>Estimating Treatment Effect Heterogeneity in Psychiatry: A Review and Tutorial with Causal Forests</title>
      <link>https://arxiv.org/abs/2409.01578</link>
      <description>arXiv:2409.01578v2 Announce Type: replace 
Abstract: Flexible machine learning tools are increasingly used to estimate heterogeneous treatment effects. This paper gives an accessible tutorial demonstrating the use of the causal forest algorithm, available in the R package grf. We start with a brief non-technical overview of treatment effect estimation methods, focusing on estimation in observational studies; the same techniques can also be applied in experimental studies. We then discuss the logic of estimating heterogeneous effects using the extension of the random forest algorithm implemented in grf. Finally, we illustrate causal forest by conducting a secondary analysis on the extent to which individual differences in resilience to high combat stress can be measured among US Army soldiers deploying to Afghanistan based on information about these soldiers available prior to deployment. We illustrate simple and interpretable exercises for model selection and evaluation, including targeting operator characteristics curves, Qini curves, area-under-the-curve summaries, and best linear projections. A replication script with simulated data is available at https://github.com/grf-labs/grf/tree/master/experiments/ijmpr</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01578v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erik Sverdrup, Maria Petukhova, Stefan Wager</dc:creator>
    </item>
    <item>
      <title>BioKlustering: a web app for semi-supervised learning of maximally imbalanced genomic data</title>
      <link>https://arxiv.org/abs/2209.11730</link>
      <description>arXiv:2209.11730v3 Announce Type: replace-cross 
Abstract: Summary: Accurate phenotype prediction from genomic sequences is a highly coveted task in biological and medical research. While machine-learning holds the key to accurate prediction in a variety of fields, the complexity of biological data can render many methodologies inapplicable. We introduce BioKlustering, a user-friendly open-source and publicly available web app for unsupervised and semi-supervised learning specialized for cases when sequence alignment and/or experimental phenotyping of all classes are not possible. Among its main advantages, BioKlustering 1) allows for maximally imbalanced settings of partially observed labels including cases when only one class is observed, which is currently prohibited in most semi-supervised methods, 2) takes unaligned sequences as input and thus, allows learning for widely diverse sequences (impossible to align) such as virus and bacteria, 3) is easy to use for anyone with little or no programming expertise, and 4) works well with small sample sizes.
  Availability and Implementation: BioKlustering (https://bioklustering.wid.wisc.edu) is a freely available web app implemented with Django, a Python-based framework, with all major browsers supported. The web app does not need any installation, and it is publicly available and open-source (https://github.com/solislemuslab/bioklustering).</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.11730v3</guid>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Ozminkowski, Yuke Wu, Hailey Bruzzone, Liule Yang, Zhiwen Xu, Luke Selberg, Chunrong Huang, Helena Jaramillo-Mesa, Claudia Solis-Lemus</dc:creator>
    </item>
    <item>
      <title>Estimating Conditional Average Treatment Effects with Heteroscedasticity by Model Averaging and Matching</title>
      <link>https://arxiv.org/abs/2304.06960</link>
      <description>arXiv:2304.06960v2 Announce Type: replace-cross 
Abstract: We propose a model averaging approach, combined with a partition and matching method to estimate the conditional average treatment effects under heteroskedastic error settings. The proposed approach has asymptotic optimality and consistency of weights and estimator. Numerical studies show that our method has good finite-sample performances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.06960v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.econlet.2024.111679</arxiv:DOI>
      <arxiv:journal_reference>Economics Letters, 2024, 238: 111679</arxiv:journal_reference>
      <dc:creator>Pengfei Shi, Xinyu Zhang, Wei Zhong</dc:creator>
    </item>
    <item>
      <title>Doubly regularized generalized linear models for spatial observations with high-dimensional covariates</title>
      <link>https://arxiv.org/abs/2401.15793</link>
      <description>arXiv:2401.15793v2 Announce Type: replace-cross 
Abstract: A discrete spatial lattice can be cast as a network structure over which spatially-correlated outcomes are observed. A second network structure may also capture similarities among measured features, when such information is available. Incorporating the network structures when analyzing such doubly-structured data can improve predictive power, and lead to better identification of important features in the data-generating process. Motivated by applications in spatial disease mapping, we develop a new doubly regularized regression framework to incorporate these network structures for analyzing high-dimensional datasets. Our estimators can be easily implemented with standard convex optimization algorithms. In addition, we describe a procedure to obtain asymptotically valid confidence intervals and hypothesis tests for our model parameters. We show empirically that our framework provides improved predictive accuracy and inferential power compared to existing high-dimensional spatial methods. These advantages hold given fully accurate network information, and also with networks which are partially misspecified or uninformative. The application of the proposed method to modeling COVID-19 mortality data suggests that it can improve prediction of deaths beyond standard spatial models, and that it selects relevant covariates more often.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15793v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arjun Sondhi, Si Cheng, Ali Shojaie</dc:creator>
    </item>
    <item>
      <title>Multidimensional Deconvolution with Profiling</title>
      <link>https://arxiv.org/abs/2409.10421</link>
      <description>arXiv:2409.10421v2 Announce Type: replace-cross 
Abstract: In many experimental contexts, it is necessary to statistically remove the impact of instrumental effects in order to physically interpret measurements. This task has been extensively studied in particle physics, where the deconvolution task is called unfolding. A number of recent methods have shown how to perform high-dimensional, unbinned unfolding using machine learning. However, one of the assumptions in all of these methods is that the detector response is correctly modeled in the Monte Carlo simulation. In practice, the detector response depends on a number of nuisance parameters that can be constrained with data. We propose a new algorithm called Profile OmniFold, which works in a similar iterative manner as the OmniFold algorithm while being able to simultaneously profile the nuisance parameters. We illustrate the method with a Gaussian example as a proof of concept highlighting its promising capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10421v2</guid>
      <category>hep-ph</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huanbiao Zhu, Krish Desai, Mikael Kuusela, Vinicius Mikuni, Benjamin Nachman, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>GeoConformal prediction: a model-agnostic framework of measuring the uncertainty of spatial prediction</title>
      <link>https://arxiv.org/abs/2412.08661</link>
      <description>arXiv:2412.08661v2 Announce Type: replace-cross 
Abstract: Spatial prediction is a fundamental task in geography. In recent years, with advances in geospatial artificial intelligence (GeoAI), numerous models have been developed to improve the accuracy of geographic variable predictions. Beyond achieving higher accuracy, it is equally important to obtain predictions with uncertainty measures to enhance model credibility and support responsible spatial prediction. Although geostatistic methods like Kriging offer some level of uncertainty assessment, such as Kriging variance, these measurements are not always accurate and lack general applicability to other spatial models. To address this issue, we propose a model-agnostic uncertainty assessment method called GeoConformal Prediction, which incorporates geographical weighting into conformal prediction. We applied it to two classic spatial prediction cases, spatial regression and spatial interpolation, to evaluate its reliability. First, in the spatial regression case, we used XGBoost to predict housing prices, followed by GeoConformal to calculate uncertainty. Our results show that GeoConformal achieved a coverage rate of 93.67%, while Bootstrap methods only reached a maximum coverage of 81.00% after 2000 runs. Next, we applied GeoConformal to spatial interpolation models. We found that the uncertainty obtained from GeoConformal aligned closely with the variance in Kriging. Finally, using GeoConformal, we analyzed the sources of uncertainty in spatial prediction. We found that explicitly including local features in AI models can significantly reduce prediction uncertainty, especially in areas with strong local dependence. Our findings suggest that GeoConformal holds potential not only for geographic knowledge discovery but also for guiding the design of future GeoAI models, paving the way for more reliable and interpretable spatial prediction frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08661v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiayin Lou, Peng Luo, Liqiu Meng</dc:creator>
    </item>
  </channel>
</rss>
