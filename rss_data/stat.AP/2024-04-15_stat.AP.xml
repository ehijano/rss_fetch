<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Apr 2024 04:00:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 15 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Shifting the Paradigm: Estimating Heterogeneous Treatment Effects in the Development of Walkable Cities Design</title>
      <link>https://arxiv.org/abs/2404.08208</link>
      <description>arXiv:2404.08208v1 Announce Type: new 
Abstract: The transformation of urban environments to accommodate growing populations has profoundly impacted public health and well-being. This paper addresses the critical challenge of estimating the impact of urban design interventions on diverse populations. Traditional approaches, reliant on questionnaires and stated preference techniques, are limited by recall bias and capturing the complex dynamics between environmental attributes and individual characteristics. To address these challenges, we integrate Virtual Reality (VR) with observational causal inference methods to estimate heterogeneous treatment effects, specifically employing Targeted Maximum Likelihood Estimation (TMLE) for its robustness against model misspecification. Our innovative approach leverages VR-based experiment to collect data that reflects perceptual and experiential factors. The result shows the heterogeneous impacts of urban design elements on public health and underscore the necessity for personalized urban design interventions. This study not only extends the application of TMLE to built environment research but also informs public health policy by illuminating the nuanced effects of urban design on mental well-being and advocating for tailored strategies that foster equitable, health-promoting urban spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08208v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jie Zhu, Bojing Liao, Theo A. Arentze</dc:creator>
    </item>
    <item>
      <title>Estimating Breakpoints between Climate States in Paleoclimate Data</title>
      <link>https://arxiv.org/abs/2404.08336</link>
      <description>arXiv:2404.08336v1 Announce Type: new 
Abstract: This study presents a statistical approach for identifying transitions between climate states, referred to as breakpoints, using well-established econometric tools. We analyse a 67.1 million year record of the oxygen isotope ratio delta-O-18 derived from benthic foraminifera. The dataset is presented in Westerhold et al. (2020), where the authors use recurrence analysis to identify six climate states. We consider several model specifications. Fixing the number of breakpoints to five, the resulting breakpoint estimates closely align with those identified by Westerhold et al. (2020) across various data binning frequencies and model specifications. Treating the number of breakpoints as a parameter to be estimated results in statistical justification for more than five breakpoints in the time series. Our approach offers the advantage of constructing confidence intervals for the breakpoints, and it allows testing for the number of breakpoints present in the time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08336v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikkel Bennedsen, Eric Hillebrand, Siem Jan Koopman, Kathrine By Larsen</dc:creator>
    </item>
    <item>
      <title>A Data Fusion Model for Meteorological Data using the INLA-SPDE method</title>
      <link>https://arxiv.org/abs/2404.08533</link>
      <description>arXiv:2404.08533v1 Announce Type: new 
Abstract: This work aims to combine two primary meteorological data sources in the Philippines: data from a sparse network of weather stations and outcomes of a numerical weather prediction model. To this end, we propose a data fusion model which is primarily motivated by the problem of sparsity in the observational data and the use of a numerical prediction model as an additional data source in order to obtain better predictions for the variables of interest. The proposed data fusion model assumes that the different data sources are error-prone realizations of a common latent process. The outcomes from the weather stations follow the classical error model while the outcomes of the numerical weather prediction model involves a constant multiplicative bias parameter and an additive bias which is spatially-structured and time-varying. We use a Bayesian model averaging approach with the integrated nested Laplace approximation (INLA) for doing inference. The proposed data fusion model outperforms the stations-only model and the regression calibration approach, when assessed using leave-group-out cross-validation (LGOCV). We assess the benefits of data fusion and evaluate the accuracy of predictions and parameter estimation through a simulation study. The results show that the proposed data fusion model generally gives better predictions compared to the stations-only approach especially with sparse observational data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08533v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen Jun Villejo, Sara Martino, Finn Lindgren, Janine Illian</dc:creator>
    </item>
    <item>
      <title>Uncertainty Aware Tropical Cyclone Wind Speed Estimation from Satellite Data</title>
      <link>https://arxiv.org/abs/2404.08325</link>
      <description>arXiv:2404.08325v1 Announce Type: cross 
Abstract: Deep neural networks (DNNs) have been successfully applied to earth observation (EO) data and opened new research avenues. Despite the theoretical and practical advances of these techniques, DNNs are still considered black box tools and by default are designed to give point predictions. However, the majority of EO applications demand reliable uncertainty estimates that can support practitioners in critical decision making tasks. This work provides a theoretical and quantitative comparison of existing uncertainty quantification methods for DNNs applied to the task of wind speed estimation in satellite imagery of tropical cyclones. We provide a detailed evaluation of predictive uncertainty estimates from state-of-the-art uncertainty quantification (UQ) methods for DNNs. We find that predictive uncertainties can be utilized to further improve accuracy and analyze the predictive uncertainties of different methods across storm categories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08325v1</guid>
      <category>physics.ao-ph</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nils Lehmann, Nina Maria Gottschling, Stefan Depeweg, Eric Nalisnick</dc:creator>
    </item>
    <item>
      <title>Combining Statistical Depth and Fermat Distance for Uncertainty Quantification</title>
      <link>https://arxiv.org/abs/2404.08476</link>
      <description>arXiv:2404.08476v1 Announce Type: cross 
Abstract: We measure the Out-of-domain uncertainty in the prediction of Neural Networks using a statistical notion called ``Lens Depth'' (LD) combined with Fermat Distance, which is able to capture precisely the ``depth'' of a point with respect to a distribution in feature space, without any assumption about the form of distribution. Our method has no trainable parameter. The method is applicable to any classification model as it is applied directly in feature space at test time and does not intervene in training process. As such, it does not impact the performance of the original model. The proposed method gives excellent qualitative result on toy datasets and can give competitive or better uncertainty estimation on standard deep learning datasets compared to strong baseline methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08476v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hai-Vy Nguyen, Fabrice Gamboa, Reda Chhaibi, Sixin Zhang, Serge Gratton, Thierry Giaccone</dc:creator>
    </item>
    <item>
      <title>Reference prior for Bayesian estimation of seismic fragility curves</title>
      <link>https://arxiv.org/abs/2302.06935</link>
      <description>arXiv:2302.06935v4 Announce Type: replace 
Abstract: One of the key elements of probabilistic seismic risk assessment studies is the fragility curve, which represents the conditional probability of failure of a mechanical structure for a given scalar measure derived from seismic ground motion. For many structures of interest, estimating these curves is a daunting task because of the limited amount of data available; data which is only binary in our framework, i.e., only describing the structure as being in a failure or non-failure state. A large number of methods described in the literature tackle this challenging framework through parametric log-normal models. Bayesian approaches, on the other hand, allow model parameters to be learned more efficiently. However, the impact of the choice of the prior distribution on the posterior distribution cannot be readily neglected and, consequently, neither can its impact on any resulting estimation. This paper proposes a comprehensive study of this parametric Bayesian estimation problem for limited and binary data. Using the reference prior theory as a cornerstone, this study develops an objective approach to choosing the prior. This approach leads to the Jeffreys prior, which is derived for this problem for the first time. The posterior distribution is proven to be proper with the Jeffreys prior but improper with some traditional priors found in the literature. With the Jeffreys prior, the posterior distribution is also shown to vanish at the boundaries of the parameters' domain, which means that sampling the posterior distribution of the parameters does not result in anomalously small or large values. Therefore, the use of the Jeffreys prior does not result in degenerate fragility curves such as unit-step functions, and leads to more robust credibility intervals. The numerical results obtained from different case studies-including an industrial example-illustrate the theoretical predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.06935v4</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antoine Van Biesbroeck, Clement Gauchy, Cyril Feau, Josselin Garnier</dc:creator>
    </item>
    <item>
      <title>A State-Space Perspective on Modelling and Inference for Online Skill Rating</title>
      <link>https://arxiv.org/abs/2308.02414</link>
      <description>arXiv:2308.02414v3 Announce Type: replace 
Abstract: We summarise popular methods used for skill rating in competitive sports, along with their inferential paradigms and introduce new approaches based on sequential Monte Carlo and discrete hidden Markov models. We advocate for a state-space model perspective, wherein players' skills are represented as time-varying, and match results serve as observed quantities. We explore the steps to construct the model and the three stages of inference: filtering, smoothing and parameter estimation. We examine the challenges of scaling up to numerous players and matches, highlighting the main approximations and reductions which facilitate statistical and computational efficiency. We additionally compare approaches in a realistic experimental pipeline that can be easily reproduced and extended with our open-source Python package, https://github.com/SamDuffield/abile.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.02414v3</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Duffield, Samuel Power, Lorenzo Rimella</dc:creator>
    </item>
    <item>
      <title>Estimating parameters of continuous-time multi-chain hidden Markov models for infectious diseases</title>
      <link>https://arxiv.org/abs/2403.18875</link>
      <description>arXiv:2403.18875v2 Announce Type: replace 
Abstract: This study aims to estimate the parameters of a stochastic exposed-infected epidemiological model for the transmission dynamics of notifiable infectious diseases, based on observations related to isolated cases counts only. We use the setting of hidden multi-chain Markov models and adapt the Baum-Welch algorithm to the special structure of the multi-chain. From the estimated transition matrix, we retrieve the parameters of interest (contamination rates, incubation rate, and isolation rate) from analytical expressions of the moments and Monte Carlo simulations. The performance of this approach is investigated on synthetic data, together with an analysis of the impact of using a model with one less compartment to fit the data in order to help for model selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18875v2</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ibrahim Bouzalmat, Beno\^ite de Saporta, Solym M. Manou-Abi</dc:creator>
    </item>
    <item>
      <title>Interpretable discriminant analysis for functional data supported on random nonlinear domains with an application to Alzheimer's disease</title>
      <link>https://arxiv.org/abs/2112.02712</link>
      <description>arXiv:2112.02712v3 Announce Type: replace-cross 
Abstract: We introduce a novel framework for the classification of functional data supported on nonlinear, and possibly random, manifold domains. The motivating application is the identification of subjects with Alzheimer's disease from their cortical surface geometry and associated cortical thickness map. The proposed model is based upon a reformulation of the classification problem as a regularized multivariate functional linear regression model. This allows us to adopt a direct approach to the estimation of the most discriminant direction while controlling for its complexity with appropriate differential regularization. Our approach does not require prior estimation of the covariance structure of the functional predictors, which is computationally prohibitive in our application setting. We provide a theoretical analysis of the out-of-sample prediction error of the proposed model and explore the finite sample performance in a simulation setting. We apply the proposed method to a pooled dataset from the Alzheimer's Disease Neuroimaging Initiative and the Parkinson's Progression Markers Initiative. Through this application, we identify discriminant directions that capture both cortical geometric and thickness predictive features of Alzheimer's disease that are consistent with the existing neuroscience literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.02712v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssb/qkae023</arxiv:DOI>
      <arxiv:journal_reference>Journal of the Royal Statistical Society Series B: Statistical Methodology, 2024</arxiv:journal_reference>
      <dc:creator>Eardi Lila, Wenbo Zhang, Swati Rane Levendovszky</dc:creator>
    </item>
    <item>
      <title>Non-robustness of diffusion estimates on networks with measurement error</title>
      <link>https://arxiv.org/abs/2403.05704</link>
      <description>arXiv:2403.05704v3 Announce Type: replace-cross 
Abstract: Network diffusion models are used to study things like disease transmission, information spread, and technology adoption. However, small amounts of mismeasurement are extremely likely in the networks constructed to operationalize these models. We show that estimates of diffusions are highly non-robust to this measurement error. First, we show that even when measurement error is vanishingly small, such that the share of missed links is close to zero, forecasts about the extent of diffusion will greatly underestimate the truth. Second, a small mismeasurement in the identity of the initial seed generates a large shift in the locations of expected diffusion path. We show that both of these results still hold when the vanishing measurement error is only local in nature. Such non-robustness in forecasting exists even under conditions where the basic reproductive number is consistently estimable. Possible solutions, such as estimating the measurement error or implementing widespread detection efforts, still face difficulties because the number of missed links are so small. Finally, we conduct Monte Carlo simulations on simulated networks, and real networks from three settings: travel data from the COVID-19 pandemic in the western US, a mobile phone marketing campaign in rural India, and in an insurance experiment in China.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05704v3</guid>
      <category>econ.EM</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arun G. Chandrasekhar, Paul Goldsmith-Pinkham, Tyler H. McCormick, Samuel Thau, Jerry Wei</dc:creator>
    </item>
  </channel>
</rss>
