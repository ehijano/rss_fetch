<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 Aug 2024 04:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Granger causal inference for climate change attribution</title>
      <link>https://arxiv.org/abs/2408.16004</link>
      <description>arXiv:2408.16004v1 Announce Type: new 
Abstract: Climate change detection and attribution (D&amp;A) is concerned with determining the extent to which anthropogenic activities have influenced specific aspects of the global climate system. D&amp;A fits within the broader field of causal inference, the collection of statistical methods that identify cause and effect relationships. There are a wide variety of methods for making attribution statements, each of which require different types of input data and focus on different types of weather and climate events and each of which are conditional to varying extents. Some methods are based on Pearl causality while others leverage Granger causality, and the causal framing provides important context for how the resulting attribution conclusion should be interpreted. However, while Granger-causal attribution analyses have become more common, there is no clear statement of their strengths and weaknesses and no clear consensus on where and when Granger-causal perspectives are appropriate. In this prospective paper, we provide a formal definition for Granger-based approaches to trend and event attribution and a clear comparison with more traditional methods for assessing the human influence on extreme weather and climate events. Broadly speaking, Granger-causal attribution statements can be constructed quickly from observations and do not require computationally-intesive dynamical experiments. These analyses also enable rapid attribution, which is useful in the aftermath of a severe weather event, and provide multiple lines of evidence for anthropogenic climate change when paired with Pearl-causal attribution. Confidence in attribution statements is increased when different methodologies arrive at similar conclusions. Moving forward, we encourage the D&amp;A community to embrace hybrid approaches to climate change attribution that leverage the strengths of both Granger and Pearl causality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16004v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark D. Risser, Mohammed Ombadi, Michael F. Wehner</dc:creator>
    </item>
    <item>
      <title>A Tutorial on Brownian Motion for Biostatisticians</title>
      <link>https://arxiv.org/abs/2408.16011</link>
      <description>arXiv:2408.16011v1 Announce Type: new 
Abstract: This manuscript provides an in-depth exploration of Brownian Motion, a fundamental stochastic process in probability theory for Biostatisticians. It begins with foundational definitions and properties, including the construction of Brownian motion and its Markovian characteristics. The document delves into advanced topics such as the Karhunen-Loeve expansion, reflection principles, and Levy's modulus of continuity. Through rigorous proofs and theorems, the manuscript examines the non-differentiability of Brownian paths, the behavior of zero sets, and the significance of local time. The notes also cover important results like Donsker's theorem and Blumenthal's 0-1 law, emphasizing their implications in the study of stochastic processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16011v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elvis Han Cui</dc:creator>
    </item>
    <item>
      <title>Direct-Assisted Bayesian Unit-level Modeling for Small Area Estimation of Rare Event Prevalence</title>
      <link>https://arxiv.org/abs/2408.16129</link>
      <description>arXiv:2408.16129v1 Announce Type: cross 
Abstract: Small area estimation using survey data can be achieved by using either a design-based or a model-based inferential approach. With respect to assumptions, design-based direct estimators are generally preferable because of their consistency and asymptotic normality. However, when data are sparse at the desired area level, as is often the case when measuring rare events for example, these direct estimators can have extremely large uncertainty, making a model-based approach preferable. A model-based approach with a random spatial effect borrows information from surrounding areas at the cost of inducing shrinkage towards the local average. As a result, estimates may be over-smoothed and inconsistent with design-based estimates at higher area levels when aggregated. We propose a unit-level Bayesian model for small area estimation of rare event prevalence which uses design-based direct estimates at a higher area level to increase accuracy, precision, and consistency in aggregation. After introducing the model and its implementation, we conduct a simulation study to compare its properties to alternative models and apply it to the estimation of the neonatal mortality rate in Zambia, using 2014 DHS data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16129v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alana McGovern, Katherine Wilson, Jon Wakefield</dc:creator>
    </item>
    <item>
      <title>Continuous Gaussian mixture solution for linear Bayesian inversion with application to Laplace priors</title>
      <link>https://arxiv.org/abs/2408.16594</link>
      <description>arXiv:2408.16594v1 Announce Type: cross 
Abstract: We focus on Bayesian inverse problems with Gaussian likelihood, linear forward model, and priors that can be formulated as a Gaussian mixture. Such a mixture is expressed as an integral of Gaussian density functions weighted by a mixing density over the mixing variables. Within this framework, the corresponding posterior distribution also takes the form of a Gaussian mixture, and we derive the closed-form expression for its posterior mixing density. To sample from the posterior Gaussian mixture, we propose a two-step sampling method. First, we sample the mixture variables from the posterior mixing density, and then we sample the variables of interest from Gaussian densities conditioned on the sampled mixing variables. However, the posterior mixing density is relatively difficult to sample from, especially in high dimensions. Therefore, we propose to replace the posterior mixing density by a dimension-reduced approximation, and we provide a bound in the Hellinger distance for the resulting approximate posterior. We apply the proposed approach to a posterior with Laplace prior, where we introduce two dimension-reduced approximations for the posterior mixing density. Our numerical experiments indicate that samples generated via the proposed approximations have very low correlation and are close to the exact posterior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16594v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rafael Flock, Yiqiu Dong, Felipe Uribe, Olivier Zahm</dc:creator>
    </item>
    <item>
      <title>Bayesian mortality modelling with pandemics: a vanishing jump approach</title>
      <link>https://arxiv.org/abs/2311.04920</link>
      <description>arXiv:2311.04920v2 Announce Type: replace 
Abstract: This paper extends the Lee-Carter model for single- and multi-populations to account for pandemic jump effects of vanishing kind, allowing for a more comprehensive and accurate representation of mortality rates during a pandemic, characterised by a high impact at the beginning and gradually vanishing effects over subsequent periods. While the Lee-Carter model is effective in capturing mortality trends, it may not be able to account for large, unexpected jumps in mortality rates caused by pandemics or wars. Existing models allow either for transient jumps with an effect of one period only or persistent jumps. However, there is no literature on estimating mortality time series with jumps having an effect over a small number of periods as typically observed in pandemics. The Bayesian approach allows to quantify the uncertainty around the parameter estimates. Empirical data from the COVID-19 pandemic shows the superiority of the proposed approach, compared to models with a transitory shock effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04920v2</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Julius Goes, Karim Barigou, Anne Leucht</dc:creator>
    </item>
    <item>
      <title>Investigating symptom duration using current status data: a case study of post-acute COVID-19 syndrome</title>
      <link>https://arxiv.org/abs/2407.04214</link>
      <description>arXiv:2407.04214v2 Announce Type: replace 
Abstract: For infectious diseases, characterizing symptom duration is of clinical and public health importance. Symptom duration may be assessed by surveying infected individuals and querying symptom status at the time of survey response. For example, in a SARS-CoV-2 testing program at the University of Washington, participants were surveyed at least 28 days after testing positive and asked to report current symptom status. This study design yielded current status data: outcome measurements for each respondent consisted only of the time of survey response and a binary indicator of whether symptoms had resolved by that time. Such study design benefits from limited risk of recall bias, but analyzing the resulting data necessitates tailored statistical tools. Here, we review methods for current status data and describe a novel application of modern nonparametric techniques to this setting. The proposed approach is valid under weaker assumptions compared to existing methods, allows use of flexible machine learning tools, and handles potential survey nonresponse. From the university study, we estimate that 19% of participants experienced ongoing symptoms 30 days after testing positive, decreasing to 7% at 90 days. Female sex, history of seasonal allergies, fatigue during acute infection, and higher viral load were associated with slower symptom resolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04214v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles J. Wolock, Susan Jacob, Julia C. Bennett, Anna Elias-Warren, Jessica O'Hanlon, Avi Kenny, Nicholas P. Jewell, Andrea Rotnitzky, Stephen R. Cole, Ana A. Weil, Helen Y. Chu, Marco Carone</dc:creator>
    </item>
    <item>
      <title>Super-efficiency of Listed Banks in China and Determinants Analysis (2006-2021)</title>
      <link>https://arxiv.org/abs/2305.10885</link>
      <description>arXiv:2305.10885v2 Announce Type: replace-cross 
Abstract: This study employs the annual unbalanced panel data of 42 listed banks in China from 2006 to 2021, adopts the non-radial and non-oriented super-efficiency Data envelopment analysis (Super-SBM-UND-VRS based DEA) model considering NPL as undesired output. Our results show that the profitability super-efficiency of State-owned banks and Rural/City Commercial Banks is better than that of Joint-stock Banks. In terms of intermediary efficiency(deposit and loan), state-owned banks have advantage on other two type of banks. The determinants analysis shows that all type of banks significantly benefits from the decrease of ownership concentration which support reformation and IPO. Regional commercial banks significantly benefit from the decrease of customer concentration and the increase of reserves. On the other hand, State-owned banks should increase its loan to deposit ratio while joint-stock banks should do the opposite.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.10885v2</guid>
      <category>q-fin.GN</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yun Liao, Ruihui Xu</dc:creator>
    </item>
    <item>
      <title>Asymptotics for non-degenerate multivariate $U$-statistics with estimated nuisance parameters under the null and local alternative hypotheses</title>
      <link>https://arxiv.org/abs/2401.11272</link>
      <description>arXiv:2401.11272v2 Announce Type: replace-cross 
Abstract: The large-sample behavior of non-degenerate multivariate $U$-statistics of arbitrary degree is investigated under the assumption that their kernel depends on parameters that can be estimated consistently. Mild regularity conditions are given which guarantee that once properly normalized, such statistics are asymptotically multivariate Gaussian both under the null hypothesis and sequences of local alternatives. The work of Randles (1982, \emph{Ann. Statist.}) is extended in three ways: the data and the kernel values can be multivariate rather than univariate, the limiting behavior under local alternatives is studied for the first time, and the effect of knowing some of the nuisance parameters is quantified. These results can be applied to a broad range of goodness-of-fit testing contexts, as shown in two specific examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11272v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 30 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alain Desgagn\'e, Christian Genest, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
  </channel>
</rss>
