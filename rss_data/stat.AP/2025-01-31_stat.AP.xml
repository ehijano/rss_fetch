<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 Jan 2025 05:01:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Comprehensive Framework for Statistical Inference in Measurement System Assessment Studies</title>
      <link>https://arxiv.org/abs/2501.18037</link>
      <description>arXiv:2501.18037v1 Announce Type: new 
Abstract: Measurement system analysis aims to quantify the variability in data attributable to the measurement system and evaluate its contribution to overall data variability. This paper conducts a rigorous theoretical investigation of the statistical methods used in such analyses, focusing on variance components and other critical parameters. While established techniques exist for single-variable cases, a systematic theoretical exploration of their properties has been largely overlooked. This study addresses this gap by examining estimators for variance components and other key parameters in measurement system assessment, analyzing their statistical properties, and providing new insights into their reliability, performance, and applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18037v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Banafsheh Lashkari, Shojaeddin Chenouri</dc:creator>
    </item>
    <item>
      <title>Statistical multi-metric evaluation and visualization of LLM system predictive performance</title>
      <link>https://arxiv.org/abs/2501.18243</link>
      <description>arXiv:2501.18243v1 Announce Type: new 
Abstract: The evaluation of generative or discriminative large language model (LLM)-based systems is often a complex multi-dimensional problem. Typically, a set of system configuration alternatives are evaluated on one or more benchmark datasets, each with one or more evaluation metrics, which may differ between datasets. We often want to evaluate -- with a statistical measure of significance -- whether systems perform differently either on a given dataset according to a single metric, on aggregate across metrics on a dataset, or across datasets. Such evaluations can be done to support decision-making, such as deciding whether a particular system component change (e.g., choice of LLM or hyperparameter values) significantly improves performance over the current system configuration, or, more generally, whether a fixed set of system configurations (e.g., a leaderboard list) have significantly different performances according to metrics of interest. We present a framework implementation that automatically performs the correct statistical tests, properly aggregates the statistical results across metrics and datasets (a nontrivial task), and can visualize the results. The framework is demonstrated on the multi-lingual code generation benchmark CrossCodeEval, for several state-of-the-art LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18243v1</guid>
      <category>stat.AP</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Ackerman, Eitan Farchi, Orna Raz, Assaf Toledo</dc:creator>
    </item>
    <item>
      <title>Robust impact localisation on composite aerostructures using kernel design and Bayesian fusion under environmental and operational uncertainties</title>
      <link>https://arxiv.org/abs/2501.18393</link>
      <description>arXiv:2501.18393v1 Announce Type: new 
Abstract: Impact localisation on composite aircraft structures remains a significant challenge due to operational and environmental uncertainties, such as variations in temperature, impact mass, and energy levels. This study proposes a novel Gaussian Process Regression framework that leverages the order invariance of time difference of arrival (TDOA) inputs to achieve probabilistic impact localisation under such uncertainties. A composite kernel function, combining radial basis function and cosine similarity kernels, is designed based on wave propagation dynamics to enhance adaptability to diverse conditions. Additionally, a task covariance kernel is introduced to enable multitask learning, facilitating the joint prediction of spatial coordinates while capturing interdependencies between outputs. To further improve robustness and accuracy, Bayesian model averaging is employed to dynamically fuse kernel predictions, assigning adaptive weights that account for varying conditions. Extensive experimental validation on a composite plate, including scenarios with large-mass drop tower impacts and small-mass guided drop mass impacts, demonstrates the proposed method's robustness and generalisability. Notably, the framework achieves accurate localisation without requiring compensation strategies for variations in temperature or impact mass, highlighting its suitability for real-world applications. The study also highlights the critical role of sample standardisation for preprocessing TDOA inputs, demonstrating its superiority over feature standardisation by preserving TDOA order invariance and enhancing model compatibility. These advancements establish the proposed method as a reliable and effective solution for structural health monitoring in complex and uncertain operational environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18393v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dong Xiao, Zahra Sharif-Khodaei, M. H. Aliabadi</dc:creator>
    </item>
    <item>
      <title>Application of Machine Learning Models for Carbon Monoxide and Nitrogen Oxides Emission Prediction in Gas Turbines</title>
      <link>https://arxiv.org/abs/2501.17865</link>
      <description>arXiv:2501.17865v1 Announce Type: cross 
Abstract: This paper addresses the environmental impacts linked to hazardous emissions from gas turbines, with a specific focus on employing various machine learning (ML) models to predict the emissions of Carbon Monoxide (CO) and Nitrogen Oxides (NOx) as part of a Predictive Emission Monitoring System (PEMS). We employ a comprehensive approach using multiple predictive models to offer insights on enhancing regulatory compliance and optimizing operational parameters to reduce environmental effects effectively. Our investigation explores a range of machine learning models including linear models, ensemble methods, and neural networks. The models we assess include Linear Regression, Support Vector Machines (SVM), Decision Trees, XGBoost, Multi-Layer Perceptron (MLP), Long Short-Term Memory networks (LSTM), Gated Recurrent Units (GRU), and K-Nearest Neighbors (KNN). This analysis provides a comparative overview of the performance of these ML models in estimating CO and NOx emissions from gas turbines, aiming to highlight the most effective techniques for this critical task. Accurate ML models for predicting gas turbine emissions help reduce environmental impact by enabling real-time adjustments and supporting effective emission control strategies, thus promoting sustainability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17865v1</guid>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kamyar Zeinalipour, Laure Barriere, David Ghelardi, Marco Gori</dc:creator>
    </item>
    <item>
      <title>Input layer regularization and automated regularization hyperparameter tuning for myelin water estimation using deep learning</title>
      <link>https://arxiv.org/abs/2501.18074</link>
      <description>arXiv:2501.18074v1 Announce Type: cross 
Abstract: We propose a novel deep learning method which combines classical regularization with data augmentation for estimating myelin water fraction (MWF) in the brain via biexponential analysis. Our aim is to design an accurate deep learning technique for analysis of signals arising in magnetic resonance relaxometry. In particular, we study the biexponential model, one of the signal models used for MWF estimation. We greatly extend our previous work on \emph{input layer regularization (ILR)} in several ways. We now incorporate optimal regularization parameter selection via a dedicated neural network or generalized cross validation (GCV) on a signal-by-signal, or pixel-by-pixel, basis to form the augmented input signal, and now incorporate estimation of MWF, rather than just exponential time constants, into the analysis. On synthetically generated data, our proposed deep learning architecture outperformed both classical methods and a conventional multi-layer perceptron. On in vivo brain data, our architecture again outperformed other comparison methods, with GCV proving to be somewhat superior to a NN for regularization parameter selection. Thus, ILR improves estimation of MWF within the biexponential model. In addition, classical methods such as GCV may be combined with deep learning to optimize MWF imaging in the human brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18074v1</guid>
      <category>q-bio.QM</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mirage Modi, Shashank Sule, Jonathan Palumbo, Michael Rozowski, Mustapha Bouhrara, Wojciech Czaja, Richard G. Spencer</dc:creator>
    </item>
    <item>
      <title>Logistic regression models: practical induced prior specification</title>
      <link>https://arxiv.org/abs/2501.18106</link>
      <description>arXiv:2501.18106v1 Announce Type: cross 
Abstract: Bayesian inference for statistical models with a hierarchical structure is often characterized by specification of priors for parameters at different levels of the hierarchy. When higher level parameters are functions of the lower level parameters, specifying a prior on the lower level parameters leads to induced priors on the higher level parameters. However, what are deemed uninformative priors for lower level parameters can induce strikingly non-vague priors for higher level parameters. Depending on the sample size and specific model parameterization, these priors can then have unintended effects on the posterior distribution of the higher level parameters.
  Here we focus on Bayesian inference for the Bernoulli distribution parameter $\theta$ which is modeled as a function of covariates via a logistic regression, where the coefficients are the lower level parameters for which priors are specified. A specific area of interest and application is the modeling of survival probabilities in capture-recapture data and occupancy and detection probabilities in presence-absence data. In particular we propose alternative priors for the coefficients that yield specific induced priors for $\theta$. We address three induced prior cases. The simplest is when the induced prior for $\theta$ is Uniform(0,1). The second case is when the induced prior for $\theta$ is an arbitrary Beta($\alpha$, $\beta$) distribution. The third case is one where the intercept in the logistic model is to be treated distinct from the partial slope coefficients; e.g., $E[\theta]$ equals a specified value on (0,1) when all covariates equal 0. Simulation studies were carried out to evaluate performance of these priors and the methods were applied to a real presence/absence data set and occupancy modelling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18106v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ken B. Newman, Cristiano Villa, Ruth King</dc:creator>
    </item>
    <item>
      <title>Nonlocal prior mixture-based Bayesian wavelet regression</title>
      <link>https://arxiv.org/abs/2501.18134</link>
      <description>arXiv:2501.18134v1 Announce Type: cross 
Abstract: We propose a novel Bayesian wavelet regression approach using a three-component spike-and-slab prior for wavelet coefficients, combining a point mass at zero, a moment (MOM) prior, and an inverse moment (IMOM) prior. This flexible prior supports small and large coefficients differently, offering advantages for highly dispersed data where wavelet coefficients span multiple scales. The IMOM prior's heavy tails capture large coefficients, while the MOM prior is better suited for smaller non-zero coefficients. Further, our method introduces innovative hyperparameter specifications for mixture probabilities and scaling parameters, including generalized logit, hyperbolic secant, and generalized normal decay for probabilities, and double exponential decay for scaling. Hyperparameters are estimated via an empirical Bayes approach, enabling posterior inference tailored to the data. Extensive simulations demonstrate significant performance gains over two-component wavelet methods. Applications to electroencephalography and noisy audio data illustrate the method's utility in capturing complex signal characteristics. We implement our method in an R package NLPwavelet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18134v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nilotpal Sanyal</dc:creator>
    </item>
    <item>
      <title>Cracks in concrete</title>
      <link>https://arxiv.org/abs/2501.18376</link>
      <description>arXiv:2501.18376v1 Announce Type: cross 
Abstract: Finding and properly segmenting cracks in images of concrete is a challenging task. Cracks are thin and rough and being air filled do yield a very weak contrast in 3D images obtained by computed tomography. Enhancing and segmenting dark lower-dimensional structures is already demanding. The heterogeneous concrete matrix and the size of the images further increase the complexity. ML methods have proven to solve difficult segmentation problems when trained on enough and well annotated data. However, so far, there is not much 3D image data of cracks available at all, let alone annotated. Interactive annotation is error-prone as humans can easily tell cats from dogs or roads without from roads with cars but have a hard time deciding whether a thin and dark structure seen in a 2D slice continues in the next one. Training networks by synthetic, simulated images is an elegant way out, bears however its own challenges. In this contribution, we describe how to generate semi-synthetic image data to train CNN like the well known 3D U-Net or random forests for segmenting cracks in 3D images of concrete. The thickness of real cracks varies widely, both, within one crack as well as from crack to crack in the same sample. The segmentation method should therefore be invariant with respect to scale changes. We introduce the so-called RieszNet, designed for exactly this purpose. Finally, we discuss how to generalize the ML crack segmentation methods to other concrete types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18376v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <category>stat.AP</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-66253-9</arxiv:DOI>
      <arxiv:journal_reference>Statistical Machine Learning for Engineering with Applications (Lecture Notes in Statistics), edited by J\"urgen Franke, Anita Sch\"obel, 2024, Springer Cham</arxiv:journal_reference>
      <dc:creator>Tin Barisin, Christian Jung, Anna Nowacka, Claudia Redenbach, Katja Schladitz</dc:creator>
    </item>
    <item>
      <title>A tutorial on conducting sample size and power calculations for detecting treatment effect heterogeneity in cluster randomized trials</title>
      <link>https://arxiv.org/abs/2501.18383</link>
      <description>arXiv:2501.18383v1 Announce Type: cross 
Abstract: Cluster-randomized trials (CRTs) are a well-established class of designs for evaluating large-scale, community-based research questions. An essential task in planning these trials is determining the required number of clusters and cluster sizes to achieve sufficient statistical power for detecting a clinically relevant effect size. Compared to methods for evaluating the average treatment effect (ATE) for the entire study population, there is more recent development of sample size methods for testing the heterogeneity of treatment effects (HTEs), i.e., modification of treatment effects by subpopulation characteristics, in CRTs. For confirmatory analyses of HTEs in CRTs, effect modifiers must be pre-specified, and ideally, accompanied by sample size or power calculations to ensure the trial has adequate power for the planned analyses. Power analysis for HTE analyses is more complex than for ATEs due to the additional design parameters that must be specified. Power and sample size formulas for HTE analyses have been separately derived under several cluster-randomized designs, including single and multi-period parallel designs, crossover designs, and stepped-wedge designs, as well as under continuous and binary outcomes. This tutorial provides a consolidated reference guide for these methods and enhances their accessibility through the development of an online R Shiny calculator. We further discuss key considerations for researchers conducting sample size and power calculations for testing pre-specified HTE hypotheses in CRTs, including the essential role of advance estimates of intracluster correlation coefficients for both outcomes and covariates on power. The sample size methodology and calculator functionality are demonstrated through real CRT examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18383v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mary Ryan Baumann, Monica Taljaard, Patrick J. Heagerty, Michael O. Harhay, Guangyu Tong, Rui Wang, Fan Li</dc:creator>
    </item>
    <item>
      <title>A Data Fusion Model for Meteorological Data using the INLA-SPDE method</title>
      <link>https://arxiv.org/abs/2404.08533</link>
      <description>arXiv:2404.08533v2 Announce Type: replace 
Abstract: This work aims to combine two primary meteorological data sources in the Philippines: data from a sparse network of weather stations and outcomes of a numerical weather prediction model. To this end, we propose a data fusion model which is primarily motivated by the problem of sparsity in the observational data and the use of a numerical prediction model as an additional data source in order to obtain better predictions for the variables of interest. The proposed data fusion model assumes that the different data sources are error-prone realizations of a common latent process. The outcomes from the weather stations follow the classical error model while the outcomes of the numerical weather prediction model involves a constant multiplicative bias parameter and an additive bias which is spatially-structured and time-varying. We use a Bayesian model averaging approach with the integrated nested Laplace approximation (INLA) for doing inference. The proposed data fusion model outperforms the stations-only model and the regression calibration approach, when assessed using leave-group-out cross-validation (LGOCV). We assess the benefits of data fusion and evaluate the accuracy of predictions and parameter estimation through a simulation study. The results show that the proposed data fusion model generally gives better predictions compared to the stations-only approach especially with sparse observational data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08533v2</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen Jun Villejo, Sara Martino, Finn Lindgren, Janine Illian</dc:creator>
    </item>
    <item>
      <title>Uncertainty quantification in automated valuation models with spatially weighted conformal prediction</title>
      <link>https://arxiv.org/abs/2312.06531</link>
      <description>arXiv:2312.06531v2 Announce Type: replace-cross 
Abstract: Non-parametric machine learning models, such as random forests and gradient boosted trees, are frequently used to estimate house prices due to their predictive accuracy, but a main drawback of such methods is their limited ability to quantify prediction uncertainty. Conformal prediction (CP) is a model-agnostic framework for constructing confidence sets around predictions of machine learning models with minimal assumptions. However, due to the spatial dependencies observed in house prices, direct application of CP leads to confidence sets that are not calibrated everywhere, i.e., the confidence sets will be too large in certain geographical regions and too small in others. We survey various approaches to adjust the CP confidence set to account for this and demonstrate their performance on a data set from the housing market in Oslo, Norway. Our findings indicate that calibrating the confidence sets on a spatially weighted version of the non-conformity scores makes the coverage more consistently calibrated across geographical regions. We also perform a simulation study on synthetically generated sale prices to empirically explore the performance of CP on housing market data under idealized conditions with known data-generating mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.06531v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anders Hjort, Gudmund Horn Hermansen, Johan Pensar, Jonathan P. Williams</dc:creator>
    </item>
    <item>
      <title>A New Perspective to Fish Trajectory Imputation: A Methodology for Spatiotemporal Modeling of Acoustically Tagged Fish Data</title>
      <link>https://arxiv.org/abs/2408.13220</link>
      <description>arXiv:2408.13220v2 Announce Type: replace-cross 
Abstract: The focus of this paper is a key component of a methodology for understanding, interpolating, and predicting fish movement patterns based on spatiotemporal data recorded by spatially static acoustic receivers. Unlike GPS trackers which emit satellite signals from the animal's location, acoustic receivers are akin to stationary motion sensors that record movements within their detection range. Thus, for periods of time, fish may be far from the receivers, resulting in the absence of observations. The lack of information on the fish's location for extended time periods poses challenges to the understanding of fish movement patterns, and hence, the identification of proper statistical inference frameworks for modeling the trajectories. As the initial step in our methodology, in this paper, we devise and implement a simulation-based imputation strategy that relies on both Markov chain and random-walk principles to enhance our dataset over time. This methodology will be generalizable and applicable to all fish species with similar migration patterns or data with similar structures due to the use of static acoustic receivers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13220v2</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahshid Ahmadian, Edward L. Boone, Grace S. Chiu</dc:creator>
    </item>
    <item>
      <title>Bayesian Despeckling of Structured Sources</title>
      <link>https://arxiv.org/abs/2501.11860</link>
      <description>arXiv:2501.11860v2 Announce Type: replace-cross 
Abstract: Speckle noise is a fundamental challenge in coherent imaging systems, significantly degrading image quality. Over the past decades, numerous despeckling algorithms have been developed for applications such as Synthetic Aperture Radar (SAR) and digital holography. In this paper, we aim to establish a theoretically grounded approach to despeckling. We propose a method applicable to general structured stationary stochastic sources. We demonstrate the effectiveness of the proposed method on piecewise constant sources. Additionally, we theoretically derive a lower bound on the despeckling performance for such sources. The proposed depseckler applied to the 1-Markov structured sources achieves better reconstruction performance with no strong simplification of the ground truth signal model or speckle noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11860v2</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Zafari, Shirin Jalali</dc:creator>
    </item>
  </channel>
</rss>
