<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 04 Jul 2025 04:04:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Neural simulation-based inference of the Higgs trilinear self-coupling via off-shell Higgs production</title>
      <link>https://arxiv.org/abs/2507.02032</link>
      <description>arXiv:2507.02032v1 Announce Type: cross 
Abstract: One of the forthcoming major challenges in particle physics is the experimental determination of the Higgs trilinear self-coupling. While efforts have largely focused on on-shell double- and single-Higgs production in proton-proton collisions, off-shell Higgs production has also been proposed as a valuable complementary probe. In this article, we design a hybrid neural simulation-based inference (NSBI) approach to construct a likelihood of the Higgs signal incorporating modifications from the Standard Model effective field theory (SMEFT), relevant background processes, and quantum interference effects. It leverages the training efficiency of matrix-element-enhanced techniques, which are vital for robust SMEFT applications, while also incorporating the practical advantages of classification-based methods for effective background estimates. We demonstrate that our NSBI approach achieves sensitivity close to the theoretical optimum and provide expected constraints for the high-luminosity upgrade of the Large Hadron Collider. While we primarily concentrate on the Higgs trilinear self-coupling, we also consider constraints on other SMEFT operators that affect off-shell Higgs production.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02032v1</guid>
      <category>hep-ph</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aishik Ghosh, Maximilian Griese, Ulrich Haisch, Tae Hyoun Park</dc:creator>
    </item>
    <item>
      <title>Learning and Testing Inverse Statistical Problems For Interacting Systems Undergoing Phase Transition</title>
      <link>https://arxiv.org/abs/2507.02574</link>
      <description>arXiv:2507.02574v1 Announce Type: cross 
Abstract: Inverse problems arise in situations where data is available, but the underlying model is not. It can therefore be necessary to infer the parameters of the latter starting from the former. Statistical mechanics offers a toolbox of techniques to address this challenge. In this work, we illustrate three of the main methods: the Maximum Likelihood, Maximum Pseudo-Likelihood, and Mean-Field approaches. We begin with a thorough theoretical introduction to these methods, followed by their application to inference in several well-known statistical physics systems undergoing phase transitions. Namely, we consider the ordered and disordered Ising models, the vector Potts model, and the Blume-Capel model on both regular lattices and random graphs. This discussion is accompanied by a GitHub repository that allows users to both reproduce the results and experiment with new systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02574v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.dis-nn</category>
      <category>physics.data-an</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefano Bae, Dario Bocchi, Luca Maria Del Bono, Luca Leuzzi</dc:creator>
    </item>
    <item>
      <title>Classification by Separating Hypersurfaces: An Entropic Approach</title>
      <link>https://arxiv.org/abs/2507.02732</link>
      <description>arXiv:2507.02732v1 Announce Type: cross 
Abstract: We consider the following classification problem: Given a population of individuals characterized by a set of attributes represented as a vector in ${\mathbb R}^N$, the goal is to find a hyperplane in ${\mathbb R}^N$ that separates two sets of points corresponding to two distinct classes. This problem, with a history dating back to the perceptron model, remains central to machine learning. In this paper we propose a novel approach by searching for a vector of parameters in a bounded $N$-dimensional hypercube centered at the origin and a positive vector in ${\mathbb R}^M$, obtained through the minimization of an entropy-based function defined over the space of unknown variables. The method extends to polynomial surfaces, allowing the separation of data points by more complex decision boundaries. This provides a robust alternative to traditional linear or quadratic optimization techniques, such as support vector machines and gradient descent. Numerical experiments demonstrate the efficiency and versatility of the method in handling diverse classification tasks, including linear and non-linear separability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02732v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Argimiro Arratia, Mahmoud El Daou, Henryk Gzyl</dc:creator>
    </item>
    <item>
      <title>Transition of AI Models in dependence of noise</title>
      <link>https://arxiv.org/abs/2506.16715</link>
      <description>arXiv:2506.16715v3 Announce Type: replace 
Abstract: We investigate the dependence of the score on noise in the data, and on the network size. As a result, we obtain the so-called "cognition transition" from good performance to zero with increasing noise. The understanding of this transition is of fundamental scientific and practical interest. We use concepts from statistical mechanics to understand how a changing finite size of models affects the cognition ability under the presence or corrupted data. On one hand, we study if there is a universal aspect in the transition to several models, on the other hand we go into detail how the approach of the cognition transition point can be captured quantitatively. Therefore, we use the so-called scaling approach from statistical mechanics and find a power-law behaviour of the transition width with increasing model size. Since our study is aimed at universal aspects we use well-know models and data for image classification. That way we avoid uncertainties in data handling or model setup. The practical implication of our results is a tool to estimate model sizes for a certain "universality class" of models, without the need to investigate large sizes, just by extrapolating the scaling results. In turn, that allows for cost reduction in hyperparameter studies. Here, we present first results on a concrete setup; we think that the understanding the mechanics of large system sizes is of fundamental interest for a further exploration of even larger models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16715v3</guid>
      <category>physics.data-an</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Thomas Seidler, Markus Abel</dc:creator>
    </item>
    <item>
      <title>Tree-based Learning for High-Fidelity Prediction of Chaos</title>
      <link>https://arxiv.org/abs/2403.13836</link>
      <description>arXiv:2403.13836v2 Announce Type: replace-cross 
Abstract: Model-free forecasting of the temporal evolution of chaotic systems is crucial but challenging. Existing solutions require hyperparameter tuning, significantly hindering their wider adoption. In this work, we introduce a tree-based approach not requiring hyperparameter tuning: TreeDOX. It uses time delay overembedding as explicit short-term memory and Extra-Trees Regressors to perform feature reduction and forecasting. We demonstrate the state-of-the-art performance of TreeDOX using the Henon map, Lorenz and Kuramoto-Sivashinsky systems, and the real-world Southern Oscillation Index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13836v2</guid>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>nlin.CD</category>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Adam Giammarese, Kamal Rana, Erik M. Bollt, Nishant Malik</dc:creator>
    </item>
  </channel>
</rss>
