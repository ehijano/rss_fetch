<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Jan 2025 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>LITE: Efficiently Estimating Gaussian Probability of Maximality</title>
      <link>https://arxiv.org/abs/2501.13535</link>
      <description>arXiv:2501.13535v1 Announce Type: cross 
Abstract: We consider the problem of computing the probability of maximality (PoM) of a Gaussian random vector, i.e., the probability for each dimension to be maximal. This is a key challenge in applications ranging from Bayesian optimization to reinforcement learning, where the PoM not only helps with finding an optimal action, but yields a fine-grained analysis of the action domain, crucial in tasks such as drug discovery. Existing techniques are costly, scaling polynomially in computation and memory with the vector size. We introduce LITE, the first approach for estimating Gaussian PoM with almost-linear time and memory complexity. LITE achieves SOTA accuracy on a number of tasks, while being in practice several orders of magnitude faster than the baselines. This also translates to a better performance on downstream tasks such as entropy estimation and optimal control of bandits. Theoretically, we cast LITE as entropy-regularized UCB and connect it to prior PoM estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13535v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Menet (ETH Z\"urich), Jonas H\"ubotter (ETH Z\"urich), Parnian Kassraie (ETH Z\"urich), Andreas Krause (ETH Z\"urich)</dc:creator>
    </item>
    <item>
      <title>Incorporating additional evidence as prior information to resolve non-identifiability in Bayesian disease model calibration. A tutorial</title>
      <link>https://arxiv.org/abs/2407.13451</link>
      <description>arXiv:2407.13451v2 Announce Type: replace 
Abstract: Disease models are used to examine the likely impact of therapies, interventions and public policy changes. Ensuring that these are well calibrated on the basis of available data and that the uncertainty in their projections is properly quantified is an important part of the process. The question of non-identifiability poses a challenge to disease model calibration where multiple parameter sets generate identical model outputs.
  For statisticians evaluating the impact of policy interventions such as screening or vaccination, this is a critical issue. This study explores the use of the Bayesian framework to provide a natural way to calibrate models and address non-identifiability in a probabilistic fashion in the context of disease modelling. We present Bayesian approaches for incorporating expert knowledge and external data to ensure that appropriately informative priors are specified on the joint parameter space. These approaches are applied to two common disease models: a basic Susceptible-Infected-Susceptible (SIS) model and a much more complex agent-based model which has previously been used to address public policy questions in HPV and cervical cancer. The conditions which allow the problem of non-identifiability to be resolved are demonstrated for the SIS model. For the larger HPV model an overview of the findings is presented, but of key importance is a discussion on how the non-identifiability impacts the calibration process. Through case studies, we demonstrate how informative priors can help resolve non-identifiability and improve model inference. We also discuss how sensitivity analysis can be used to assess the impact of prior specifications on model results. Overall, this work provides an important tutorial for researchers interested in applying Bayesian methods to calibrate models and handle non-identifiability in disease models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13451v2</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daria Semochkina, Cathal Walsh</dc:creator>
    </item>
    <item>
      <title>Block Vecchia Approximation for Scalable and Efficient Gaussian Process Computations</title>
      <link>https://arxiv.org/abs/2410.04477</link>
      <description>arXiv:2410.04477v2 Announce Type: replace 
Abstract: Gaussian Processes (GPs) are vital for modeling and predicting irregularly-spaced, large geospatial datasets. However, their computations often pose significant challenges in large-scale applications. One popular method to approximate GPs is the Vecchia approximation, which approximates the full likelihood via a series of conditional probabilities. The classical Vecchia approximation uses univariate conditional distributions, which leads to redundant evaluations and memory burdens. To address this challenge, our study introduces block Vecchia, which evaluates each multivariate conditional distribution of a block of observations, with blocks formed using the K-means algorithm. The proposed GPU framework for the block Vecchia uses varying batched linear algebra operations to compute multivariate conditional distributions concurrently, notably diminishing the frequent likelihood evaluations. Diving into the factor affecting the accuracy of the block Vecchia, the neighbor selection criterion is investigated, where we found that the random ordering markedly enhances the approximated quality as the block count becomes large. To verify the scalability and efficiency of the algorithm, we conduct a series of numerical studies and simulations, demonstrating their practical utility and effectiveness compared to the exact GP. Moreover, we tackle large-scale real datasets using the block Vecchia method, i.e., high-resolution 3D profile wind speed with a million points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04477v2</guid>
      <category>stat.CO</category>
      <category>cs.CE</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qilong Pan, Sameh Abdulah, Marc G. Genton, Ying Sun</dc:creator>
    </item>
    <item>
      <title>Numerically Robust Fixed-Point Smoothing Without State Augmentation</title>
      <link>https://arxiv.org/abs/2409.20004</link>
      <description>arXiv:2409.20004v2 Announce Type: replace-cross 
Abstract: Practical implementations of Gaussian smoothing algorithms have received a great deal of attention in the last 60 years. However, almost all work focuses on estimating complete time series (''fixed-interval smoothing'', $\mathcal{O}(K)$ memory) through variations of the Rauch--Tung--Striebel smoother, rarely on estimating the initial states (''fixed-point smoothing'', $\mathcal{O}(1)$ memory). Since fixed-point smoothing is a crucial component of algorithms for dynamical systems with unknown initial conditions, we close this gap by introducing a new formulation of a Gaussian fixed-point smoother. In contrast to prior approaches, our perspective admits a numerically robust Cholesky-based form (without downdates) and avoids state augmentation, which would needlessly inflate the state-space model and reduce the numerical practicality of any fixed-point smoother code. The experiments demonstrate how a JAX implementation of our algorithm matches the runtime of the fastest methods and the robustness of the most robust techniques while existing implementations must always sacrifice one for the other.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20004v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas Kr\"amer</dc:creator>
    </item>
    <item>
      <title>A mixture representation of the spectral distribution of isotropic kernels with application to random Fourier features</title>
      <link>https://arxiv.org/abs/2411.02770</link>
      <description>arXiv:2411.02770v2 Announce Type: replace-cross 
Abstract: Rahimi and Recht (2007) introduced the idea of decomposing positive definite shift-invariant kernels by randomly sampling from their spectral distribution. This famous technique, known as Random Fourier Features (RFF), is in principle applicable to any such kernel whose spectral distribution can be identified and simulated. In practice, however, it is usually applied to the Gaussian kernel because of its simplicity, since its spectral distribution is also Gaussian. Clearly, simple spectral sampling formulas would be desirable for broader classes of kernels. In this paper, we prove that the spectral distribution of every positive definite isotropic kernel can be decomposed as a scale mixture of $\alpha$-stable random vectors, and we identify the scaling distribution as a function of the kernel. This constructive decomposition provides a simple and ready-to-use spectral sampling formula for every multivariate positive definite shift-invariant kernel, including exponential power kernels, generalized Mat\'ern kernels, generalized Cauchy kernels, as well as newly introduced kernels such as the Beta, Kummer, and Tricomi kernels. In particular, we show that the spectral distributions of these kernels are scale mixtures of the multivariate Gaussian distribution. This provides a very simple way to adapt existing random Fourier features software based on Gaussian kernels to any positive definite shift-invariant kernel. This result has broad applications for support vector machines, kernel ridge regression, Gaussian processes, and other kernel-based machine learning techniques for which the random Fourier features technique is applicable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02770v2</guid>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Langren\'e, Xavier Warin, Pierre Gruet</dc:creator>
    </item>
  </channel>
</rss>
