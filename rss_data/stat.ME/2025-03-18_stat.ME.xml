<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Mar 2025 02:08:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Comment on arXiv:2202.01553: The Distribution of a Gaussian Covariate Statistic</title>
      <link>https://arxiv.org/abs/2503.11712</link>
      <description>arXiv:2503.11712v1 Announce Type: new 
Abstract: This is a comment on arXiv:2202.01553. In regression Gaussian covariate p-values (Davies and D{\"u}mbgen, arXiv:2202.01553) are used to control greedy forward subset selection by accounting for choosing the best when fitting many variables. Here we outline a simple proof of their Theorems 1 and 2, making alterations to simplify the exposition by including a new variable rather than excluding an included variable and some slight changes in notation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11712v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joe Whittaker (Lancaster University, U.K.)</dc:creator>
    </item>
    <item>
      <title>Generalized Bayesian Ensemble Survival Tree (GBEST) model</title>
      <link>https://arxiv.org/abs/2503.11738</link>
      <description>arXiv:2503.11738v1 Announce Type: new 
Abstract: This paper proposes a new class of predictive models for survival analysis called Generalized Bayesian Ensemble Survival Tree (GBEST). It is well known that survival analysis poses many different challenges, in particular when applied to small data or censorship mechanism. Our contribution is the proposal of an ensemble approach that uses Bayesian bootstrap and beta Stacy bootstrap methods to improve the outcome in survival application with a special focus on small datasets. More precisely, a novel approach to integrate Beta Stacy Bayesian bootstrap in bagging tree models for censored data is proposed in this paper. Empirical evidence achieved on simulated and real data underlines that our approach performs better in terms of predictive performances and stability of the results compared with classical survival models available in the literature. In terms of methodology our novel contribution considers the adaptation of recent Bayesian ensemble approaches to survival data, providing a new model called Generalized Bayesian Ensemble Survival Tree (GBEST). A further result in terms of computational novelty is the implementation in R of GBEST, available in a public GitHub repository.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11738v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Ballante, Pietro Muliere, Silvia Figini</dc:creator>
    </item>
    <item>
      <title>Proposal for the Application of Fractional Operators in Polynomial Regression Models to Enhance the Determination Coefficient $R^2$ on Unseen Data</title>
      <link>https://arxiv.org/abs/2503.11749</link>
      <description>arXiv:2503.11749v1 Announce Type: new 
Abstract: Since polynomial regression models are generally quite reliable for data with a linear trend, it is important to note that, in some cases, they may encounter overfitting issues during the training phase, which could result in negative values of the coefficient of determination $R^2$ for unseen data. For this reason, this work proposes the partial implementation of fractional operators in polynomial regression models to generate a fractional regression model. The goal of this proposal is to attempt to mitigate overfitting, which could improve the value of the coefficient of determination for unseen data, compared to the polynomial model, under the assumption that this would contribute to generating predictive models with better performance. The methodology for constructing these fractional regression models is detailed, and examples applicable to both Riemann-Liouville and Caputo fractional operators are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11749v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anthony Torres-Hernandez</dc:creator>
    </item>
    <item>
      <title>Movement Dynamics in Elite Female Soccer Athletes: The Quantile Cube Approach</title>
      <link>https://arxiv.org/abs/2503.11815</link>
      <description>arXiv:2503.11815v1 Announce Type: new 
Abstract: This paper presents an innovative adaptation of existing methodology to investigate external load in elite female soccer athletes using GPS-derived movement data from 23 matches. We developed a quantitative framework to examine velocity, acceleration, and movement angle across game halves, enabling transparent and meaningful performance insights. By constructing a quantile cube to quantify movement patterns, we segmented athletes' movements into distinct velocity, acceleration, and angle quantiles. Statistical analysis revealed significant differences in movement distributions between match halves for individual athletes. Principal Component Analysis (PCA) identified anomalous games with unique movement dynamics, particularly at the start and end of the season. Dirichlet-multinomial regression further explored how factors like athlete position, playing time, and game characteristics influenced movement profiles. This approach provides a structured method for analyzing movement dynamics, revealing external load variations over time and offering insights into performance optimization. The integration of these statistical techniques demonstrates the potential of data-driven strategies to enhance athlete monitoring in soccer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11815v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kendall L. Thomas, Jan Hannig</dc:creator>
    </item>
    <item>
      <title>GPDFlow: Generative Multivariate Threshold Exceedance Modeling via Normalizing Flows</title>
      <link>https://arxiv.org/abs/2503.11822</link>
      <description>arXiv:2503.11822v1 Announce Type: new 
Abstract: The multivariate generalized Pareto distribution (mGPD) is a common method for modeling extreme threshold exceedance probabilities in environmental and financial risk management. Despite its broad applicability, mGPD faces challenges due to the infinite possible parametrizations of its dependence function, with only a few parametric models available in practice. To address this limitation, we introduce GPDFlow, an innovative mGPD model that leverages normalizing flows to flexibly represent the dependence structure. Unlike traditional parametric mGPD approaches, GPDFlow does not impose explicit parametric assumptions on dependence, resulting in greater flexibility and enhanced performance. Additionally, GPDFlow allows direct inference of marginal parameters, providing insights into marginal tail behavior. We derive tail dependence coefficients for GPDFlow, including a bivariate formulation, a $d$-dimensional extension, and an alternative measure for partial exceedance dependence. A general relationship between the bivariate tail dependence coefficient and the generative samples from normalizing flows is discussed. Through simulations and a practical application analyzing the risk among five major US banks, we demonstrate that GPDFlow significantly improves modeling accuracy and flexibility compared to traditional parametric methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11822v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenglei Hu, Daniela Castro-Camilo</dc:creator>
    </item>
    <item>
      <title>Testing Stochastic Block Models Based on Maximum Sampling Entry-Wise Deviations</title>
      <link>https://arxiv.org/abs/2503.11990</link>
      <description>arXiv:2503.11990v1 Announce Type: new 
Abstract: The stochastic block model (SBM) has been widely used to analyze network data. Various goodness-of-fit tests have been proposed to assess the adequacy of model structures. To the best of our knowledge, however, none of the existing approaches are applicable for sparse networks in which the connection probability of any two communities is of order log n/n, and the number of communities is divergent. To fill this gap, we propose a novel goodness-of-fit test for the stochastic block model. The key idea is to construct statistics by sampling the maximum entry-deviations of the adjacency matrix that the negative impacts of network sparsity are alleviated by the sampling process. We demonstrate theoretically that the proposed test statistic converges to the Type-I extreme value distribution under the null hypothesis regardless of the network structure. Accordingly, it can be applied to both dense and sparse networks. In addition, we obtain the asymptotic power against alternatives. Moreover, we introduce a bootstrap-corrected test statistic to improve the finite sample performance, recommend an augmented test statistic to increase the power, and extend the proposed test to the degree-corrected SBM. Simulation studies and two empirical examples with both dense and sparse networks indicate that the proposed method performs well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11990v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujia Wu, Wei Lan, Long Feng, Chih-Ling Tsai</dc:creator>
    </item>
    <item>
      <title>A Bayesian Proportional Mean Model Using Panel Binary Data-An Application to Health and Retirement Study</title>
      <link>https://arxiv.org/abs/2503.11994</link>
      <description>arXiv:2503.11994v1 Announce Type: new 
Abstract: In recurrent event studies, panel binary data arise when subjects are observed at discrete time points and only the recurrent event status within each observation window is recorded. Such data frequently occur in longitudinal studies due to recall difficulties or participants' privacy concerns during follow-ups, necessitating rigorous statistical analysis. While frequentist methods exist for handling such data, Bayesian approaches remain largely unexplored. This article proposes an efficient Bayesian proportional mean model for analysing recurrent events using panel binary data. In addition to the estimation procedure, the article introduces techniques for model validation, selection, and Bayesian influence diagnostics. Simulation studies demonstrate the method's effectiveness and robustness in different practical scenarios. The proposed approach is then applied to analyse the latest version of the Health and Retirement Study dataset, identifying key risk factors influencing doctor visits among the elderly. The analysis is therefore capable of providing valuable insights into healthcare utilisation patterns in ageing populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11994v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pavithra Hariharan, P. G. Sankaran</dc:creator>
    </item>
    <item>
      <title>Auditing Differential Privacy in the Black-Box Setting</title>
      <link>https://arxiv.org/abs/2503.12045</link>
      <description>arXiv:2503.12045v1 Announce Type: new 
Abstract: This paper introduces a novel theoretical framework for auditing differential privacy (DP) in a black-box setting. Leveraging the concept of $f$-differential privacy, we explicitly define type I and type II errors and propose an auditing mechanism based on conformal inference. Our approach robustly controls the type I error rate under minimal assumptions. Furthermore, we establish a fundamental impossibility result, demonstrating the inherent difficulty of simultaneously controlling both type I and type II errors without additional assumptions. Nevertheless, under a monotone likelihood ratio (MLR) assumption, our auditing mechanism effectively controls both errors. We also extend our method to construct valid confidence bands for the trade-off function in the finite-sample regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12045v1</guid>
      <category>stat.ME</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaining Shi, Cong Ma</dc:creator>
    </item>
    <item>
      <title>On self-training of summary data with genetic applications</title>
      <link>https://arxiv.org/abs/2503.12155</link>
      <description>arXiv:2503.12155v1 Announce Type: new 
Abstract: Prediction model training is often hindered by limited access to individual-level data due to privacy concerns and logistical challenges, particularly in biomedical research. Resampling-based self-training presents a promising approach for building prediction models using only summary-level data. These methods leverage summary statistics to sample pseudo datasets for model training and parameter optimization, allowing for model development without individual-level data. Although increasingly used in precision medicine, the general behaviors of self-training remain unexplored. In this paper, we leverage a random matrix theory framework to establish the statistical properties of self-training algorithms for high-dimensional sparsity-free summary data. We demonstrate that, within a class of linear estimators, resampling-based self-training achieves the same asymptotic predictive accuracy as conventional training methods that require individual-level datasets. These results suggest that self-training with only summary data incurs no additional cost in prediction accuracy, while offering significant practical convenience. Our analysis provides several valuable insights and counterintuitive findings. For example, while pseudo-training and validation datasets are inherently dependent, their interdependence unexpectedly cancels out when calculating prediction accuracy measures, preventing overfitting in self-training algorithms. Furthermore, we extend our analysis to show that the self-training framework maintains this no-cost advantage when combining multiple methods or when jointly training on data from different distributions. We numerically validate our findings through simulations and real data analyses using the UK Biobank. Our study highlights the potential of resampling-based self-training to advance genetic risk prediction and other fields that make summary data publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12155v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Buxin Su, Jiaoyang Huang, Jin Jin, Bingxin Zhao</dc:creator>
    </item>
    <item>
      <title>Multivariate Distribution-Free Nonparametric Testing: Generalizing Wilcoxon's Tests via Optimal Transport</title>
      <link>https://arxiv.org/abs/2503.12236</link>
      <description>arXiv:2503.12236v1 Announce Type: new 
Abstract: This paper reviews recent advancements in the application of optimal transport (OT) to multivariate distribution-free nonparametric testing. Inspired by classical rank-based methods, such as Wilcoxon's rank-sum and signed-rank tests, we explore how OT-based ranks and signs generalize these concepts to multivariate settings, while preserving key properties, including distribution-freeness, robustness, and efficiency. Using the framework of asymptotic relative efficiency (ARE), we compare the power of the proposed (generalized Wilcoxon) tests against the Hotelling's $T^2$ test. The ARE lower bounds reveal the Hodges-Lehmann and Chernoff-Savage phenomena in the context of multivariate location testing, underscoring the high power and efficiency of the proposed methods. We also demonstrate how OT-based ranks and signs can be seamlessly integrated with more modern techniques, such as kernel methods, to develop universally consistent, distribution-free tests. Additionally, we present novel results on the construction of consistent and distribution-free kernel-based tests for multivariate symmetry, leveraging OT-based ranks and signs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12236v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhen Huang, Bodhisattva Sen</dc:creator>
    </item>
    <item>
      <title>Prior distributions for Gaussian processes in computer model emulation and calibration</title>
      <link>https://arxiv.org/abs/2503.12257</link>
      <description>arXiv:2503.12257v1 Announce Type: new 
Abstract: This article discusses prior distributions for the parameters of Gaussian processes (GPs) that are widely used as surrogate models to emulate expensive computer simulations. The parameters typically involve mean parameters, a variance parameter, and correlation parameters. These parameters are often estimated by maximum likelihood (MLE). In some scenarios, however, the MLE can be unstable, particularly when the number of simulation runs is small, and some Bayesian estimators display better properties. We introduce default Bayesian priors for the parameters of GPs with isotropic and separable correlation functions for emulating computer simulations with both scalar-valued and vector-valued outputs. We also summarize recent developments of Bayesian priors for calibrating computer models by field or experimental observations. Finally, we review software packages for computer model emulation and calibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12257v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengyang Gu, Victor De Oliveira</dc:creator>
    </item>
    <item>
      <title>A Bayesian location-scale joint model for time-to-event and multivariate longitudinal data with association based on within-individual variability</title>
      <link>https://arxiv.org/abs/2503.12270</link>
      <description>arXiv:2503.12270v1 Announce Type: new 
Abstract: Within-individual variability of health indicators measured over time is becoming commonly used to inform about disease progression. Simple summary statistics (e.g. the standard deviation for each individual) are often used but they are not suited to account for time changes. In addition, when these summary statistics are used as covariates in a regression model for time-to-event outcomes, the estimates of the hazard ratios are subject to regression dilution. To overcome these issues, a joint model is built where the association between the time-to-event outcome and multivariate longitudinal markers is specified in terms of the within-individual variability of the latter. A mixed-effect location-scale model is used to analyse the longitudinal biomarkers, their within-individual variability and their correlation. The time to event is modelled using a proportional hazard regression model, with a flexible specification of the baseline hazard, and the information from the longitudinal biomarkers is shared as a function of the random effects. The model can be used to quantify within-individual variability for the longitudinal markers and their association with the time-to-event outcome. We show through a simulation study the performance of the model in comparison with the standard joint model with constant variance. The model is applied on a dataset of adult women from the UK cystic fibrosis registry, to evaluate the association between lung function, malnutrition and mortality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12270v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Palma, Ruth H Keogh, Siobh\'an B Carr, Rhonda Szczesniak, David Taylor-Robinson, Angela M Wood, Graciela Muniz-Terrera, Jessica K Barrett</dc:creator>
    </item>
    <item>
      <title>Simulation-based Bayesian inference under model misspecification</title>
      <link>https://arxiv.org/abs/2503.12315</link>
      <description>arXiv:2503.12315v1 Announce Type: new 
Abstract: Simulation-based Bayesian inference (SBI) methods are widely used for parameter estimation in complex models where evaluating the likelihood is challenging but generating simulations is relatively straightforward. However, these methods commonly assume that the simulation model accurately reflects the true data-generating process, an assumption that is frequently violated in realistic scenarios. In this paper, we focus on the challenges faced by SBI methods under model misspecification. We consolidate recent research aimed at mitigating the effects of misspecification, highlighting three key strategies: i) robust summary statistics, ii) generalised Bayesian inference, and iii) error modelling and adjustment parameters. To illustrate both the vulnerabilities of popular SBI methods and the effectiveness of misspecification-robust alternatives, we present empirical results on an illustrative example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12315v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan P. Kelly, David J. Warne, David T. Frazier, David J. Nott, Michael U. Gutmann, Christopher Drovandi</dc:creator>
    </item>
    <item>
      <title>Clustered random forests with correlated data for optimal estimation and inference under potential covariate shift</title>
      <link>https://arxiv.org/abs/2503.12634</link>
      <description>arXiv:2503.12634v1 Announce Type: new 
Abstract: We develop Clustered Random Forests, a random forests algorithm for clustered data, arising from independent groups that exhibit within-cluster dependence. The leaf-wise predictions for each decision tree making up clustered random forests takes the form of a weighted least squares estimator, which leverage correlations between observations for improved prediction accuracy. Clustered random forests are shown for certain tree splitting criteria to be minimax rate optimal for pointwise conditional mean estimation, while being computationally competitive with standard random forests. Further, we observe that the optimality of a clustered random forest, with regards to how (population level) optimal weights are chosen within this framework i.e. those that minimise mean squared prediction error, vary under covariate distribution shift. In light of this, we advocate weight estimation to be determined by a user-chosen covariate distribution with respect to which optimal prediction or inference is desired. This highlights a key difference in behaviour, between correlated and independent data, with regards to nonparametric conditional mean estimation under covariate shift. We demonstrate our theoretical findings numerically in a number of simulated and real-world settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12634v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elliot H. Young, Peter B\"uhlmann</dc:creator>
    </item>
    <item>
      <title>A Doubly Robust Instrumental Variable Approach for Estimating Average Treatment Effects in Time-to-Event Data with Unmeasured Confounding: Application to Real-World Data on ICU Patients with Septic Shock</title>
      <link>https://arxiv.org/abs/2503.12666</link>
      <description>arXiv:2503.12666v1 Announce Type: new 
Abstract: Motivated by conflicting conclusions regarding hydrocortisone's treatment effect on ICU patients with vasopressor-dependent septic shock, we developed a novel instrumental variable (IV) estimator to assess the average treatment effect (ATE) in time-to-event data. In real-world data, IV methods are widely used for estimating causal treatment effects in the presence of unmeasured confounding, but existing approaches for time-to-event outcomes are often constrained by strong parametric assumptions and lack desired statistical properties. Based on our derived the efficient influence function (EIF), the proposed estimator possesses double robustness and achieves asymptotic efficiency. It is also flexible to accommodate machine learning models for outcome, treatment, instrument, and censoring for handling complex real-world data. Through extensive simulations, we demonstrate its double robustness, asymptotic normality, and ideal performance in complex data settings. Using electronic health records (EHR) from ICU patients, we identified physician preferences for prescribing hydrocortisone as the IV to evaluate the treatment effect of hydrocortisone on mortality. Our analysis shows no significant benefit or harm of hydrocortisone on these patients. Applying the proposed doubly robust IV method provides reliable estimates in the presence of unmeasured confounders and offers clinicians with valuable evidence-based guidance for prescribing decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12666v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Runjia Li, Victor B. Talisa, Chung-Chou H. Chang</dc:creator>
    </item>
    <item>
      <title>Causal Feature Learning in the Social Sciences</title>
      <link>https://arxiv.org/abs/2503.12784</link>
      <description>arXiv:2503.12784v1 Announce Type: new 
Abstract: Variable selection poses a significant challenge in causal modeling, particularly within the social sciences, where constructs often rely on inter-related factors such as age, socioeconomic status, gender, and race. Indeed, it has been argued that such attributes must be modeled as macro-level abstractions of lower-level manipulable features, in order to preserve the modularity assumption essential to causal inference. This paper accordingly extends the theoretical framework of Causal Feature Learning (CFL). Empirically, we apply the CFL algorithm to diverse social science datasets, evaluating how CFL-derived macrostates compare with traditional microstates in downstream modeling tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12784v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingzhou Huang, Jiuyao Lu, Alexander Williams Tolbert</dc:creator>
    </item>
    <item>
      <title>Comparative Review of Modern Competing Risk Methods in High-dimensional Settings</title>
      <link>https://arxiv.org/abs/2503.12824</link>
      <description>arXiv:2503.12824v1 Announce Type: new 
Abstract: Competing risk analysis accounts for multiple mutually exclusive events, improving risk estimation over traditional survival analysis. Despite methodological advancements, a comprehensive comparison of competing risk methods, especially in high-dimensional settings, remains limited. This study evaluates penalized regression (LASSO, SCAD, MCP), boosting (CoxBoost, CB), random forest (RF), and deep learning (DeepHit, DH) methods for competing risk analysis through extensive simulations, assessing variable selection, estimation accuracy, discrimination, and calibration under diverse data conditions. Our results show that CB achieves the best variable selection, estimation stability, and discriminative ability, particularly in high-dimensional settings. while MCP and SCAD provide superior calibration in $n&gt;p$ scenarios. RF and DH capture nonlinear effects but exhibit instability, with RF showing high false discovery rates and DH suffering from poor calibration. Further, we compare the flexibility of these methods through the analysis of a melanoma gene expression data with survival information. This study provides practical guidelines for selecting competing risk models to ensure robust and interpretable analysis in high-dimensional settings and outlines important directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12824v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul M. Djangang, Summer S. Han, Nilotpal Sanyal</dc:creator>
    </item>
    <item>
      <title>H-AddiVortes: Heteroscedastic (Bayesian) Additive Voronoi Tessellations</title>
      <link>https://arxiv.org/abs/2503.13037</link>
      <description>arXiv:2503.13037v1 Announce Type: new 
Abstract: This paper introduces the Heteroscedastic AddiVortes model, a Bayesian non-parametric regression framework that simultaneously models the conditional mean and variance of a response variable using adaptive Voronoi tessellations. By employing a sum-of-tessellations approach for the mean and a product-of-tessellations approach for the variance, the model provides a flexible and interpretable means to capture complex, predictor-dependent relationships and heteroscedastic patterns in data. This dual-layer representation enables precise inference, even in high-dimensional settings, while maintaining computational feasibility through efficient Markov Chain Monte Carlo (MCMC) sampling and conjugate prior structures. We illustrate the model's capability through both simulated and real-world datasets, demonstrating its ability to capture nuanced variance structures, provide reliable predictive uncertainty quantification, and highlight key predictors influencing both the mean response and its variability. Empirical results show that the Heteroscedastic AddiVortes model offers a substantial improvement in capturing distributional properties compared to both homoscedastic and heteroscedastic alternatives, making it a robust tool for complex regression problems in various applied settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13037v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam J. Stone, John Paul Gosling</dc:creator>
    </item>
    <item>
      <title>A robust score test in g-computation for covariate adjustment in randomized clinical trials leveraging different variance estimators via influence functions</title>
      <link>https://arxiv.org/abs/2503.13066</link>
      <description>arXiv:2503.13066v1 Announce Type: new 
Abstract: G-computation has become a widely used robust method for estimating unconditional (marginal) treatment effects with covariate adjustment in the analysis of randomized clinical trials. Statistical inference in this context typically relies on the Wald test or Wald interval, which can be easily implemented using a consistent variance estimator. However, existing literature suggests that when sample sizes are small or when parameters of interest are near boundary values, Wald-based methods may be less reliable due to type I error rate inflation and insufficient interval coverage. In this article, we propose a robust score test for g-computation estimators in the context of two-sample treatment comparisons. The proposed test is asymptotically valid under simple and stratified (biased-coin) randomization schemes, even when regression models are misspecified. These test statistics can be conveniently computed using existing variance estimators, and the corresponding confidence intervals have closed-form expressions, making them convenient to implement. Through extensive simulations, we demonstrate the superior finite-sample performance of the proposed method. Finally, we apply the proposed method to reanalyze a completed randomized clinical trial. The new analysis using our proposed score test achieves statistical significance, whilst reducing the issue of type I error inflation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13066v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Zhang, Haitao Chu, Lin Liu, Satrajit Roychoudhury</dc:creator>
    </item>
    <item>
      <title>Bayesian Cox model with graph-structured variable selection priors for multi-omics biomarker identification</title>
      <link>https://arxiv.org/abs/2503.13078</link>
      <description>arXiv:2503.13078v1 Announce Type: new 
Abstract: An important goal in cancer research is the survival prognosis of a patient based on a minimal panel of genomic and molecular markers such as genes or proteins. Purely data-driven models without any biological knowledge can produce non-interpretable results. We propose a penalized semiparametric Bayesian Cox model with graph-structured selection priors for sparse identification of multi-omics features by making use of a biologically meaningful graph via a Markov random field (MRF) prior to capturing known relationships between multi-omics features. Since the fixed graph in the MRF prior is for the prior probability distribution, it is not a hard constraint to determine variable selection, so the proposed model can verify known information and has the potential to identify new and novel biomarkers for drawing new biological knowledge. Our simulation results show that the proposed Bayesian Cox model with graph-based prior knowledge results in more trustable and stable variable selection and non-inferior survival prediction, compared to methods modeling the covariates independently without any prior knowledge. The results also indicate that the performance of the proposed model is robust to a partially correct graph in the MRF prior, meaning that in a real setting where not all the true network information between covariates is known, the graph can still be useful. The proposed model is applied to the primary invasive breast cancer patients data in The Cancer Genome Atlas project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13078v1</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias {\O}stmo Hermansen, Manuela Zucknick, Zhi Zhao</dc:creator>
    </item>
    <item>
      <title>Spearman's rho for bivariate zero-inflated data</title>
      <link>https://arxiv.org/abs/2503.13148</link>
      <description>arXiv:2503.13148v1 Announce Type: new 
Abstract: Quantifying the association between two random variables is crucial in applications. Traditional estimation techniques for common association measures, such as Spearman's rank correlation coefficient, $\rho_S$, often fail when data contain ties. This is particularly problematic in zero-inflated contexts and fields like insurance, healthcare, and weather forecasting, where zeros are more frequent and require an extra probability mass. In this paper, we provide a new formulation of Spearman's rho specifically designed for zero-inflated data and propose a novel estimator of Spearman's rho based on our derived expression. Besides, we make our proposed estimator useful in practice by deriving its achievable bounds and suggest how to estimate them. We analyze our method in a comprehensive simulation study and show that our approach overcomes state-of-the-art methods in all the simulated scenarios. Additionally, we illustrate how the proposed theory can be used in practice for a more accurate quantification of association by considering two real-life applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13148v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jasper Arends, Elisa Perrone</dc:creator>
    </item>
    <item>
      <title>fkbma: An R Package for Detecting Tailoring Variables with Free-Knot B-Splines and Bayesian Model Averaging</title>
      <link>https://arxiv.org/abs/2503.13297</link>
      <description>arXiv:2503.13297v1 Announce Type: new 
Abstract: Precision medicine aims to optimize treatment by identifying patient subgroups most likely to benefit from specific interventions. To support this goal, we introduce fkbma, an R package that implements a Bayesian model averaging approach with free-knot B-splines for identifying tailoring variables. The package employs a reversible jump Markov chain Monte Carlo algorithm to flexibly model treatment effect heterogeneity while accounting for uncertainty in both variable selection and non-linear relationships. fkbma provides a comprehensive framework for detecting predictive biomarkers, integrating Bayesian adaptive enrichment strategies, and enabling robust subgroup identification in clinical trials and observational studies. This paper details the statistical methodology underlying fkbma, outlines its computational implementation, and demonstrates its application through simulations and real-world examples. The package's flexibility makes it a valuable tool for precision medicine research, offering a principled approach to treatment personalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13297v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lara Maleyeff, Shirin Golchi, Erica E. M. Moodie</dc:creator>
    </item>
    <item>
      <title>The Relativity of Causal Knowledge</title>
      <link>https://arxiv.org/abs/2503.11718</link>
      <description>arXiv:2503.11718v1 Announce Type: cross 
Abstract: Recent advances in artificial intelligence reveal the limits of purely predictive systems and call for a shift toward causal and collaborative reasoning. Drawing inspiration from the revolution of Grothendieck in mathematics, we introduce the relativity of causal knowledge, which posits structural causal models (SCMs) are inherently imperfect, subjective representations embedded within networks of relationships. By leveraging category theory, we arrange SCMs into a functor category and show that their observational and interventional probability measures naturally form convex structures. This result allows us to encode non-intervened SCMs with convex spaces of probability measures. Next, using sheaf theory, we construct the network sheaf and cosheaf of causal knowledge. These structures enable the transfer of causal knowledge across the network while incorporating interventional consistency and the perspective of the subjects, ultimately leading to the formal, mathematical definition of relative causal knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11718v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.CT</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriele D'Acunto, Claudio Battiloro</dc:creator>
    </item>
    <item>
      <title>The Architecture and Evaluation of Bayesian Neural Networks</title>
      <link>https://arxiv.org/abs/2503.11808</link>
      <description>arXiv:2503.11808v1 Announce Type: cross 
Abstract: As modern neural networks get more complex, specifying a model with high predictive performance and sound uncertainty quantification becomes a more challenging task. Despite some promising theoretical results on the true posterior predictive distribution of Bayesian neural networks, the properties of even the most commonly used posterior approximations are often questioned. Computational burdens and intractable posteriors expose miscalibrated Bayesian neural networks to poor accuracy and unreliable uncertainty estimates. Approximate Bayesian inference aims to replace unknown and intractable posterior distributions with some simpler but feasible distributions. The dimensions of modern deep models coupled with the lack of identifiability make Markov chain Monte Carlo tremendously expensive and unable to fully explore the multimodal posterior. On the other hand, variational inference benefits from improved computational complexity but lacks the asymptotical guarantees of sampling-based inference and tends to concentrate around a single mode. The performance of both approaches heavily depends on architectural choices; this paper aims to shed some light on this, by considering the computational costs, accuracy and uncertainty quantification in different scenarios including large width and out-of-sample data. To improve posterior exploration, different model averaging and ensembling techniques are studied, along with their benefits on predictive performance. In our experiments, variational inference overall provided better uncertainty quantification than Markov chain Monte Carlo; further, stacking and ensembles of variational approximations provided comparable to Markov chain Monte Carlo accuracy at a much-reduced cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11808v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alisa Sheinkman, Sara Wade</dc:creator>
    </item>
    <item>
      <title>Spatiotemporal clustering of GHGs emissions in Europe: exploring the role of spatial component</title>
      <link>https://arxiv.org/abs/2503.11909</link>
      <description>arXiv:2503.11909v1 Announce Type: cross 
Abstract: In this study, we propose a novel application of spatiotemporal clustering in the environmental sciences, with a particular focus on regionalised time series of greenhouse gases (GHGs) emissions from a range of economic sectors. Utilising a hierarchical spatiotemporal clustering methodology, we analyse yearly time series of emissions by gases and sectors from 1990 to 2022 for European regions at the NUTS-2 level. While the clustering algorithm inherently incorporates spatial information based on geographical distance, the extent to which space contributes to the definition of groups still requires further exploration. To address this gap in the literature, we propose a novel indicator, namely the Joint Inertia, which quantifies the contribution of spatial distances when integrated with other features. Through a simulation experiment, we explore the relationship between the Joint Inertia and the relevance of geography in exploiting the groups structure under several configurations of spatial and features patterns, providing insights into the behaviour and potential of the proposed indicator. The empirical findings demonstrate the relevance of the spatial component in identifying emission patterns and dynamics, and the results reveal significant heterogeneity across clusters in trends and dynamics by gases and sectors. This reflects the heterogeneous economic and industrial characteristics of European regions. The study highlights the importance of the spatial and temporal dimensions in understanding GHGs emissions, offering baseline insights for future spatiotemporal modelling and supporting more targeted and regionally informed environmental policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11909v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caterina Morelli, Paolo Maranzano, Philipp Otto</dc:creator>
    </item>
    <item>
      <title>Nonlinear Principal Component Analysis with Random Bernoulli Features for Process Monitoring</title>
      <link>https://arxiv.org/abs/2503.12456</link>
      <description>arXiv:2503.12456v1 Announce Type: cross 
Abstract: The process generates substantial amounts of data with highly complex structures, leading to the development of numerous nonlinear statistical methods. However, most of these methods rely on computations involving large-scale dense kernel matrices. This dependence poses significant challenges in meeting the high computational demands and real-time responsiveness required by online monitoring systems. To alleviate the computational burden of dense large-scale matrix multiplication, we incorporate the bootstrap sampling concept into random feature mapping and propose a novel random Bernoulli principal component analysis method to efficiently capture nonlinear patterns in the process. We derive a convergence bound for the kernel matrix approximation constructed using random Bernoulli features, ensuring theoretical robustness. Subsequently, we design four fast process monitoring methods based on random Bernoulli principal component analysis to extend its nonlinear capabilities for handling diverse fault scenarios. Finally, numerical experiments and real-world data analyses are conducted to evaluate the performance of the proposed methods. Results demonstrate that the proposed methods offer excellent scalability and reduced computational complexity, achieving substantial cost savings with minimal performance loss compared to traditional kernel-based approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12456v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ke Chen, Dandan Jiang</dc:creator>
    </item>
    <item>
      <title>Functional Factor Regression with an Application to Electricity Price Curve Modeling</title>
      <link>https://arxiv.org/abs/2503.12611</link>
      <description>arXiv:2503.12611v1 Announce Type: cross 
Abstract: We propose a function-on-function linear regression model for time-dependent curve data that is consistently estimated by imposing factor structures on the regressors. An integral operator based on cross-covariances identifies two components for each functional regressor: a predictive low-dimensional component, along with associated factors that are guaranteed to be correlated with the dependent variable, and an infinite-dimensional component that has no predictive power. In order to consistently estimate the correct number of factors for each regressor, we introduce a functional eigenvalue difference test. Our setting allows us to construct a novel central limit theorem for the regression parameters in a fully functional model, making it possible to construct confidence bands and conduct statistical inference. The model is applied to forecast electricity price curves in three different energy markets. Its prediction accuracy is found to be comparable to popular machine learning approaches, while providing statistically valid inference and interpretable insights into the conditional correlation structures of electricity prices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12611v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sven Otto, Luis Winter</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Weighted Sample Average Approximation in Contextual Stochastic Optimization</title>
      <link>https://arxiv.org/abs/2503.12747</link>
      <description>arXiv:2503.12747v1 Announce Type: cross 
Abstract: Contextual stochastic optimization provides a framework for decision-making under uncertainty incorporating observable contextual information through covariates. We analyze statistical inference for weighted sample average approximation (wSAA), a widely-used method for solving contextual stochastic optimization problems. We first establish central limit theorems for wSAA estimates of optimal values when problems can be solved exactly, characterizing how estimation uncertainty scales with covariate sample size. We then investigate practical scenarios with computational budget constraints, revealing a fundamental tradeoff between statistical accuracy and computational cost as sample sizes increase. Through central limit theorems for budget-constrained wSAA estimates, we precisely characterize this statistical-computational tradeoff. We also develop "over-optimizing" strategies for solving wSAA problems that ensure valid statistical inference. Extensive numerical experiments on both synthetic and real-world datasets validate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12747v1</guid>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yanyuan Wang, Xiaowei Zhang</dc:creator>
    </item>
    <item>
      <title>Stein's method of moment estimators for local dependency exponential random graph models</title>
      <link>https://arxiv.org/abs/2503.13191</link>
      <description>arXiv:2503.13191v1 Announce Type: cross 
Abstract: Providing theoretical guarantees for parameter estimation in exponential random graph models is a largely open problem. While maximum likelihood estimation has theoretical guarantees in principle, verifying the assumptions for these guarantees to hold can be very difficult. Moreover, in complex networks, numerical maximum likelihood estimation is computer-intensive and may not converge in reasonable time. To ameliorate this issue, local dependency exponential random graph models have been introduced, which assume that the network consists of many independent exponential random graphs. In this setting, progress towards maximum likelihood estimation has been made. However the estimation is still computer-intensive. Instead, we propose to use so-called Stein estimators: we use the Stein characterizations to obtain new estimators for local dependency exponential random graph models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13191v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adrian Fischer, Gesine Reinert, Wenkai Xu</dc:creator>
    </item>
    <item>
      <title>Difference-in-Differences Designs: A Practitioner's Guide</title>
      <link>https://arxiv.org/abs/2503.13323</link>
      <description>arXiv:2503.13323v1 Announce Type: cross 
Abstract: Difference-in-Differences (DiD) is arguably the most popular quasi-experimental research design. Its canonical form, with two groups and two periods, is well-understood. However, empirical practices can be ad hoc when researchers go beyond that simple case. This article provides an organizing framework for discussing different types of DiD designs and their associated DiD estimators. It discusses covariates, weights, handling multiple periods, and staggered treatments. The organizational framework, however, applies to other extensions of DiD methods as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13323v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrew Baker, Brantly Callaway, Scott Cunningham, Andrew Goodman-Bacon, Pedro H. C. Sant'Anna</dc:creator>
    </item>
    <item>
      <title>Multivariate Sparse Functional Linear Discriminant Analysis: An Application to Inflammatory Bowel Disease Classification</title>
      <link>https://arxiv.org/abs/2503.13372</link>
      <description>arXiv:2503.13372v1 Announce Type: cross 
Abstract: Inflammatory Bowel Disease (IBD), including Crohn's Disease (CD) and Ulcerative Colitis (UC), presents significant public health challenges due to its complex etiology. Motivated by the IBD study of the Integrative Human Microbiome Project, our objective is to identify microbial pathways that distinguish between CD, UC and non-IBD over time. Most current research relies on simplistic analyses that examine one variable or time point at a time, or address binary classification problems, limiting our understanding of the dynamic interactions within the microbiome over time. To address these limitations, we develop a novel functional data analysis approach for discriminant analysis of multivariate functional data that can effectively handle multiple high-dimensional predictors, sparse time points, and categorical outcomes. Our method seeks linear combinations of functions (i.e., discriminant functions) that maximize separation between two or more groups over time. We impose a sparsity-inducing penalty when estimating the discriminant functions, allowing us to identify relevant discriminating variables over time. Applications of our method to the motivating data identified microbial features related to mucin degradation, amino acid metabolism, and peptidoglycan recognition, which are implicated in the progression and development of IBD. Furthermore, our method highlighted the role of multiple vitamin B deficiencies in the context of IBD. By moving beyond traditional analytical frameworks, our innovative approach holds the potential for uncovering clinically meaningful discoveries in IBD research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13372v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Limeng Liu, Guannan Wang, Sandra E. Safo</dc:creator>
    </item>
    <item>
      <title>Sequential Kernel Embedding for Mediated and Time-Varying Dose Response Curves</title>
      <link>https://arxiv.org/abs/2111.03950</link>
      <description>arXiv:2111.03950v5 Announce Type: replace 
Abstract: We propose simple nonparametric estimators for mediated and time-varying dose response curves based on kernel ridge regression. By embedding Pearl's mediation formula and Robins' g-formula with kernels, we allow treatments, mediators, and covariates to be continuous in general spaces, and also allow for nonlinear treatment-confounder feedback. Our key innovation is a reproducing kernel Hilbert space technique called sequential kernel embedding, which we use to construct simple estimators that account for complex feedback. Our estimators preserve the generality of classic identification while also achieving nonasymptotic uniform rates. In nonlinear simulations with many covariates, we demonstrate strong performance. We estimate mediated and time-varying dose response curves of the US Job Corps, and clean data that may serve as a benchmark in future work. We extend our results to mediated and time-varying treatment effects and counterfactual distributions, verifying semiparametric efficiency and weak convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.03950v5</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rahul Singh, Liyuan Xu, Arthur Gretton</dc:creator>
    </item>
    <item>
      <title>Simultaneous inference for generalized linear models with unmeasured confounders</title>
      <link>https://arxiv.org/abs/2309.07261</link>
      <description>arXiv:2309.07261v5 Announce Type: replace 
Abstract: Tens of thousands of simultaneous hypothesis tests are routinely performed in genomic studies to identify differentially expressed genes. However, due to unmeasured confounders, many standard statistical approaches may be substantially biased. This paper investigates the large-scale hypothesis testing problem for multivariate generalized linear models in the presence of confounding effects. Under arbitrary confounding mechanisms, we propose a unified statistical estimation and inference framework that harnesses orthogonal structures and integrates linear projections into three key stages. It begins by disentangling marginal and uncorrelated confounding effects to recover the latent coefficients. Subsequently, latent factors and primary effects are jointly estimated through lasso-type optimization. Finally, we incorporate projected and weighted bias-correction steps for hypothesis testing. Theoretically, we establish the identification conditions of various effects and non-asymptotic error bounds. We show effective Type-I error control of asymptotic $z$-tests as sample and response sizes approach infinity. Numerical experiments demonstrate that the proposed method controls the false discovery rate by the Benjamini-Hochberg procedure and is more powerful than alternative methods. By comparing single-cell RNA-seq counts from two groups of samples, we demonstrate the suitability of adjusting confounding effects when significant covariates are absent from the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.07261v5</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>q-bio.GN</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin-Hong Du, Larry Wasserman, Kathryn Roeder</dc:creator>
    </item>
    <item>
      <title>Causal Quantile Treatment Effects with missing data by double-sampling</title>
      <link>https://arxiv.org/abs/2310.09239</link>
      <description>arXiv:2310.09239v3 Announce Type: replace 
Abstract: Causal weighted quantile treatment effects (WQTE) are a useful complement to standard causal contrasts that focus on the mean when interest lies at the tails of the counterfactual distribution. To-date, however, methods for estimation and inference regarding causal WQTEs have assumed complete data on all relevant factors. In most practical settings, however, data will be missing or incomplete data, particularly when the data are not collected for research purposes, as is the case for electronic health records and disease registries. Furthermore, such data sources may be particularly susceptible to the outcome data being missing-not-at-random (MNAR). In this paper, we consider the use of double-sampling, through which the otherwise missing data are ascertained on a sub-sample of study units, as a strategy to mitigate bias due to MNAR data in the estimation of causal WQTEs. With the additional data in-hand, we present identifying conditions that do not require assumptions regarding missingness in the original data. We then propose a novel inverse-probability weighted estimator and derive its asymptotic properties, both pointwise at specific quantiles and uniformly across a range of quantiles over some compact subset of (0,1), allowing the propensity score and double-sampling probabilities to be estimated. For practical inference, we develop a bootstrap method that can be used for both pointwise and uniform inference. A simulation study is conducted to examine the finite sample performance of the proposed estimators. The proposed method is illustrated with data from an EHR-based study examining the relative effects of two bariatric surgery procedures on BMI loss at 3 years post-surgery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.09239v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/biomtc/ujaf038</arxiv:DOI>
      <dc:creator>Shuo Sun, Sebastien Haneuse, Alexander W. Levis, Catherine Lee, David E Arterburn, Heidi Fischer, Susan Shortreed, Rajarshi Mukherjee</dc:creator>
    </item>
    <item>
      <title>Scalable Causal Structure Learning via Amortized Conditional Independence Testing</title>
      <link>https://arxiv.org/abs/2310.16626</link>
      <description>arXiv:2310.16626v2 Announce Type: replace 
Abstract: Controlling false positives (Type I errors) through statistical hypothesis testing is a foundation of modern scientific data analysis. Existing causal structure discovery algorithms either do not provide Type I error control or cannot scale to the size of modern scientific datasets. We consider a variant of the causal discovery problem with two sets of nodes, where the only edges of interest form a bipartite causal subgraph between the sets. We develop Scalable Causal Structure Learning (SCSL), a method for causal structure discovery on bipartite subgraphs that provides Type I error control. SCSL recasts the discovery problem as a simultaneous hypothesis testing problem and uses discrete optimization over the set of possible confounders to obtain an upper bound on the test statistic for each edge. Semi-synthetic simulations demonstrate that SCSL scales to handle graphs with hundreds of nodes while maintaining error control and good power. We demonstrate the practical applicability of the method by applying it to a cancer dataset to reveal connections between somatic gene mutations and metastases to different tissues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16626v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Leiner, Brian Manzo, Aaditya Ramdas, Wesley Tansey</dc:creator>
    </item>
    <item>
      <title>Rejection Sampling with Vertical Weighted Strips</title>
      <link>https://arxiv.org/abs/2401.09696</link>
      <description>arXiv:2401.09696v2 Announce Type: replace 
Abstract: A number of distributions that arise in statistical applications can be expressed in the form of a weighted density: the product of a base density and a nonnegative weight function. Generating variates from such a distribution may be nontrivial and can involve an intractable normalizing constant. Rejection sampling may be used to generate exact draws, but requires formulation of a suitable proposal distribution. To be practically useful, the proposal must both be convenient to sample from and not reject candidate draws too frequently. A well-known approach to design a proposal involves decomposing the target density into a finite mixture, whose components may correspond to a partition of the support. This work considers such a construction that focuses on majorization of the weight function. This approach may be applicable when assumptions for adaptive rejection sampling and related algorithms are not met. An upper bound for the rejection probability based on this construction can be expressed to evaluate the efficiency of the proposal before sampling. A method to partition the support is considered where regions are bifurcated based on their contribution to the bound. Several applications based on the von Mises Fisher distribution are presented to illustrate the framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09696v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew M. Raim, James A. Livsey, Kyle M. Irimata</dc:creator>
    </item>
    <item>
      <title>Recovery and inference of causal effects with sequential adjustment for confounding and attrition</title>
      <link>https://arxiv.org/abs/2401.16990</link>
      <description>arXiv:2401.16990v5 Announce Type: replace 
Abstract: Confounding bias and selection bias bring two significant challenges to the validity of conclusions drawn from applied causal inference. The latter can stem from informative missingness, such as in cases of attrition. We introduce the Sequential Adjustment Criteria (SAC), which extend available graphical conditions for recovering causal effects from confounding and attrition using sequential regressions, allowing for the inclusion of post-exposure and forbidden variables in the adjustment sets. We propose an estimator for the recovered Average Treatment Effect (ATE) based on Targeted Minimum-Loss Estimation (TMLE), which exhibits multiple robustness under certain technical conditions. This approach ensures consistency even in scenarios where the Double Inverse Probability Weighting (DIPW) and the naive plug-in sequential regressions approaches fall short. Through a simulation study, we assess the performance of the proposed estimator against alternative methods across different graph setups and model specification scenarios. As a motivating application, we examine the effect of pharmacological treatment for Attention-Deficit/Hyperactivity Disorder (ADHD) upon the scores obtained by diagnosed Norwegian schoolchildren in national tests using observational data ($n=9\,352$). Our findings align with the accumulated clinical evidence, affirming a positive but small impact of medication on academic achievement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16990v5</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Johan de Aguas, Johan Pensar, Tom\'as Varnet P\'erez, Guido Biele</dc:creator>
    </item>
    <item>
      <title>Logistic-beta processes for dependent random probabilities with beta marginals</title>
      <link>https://arxiv.org/abs/2402.07048</link>
      <description>arXiv:2402.07048v3 Announce Type: replace 
Abstract: The beta distribution serves as a canonical tool for modeling probabilities in statistics and machine learning. However, there is limited work on flexible and computationally convenient stochastic process extensions for modeling dependent random probabilities. We propose a novel stochastic process called the logistic-beta process, whose logistic transformation yields a stochastic process with common beta marginals. Logistic-beta processes can model dependence on both discrete and continuous domains, such as space or time, and have a flexible dependence structure through correlation kernels. Moreover, its normal variance-mean mixture representation leads to effective posterior inference algorithms. We show how the proposed logistic-beta process can be used to design computationally tractable dependent Bayesian nonparametric models, including dependent Dirichlet processes and extensions. We illustrate the benefits through nonparametric binary regression and conditional density estimation examples, both in simulation studies and in a pregnancy outcome application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07048v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changwoo J. Lee, Alessandro Zito, Huiyan Sang, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Change Point Detection in Dynamic Graphs with Decoder-only Latent Space Model</title>
      <link>https://arxiv.org/abs/2404.04719</link>
      <description>arXiv:2404.04719v3 Announce Type: replace 
Abstract: This manuscript studies the unsupervised change point detection problem in time series of graphs using a decoder-only latent space model. The proposed framework consists of learnable prior distributions for low-dimensional graph representations and of a decoder that bridges the observed graphs and latent representations. The prior distributions of the latent spaces are learned from the observed data as empirical Bayes to assist change point detection. Specifically, the model parameters are estimated via maximum approximate likelihood, with a Group Fused Lasso regularization imposed on the prior parameters. The augmented Lagrangian is solved via Alternating Direction Method of Multipliers, and Langevin Dynamics are recruited for posterior inference. Simulation studies show good performance of the latent space model in supporting change point detection and real data experiments yield change points that align with significant events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04719v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yik Lun Kei, Jialiang Li, Hangjian Li, Yanzhen Chen, Oscar Hernan Madrid Padilla</dc:creator>
    </item>
    <item>
      <title>Nested Instrumental Variables Design: Switcher Average Treatment Effect, Identification, Efficient Estimation and Generalizability</title>
      <link>https://arxiv.org/abs/2405.07102</link>
      <description>arXiv:2405.07102v4 Announce Type: replace 
Abstract: Instrumental variables (IV) are a commonly used tool to estimate causal effects from non-randomized data. An archetype of an IV is a randomized trial with non-compliance where the randomized treatment assignment serves as an IV for the non-ignorable treatment received. Under a monotonicity assumption, a valid IV non-parametrically identifies the average treatment effect among a non-identified, latent complier subgroup, whose generalizability is often under debate. In many studies, there could exist multiple versions of an IV, for instance, different nudges to take the same treatment in different study sites in a multicentre clinical trial. These different versions of an IV may result in different compliance rates and offer a unique opportunity to study IV estimates' generalizability. In this article, we introduce a novel nested IV assumption and study identification of the average treatment effect among two latent subgroups: always-compliers and switchers, who are defined based on the joint potential treatment received under two versions of a binary IV. We derive the efficient influence function for the SWitcher Average Treatment Effect (SWATE) under a non-parametric model and propose efficient estimators. We then propose formal statistical tests of the generalizability of IV estimates under the nested IV framework. We apply the proposed method to the Prostate, Lung, Colorectal and Ovarian (PLCO) Cancer Screening Trial and study the causal effect of colorectal cancer screening and its generalizability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07102v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Wang, Ying-Qi Zhao, Oliver Dukes, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>Item response parameter estimation performance using Gaussian quadrature and Laplace</title>
      <link>https://arxiv.org/abs/2405.20164</link>
      <description>arXiv:2405.20164v2 Announce Type: replace 
Abstract: Item parameter estimation in pharmacometric item response theory (IRT) models is predominantly performed using the Laplace estimation algorithm as implemented in NONMEM. In psychometrics a wide range of different software tools, including several packages for the open-source software R for implementation of IRT are also available. Each have their own set of benefits and limitations and to date a systematic comparison of the primary estimation algorithms has not been evaluated. A simulation study evaluating varying number of hypothetical sample sizes and item scenarios at baseline was performed using both Laplace and Gauss-hermite quadrature (GHQ-EM). In scenarios with at least 20 items and more than 100 subjects, item parameters were estimated with good precision and were similar between estimation algorithms as demonstrated by several measures of bias and precision. The minimal differences observed for certain parameters or sample size scenarios were reduced when translating to the total score scale. The ease of use, speed of estimation and relative accuracy of the GHQ-EM method employed in mirt make it an appropriate alternative or supportive analytical approach to NONMEM for potential pharmacometrics IRT applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20164v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leticia Arrington (Uppsala University), Sebastian Ueckert (Uppsala University)</dc:creator>
    </item>
    <item>
      <title>Modeling heterogeneity in higher-order moments while preserving mean and variance: application to spatio-temporal modeling</title>
      <link>https://arxiv.org/abs/2407.05288</link>
      <description>arXiv:2407.05288v2 Announce Type: replace 
Abstract: In this study, we propose a general model capable of addressing heterogeneity in higher-order moments while preserving mean and variance, including the t, Laplace, and skew-normal distributions as special cases. Our model flexibly accommodates variations in tail heaviness and asymmetry at each data point while maintaining interpretability similar to normal distribution models. Notably, it is closed under linear transformations and provides explicit analytical expressions for skewness and kurtosis. The proposed model is applied to spatial and temporal data analysis, demonstrating that its properties vary based on the chosen matrix decomposition approach. To facilitate efficient inference, we develop a Bayesian estimation method using data augmentation, which is particularly effective for temporal models. Simulation studies confirm that accounting for heterogeneity in higher-order moments enhances parameter estimation accuracy and predictive performance. To illustrate real-world applicability, we analyze production functions across U.S. states. The results indicate that our model effectively captures heterogeneity in higher-order moments, leading to superior model fit in empirical data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05288v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hajime Kuno, Daisuke Murakami</dc:creator>
    </item>
    <item>
      <title>Assessing Spatial Disparities: A Bayesian Linear Regression Approach</title>
      <link>https://arxiv.org/abs/2407.19171</link>
      <description>arXiv:2407.19171v3 Announce Type: replace 
Abstract: Epidemiological investigations of regionally aggregated spatial data often involve detecting spatial health disparities among neighboring regions on a map of disease mortality or incidence rates. Analyzing such data introduces spatial dependence among the health outcomes and seeks to report statistically significant spatial disparities by delineating boundaries that separate neighboring regions with disparate health outcomes. However, there are statistical challenges to appropriately defining what constitutes a spatial disparity and to construct robust probabilistic inference for spatial disparities. We enrich the familiar Bayesian linear regression framework to introduce spatial autoregression and offer model-based detection of spatial disparities. We derive exploitable analytical tractability that considerably accelerates computation. Simulation experiments conducted over a county map of the entire United States demonstrate the effectiveness of our method and we apply our method to a data set from the Institute of Health Metrics and Evaluation (IHME) on age-standardized US county-level estimates of lung cancer mortality rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19171v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Lin Wu, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Penalized Principal Component Analysis for Large-dimension Factor Model with Group Pursuit</title>
      <link>https://arxiv.org/abs/2407.19378</link>
      <description>arXiv:2407.19378v3 Announce Type: replace 
Abstract: This paper investigates the intrinsic group structures within the framework of large-dimensional approximate factor models, which portrays homogeneous effects of the common factors on the individuals that fall into the same group. To this end, we propose a fusion Penalized Principal Component Analysis (PPCA) method and derive a closed-form solution for the $\ell_2$-norm optimization problem. We also show the asymptotic properties of our proposed PPCA estimates. With the PPCA estimates as an initialization, we identify the unknown group structure by a combination of the agglomerative hierarchical clustering algorithm and an information criterion. Then the factor loadings and factor scores are re-estimated conditional on the identified latent groups. Under some regularity conditions, we establish the consistency of the membership estimators as well as that of the group number estimator derived from the information criterion. Theoretically, we show that the post-clustering estimators for the factor loadings and factor scores with group pursuit achieve efficiency gains compared to the estimators by conventional PCA method. Thorough numerical studies validate the established theory and a real financial example illustrates the practical usefulness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19378v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yong He, Dong Liu, Guangming Pan, Yiming Wang</dc:creator>
    </item>
    <item>
      <title>Interval Estimation of Coefficients in Penalized Regression Models of Insurance Data</title>
      <link>https://arxiv.org/abs/2410.01008</link>
      <description>arXiv:2410.01008v3 Announce Type: replace 
Abstract: The Tweedie exponential dispersion family is a popular choice among many to model insurance losses that consist of zero-inflated semicontinuous data. In such data, it is often important to obtain credibility (inference) of the most important features that describe the endogenous variables. Post-selection inference is the standard procedure in statistics to obtain confidence intervals of model parameters after performing a feature extraction procedure. For a linear model, the lasso estimate often has non-negligible estimation bias for large coefficients corresponding to exogenous variables. To have valid inference on those coefficients, it is necessary to correct the bias of the lasso estimate. Traditional statistical methods, such as hypothesis testing or standard confidence interval construction might lead to incorrect conclusions during post-selection, as they are generally too optimistic. Here we discuss a few methodologies for constructing confidence intervals of the coefficients after feature selection in the Generalized Linear Model (GLM) family with application to insurance data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01008v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alokesh Manna, Zijian Huang, Dipak K. Dey, Yuwen Gu, Robin He</dc:creator>
    </item>
    <item>
      <title>Longitudinal Causal Inference with Selective Eligibility</title>
      <link>https://arxiv.org/abs/2410.17864</link>
      <description>arXiv:2410.17864v2 Announce Type: replace 
Abstract: Dropout poses a significant challenge to causal inference in longitudinal studies with time-varying treatments. However, existing research does not simultaneously address dropout and time-varying treatments. We examine selective eligibility, an important yet overlooked source of non-ignorable dropout in such settings. This problem arises when a unit's prior treatment history influences its eligibility for subsequent treatments, a common scenario in medical and other settings. We propose a general methodological framework for longitudinal causal inference with selective eligibility. By focusing on a subgroup of units who would become eligible for treatment given a specific past treatment sequence, we define the time-specific eligible treatment effect and expected number of outcome events under a treatment sequence of interest. Under a generalized version of sequential ignorability, we derive two nonparametric identification formulae, each leveraging different parts of the observed data distribution. We then derive the efficient influence function of each causal estimand, yielding the corresponding doubly robust estimator. Finally, we apply the proposed methodology to an impact evaluation of a pre-trial risk assessment instrument in the criminal justice system, in which selective eligibility arises due to recidivism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17864v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhichao Jiang, Eli Ben-Michael, D. James Greiner, Ryan Halen, Kosuke Imai</dc:creator>
    </item>
    <item>
      <title>BudgetIV: Optimal Partial Identification of Causal Effects with Mostly Invalid Instruments</title>
      <link>https://arxiv.org/abs/2411.06913</link>
      <description>arXiv:2411.06913v2 Announce Type: replace 
Abstract: Instrumental variables (IVs) are widely used to estimate causal effects in the presence of unobserved confounding between exposure and outcome. An IV must affect the outcome exclusively through the exposure and be unconfounded with the outcome. We present a framework for relaxing either or both of these strong assumptions with tuneable and interpretable budget constraints. Our algorithm returns a feasible set of causal effects that can be identified exactly given relevant covariance parameters. The feasible set may be disconnected but is a finite union of convex subsets. We discuss conditions under which this set is sharp, i.e., contains all and only effects consistent with the background assumptions and the joint distribution of observable variables. Our method applies to a wide class of semiparametric models, and we demonstrate how its ability to select specific subsets of instruments confers an advantage over convex relaxations in both linear and nonlinear settings. We also adapt our algorithm to form confidence sets that are asymptotically valid under a common statistical assumption from the Mendelian randomization literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06913v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordan Penn, Lee M. Gunderson, Gecia Bravo-Hermsdorff, Ricardo Silva, David S. Watson</dc:creator>
    </item>
    <item>
      <title>Rethinking the Win Ratio: A Causal Framework for Hierarchical Outcome Analysis</title>
      <link>https://arxiv.org/abs/2501.16933</link>
      <description>arXiv:2501.16933v2 Announce Type: replace 
Abstract: Quantifying causal effects in the presence of complex and multivariate outcomes is a key challenge to evaluate treatment effects. For hierarchical multivarariates outcomes, the FDA recommends the Win Ratio and Generalized Pairwise Comparisons approaches. However, as far as we know, these empirical methods lack causal or statistical foundations to justify their broader use in recent studies. To address this gap, we establish causal foundations for hierarchical comparison methods. We define related causal effect measures, and highlight that depending on the methodology used to compute Win Ratios or Net Benefits of treatments, the causal estimand targeted can be different, as proved by our consistency results. Quite dramatically, it appears that the causal estimand related to the historical estimation approach can yield reversed and incorrect treatment recommendations in heterogeneous populations, as we illustrate through striking examples. In order to compensate for this fallacy, we introduce a novel, individual-level yet identifiable causal effect measure that better approximates the ideal, non-identifiable individual-level estimand. We prove that computing Win Ratio or Net Benefits using a Nearest Neighbor pairing approach between treated and controlled patients, an approach that can be seen as an extreme form of stratification, leads to estimating this new causal estimand measure. We extend our methods to observational settings via propensity weighting, distributional regression to address the curse of dimensionality, and a doubly robust framework. We prove the consistency of our methods, and the double robustness of our augmented estimator. Finally, we validate our approach using synthetic data and on CRASH-3, a major clinical trial focused on assessing the effects of tranexamic acid in patients with traumatic brain injury.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16933v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathieu Even, Julie Josse</dc:creator>
    </item>
    <item>
      <title>The Probability of Tiered Benefit: Partial Identification with Robust and Stable Inference</title>
      <link>https://arxiv.org/abs/2502.10049</link>
      <description>arXiv:2502.10049v2 Announce Type: replace 
Abstract: We define the Probability of Tiered Benefit in scenarios with a binary exposure and an outcome that is either categorical with $K \geq 2$ ordered tiers or continuous partitioned by $K-1$ fixed thresholds into disjoint intervals. Similarly to other pure counterfactual queries, this parameter is not $g$-identifiable without additional assumptions. We demonstrate that strong monotonicity does not suffice for point identification when $K \geq 3$ and provide sharp bounds both with and without such constraint. Inference and uncertainty quantification for these bounds are challenging tasks due to potential nonregularity induced by ambiguities in the underlying individualized optimization problems. Such ambiguities can arise from immunities or null treatment effects in subpopulations with positive probability, affecting the lower bound estimate and hindering conservative inference. To address these issues, we extend the available Stabilized One-Step Correction (S1S) procedure by incorporating stratum-specific stabilizing matrices. Through simulations, we illustrate the benefits of this approach over existing alternatives. We apply our method to estimate bounds on the probabilities of tiered benefit and harm from pharmacological treatment for ADHD upon academic achievement, employing observational data from diagnosed Norwegian schoolchildren. Our findings indicate that while girls and children with low prior test performance could have moderate chances of both benefit and harm from treatment, a clear-cut recommendation remains uncertain across all strata.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10049v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Johan de Aguas, Sebastian Krumscheid, Johan Pensar, Guido Biele</dc:creator>
    </item>
    <item>
      <title>Nonparametric spectral density estimation from irregularly sampled data</title>
      <link>https://arxiv.org/abs/2503.00492</link>
      <description>arXiv:2503.00492v2 Announce Type: replace 
Abstract: We introduce a nonparametric spectral density estimator for continuous-time and continuous-space processes measured at fully irregular locations. Our estimator is constructed using a weighted nonuniform Fourier sum whose weights yield a high-accuracy quadrature rule with respect to a user-specified window function. The resulting estimator significantly reduces the aliasing seen in periodogram approaches and least squares spectral analysis, sidesteps the dangers of ill-conditioning of the nonuniform Fourier inverse problem, and can be adapted to a wide variety of irregular sampling settings. After a discussion of methods for computing the necessary weights and a theoretical analysis of sources of bias, we close with demonstrations of the method's efficacy, including for processes that exhibit very slow spectral decay and for processes in multiple dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00492v2</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher J. Geoga, Paul G. Beckman</dc:creator>
    </item>
    <item>
      <title>On "confirmatory" methodological research in statistics and related fields</title>
      <link>https://arxiv.org/abs/2503.08124</link>
      <description>arXiv:2503.08124v2 Announce Type: replace 
Abstract: Empirical substantive research, such as in the life or social sciences, is commonly categorized into the two modes exploratory and confirmatory, both of which are essential to scientific progress. The former is also referred to as hypothesis-generating or data-contingent research, the latter is also called hypothesis-testing research. In the context of empirical methodological research in statistics, however, the exploratory-confirmatory distinction has received very little attention so far. Our paper aims to fill this gap. First, we revisit the concept of empirical methodological research through the lens of the exploratory-confirmatory distinction. Secondly, we examine current practice with respect to this distinction through a literature survey including 115 articles from the field of biostatistics. Thirdly, we provide practical recommendations towards more appropriate design, interpretation, and reporting of empirical methodological research in light of this distinction. In particular, we argue that both modes of research are crucial to methodological progress, but that most published studies -- even if sometimes disguised as confirmatory -- are essentially of exploratory nature. We emphasize that it may be adequate to consider empirical methodological research as a continuum between "pure" exploration and "strict" confirmation, recommend transparently reporting the mode of conducted research within the spectrum between exploratory and confirmatory, and stress the importance of study protocols written before conducting the study, especially in confirmatory methodological research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08124v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>F. J. D. Lange, Juliane C. Wilcke, Sabine Hoffmann, Moritz Herrmann, Anne-Laure Boulesteix</dc:creator>
    </item>
    <item>
      <title>Bayes factor functions for testing partial correlation coefficients</title>
      <link>https://arxiv.org/abs/2503.10787</link>
      <description>arXiv:2503.10787v2 Announce Type: replace 
Abstract: Partial correlation coefficients are widely applied in the social sciences to evaluate the relationship between two variables after accounting for the influence of others. In this article, we present Bayes Factor Functions (BFFs) for assessing the presence of partial correlation. BFFs represent Bayes factors derived from test statistics and are expressed as functions of a standardized effect size. While traditional frequentist methods based on $p$-values have been criticized for their inability to provide cumulative evidence in favor of the true hypothesis, Bayesian approaches are often challenged due to their computational demands and sensitivity to prior distributions. BFFs overcome these limitations and offer summaries of hypothesis tests as alternative hypotheses are varied over a range of prior distributions on standardized effects. They also enable the integration of evidence across multiple studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10787v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saptati Datta, Valen E. Johnson</dc:creator>
    </item>
    <item>
      <title>Exponential Inequalities for Some Mixing Processes and Dynamic Systems</title>
      <link>https://arxiv.org/abs/2208.11481</link>
      <description>arXiv:2208.11481v4 Announce Type: replace-cross 
Abstract: Many important dynamic systems, time series models or even algorithms exhibit non-strong mixing properties. In this paper, we introduce the general concept of $\mathcal{C}_{p,\mathcal{F}}$-mixing to cover such cases, where assumptions on the dependence structure become stronger with increasing $p\in [1, \infty].$ We derive a series of sharp exponential-type (or Bernstein-type) inequalities under this dependence concept for $p=1$ and $p=\infty$.
  More specifically, $\mathcal{C}_{\infty,\mathcal{F}}$-mixing is equal to the widely discussed $\mathcal{C}$-mixing \citep{maume2006exponential}, and we prove a refinement of an Berntsein-type inequality in \cite{hang2017bernstein} for $\mathcal{C}$-mixing processes under more general assumptions. As there exist many stochastic processes and dynamic systems, which are not $\mathcal{C}$ (or $\mathcal{C}_{\infty,\mathcal{F}}$)-mixing, we derive Bernstein-type inequalities for $\mathcal{C}_{1,\mathcal{F}}$-mixing processes as well and we use this result to investigate the convergence rates of plug-in-type estimators of the local conditional mode set for vector-valued output, in particular in situations where the density is less smooth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.11481v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihao Yuan, Holger Dette</dc:creator>
    </item>
    <item>
      <title>Compositional Models for Estimating Causal Effects</title>
      <link>https://arxiv.org/abs/2406.17714</link>
      <description>arXiv:2406.17714v3 Announce Type: replace-cross 
Abstract: Many real-world systems can be usefully represented as sets of interacting components. Examples include computational systems, such as query processors and compilers, natural systems, such as cells and ecosystems, and social systems, such as families and organizations. However, current approaches to estimating potential outcomes and causal effects typically treat such systems as single units, represent them with a fixed set of variables, and assume a homogeneous data-generating process. In this work, we study a compositional approach for estimating individual-level potential outcomes and causal effects in structured systems, where each unit is represented by an instance-specific composition of multiple heterogeneous components. The compositional approach decomposes unit-level causal queries into more fine-grained queries, explicitly modeling how unit-level interventions affect component-level outcomes to generate a unit's outcome. We demonstrate this approach using modular neural network architectures and show that it provides benefits for causal effect estimation from observational data, such as accurate causal effect estimation for structured units, increased sample efficiency, improved overlap between treatment and control groups, and compositional generalization to units with unseen combinations of components. Remarkably, our results show that compositional modeling can improve the accuracy of causal estimation even when component-level outcomes are unobserved. We also create and use a set of real-world evaluation environments for the empirical evaluation of compositional approaches for causal effect estimation and demonstrate the role of composition structure, varying amounts of component-level data access, and component heterogeneity in the performance of compositional models as compared to the non-compositional approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17714v3</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Purva Pruthi, David Jensen</dc:creator>
    </item>
    <item>
      <title>Investigating symptom duration using current status data: a case study of post-acute COVID-19 syndrome</title>
      <link>https://arxiv.org/abs/2407.04214</link>
      <description>arXiv:2407.04214v3 Announce Type: replace-cross 
Abstract: For infectious diseases, characterizing symptom duration is of clinical and public health importance. Symptom duration may be assessed by surveying infected individuals and querying symptom status at the time of survey response. For example, in a SARS-CoV-2 testing program at the University of Washington, participants were surveyed at least $28$ days after testing positive and asked to report current symptom status. This study design yielded current status data: outcome measurements for each respondent consisted only of the time of survey response and a binary indicator of whether symptoms had resolved by that time. Such study design benefits from limited risk of recall bias, but analyzing the resulting data necessitates tailored statistical tools. Here, we review methods for current status data and describe a novel application of modern nonparametric techniques to this setting. The proposed approach is valid under weaker assumptions compared to existing methods, allows use of flexible machine learning tools, and handles potential survey nonresponse. From the university study, under an assumption that the survey response time is conditionally independent of symptom resolution time within strata of measured covariates, we estimate that 19% of participants experienced ongoing symptoms 30 days after testing positive, decreasing to 7% at 90 days. We assess the sensitivity of these results to deviations from conditional independence, finding the estimates to be more sensitive to assumption violations at 30 days compared to 90 days. Female sex, fatigue during acute infection, and higher viral load were associated with slower symptom resolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04214v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles J. Wolock, Susan Jacob, Julia C. Bennett, Anna Elias-Warren, Jessica O'Hanlon, Avi Kenny, Nicholas P. Jewell, Andrea Rotnitzky, Stephen R. Cole, Ana A. Weil, Helen Y. Chu, Marco Carone</dc:creator>
    </item>
    <item>
      <title>Causal Representation Learning from Multimodal Biomedical Observations</title>
      <link>https://arxiv.org/abs/2411.06518</link>
      <description>arXiv:2411.06518v3 Announce Type: replace-cross 
Abstract: Prevalent in biomedical applications (e.g., human phenotype research), multimodal datasets can provide valuable insights into the underlying physiological mechanisms. However, current machine learning (ML) models designed to analyze these datasets often lack interpretability and identifiability guarantees, which are essential for biomedical research. Recent advances in causal representation learning have shown promise in identifying interpretable latent causal variables with formal theoretical guarantees. Unfortunately, most current work on multimodal distributions either relies on restrictive parametric assumptions or yields only coarse identification results, limiting their applicability to biomedical research that favors a detailed understanding of the mechanisms.
  In this work, we aim to develop flexible identification conditions for multimodal data and principled methods to facilitate the understanding of biomedical datasets. Theoretically, we consider a nonparametric latent distribution (c.f., parametric assumptions in previous work) that allows for causal relationships across potentially different modalities. We establish identifiability guarantees for each latent component, extending the subspace identification results from previous work. Our key theoretical contribution is the structural sparsity of causal connections between modalities, which, as we will discuss, is natural for a large collection of biomedical systems. Empirically, we present a practical framework to instantiate our theoretical insights. We demonstrate the effectiveness of our approach through extensive experiments on both numerical and synthetic datasets. Results on a real-world human phenotype dataset are consistent with established biomedical research, validating our theoretical and methodological framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06518v3</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuewen Sun, Lingjing Kong, Guangyi Chen, Loka Li, Gongxu Luo, Zijian Li, Yixuan Zhang, Yujia Zheng, Mengyue Yang, Petar Stojanov, Eran Segal, Eric P. Xing, Kun Zhang</dc:creator>
    </item>
    <item>
      <title>Graph Topic Modeling for Documents with Spatial or Covariate Dependencies</title>
      <link>https://arxiv.org/abs/2412.14477</link>
      <description>arXiv:2412.14477v2 Announce Type: replace-cross 
Abstract: We address the challenge of incorporating document-level metadata into topic modeling to improve topic mixture estimation. To overcome the computational complexity and lack of theoretical guarantees in existing Bayesian methods, we extend probabilistic latent semantic indexing (pLSI), a frequentist framework for topic modeling, by incorporating document-level covariates or known similarities between documents through a graph formalism. Modeling documents as nodes and edges denoting similarities, we propose a new estimator based on a fast graph-regularized iterative singular value decomposition (SVD) that encourages similar documents to share similar topic mixture proportions. We characterize the estimation error of our proposed method by deriving high-probability bounds and develop a specialized cross-validation method to optimize our regularization parameters. We validate our model through comprehensive experiments on synthetic datasets and three real-world corpora, demonstrating improved performance and faster inference compared to existing Bayesian methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14477v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yeo Jin Jung, Claire Donnat</dc:creator>
    </item>
  </channel>
</rss>
