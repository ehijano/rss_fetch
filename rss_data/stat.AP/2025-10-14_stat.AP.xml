<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 15 Oct 2025 04:00:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Multi-objective Bayesian optimization for blocking in extreme value analysis and its application in additive manufacturing</title>
      <link>https://arxiv.org/abs/2510.11960</link>
      <description>arXiv:2510.11960v1 Announce Type: new 
Abstract: Extreme value theory (EVT) is well suited to model extreme events, such as floods, heatwaves, or mechanical failures, which is required for reliability assessment of systems across multiple domains for risk management and loss prevention. The block maxima (BM) method, a particular approach within EVT, starts by dividing the historical observations into blocks. Then the sample of the maxima for each block can be shown, under some assumptions, to converge to a known class of distributions, which can then be used for analysis. The question of automatic (i.e., without explicit expert input) selection of the block size remains an open challenge. This work proposes a novel Bayesian framework, namely, multi-objective Bayesian optimization (MOBO-D*), to optimize BM blocking for accurate modeling and prediction of extremes in EVT. MOBO-D* formulates two objectives: goodness-of-fit of the distribution of extreme events and the accurate prediction of extreme events to construct an estimated Pareto front for optimal blocking choices. The efficacy of the proposed framework is illustrated by applying it to a real-world case study from the domain of additive manufacturing as well as a synthetic dataset. MOBO-D* outperforms a number of benchmarks and can be naturally extended to high-dimensional cases. The computational experiments show that it can be a promising approach in applications that require repeated automated block size selection, such as optimization or analysis of many datasets at once.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11960v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shehzaib Irfan, Nabeel Ahmad, Alexander Vinel, Daniel F. Silva, Shuai Shao, Nima Shamsaei, Jia Liu</dc:creator>
    </item>
    <item>
      <title>The Living Forecast: Evolving Day-Ahead Predictions into Intraday Reality</title>
      <link>https://arxiv.org/abs/2510.12271</link>
      <description>arXiv:2510.12271v1 Announce Type: new 
Abstract: Accurate intraday forecasts are essential for power system operations, complementing day-ahead forecasts that gradually lose relevance as new information becomes available. This paper introduces a Bayesian updating mechanism that converts fully probabilistic day-ahead forecasts into intraday forecasts without retraining or re-inference. The approach conditions the Gaussian mixture output of a conditional variational autoencoder-based forecaster on observed measurements, yielding an updated distribution for the remaining horizon that preserves its probabilistic structure. This enables consistent point, quantile, and ensemble forecasts while remaining computationally efficient and suitable for real-time applications. Experiments on household electricity consumption and photovoltaic generation datasets demonstrate that the proposed method improves forecast accuracy up to 25% across likelihood-, sample-, quantile-, and point-based metrics. The largest gains occur in time steps with strong temporal correlation to observed data, and the use of pattern dictionary-based covariance structures further enhances performance. The results highlight a theoretically grounded framework for intraday forecasting in modern power systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12271v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kutay B\"olat, Peter Palensky, Simon Tindemans</dc:creator>
    </item>
    <item>
      <title>Monitoring 3D Lattice Structures in Additive Manufacturing Using Topological Data Analysis</title>
      <link>https://arxiv.org/abs/2510.11740</link>
      <description>arXiv:2510.11740v1 Announce Type: cross 
Abstract: We present a new method for the statistical process control of lattice structures using tools from Topological Data Analysis. Motivated by applications in additive manufacturing, such as aerospace components and biomedical implants, where hollow lattice geometries are critical, the proposed framework is based on monitoring the persistent homology properties of parts. Specifically, we focus on homological features of dimensions zero and one, corresponding to connected components and one-dimensional loops, to characterize and detect changes in the topology of lattice structures. A nonparametric hypothesis testing procedure and a control charting scheme are introduced to monitor these features during production. Furthermore, we conduct extensive run-length analysis via various simulated but real-life lattice-structured parts. Our results demonstrate that persistent homology is well-suited for detecting topological anomalies in complex geometries and offers a robust, intrinsically geometrical alternative to other SPC methods for mesh and point data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11740v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yulin An, Xueqi Zhao, Enrique del Castillo</dc:creator>
    </item>
    <item>
      <title>Optimal Pair Matching Combined with Machine Learning Predicts a Significant Reduction in Myocardial Infarction Risk in African Americans following Omega-3 Fatty Acid Supplementation</title>
      <link>https://arxiv.org/abs/2510.11756</link>
      <description>arXiv:2510.11756v1 Announce Type: cross 
Abstract: Conflicting clinical trial results on omega-3 highly unsaturated fatty acids (n-3 HUFA) have prompted uncertainty about their cardioprotective effects. While the VITAL trial found no overall cardiovascular benefit from n-3 HUFA supplementation, its substantial African American (AfAm) enrollment provided a unique opportunity to explore racial differences in response to n-3 HUFA supplementation. The current observational study aimed to simulate randomized clinical trial (RCT) conditions by matching 3,766 AfAm and 15,553 non-Hispanic White (NHW) individuals from the VITAL trial utilizing propensity score matching to address the limitations related to differences in confounding variables between the two groups. Within matched groups (3,766 AfAm and 3,766 NHW), n-3 HUFA supplementation's impact on myocardial infarction (MI), stroke, and cardiovascular disease (CVD) mortality was assessed. A weighted decision tree analysis revealed belonging to the n-3 supplementation group as the most significant predictor of MI among AfAm but not NHW. Further logistic regression using the LASSO method and bootstrap estimation of standard errors indicated n-3 supplementation significantly lowered MI risk in AfAm (OR 0.17, 95% CI [0.048, 0.60]), with no such effect in NHW. This study underscores the critical need for future RCT to explore racial disparities in MI risk associated with n-3 HUFA supplementation and highlights potential causal differences between supplementation health outcomes in AfAm versus NHW populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11756v1</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/nu16172933</arxiv:DOI>
      <arxiv:journal_reference>Nutrients 2024, 16(17), 2933</arxiv:journal_reference>
      <dc:creator>Shudong Sun, Aki Hara, Laurel Johnstone, Brian Hallmark, Joseph C. Watkins, Cynthia A. Thomson, Susan M. Schembre, Susan Sergeant, Jason Umans, Guang Yao, Hao Helen Zhang, Floyd H. Chilton</dc:creator>
    </item>
    <item>
      <title>Inpainting the Neural Picture: Inferring Unrecorded Brain Area Dynamics from Multi-Animal Datasets</title>
      <link>https://arxiv.org/abs/2510.11924</link>
      <description>arXiv:2510.11924v1 Announce Type: cross 
Abstract: Characterizing interactions between brain areas is a fundamental goal of systems neuroscience. While such analyses are possible when areas are recorded simultaneously, it is rare to observe all combinations of areas of interest within a single animal or recording session. How can we leverage multi-animal datasets to better understand multi-area interactions? Building on recent progress in large-scale, multi-animal models, we introduce NeuroPaint, a masked autoencoding approach for inferring the dynamics of unrecorded brain areas. By training across animals with overlapping subsets of recorded areas, NeuroPaint learns to reconstruct activity in missing areas based on shared structure across individuals. We train and evaluate our approach on synthetic data and two multi-animal, multi-area Neuropixels datasets. Our results demonstrate that models trained across animals with partial observations can successfully in-paint the dynamics of unrecorded areas, enabling multi-area analyses that transcend the limitations of any single experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11924v1</guid>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ji Xia, Yizi Zhang, Shuqi Wang, Genevera I. Allen, Liam Paninski, Cole Lincoln Hurwitz, Kenneth D. Miller</dc:creator>
    </item>
    <item>
      <title>Robust Functional Logistic Regression</title>
      <link>https://arxiv.org/abs/2510.12048</link>
      <description>arXiv:2510.12048v1 Announce Type: cross 
Abstract: Functional logistic regression is a popular model to capture a linear relationship between binary response and functional predictor variables. However, many methods used for parameter estimation in functional logistic regression are sensitive to outliers, which may lead to inaccurate parameter estimates and inferior classification accuracy. We propose a robust estimation procedure for functional logistic regression, in which the observations of the functional predictor are projected onto a set of finite-dimensional subspaces via robust functional principal component analysis. This dimension-reduction step reduces the outlying effects in the functional predictor. The logistic regression coefficient is estimated using an M-type estimator based on binary response and robust principal component scores. In doing so, we provide robust estimates by minimizing the effects of outliers in the binary response and functional predictor variables. Via a series of Monte-Carlo simulations and using hand radiograph data, we examine the parameter estimation and classification accuracy for the response variable. We find that the robust procedure outperforms some existing robust and non-robust methods when outliers are present, while producing competitive results when outliers are absent. In addition, the proposed method is computationally more efficient than some existing robust alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12048v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Berkay Akturk, Ufuk Beyaztas, Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>Pooling Probabilistic Forecasts for Cooperative Wind Power Offering</title>
      <link>https://arxiv.org/abs/2510.12382</link>
      <description>arXiv:2510.12382v1 Announce Type: cross 
Abstract: Wind power producers can benefit from forming coalitions to participate cooperatively in electricity markets. To support such collaboration, various profit allocation rules rooted in cooperative game theory have been proposed. However, existing approaches overlook the lack of coherence among producers regarding forecast information, which may lead to ambiguity in offering and allocations. In this paper, we introduce a ``reconcile-then-optimize'' framework for cooperative market offerings. This framework first aligns the individual forecasts into a coherent joint forecast before determining market offers. With such forecasts, we formulate and solve a two-stage stochastic programming problem to derive both the aggregate offer and the corresponding scenario-based dual values for each trading hour. Based on these dual values, we construct a profit allocation rule that is budget-balanced and stable. Finally, we validate the proposed method through empirical case studies, demonstrating its practical effectiveness and theoretical soundness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12382v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Honglin Wen, Pierre Pinson</dc:creator>
    </item>
    <item>
      <title>Restoring the Forecasting Power of Google Trends with Statistical Preprocessing</title>
      <link>https://arxiv.org/abs/2504.07032</link>
      <description>arXiv:2504.07032v2 Announce Type: replace 
Abstract: Google Trends reports how frequently specific queries are searched on Google over time. It is widely used in research and industry to gain early insights into public interest. However, its data generation mechanism introduces missing values, sampling variability, noise, and trends. These issues arise from privacy thresholds mapping low search volumes to zeros, daily sampling variations causing discrepancies across historical downloads, and algorithm updates altering volume magnitudes over time. Data quality has recently deteriorated, with more zeros and noise, even for previously stable queries. We propose a comprehensive statistical methodology to preprocess Google Trends search information using hierarchical clustering, smoothing splines, and detrending. We validate our approach by forecasting U.S. influenza hospitalizations up to three weeks ahead with several statistical and machine learning models. Compared to omitting exogenous variables, our results show that preprocessed signals enhance forecast accuracy, while raw Google Trends data often degrades performance in statistical models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07032v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Candice Djorno, Mauricio Santillana, Shihao Yang</dc:creator>
    </item>
    <item>
      <title>Precrop Payoffs: Causal machine learning reveals large but variable yield benefits of crop rotation in major breadbaskets</title>
      <link>https://arxiv.org/abs/2506.02384</link>
      <description>arXiv:2506.02384v2 Announce Type: replace 
Abstract: Building sustainable food systems that are resilient to climate change will require improved agricultural management and policy. One common practice that is well-known to benefit crop yields is crop rotation, yet there remains limited understanding of how the benefits of crop rotation vary for different crop sequences and for different weather conditions. To address these gaps, we leverage crop type maps, satellite data, and causal machine learning to study how precrop effects on subsequent yields vary with cropping sequence choice and weather. Complementing and going beyond what is known from randomized field trials, we find that (i) for those farmers who do rotate, the most common precrop choices tend to be among the most beneficial, (ii) the effects of switching from a simple rotation (which alternates between two crops) to a more diverse rotation were typically small and sometimes even negative, (iii) precrop effects tended to be greater under rainier conditions, (iv) precrop effects were greater under warmer conditions for soybean yields but not for other crops, and (v) legume precrops conferred smaller benefits under warmer conditions. Our results and the methods we use can enable farmers and policy makers to identify which rotations will be most effective at improving crop yields in a changing climate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02384v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1088/1748-9326/ae0f45</arxiv:DOI>
      <dc:creator>Dan M. Kluger, Stefania Di Tommaso, David B. Lobell</dc:creator>
    </item>
    <item>
      <title>Inference on the state process of periodically inhomogeneous hidden Markov models for animal behavior</title>
      <link>https://arxiv.org/abs/2312.14583</link>
      <description>arXiv:2312.14583v3 Announce Type: replace-cross 
Abstract: Over the last decade, hidden Markov models (HMMs) have become increasingly popular in statistical ecology, where they constitute natural tools for studying animal behavior based on complex sensor data. Corresponding analyses sometimes explicitly focus on - and in any case need to take into account - periodic variation, for example by quantifying the activity distribution over the daily cycle or seasonal variation such as migratory behavior. For HMMs including periodic components, we establish important mathematical properties that allow for comprehensive statistical inference related to periodic variation, thereby also providing guidance for model building and model checking. Specifically, we derive the periodically varying unconditional state distribution as well as the time-varying and overall state dwell-time distributions - all of which are of key interest when the inferential focus lies on the dynamics of the state process. We use the associated novel inference and model-checking tools to investigate changes in the diel activity patterns of fruit flies in response to changing light conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14583v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan-Ole Koslik, Carlina C. Feldmann, Sina Mews, Rouven Michels, Roland Langrock</dc:creator>
    </item>
    <item>
      <title>Bootstrap tests for almost goodness-of-fit</title>
      <link>https://arxiv.org/abs/2410.20918</link>
      <description>arXiv:2410.20918v2 Announce Type: replace-cross 
Abstract: We introduce the \textit{almost goodness-of-fit} test, a procedure to assess whether a (parametric) model provides a good representation of the probability distribution generating the observed sample. Specifically, given a distribution function $F$ and a parametric family $\mathcal{G}=\{ G(\boldsymbol{\theta}) : \boldsymbol{\theta} \in \Theta\}$, we consider the testing problem \[ H_0: \| F - G(\boldsymbol{\theta}_F) \|_p \geq \epsilon \quad \text{vs} \quad H_1: \| F - G(\boldsymbol{\theta}_F) \|_p &lt; \epsilon, \] where $\epsilon&gt;0$ is a margin of error and $G(\boldsymbol{\theta}_F)$ denotes a representative of $F$ within the parametric class. The approximate model is determined via an M-estimator of the parameters. %The objective is the approximate validation of a distribution or an entire parametric family up to a pre-specified threshold value. The methodology also quantifies the percentage improvement of the proposed model relative to a non-informative (constant) benchmark. The test statistic is the $\mathrm{L}^p$-distance between the empirical distribution function and that of the estimated model. We present two consistent, easy-to-implement, and flexible bootstrap schemes to carry out the test. The performance of the proposal is illustrated through simulation studies and analysis and real-data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20918v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Statistics and Computing, 2025</arxiv:journal_reference>
      <dc:creator>Amparo Ba\'illo, Javier C\'arcamo</dc:creator>
    </item>
    <item>
      <title>Wasserstein-based Kernel Principal Component Analysis for Clustering Applications</title>
      <link>https://arxiv.org/abs/2503.14357</link>
      <description>arXiv:2503.14357v2 Announce Type: replace-cross 
Abstract: Many data clustering applications must handle objects that cannot be represented as vectors. In this context, the bag-of-vectors representation describes complex objects through discrete distributions, for which the Wasserstein distance provides a well-conditioned dissimilarity measure. Kernel methods extend this by embedding distance information into feature spaces that facilitate analysis. However, an unsupervised framework that combines kernels with Wasserstein distances for clustering distributional data is still lacking. We address this gap by introducing a computationally tractable framework that integrates Wasserstein metrics with kernel methods for clustering. The framework can accommodate both vectorial and distributional data, enabling applications in various domains. It comprises three components: (i) an efficient approximation of pairwise Wasserstein distances using multiple reference distributions; (ii) shifted positive definite kernel functions based on Wasserstein distances, combined with kernel principal component analysis for feature mapping; and (iii) scalable, distance-agnostic validity indices for clustering evaluation and kernel parameter optimization. Experiments on power distribution graphs and real-world time series demonstrate the effectiveness and efficiency of the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14357v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alfredo Oneto, Blazhe Gjorgiev, Giovanni Sansavini</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Variable Selection in Model-Based Clustering with Missing Not at Random</title>
      <link>https://arxiv.org/abs/2505.19093</link>
      <description>arXiv:2505.19093v2 Announce Type: replace-cross 
Abstract: Model-based clustering integrated with variable selection is a powerful tool for uncovering latent structures within complex data. However, its effectiveness is often hindered by challenges such as identifying relevant variables that define heterogeneous subgroups and handling data that are missing not at random, a prevalent issue in fields like transcriptomics. While several notable methods have been proposed to address these problems, they typically tackle each issue in isolation, thereby limiting their flexibility and adaptability. This paper introduces a unified framework designed to address these challenges simultaneously. Our approach incorporates a data-driven penalty matrix into penalized clustering to enable more flexible variable selection, along with a mechanism that explicitly models the relationship between missingness and latent class membership. We demonstrate that, under certain regularity conditions, the proposed framework achieves both asymptotic consistency and selection consistency, even in the presence of missing data. This unified strategy significantly enhances the capability and efficiency of model-based clustering, advancing methodologies for identifying informative variables that define homogeneous subgroups in the presence of complex missing data patterns. The performance of the framework, including its computational efficiency, is evaluated through simulations and demonstrated using both synthetic and real-world transcriptomic datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19093v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>39th Conference on Neural Information Processing Systems (NeurIPS 2025)</arxiv:journal_reference>
      <dc:creator>Binh H. Ho, Long Nguyen Chi, TrungTin Nguyen, Binh T. Nguyen, Van Ha Hoang, Christopher Drovandi</dc:creator>
    </item>
    <item>
      <title>Generalized Jeffreys's approximate objective Bayes factor: Model-selection consistency, finite-sample accuracy, and statistical evidence in 71,126 clinical trial findings</title>
      <link>https://arxiv.org/abs/2510.10358</link>
      <description>arXiv:2510.10358v2 Announce Type: replace-cross 
Abstract: Concerns about the misuse and misinterpretation of p-values and statistical significance have motivated alternatives for quantifying evidence. We define a generalized form of Jeffreys's approximate objective Bayes factor (eJAB), a one-line calculation that is a function of the p-value, sample size, and parameter dimension. We establish conditions under which eJAB is model-selection consistent and verify them for ten statistical tests. We assess finite-sample accuracy by comparing eJAB with Markov chain Monte Carlo computed Bayes factors in 12 simulation studies. We then apply eJAB to 71,126 results from ClinicalTrials.gov (CTG) and find that the proportion of findings with $\text{p-value} \le \alpha$ yet $eJAB_{01}&gt;1$ (favoring the null) closely tracks the significance level $\alpha$, suggesting that such contradictions are pointing to the type I errors. We catalog 4,088 such candidate type I errors and provide details for 131 with reported $\text{p-value} \le 0.01$. We also identify 487 instances of the Jeffreys-Lindley paradox. Finally, we estimate that 75% (6%) of clinical trial plans from CTG set $\alpha \ge 0.05$ as the target evidence threshold, and that 35.5% (0.22%) of results significant at $\alpha =0.05$ correspond to evidence that is no stronger than anecdotal under eJAB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10358v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Puneet Velidi, Zhengxiao Wei, Shreena Nisha Kalaria, Yimeng Liu, C\'eline M. Laumont, Brad H. Nelson, Farouk S. Nathoo</dc:creator>
    </item>
  </channel>
</rss>
