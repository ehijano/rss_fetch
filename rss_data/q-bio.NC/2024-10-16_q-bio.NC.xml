<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Oct 2024 01:56:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Attractor-based models for sequences and pattern generation in neural circuits</title>
      <link>https://arxiv.org/abs/2410.11012</link>
      <description>arXiv:2410.11012v1 Announce Type: new 
Abstract: Neural circuits in the brain perform a variety of essential functions, including input classification, pattern completion, and the generation of rhythms and oscillations that support processes such as breathing and locomotion. There is also substantial evidence that the brain encodes memories and processes information via sequences of neural activity. In this dissertation, we are focused on the general problem of how neural circuits encode rhythmic activity, as in central pattern generators (CPGs), as well as the encoding of sequences. Traditionally, rhythmic activity and CPGs have been modeled using coupled oscillators. Here we take a different approach, and present models for several different neural functions using threshold-linear networks. Our approach aims to unify attractor-based models (e.g., Hopfield networks) which encode static and dynamic patterns as attractors of the network.
  In the first half of this dissertation, we present several attractor-based models. These include: a network that can count the number of external inputs it receives; two models for locomotion, one encoding five different quadruped gaits and another encoding the orientation system of a swimming mollusk; and, finally, a model that connects the fixed point sequences with locomotion attractors to obtain a network that steps through a sequence of dynamic attractors. In the second half of the thesis, we present new theoretical results, some of which have already been published. There, we established conditions on network architectures to produce sequential attractors. Here we also include several new theorems relating the fixed points of composite networks to those of their component subnetworks, as well as a new architecture for layering networks which produces "fusion" attractors by minimizing interference between the attractors of individual layers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11012v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Juliana Londono Alvarez</dc:creator>
    </item>
    <item>
      <title>Intramuscular High-Density Micro-Electrode Arrays Enable High-Precision Decoding and Mapping of Spinal Motor Neurons to Reveal Hand Control</title>
      <link>https://arxiv.org/abs/2410.11016</link>
      <description>arXiv:2410.11016v1 Announce Type: new 
Abstract: Decoding nervous system activity is a key challenge in neuroscience and neural interfacing. In this study, we propose a novel neural decoding system that enables unprecedented large-scale sampling of muscle activity. Using micro-electrode arrays with more than 100 channels embedded within the forearm muscles, we recorded high-density signals that captured multi-unit motor neuron activity. This extensive sampling was complemented by advanced methods for neural decomposition, analysis, and classification, allowing us to accurately detect and interpret the spiking activity of spinal motor neurons that innervate hand muscles. We evaluated this system in two healthy participants, each implanted with three electromyogram (EMG) micro-electrode arrays (comprising 40 electrodes each) in the forearm. These arrays recorded muscle activity during both single- and multi-digit isometric contractions. For the first time under controlled conditions, we demonstrate that multi-digit tasks elicit unique patterns of motor neuron recruitment specific to each task, rather than employing combinations of recruitment patterns from single-digit tasks. This observation led us to hypothesize that hand tasks could be classified with high precision based on the decoded neural activity. We achieved perfect classification accuracy (100%) across 12 distinct single- and multi-digit tasks, and consistently high accuracy (&gt;96\%) across all conditions and subjects, for up to 16 task classes. These results significantly outperformed conventional EMG classification methods. The exceptional performance of this system paves the way for developing advanced neural interfaces based on invasive high-density EMG technology. This innovation could greatly enhance human-computer interaction and lead to substantial improvements in assistive technologies, offering new possibilities for restoring motor function in clinical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11016v1</guid>
      <category>q-bio.NC</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <category>eess.SP</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agnese Grison, Jaime Ibanez Pereda, Silvia Muceli, Aritra Kundu, Farah Baracat, Giacomo Indiveri, Elisa Donati, Dario Farina</dc:creator>
    </item>
    <item>
      <title>Parsing altered brain connectivity in neurodevelopmental disorders by integrating graph-based normative modeling and deep generative networks</title>
      <link>https://arxiv.org/abs/2410.11064</link>
      <description>arXiv:2410.11064v1 Announce Type: new 
Abstract: Many neurodevelopmental disorders can be understood as divergent patterns of neural interactions during brain development. Advances in neuroimaging have illuminated these patterns by modeling the brain as a network structure using diffution MRI tractography. However, characterizing and quantifying individual heterogeneity in neurodevelopmental disorders within these highly complex brain networks remains a significant challenge. In this paper, we present for the first time, a framework that integrates deep generative models with graph-based normative modeling to characterize brain network development in the neurotypical population, which can then be used to quantify the individual-level neurodivergence associated with disorders. Our deep generative model incorporates bio-inspired wiring constraints to effectively capture the developmental trajectories of neurotypical brain networks. Neurodivergence is quantified by comparing individuals to this neurotypical trajectory, enabling the creation of region-wise divergence maps that reveal latent developmental differences at each brain regions, along with overall neurodivergence scores based on predicted brain age gaps. We demonstrate the clinical utility of this framework by applying it to a large sample of children with autism spectrum disorders, showing that the individualized region-wise maps help parse the heterogeneity in autism, and the neurodivergence scores correlate with clinical assessments. Together, we provide powerful tools for quantifying neurodevelopmental divergence in brain networks, paying the way for developing imaging markers that will support disorder stratification, monitor progression, and evaluate therapeutic effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11064v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rui Sherry Shen, Yusuf Osmanl{\i}o\u{g}lu, Drew Parker, Darien Aunapu, Benjamin E. Yerys, Birkan Tun\c{c}, Ragini Verma</dc:creator>
    </item>
    <item>
      <title>How Initial Connectivity Shapes Biologically Plausible Learning in Recurrent Neural Networks</title>
      <link>https://arxiv.org/abs/2410.11164</link>
      <description>arXiv:2410.11164v1 Announce Type: cross 
Abstract: The impact of initial connectivity on learning has been extensively studied in the context of backpropagation-based gradient descent, but it remains largely underexplored in biologically plausible learning settings. Focusing on recurrent neural networks (RNNs), we found that the initial weight magnitude significantly influences the learning performance of biologically plausible learning rules in a similar manner to its previously observed effect on training via backpropagation through time (BPTT). By examining the maximum Lyapunov exponent before and after training, we uncovered the greater demands that certain initialization schemes place on training to achieve desired information propagation properties. Consequently, we extended the recently proposed gradient flossing method, which regularizes the Lyapunov exponents, to biologically plausible learning and observed an improvement in learning performance. To our knowledge, we are the first to examine the impact of initialization on biologically plausible learning rules for RNNs and to subsequently propose a biologically plausible remedy. Such an investigation could lead to predictions about the influence of initial connectivity on learning dynamics and performance, as well as guide neuromorphic design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11164v1</guid>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xinyue Zhang, Weixuan Liu, Yuhan Helena Liu</dc:creator>
    </item>
    <item>
      <title>A Case for AI Consciousness: Language Agents and Global Workspace Theory</title>
      <link>https://arxiv.org/abs/2410.11407</link>
      <description>arXiv:2410.11407v1 Announce Type: cross 
Abstract: It is generally assumed that existing artificial systems are not phenomenally conscious, and that the construction of phenomenally conscious artificial systems would require significant technological progress if it is possible at all. We challenge this assumption by arguing that if Global Workspace Theory (GWT) - a leading scientific theory of phenomenal consciousness - is correct, then instances of one widely implemented AI architecture, the artificial language agent, might easily be made phenomenally conscious if they are not already. Along the way, we articulate an explicit methodology for thinking about how to apply scientific theories of consciousness to artificial systems and employ this methodology to arrive at a set of necessary and sufficient conditions for phenomenal consciousness according to GWT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11407v1</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Goldstein, Cameron Domenico Kirk-Giannini</dc:creator>
    </item>
    <item>
      <title>Probabilistic Principles for Biophysics and Neuroscience: Entropy Production, Bayesian Mechanics &amp; the Free-Energy Principle</title>
      <link>https://arxiv.org/abs/2410.11735</link>
      <description>arXiv:2410.11735v1 Announce Type: cross 
Abstract: This thesis focuses on three fundamental aspects of biological systems; namely, entropy production, Bayesian mechanics, and the free-energy principle. The contributions are threefold: 1) We compute the entropy production for a greater class of systems than before, including almost any stationary diffusion process, such as degenerate diffusions where the driving noise does not act on all coordinates of the system. Importantly, this class of systems encompasses Markovian approximations of stochastic differential equations driven by colored noise, which is significant since biological systems at the macro- and meso-scale are generally subject to colored fluctuations. 2) We develop a Bayesian mechanics for biological and physical entities that interact with their environment in which we give sufficient and necessary conditions for the internal states of something to infer its external states, consistently with variational Bayesian inference in statistics and theoretical neuroscience. 3) We refine the constraints on Bayesian mechanics to obtain a description that is more specific to biological systems, called the free-energy principle. This says that active and internal states of biological systems unfold as minimising a quantity known as free energy. The mathematical foundation to the free-energy principle, presented here, unlocks a first principles approach to modeling and simulating behavior in neurobiology and artificial intelligence, by minimising free energy given a generative model of external and sensory states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11735v1</guid>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>nlin.AO</category>
      <category>physics.bio-ph</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>PhD Thesis Imperial College London 2024</arxiv:journal_reference>
      <dc:creator>Lancelot Da Costa</dc:creator>
    </item>
    <item>
      <title>Enhancing learning in spiking neural networks through neuronal heterogeneity and neuromodulatory signaling</title>
      <link>https://arxiv.org/abs/2407.04525</link>
      <description>arXiv:2407.04525v3 Announce Type: replace 
Abstract: Recent progress in artificial intelligence (AI) has been driven by insights from neuroscience, particularly with the development of artificial neural networks (ANNs). This has significantly enhanced the replication of complex cognitive tasks such as vision and natural language processing. Despite these advances, ANNs struggle with continual learning, adaptable knowledge transfer, robustness, and resource efficiency - capabilities that biological systems handle seamlessly. Specifically, ANNs often overlook the functional and morphological diversity of the brain, hindering their computational capabilities. Furthermore, incorporating cell-type specific neuromodulatory effects into ANNs with neuronal heterogeneity could enable learning at two spatial scales: spiking behavior at the neuronal level, and synaptic plasticity at the circuit level, thereby potentially enhancing their learning abilities. In this article, we summarize recent bio-inspired models, learning rules and architectures and propose a biologically-informed framework for enhancing ANNs. Our proposed dual-framework approach highlights the potential of spiking neural networks (SNNs) for emulating diverse spiking behaviors and dendritic compartments to simulate morphological and functional diversity of neuronal computations. Finally, we outline how the proposed approach integrates brain-inspired compartmental models and task-driven SNNs, balances bioinspiration and complexity, and provides scalable solutions for pressing AI challenges, such as continual learning, adaptability, robustness, and resource-efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04525v3</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Rodriguez-Garcia, Jie Mei, Srikanth Ramaswamy</dc:creator>
    </item>
    <item>
      <title>Unsupervised Learning of Spatio-Temporal Patterns in Spiking Neuronal Networks</title>
      <link>https://arxiv.org/abs/2410.08637</link>
      <description>arXiv:2410.08637v2 Announce Type: replace 
Abstract: The ability to predict future events or patterns based on previous experience is crucial for many applications such as traffic control, weather forecasting, or supply chain management. While modern supervised Machine Learning approaches excel at such sequential tasks, they are computationally expensive and require large training data. A previous work presented a biologically plausible sequence learning model, developed through a bottom-up approach, consisting of a spiking neural network and unsupervised local learning rules. The model in its original formulation identifies only a specific type of sequence elements composed of synchronous spikes by activating a subset of neurons with identical stimulus preference. In this work, we extend the model to detect and learn sequences of various spatio-temporal patterns (STPs) by incorporating plastic connections in the input synapses. We showcase that the model is able to learn and predict high-order sequences. We further study the robustness of the model against different input settings and parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08637v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Feiler, Emre Neftci, Younes Bouhadjar</dc:creator>
    </item>
    <item>
      <title>Curriculum effects and compositionality emerge with in-context learning in neural networks</title>
      <link>https://arxiv.org/abs/2402.08674</link>
      <description>arXiv:2402.08674v3 Announce Type: replace-cross 
Abstract: Human learning embodies a striking duality: sometimes, we appear capable of following logical, compositional rules and benefit from structured curricula (e.g., in formal education), while other times, we rely on an incremental approach or trial-and-error, learning better from curricula that are unstructured or randomly interleaved. Influential psychological theories explain this seemingly disparate behavioral evidence by positing two qualitatively different learning systems -- one for rapid, rule-based inferences and another for slow, incremental adaptation. It remains unclear how to reconcile such theories with neural networks, which learn via incremental weight updates and are thus a natural model for the latter type of learning, but are not obviously compatible with the former. However, recent evidence suggests that both metalearning neural networks and large language models are capable of "in-context learning" (ICL) -- the ability to flexibly grasp the structure of a new task from a few examples given at inference time. Here, we show that networks capable of ICL can reproduce human-like learning and compositional behavior on rule-governed tasks, while at the same time replicating human behavioral phenomena in tasks lacking rule-like structure via their usual in-weight learning (IWL). Our work shows how emergent ICL can equip neural networks with fundamentally different learning properties than those traditionally attributed to them, and that these can coexist with the properties of their native IWL, thus offering a novel perspective on dual-process theories and human cognitive flexibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08674v3</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacob Russin, Ellie Pavlick, Michael J. Frank</dc:creator>
    </item>
    <item>
      <title>Disentangling Representations through Multi-task Learning</title>
      <link>https://arxiv.org/abs/2407.11249</link>
      <description>arXiv:2407.11249v2 Announce Type: replace-cross 
Abstract: Intelligent perception and interaction with the world hinges on internal representations that capture its underlying structure ("disentangled" or "abstract" representations). Disentangled representations serve as world models, isolating latent factors of variation in the world along orthogonal directions, thus facilitating feature-based generalization. We provide experimental and theoretical results guaranteeing the emergence of disentangled representations in agents that optimally solve multi-task evidence aggregation classification tasks, canonical in the cognitive neuroscience literature. The key conceptual finding is that, by producing accurate multi-task classification estimates, a system implicitly represents a set of coordinates specifying a disentangled representation of the underlying latent state of the data it receives. The theory provides conditions for the emergence of these representations in terms of noise, number of tasks, and evidence aggregation time. We experimentally validate these predictions in RNNs trained on multi-task classification, which learn disentangled representations in the form of continuous attractors, leading to zero-shot out-of-distribution (OOD) generalization in predicting latent factors. We demonstrate the robustness of our framework across autoregressive architectures, decision boundary geometries and in tasks requiring classification confidence estimation. We find that transformers are particularly suited for disentangling representations, which might explain their unique world understanding abilities. Overall, our framework puts forth parallel processing as a general principle for the formation of cognitive maps that capture the structure of the world in both biological and artificial systems, and helps explain why ANNs often arrive at human-interpretable concepts, and how they both may acquire exceptional zero-shot generalization capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11249v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pantelis Vafidis, Aman Bhargava, Antonio Rangel</dc:creator>
    </item>
    <item>
      <title>Toward Universal and Interpretable World Models for Open-ended Learning Agents</title>
      <link>https://arxiv.org/abs/2409.18676</link>
      <description>arXiv:2409.18676v2 Announce Type: replace-cross 
Abstract: We introduce a generic, compositional and interpretable class of generative world models that supports open-ended learning agents. This is a sparse class of Bayesian networks capable of approximating a broad range of stochastic processes, which provide agents with the ability to learn world models in a manner that may be both interpretable and computationally scalable. This approach integrating Bayesian structure learning and intrinsically motivated (model-based) planning enables agents to actively develop and refine their world models, which may lead to developmental learning and more robust, adaptive behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18676v2</guid>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 16 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>NeurIPS 2024 Workshop on Intrinsically Motivated Open-ended Learning (IMOL)</arxiv:journal_reference>
      <dc:creator>Lancelot Da Costa</dc:creator>
    </item>
  </channel>
</rss>
