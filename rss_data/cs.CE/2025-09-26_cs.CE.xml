<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 26 Sep 2025 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Hierarchical Adaptive Diffusion Model for Flexible Protein-Protein Docking</title>
      <link>https://arxiv.org/abs/2509.20542</link>
      <description>arXiv:2509.20542v1 Announce Type: new 
Abstract: Structural prediction of protein-protein interactions is important to understand the molecular basis of cellular interactions, but it still faces major challenges when significant conformational changes are present. We propose a generative framework of hierarchical adaptive diffusion to improve accuracy and efficiency in such cases. It is hierarchical in separating global inter-protein rigid-body motions and local intra-protein flexibility in diffusion processes, and the distinct local and global noise schedules are designed to mimic the induced-fit effect. It is adaptive in conditioning the local flexibility schedule on predicted levels of conformational change, allowing faster flexing for larger anticipated conformational changes. Furthermore, it couples the local and global diffusion processes through a common score and confidence network with sequence, evolution, structure, and dynamics features as inputs, and maintains rotational or translational invariance or equivariance in outputs. It builds on our newly curated DIPS-AF dataset of nearly 39,000 examples for pre-training. Experiments on the independent docking benchmark dataset DB5.5 show that our model outperforms an AlphaFold2-like iterative transformer (GeoDock) and a diffusion model (DiffDock-PP) in both rigid and flexible cases, with larger improvements in more flexible cases. Ablation studies prove the importance of adaptive schedules, dynamics features, and pre-training. Additional analyses and case studies reveal remaining gaps in sampling, scoring, and conformational resolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20542v1</guid>
      <category>cs.CE</category>
      <category>q-bio.BM</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rujie Yin, Yang Shen</dc:creator>
    </item>
    <item>
      <title>Difference-Guided Reasoning: A Temporal-Spatial Framework for Large Language Models</title>
      <link>https://arxiv.org/abs/2509.20713</link>
      <description>arXiv:2509.20713v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are important tools for reasoning and problem-solving, while they often operate passively, answering questions without actively discovering new ones. This limitation reduces their ability to simulate human-like thinking, where noticing differences is a key trigger for reasoning. Thus, in this paper we propose a difference-guided reasoning framework, which enables LLMs to identify and act upon changes across time and space. The model formalizes differences through feature extraction, prioritizes the most impactful and latest changes, and links them to appropriate actions. We further extend the framework with mechanisms for abnormal behavior detection and the integration of external information from users or sensors, ensuring more reliable and grounded reasoning. Verification results show that prompting LLMs with differences improves focus on critical issues, leading to higher alignment with desired reasoning outcomes compared to direct prompting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20713v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hong Su</dc:creator>
    </item>
    <item>
      <title>Extrapolating Phase-Field Simulations in Space and Time with Purely Convolutional Architectures</title>
      <link>https://arxiv.org/abs/2509.20770</link>
      <description>arXiv:2509.20770v1 Announce Type: new 
Abstract: Phase-field models of liquid metal dealloying (LMD) can resolve rich microstructural dynamics but become intractable for large domains or long time horizons. We present a conditionally parameterized, fully convolutional U-Net surrogate that generalizes far beyond its training window in both space and time. The design integrates convolutional self-attention and physics-aware padding, while parameter conditioning enables variable time-step skipping and adaptation to diverse alloy systems. Although trained only on short, small-scale simulations, the surrogate exploits the translational invariance of convolutions to extend predictions to much longer horizons than traditional solvers. It accurately reproduces key LMD physics, with relative errors typically under 5% within the training regime and below 10% when extrapolating to larger domains and later times. The method accelerates computations by up to 16,000 times, cutting weeks of simulation down to seconds, and marks an early step toward scalable, high-fidelity extrapolation of LMD phase-field models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20770v1</guid>
      <category>cs.CE</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christophe Bonneville, Nathan Bieberdorf, Pieterjan Robbe, Mark Asta, Habib N. Najm, Laurent Capolungo, Cosmin Safta</dc:creator>
    </item>
    <item>
      <title>Mesh Interpolation Graph Network for Dynamic and Spatially Irregular Global Weather Forecasting</title>
      <link>https://arxiv.org/abs/2509.20911</link>
      <description>arXiv:2509.20911v1 Announce Type: new 
Abstract: Graph neural networks have shown promising results in weather forecasting, which is critical for human activity such as agriculture planning and extreme weather preparation. However, most studies focus on finite and local areas for training, overlooking the influence of broader areas and limiting their ability to generalize effectively. Thus, in this work, we study global weather forecasting that is irregularly distributed and dynamically varying in practice, requiring the model to generalize to unobserved locations. To address such challenges, we propose a general Mesh Interpolation Graph Network (MIGN) that models the irregular weather station forecasting, consisting of two key designs: (1) learning spatially irregular data with regular mesh interpolation network to align the data; (2) leveraging parametric spherical harmonics location embedding to further enhance spatial generalization ability. Extensive experiments on an up-to-date observation dataset show that MIGN significantly outperforms existing data-driven models. Besides, we show that MIGN has spatial generalization ability, and is capable of generalizing to previous unseen stations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20911v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zinan Zheng, Yang Liu, Jia Li</dc:creator>
    </item>
    <item>
      <title>Extensions of a Line-Graph-Based Method for Token Routing in Decentralized Exchanges</title>
      <link>https://arxiv.org/abs/2509.21152</link>
      <description>arXiv:2509.21152v1 Announce Type: new 
Abstract: Decentralized exchanges (DEXs) form a cornerstone of the decentralized finance (DeFi) ecosystem, processing token trades worth billions of dollars daily. Yet, a significant fraction of these trades are suboptimal: alternative routing paths could yield more target tokens. Addressing this inefficiency is both practically urgent and theoretically compelling. Building on the linear line-graph-based routing method of Zhang et al. (2025), we propose three key extensions that better capture real-world trading complexity. First, we introduce a breadth-first search (BFS) link iteration rule that reduces computational cost and average execution time without sacrificing profitability. Second, we design a route-splitting strategy that divides large trades into smaller ones, alleviating price slippage and increasing average trader profits, albeit at the cost of higher computational overhead. Third, we generalize the method beyond a single DEX to a multi-DEX aggregator setting, reflecting actual trading environments. Using empirical data from Uniswap V2 and Sushiswap V2, we demonstrate that these extensions substantially improve both computational efficiency and profitability, establishing a foundation for future routing enhancements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21152v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Zhang, Claudio J. Tessone</dc:creator>
    </item>
    <item>
      <title>PIRF: Physics-Informed Reward Fine-Tuning for Diffusion Models</title>
      <link>https://arxiv.org/abs/2509.20570</link>
      <description>arXiv:2509.20570v1 Announce Type: cross 
Abstract: Diffusion models have demonstrated strong generative capabilities across scientific domains, but often produce outputs that violate physical laws. We propose a new perspective by framing physics-informed generation as a sparse reward optimization problem, where adherence to physical constraints is treated as a reward signal. This formulation unifies prior approaches under a reward-based paradigm and reveals a shared bottleneck: reliance on diffusion posterior sampling (DPS)-style value function approximations, which introduce non-negligible errors and lead to training instability and inference inefficiency. To overcome this, we introduce Physics-Informed Reward Fine-tuning (PIRF), a method that bypasses value approximation by computing trajectory-level rewards and backpropagating their gradients directly. However, a naive implementation suffers from low sample efficiency and compromised data fidelity. PIRF mitigates these issues through two key strategies: (1) a layer-wise truncated backpropagation method that leverages the spatiotemporally localized nature of physics-based rewards, and (2) a weight-based regularization scheme that improves efficiency over traditional distillation-based methods. Across five PDE benchmarks, PIRF consistently achieves superior physical enforcement under efficient sampling regimes, highlighting the potential of reward fine-tuning for advancing scientific generative modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20570v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingze Yuan, Pengfei Jin, Na Li, Quanzheng Li</dc:creator>
    </item>
    <item>
      <title>MLIP Arena: Advancing Fairness and Transparency in Machine Learning Interatomic Potentials via an Open, Accessible Benchmark Platform</title>
      <link>https://arxiv.org/abs/2509.20630</link>
      <description>arXiv:2509.20630v1 Announce Type: cross 
Abstract: Machine learning interatomic potentials (MLIPs) have revolutionized molecular and materials modeling, but existing benchmarks suffer from data leakage, limited transferability, and an over-reliance on error-based metrics tied to specific density functional theory (DFT) references. We introduce MLIP Arena, a benchmark platform that evaluates force field performance based on physics awareness, chemical reactivity, stability under extreme conditions, and predictive capabilities for thermodynamic properties and physical phenomena. By moving beyond static DFT references and revealing the important failure modes of current foundation MLIPs in real-world settings, MLIP Arena provides a reproducible framework to guide the next-generation MLIP development toward improved predictive accuracy and runtime efficiency while maintaining physical consistency. The Python package and online leaderboard are available at https://github.com/atomind-ai/mlip-arena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20630v1</guid>
      <category>physics.chem-ph</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.CE</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan Chiang, Tobias Kreiman, Christine Zhang, Matthew C. Kuner, Elizabeth Weaver, Ishan Amin, Hyunsoo Park, Yunsung Lim, Jihan Kim, Daryl Chrzan, Aron Walsh, Samuel M. Blau, Mark Asta, Aditi S. Krishnapriyan</dc:creator>
    </item>
    <item>
      <title>Guiding Application Users via Estimation of Computational Resources for Massively Parallel Chemistry Computations</title>
      <link>https://arxiv.org/abs/2509.20667</link>
      <description>arXiv:2509.20667v1 Announce Type: cross 
Abstract: In this work, we develop machine learning (ML) based strategies to predict resources (costs) required for massively parallel chemistry computations, such as coupled-cluster methods, to guide application users before they commit to running expensive experiments on a supercomputer. By predicting application execution time, we determine the optimal runtime parameter values such as number of nodes and tile sizes. Two key questions of interest to users are addressed. The first is the shortest-time question, where the user is interested in knowing the parameter configurations (number of nodes and tile sizes) to achieve the shortest execution time for a given problem size and a target supercomputer. The second is the cheapest-run question in which the user is interested in minimizing resource usage, i.e., finding the number of nodes and tile size that minimizes the number of node-hours for a given problem size.
  We evaluate a rich family of ML models and strategies, developed based on the collections of runtime parameter values for the CCSD (Coupled Cluster with Singles and Doubles) application executed on the Department of Energy (DOE) Frontier and Aurora supercomputers. Our experiments show that when predicting the total execution time of a CCSD iteration, a Gradient Boosting (GB) ML model achieves a Mean Absolute Percentage Error (MAPE) of 0.023 and 0.073 for Aurora and Frontier, respectively. In the case where it is expensive to run experiments just to collect data points, we show that active learning can achieve a MAPE of about 0.2 with just around 450 experiments collected from Aurora and Frontier.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20667v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>cs.DC</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tanzila Tabassum (Saday), Omer Subasi (Saday), Ajay Panyala (Saday), Epiya Ebiapia (Saday), Gerald Baumgartner (Saday), Erdal Mutlu (Saday),  P. (Saday),  Sadayappan, Karol Kowalski</dc:creator>
    </item>
    <item>
      <title>Human Semantic Representations of Social Interactions from Moving Shapes</title>
      <link>https://arxiv.org/abs/2509.20673</link>
      <description>arXiv:2509.20673v1 Announce Type: cross 
Abstract: Humans are social creatures who readily recognize various social interactions from simple display of moving shapes. While previous research has often focused on visual features, we examine what semantic representations that humans employ to complement visual features. In Study 1, we directly asked human participants to label the animations based on their impression of moving shapes. We found that human responses were distributed. In Study 2, we measured the representational geometry of 27 social interactions through human similarity judgments and compared it with model predictions based on visual features, labels, and semantic embeddings from animation descriptions. We found that semantic models provided complementary information to visual features in explaining human judgments. Among the semantic models, verb-based embeddings extracted from descriptions account for human similarity judgments the best. These results suggest that social perception in simple displays reflects the semantic structure of social interactions, bridging visual and abstract representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20673v1</guid>
      <category>cs.CV</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yiling Yun, Hongjing Lu</dc:creator>
    </item>
    <item>
      <title>FHRFormer: A Self-supervised Transformer Approach for Fetal Heart Rate Inpainting and Forecasting</title>
      <link>https://arxiv.org/abs/2509.20852</link>
      <description>arXiv:2509.20852v1 Announce Type: cross 
Abstract: Approximately 10\% of newborns require assistance to initiate breathing at birth, and around 5\% need ventilation support. Fetal heart rate (FHR) monitoring plays a crucial role in assessing fetal well-being during prenatal care, enabling the detection of abnormal patterns and supporting timely obstetric interventions to mitigate fetal risks during labor. Applying artificial intelligence (AI) methods to analyze large datasets of continuous FHR monitoring episodes with diverse outcomes may offer novel insights into predicting the risk of needing breathing assistance or interventions. Recent advances in wearable FHR monitors have enabled continuous fetal monitoring without compromising maternal mobility. However, sensor displacement during maternal movement, as well as changes in fetal or maternal position, often lead to signal dropouts, resulting in gaps in the recorded FHR data. Such missing data limits the extraction of meaningful insights and complicates automated (AI-based) analysis. Traditional approaches to handle missing data, such as simple interpolation techniques, often fail to preserve the spectral characteristics of the signals. In this paper, we propose a masked transformer-based autoencoder approach to reconstruct missing FHR signals by capturing both spatial and frequency components of the data. The proposed method demonstrates robustness across varying durations of missing data and can be used for signal inpainting and forecasting. The proposed approach can be applied retrospectively to research datasets to support the development of AI-based risk algorithms. In the future, the proposed method could be integrated into wearable FHR monitoring devices to achieve earlier and more robust risk detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20852v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CV</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kjersti Engan, Neel Kanwal, Anita Yeconia, Ladislaus Blacy, Yuda Munyaw, Estomih Mduma, Hege Ersdal</dc:creator>
    </item>
    <item>
      <title>Mojo: MLIR-Based Performance-Portable HPC Science Kernels on GPUs for the Python Ecosystem</title>
      <link>https://arxiv.org/abs/2509.21039</link>
      <description>arXiv:2509.21039v1 Announce Type: cross 
Abstract: We explore the performance and portability of the novel Mojo language for scientific computing workloads on GPUs. As the first language based on the LLVM's Multi-Level Intermediate Representation (MLIR) compiler infrastructure, Mojo aims to close performance and productivity gaps by combining Python's interoperability and CUDA-like syntax for compile-time portable GPU programming. We target four scientific workloads: a seven-point stencil (memory-bound), BabelStream (memory-bound), miniBUDE (compute-bound), and Hartree-Fock (compute-bound with atomic operations); and compare their performance against vendor baselines on NVIDIA H100 and AMD MI300A GPUs. We show that Mojo's performance is competitive with CUDA and HIP for memory-bound kernels, whereas gaps exist on AMD GPUs for atomic operations and for fast-math compute-bound kernels on both AMD and NVIDIA GPUs. Although the learning curve and programming requirements are still fairly low-level, Mojo can close significant gaps in the fragmented Python ecosystem in the convergence of scientific computing and AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21039v1</guid>
      <category>cs.DC</category>
      <category>cs.CE</category>
      <category>cs.ET</category>
      <category>cs.PL</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3731599.3767573</arxiv:DOI>
      <dc:creator>William F. Godoy, Tatiana Melnichenko, Pedro Valero-Lara, Wael Elwasif, Philip Fackler, Rafael Ferreira Da Silva, Keita Teranishi, Jeffrey S. Vetter</dc:creator>
    </item>
    <item>
      <title>MolFORM: Multi-modal Flow Matching for Structure-Based Drug Design</title>
      <link>https://arxiv.org/abs/2507.05503</link>
      <description>arXiv:2507.05503v2 Announce Type: replace 
Abstract: Structure-based drug design (SBDD) seeks to generate molecules that bind effectively to protein targets by leveraging their 3D structural information. While diffusion-based generative models have become the predominant approach for SBDD, alternative non-autoregressive frameworks remain relatively underexplored. In this work, we introduce MolFORM, a novel generative framework that jointly models discrete (atom types) and continuous (3D coordinates) molecular modalities using multi-flow matching. To further enhance generation quality, we incorporate a preference-guided fine-tuning stage based on Direct Preference Optimization (DPO), using Vina score as a reward signal. We propose a multi-modal flow DPO co-modeling strategy that simultaneously aligns discrete and continuous modalities, leading to consistent improvements across multiple evaluation metrics. The code is provided at: https://github.com/huang3170/MolForm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05503v2</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jie Huang, Daiheng Zhang</dc:creator>
    </item>
    <item>
      <title>R&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization</title>
      <link>https://arxiv.org/abs/2505.15155</link>
      <description>arXiv:2505.15155v2 Announce Type: replace-cross 
Abstract: Financial markets pose fundamental challenges for asset return prediction due to their high dimensionality, non-stationarity, and persistent volatility. Despite advances in large language models and multi-agent systems, current quantitative research pipelines suffer from limited automation, weak interpretability, and fragmented coordination across key components such as factor mining and model innovation. In this paper, we propose R&amp;D-Agent for Quantitative Finance, in short RD-Agent(Q), the first data-centric multi-agent framework designed to automate the full-stack research and development of quantitative strategies via coordinated factor-model co-optimization. RD-Agent(Q) decomposes the quant process into two iterative stages: a Research stage that dynamically sets goal-aligned prompts, formulates hypotheses based on domain priors, and maps them to concrete tasks, and a Development stage that employs a code-generation agent, Co-STEER, to implement task-specific code, which is then executed in real-market backtests. The two stages are connected through a feedback stage that thoroughly evaluates experimental outcomes and informs subsequent iterations, with a multi-armed bandit scheduler for adaptive direction selection. Empirically, RD-Agent(Q) achieves up to 2X higher annualized returns than classical factor libraries using 70% fewer factors, and outperforms state-of-the-art deep time-series models on real markets. Its joint factor-model optimization delivers a strong balance between predictive accuracy and strategy robustness. Our code is available at: https://github.com/microsoft/RD-Agent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15155v2</guid>
      <category>q-fin.CP</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuante Li, Xu Yang, Xiao Yang, Minrui Xu, Xisen Wang, Weiqing Liu, Jiang Bian</dc:creator>
    </item>
    <item>
      <title>Discovery Learning accelerates battery design evaluation</title>
      <link>https://arxiv.org/abs/2508.06985</link>
      <description>arXiv:2508.06985v2 Announce Type: replace-cross 
Abstract: Fast and reliable validation of novel designs in complex physical systems such as batteries is critical to accelerating technological innovation. However, battery research and development remain bottlenecked by the prohibitively high time and energy costs required to evaluate numerous new design candidates, particularly in battery prototyping and life testing. Despite recent progress in data-driven battery lifetime prediction, existing methods require labeled data of target designs to improve accuracy and cannot make reliable predictions until after prototyping, thus falling far short of the efficiency needed to enable rapid feedback for battery design. Here, we introduce Discovery Learning (DL), a scientific machine-learning paradigm that integrates active learning, physics-guided learning, and zero-shot learning into a human-like reasoning loop, drawing inspiration from learning theories in educational psychology. DL can learn from historical battery designs and actively reduce the need for prototyping, thus enabling rapid lifetime evaluation for unobserved material-design combinations without requiring additional data labeling. To test DL, we present 123 industrial-grade large-format lithium-ion pouch cells, spanning eight material-design combinations and diverse cycling protocols. Trained solely on public datasets of small-capacity cylindrical cells, DL achieves 7.2% test error in predicting the average cycle life under unknown device variability. This results in savings of 98% in time and 95% in energy compared to industrial practices. This work highlights the potential of uncovering insights from historical designs to inform and accelerate the development of next-generation battery technologies. DL represents a key advance toward efficient data-driven modeling and helps realize the promise of machine learning for accelerating scientific discovery and engineering innovation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06985v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Zhang, Yifei Zhang, Baozhao Yi, Yao Ren, Qi Jiao, Hanyu Bai, Weiran Jiang, Ziyou Song</dc:creator>
    </item>
    <item>
      <title>Isogeometric Topology Optimization Based on Topological Derivatives</title>
      <link>https://arxiv.org/abs/2509.09236</link>
      <description>arXiv:2509.09236v2 Announce Type: replace-cross 
Abstract: Topology optimization is a valuable tool in engineering, facilitating the design of optimized structures. However, topological changes often require a remeshing step, which can become challenging. In this work, we propose an isogeometric approach to topology optimization driven by topological derivatives. The combination of a level-set method together with an immersed isogeometric framework allows seamless geometry updates without the necessity of remeshing. At the same time, topological derivatives provide topological modifications without the need to define initial holes [7]. We investigate the influence of higher-degree basis functions in both the level-set representation and the approximation of the solution. Two numerical examples demonstrate the proposed approach, showing that employing higher-degree basis functions for approximating the solution improves accuracy, while linear basis functions remain sufficient for the level-set function representation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09236v2</guid>
      <category>math.NA</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guilherme Henrique Teixeira, Nepomuk Krenn, Peter Gangl, Benjamin Marussig</dc:creator>
    </item>
  </channel>
</rss>
