<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 Aug 2024 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 16 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>CarbonClipper: Optimal Algorithms for Carbon-Aware Spatiotemporal Workload Management</title>
      <link>https://arxiv.org/abs/2408.07831</link>
      <description>arXiv:2408.07831v1 Announce Type: new 
Abstract: We study carbon-aware spatiotemporal workload management, which seeks to address the growing environmental impact of data centers. We formalize this as an online problem called spatiotemporal online allocation with deadline constraints ($\mathsf{SOAD}$), in which an online player completes a workload (e.g., a batch compute job) by moving and scheduling the workload across a network subject to a deadline $T$. At each time step, a service cost function is revealed, representing, e.g., the carbon intensity of servicing a workload at each location, and the player must irrevocably decide the current allocation. Furthermore, whenever the player moves the allocation, it incurs a movement cost defined by a metric space $(X,d)$ that captures, e.g., the overhead of migrating a compute job. $\mathsf{SOAD}$ formalizes the open problem of combining general metrics and deadline constraints in the online algorithms literature, unifying problems such as metrical task systems and online search. We propose a competitive algorithm for $\mathsf{SOAD}$ along with a matching lower bound that proves it is optimal. Our main algorithm, ${\rm C{\scriptsize ARBON}C{\scriptsize LIPPER}}$, is a learning-augmented algorithm that takes advantage of predictions (e.g., carbon intensity forecasts) and achieves an optimal consistency-robustness trade-off. We evaluate our proposed algorithms for carbon-aware spatiotemporal workload management on a simulated global data center network, showing that ${\rm C{\scriptsize ARBON}C{\scriptsize LIPPER}}$ significantly improves performance compared to baseline methods and delivers meaningful carbon reductions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07831v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Lechowicz, Nicolas Christianson, Bo Sun, Noman Bashir, Mohammad Hajiesmaili, Adam Wierman, Prashant Shenoy</dc:creator>
    </item>
    <item>
      <title>Coupling without Communication and Drafter-Invariant Speculative Decoding</title>
      <link>https://arxiv.org/abs/2408.07978</link>
      <description>arXiv:2408.07978v1 Announce Type: new 
Abstract: Suppose Alice has a distribution $P$ and Bob has a distribution $Q$. Alice wants to generate a sample $a\sim P$ and Bob a sample $b \sim Q$ such that $a = b$ with has as high of probability as possible. It is well-known that, by sampling from an optimal coupling between the distributions, Alice and Bob can achieve $Pr[a = b] = 1 - D_{TV}(P,Q)$, where $D_{TV}(P,Q)$ is the total variation distance. What if Alice and Bob must solve this same problem without communicating at all? Perhaps surprisingly, with access to public randomness, they can still achieve $Pr[a = b] \geq \frac{1 - D_{TV}(P,Q)}{1 + D_{TV}(P,Q)} \geq 1-2D_{TV}(P,Q)$. In fact, this bound can be obtained using a simple protocol based on the Weighted MinHash algorithm. In this work, we explore the communication-free coupling in greater depth. First, we show that an equally simple protocol based on Gumbel sampling matches the worst-case guarantees of the Weighted MinHash approach, but tends to perform better in practice. Conversely, we prove that both approaches are actually sharp: no communication-free protocol can achieve $Pr[a=b]&gt;\frac{1 - D_{TV}(P,Q)}{1 + D_{TV}(P,Q)}$ in the worst-case. Finally, we prove that, for distributions over $n$ items, there exists a scheme that uses just $O(\log(n/\epsilon))$ bits of communication to achieve $Pr[a = b] = 1 - D_{TV}(P,Q) - \epsilon$, i.e. to essentially match optimal coupling. Beyond our theoretical results, we demonstrate an application of communication-free coupling to speculative decoding, a recent method for accelerating autoregressive large language models [Leviathan, Kalman, Matias, ICML 2023]. We show that communication-free protocols yield a variant of speculative decoding that we call Drafter-Invariant Speculative Decoding, which has the desirable property that the output of the method is fixed given a fixed random seed, regardless of what drafter is used for speculation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07978v1</guid>
      <category>cs.DS</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Majid Daliri, Christopher Musco, Ananda Theertha Suresh</dc:creator>
    </item>
    <item>
      <title>Evolving A* to Efficiently Solve the k Shortest-Path Problem (Extended Version)</title>
      <link>https://arxiv.org/abs/2408.08227</link>
      <description>arXiv:2408.08227v1 Announce Type: new 
Abstract: The problem of finding the shortest path in a graph G(V, E) has been widely studied. However, in many applications it is necessary to compute an arbitrary number of them, k. Even though the problem has raised a lot of interest from different research communities and many applications of it are known, it has not been addressed to the same extent as the single shortest path problem. The best algorithm known for efficiently solving this task has a time complexity of O (|E| + |V|log{|V|}+k|V|)$ when computing paths in explicit form, and is based on best-first search. This paper introduces a new search algorithm with the same time complexity, which results from a natural evolution of A* thus, it preserves all its interesting properties, making it widely applicable to many different domains. Experiments in various testbeds show a significant improvement in performance over the state of the art, often by one or two orders of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08227v1</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Linares L\'opez, Ian Herman</dc:creator>
    </item>
    <item>
      <title>Palette Sparsification for Graphs with Sparse Neighborhoods</title>
      <link>https://arxiv.org/abs/2408.08256</link>
      <description>arXiv:2408.08256v1 Announce Type: new 
Abstract: A celebrated palette sparsification result of Assadi, Chen, and Khanna states that in every $n$-vertex graph of maximum degree $\Delta$, sampling $\Theta(\log n)$ colors per vertex from $\{1,\ldots,\Delta+1\}$ almost certainly allows for a proper coloring from the sampled colors. Alon and Assadi extended this work proving a similar result for $O(\Delta/\log \Delta)$-coloring triangle-free graphs. Apart from being interesting results from a combinatorial standpoint, their results have various applications to the design of graph coloring algorithms in different models of computation.
  In this work, we focus on graphs with sparse neighborhoods. We say a graph $G = (V,E)$ is $k$-locally-sparse if for each vertex $v \in V$, the subgraph $G[N(v)]$ contains at most $k$ edges. A celebrated result of Alon, Krivelevich, and Sudakov shows that such graphs are $O(\Delta/\log (\Delta/\sqrt{k}))$-colorable. For any $\alpha\in(0,1)$ and $k\ll\Delta^{2\alpha}$, let $G$ be a $k$-locally-sparse graph. We show the following for $q=\Theta\left(\Delta/\log\left(\Delta^\alpha/\sqrt{k}\right)\right)$:
  1. Sampling $O(\Delta^\alpha+\sqrt{\log n})$ colors per vertex is sufficient to obtain a proper $q$-coloring of $G$ from the sampled colors.
  2. There exists a single-pass streaming algorithm which computes a proper $q$-coloring of $G$ with high probability using $\tilde O(n\Delta^{2\alpha})$ space.
  3. There exists a randomized non-adaptive sublinear-time algorithm which computes a proper $q$-coloring of $G$ with high probability using at most $\tilde O\left(n^{\frac32+\frac{\alpha}{2-2\alpha}}\right)$ queries.
  Our results recover and improve upon earlier work of Alon and Assadi. A key element in our proof is a proposition regarding correspondence coloring in the so-called color-degree setting, which improves upon recent work of Anderson, Kuchukova, and the author and is of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08256v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhishek Dhawan</dc:creator>
    </item>
    <item>
      <title>Optimal Scalarizations for Sublinear Hypervolume Regret</title>
      <link>https://arxiv.org/abs/2307.03288</link>
      <description>arXiv:2307.03288v3 Announce Type: replace-cross 
Abstract: Scalarization is a general, parallizable technique that can be deployed in any multiobjective setting to reduce multiple objectives into one, yet some have dismissed this versatile approach because linear scalarizations cannot explore concave regions of the Pareto frontier. To that end, we aim to find simple non-linear scalarizations that provably explore a diverse set of $k$ objectives on the Pareto frontier, as measured by the dominated hypervolume. We show that hypervolume scalarizations with uniformly random weights achieves an optimal sublinear hypervolume regret bound of $O(T^{-1/k})$, with matching lower bounds that preclude any algorithm from doing better asymptotically. For the setting of multiobjective stochastic linear bandits, we utilize properties of hypervolume scalarizations to derive a novel non-Euclidean analysis to get regret bounds of $\tilde{O}( d T^{-1/2} + T^{-1/k})$, removing unnecessary $\text{poly}(k)$ dependencies. We support our theory with strong empirical performance of using non-linear scalarizations that outperforms both their linear counterparts and other standard multiobjective algorithms in a variety of natural settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.03288v3</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qiuyi Zhang (Richard)</dc:creator>
    </item>
    <item>
      <title>Scalable Learning of Item Response Theory Models</title>
      <link>https://arxiv.org/abs/2403.00680</link>
      <description>arXiv:2403.00680v2 Announce Type: replace-cross 
Abstract: Item Response Theory (IRT) models aim to assess latent abilities of $n$ examinees along with latent difficulty characteristics of $m$ test items from categorical data that indicates the quality of their corresponding answers. Classical psychometric assessments are based on a relatively small number of examinees and items, say a class of $200$ students solving an exam comprising $10$ problems. More recent global large scale assessments such as PISA, or internet studies, may lead to significantly increased numbers of participants. Additionally, in the context of Machine Learning where algorithms take the role of examinees and data analysis problems take the role of items, both $n$ and $m$ may become very large, challenging the efficiency and scalability of computations. To learn the latent variables in IRT models from large data, we leverage the similarity of these models to logistic regression, which can be approximated accurately using small weighted subsets called coresets. We develop coresets for their use in alternating IRT training algorithms, facilitating scalable learning from large data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00680v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Susanne Frick, Amer Krivo\v{s}ija, Alexander Munteanu</dc:creator>
    </item>
    <item>
      <title>Counting Small Induced Subgraphs: Hardness via Fourier Analysis</title>
      <link>https://arxiv.org/abs/2407.07051</link>
      <description>arXiv:2407.07051v2 Announce Type: replace-cross 
Abstract: For a fixed graph property $\Phi$ and integer $k \geq 1$, the problem $\#\mathrm{IndSub}(\Phi,k)$ asks to count the induced $k$-vertex subgraphs satisfying $\Phi$ in an input graph $G$. If $\Phi$ is trivial on $k$-vertex graphs (i.e., if $\Phi$ contains either all or no $k$-vertex graphs), this problem is trivial. Otherwise we prove, among other results:
  If $\Phi$ is edge-monotone (i.e., closed under deleting edges), then $\#\mathrm{IndSub}(\Phi,k)$ cannot be solved in time $n^{o(k)}$ assuming ETH. This strengthens a result by D\"oring, Marx and Wellnitz [STOC 2024] that only ruled out an exponent of $o(\sqrt{\log k}/ \log \log k)$. Our results also hold when counting modulo fixed primes.
  If there is some fixed $\varepsilon &gt; 0$ such that at most $(2-\varepsilon)^{\binom{k}{2}}$ graphs on $k$ vertices satisfy $\Phi$, then $\#\mathrm{IndSub}(\Phi,k)$ cannot be solved in time $n^{o(k/\sqrt{\log k})}$ assuming ETH. Our results hold even when each of the graphs in $\Phi$ may come with an arbitrary individual weight. This generalizes previous results for hereditary properties by Focke and Roth [SIAM J.\ Comput.\ 2024] up to a $\sqrt{\log k}$ factor in the exponent of the lower bound.
  If $\Phi$ only depends on the number of edges, then $\#\mathrm{IndSub}(\Phi,k)$ cannot be solved in time $n^{o(k)}$ assuming ETH. This improves on a lower bound by Roth, Schmitt and Wellnitz [FOCS 2020] that only ruled out an exponent of $o(k / \sqrt{\log k})$.
  In all cases, we also obtain $\mathsf{\#W[1]}$-hardness if $k$ is part of the input and the problem is parameterized by $k$. We also obtain lower bounds on the Weisfeiler-Leman dimension. Our results follow from relatively straightforward Fourier analysis, and our paper subsumes most of the known $\mathsf{\#W[1]}$-hardness results known in the area, often with tighter lower bounds under ETH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07051v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Radu Curticapean, Daniel Neuen</dc:creator>
    </item>
  </channel>
</rss>
