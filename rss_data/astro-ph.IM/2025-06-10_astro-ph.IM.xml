<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>astro-ph.IM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/astro-ph.IM</link>
    <description>astro-ph.IM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/astro-ph.IM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Jun 2025 10:51:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Additive Manufacturing of Lunar Regolith for Reconfigurable Building Blocks toward Lunar Habitation</title>
      <link>https://arxiv.org/abs/2506.06392</link>
      <description>arXiv:2506.06392v1 Announce Type: new 
Abstract: Utilizing locally available materials is a crucial step towards sustainable planetary habitation. Lunar regolith has gained tremendous interest in additive manufacturing in the past decades. However, due to the constrained manufacturing facilities and materials on the moon, many existing additive manufacturing methods are not suitable for practical on-site manufacturing. Here, we envision that light-based direct sintering of lunar regolith can be a feasible approach. Instead of directly manufacturing large structures, we hypothesize that small-scale, reconfigurable building blocks can be an alternative to form large and complex structures. To verify the feasibility, we conducted laser sintering of lunar regolith simulants as a proof of concept, following a simple theoretical calculation for direct sintering using the light available in space. Different laser processing parameters are investigated to obtain controllable lunar regolith sintering. We further designed Lego-like interlocking bricks that are reconfigurable for different structure assemblies without additional material. Mechanical performance (compressive strength) of sintered cubic blocks is evaluated, showing a peak stress of ~1.5 MPa. We hope this work will inspire other in-space manufacturing techniques and enable low-cost space habitation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06392v1</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.EP</category>
      <category>cond-mat.other</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cole McCallum, Youwen Liang, Nahid Tushar, Ben Xu, Bo Zhao, Hao Zeng, Wan Shou</dc:creator>
    </item>
    <item>
      <title>A Python client for the ATLAS API</title>
      <link>https://arxiv.org/abs/2506.06403</link>
      <description>arXiv:2506.06403v1 Announce Type: new 
Abstract: The Asteroid Terrestrial-impact Last Alert System (ATLAS) is an all-sky optical sky survey with a cadence of 24 to 48 hours and the ATLAS Transient Server processes the alert stream to enable the discovery and follow-up of extra-galactic transients. The data from the ATLAS server can be accessed through a REST API, which has allowed the development of bots that need direct access to the data to help rank alerts and trigger followup observations of promising targets. Here we present the python client we have developed for the ATLAS API to help connect bots and scientists to our data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06403v1</guid>
      <category>astro-ph.IM</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Heloise F. Stevance, Jack Leland, Ken W. Smith</dc:creator>
    </item>
    <item>
      <title>High count rate effects in event processing for XRISM/Resolve x-ray microcalorimeter: II. Energy scale and resolution in orbit</title>
      <link>https://arxiv.org/abs/2506.06692</link>
      <description>arXiv:2506.06692v1 Announce Type: new 
Abstract: The Resolve instrument on the X-ray Imaging and Spectroscopy Mission (XRISM) uses a 36-pixel microcalorimeter designed to deliver high-resolution, non-dispersive X-ray spectroscopy. Although it is optimized for extended sources with low count rates, Resolve observations of bright point sources are still able to provide unique insights into the physics of these objects, as long as high count rate effects are addressed in the analysis. These effects include {the loss of exposure time for each pixel}, change on the energy scale, and change on the energy resolution. To investigate these effects under realistic observational conditions, we observed the bright X-ray source, the Crab Nebula, with XRISM at several offset positions with respect to the Resolve field of view and with continuous illumination from {$^{55}$Fe sources} on the filter wheel. For the spectral analysis, we excluded data where exposure time loss was too significant to ensure reliable spectral statistics. The energy scale at 6 keV shows a slight negative shift in the high-count-rate regime. The energy resolution at 6 keV worsens as the count rate in electrically neighboring pixels increases, but can be restored by applying a nearest-neighbor coincidence cut (``cross-talk cut''). We examined how these effects influence the observation of bright point sources, using GX 13+1 as a test case, and identified an eV-scale energy offset at 6 keV between the inner (brighter) and outer (fainter) pixels. Users who seek to analyze velocity structures on the order of tens of km~s$^{-1}$ should account for such high count rate effects. These findings will aid in the interpretation of Resolve data from bright sources and provide valuable considerations for designing and planning for future microcalorimeter missions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06692v1</guid>
      <category>astro-ph.IM</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Misaki Mizumoto, Yoshiaki Kanemaru, Shinya Yamada, Caroline A. Kilbourne, Megan E. Eckart, Edmund Hodges-Kluck, Yoshitaka Ishisaki, Frederick S. Porter, Katja Pottschmidt, Tsubasa Tamba</dc:creator>
    </item>
    <item>
      <title>Development of an imager with high time resolution optical photon counter</title>
      <link>https://arxiv.org/abs/2506.07442</link>
      <description>arXiv:2506.07442v1 Announce Type: new 
Abstract: Astrophysical transient phenomena on sub-millisecond timescales, such as fast radio bursts and giant radio pulses from the Crab pulsar, have been primarily observed in radio wavebands. To investigate their origins, a photon detector with high sensitivity and high time resolution is required also in other wavelengths. Recently, we developed the Imager of MPPC-based Optical photoN counter from Yamagata (IMONY), an observation system utilizing a Geiger-mode avalanche photodiode (GAPD) as a sensor. The sensor consists of 64 pixels, each comprising a GAPD and a quenching resistor, with pixel sizes of 75, 100, 150, and 200\,$\mu$m. Each pixel signal is read out independently, enabling single-photon detection. After successfully observing the Crab pulsar using two Japanese telescopes, we upgraded the readout boards to achieve a more compact and stable system. The new system incorporates an analog application-specific integrated circuit (ASIC) developed at KEK for multi-purpose fast readout for silicon photomultipliers. This ASIC features a fast transimpedance amplifier and a comparator, independently processing 16 channels. A Global Navigation Satellite System receiver and a Field Programmable Gate Array (FPGA) provide timestamps for each detected photon with a resolution of 100 ns. The FPGA transmits the acquired data to a PC via Ethernet. This paper presents the details of the new system and the results of its initial evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07442v1</guid>
      <category>astro-ph.IM</category>
      <category>physics.ins-det</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1088/1748-0221/20/06/C06002</arxiv:DOI>
      <arxiv:journal_reference>published by JINST, 20, C06002, 2025</arxiv:journal_reference>
      <dc:creator>A. Sato, T. Nakamori, M. Shoji, T. Sato, K. Hashiyama, M. Hasebe, M. Maeshiro, R. Sato, R. Honda, M. Miyahara</dc:creator>
    </item>
    <item>
      <title>Mitigating Polarization Leakage in Gas Pixel Detectors through Hybrid Machine Learning and Analytic Event Reconstruction</title>
      <link>https://arxiv.org/abs/2506.07828</link>
      <description>arXiv:2506.07828v1 Announce Type: new 
Abstract: Spatially resolved polarization measurements of extended X-ray sources are expanding our understanding of the emission mechanisms and magnetic field properties involved. Such measurements have been possible in the past few years thanks to the Imaging X-ray Polarimetry Explorer (IXPE). However, the analysis of extended sources suffers a systematic effect known as polarization leakage, which artificially affects the measured polarization signal. To address this issue, we built a hybrid reconstruction algorithm, which combines machine learning and analytic techniques to improve the reconstruction of photoelectron tracks in the Gas Pixel Detector and to significantly mitigate polarization leakage. This work presents the first application of this hybrid method to experimental data, including both calibration lab measurements and IXPE observational data. We confirmed the reliable performance of the hybrid method for both cases. Additionally, we demonstrated the algorithm's effectiveness in reducing the polarization leakage effect through the analysis of the IXPE observation of the supernova remnant G21.5-0.9. By enabling more reliable polarization measurements, this method can potentially yield deeper insights into the magnetic field structures, particle acceleration processes, and emission mechanisms at work within extended X-ray sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07828v1</guid>
      <category>astro-ph.IM</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3847/1538-4357/adc92c</arxiv:DOI>
      <arxiv:journal_reference>The Astrophysical Journal (2025), Volume 984, Number 2</arxiv:journal_reference>
      <dc:creator>Nicol\'o Cibrario, Michela Negro, Raffaella Bonino, Nikita Moriakov, Luca Baldini, Niccol\'o Di Lalla, Alessandro Di Marco, Sergio Fabiani, Andrea Frass\'a, Alessio Gorgi, Fabio La Monaca, Luca Latronico, Simone Maldera, Alberto Manfreda, Fabio Muleri, Nicola Omodei, John Rankin, Carmelo Sgr\'o, Stefano Silvestri, Paolo Soffitta, Stefano Tugliani</dc:creator>
    </item>
    <item>
      <title>Maximizing Ariel's Survey Leverage for Population-Level Studies of Exoplanets</title>
      <link>https://arxiv.org/abs/2506.06429</link>
      <description>arXiv:2506.06429v1 Announce Type: cross 
Abstract: ESA's Ariel mission will be uniquely suited to performing population-level studies of exoplanets. Most of these studies consist of quantifying trends between an Ariel-measured quantity, y, and an a priori planetary property, x; for example, atmospheric metallicity as inferred from Ariel transit spectroscopy vs. planetary radius. We define the leverage of a survey with N targets as L = sqrt(N)stdev(x) and show that it quantitatively predicts the precision of population-level trends. The target selection challenge of Ariel can therefore be summarized as maximizing L along some axes of diversity for a given cumulative observing time. To this end, we consider different schemes to select the mission reference sample for a notional three year transit spectroscopy survey with Ariel. We divide the exoplanets in the mission candidate sample into logarithmic classes based on radius, equilibrium temperature, and host star temperature. We then construct a target list by cyclically choosing the easiest remaining target in each class. We find that in many cases the leverage is greatest for a single class: dividing planets into multiple classes increases the diversity of targets, but reduces their numbers. The leverage on a single axis of diversity can be increased by dividing that axis into many planet classes, but this sacrifices leverage along other axes of diversity. We conclude that a modest number of classes, possibly only one, should be defined when selecting Ariel targets. Lastly, we note that the statistical leverage of the Ariel transit survey would be significantly increased if current candidate planets were confirmed. This highlights the urgency of vetting and confirming the easiest transmission and emission spectroscopy targets in the Ariel mission candidate sample.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06429v1</guid>
      <category>astro-ph.EP</category>
      <category>astro-ph.IM</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas B. Cowan, Ben Coull-Neveu</dc:creator>
    </item>
    <item>
      <title>New Methods for Offline GstLAL Analyses</title>
      <link>https://arxiv.org/abs/2506.06497</link>
      <description>arXiv:2506.06497v1 Announce Type: cross 
Abstract: In this work, we present new methods implemented in the GstLAL offline gravitational wave search. These include a technique to reuse the matched filtering data products from a GstLAL online analysis, which hugely reduces the time and computational resources required to obtain offline results; a technique to combine these results with a separate search for heavier black hole mergers, enabling detections from a larger set of gravitational wave sources; changes to the likelihood ratio which increases the sensitivity of the analysis; and two separate changes to the background estimation, allowing more precise significance estimation of gravitational wave candidates. Some of these methods increase the sensitivity of the analysis, whereas others correct previous mis-estimations of sensitivity by eliminating false positives. These methods have been adopted for GstLAL's offline results during the fourth observing run of LIGO, Virgo, and KAGRA (O4). To test these new methods, we perform an offline analysis over one chunk of O3 data, lasting from May 12 19:36:42 UTC 2019 to May 21 14:45:08 UTC 2019, and compare it with previous GstLAL results over the same period of time. We show that cumulatively these methods afford around a 50% - 100% increase in sensitivity in the highest mass space, while simultaneously increasing the reliability of results, and making them more reusable and computationally cheaper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06497v1</guid>
      <category>gr-qc</category>
      <category>astro-ph.IM</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Prathamesh Joshi, Leo Tsukada, Chad Hanna, Shomik Adhicary, Debnandini Mukherjee, Wanting Niu, Shio Sakon, Divya Singh, Pratyusava Baral, Amanda Baylor, Kipp Cannon, Sarah Caudill, Bryce Cousins, Jolien D. E. Creighton, Becca Ewing, Heather Fong, Richard N. George, Patrick Godwin, Reiko Harada, Yun-Jing Huang, Rachael Huxford, James Kennington, Soichiro Kuwahara, Alvin K. Y. Li, Ryan Magee, Duncan Meacher, Cody Messick, Soichiro Morisaki, Alexander Pace, Cort Posnansky, Anarya Ray, Surabhi Sachdev, Stefano Schmidt, Urja Shah, Ron Tapia, Koh Ueno, Aaron Viets, Leslie Wade, Madeline Wade, Zach Yarbrough, Noah Zhang</dc:creator>
    </item>
    <item>
      <title>Teaching Astronomy with Large Language Models</title>
      <link>https://arxiv.org/abs/2506.06921</link>
      <description>arXiv:2506.06921v1 Announce Type: cross 
Abstract: We present a study of LLM integration in final-year undergraduate astronomy education, examining how students develop AI literacy through structured guidance and documentation requirements. We developed AstroTutor, a domain-specific astronomy tutoring system enhanced with curated arXiv content, and deployed it alongside general-purpose LLMs in the course. Students documented their AI usage through homework reflections and post-course surveys. We analyzed student evolution in AI interaction strategies and conducted experimental comparisons of LLM-assisted versus traditional grading methods. LLM grading showed strong correlation with human evaluation while providing more detailed and consistent feedback. We also piloted LLM-facilitated interview-based examinations as a scalable alternative to traditional assessments, demonstrating potential for individualized evaluation that addresses common testing limitations. Students experienced decreased rather than increased reliance on LLMs over the semester, developing critical evaluation skills and strategic tool selection. They evolved from basic assistance-seeking to verification workflows, with documentation requirements fostering metacognitive awareness. Students developed effective prompting strategies, contextual enrichment techniques, and cross-verification practices. Our findings suggest that structured LLM integration with transparency requirements and domain-specific tools can enhance astronomy education while building essential AI literacy skills. We provide implementation guidelines for educators and make our AstroTutor repository freely available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06921v1</guid>
      <category>physics.ed-ph</category>
      <category>astro-ph.CO</category>
      <category>astro-ph.GA</category>
      <category>astro-ph.IM</category>
      <category>astro-ph.SR</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan-Sen Ting, Teaghan O'Briain</dc:creator>
    </item>
    <item>
      <title>Extracting the Epoch of Reionization Signal with 3D U-Net Neural Networks Using Data-driven Systematic Effect Model</title>
      <link>https://arxiv.org/abs/2412.16853</link>
      <description>arXiv:2412.16853v2 Announce Type: replace 
Abstract: Neutral hydrogen (HI) serves as a crucial probe for the Cosmic Dawn and the Epoch of Reionization (EoR). Actual observations of the 21-cm signal often encounter challenges such as thermal noise and various systematic effects. To overcome these challenges, we simulate SKA-Low-depth images in South Celestial Pole (SCP) field and process them with a deep learning method. We utilized foreground residuals acquired by LOFAR during actual North Celestial Pole (NCP) field observations, thermal and excess variances calculated via Gaussian process regression (GPR), and 21-cm signals generated with 21cmFAST for signal extraction tests. Our approach to overcome these foreground, thermal noise, and excess variance components employs a 3D U-Net neural network architecture for image analysis. When considering thermal noise corresponding to 1752 hours of integration time, U-Net provides reliable 2D power spectrum predictions, and robustness tests ensure that we get realistic EoR signals. Adding foreground residuals, however, causes inconsistencies below the horizon delay-line. Lastly, evaluating both thermal noise and excess variances with observations up to 4380 hours and 13140 hours ensures reliable power spectrum estimations within the EoR window and across nearly all scales, respectively. The incoherence of excess variances in the frequency direction can greatly affect deep learning to extract 21-cm signals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16853v2</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.CO</category>
      <category>gr-qc</category>
      <category>hep-ph</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Li-Yang Gao, L\'eon V. E. Koopmans, Florent G. Mertens, Satyapan Munshi, Yichao Li, Stefanie A. Brackenhoff, Emilio Ceccotti, J. Kariuki Chege, Anshuman Acharya, Raghunath Ghara, Sambit K. Giri, Ilian T. Iliev, Garrelt Mellema, Xin Zhang</dc:creator>
    </item>
    <item>
      <title>A Cloud-native Agile approach to cyber platform prototyping and integration for astronomy: the ENGAGE SKA case</title>
      <link>https://arxiv.org/abs/2502.04039</link>
      <description>arXiv:2502.04039v2 Announce Type: replace 
Abstract: The Square Kilometre Array (SKA) Observatory is gearing up the formal construction of its two radio interferometers in Australia and South Africa after the end of design and pre-construction phases. Agile methodologies, the Cloud native Computing technologies and the DevOps software ideas are influencing the design of compute infrastructures that will be key to reduce the operational costs of SKA while improving the control and monitoring of the SKA antennas and ancillary systems, Correlators, HPC facilities or related data centre tiered systems. These tools will likely include advanced power metering technologies and efficient distribution automation and Network Operation Centres (NOC). SKA will become the world's largest radio telescope and is expected to achieve its first science by 2026. To cope with this dimension and complexity, a key part of this distributed Observatory is the overall software control and monitoring system embodied in the Observatory Management and Control (OMC) and the Services Teams that requires specialized Agile Teams to assist in software and cyber infrastructure building using an Agile development environment that includes test automation, Continuous Integration, and Continuous Deployment. To manage such a large and distributed machine, the Agile approach was adopted for the core software package of the SKA Telescope aimed at scheduling observations, controlling their execution, monitoring the telescope status and ensuring scalability and reliability. Here, we report on the ENGAGE SKA ciberinfrastructure prototyping support to the SKA Agile Software Development Life Cycle (SDLC).</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04039v2</guid>
      <category>astro-ph.IM</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>physics.med-ph</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Domingos Barbosa, Diogo Regateiro, Jo\~ao Paulo Barraca, Dzianis Bartashevich, Marco Bartolini, Matteo di Carlo, Piers Harding, Dalmiro Maia, Bruno Morgado, Domingos Nunes, Bruno Ribeiro, Bruno Coelho, Val\'erio Ribeiro, Allan K. de Almeida Jr, Timoth\'ee Vaillant, U\u{g}ur Yilmaz</dc:creator>
    </item>
    <item>
      <title>Atacama Large Aperture Submillimeter Telescope (AtLAST) Science: Resolving the Hot and Ionized Universe through the Sunyaev-Zeldovich effect</title>
      <link>https://arxiv.org/abs/2403.00909</link>
      <description>arXiv:2403.00909v2 Announce Type: replace-cross 
Abstract: An omnipresent feature of the multi-phase ``cosmic web'' is that warm/hot (&gt;$10^5$ K) ionized gas pervades it. This gas constitutes a relevant contribution to the overall universal matter budget across multiple scales, from the several tens of Mpc-scale IGM filaments, to the Mpc ICM, all the way down to the CGM surrounding individual galaxies from ~1 kpc up to their respective virial radii (~100 kpc). The study of the hot baryonic component of cosmic matter density represents a powerful means for constraining the intertwined evolution of galactic populations and large-scale cosmological structures, for tracing the matter assembly in the Universe and its thermal history. To this end, the SZ effect provides the ideal observational tool for measurements out to the beginnings of structure formation. The SZ effect is caused by the scattering of the photons from the cosmic microwave background off the hot electrons embedded within cosmic structures, and provides a redshift-independent perspective on the thermal and kinematic properties of the warm/hot gas. Still, current and future (sub)mm facilities have been providing only a partial view of the SZ Universe due to any combination of: limited angular resolution, spectral coverage, field of view, spatial dynamic range, sensitivity. In this paper, we motivate the development of a wide-field, broad-band, multi-chroic continuum instrument for the Atacama Large Aperture Submillimeter Telescope (AtLAST) by identifying the scientific drivers that will deepen our understanding of the complex thermal evolution of cosmic structures. On a technical side, this will necessarily require efficient multi-wavelength mapping of the SZ signal with an unprecedented spatial dynamic range (from arcsecond to tens of arcminutes) and we employ theoretical forecasts to determine the key instrumental constraints for achieving our goals. [abridged]</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00909v2</guid>
      <category>astro-ph.CO</category>
      <category>astro-ph.GA</category>
      <category>astro-ph.IM</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.12688/openreseurope.17449.2</arxiv:DOI>
      <arxiv:journal_reference>Open Research Europe, 4:113 (2025)</arxiv:journal_reference>
      <dc:creator>Luca Di Mascolo, Yvette Perrott, Tony Mroczkowski, Srinivasan Raghunathan, Stefano Andreon, Stefano Ettori, Aurora Simionescu, Joshiwa van Marrewijk, Claudia Cicone, Minju Lee, Dylan Nelson, Laura Sommovigo, Mark Booth, Pamela Klaassen, Paola Andreani, Martin A. Cordiner, Doug Johnstone, Eelco van Kampen, Daizhong Liu, Thomas J. Maccarone, Thomas W. Morris, John Orlowski-Scherer, Am\'elie Saintonge, Matthew Smith, Alexander E. Thelen, Sven Wedemeyer</dc:creator>
    </item>
    <item>
      <title>Length dependence of waveform mismatch: a caveat on waveform accuracy</title>
      <link>https://arxiv.org/abs/2502.14025</link>
      <description>arXiv:2502.14025v2 Announce Type: replace-cross 
Abstract: The Simulating eXtreme Spacetimes Collaboration's code SpEC can now routinely simulate binary black hole mergers undergoing $\sim25$ orbits, with the longest simulations undergoing nearly $\sim180$ orbits. While this sounds impressive, the mismatch between the highest resolutions for this long simulation is $\mathcal{O}(10^{-1})$. Meanwhile, the mismatch between resolutions for the more typical simulations tends to be $\mathcal{O}(10^{-4})$, despite the resolutions being similar to the long simulations'. In this note, we explain why mismatch alone gives an incomplete picture of code -- and waveform -- quality, especially in the context of providing waveform templates for LISA and 3G detectors, which require templates with $\mathcal{O}(10^{3}) - \mathcal{O}(10^{5})$ orbits. We argue that to ready the GW community for the sensitivity of future detectors, numerical relativity groups must be aware of this caveat, and also run future simulations with at least three resolutions to properly assess waveform accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14025v2</guid>
      <category>gr-qc</category>
      <category>astro-ph.IM</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1088/1361-6382/add8d9</arxiv:DOI>
      <arxiv:journal_reference>2025 Class. Quantum Grav. 42 117001</arxiv:journal_reference>
      <dc:creator>Keefe Mitman, Leo C. Stein, Michael Boyle, Nils Deppe, Lawrence E. Kidder, Harald P. Pfeiffer, Mark A. Scheel</dc:creator>
    </item>
    <item>
      <title>Comparative Analysis of Richardson-Lucy Deconvolution and Data Unfolding with Mean Integrated Square Error Optimization</title>
      <link>https://arxiv.org/abs/2505.10283</link>
      <description>arXiv:2505.10283v2 Announce Type: replace-cross 
Abstract: Two maximum likelihood-based algorithms for unfolding or deconvolution are considered: the Richardson-Lucy method and the Data Unfolding method with Mean Integrated Square Error (MISE) optimization [10]. Unfolding is viewed as a procedure for estimating an unknown probability density function. Both external and internal quality assessment methods can be applied for this purpose. In some cases, external criteria exist to evaluate deconvolution quality. A typical example is the deconvolution of a blurred image, where the sharpness of the restored image serves as an indicator of quality. However, defining such external criteria can be challenging, particularly when a measurement has not been performed previously. In such instances, internal criteria are necessary to assess the quality of the result independently of external information. The article discusses two internal criteria: MISE for the unfolded distribution and the condition number of the correlation matrix of the unfolded distribution. These internal quality criteria are applied to a comparative analysis of the two methods using identical numerical data. The results of the analysis demonstrate the superiority of the Data Unfolding method with MISE optimization over the Richardson-Lucy method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10283v2</guid>
      <category>physics.data-an</category>
      <category>astro-ph.IM</category>
      <category>hep-ex</category>
      <category>nucl-ex</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolay D. Gagunashvili</dc:creator>
    </item>
    <item>
      <title>Determination of Light Curve Parameters of Poorly Studied Eclipsing Variables Using Data from Tess and Other Sky Surveys</title>
      <link>https://arxiv.org/abs/2505.21843</link>
      <description>arXiv:2505.21843v2 Announce Type: replace-cross 
Abstract: A group of poorly studied eclipsing variables (the classification of which is marked as uncertain and/or the period of brightness changes is uncertain) has been studied with the using of the photometric observations of the TESS mission and NSVS, ASAS-SN sky surveys. We also obtained some observations covering the brightness minima of our variables by our group using the telescopes at Astronomical Observatory on Kolonica Saddle (Slovakia) and Observatory and Planetarium in Hlohovec (Slovakia) during the "Variable-2024" astrocamp. The periods and classification were corrected. For NSV 575 and NSV 014 the periods were found for the first time, but it is doubtful that NSV 014 is an eclipsing variable, because there are no eclipses but the asymmetric wave is present, which indicates that the variable star can be re-classified as a low-amplitude pulsating one. Different methods were used for approximation of the light curves and further calculation of stellar system's parameters such as eclipse depths and durations, values of reflection effect and effect of ellipticity of stars. The initial period was estimated using the periodogram based on the trigonometrical polynomial fit of high order (up to 10). For better approximation of the complete eclipsing phase curve, the "New Algol Variable" (NAV) software was used. The methods of "asymptotic parabolas" and "wall-supported asymptotic parabolas" were used for calculation of moments of eclipses, which use only near-eclipse part of the observations instead of a complete curve. These methods were implemented in the software MAVKA among a larger set of features. For the variables NSV 489 and NSV 1884, our moments of eclipses and the ones found in the literature, were used for the O-C curves. For NSV 489, the period was adjusted taking into account the slope of the O-C diagram.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21843v2</guid>
      <category>astro-ph.SR</category>
      <category>astro-ph.IM</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.15407/kfnt2025.04.020</arxiv:DOI>
      <arxiv:journal_reference>"Kinematyka i Fizika Nebesnykh til" (KFNT), v.41, No 4 (July-August), p. 20-30 (2025)</arxiv:journal_reference>
      <dc:creator>Vladyslava I. Marsakova, Ivan L. Andronov, Victoriia O. Borshchenko, Illia. A. Garbazhii-Romanchenko, Anastasiia D. Lashkova, Sofia A. Kreminska, Pavol A. Dubovsky, Vladyslav V. Dubovskyi, Karol Petrik</dc:creator>
    </item>
  </channel>
</rss>
