<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 27 May 2024 04:00:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 27 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Parallel Approximations for High-Dimensional Multivariate Normal Probability Computation in Confidence Region Detection Applications</title>
      <link>https://arxiv.org/abs/2405.14892</link>
      <description>arXiv:2405.14892v1 Announce Type: new 
Abstract: Addressing the statistical challenge of computing the multivariate normal (MVN) probability in high dimensions holds significant potential for enhancing various applications. One common way to compute high-dimensional MVN probabilities is the Separation-of-Variables (SOV) algorithm. This algorithm is known for its high computational complexity of O(n^3) and space complexity of O(n^2), mainly due to a Cholesky factorization operation for an n X n covariance matrix, where $n$ represents the dimensionality of the MVN problem. This work proposes a high-performance computing framework that allows scaling the SOV algorithm and, subsequently, the confidence region detection algorithm. The framework leverages parallel linear algebra algorithms with a task-based programming model to achieve performance scalability in computing process probabilities, especially on large-scale systems. In addition, we enhance our implementation by incorporating Tile Low-Rank (TLR) approximation techniques to reduce algorithmic complexity without compromising the necessary accuracy. To evaluate the performance and accuracy of our framework, we conduct assessments using simulated data and a wind speed dataset. Our proposed implementation effectively handles high-dimensional multivariate normal (MVN) probability computations on shared and distributed-memory systems using finite precision arithmetics and TLR approximation computation. Performance results show a significant speedup of up to 20X in solving the MVN problem using TLR approximation compared to the reference dense solution without sacrificing the application's accuracy. The qualitative results on synthetic and real datasets demonstrate how we maintain high accuracy in detecting confidence regions even when relying on TLR approximation to perform the underlying linear algebra operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14892v1</guid>
      <category>cs.DC</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiran Zhang, Sameh Abdulah, Jian Cao, Hatem Ltaief, Ying Sun, Marc G. Genton, David E. Keyes</dc:creator>
    </item>
    <item>
      <title>Dynamically Sharded Ledgers on a Distributed Hash Table</title>
      <link>https://arxiv.org/abs/2405.14991</link>
      <description>arXiv:2405.14991v1 Announce Type: new 
Abstract: Distributed ledger technology such as blockchain is considered essential for supporting large numbers of micro-transactions in the Machine Economy, which is envisioned to involve billions of connected heterogeneous and decentralized cyber-physical systems. This stresses the need for performance and scalability of distributed ledger technologies. Sharding divides the blockchain network into multiple committees and is a common approach to improve scalability. However, with current sharding approaches, costly cross-shard verification is needed to prevent double-spending. This paper proposes a novel and more scalable distributed ledger method named ScaleGraph that implements dynamic sharding by using routing and logical proximity concepts from distributed hash tables. ScaleGraph addresses cyber security in terms of integrity, availability, and trust, to support frequent micro-transactions between autonomous devices. Benefits of ScaleGraph include a total storage space complexity of O(t), where t is the global number of transactions (assuming a constant replication degree). This space is sharded over n nodes so that each node needs O(t/n) storage, which provides a high level of concurrency and data localization as compared to other delegated consensus proposals. ScaleGraph allows for a dynamic grouping of validators which are selected based on a distance metric. We analyze the consensus requirements in such a dynamic setting and show that a synchronous consensus protocol allows shards to be smaller than an asynchronous one, and likely yields better performance. Moreover, we provide an experimental analysis of security aspects regarding the required size of the consensus groups with ScaleGraph. Our analysis shows that dynamic sharding based on proximity concepts brings attractive scalability properties in general, especially when the fraction of corrupt nodes is small.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14991v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoffer Fink, Olov Schel\'en, Ulf Bodin</dc:creator>
    </item>
    <item>
      <title>Fast Transaction Scheduling in Blockchain Sharding</title>
      <link>https://arxiv.org/abs/2405.15015</link>
      <description>arXiv:2405.15015v1 Announce Type: new 
Abstract: Sharding is a promising technique for addressing the scalability issues of blockchain. It divides the $n$ participating nodes into $s$ disjoint groups called shards, where each shard processes transactions in parallel. We investigate scheduling algorithms for the blockchain sharding systems, where each transaction resides in a shard of the communication graph and attempts to access accounts at possibly remote shards. We examine batch scheduling problems on the shard graph $G_s$, where given a set of transactions, we aim to find efficient schedules to execute them as fast as possible. First, we present a centralized scheduler where one of the shards has global knowledge of transactions to be processed. For general graphs, where the transaction and its accessing objects are arbitrarily far from each other with a maximum distance $d$, the centralized scheduler provides $O(kd)$ approximation to the optimal schedule, where $k$ is the maximum number of shards each transaction accesses. Consequently, for a Clique graph where shards are at a unit distance from each other, we obtain $O(k)$ approximation to the optimal schedule. We also get $O(k \log s)$ approximation for Hypercube, Butterfly, and $g$-dimensional Grid, where $g=O(\log s)$. Next, we provide a centralized scheduler with a bucketing approach that offers improved bounds for special cases. Finally, we provide a distributed scheduler where shards do not require global transaction information. We achieve this by using a hierarchical clustering of the shards and using the centralized scheduler in each cluster. We show that the distributed scheduler has a competitive ratio of $O(\mathcal{A_\mathcal{CS}} \log ^2 s)$, where $\mathcal{A_\mathcal{CS}}$ is the approximation ratio of the centralized scheduler. To our knowledge, we are the first to give provably fast transaction scheduling algorithms for blockchain sharding systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15015v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ramesh Adhikari, Costas Busch, Miroslav Popovic</dc:creator>
    </item>
    <item>
      <title>A Note on Solving Problems of Substantially Super-linear Complexity in $N^{o(1)}$ Rounds of the Congested Clique</title>
      <link>https://arxiv.org/abs/2405.15270</link>
      <description>arXiv:2405.15270v1 Announce Type: new 
Abstract: We study the possibility of designing $N^{o(1)}$-round protocols for problems of substantially super-linear polynomial-time complexity on the congested clique with about $N^{1/2}$ nodes, where $N$ is the input size. We show that the exponent of the polynomial (if any) bounding the average time complexity of local computations performed at a node in such protocols has to be larger than that of the polynomial bounding the time complexity of the given problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15270v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrzej Lingas</dc:creator>
    </item>
    <item>
      <title>Decentralized Virtual Research Environment: Empowering Peer-to-Peer Trustworthy Data Sharing and Collaboration</title>
      <link>https://arxiv.org/abs/2405.15392</link>
      <description>arXiv:2405.15392v1 Announce Type: new 
Abstract: Scientific research, increasingly reliant on data and computational analysis, confronts the challenge of integrating collaboration and data sharing across disciplines. Collaborative frameworks that support decentralized decision-making and knowledge-sharing are essential, yet integrating them into computational environments presents technical challenges, such as decentralized identity, user-centered policy-making, flexible asset management, automated provenance, and distributed collaborative workflow management. In this work, we propose a novel framework called Decentralized Virtual Research Environment (D-VRE) that addresses barriers to scientific research collaboration and data sharing, offering a scalable and adaptable decentralized model. It enhances seamless, trusted data sharing and collaboration within research lifecycles. It incorporates custom sharing policies, secure asset management, collaborative workflows, and research activity tracking, all without centralized oversight. Demonstrated through a real-world case study in the CLARIFY project, D-VRE proved effective in enabling advanced data sharing and collaborative scenarios, showcasing its adaptability in scientific research. Integrated into JupyterLab, D-VRE supports custom collaboration agreements and smart contract-based automated execution on the Ethereum blockchain. This ensures secure, verifiable transactions and promotes trust and reliability in shared research findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15392v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuandou Wang, Siamak Farshidi, Sheejan Tripathi, Zhiming Zhao</dc:creator>
    </item>
    <item>
      <title>Increasing Efficiency and Result Reliability of Continuous Benchmarking for FaaS Applications</title>
      <link>https://arxiv.org/abs/2405.15610</link>
      <description>arXiv:2405.15610v1 Announce Type: new 
Abstract: In a continuous deployment setting, Function-as-a-Service (FaaS) applications frequently receive updated releases, each of which can cause a performance regression. While continuous benchmarking, i.e., comparing benchmark results of the updated and the previous version, can detect such regressions, performance variability of FaaS platforms necessitates thousands of function calls, thus, making continuous benchmarking time-intensive and expensive.
  In this paper, we propose DuetFaaS, an approach which adapts duet benchmarking to FaaS applications. With DuetFaaS, we deploy two versions of FaaS function in a single cloud function instance and execute them in parallel to reduce the impact of platform variability. We evaluate our approach against state-of-the-art approaches, running on AWS Lambda. Overall, DuetFaaS requires fewer invocations to accurately detect performance regressions than other state-of-the-art approaches. In 99.65% of evaluated cases, our approach provides smaller confidence interval sizes than the comparing approaches, and can reduce the size by up to 98.23%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15610v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim C. Rese, Nils Japke, Sebastian Koch, Tobias Pfandzelter, David Bermbach</dc:creator>
    </item>
    <item>
      <title>A Comparative Analysis of Distributed Training Strategies for GPT-2</title>
      <link>https://arxiv.org/abs/2405.15628</link>
      <description>arXiv:2405.15628v1 Announce Type: new 
Abstract: The rapid advancement in Large Language Models has been met with significant challenges in their training processes, primarily due to their considerable computational and memory demands. This research examines parallelization techniques developed to address these challenges, enabling the efficient and scalable training of Large Language Models. A comprehensive analysis of both data and model parallelism strategies, including Fully Sharded Data Parallelism and Distributed Data-Parallel frameworks, is provided to assess methods that facilitate efficient model training. Furthermore, the architectural complexities and training methodologies of the Generative Pre-Trained Transformer-2 model are explored. The application of these strategies is further investigated, which is crucial in managing the substantial computational and memory demands of training sophisticated models. This analysis not only highlights the effectiveness of these parallel training strategies in enhancing training efficiency but also their role in enabling the scalable training of large language models. Drawing on recent research findings, through a comprehensive literature review, this research underscores the critical role of parallelization techniques in addressing the computational challenges of training state-of-the-art Large Language Models, thereby contributing to the advancement of training more sophisticated and capable artificial intelligence systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15628v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ishan Patwardhan, Shubham Gandhi, Om Khare, Amit Joshi, Suraj Sawant</dc:creator>
    </item>
    <item>
      <title>Collaborative Access Control for IoT -- A Blockchain Approach</title>
      <link>https://arxiv.org/abs/2405.15749</link>
      <description>arXiv:2405.15749v1 Announce Type: new 
Abstract: The Internet of Things (IoT) necessitates robust access control mechanisms to secure a vast array of interconnected devices. Most of the existing IoT systems in practice use centralized solutions. We identify the problems in such solutions and adopt the blockchain based decentralized access control approach. Though there are works in the literature that use blockchain for access control, there are some gaps in these works. We develop a blockchain embedded access control (BEAC) framework to bridge the gaps. First, blockchain based solutions for access control require an enabling P2P network while existing P2P overlays do not support some required features. We develop a novel P2P infrastructure to seamlessly support our BEAC framework. Second, most of the works consider blockchain based access control for a single access control model, and we develop a generic blockchain mechanism and show that it can support the embedding of various access control models. Finally, existing works adopt existing blockchain mechanisms which may incur a high communication overhead. We develop a shortcut approach to improve the number of message rounds in the access protocol. Our experiments demonstrate the efficacy of our system, showing that the shortcut mechanism can reduces access time by approximately 43%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15749v1</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongtao Huang, I-Ling Yen, Farokh Bastani</dc:creator>
    </item>
    <item>
      <title>Implementation of New Security Features in CMSWEB Kubernetes Cluster at CERN</title>
      <link>https://arxiv.org/abs/2405.15342</link>
      <description>arXiv:2405.15342v1 Announce Type: cross 
Abstract: The CMSWEB cluster is pivotal to the activities of the Compact Muon Solenoid (CMS) experiment, as it hosts critical services required for the operational needs of the CMS experiment. The security of these services and the corresponding data is crucial to CMS. Any malicious attack can compromise the availability of our services. Therefore, it is important to construct a robust security infrastructure. In this work, we discuss new security features introduced to the CMSWEB Kubernetes ("k8s") cluster. The new features include the implementation of network policies, deployment of Open Policy Agent (OPA), enforcement of OPA policies, and the integration of Vault. The network policies act as an inside-the-cluster firewall to limit the network communication between the pods to the minimum necessary, and its dynamic nature allows us to work with microservices. The OPA validates the objects against some custom-defined policies during create, update, and delete operations to further enhance security. Without recompiling or changing the configuration of the Kubernetes API server, it can apply customized policies on Kubernetes objects and their audit functionality enabling us to detect pre-existing conflicts and issues. Although Kubernetes incorporates the concepts of secrets, they are only base64 encoded and are not dynamically configured. This is where Vault comes into play: Vault dynamically secures, stores, and tightly controls access to sensitive data. This way, the secret information is encrypted, secured, and centralized, making it more scalable and easier to manage. Thus, the implementation of these three security features corroborate the enhanced security and reliability of the CMSWEB Kubernetes infrastructure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15342v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1051/epjconf/202429507026</arxiv:DOI>
      <dc:creator>Aamir Ali, Muhammad Imran, Valentin Kuznetsov, Spyridon Trigazis, Aroosha Pervaiz, Andreas Pfeiffer, Marco Mascheroni</dc:creator>
    </item>
    <item>
      <title>Pipeline Parallelism with Controllable Memory</title>
      <link>https://arxiv.org/abs/2405.15362</link>
      <description>arXiv:2405.15362v1 Announce Type: cross 
Abstract: Pipeline parallelism has been widely explored, but most existing schedules lack a systematic methodology. In this paper, we propose a framework to decompose pipeline schedules as repeating a building block and we show that the lifespan of the building block decides the peak activation memory of the pipeline schedule. Guided by the observations, we find that almost all existing pipeline schedules, to the best of our knowledge, are memory inefficient. To address this, we introduce a family of memory efficient building blocks with controllable activation memory, which can reduce the peak activation memory to 1/2 of 1F1B without sacrificing efficiency, and even to 1/3 with comparable throughput. We can also achieve almost zero pipeline bubbles while maintaining the same activation memory as 1F1B. Our evaluations demonstrate that in pure pipeline parallelism settings, our methods outperform 1F1B by from 7% to 55% in terms of throughput. When employing a grid search over hybrid parallelism hyperparameters in practical scenarios, our proposed methods demonstrate a 16% throughput improvement over the 1F1B baseline for large language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15362v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Penghui Qi, Xinyi Wan, Nyamdavaa Amar, Min Lin</dc:creator>
    </item>
    <item>
      <title>A Planet Scale Spatial-Temporal Knowledge Graph Based On OpenStreetMap And H3 Grid</title>
      <link>https://arxiv.org/abs/2405.15375</link>
      <description>arXiv:2405.15375v1 Announce Type: cross 
Abstract: Geospatial data plays a central role in modeling our world, for which OpenStreetMap (OSM) provides a rich source of such data. While often spatial data is represented in a tabular format, a graph based representation provides the possibility to interconnect entities which would have been separated in a tabular representation. We propose in our paper a framework which supports a planet scale transformation of OpenStreetMap data into a Spatial Temporal Knowledge Graph. In addition to OpenStreetMap data, we align the different OpenStreetMap geometries on individual h3 grid cells. We compare our constructed spatial knowledge graph to other spatial knowledge graphs and outline our contribution in this paper. As a basis for our computation, we use Apache Sedona as a computational framework for our Spatial Temporal Knowledge Graph construction</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15375v1</guid>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin B\"ockling, Heiko Paulheim, Sarah Detzler</dc:creator>
    </item>
    <item>
      <title>PriCE: Privacy-Preserving and Cost-Effective Scheduling for Parallelizing the Large Medical Image Processing Workflow over Hybrid Clouds</title>
      <link>https://arxiv.org/abs/2405.15398</link>
      <description>arXiv:2405.15398v1 Announce Type: cross 
Abstract: Running deep neural networks for large medical images is a resource-hungry and time-consuming task with centralized computing. Outsourcing such medical image processing tasks to hybrid clouds has benefits, such as a significant reduction of execution time and monetary cost. However, due to privacy concerns, it is still challenging to process sensitive medical images over clouds, which would hinder their deployment in many real-world applications. To overcome this, we first formulate the overall optimization objectives of the privacy-preserving distributed system model, i.e., minimizing the amount of information about the private data learned by the adversaries throughout the process, reducing the maximum execution time and cost under the user budget constraint. We propose a novel privacy-preserving and cost-effective method called PriCE to solve this multi-objective optimization problem. We performed extensive simulation experiments for artifact detection tasks on medical images using an ensemble of five deep convolutional neural network inferences as the workflow task. Experimental results show that PriCE successfully splits a wide range of input gigapixel medical images with graph-coloring-based strategies, yielding desired output utility and lowering the privacy risk, makespan, and monetary cost under user's budget.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15398v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuandou Wang, Neel Kanwal, Kjersti Engan, Chunming Rong, Paola Grosso, Zhiming Zhao</dc:creator>
    </item>
    <item>
      <title>Towards Client Driven Federated Learning</title>
      <link>https://arxiv.org/abs/2405.15407</link>
      <description>arXiv:2405.15407v1 Announce Type: cross 
Abstract: Conventional federated learning (FL) frameworks follow a server-driven model where the server determines session initiation and client participation, which faces challenges in accommodating clients' asynchronous needs for model updates. We introduce Client-Driven Federated Learning (CDFL), a novel FL framework that puts clients at the driving role. In CDFL, each client independently and asynchronously updates its model by uploading the locally trained model to the server and receiving a customized model tailored to its local task. The server maintains a repository of cluster models, iteratively refining them using received client models. Our framework accommodates complex dynamics in clients' data distributions, characterized by time-varying mixtures of cluster distributions, enabling rapid adaptation to new tasks with superior performance. In contrast to traditional clustered FL protocols that send multiple cluster models to a client to perform distribution estimation, we propose a paradigm that offloads the estimation task to the server and only sends a single model to a client, and novel strategies to improve estimation accuracy. We provide a theoretical analysis of CDFL's convergence. Extensive experiments across various datasets and system settings highlight CDFL's substantial advantages in model performance and computation efficiency over baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15407v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Songze Li, Chenqing Zhu</dc:creator>
    </item>
    <item>
      <title>FedCal: Achieving Local and Global Calibration in Federated Learning via Aggregated Parameterized Scaler</title>
      <link>https://arxiv.org/abs/2405.15458</link>
      <description>arXiv:2405.15458v1 Announce Type: cross 
Abstract: Federated learning (FL) enables collaborative machine learning across distributed data owners, but data heterogeneity poses a challenge for model calibration. While prior work focused on improving accuracy for non-iid data, calibration remains under-explored. This study reveals existing FL aggregation approaches lead to sub-optimal calibration, and theoretical analysis shows despite constraining variance in clients' label distributions, global calibration error is still asymptotically lower bounded. To address this, we propose a novel Federated Calibration (FedCal) approach, emphasizing both local and global calibration. It leverages client-specific scalers for local calibration to effectively correct output misalignment without sacrificing prediction accuracy. These scalers are then aggregated via weight averaging to generate a global scaler, minimizing the global calibration error. Extensive experiments demonstrate FedCal significantly outperforms the best-performing baseline, reducing global calibration error by 47.66% on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15458v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongyi Peng, Han Yu, Xiaoli Tang, Xiaoxiao Li</dc:creator>
    </item>
    <item>
      <title>Unlearning during Learning: An Efficient Federated Machine Unlearning Method</title>
      <link>https://arxiv.org/abs/2405.15474</link>
      <description>arXiv:2405.15474v1 Announce Type: cross 
Abstract: In recent years, Federated Learning (FL) has garnered significant attention as a distributed machine learning paradigm. To facilitate the implementation of the right to be forgotten, the concept of federated machine unlearning (FMU) has also emerged. However, current FMU approaches often involve additional time-consuming steps and may not offer comprehensive unlearning capabilities, which renders them less practical in real FL scenarios. In this paper, we introduce FedAU, an innovative and efficient FMU framework aimed at overcoming these limitations. Specifically, FedAU incorporates a lightweight auxiliary unlearning module into the learning process and employs a straightforward linear operation to facilitate unlearning. This approach eliminates the requirement for extra time-consuming steps, rendering it well-suited for FL. Furthermore, FedAU exhibits remarkable versatility. It not only enables multiple clients to carry out unlearning tasks concurrently but also supports unlearning at various levels of granularity, including individual data samples, specific classes, and even at the client level. We conducted extensive experiments on MNIST, CIFAR10, and CIFAR100 datasets to evaluate the performance of FedAU. The results demonstrate that FedAU effectively achieves the desired unlearning effect while maintaining model accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15474v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hanlin Gu, Gongxi Zhu, Jie Zhang, Xinyuan Zhao, Yuxing Han, Lixin Fan, Qiang Yang</dc:creator>
    </item>
    <item>
      <title>SATSense: Multi-Satellite Collaborative Framework for Spectrum Sensing</title>
      <link>https://arxiv.org/abs/2405.15542</link>
      <description>arXiv:2405.15542v1 Announce Type: cross 
Abstract: Low Earth Orbit satellite Internet has recently been deployed, providing worldwide service with non-terrestrial networks. With the large-scale deployment of both non-terrestrial and terrestrial networks, limited spectrum resources will not be allocated enough. Consequently, dynamic spectrum sharing is crucial for their coexistence in the same spectrum, where accurate spectrum sensing is essential. However, spectrum sensing in space is more challenging than in terrestrial networks due to variable channel conditions, making single-satellite sensing unstable. Therefore, we first attempt to design a collaborative sensing scheme utilizing diverse data from multiple satellites. However, it is non-trivial to achieve this collaboration due to heterogeneous channel quality, considerable raw sampling data, and packet loss. To address the above challenges, we first establish connections between the satellites by modeling their sensing data as a graph and devising a graph neural network-based algorithm to achieve effective spectrum sensing. Meanwhile, we establish a joint sub-Nyquist sampling and autoencoder data compression framework to reduce the amount of transmitted sensing data. Finally, we propose a contrastive learning-based mechanism compensates for missing packets. Extensive experiments demonstrate that our proposed strategy can achieve efficient spectrum sensing performance and outperform the conventional deep learning algorithm in spectrum sensing accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15542v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoxuan Yuan, Zhe Chen, Zheng Lin, Jinbo Peng, Zihan Fang, Yuhang Zhong, Zihang Song, Yue Gao</dc:creator>
    </item>
    <item>
      <title>Distributed Locking as a Data Type</title>
      <link>https://arxiv.org/abs/2405.15578</link>
      <description>arXiv:2405.15578v1 Announce Type: cross 
Abstract: Mixed-consistency programming models assist programmers in designing applications that provide high availability while still ensuring application-specific safety invariants. However, existing models often make specific system assumptions, such as building on a particular database system or having baked-in coordination strategies. This makes it difficult to apply these strategies in diverse settings, ranging from client/server to ad-hoc peer-to-peer networks.
  This work proposes a new strategy for building programmable coordination mechanisms based on the algebraic replicated data types (ARDTs) approach. ARDTs allow for simple and composable implementations of various protocols, while making minimal assumptions about the network environment. As a case study, two different locking protocols are presented, both implemented as ARDTs. In addition, we elaborate on our ongoing efforts to integrate the approach into the LoRe mixed-consistency programming language.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15578v1</guid>
      <category>cs.PL</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julian Haas (Technische Universit\"at Darmstadt), Ragnar Mogk (Technische Universit\"at Darmstadt), Annette Bieniusa (University of Kaiserslautern-Landau), Mira Mezini (Technische Universit\"at Darmstadt)</dc:creator>
    </item>
    <item>
      <title>DAGER: Exact Gradient Inversion for Large Language Models</title>
      <link>https://arxiv.org/abs/2405.15586</link>
      <description>arXiv:2405.15586v1 Announce Type: cross 
Abstract: Federated learning works by aggregating locally computed gradients from multiple clients, thus enabling collaborative training without sharing private client data. However, prior work has shown that the data can actually be recovered by the server using so-called gradient inversion attacks. While these attacks perform well when applied on images, they are limited in the text domain and only permit approximate reconstruction of small batches and short input sequences. In this work, we propose DAGER, the first algorithm to recover whole batches of input text exactly. DAGER leverages the low-rank structure of self-attention layer gradients and the discrete nature of token embeddings to efficiently check if a given token sequence is part of the client data. We use this check to exactly recover full batches in the honest-but-curious setting without any prior on the data for both encoder- and decoder-based architectures using exhaustive heuristic search and a greedy approach, respectively. We provide an efficient GPU implementation of DAGER and show experimentally that it recovers full batches of size up to 128 on large language models (LLMs), beating prior attacks in speed (20x at same batch size), scalability (10x larger batches), and reconstruction quality (ROUGE-1/2 &gt; 0.99).</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15586v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivo Petrov, Dimitar I. Dimitrov, Maximilian Baader, Mark Niklas M\"uller, Martin Vechev</dc:creator>
    </item>
    <item>
      <title>Federated Behavioural Planes: Explaining the Evolution of Client Behaviour in Federated Learning</title>
      <link>https://arxiv.org/abs/2405.15632</link>
      <description>arXiv:2405.15632v1 Announce Type: cross 
Abstract: Federated Learning (FL), a privacy-aware approach in distributed deep learning environments, enables many clients to collaboratively train a model without sharing sensitive data, thereby reducing privacy risks. However, enabling human trust and control over FL systems requires understanding the evolving behaviour of clients, whether beneficial or detrimental for the training, which still represents a key challenge in the current literature. To address this challenge, we introduce Federated Behavioural Planes (FBPs), a novel method to analyse, visualise, and explain the dynamics of FL systems, showing how clients behave under two different lenses: predictive performance (error behavioural space) and decision-making processes (counterfactual behavioural space). Our experiments demonstrate that FBPs provide informative trajectories describing the evolving states of clients and their contributions to the global model, thereby enabling the identification of clusters of clients with similar behaviours. Leveraging the patterns identified by FBPs, we propose a robust aggregation technique named Federated Behavioural Shields to detect malicious or noisy client models, thereby enhancing security and surpassing the efficacy of existing state-of-the-art FL defense mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15632v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dario Fenoglio, Gabriele Dominici, Pietro Barbiero, Alberto Tonda, Martin Gjoreski, Marc Langheinrich</dc:creator>
    </item>
    <item>
      <title>Harnessing Increased Client Participation with Cohort-Parallel Federated Learning</title>
      <link>https://arxiv.org/abs/2405.15644</link>
      <description>arXiv:2405.15644v1 Announce Type: cross 
Abstract: Federated Learning (FL) is a machine learning approach where nodes collaboratively train a global model. As more nodes participate in a round of FL, the effectiveness of individual model updates by nodes also diminishes. In this study, we increase the effectiveness of client updates by dividing the network into smaller partitions, or cohorts. We introduce Cohort-Parallel Federated Learning (CPFL): a novel learning approach where each cohort independently trains a global model using FL, until convergence, and the produced models by each cohort are then unified using one-shot Knowledge Distillation (KD) and a cross-domain, unlabeled dataset. The insight behind CPFL is that smaller, isolated networks converge quicker than in a one-network setting where all nodes participate. Through exhaustive experiments involving realistic traces and non-IID data distributions on the CIFAR-10 and FEMNIST image classification tasks, we investigate the balance between the number of cohorts, model accuracy, training time, and compute and communication resources. Compared to traditional FL, CPFL with four cohorts, non-IID data distribution, and CIFAR-10 yields a 1.9$\times$ reduction in train time and a 1.3$\times$ reduction in resource usage, with a minimal drop in test accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15644v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akash Dhasade, Anne-Marie Kermarrec, Tuan-Anh Nguyen, Rafael Pires, Martijn de Vos</dc:creator>
    </item>
    <item>
      <title>An Online Probabilistic Distributed Tracing System</title>
      <link>https://arxiv.org/abs/2405.15645</link>
      <description>arXiv:2405.15645v1 Announce Type: cross 
Abstract: Distributed tracing has become a fundamental tool for diagnosing performance issues in the cloud by recording causally ordered, end-to-end workflows of request executions. However, tracing in production workloads can introduce significant overheads due to the extensive instrumentation needed for identifying performance variations. This paper addresses the trade-off between the cost of tracing and the utility of the "spans" within that trace through Astraea, an online probabilistic distributed tracing system. Astraea is based on our technique that combines online Bayesian learning and multi-armed bandit frameworks. This formulation enables Astraea to effectively steer tracing towards the useful instrumentation needed for accurate performance diagnosis. Astraea localizes performance variations using only 10-28% of available instrumentation, markedly reducing tracing overhead, storage, compute costs, and trace analysis time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15645v1</guid>
      <category>cs.PF</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. Toslali, S. Qasim, S. Parthasarathy, F. A. Oliveira, H. Huang, G. Stringhini, Z. Liu, A. K. Coskun</dc:creator>
    </item>
    <item>
      <title>CAFe: Cost and Age aware Federated Learning</title>
      <link>https://arxiv.org/abs/2405.15744</link>
      <description>arXiv:2405.15744v1 Announce Type: cross 
Abstract: In many federated learning (FL) models, a common strategy employed to ensure the progress in the training process, is to wait for at least $M$ clients out of the total $N$ clients to send back their local gradients based on a reporting deadline $T$, once the parameter server (PS) has broadcasted the global model. If enough clients do not report back within the deadline, the particular round is considered to be a failed round and the training round is restarted from scratch. If enough clients have responded back, the round is deemed successful and the local gradients of all the clients that responded back are used to update the global model. In either case, the clients that failed to report back an update within the deadline would have wasted their computational resources. Having a tighter deadline (small $T$) and waiting for a larger number of participating clients (large $M$) leads to a large number of failed rounds and therefore greater communication cost and computation resource wastage. However, having a larger $T$ leads to longer round durations whereas smaller $M$ may lead to noisy gradients. Therefore, there is a need to optimize the parameters $M$ and $T$ such that communication cost and the resource wastage is minimized while having an acceptable convergence rate. In this regard, we show that the average age of a client at the PS appears explicitly in the theoretical convergence bound, and therefore, can be used as a metric to quantify the convergence of the global model. We provide an analytical scheme to select the parameters $M$ and $T$ in this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15744v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sahan Liyanaarachchi, Kanchana Thilakarathna, Sennur Ulukus</dc:creator>
    </item>
    <item>
      <title>Network Abstractions for Characterizing Communication Requirements in Asynchronous Distributed Systems</title>
      <link>https://arxiv.org/abs/2310.12615</link>
      <description>arXiv:2310.12615v4 Announce Type: replace 
Abstract: Whereas distributed computing research has been very successful in exploring the solvability/impossibility border of distributed computing problems like consensus in representative classes of computing models with respect to model parameters like failure bounds, this is not the case for characterizing necessary and sufficient communication requirements. In this paper, we introduce network abstractions as a novel approach for modeling communication requirements in asynchronous distributed systems. A network abstraction of a run is a sequence of directed graphs on the set of processes, where the $i$-th graph specifies some ``potential'' message chains that can be guaranteed to arise in the $i$-th portion of the run. Formally, they are defined via associating message sending times with the end-to-end delays that would arise if the message was indeed sent by the sender's protocol. Network abstractions also allow to reason about future causal cones that might arise in a run, hence also facilitate reasoning about liveness properties, and are inherently compatible with temporal epistemic reasoning frameworks. We demonstrate the utility of our approach by providing necessary and sufficient network abstractions for solving the canonical firing rebels with relay (FRR) problem, and variants thereof, in asynchronous message-passing systems with up to $f$ byzantine processes connected via point-to-point links. FRR is not only a basic primitive in clock synchronization and consensus algorithms, but also integrates several distributed computing problems, namely triggering events, agreement and even stabilizing agreement, in a single problem instance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12615v4</guid>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <category>cs.LO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugo Rincon Galeana, Ulrich Schmid</dc:creator>
    </item>
    <item>
      <title>ZeroPP: Unleashing Exceptional Parallelism Efficiency through Tensor-Parallelism-Free Methodology</title>
      <link>https://arxiv.org/abs/2402.03791</link>
      <description>arXiv:2402.03791v3 Announce Type: replace 
Abstract: Large-scale models rely heavily on 3D parallelism for distributed training, which utilizes tensor parallelism (TP) as the intra-operator parallelism to partition model states across GPUs. However, TP introduces significant communication overheads and complexity in modifying single-GPU code. In this paper, we propose a TP-free distributed framework ZeroPP, which leverages the hybrid of scalable inter-operator pipeline parallelism and intra-operator fully sharded data parallelism to train models at scale, reducing memory consumption and enabling high training efficiency. Through extensive experimentation, we demonstrate that ZeroPP achieves significant performance gains of up to 33% compared to conventional 3D parallelism while maintaining comparable GPU memory consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03791v3</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ding Tang, Lijuan Jiang, Jiecheng Zhou, Minxi Jin, Hengjie Li, Xingcheng Zhang, Zhilin Pei, Jidong Zhai</dc:creator>
    </item>
    <item>
      <title>Distributing Context-Aware Shared Memory Data Structures: A Case Study on Singly-Linked Lists</title>
      <link>https://arxiv.org/abs/2404.10151</link>
      <description>arXiv:2404.10151v2 Announce Type: replace 
Abstract: In this paper, we study the partitioning of a context-aware shared memory data structure so that it can be implemented as a distributed data structure running on multiple machines. By context-aware data structures, we mean that the result of an operation not only depends upon the value of the shared data but also upon the previous operations performed by the same client. While there is substantial work on designing distributed data structures, designing distributed context-aware data structures has not received much attention.
  We focus on singly-linked lists as a case study of the context-aware data structure. We start with a shared memory context-aware lock-free singly-linked list and show how it can be transformed into a distributed lock-free context-aware singly-linked list. The main challenge in such a transformation is to preserve properties of client-visible operations of the underlying data structure. We present two protocols that preserve these properties of client-visible operations of the linked list. In the first protocol, the distribution is done in the background as a low priority task, while in the second protocol the client-visible operations help the task of distribution without affecting client latency. In both protocols, the client-visible operations remain lock-free. Also, our transformation approach does not utilize any hardware primitives (except a compare-and-swap operation on a single word). We note that our transformation is generic and can be used for other lock-free context-aware data structures that can be constructed from singly-linked lists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10151v2</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raaghav Ravishankar, Sandeep Kulkarni, Sathya Peri, Gokarna Sharma</dc:creator>
    </item>
    <item>
      <title>Nearly-Optimal Consensus Tolerating Adaptive Omissions: Why is a Lot of Randomness Needed?</title>
      <link>https://arxiv.org/abs/2405.04762</link>
      <description>arXiv:2405.04762v2 Announce Type: replace 
Abstract: We study the problem of reaching agreement in a synchronous distributed system by $n$ autonomous parties, when the communication links from/to faulty parties can omit messages. The faulty parties are selected and controlled by an adaptive, full-information, computationally unbounded adversary. We design a randomized algorithm that works in $O(\sqrt{n}\log^2 n)$ rounds and sends $O(n^2\log^3 n)$ communication bits, where the number of faulty parties is $\Theta(n)$. Our result is simultaneously tight for both these measures within polylogarithmic factors: due to the $\Omega(n^2)$ lower bound on communication by Abraham et al. (PODC'19) and $\Omega(\sqrt{n/\log n})$ lower bound on the number of rounds by Bar-Joseph and Ben-Or (PODC'98). We also quantify how much randomness is necessary and sufficient to reduce time complexity to a certain value, while keeping the communication complexity (nearly) optimal. We prove that no MC algorithm can work in less than $\Omega(\frac{n^2}{\max\{R,n\}\log n})$ rounds if it uses less than $O(R)$ calls to a random source, assuming a constant fraction of faulty parties. This can be contrasted with a long line of work on consensus against an {\em adversary limited to polynomial computation time}, thus unable to break cryptographic primitives, culminating in a work by Ghinea et al. (EUROCRYPT'22), where an optimal $O(r)$-round solution with probability $1-(cr)^{-r}$ is given. Our lower bound strictly separates these two regimes, by excluding such results if the adversary is computationally unbounded. On the upper bound side, we show that for $R\in\tilde{O}(n^{3/2})$ there exists an algorithm solving consensus in $\tilde{O}(\frac{n^2}{R})$ rounds with high probability, where tilde notation hides a polylogarithmic factor. The communication complexity of the algorithm does not depend on the amount of randomness $R$ and stays optimal within polylogarithmic factor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04762v2</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad T. Hajiaghayi, Dariusz R. Kowalski, Jan Olkowski</dc:creator>
    </item>
    <item>
      <title>Full private delegated quantum computing tailored from user to industry</title>
      <link>https://arxiv.org/abs/2405.11608</link>
      <description>arXiv:2405.11608v2 Announce Type: replace-cross 
Abstract: In this paper, we present a set of private and secure delegated quantum computing protocols and techniques tailored to user-level and industry-level use cases, depending on the computational resources available to the client, the specific privacy needs required, and the type of algorithm. Our protocols are presented at a high level as they are independent of the particular algorithm used for such encryption and decryption processes. Additionally, we propose a method to verify the correct execution of operations by the external server.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11608v2</guid>
      <category>quant-ph</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Mata Ali, Adriano Mauricio Lusso, Edgar Mencia</dc:creator>
    </item>
    <item>
      <title>CG-FedLLM: How to Compress Gradients in Federated Fune-tuning for Large Language Models</title>
      <link>https://arxiv.org/abs/2405.13746</link>
      <description>arXiv:2405.13746v2 Announce Type: replace-cross 
Abstract: The success of current Large-Language Models (LLMs) hinges on extensive training data that is collected and stored centrally, called Centralized Learning (CL). However, such a collection manner poses a privacy threat, and one potential solution is Federated Learning (FL), which transfers gradients, not raw data, among clients. Unlike traditional networks, FL for LLMs incurs significant communication costs due to their tremendous parameters. This study introduces an innovative approach to compress gradients to improve communication efficiency during LLM FL, formulating the new FL pipeline named CG-FedLLM. This approach integrates an encoder on the client side to acquire the compressed gradient features and a decoder on the server side to reconstruct the gradients. We also developed a novel training strategy that comprises Temporal-ensemble Gradient-Aware Pre-training (TGAP) to identify characteristic gradients of the target model and Federated AutoEncoder-Involved Fine-tuning (FAF) to compress gradients adaptively. Extensive experiments confirm that our approach reduces communication costs and improves performance (e.g., average 3 points increment compared with traditional CL- and FL-based fine-tuning with LlaMA on a well-recognized benchmark, C-Eval). This improvement is because our encoder-decoder, trained via TGAP and FAF, can filter gradients while selectively preserving critical features. Furthermore, we present a series of experimental analyses focusing on the signal-to-noise ratio, compression rate, and robustness within this privacy-centric framework, providing insight into developing more efficient and secure LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13746v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huiwen Wu, Xiaohan Li, Deyi Zhang, Xiaogang Xu, Jiafei Wu, Puning Zhao, Zhe Liu</dc:creator>
    </item>
  </channel>
</rss>
