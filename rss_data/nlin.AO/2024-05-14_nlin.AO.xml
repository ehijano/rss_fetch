<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>nlin.AO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/nlin.AO</link>
    <description>nlin.AO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/nlin.AO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 15 May 2024 04:02:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 15 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Bifurcation analysis of a two-neuron central pattern generator model for both oscillatory and convergent neuronal activities</title>
      <link>https://arxiv.org/abs/2405.08409</link>
      <description>arXiv:2405.08409v1 Announce Type: new 
Abstract: The neural oscillator model proposed by Matsuoka is a piecewise affine system, which exhibits distinctive periodic solutions. Although such typical oscillation patterns have been widely studied, little is understood about the dynamics of convergence to certain fixed points and bifurcations between the periodic orbits and fixed points in this model. We performed fixed point analysis on a two-neuron version of the Matsuoka oscillator model, the result of which explains the mechanism of oscillation and the discontinuity-induced bifurcations such as subcritical/supercritical Hopf-like, homoclinic-like, and grazing bifurcations. Furthermore, it provided theoretical predictions concerning a logarithmic oscillation-period scaling law and noise-induced oscillations, which are both observed around those bifurcations. These results are expected to underpin further investigations into both oscillatory and transient neuronal activities with respect to central pattern generators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08409v1</guid>
      <category>nlin.AO</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kotaro Muramatsu, Hiroshi Kori</dc:creator>
    </item>
    <item>
      <title>ICO learning as a measure of transient chaos in PT-symmetric Li\'enard systems</title>
      <link>https://arxiv.org/abs/2405.08414</link>
      <description>arXiv:2405.08414v1 Announce Type: new 
Abstract: In this article, we investigate the implications of the unsupervised learning rule known as Input-Correlations (ICO) learning in the nonlinear dynamics of two linearly coupled PT-symmetric Li\'enard oscillators. The fixed points of the oscillator have been evaluated analytically and the Jacobian linearization is employed to study their stability. We find that on increasing the amplitude of the external periodic drive, the system exhibits period-doubling cascade to chaos within a specific parametric regime wherein we observe emergent chaotic dynamics. We further notice that the system indicates an intermittency route to chaos in the chaotic regime. Finally, in the period-4 regime of our bifurcation analysis, we predict the emergence of transient chaos which eventually settles down to a period-2 oscillator response which has been further validated by both the maximal Finite-Time Lyapunov Exponent (FTLE) using the well-known Gram-Schmidt orthogonalization technique and the Hilbert Transform of the time-series. In the transiently chaotic regime, we deploy the ICO learning to analyze the time-series from which we identify that when the chaotic evolution transforms into periodic dynamics, the synaptic weight associated with the time-series of the loss oscillator exhibits stationary temporal evolution. This signifies that in the periodic regime, there is no overlap between the filtered signals obtained from the time-series of the coupled PT-symmetric oscillators. In addition, the temporal evolution of the weight associated with the stimulus mimics the behaviour of the Hilbert transform of the time-series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08414v1</guid>
      <category>nlin.AO</category>
      <category>nlin.CD</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. P. Deka, A. Govindarajan, A. K. Sarma</dc:creator>
    </item>
    <item>
      <title>Mechanical memories in solids, from disorder to design</title>
      <link>https://arxiv.org/abs/2405.08158</link>
      <description>arXiv:2405.08158v1 Announce Type: cross 
Abstract: Solids are rigid, which means that when left undisturbed, their structures are nearly static. It follows that these structures depend on history -- but it is surprising that they hold readable memories of past events. Here we review the research that has recently flourished around mechanical memory formation, beginning with amorphous solids' various memories of deformation and mesoscopic models based on particle rearrangements. We describe how these concepts apply to a much wider range of solids and glassy matter -- and how they are a bridge to memory and physical computing in mechanical metamaterials. An understanding of memory in all these solids can potentially be the basis for designing or training functionality into materials. Just as important is memory's value for understanding matter whenever it is complex, frustrated, and out of equilibrium.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08158v1</guid>
      <category>cond-mat.soft</category>
      <category>nlin.AO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph D. Paulsen, Nathan C. Keim</dc:creator>
    </item>
    <item>
      <title>Collective behavior from surprise minimization</title>
      <link>https://arxiv.org/abs/2307.14804</link>
      <description>arXiv:2307.14804v4 Announce Type: replace 
Abstract: Collective motion is ubiquitous in nature; groups of animals, such as fish, birds, and ungulates appear to move as a whole, exhibiting a rich behavioral repertoire that ranges from directed movement to milling to disordered swarming. Typically, such macroscopic patterns arise from decentralized, local interactions among constituent components (e.g., individual fish in a school). Preeminent models of this process describe individuals as self-propelled particles, subject to self-generated motion and 'social forces' such as short-range repulsion and long-range attraction or alignment. However, organisms are not particles; they are probabilistic decision-makers. Here, we introduce an approach to modelling collective behavior based on active inference. This cognitive framework casts behavior as the consequence of a single imperative: to minimize surprise. We demonstrate that many empirically-observed collective phenomena, including cohesion, milling and directed motion, emerge naturally when considering behavior as driven by active Bayesian inference -- without explicitly building behavioral rules or goals into individual agents. Furthermore, we show that active inference can recover and generalize the classical notion of social forces as agents attempt to suppress prediction errors that conflict with their expectations. By exploring the parameter space of the belief-based model, we reveal non-trivial relationships between the individual beliefs and group properties like polarization and the tendency to visit different collective states. We also explore how individual beliefs about uncertainty determine collective decision-making accuracy. Finally, we show how agents can update their generative model over time, resulting in groups that are collectively more sensitive to external fluctuations and encode information more robustly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.14804v4</guid>
      <category>nlin.AO</category>
      <category>cs.MA</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1073/pnas.2320239121</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the National Academy of Sciences, 121(17), e2320239121 (2024)</arxiv:journal_reference>
      <dc:creator>Conor Heins, Beren Millidge, Lancelot da Costa, Richard Mann, Karl Friston, Iain Couzin</dc:creator>
    </item>
    <item>
      <title>Minimal Model for Reservoir Computing</title>
      <link>https://arxiv.org/abs/2312.01089</link>
      <description>arXiv:2312.01089v3 Announce Type: replace 
Abstract: A minimal model for reservoir computing is studied. We demonstrate that a reservoir computer exists that emulates given coupled maps by constructing a modularized network. We describe a possible mechanism for collapses of the emulation in the reservoir computing by introducing a measure of finite scale deviation. Such transitory behaviour is caused by either (i) an escape from a finite-time stagnation near an unstable chaotic set, or (ii) a critical transition driven by the effective parameter drift. Our approach reveals the essential mechanism for reservoir computing and provides insights into the design of reservoir computer for practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01089v3</guid>
      <category>nlin.AO</category>
      <category>nlin.CD</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuzuru Sato, Miki Kobayashi</dc:creator>
    </item>
  </channel>
</rss>
