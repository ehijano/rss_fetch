<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 May 2024 04:01:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 08 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Trajectory analysis through entropy characterization over coded representation</title>
      <link>https://arxiv.org/abs/2405.03693</link>
      <description>arXiv:2405.03693v1 Announce Type: new 
Abstract: Any continuous curve in a higher dimensional space can be considered a trajectory that can be parameterized by a single variable, usually taken as time. It is well known that a continuous curve can have a fractional dimensionality, which can be estimated using already standard algorithms. However, characterizing a trajectory from an entropic perspective is far less developed. The search for such characterization leads us to use chain coding to discretize the description of a curve. Calculating the entropy density and entropy-related magnitudes from the resulting finite alphabet code becomes straightforward. In such a way, the entropy of a trajectory can be defined and used as an effective tool to assert creativity and pattern formation from a Shannon perspective. Applying the procedure to actual experimental physiological data and modelled trajectories of astronomical dynamics proved the robustness of the entropic characterization in a wealth of trajectories of different origins and the insight that can be gained from its use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03693v1</guid>
      <category>physics.data-an</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roxana Pe\~na-Mendieta, Ania Mesa-Rodr\'iguez, Ernesto Estevez-Rams, Daniel Estevez-Moya, Danays Kunka</dc:creator>
    </item>
    <item>
      <title>The big bang of an epidemic</title>
      <link>https://arxiv.org/abs/2405.03703</link>
      <description>arXiv:2405.03703v1 Announce Type: cross 
Abstract: In this paper, we propose a mathematical framework that governs the evolution of epidemic dynamics, encompassing both intra-population dynamics and inter-population mobility within a metapopulation network. By linearizing this dynamical system, we can identify the spatial starting point(s), namely the source(s) (A) and the initiation time (B) of any epidemic, which we refer to as the "Big Bang" of the epidemic. Furthermore, we introduce a novel concept of effective distance to track disease spread within the network. Our analysis reveals that the contagion geometry can be represented as a line with a universal slope, independent of disease type (R0) or mobility network configuration. The mathematical derivations presented in this framework are corroborated by empirical data, including observations from the COVID-19 pandemic in Iran and the US, as well as the H1N1 outbreak worldwide. Within this framework, in order to detect the Big Bang of an epidemic we require two types of data: A) A snapshot of the active infected cases in each subpopulation during the linear phase. B) A coarse-grained representation of inter-population mobility. Also even with access to only type A data, we can still demonstrate the universal contagion geometric pattern. Additionally, we can estimate errors and assess the precision of the estimations. This comprehensive approach enhances our understanding of when and where epidemics began and how they spread, and equips us with valuable insights for developing effective public health policies and mitigating the impact of infectious diseases on populations worldwide.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03703v1</guid>
      <category>physics.soc-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yazdan Babazadeh, Amin Safaeesirat, Fakhteh Ghanbarnejad</dc:creator>
    </item>
    <item>
      <title>Exact calculation of the probabilities of rare events in cluster-cluster aggregation</title>
      <link>https://arxiv.org/abs/2405.04201</link>
      <description>arXiv:2405.04201v1 Announce Type: cross 
Abstract: We develop an action formalism to calculate probabilities of rare events in cluster-cluster aggregation for arbitrary collision kernels and establish a pathwise large deviation principle with total mass being the rate. As an application, the rate function for the number of surviving particles as well as the optimal evolution trajectory are calculated exactly for the constant, sum and product kernels. For the product kernel, we argue that the second derivative of the rate function has a discontinuity. The theoretical results agree with simulations tailored to the calculation of rare events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04201v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>physics.bio-ph</category>
      <category>physics.class-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R. Rajesh, V. Subashri, Oleg Zaboronski</dc:creator>
    </item>
    <item>
      <title>The Random Hivemind: An Ensemble Deep Learner Application to Solar Energetic Particle Prediction Problem</title>
      <link>https://arxiv.org/abs/2303.08092</link>
      <description>arXiv:2303.08092v2 Announce Type: replace-cross 
Abstract: The application of machine learning and deep learning, including the wide use of non-ensemble, conventional neural networks (CoNN), for predicting various phenomena has become very popular in recent years thanks to the efficiencies and the abilities of these techniques to find relationships in data without human intervention. However, certain CoNN setups may not work on some datasets, especially if the parameters passed to it, including model parameters and hyperparameters, are arguably arbitrary in nature and need to continuously be updated with the need to retrain the model. This concern can be partially alleviated by employing committees of neural networks that are identical in terms of input features and architectures, initialized randomly, and "vote" on the decisions made by the committees as a whole. Yet, it is possible for the committee members to "agree" on identical sets of weights and biases for all nodes and edges. Members of these committees also cannot be expanded to accommodate new features and entire committees must therefore be retrained in order to do so. We propose the Random Hivemind (RH) approach, which helps to alleviate this concern by having multiple neural network estimators make decisions based on random permutations of features and prescribing a method to determine the weight of the decision of each individual estimator. The effectiveness of RH is demonstrated through experimentation in the predictions of hazardous Solar Energetic Particle (SEP) events by comparing it to that of using both CoNNs and the aforementioned setup of committees. Our results demonstrate that RH, while having a comparable or better performance than the CoNN and a Committee-based approach, demonstrates a lesser score spread for the individual experiments, and shows promising results with respect to capturing almost every single flare instance leading to SEPs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.08092v2</guid>
      <category>astro-ph.SR</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.asr.2024.04.044</arxiv:DOI>
      <dc:creator>Patrick M. O'Keefe, Viacheslav Sadykov, Alexander Kosovichev, Irina N. Kitiashvili, Vincent Oria, Gelu M. Nita, Fraila Francis, Chun-Jie Chong, Paul Kosovich, Aatiya Ali, Russell D. Marroquin</dc:creator>
    </item>
    <item>
      <title>Agnostic detection of large-scale weather patterns in the northern hemisphere: from blockings to teleconnections</title>
      <link>https://arxiv.org/abs/2309.06833</link>
      <description>arXiv:2309.06833v2 Announce Type: replace-cross 
Abstract: Detecting recurrent weather patterns and understanding the transitions between such regimes are key to advancing our knowledge on the low-frequency variability of the atmosphere and have important implications in terms of weather and climate-related risks. We adapt an analysis pipeline inspired by Markov State Modelling and detect in an unsupervised manner the dominant winter mid-latitude Northern Hemisphere weather patterns in the Atlantic and Pacific sectors. The daily 500 hPa geopotential height fields are first classified in $\sim 200$ microstates. The weather dynamics is then represented in the basis of these microstates and the slowest decaying modes are identified from the spectral properties of the transition probability matrix. These modes are defined on the basis of the nonlinear dynamical processes of the system and not as tentative metastable states as often done in Markov state analysis. In the Atlantic and Pacific sectors slow relaxation processes are mainly related to transitions between blocked regimes and zonal flow. We also find strong evidence of a dynamical regime associated with the simultaneous Atlantic-Pacific blocking. When the analysis is performed in a broader geographical region of the Atlantic sector, we discover that the slowest relaxation modes of the system are associated with transitions between dynamical regimes that resemble teleconnection patterns like the North Atlantic Oscillation and weather regimes like the Scandinavian and Greenland blocking, yet have a much stronger dynamical foundation than classical methods based e.g. on EOF analysis. Our method clarifies that, as a result of the lack of a time-scale separation in the atmospheric variability of the mid-latitudes, there is no clear-cut way to represent the atmospheric dynamics in terms of few, well-defined modes of variability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.06833v2</guid>
      <category>physics.ao-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.data-an</category>
      <category>physics.geo-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Springer, Vera Melinda Galfi, Alessandro Laio, Valerio Lucarini</dc:creator>
    </item>
    <item>
      <title>Catalog variance of testing general relativity with gravitational-wave data</title>
      <link>https://arxiv.org/abs/2310.03811</link>
      <description>arXiv:2310.03811v2 Announce Type: replace-cross 
Abstract: Combining multiple gravitational-wave observations allows for stringent tests of general relativity, targeting effects that would otherwise be undetectable using single-event analyses. We highlight how the finite size of the observed catalog induces a significant source of variance. If not appropriately accounted for, general relativity can be excluded with arbitrarily large credibility even if it is the underlying theory of gravity. This effect is generic and holds for arbitrarily large catalogs. Moreover, we show that it cannot be suppressed by selecting "golden" observations with large signal-to-noise ratios. We present a mitigation strategy based on bootstrapping (i.e. resampling with repetition) that allows assigning uncertainties to one's credibility on the targeted test. We demonstrate our findings using both toy models and real gravitational-wave data. In particular, we quantify the impact of the catalog variance on the ringdown properties of black holes using the latest LIGO/Virgo catalog.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.03811v2</guid>
      <category>gr-qc</category>
      <category>astro-ph.HE</category>
      <category>hep-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevD.109.L081302</arxiv:DOI>
      <arxiv:journal_reference>Physical Review D 109.8, L081302 (2024)</arxiv:journal_reference>
      <dc:creator>Costantino Pacilio, Davide Gerosa, Swetha Bhagwat</dc:creator>
    </item>
    <item>
      <title>Non-resonant Anomaly Detection with Background Extrapolation</title>
      <link>https://arxiv.org/abs/2311.12924</link>
      <description>arXiv:2311.12924v3 Announce Type: replace-cross 
Abstract: Complete anomaly detection strategies that are both signal sensitive and compatible with background estimation have largely focused on resonant signals. Non-resonant new physics scenarios are relatively under-explored and may arise from off-shell effects or final states with significant missing energy. In this paper, we extend a class of weakly supervised anomaly detection strategies developed for resonant physics to the non-resonant case. Machine learning models are trained to reweight, generate, or morph the background, extrapolated from a control region. A classifier is then trained in a signal region to distinguish the estimated background from the data. The new methods are demonstrated using a semi-visible jet signature as a benchmark signal model, and are shown to automatically identify the anomalous events without specifying the signal ahead of time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.12924v3</guid>
      <category>hep-ph</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/JHEP04(2024)059</arxiv:DOI>
      <arxiv:journal_reference>JHEP 04 (2024) 059</arxiv:journal_reference>
      <dc:creator>Kehang Bai, Radha Mastandrea, Benjamin Nachman</dc:creator>
    </item>
    <item>
      <title>Scalable network reconstruction in subquadratic time</title>
      <link>https://arxiv.org/abs/2401.01404</link>
      <description>arXiv:2401.01404v5 Announce Type: replace-cross 
Abstract: Network reconstruction consists in determining the unobserved pairwise couplings between $N$ nodes given only observational data on the resulting behavior that is conditioned on those couplings -- typically a time-series or independent samples from a graphical model. A major obstacle to the scalability of algorithms proposed for this problem is a seemingly unavoidable quadratic complexity of $\Omega(N^2)$, corresponding to the requirement of each possible pairwise coupling being contemplated at least once, despite the fact that most networks of interest are sparse, with a number of non-zero couplings that is only $O(N)$. Here we present a general algorithm applicable to a broad range of reconstruction problems that significantly outperforms this quadratic baseline. Our algorithm relies on a stochastic second neighbor search (Dong et al., 2011) that produces the best edge candidates with high probability, thus bypassing an exhaustive quadratic search. If we rely on the conjecture that the second-neighbor search finishes in log-linear time (Baron &amp; Darling, 2020; 2022), we demonstrate theoretically that our algorithm finishes in subquadratic time, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\log N)$, but with a more typical log-linear complexity of $O(N\log^2N)$. In practice, we show that our algorithm achieves a performance that is many orders of magnitude faster than the quadratic baseline -- in a manner consistent with our theoretical analysis -- allows for easy parallelization, and thus enables the reconstruction of networks with hundreds of thousands and even millions of nodes and edges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01404v5</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tiago P. Peixoto</dc:creator>
    </item>
    <item>
      <title>Parameter uncertainties for imperfect surrogate models in the low-noise regime</title>
      <link>https://arxiv.org/abs/2402.01810</link>
      <description>arXiv:2402.01810v3 Announce Type: replace-cross 
Abstract: Bayesian regression determines model parameters by minimizing the expected loss, an upper bound to the true generalization error. However, the loss ignores misspecification, where models are imperfect. Parameter uncertainties from Bayesian regression are thus significantly underestimated and vanish in the large data limit. This is particularly problematic when building models of low-noise, or near-deterministic, calculations, as the main source of uncertainty is neglected. We analyze the generalization error of misspecified, near-deterministic surrogate models, a regime of broad relevance in science and engineering. We show posterior distributions must cover every training point to avoid a divergent generalization error and design an ansatz that respects this constraint, which for linear models incurs minimal overhead. This is demonstrated on model problems before application to thousand dimensional datasets in atomistic machine learning. Our efficient misspecification-aware scheme gives accurate prediction and bounding of test errors where existing schemes fail, allowing this important source of uncertainty to be incorporated in computational workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01810v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas D Swinburne, Danny Perez</dc:creator>
    </item>
    <item>
      <title>Photon Classification with Gradient Boosted Trees at CLAS12</title>
      <link>https://arxiv.org/abs/2402.13105</link>
      <description>arXiv:2402.13105v2 Announce Type: replace-cross 
Abstract: Dihadron semi-inclusive deep inelastic scattering (SIDIS) of 10.6 GeV longitudinally polarized electrons off the proton has been measured using the CLAS12 detector at Jefferson Lab. Two separate channels, $\pi^+\pi^0$ and $\pi^-\pi^0$, were analyzed, requiring the reconstruction of diphoton pairs. In this analysis, we addressed the problem of false neutral particles being reconstructed by CLAS12's event builder, polluting the otherwise physical combinatorial background underneath the $\pi^0$ peak. A photon classifier using a Gradient Boosted Trees (GBTs) architecture was trained with Monte Carlo simulations to reduce the amount of background $\pi^0$'s. We show that the nearest-neighbor features learned by the model lead to a substantial increase in signal vs. background discrimination compared to previous CLAS12 $\pi^0$ analyses. The machine learning approach recovers several times more dihadron statistics for the dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13105v2</guid>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gregory Matousek, Anselm Vossen</dc:creator>
    </item>
    <item>
      <title>Environmental monitoring using orbital angular momentum mode decomposition enhanced machine learning</title>
      <link>https://arxiv.org/abs/2403.19179</link>
      <description>arXiv:2403.19179v3 Announce Type: replace-cross 
Abstract: Atmospheric interaction with light has been an area of fascination for many researchers over the last century. Environmental conditions, such as temperature and wind speed, heavily influence the complex and rapidly varying optical distortions propagating optical fields experience. The continuous random phase fluctuations commonly make deciphering the exact origins of specific optical aberrations challenging. The generation of eddies is a major contributor to atmospheric turbulence, similar in geometric structure to optical vortices that sit at the centre of OAM beams. Decomposing the received optical fields into OAM provides a unique spatial similarity that can be used to analyse turbulent channels. In this work, we present a novel mode decomposition assisted machine learning approach that reveals trainable features in the distortions of vortex beams that allow for effective environmental monitoring. This novel technique can be used reliably with Support Vector Machine regression models to measure temperature variations of 0.49C and wind speed variations of 0.029 m/s over a 36m experimental turbulent free-space channels with controllable and verifiable temperature and wind speed with short 3s measurement. The predictable nature of these findings could indicate the presence of an underlying physical relationship between environmental conditions that lead to specific eddy formation and the OAM spiral spectra.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19179v3</guid>
      <category>physics.optics</category>
      <category>physics.ao-ph</category>
      <category>physics.app-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaozhong Chen, Ultan Daly, Aleksandr Boldin, Lenny Hirsch, Mingjian Cheng, Martin P. J. Lavery</dc:creator>
    </item>
    <item>
      <title>Network reconstruction via the minimum description length principle</title>
      <link>https://arxiv.org/abs/2405.01015</link>
      <description>arXiv:2405.01015v2 Announce Type: replace-cross 
Abstract: A fundamental problem associated with the task of network reconstruction from dynamical or behavioral data consists in determining the most appropriate model complexity in a manner that prevents overfitting, and produces an inferred network with a statistically justifiable number of edges. The status quo in this context is based on $L_{1}$ regularization combined with cross-validation. However, besides its high computational cost, this commonplace approach unnecessarily ties the promotion of sparsity with weight "shrinkage". This combination forces a trade-off between the bias introduced by shrinkage and the network sparsity, which often results in substantial overfitting even after cross-validation. In this work, we propose an alternative nonparametric regularization scheme based on hierarchical Bayesian inference and weight quantization, which does not rely on weight shrinkage to promote sparsity. Our approach follows the minimum description length (MDL) principle, and uncovers the weight distribution that allows for the most compression of the data, thus avoiding overfitting without requiring cross-validation. The latter property renders our approach substantially faster to employ, as it requires a single fit to the complete data. As a result, we have a principled and efficient inference scheme that can be used with a large variety of generative models, without requiring the number of edges to be known in advance. We also demonstrate that our scheme yields systematically increased accuracy in the reconstruction of both artificial and empirical networks. We highlight the use of our method with the reconstruction of interaction networks between microbial communities from large-scale abundance samples involving in the order of $10^{4}$ to $10^{5}$ species, and demonstrate how the inferred model can be used to predict the outcome of interventions in the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01015v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>physics.data-an</category>
      <category>q-bio.PE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tiago P. Peixoto</dc:creator>
    </item>
  </channel>
</rss>
