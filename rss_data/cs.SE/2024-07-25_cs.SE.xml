<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 26 Jul 2024 04:00:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 26 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>MathViz-E: A Case-study in Domain-Specialized Tool-Using Agents</title>
      <link>https://arxiv.org/abs/2407.17544</link>
      <description>arXiv:2407.17544v1 Announce Type: new 
Abstract: There has been significant recent interest in harnessing LLMs to control software systems through multi-step reasoning, planning and tool-usage. While some promising results have been obtained, application to specific domains raises several general issues including the control of specialized domain tools, the lack of existing datasets for training and evaluation, and the non-triviality of automated system evaluation and improvement. In this paper, we present a case-study where we examine these issues in the context of a specific domain. Specifically, we present an automated math visualizer and solver system for mathematical pedagogy. The system orchestrates mathematical solvers and math graphing tools to produce accurate visualizations from simple natural language commands. We describe the creation of specialized data-sets, and also develop an auto-evaluator to easily evaluate the outputs of our system by comparing them to ground-truth expressions. We have open sourced the data-sets and code for the proposed system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17544v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arya Bulusu, Brandon Man, Ashish Jagmohan, Aditya Vempaty, Jennifer Mari-Wyka, Deepak Akkil</dc:creator>
    </item>
    <item>
      <title>Large Language Models for Anomaly Detection in Computational Workflows: from Supervised Fine-Tuning to In-Context Learning</title>
      <link>https://arxiv.org/abs/2407.17545</link>
      <description>arXiv:2407.17545v1 Announce Type: new 
Abstract: Anomaly detection in computational workflows is critical for ensuring system reliability and security. However, traditional rule-based methods struggle to detect novel anomalies. This paper leverages large language models (LLMs) for workflow anomaly detection by exploiting their ability to learn complex data patterns. Two approaches are investigated: 1) supervised fine-tuning (SFT), where pre-trained LLMs are fine-tuned on labeled data for sentence classification to identify anomalies, and 2) in-context learning (ICL) where prompts containing task descriptions and examples guide LLMs in few-shot anomaly detection without fine-tuning. The paper evaluates the performance, efficiency, generalization of SFT models, and explores zero-shot and few-shot ICL prompts and interpretability enhancement via chain-of-thought prompting. Experiments across multiple workflow datasets demonstrate the promising potential of LLMs for effective anomaly detection in complex executions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17545v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongwei Jin, George Papadimitriou, Krishnan Raghavan, Pawel Zuk, Prasanna Balaprakash, Cong Wang, Anirban Mandal, Ewa Deelman</dc:creator>
    </item>
    <item>
      <title>BLAZE: Cross-Language and Cross-Project Bug Localization via Dynamic Chunking and Hard Example Learning</title>
      <link>https://arxiv.org/abs/2407.17631</link>
      <description>arXiv:2407.17631v1 Announce Type: new 
Abstract: Software bugs require developers to exert significant effort to identify and resolve them, often consuming about one-third of their time. Bug localization, the process of pinpointing the exact source code files that need modification, is crucial in reducing this effort. Existing bug localization tools, typically reliant on deep learning techniques, face limitations in cross-project applicability and effectiveness in multi-language environments. Recent advancements with Large Language Models (LLMs) offer detailed representations for bug localization. However, they encounter challenges with limited context windows and mapping accuracy. To address these issues, we propose BLAZE, an approach that employs dynamic chunking and hard example learning. First, BLAZE dynamically segments source code to minimize continuity loss. Then, BLAZE fine-tunes a GPT-based model using challenging bug cases, in order to enhance cross-project and cross-language bug localization. To support the capability of BLAZE, we create the BEETLEBOX dataset, which comprises 26,321 bugs from 29 large and thriving open-source projects across five different programming languages (Java, C++, Python, Go, and JavaScript). Our evaluations of BLAZE on three benchmark datasets BEETLEBOX, SWE-Bench, and Ye et al. demonstrate substantial improvements compared to six state-of-the-art baselines. Specifically, BLAZE achieves up to an increase of 120% in Top 1 accuracy, 144% in Mean Average Precision (MAP), and 100% in Mean Reciprocal Rank (MRR). An extensive ablation study confirms the contributions of our pipeline components to the overall performance enhancement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17631v1</guid>
      <category>cs.SE</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Partha Chakraborty, Mahmoud Alfadel, Meiyappan Nagappan</dc:creator>
    </item>
    <item>
      <title>A Proposal for a Debugging Learning Support Environment for Undergraduate Students Majoring in Computer Science</title>
      <link>https://arxiv.org/abs/2407.17743</link>
      <description>arXiv:2407.17743v1 Announce Type: new 
Abstract: In software development, encountering bugs is inevitable. However, opportunities to learn more about bug removal are limited. When students perform debugging tasks, they often use print statements because students do not know how to use a debugger or have never used one.In this study, among various debugging methods, we focused on debugging using breakpoints. We implemented a function in Scratch, a visual programming language, that allows for self-learning of correct breakpoint placement and systematic debugging procedures.In this paper, we discuss experimental results that clarify the changes that occur in subjects when they learn debugging in Scratch.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17743v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aoi Kanaya, Takuma Migo, Hiroaki Hashiura</dc:creator>
    </item>
    <item>
      <title>Automatic Data Labeling for Software Vulnerability Prediction Models: How Far Are We?</title>
      <link>https://arxiv.org/abs/2407.17803</link>
      <description>arXiv:2407.17803v1 Announce Type: new 
Abstract: Background: Software Vulnerability (SV) prediction needs large-sized and high-quality data to perform well. Current SV datasets mostly require expensive labeling efforts by experts (human-labeled) and thus are limited in size. Meanwhile, there are growing efforts in automatic SV labeling at scale. However, the fitness of auto-labeled data for SV prediction is still largely unknown. Aims: We quantitatively and qualitatively study the quality and use of the state-of-the-art auto-labeled SV data, D2A, for SV prediction. Method: Using multiple sources and manual validation, we curate clean SV data from human-labeled SV-fixing commits in two well-known projects for investigating the auto-labeled counterparts. Results: We discover that 50+% of the auto-labeled SVs are noisy (incorrectly labeled), and they hardly overlap with the publicly reported ones. Yet, SV prediction models utilizing the noisy auto-labeled SVs can perform up to 22% and 90% better in Matthews Correlation Coefficient and Recall, respectively, than the original models. We also reveal the promises and difficulties of applying noise-reduction methods for automatically addressing the noise in auto-labeled SV data to maximize the data utilization for SV prediction. Conclusions: Our study informs the benefits and challenges of using auto-labeled SVs, paving the way for large-scale SV prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17803v1</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Triet H. M. Le, M. Ali Babar</dc:creator>
    </item>
    <item>
      <title>Compilation of Commit Changes within Java Source Code Repositories</title>
      <link>https://arxiv.org/abs/2407.17853</link>
      <description>arXiv:2407.17853v1 Announce Type: new 
Abstract: Java applications include third-party dependencies as bytecode. To keep these applications secure, researchers have proposed tools to re-identify dependencies that contain known vulnerabilities. Yet, to allow such re-identification, one must obtain, for each vulnerability patch, the bytecode fixing the respective vulnerability at first. Such patches for dependencies are curated in databases in the form of fix-commits. But fixcommits are in source code, and automatically compiling whole Java projects to bytecode is notoriously hard, particularly for non-current versions of the code. In this paper, we thus propose JESS, an approach that largely avoids this problem by compiling solely the relevant code that was modified within a given commit. JESS reduces the code, retaining only those parts that the committed change references. To avoid name-resolution errors, JESS automatically infers stubs for references to entities that are unavailable to the compiler. A challenge is here that, to facilitate the above mentioned reidentification, JESS must seek to produce bytecode that is almost identical to the bytecode which one would obtain by a successful compilation of the full project. An evaluation on 347 GitHub projects shows that JESS is able to compile, in isolation, 72% of methods and constructors, of which 89% have bytecode equal to the original one. Furthermore, on the Project KB database of fix-commits, in which only 8% of files modified within the commits can be compiled with the provided build scripts, JESS is able to compile 73% of all files that these commits modify.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17853v1</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefan Schott, Wolfram Fischer, Serena Elisa Ponta, Jonas Klauke, Eric Bodden</dc:creator>
    </item>
    <item>
      <title>RDFGraphGen: A Synthetic RDF Graph Generator based on SHACL Constraints</title>
      <link>https://arxiv.org/abs/2407.17941</link>
      <description>arXiv:2407.17941v1 Announce Type: new 
Abstract: This paper introduces RDFGraphGen, a general-purpose, domain-independent generator of synthetic RDF graphs based on SHACL constraints. The Shapes Constraint Language (SHACL) is a W3C standard which specifies ways to validate data in RDF graphs, by defining constraining shapes. However, even though the main purpose of SHACL is validation of existing RDF data, in order to solve the problem with the lack of available RDF datasets in multiple RDF-based application development processes, we envisioned and implemented a reverse role for SHACL: we use SHACL shape definitions as a starting point to generate synthetic data for an RDF graph. The generation process involves extracting the constraints from the SHACL shapes, converting the specified constraints into rules, and then generating artificial data for a predefined number of RDF entities, based on these rules. The purpose of RDFGraphGen is the generation of small, medium or large RDF knowledge graphs for the purpose of benchmarking, testing, quality control, training and other similar purposes for applications from the RDF, Linked Data and Semantic Web domain. RDFGraphGen is open-source and is available as a ready-to-use Python package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17941v1</guid>
      <category>cs.SE</category>
      <category>cs.DB</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marija Vecovska, Milos Jovanovik</dc:creator>
    </item>
    <item>
      <title>Towards Living Software Architecture Diagrams</title>
      <link>https://arxiv.org/abs/2407.17990</link>
      <description>arXiv:2407.17990v1 Announce Type: new 
Abstract: Software architecture often consists of interconnected components dispersed across source code and other development artifacts, making visualization difficult without costly additional documentation. Although some tools can automatically generate architectural diagrams, these hardly fully reflect the architecture of the system. We propose the value of automatic architecture recovery from multiple software artifacts, combined with the ability to manually adjust recovered models and automate the recovery process. We present a general approach to achieve this and describe a tool that generates architectural diagrams for a software system by analyzing its software artifacts and unifying them into a comprehensive system representation. This representation can be manually modified while ensuring that changes are reintegrated into the diagram when it is regenerated. We argue that adopting a similar approach in other types of documentation tools is possible and can render similar benefits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17990v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filipe F. Correia, Ricardo Ferreira, Paulo G. G Queiroz, Henrique Nunes, Matilde Barra, Duarte Figueiredo</dc:creator>
    </item>
    <item>
      <title>An Exploration Study on Developing Blockchain Systems the Practitioners Perspective</title>
      <link>https://arxiv.org/abs/2407.18005</link>
      <description>arXiv:2407.18005v1 Announce Type: new 
Abstract: Context: Blockchain-based software (BBS) exploits the concepts and technologies popularized by cryptocurrencies offering decentralized transaction ledgers with immutable content for security-critical and transaction critical systems. Recent research has explored the strategic benefits and technical limitations of BBS in various fields, including cybersecurity, healthcare, education, and financial technologies. Despite growing interest from academia and industry, there is a lack of empirical evidence, leading to an incomplete understanding of the processes, methods, and techniques necessary for systematic BBS development. Objectives: Existing research lacks a consolidated view, particularly empirically driven guidelines based on published evidence and development practices. This study aims to address the gap by consolidating empirical evidence and development practices to derive or leverage existing processes, patterns, and models for designing, implementing, and validating BBS systems. Method: Tied to this knowledge gap, we conducted a two-phase research project. First, a systematic literature review of 58 studies was performed to identify a development process comprising 23 tasks for BBS systems. Second, a survey of 102 blockchain practitioners from 35 countries across six continents was conducted to validate the BBS system development process. Results: Our results revealed a statistically significant difference (p-value &lt;.001) in the importance ratings of 24 out of 26 BBS tasks by our participants. The only two tasks that were not statistically significant were incentive protocol design and granularity design. Conclusion: Our research is among the first to advance understanding on the aspect of development process for blockchain-based systems and helps researchers and practitioners in their quests on challenges and recommendations associated with the development of BBS systems</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18005v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bakheet Aljedaani, Aakash Ahmad, Mahdi Fahmideh, Arif Ali Khan, Jun Shen</dc:creator>
    </item>
    <item>
      <title>Test2VA: Reusing GUI Test Cases for Voice Assistant Features Development in Mobile Applications</title>
      <link>https://arxiv.org/abs/2407.18155</link>
      <description>arXiv:2407.18155v1 Announce Type: new 
Abstract: Voice Assistant (VA) in smartphones has become very popular with millions of users nowadays. A key trend is the rise of custom VA embedding, which enables users to perform the customized tasks of their favorite app through voice control. However, with such a great demand, little effort has been made to support app developers in VA development. Moreover, many user-oriented VA control approaches even increase the programming burden on developers. To reduce the workload and improve code efficiency, in this paper, we propose a novel approach, Test2VA, that reuses the test code of an application to support its VA development. Specifically, Test2VA extracts the task completion pattern from the GUI test code and then generates an execution method to perform the same task in general. To identify the pattern, Test2VA uses a mutation-based exploration to detect the mutable GUI event in the test case and later parameterize it in the VA method. We conducted an evaluation on 48 test cases from eight real-world applications. The results show that Test2VA correctly detects 75.68% of the mutable events from 48 original test cases and then generates 33 methods and have them successfully executed and manually examined.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18155v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Garrett Weaver, Xue Qin</dc:creator>
    </item>
    <item>
      <title>In Search of Metrics to Guide Developer-Based Refactoring Recommendations</title>
      <link>https://arxiv.org/abs/2407.18169</link>
      <description>arXiv:2407.18169v1 Announce Type: new 
Abstract: Context. Source code refactoring is a well-established approach to improving source code quality without compromising its external behavior. Motivation. The literature described the benefits of refactoring, yet its application in practice is threatened by the high cost of time, resource allocation, and effort required to perform it continuously. Providing refactoring recommendations closer to what developers perceive as relevant may support the broader application of refactoring in practice and drive prioritization efforts. Aim. In this paper, we aim to foster the design of a developer-based refactoring recommender, proposing an empirical study into the metrics that study the developer's willingness to apply refactoring operations. We build upon previous work describing the developer's motivations for refactoring and investigate how product and process metrics may grasp those motivations. Expected Results. We will quantify the value of product and process metrics in grasping developers' motivations to perform refactoring, thus providing a catalog of metrics for developer-based refactoring recommenders to use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18169v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikel Robredo, Matteo Esposito, Fabio Palomba, Rafael Pe\~naloza, Valentina Lenarduzzi</dc:creator>
    </item>
    <item>
      <title>An Approach to Detect Abnormal Submissions for CodeWorkout Dataset</title>
      <link>https://arxiv.org/abs/2407.17475</link>
      <description>arXiv:2407.17475v1 Announce Type: cross 
Abstract: Students interactions while solving problems in learning environments (i.e. log data) are often used to support students learning. For example, researchers use log data to develop systems that can provide students with personalized problem recommendations based on their knowledge level. However, anomalies in the students log data, such as cheating to solve programming problems, could introduce a hidden bias in the log data. As a result, these systems may provide inaccurate problem recommendations, and therefore, defeat their purpose. Classical cheating detection methods, such as MOSS, can be used to detect code plagiarism. However, these methods cannot detect other abnormal events such as a student gaming a system with multiple attempts of similar solutions to a particular programming problem. This paper presents a preliminary study to analyze log data with anomalies. The goal of our work is to overcome the abnormal instances when modeling personalizable recommendations in programming learning environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17475v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alex Hicks, Yang Shi, Arun-Balajiee Lekshmi-Narayanan, Wei Yan, Samiha Marwan</dc:creator>
    </item>
    <item>
      <title>LAMBDA: A Large Model Based Data Agent</title>
      <link>https://arxiv.org/abs/2407.17535</link>
      <description>arXiv:2407.17535v1 Announce Type: cross 
Abstract: We introduce ``LAMBDA," a novel open-source, code-free multi-agent data analysis system that that harnesses the power of large models. LAMBDA is designed to address data analysis challenges in complex data-driven applications through the use of innovatively designed data agents that operate iteratively and generatively using natural language. At the core of LAMBDA are two key agent roles: the programmer and the inspector, which are engineered to work together seamlessly. Specifically, the programmer generates code based on the user's instructions and domain-specific knowledge, enhanced by advanced models. Meanwhile, the inspector debugs the code when necessary. To ensure robustness and handle adverse scenarios, LAMBDA features a user interface that allows direct user intervention in the operational loop. Additionally, LAMBDA can flexibly integrate external models and algorithms through our knowledge integration mechanism, catering to the needs of customized data analysis. LAMBDA has demonstrated strong performance on various machine learning datasets. It has the potential to enhance data science practice and analysis paradigm by seamlessly integrating human and artificial intelligence, making it more accessible, effective, and efficient for individuals from diverse backgrounds. The strong performance of LAMBDA in solving data science problems is demonstrated in several case studies, which are presented at \url{https://www.polyu.edu.hk/ama/cmfai/lambda.html}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17535v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maojun Sun, Ruijian Han, Binyan Jiang, Houduo Qi, Defeng Sun, Yancheng Yuan, Jian Huang</dc:creator>
    </item>
    <item>
      <title>Unraveling the Never-Ending Story of Lifecycles and Vitalizing Processes</title>
      <link>https://arxiv.org/abs/2407.17881</link>
      <description>arXiv:2407.17881v1 Announce Type: cross 
Abstract: Business process management (BPM) has been widely used to discover, model, analyze, and optimize organizational processes. BPM looks at these processes with analysis techniques that assume a clearly defined start and end. However, not all processes adhere to this logic, with the consequence that their behavior cannot be appropriately captured by BPM analysis techniques. This paper addresses this research problem at a conceptual level. More specifically, we introduce the notion of vitalizing business processes that target the lifecycle process of one or more entities. We show the existence of lifecycle processes in many industries and that their appropriate conceptualizations pave the way for the definition of suitable modeling and analysis techniques. This paper provides a set of requirements for their analysis, and a conceptualization of lifecycle and vitalizing processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17881v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stephan A. Fahrenkrog-Petersen, Saimir Bala, Luise Pufahl, Jan Mendling</dc:creator>
    </item>
    <item>
      <title>The Larger the Better? Improved LLM Code-Generation via Budget Reallocation</title>
      <link>https://arxiv.org/abs/2404.00725</link>
      <description>arXiv:2404.00725v2 Announce Type: replace 
Abstract: It is a common belief that large language models (LLMs) are better than smaller-sized ones. However, larger models also require significantly more time and compute during inference. This begs the question: what happens when both models operate under the same budget? (e.g., compute, run-time). To address this question, we analyze code generation LLMs of various sizes and make comparisons such as running a 70B model once vs. generating five outputs from a 13B model. We consider a standard unit-test setup, which can be used to select the correct output from the smaller model. Our findings reveal that the repeated use of smaller models can yield consistent improvements, with gains of up to 15% across five tasks. On the other hand, in scenarios where unit-tests are unavailable, a ranking-based selection of candidates from the smaller model falls short of the performance of a single output from larger ones. Our results highlight the potential of using smaller models instead of larger ones, and the importance of studying approaches for ranking LLM outputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00725v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Michael Hassid, Tal Remez, Jonas Gehring, Roy Schwartz, Yossi Adi</dc:creator>
    </item>
    <item>
      <title>AutoCodeRover: Autonomous Program Improvement</title>
      <link>https://arxiv.org/abs/2404.05427</link>
      <description>arXiv:2404.05427v3 Announce Type: replace 
Abstract: Researchers have made significant progress in automating the software development process in the past decades. Recent progress in Large Language Models (LLMs) has significantly impacted the development process, where developers can use LLM-based programming assistants to achieve automated coding. Nevertheless, software engineering involves the process of program improvement apart from coding, specifically to enable software maintenance (e.g. bug fixing) and software evolution (e.g. feature additions). In this paper, we propose an automated approach for solving GitHub issues to autonomously achieve program improvement. In our approach called AutoCodeRover, LLMs are combined with sophisticated code search capabilities, ultimately leading to a program modification or patch. In contrast to recent LLM agent approaches from AI researchers and practitioners, our outlook is more software engineering oriented. We work on a program representation (abstract syntax tree) as opposed to viewing a software project as a mere collection of files. Our code search exploits the program structure in the form of classes/methods to enhance LLM's understanding of the issue's root cause, and effectively retrieve a context via iterative search. The use of spectrum-based fault localization using tests, further sharpens the context, as long as a test-suite is available. Experiments on SWE-bench-lite (300 real-life GitHub issues) show increased efficacy in solving GitHub issues (19% on SWE-bench-lite), which is higher than the efficacy of the recently reported SWE-agent. In addition, AutoCodeRover achieved this efficacy with significantly lower cost (on average, $0.43 USD), compared to other baselines. We posit that our workflow enables autonomous software engineering, where, in future, auto-generated code from LLMs can be autonomously improved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05427v3</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, Abhik Roychoudhury</dc:creator>
    </item>
    <item>
      <title>Automated Code-centric Software Vulnerability Assessment: How Far Are We? An Empirical Study in C/C++</title>
      <link>https://arxiv.org/abs/2407.17053</link>
      <description>arXiv:2407.17053v2 Announce Type: replace 
Abstract: Background: The C and C++ languages hold significant importance in Software Engineering research because of their widespread use in practice. Numerous studies have utilized Machine Learning (ML) and Deep Learning (DL) techniques to detect software vulnerabilities (SVs) in the source code written in these languages. However, the application of these techniques in function-level SV assessment has been largely unexplored. SV assessment is increasingly crucial as it provides detailed information on the exploitability, impacts, and severity of security defects, thereby aiding in their prioritization and remediation. Aims: We conduct the first empirical study to investigate and compare the performance of ML and DL models, many of which have been used for SV detection, for function-level SV assessment in C/C++. Method: Using 9,993 vulnerable C/C++ functions, we evaluated the performance of six multi-class ML models and five multi-class DL models for the SV assessment at the function level based on the Common Vulnerability Scoring System (CVSS). We further explore multi-task learning, which can leverage common vulnerable code to predict all SV assessment outputs simultaneously in a single model, and compare the effectiveness and efficiency of this model type with those of the original multi-class models. Results: We show that ML has matching or even better performance compared to the multi-class DL models for function-level SV assessment with significantly less training time. Employing multi-task learning allows the DL models to perform significantly better, with an average of 8-22% increase in Matthews Correlation Coefficient (MCC). Conclusions: We distill the practices of using data-driven techniques for function-level SV assessment in C/C++, including the use of multi-task DL to balance efficiency and effectiveness. This can establish a strong foundation for future work in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17053v2</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anh The Nguyen, Triet Huynh Minh Le, M. Ali Babar</dc:creator>
    </item>
    <item>
      <title>Brand Network Booster: A new system for improving brand connectivity</title>
      <link>https://arxiv.org/abs/2309.16228</link>
      <description>arXiv:2309.16228v2 Announce Type: replace-cross 
Abstract: This paper presents a new decision support system offered for an in-depth analysis of semantic networks, which can provide insights for a better exploration of a brand's image and the improvement of its connectivity. In terms of network analysis, we show that this goal is achieved by solving an extended version of the Maximum Betweenness Improvement problem, which includes the possibility of considering adversarial nodes, constrained budgets, and weighted networks - where connectivity improvement can be obtained by adding links or increasing the weight of existing connections. Our contribution includes a new algorithmic framework and the integration of this framework into a software system called Brand Network Booster (BNB), which supports brand connectivity evaluation and improvement. We present this new system together with three case studies, and we also discuss its performance. Our tool and approach are valuable to both network scholars and in facilitating strategic decision-making processes for marketing and communication managers across various sectors, be it public or private.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.16228v2</guid>
      <category>cs.SI</category>
      <category>cs.CL</category>
      <category>cs.SE</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cie.2024.110389</arxiv:DOI>
      <arxiv:journal_reference>Computers &amp; Industrial Engineering 194, 110389 (2024)</arxiv:journal_reference>
      <dc:creator>J. Cancellieri, W. Didimo, A. Fronzetti Colladon, F. Montecchiani, R. Vestrelli</dc:creator>
    </item>
  </channel>
</rss>
