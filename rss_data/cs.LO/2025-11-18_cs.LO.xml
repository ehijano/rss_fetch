<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.LO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.LO</link>
    <description>cs.LO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.LO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Nov 2025 02:45:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Logspace Constructive Proof of L=SL</title>
      <link>https://arxiv.org/abs/2511.12011</link>
      <description>arXiv:2511.12011v1 Announce Type: new 
Abstract: We formalize the proof of Reingold's Theorem that SL=L [Rei05] in the theory of bounded arithmetic VL, which corresponds to ``logspace reasoning''. As a consequence, we get that VL=VSL, where VSL is the theory of bounded arithmetic for ``symmetric-logspace reasoning''. This resolves in the affirmative an old open question from Kolokolova [Kol05] (see also Cook-Nguyen [NC10]).
  Our proof relies on the Rozenman-Vadhan alternative proof of Reingold's Theorem ([RV05]). To formalize this proof in VL, we need to avoid reasoning about eigenvalues and eigenvectors (common in both original proofs of SL=L). We achieve this by using some results from Buss-Kabanets-Kolokolova-Kouck\'y [Bus+20] that allow VL to reason about graph expansion in combinatorial terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12011v1</guid>
      <category>cs.LO</category>
      <category>math.LO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sam Buss, Anant Dhayal, Valentine Kabanets, Antonina Kolokolova, Sasank Mouli</dc:creator>
    </item>
    <item>
      <title>Proceedings Seventh International Workshop on Formal Methods for Autonomous Systems</title>
      <link>https://arxiv.org/abs/2511.13245</link>
      <description>arXiv:2511.13245v1 Announce Type: new 
Abstract: This EPTCS volume contains the papers from the Seventh International Workshop on Formal Methods for Autonomous Systems (FMAS 2025), which was held between the 17th and 19th of November 2025. The goal of the FMAS workshop series is to bring together leading researchers who are using formal methods to tackle the unique challenges that autonomous systems present, so that they can publish and discuss their work with a growing community of researchers. FMAS 2025 was co-located with the 20th International Conference on integrated Formal Methods (iFM'25), hosted by Inria Paris, France at the Inria Paris Center. 
  In total, FMAS 2025 received 16 submissions from researchers at institutions in: Canada, China, France, Germany, Ireland, Italy, Japan, the Netherlands, Portugal, Sweden, the United States of America, and the United Kingdom. Though we received fewer submissions than last year, we are encouraged to see the submissions being sent from a wide range of countries. Submissions come from both past and new FMAS authors, which shows us that the existing community appreciates the network that FMAS has built over the past 7 years, while new authors also show the FMAS community's great potential of growth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13245v1</guid>
      <category>cs.LO</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.436</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 436, 2025</arxiv:journal_reference>
      <dc:creator>Matt Luckcuck, Maike Schwammberger, Mengwei Xu</dc:creator>
    </item>
    <item>
      <title>Multi-Objective Statistical Model Checking using Lightweight Strategy Sampling (extended version)</title>
      <link>https://arxiv.org/abs/2511.13460</link>
      <description>arXiv:2511.13460v1 Announce Type: new 
Abstract: Statistical model checking delivers quantitative verification results with statistical guarantees by applying Monte Carlo simulation to formal models. It scales to model sizes and model types that are out of reach for exhaustive, analytical techniques. So far, it has been used to evaluate one property value at a time only. Many practical problems, however, require finding the Pareto front of optimal tradeoffs between multiple possibly conflicting optimisation objectives. In this paper, we present the first statistical model checking approach for such multi-objective Pareto queries, using lightweight strategy sampling to optimise over the model's nondeterministic choices. We first introduce an incremental scheme that almost surely converges to a statistically sound confidence band bounding the true Pareto front from both sides in the long run. To obtain a close underapproximation of the true front in finite time, we then propose three heuristic approaches that try to make the best of an a-priori fixed sampling budget. We implement our new techniques in the Modest Toolset's 'modes' simulator, and experimentally show their effectiveness on quantitative verification benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13460v1</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pedro R. D'Argenio, Arnd Hartmanns, Patrick Wienh\"oft, Mark van Wijk</dc:creator>
    </item>
    <item>
      <title>Subgraph Isomorphism: Prolog vs. Conventional</title>
      <link>https://arxiv.org/abs/2511.13600</link>
      <description>arXiv:2511.13600v1 Announce Type: new 
Abstract: Subgraph Isomorphism uses a small graph as a pattern to identify within a larger graph a set of vertices that have matching edges. This paper addresses a logic program written in Prolog for a specific relatively complex graph pattern for which multiple conventional implementations (including parallel) exist. The goal is to understand the complexity differences between programming logically and programming conventionally. Discussion includes the process of converting the graph pattern into logic statements in Prolog, and the resulting characteristics as the size of the graph increased. The analysis shows that using a logic paradigm is an efficient way to attack complex graph problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13600v1</guid>
      <category>cs.LO</category>
      <category>cs.DS</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claire Y. Yin, Peter M. Kogge</dc:creator>
    </item>
    <item>
      <title>Sound Logical Explanations for Mean Aggregation Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2511.11593</link>
      <description>arXiv:2511.11593v1 Announce Type: cross 
Abstract: Graph neural networks (GNNs) are frequently used for knowledge graph completion. Their black-box nature has motivated work that uses sound logical rules to explain predictions and characterise their expressivity. However, despite the prevalence of GNNs that use mean as an aggregation function, explainability and expressivity results are lacking for them. We consider GNNs with mean aggregation and non-negative weights (MAGNNs), proving the precise class of monotonic rules that can be sound for them, as well as providing a restricted fragment of first-order logic to explain any MAGNN prediction. Our experiments show that restricting mean-aggregation GNNs to have non-negative weights yields comparable or improved performance on standard inductive benchmarks, that sound rules are obtained in practice, that insightful explanations can be generated in practice, and that the sound rules can expose issues in the trained models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11593v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Morris, Ian Horrocks</dc:creator>
    </item>
    <item>
      <title>Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy</title>
      <link>https://arxiv.org/abs/2511.11816</link>
      <description>arXiv:2511.11816v1 Announce Type: cross 
Abstract: Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11816v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Brunello, Luca Geatti, Michele Mignani, Angelo Montanari, Nicola Saccomanno</dc:creator>
    </item>
    <item>
      <title>Towards Autoformalization of LLM-generated Outputs for Requirement Verification</title>
      <link>https://arxiv.org/abs/2511.11829</link>
      <description>arXiv:2511.11829v1 Announce Type: cross 
Abstract: Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11829v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mihir Gupte, Ramesh S</dc:creator>
    </item>
    <item>
      <title>Computational and Categorical Frameworks of Finite Ternary $\Gamma$-Semirings: Foundations, Algorithms, and Industrial Modeling Applications</title>
      <link>https://arxiv.org/abs/2511.12323</link>
      <description>arXiv:2511.12323v1 Announce Type: cross 
Abstract: Purpose: This study extends the structural theory of finite commutative ternary $\Gamma$-semirings into a computational and categorical framework for explicit classification and constructive reasoning.Methods: Constraint-driven enumeration algorithms are developed to generate all non-isomorphic finite ternary $\Gamma$-semirings satisfying closure, distributivity, and symmetry. Automorphism analysis, canonical labeling, and pruning strategies ensure uniqueness and tractability, while categorical constructs formalize algebraic relationships. \textit{Results:} The implementation classifies all systems of order $|T|\!\le\!4$ and verifies symmetry-based subvarieties. Complexity analysis confirms polynomial-time performance, and categorical interpretation connects ternary $\Gamma$-semirings with functorial models in universal algebra. \\ Conclusion: The work establishes a verified computational theory and categorical synthesis for finite ternary $\Gamma$-semirings, integrating algebraic structure, algorithmic enumeration, and symbolic computation to support future industrial and decision-model applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12323v1</guid>
      <category>math.RA</category>
      <category>cs.LO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chandrasekhar Gokavarapu (Department of Mathematics, Government College), Madhusudhana Rao Dasari (Department of Mathematics, Government College for Women)</dc:creator>
    </item>
    <item>
      <title>Evaluating Autoformalization Robustness via Semantically Similar Paraphrasing</title>
      <link>https://arxiv.org/abs/2511.12784</link>
      <description>arXiv:2511.12784v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for autoformalization. Despite their impressive performance, these models can still struggle to produce grounded and verifiable formalizations. Recent work in text-to-SQL, has revealed that LLMs can be sensitive to paraphrased natural language (NL) inputs, even when high degrees of semantic fidelity are preserved (Safarzadeh, Oroojlooyjadid, and Roth 2025). In this paper, we investigate this claim in the autoformalization domain. Specifically, we evaluate the robustness of LLMs generating formal proofs with semantically similar paraphrased NL statements by measuring semantic and compilation validity. Using the formal benchmarks MiniF2F (Zheng, Han, and Polu 2021) and Lean 4 version of ProofNet (Xin et al. 2024), and two modern LLMs, we generate paraphrased natural language statements and cross-evaluate these statements across both models. The results of this paper reveal performance variability across paraphrased inputs, demonstrating that minor shifts in NL statements can significantly impact model outputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12784v1</guid>
      <category>cs.CL</category>
      <category>cs.LO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hayden Moore, Asfahan Shah</dc:creator>
    </item>
    <item>
      <title>Expressive Temporal Specifications for Reward Monitoring</title>
      <link>https://arxiv.org/abs/2511.12808</link>
      <description>arXiv:2511.12808v1 Announce Type: cross 
Abstract: Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\text{LTL}_f[\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12808v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omar Adalat, Francesco Belardinelli</dc:creator>
    </item>
    <item>
      <title>Formal Foundations for Controlled Stochastic Activity Networks</title>
      <link>https://arxiv.org/abs/2511.12974</link>
      <description>arXiv:2511.12974v1 Announce Type: cross 
Abstract: We introduce Controlled Stochastic Activity Networks (Controlled SANs), a formal extension of classical Stochastic Activity Networks that integrates explicit control actions into a unified semantic framework for modeling distributed real-time systems. Controlled SANs systematically capture dynamic behavior involving nondeterminism, probabilistic branching, and stochastic timing, while enabling policy-driven decision-making within a rigorous mathematical framework.
  We develop a hierarchical, automata-theoretic semantics for Controlled SANs that encompasses nondeterministic, probabilistic, and stochastic models in a uniform manner. A structured taxonomy of control policies, ranging from memoryless and finite-memory strategies to computationally augmented policies, is formalized, and their expressive power is characterized through associated language classes. To support model abstraction and compositional reasoning, we introduce behavioral equivalences, including bisimulation and stochastic isomorphism.
  Controlled SANs generalize classical frameworks such as continuous-time Markov decision processes (CTMDPs), providing a rigorous foundation for the specification, verification, and synthesis of dependable systems operating under uncertainty. This framework enables both quantitative and qualitative analysis, advancing the design of safety-critical systems where control, timing, and stochasticity are tightly coupled.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12974v1</guid>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ali Movaghar</dc:creator>
    </item>
    <item>
      <title>Are automated proof assistants ready for semigroup research? Orientation-preserving mappings and proof assistant Lean</title>
      <link>https://arxiv.org/abs/2511.13304</link>
      <description>arXiv:2511.13304v1 Announce Type: cross 
Abstract: An orientation-preserving mapping is not always defined by how it acts on triples of elements. Although this fact is simple and well-known, it sometimes gets overlooked, resulting in wrong statements in publications. Automated proof assistants are a technology that is supposed to ensure that proofs exactly match the results, thus pre-empting mistakes. In this paper we successfully formalise the definition of orientation-preserving mappings in proof assistant software Lean and construct a computer-verified proof of the above fact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13304v1</guid>
      <category>math.CO</category>
      <category>cs.LO</category>
      <category>math.GR</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alastair Litterick, Alexei Vernitski, Billy Woods</dc:creator>
    </item>
    <item>
      <title>Computation and Concurrency</title>
      <link>https://arxiv.org/abs/2409.02595</link>
      <description>arXiv:2409.02595v3 Announce Type: replace 
Abstract: We try to clarify the relationship between computation and concurrency. Base on the so-called pomset automata, we introduce communication and more operators, and establish the algebras modulo language equivalence and truly concurrent bisimilarities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02595v3</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yong Wang</dc:creator>
    </item>
    <item>
      <title>Weak Simplicial Bisimilarity and Minimisation for Polyhedral Model Checking</title>
      <link>https://arxiv.org/abs/2411.11428</link>
      <description>arXiv:2411.11428v3 Announce Type: replace 
Abstract: The work described in this paper builds on the polyhedral semantics of the Spatial Logic for Closure Spaces (SLCS) and the geometric spatial model checker PolyLogicA. Polyhedral models are central in domains that exploit mesh processing, such as 3D computer graphics. A discrete representation of polyhedral models is given by cell poset models, which are amenable to geometric spatial model checking on polyhedral models using the logical language SLCS$\eta$, a weaker version of SLCS. In this work we show that the mapping from polyhedral models to cell poset models preserves and reflects SLCS$\eta$. We also propose weak simplicial bisimilarity on polyhedral models and weak $\pm$-bisimilarity on cell poset models, where by ``weak'' we mean that the relevant equivalence is coarser than the corresponding one for SLCS, leading to a greater reduction of the size of models and thus to more efficient model checking. We show that the proposed bisimilarities enjoy the Hennessy-Milner property, i.e. two points are weakly simplicial bisimilar iff they are logically equivalent for SLCS$\eta$. Similarly, two cells are weakly $\pm$-bisimilar iff they are logically equivalent in the poset-model interpretation of SLCS$\eta$. Furthermore we present a model minimisation procedure and prove that it correctly computes the minimal model with respect to weak $\pm$-bisimilarity, i.e. with respect to logical equivalence of SLCS$\eta$. The procedure works via an encoding into LTSs and then exploits branching bisimilarity on those LTSs, exploiting the minimisation capabilities as included in the mCRL2 toolset. Various examples show the effectiveness of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11428v3</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nick Bezhanishvili, Laura Bussi, Vincenzo Ciancia, David Gabelaia, Mamuka Jibladze, Diego Latella, Mieke Massink, Erik P. de Vink</dc:creator>
    </item>
    <item>
      <title>A Sequent Calculus For Trace Formula Implication</title>
      <link>https://arxiv.org/abs/2505.03693</link>
      <description>arXiv:2505.03693v3 Announce Type: replace 
Abstract: Specification languages are essential in deductive program verification, but they are usually based on first-order logic, hence less expressive than the programs they specify. Recently, trace specification logics with fixed points that are at least as expressive as their target programs were proposed. This makes it possible to specify not merely pre- and postconditions, but the whole trace of even recursive programs. Previous work established a sound and complete calculus to determine whether a program satisfies a given trace formula. However, the applicability of the calculus and prospects for mechanized verification rely on the ability to prove consequence between trace formulas. We present a sound sequent calculus for proving implication (i.e. trace inclusion) between trace formulas. To handle fixed point operations with an unknown recursive bound, fixed point induction rules are used. We also employ contracts and {\mu}-formula synchronization. While this does not yet result in a complete calculus for trace formula implication, it is possible to prove many non-trivial properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03693v3</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niklas Heidler, Reiner H\"ahnle</dc:creator>
    </item>
    <item>
      <title>DHoTT: A Temporal Extension of Homotopy Type Theory for Semantic Drift</title>
      <link>https://arxiv.org/abs/2506.09671</link>
      <description>arXiv:2506.09671v3 Announce Type: replace 
Abstract: Dynamic HoTT (DHoTT) is a conservative extension of Homotopy Type Theory designed for evolving texts in conversational AI. In a chat system, a large language model (LLM) is queried with a growing prefix: at turn tau the input is C(tau), the concatenation of all previous prompts and replies, and the new answer extends C(tau+1). We study the logical semantics of these time indexed texts and how their meanings drift or break over time, linking Homotopy Type Theory with distributional semantics and topological data analysis on embedding spaces.
  For each turn we embed all tokens seen so far using a frozen encoder and map them to the unit sphere, build a good cover by spherical caps, and form the Cech nerve. A Kan fibrant replacement yields a Kan complex ET(tau), the Evolving Text at time tau, where identity types are path spaces and dependent types support ordinary HoTT transport. Time is treated functorially so that all HoTT rules interpret fibrewise and standard substitution and conservativity properties hold.
  On top of this fibrewise HoTT layer we add a small cross time calculus for semantic change. A carry records when a later use has a certified path back into an earlier fibre under an admissibility policy. A rupture is a positive sigma type storing finite, policy checked failed attempts together with an open horn tag. An append only ledger accumulates carries and ruptures so that earlier failures are not erased when later alignments appear.
  We illustrate the construction on small, replicable examples from real human LLM dialogues using a concrete DeBERTa based embedding pipeline. This provides a principled, geometrically grounded core for analysing and auditing semantic drift in large language models and other embedding based systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09671v3</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Poernomo</dc:creator>
    </item>
    <item>
      <title>Model Counting for Dependency Quantified Boolean Formulas</title>
      <link>https://arxiv.org/abs/2511.07337</link>
      <description>arXiv:2511.07337v2 Announce Type: replace 
Abstract: Dependency Quantified Boolean Formulas (DQBF) generalize QBF by explicitly specifying which universal variables each existential variable depends on, instead of relying on a linear quantifier order. The satisfiability problem of DQBF is NEXP-complete, and many hard problems can be succinctly encoded as DQBF. Recent work has revealed a strong analogy between DQBF and SAT: k-DQBF (with k existential variables) is a succinct form of k-SAT, and satisfiability is NEXP-complete for 3-DQBF but PSPACE-complete for 2-DQBF, mirroring the complexity gap between 3-SAT (NP-complete) and 2-SAT (NL-complete).
  Motivated by this analogy, we study the model counting problem for DQBF, denoted #DQBF. Our main theoretical result is that #2-DQBF is #EXP-complete, where #EXP is the exponential-time analogue of #P. This parallels Valiant's classical theorem stating that #2-SAT is #P-complete. As a direct application, we show that first-order model counting (FOMC) remains #EXP-complete even when restricted to a PSPACE-decidable fragment of first-order logic and domain size two.
  Building on recent successes in reducing 2-DQBF satisfiability to symbolic model checking, we develop a dedicated 2-DQBF model counter. Using a diverse set of crafted instances, we experimentally evaluated it against a baseline that expands 2-DQBF formulas into propositional formulas and applies propositional model counting. While the baseline worked well when each existential variable depends on few variables, our implementation scaled significantly better to larger dependency sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07337v2</guid>
      <category>cs.LO</category>
      <category>cs.CC</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Long-Hin Fung, Che Cheng, Jie-Hong Roland Jiang, Friedrich Slivovsky, Tony Tan</dc:creator>
    </item>
    <item>
      <title>Constrained and Robust Policy Synthesis with Satisfiability-Modulo-Probabilistic-Model-Checking</title>
      <link>https://arxiv.org/abs/2511.08078</link>
      <description>arXiv:2511.08078v2 Announce Type: replace 
Abstract: The ability to compute reward-optimal policies for given and known finite Markov decision processes (MDPs) underpins a variety of applications across planning, controller synthesis, and verification. However, we often want policies (1) to be robust, i.e., they perform well on perturbations of the MDP and (2) to satisfy additional structural constraints regarding, e.g., their representation or implementation cost. Computing such robust and constrained policies is indeed computationally more challenging. This paper contributes the first approach to effectively compute robust policies subject to arbitrary structural constraints using a flexible and efficient framework. We achieve flexibility by allowing to express our constraints in a first-order theory over a set of MDPs, while the root for our efficiency lies in the tight integration of satisfiability solvers to handle the combinatorial nature of the problem and probabilistic model checking algorithms to handle the analysis of MDPs. Experiments on a few hundred benchmarks demonstrate the feasibility for constrained and robust policy synthesis and the competitiveness with state-of-the-art methods for various fragments of the problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08078v2</guid>
      <category>cs.LO</category>
      <category>cs.AI</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linus Heck, Filip Mac\'ak, Milan \v{C}e\v{s}ka, Sebastian Junges</dc:creator>
    </item>
    <item>
      <title>Certified Branch-and-Bound MaxSAT Solving (Extended Version)</title>
      <link>https://arxiv.org/abs/2511.10273</link>
      <description>arXiv:2511.10273v2 Announce Type: replace 
Abstract: Over the past few decades, combinatorial solvers have seen remarkable performance improvements, enabling their practical use in real-world applications. In some of these applications, ensuring the correctness of the solver's output is critical. However, the complexity of modern solvers makes them susceptible to bugs in their source code. In the domain of satisfiability checking (SAT), this issue has been addressed through proof logging, where the solver generates a formal proof of the correctness of its answer. For more expressive problems like MaxSAT, the optimization variant of SAT, proof logging had not seen a comparable breakthrough until recently.
  In this paper, we show how to achieve proof logging for state-of-the-art techniques in Branch-and-Bound MaxSAT solving. This includes certifying look-ahead methods used in such algorithms as well as advanced clausal encodings of pseudo-Boolean constraints based on so-called Multi-Valued Decision Diagrams (MDDs). We implement these ideas in MaxCDCL, the dominant branch-and-bound solver, and experimentally demonstrate that proof logging is feasible with limited overhead, while proof checking remains a challenge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10273v2</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dieter Vandesande, Jordi Coll, Bart Bogaerts</dc:creator>
    </item>
    <item>
      <title>A bialgebraic characterization of symmetric powers in $\mathbb{Q}_{\ge 0}$-linear symmetric monoidal categories</title>
      <link>https://arxiv.org/abs/2308.02094</link>
      <description>arXiv:2308.02094v2 Announce Type: replace-cross 
Abstract: In any symmetric monoidal category, the $n$-th (co)equalizer symmetric power of an object $A$ is the (co)equalizer of all the permutations from $A^{\otimes n}$ to itself. If the symmetric monoidal is $\mathbb{Q}_{\ge 0}$-linear, that is, enriched in $\mathbb{Q}_{\ge 0}$-modules, the notions of $n$-th equalizer symmetric power and $n$-th coequalizer symmetric power are equivalent. In this context, the $n$-th symmetric power of $A$ can be described as the intermediate object $A_n$ in a splitting of the idempotent $\frac{1}{n!}\underset{\sigma \in S_n}{\sum}\sigma\colon A^{\otimes n} \rightarrow A^{\otimes n}$. We define a permutation splitting as a countable family of such splittings.
  The main goal of this paper is to prove two theorems. The first theorem exhibits in any $\mathbb{Q}_{\ge 0}$-linear symmetric monoidal category a bijection between operations making a graded object $(A_n)_{n \ge 0}$ into a permutation splitting and operations making this graded object into a bialgebraic structure that we call a binomial bimonoid. Binomial bimonoids can be defined in any additive symmetric monoidal category. The second theorem shows that, in any $\mathbb{Q}_{\ge 0}$-linear symmetric monoidal category, the biassociativity and bicommutativity axioms may be omitted from the definition of a binomial bimonoid.
  We then show that being a binomial bimonoid in a $\mathbb{Q}_{\ge 0}$-linear symmetric monoidal category is a property: two binomial bimonoids are isomorphic whenever their underlying graded objects are isomorphic. This result does not extend to arbitrary additive symmetric monoidal categories since both the one-variable polynomial algebra and the one-variable divided power polynomial algebra over a field $k$ of positive characteristic are non-isomorphic binomial $k$-bialgebras with isomorphic underlying $\mathbb{N}$-graded vector spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.02094v2</guid>
      <category>math.CT</category>
      <category>cs.LO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean-Baptiste Vienney</dc:creator>
    </item>
    <item>
      <title>From Semantics to Syntax: A Type Theory for Comprehension Categories</title>
      <link>https://arxiv.org/abs/2503.10868</link>
      <description>arXiv:2503.10868v2 Announce Type: replace-cross 
Abstract: Recent models of intensional type theory have been constructed in algebraic weak factorization systems (AWFSs). AWFSs give rise to comprehension categories that feature non-trivial morphisms between types; these morphisms are not used in the standard interpretation of Martin-L\"of type theory in comprehension categories.
  We develop a type theory that internalizes morphisms between types, reflecting this semantic feature back into syntax. Our type theory comes with $\Pi$-, $\Sigma$-, and identity types. We discuss how it can be viewed as an extension of Martin-L\"of type theory with coercive subtyping, as sketched by Coraglia and Emmenegger. We furthermore define semantic structure that interprets our type theory and prove a soundness result. Finally, we exhibit many examples of the semantic structure, yielding a plethora of interpretations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10868v2</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <category>math.CT</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niyousha Najmaei, Niels van der Weide, Benedikt Ahrens, Paige Randall North</dc:creator>
    </item>
    <item>
      <title>Consistency-based Abductive Reasoning over Perceptual Errors of Multiple Pre-trained Models in Novel Environments</title>
      <link>https://arxiv.org/abs/2505.19361</link>
      <description>arXiv:2505.19361v3 Announce Type: replace-cross 
Abstract: The deployment of pre-trained perception models in novel environments often leads to performance degradation due to distributional shifts. Although recent artificial intelligence approaches for metacognition use logical rules to characterize and filter model errors, improving precision often comes at the cost of reduced recall. This paper addresses the hypothesis that leveraging multiple pre-trained models can mitigate this recall reduction. We formulate the challenge of identifying and managing conflicting predictions from various models as a consistency-based abduction problem, building on the idea of abductive learning (ABL) but applying it to test-time instead of training. The input predictions and the learned error detection rules derived from each model are encoded in a logic program. We then seek an abductive explanation--a subset of model predictions--that maximizes prediction coverage while ensuring the rate of logical inconsistencies (derived from domain constraints) remains below a specified threshold. We propose two algorithms for this knowledge representation task: an exact method based on Integer Programming (IP) and an efficient Heuristic Search (HS). Through extensive experiments on a simulated aerial imagery dataset featuring controlled, complex distributional shifts, we demonstrate that our abduction-based framework outperforms individual models and standard ensemble baselines, achieving, for instance, average relative improvements of approximately 13.6\% in F1-score and 16.6\% in accuracy across 15 diverse test datasets when compared to the best individual model. Our results validate the use of consistency-based abduction as an effective mechanism to robustly integrate knowledge from multiple imperfect models in challenging, novel scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19361v3</guid>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.LO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mario Leiva, Noel Ngu, Joshua Shay Kricheli, Aditya Taparia, Ransalu Senanayake, Paulo Shakarian, Nathaniel Bastian, John Corcoran, Gerardo Simari</dc:creator>
    </item>
    <item>
      <title>The Fractal Logic of Phi-adic Recursion</title>
      <link>https://arxiv.org/abs/2510.08934</link>
      <description>arXiv:2510.08934v2 Announce Type: replace-cross 
Abstract: Our central observation is that unbounded additive recurrence establishes a homomorphism between $\mathbb{N}$ and Modus Ponens in a constructive sense. By finding sums of nonconsecutive Fibonacci indices, each inference step corresponds to a geometric constraint whose verification requires $O(M(\log n))$ bit-operations. Logical entailment can be interpreted constructively as arc-closures under $\Phi$-scaling, offering a bridge between additive combinatorics, proof theory, and symbolic computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08934v2</guid>
      <category>math.LO</category>
      <category>cs.LO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Milan Rosko</dc:creator>
    </item>
  </channel>
</rss>
