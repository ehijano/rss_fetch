<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.comp-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.comp-ph</link>
    <description>physics.comp-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.comp-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 07 Apr 2025 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Learning Distributions of Complex Fluid Simulations with Diffusion Graph Networks</title>
      <link>https://arxiv.org/abs/2504.02843</link>
      <description>arXiv:2504.02843v1 Announce Type: new 
Abstract: Physical systems with complex unsteady dynamics, such as fluid flows, are often poorly represented by a single mean solution. For many practical applications, it is crucial to access the full distribution of possible states, from which relevant statistics (e.g., RMS and two-point correlations) can be derived. Here, we propose a graph-based latent diffusion (or alternatively, flow-matching) model that enables direct sampling of states from their equilibrium distribution, given a mesh discretization of the system and its physical parameters. This allows for the efficient computation of flow statistics without running long and expensive numerical simulations. The graph-based structure enables operations on unstructured meshes, which is critical for representing complex geometries with spatially localized high gradients, while latent-space diffusion modeling with a multi-scale GNN allows for efficient learning and inference of entire distributions of solutions. A key finding is that the proposed networks can accurately learn full distributions even when trained on incomplete data from relatively short simulations. We apply this method to a range of fluid dynamics tasks, such as predicting pressure distributions on 3D wing models in turbulent flow, demonstrating both accuracy and computational efficiency in challenging scenarios. The ability to directly sample accurate solutions, and capturing their diversity from short ground-truth simulations, is highly promising for complex scientific modeling tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02843v1</guid>
      <category>physics.comp-ph</category>
      <category>cs.AI</category>
      <category>physics.flu-dyn</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>The Thirteenth International Conference on Learning Representations, April 2025</arxiv:journal_reference>
      <dc:creator>Mario Lino, Tobias Pfaff, Nils Thuerey</dc:creator>
    </item>
    <item>
      <title>Analytical and Neural Network Approaches for Solving Two-Dimensional Nonlinear Transient Heat Conduction</title>
      <link>https://arxiv.org/abs/2504.02845</link>
      <description>arXiv:2504.02845v1 Announce Type: new 
Abstract: Accurately predicting nonlinear transient thermal fields in two-dimensional domains is a significant challenge in various engineering fields, where conventional analytical and numerical methods struggle to balance physical fidelity with computational efficiency when dealing with strong material nonlinearities and evolving multiphysics boundary conditions. To address this challenge, we propose a novel cross-disciplinary approach integrating Green's function formulations with adaptive neural operators, enabling a new paradigm for multiphysics thermal analysis. Our methodology combines rigorous analytical derivations with a physics-informed neural architecture consisting of five adaptive hidden layers (64 neurons per layer) that incorporates solutions as physical constraints, optimizing learning rates to balance convergence stability and computational speed. Extensive validation demonstrates superior performance in handling rapid thermal transients and strongly coupled nonlinear responses, which significantly improves computational efficiency while maintaining high agreement with analytical benchmarks across a range of material configurations and boundary conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02845v1</guid>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ze Tao, Fujun Liu, Jinhua Li, Guibo Chen</dc:creator>
    </item>
    <item>
      <title>Behavior of the scaling correlation functions under severe subsampling</title>
      <link>https://arxiv.org/abs/2504.03203</link>
      <description>arXiv:2504.03203v1 Announce Type: new 
Abstract: Scale-invariance is a ubiquitous observation in the dynamics of large distributed complex systems. The computation of its scaling exponents, which provide clues on its origin, is often hampered by the limited available sampling data, making an appropriate mathematical description a challenge. This work investigates the behavior of correlation functions in fractal systems under conditions of severe subsampling. Analytical and numerical results reveal a striking robustness: the correlation functions continue to capture the expected scaling exponents despite substantial data reduction. This behavior is demonstrated numerically for the random 2-D Cantor set and the Sierpinski gasket, both consistent with exact analytical predictions. Similar robustness is observed in 1-D time series both synthetic and experimental, as well as in high-resolution images of a neuronal structure. Overall, these findings are broadly relevant for the structural characterization of biological systems under realistic sampling constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03203v1</guid>
      <category>physics.comp-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.bio-ph</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sabrina Camargo, Nahuel Zamponi, Daniel A. Martin, Tatyana Turova, Tom\'as S. Grigera, Qian-Yuan Tang, Dante R. Chialvo</dc:creator>
    </item>
    <item>
      <title>Advancing AI-Scientist Understanding: Making LLM Think Like a Physicist with Interpretable Reasoning</title>
      <link>https://arxiv.org/abs/2504.01911</link>
      <description>arXiv:2504.01911v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are playing an expanding role in physics research by enhancing reasoning, symbolic manipulation, and numerical computation. However, ensuring the reliability and interpretability of their outputs remains a significant challenge. In our framework, we conceptualize the collaboration between AI and human scientists as a dynamic interplay among three modules: the reasoning module, the interpretation module, and the AI-scientist interaction module. Recognizing that effective physics reasoning demands rigorous logical consistency, quantitative precision, and deep integration with established theoretical models, we introduce the interpretation module to improve the understanding of AI-generated outputs, which is not previously explored in the literature. This module comprises multiple specialized agents, including summarizers, model builders, UI builders, and testers, which collaboratively structure LLM outputs within a physically grounded framework, by constructing a more interpretable science model. A case study demonstrates that our approach enhances transparency, facilitates validation, and strengthens AI-augmented reasoning in scientific discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01911v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinggan Xu, Hana Kimlee, Yijia Xiao, Di Luo</dc:creator>
    </item>
    <item>
      <title>Transfer learning from first-principles calculations to experiments with chemistry-informed domain transformation</title>
      <link>https://arxiv.org/abs/2504.02848</link>
      <description>arXiv:2504.02848v1 Announce Type: cross 
Abstract: Simulation-to-Real (Sim2Real) transfer learning, the machine learning technique that efficiently solves a real-world task by leveraging knowledge from computational data, has received increasing attention in materials science as a promising solution to the scarcity of experimental data. We proposed an efficient transfer learning scheme from first-principles calculations to experiments based on the chemistry-informed domain transformation, that integrates the heterogeneous source and target domains by harnessing the underlying physics and chemistry. The proposed method maps the computational data from the simulation space (source domain) into the space of experimental data (target domain). During this process, these qualitatively different domains are efficiently bridged by prior knowledge of chemistry, the statistical ensemble and the relationship between source and target quantities. As a proof-of-concept, we predict the catalyst activity for the reverse water-gas shift reaction by using the abundant first-principles data in addition to the experimental data. Through the demonstration, we confirmed that the transfer learning model exhibits positive transfer in accuracy and data efficiency. In particular, a significantly high accuracy was achieved despite using a few (less than ten) target data in domain transformation, whose accuracy is one order of magnitude smaller than that of a full scratch model trained with over 100 target data. This result indicates that the proposed method leverages the high prediction performance with few target data, which helps to save the number of trials in real laboratories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02848v1</guid>
      <category>physics.chem-ph</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuta Yahagi, Kiichi Obuchi, Fumihiko Kosaka, Kota Matsui</dc:creator>
    </item>
    <item>
      <title>Objective Reduction of the Wave Function Demonstrated on Superconducting Quantum Compute</title>
      <link>https://arxiv.org/abs/2504.02914</link>
      <description>arXiv:2504.02914v1 Announce Type: cross 
Abstract: We describe an experiment using superconducting transmon qubits that demonstrates wavefunction collapse consistent with Orchestrated Objective Reduction (Orch-OR) - the theory of consciousness proposed by Sir Roger Penrose and Stuart Hameroff. The experiment performs a partial measurement on a qubit. It uses the result of that measurement to move an estimated 10e-12 kg of mass in one of two locations separated by approximately 1 mm. In standard quantum mechanics, the partial measurement leaves the system in an improper mixture, a state that appears probabilistic but retains quantum coherence. This is mathematically indistinguishable from a proper mixture, where the state has genuinely collapsed. According to Penrose's theory, improper mixtures can lead to gravitationally induced collapse. Results of our experiment show a change in the circuit evolution that is consistent with wavefunction collapse driven by such an improper mixture. The experiment is performed on an IBM Eagle 127-qubit processor using IBM's programming framework Qiskit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02914v1</guid>
      <category>quant-ph</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>James Tagg, William Reid</dc:creator>
    </item>
    <item>
      <title>JanusDDG: A Thermodynamics-Compliant Model for Sequence-Based Protein Stability via Two-Fronts Multi-Head Attention</title>
      <link>https://arxiv.org/abs/2504.03278</link>
      <description>arXiv:2504.03278v1 Announce Type: cross 
Abstract: Understanding how residue variations affect protein stability is crucial for designing functional proteins and deciphering the molecular mechanisms underlying disease-related mutations. Recent advances in protein language models (PLMs) have revolutionized computational protein analysis, enabling, among other things, more accurate predictions of mutational effects. In this work, we introduce JanusDDG, a deep learning framework that leverages PLM-derived embeddings and a bidirectional cross-attention transformer architecture to predict $\Delta \Delta G$ of single and multiple-residue mutations while simultaneously being constrained to respect fundamental thermodynamic properties, such as antisymmetry and transitivity. Unlike conventional self-attention, JanusDDG computes queries (Q) and values (V) as the difference between wild-type and mutant embeddings, while keys (K) alternate between the two. This cross-interleaved attention mechanism enables the model to capture mutation-induced perturbations while preserving essential contextual information. Experimental results show that JanusDDG achieves state-of-the-art performance in predicting $\Delta \Delta G$ from sequence alone, matching or exceeding the accuracy of structure-based methods for both single and multiple mutations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03278v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Guido Barducci, Ivan Rossi, Francesco Codic\`e, Cesare Rollo, Valeria Repetto, Corrado Pancotti, Virginia Iannibelli, Tiziana Sanavia, Piero Fariselli</dc:creator>
    </item>
    <item>
      <title>Effect of Nonlinear Surface Inflows into Activity Belts on Solar Cycle Modulation</title>
      <link>https://arxiv.org/abs/2504.03283</link>
      <description>arXiv:2504.03283v1 Announce Type: cross 
Abstract: Converging flows are visible around bipolar magnetic regions (BMRs) on the solar surface, according to observations. Average flows are created by these inflows combined, and the strength of these flows depends on the amount of flux present during the solar cycle. In models of the solar cycle, this average flow can be depicted as perturbations to the meridional flow. In this article, we study the effects of introducing surface inflow to the surface flux transport models (SFT) as a possible nonlinear mechanism in the presence of latitude quenching for an inflow profile whose amplitude varies within a cycle depending on the magnetic activity. The results show that including surface inflows in the model in the presence of both LQ and tilt quenching (TQ) produced a polar field within a $\pm$1$\sigma$ of an average cycle polar field ($\sigma$ is the standard deviation) and a correlation coefficient of 0.85. We confirm that including inflows produces a lower net contribution to the dipole moment (10\,--\,25\%). Furthermore, the relative importance of LQ vs. inflows is inversely correlated with the dynamo effectivity range ($\lambda_{R}$). With no decay term, introducing inflows into the model resulted in a less significant net contribution to the dipole moment. Including inflows in the SFT model shows a possible nonlinear relationship between the surface inflows and the solar dipole moment, suggesting a potential nonlinear mechanism contributing to the saturation of the global dynamo. For lower $\lambda_R$ ($\lessapprox$ 10 $^\circ$), TQ always dominates LQ, and for higher $\lambda_R$ LQ dominate. However, including inflows will make the domination a little bit earlier in case of having a decay term in the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03283v1</guid>
      <category>astro-ph.SR</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohammed H. Talafha, Krist\'of Petrovay, Andrea Opitz</dc:creator>
    </item>
    <item>
      <title>A silicon spin vacuum: isotopically enriched $^{28}$silicon-on-insulator and $^{28}$silicon from ultra-high fluence ion implantation</title>
      <link>https://arxiv.org/abs/2504.03332</link>
      <description>arXiv:2504.03332v1 Announce Type: cross 
Abstract: Isotopically enriched silicon (Si) can greatly enhance qubit coherence times by minimizing naturally occurring $^{29}$Si which has a non-zero nuclear spin. Ultra-high fluence $^{28}$Si ion implantation of bulk natural Si substrates was recently demonstrated as an attractive technique to ultra-high $^{28}$Si isotopic purity. In this work, we apply this $^{28}$Si enrichment process to produce $^{28}$Si and $^{28}$Si-on-insulator (SOI) samples. Experimentally, we produced a $^{28}$Si sample on natural Si substrate with $^{29}$Si depleted to 7~ppm (limited by measurement noise floor), that is at least 100 nm thick. This is achieved with an ion energy that results in a sputter yield of less than one and a high ion fluence, as supported by simulations. Further, our simulations predict the $^{29}$Si and $^{30}$Si depletion in our sample to be less than 1~ppm. In the case of SOI, ion implantation conditions are found to be more stringent than those of bulk natural Si in terms of minimizing threading dislocations upon subsequent solid phase epitaxy annealing. Finally, we report the observation of nanoscopic voids in our $^{28}$SOI and $^{28}$Si samples located in the depth region between the surface and 70~nm. These voids appear to be stabilized by the presence of gold impurities and can be annealed out completely by a standard SPE annealing protocol at 620\deg C for 10 minutes in the absence of gold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03332v1</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>physics.comp-ph</category>
      <category>quant-ph</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shao Qi Lim, Brett C. Johnson, Sergey Rubanov, Nico Klingner, Bin Gong, Alexander M. Jakob, Danielle Holmes, David N. Jamieson, Jim S. Williams, Jeffrey C. McCallum</dc:creator>
    </item>
    <item>
      <title>Complete design of a fully integrated graphene-based compact plasmon coupler for the infrared</title>
      <link>https://arxiv.org/abs/2504.03403</link>
      <description>arXiv:2504.03403v1 Announce Type: cross 
Abstract: A fully integrated waveguide-based, efficient surface plasmon coupler composed of a realistic non-tapered dielectric waveguide with graphene patches and sheet is designed and optimized for the infrared. The coupling efficiency can reach nearly 80\% for a coupler as short as 700 nm for an operating wavelength of 12 $\mu$m. This \gui{work} is carried out \gui{using rigorous numerical models} based on the finite element method \gui{taking into account 2D-materials as surface conductivities}. \gui{The} key numerical results are \gui{supported by} phdisplaycopyrightysical arguments based on modal approach or resonance condition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03403v1</guid>
      <category>physics.optics</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1364/OL.441681</arxiv:DOI>
      <arxiv:journal_reference>Optics Letters, Vol. 47, No. 8 / 15 April 2022, p. 2004-2007</arxiv:journal_reference>
      <dc:creator>Aswani Natarajan, Guillaume Dem\'esy, Gilles Renversez</dc:creator>
    </item>
    <item>
      <title>NDFT: Accelerating Density Functional Theory Calculations via Hardware/Software Co-Design on Near-Data Computing System</title>
      <link>https://arxiv.org/abs/2504.03451</link>
      <description>arXiv:2504.03451v1 Announce Type: cross 
Abstract: Linear-response time-dependent Density Functional Theory (LR-TDDFT) is a widely used method for accurately predicting the excited-state properties of physical systems. Previous works have attempted to accelerate LR-TDDFT using heterogeneous systems such as GPUs, FPGAs, and the Sunway architecture. However, a major drawback of these approaches is the constant data movement between host memory and the memory of the heterogeneous systems, which results in substantial \textit{data movement overhead}. Moreover, these works focus primarily on optimizing the compute-intensive portions of LR-TDDFT, despite the fact that the calculation steps are fundamentally \textit{memory-bound}.
  To address these challenges, we propose NDFT, a \underline{N}ear-\underline{D}ata Density \underline{F}unctional \underline{T}heory framework. Specifically, we design a novel task partitioning and scheduling mechanism to offload each part of LR-TDDFT to the most suitable computing units within a CPU-NDP system. Additionally, we implement a hardware/software co-optimization of a critical kernel in LR-TDDFT to further enhance performance on the CPU-NDP system. Our results show that NDFT achieves performance improvements of 5.2x and 2.5x over CPU and GPU baselines, respectively, on a large physical system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03451v1</guid>
      <category>cs.AR</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Qingcai Jiang, Buxin Tu, Xiaoyu Hao, Junshi Chen, Hong An</dc:creator>
    </item>
    <item>
      <title>Supersolid phase in two-dimensional soft-core bosons at finite temperature</title>
      <link>https://arxiv.org/abs/2504.03482</link>
      <description>arXiv:2504.03482v1 Announce Type: cross 
Abstract: The supersolid phase of soft-core bosons in two dimensions is investigated using the self-consistent Hartree-Fock and quantum Monte Carlo methods. An approximate phase diagram at finite temperatures is initially constructed using the mean-field approach, which is subsequently validated through precise path-integral simulations, enabling a microscopic characterization of the various phases. Superfluid and melting/freezing transitions are analyzed through the superfluid density and the long-range behavior of correlation functions associated with positional and orientational order, in accordance with the general picture of Berezinskii-Kosterlitz-Thouless transitions. A broad region at low temperatures is identified where the supersolid phase exists, separating the uniform superfluid phase from the normal quasi-crystal phase. Additionally, a potential intermediate hexatic phase with quasi long-range orientational order is identified in a narrow region between the normal solid and fluid phases. These findings establish self-consistent Hartree-Fock theory beyond the local density approximation as an effective tool, complementary to computationally intensive quantum Monte Carlo simulations, for investigating the melting of the supersolid phase and the possible emergence of the hexatic superfluid phase in bosonic systems with various interaction potentials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03482v1</guid>
      <category>cond-mat.quant-gas</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.comp-ph</category>
      <category>quant-ph</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastiano Peotta, Gabriele Spada, Stefano Giorgini, Sebastiano Pilati, Alessio Recati</dc:creator>
    </item>
    <item>
      <title>Conserved discrete unified gas-kinetic scheme with unstructured discrete velocity space</title>
      <link>https://arxiv.org/abs/1907.07109</link>
      <description>arXiv:1907.07109v2 Announce Type: replace 
Abstract: Discrete unified gas-kinetic scheme (DUGKS) is a multi-scale numerical method for flows from continuum limit to free molecular limit, and is especially suitable for the simulation of multi-scale flows, benefiting from its multi-scale property. To reduce integration error of the DUGKS and ensure the conservation property of the collision term in isothermal flow simulations, a Conserved-DUGKS (C-DUGKS) is proposed. On the other hand, both DUGKS and C-DUGKS adopt Cartesian-type discrete velocity space, in which Gaussian and Newton-Cotes numerical quadrature are used for calculating the macroscopic physical variables in low speed and high speed flows, respectively. While, the Cartesian-type discrete velocity space leads to huge computational cost and memory demand. In this paper, the isothermal C-DUGKS is extended to the non-isothermal case by adopting coupled mass and inertial energy distribution functions. Moreover, since the unstructured mesh, such as the triangular mesh in two dimensional case, is more flexible than the structured Cartesian mesh, it is introduced to the discrete velocity space of C-DUGKS, such that more discrete velocity points can be arranged in the velocity regions that enclose large number of molecules, and only a few discrete velocity points need to be arranged in the velocity regions with small amount of molecules in it. By using the unstructured discrete velocity space, the computational efficiency of C-DUGKS is significantly increased. A series of numerical tests in a wide range of Knudsen numbers, such as the Couette flow, lid-driven cavity flow, two-dimensional rarefied Riemann problem and the supersonic cylinder flows, are carried out to examine the validity and efficiency of the present method.</description>
      <guid isPermaLink="false">oai:arXiv.org:1907.07109v2</guid>
      <category>physics.comp-ph</category>
      <category>physics.flu-dyn</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevE.100.043305</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. E 100, 043305 (2019)</arxiv:journal_reference>
      <dc:creator>Jianfeng Chen, Sha Liu, Yong Wang, Chengwen Zhong</dc:creator>
    </item>
    <item>
      <title>Applying Space-Group Symmetry to Speed up Hybrid-Functional Calculations within the Framework of Numerical Atomic Orbitals</title>
      <link>https://arxiv.org/abs/2504.02422</link>
      <description>arXiv:2504.02422v2 Announce Type: replace 
Abstract: Building upon the efficient implementation of hybrid density functionals (HDFs) for large-scale periodic systems within the framework of numerical atomic orbital bases using the localized resolution of identity (RI) technique, we have developed an algorithm that exploits the space group symmetry in key operation steps of HDF calculations, leading to further improvements in two ways. First, the reduction of $\mathbf{k}$-points in the Brillouin zone can reduce the number of Kohn-Sham equations to be solved. This necessitates the correct implementation of the rotation relation between the density matrices of equivalent $\mathbf{k}$-points within the representation of atomic orbitals. Second, the reduction of the real-space sector can accelerate the construction of the exact-exchange part of the Hamiltonian in real space. We have implemented this algorithm in the ABACUS software interfaced with LibRI, and tested its performance for several types of crystal systems with different symmetries. The expected speed-up is achieved in both aspects: the time of solving the Kohn-Sham equations decreases in proportion with the reduction of $\mathbf{k}$-points, while the construction of the Hamiltonian in real space is sped up by several times, with the degree of acceleration depending on the size and symmetry of the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02422v2</guid>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Cao, Min-Ye Zhang, Peize Lin, Mohan Chen, Xinguo Ren</dc:creator>
    </item>
    <item>
      <title>Computing Quantum Resources using Tensor Cross Interpolation</title>
      <link>https://arxiv.org/abs/2502.06956</link>
      <description>arXiv:2502.06956v2 Announce Type: replace-cross 
Abstract: Quantum information quantifiers are indispensable tools for analyzing strongly correlated systems. Consequently, developing efficient and robust numerical methods for their computation is crucial. We propose a general procedure based on the family of Tensor Cross Interpolation (TCI) algorithms to address this challenge in a fully general framework, independent of the system or the quantifier under consideration. To substantiate our approach, we compute the non-stabilizerness R\'{e}nyi entropy (SRE) and Relative Entropy of Coherence (REC) considering the 1D and 2D ferromagnetic Ising models with minimal modifications to the numerical procedure. This method not only demonstrates its versatility, but also provides a generic framework for exploring other quantum information quantifiers in complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06956v2</guid>
      <category>quant-ph</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sven Benjamin Ko\v{z}i\'c, Gianpaolo Torre</dc:creator>
    </item>
    <item>
      <title>Stochastic reduced-order Koopman model for turbulent flows</title>
      <link>https://arxiv.org/abs/2503.22649</link>
      <description>arXiv:2503.22649v2 Announce Type: replace-cross 
Abstract: A stochastic data-driven reduced-order model applicable to a wide range of turbulent natural and engineering flows is presented. Combining ideas from Koopman theory and spectral model order reduction, the stochastic low-dimensional inflated convolutional Koopman model (SLICK) accurately forecasts short-time transient dynamics while preserving long-term statistical properties. A discrete Koopman operator is used to evolve convolutional coordinates that govern the temporal dynamics of spectral orthogonal modes, which in turn represent the energetically most salient large-scale coherent flow structures. Turbulence closure is achieved in two steps: first, by inflating the convolutional coordinates to incorporate nonlinear interactions between different scales, and second, by modeling the residual error as a stochastic source. An empirical dewhitening filter informed by the data is used to maintain the second-order flow statistics within the long-time limit. The model uncertainty is quantified through either Monte Carlo simulation or by directly propagating the model covariance matrix. The model is demonstrated on the Ginzburg-Landau equations, large-eddy simulation (LES) data of a turbulent jet, and particle image velocimetry (PIV) data of the flow over an open cavity. In all cases, the model is predictive over time horizons indicated by a detailed error analysis and integrates stably over arbitrary time horizons, generating realistic surrogate data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22649v2</guid>
      <category>physics.flu-dyn</category>
      <category>nlin.CD</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyi Chu, Oliver T. Schmidt</dc:creator>
    </item>
  </channel>
</rss>
