<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 Jul 2025 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Fully Dynamic Spectral and Cut Sparsifiers for Directed Graphs</title>
      <link>https://arxiv.org/abs/2507.19632</link>
      <description>arXiv:2507.19632v1 Announce Type: new 
Abstract: Recent years have seen extensive research on directed graph sparsification. In this work, we initiate the study of fast fully dynamic spectral and cut sparsification algorithms for directed graphs.
  We introduce a new notion of spectral sparsification called degree-balance preserving spectral approximation, which maintains the difference between the in-degree and out-degree of each vertex. The approximation error is measured with respect to the corresponding undirected Laplacian. This notion is equivalent to direct Eulerian spectral approximation when the input graph is Eulerian. Our algorithm achieves an amortized update time of $O(\varepsilon^{-2} \cdot \text{polylog}(n))$ and produces a sparsifier of size $O(\varepsilon^{-2} n \cdot \text{polylog}(n))$. Additionally, we present an algorithm that maintains a constant-factor approximation sparsifier of size $O(n \cdot \text{polylog}(n))$ against an adaptive adversary for $O(\text{polylog}(n))$-partially symmetrized graphs, a notion introduced in [Kyng-Meierhans-Probst Gutenberg '22]. A $\beta$-partial symmetrization of a directed graph $\vec{G}$ is the union of $\vec{G}$ and $\beta \cdot G$, where $G$ is the corresponding undirected graph of $\vec{G}$. This algorithm also achieves a polylogarithmic amortized update time.
  Moreover, we develop a fully dynamic algorithm for maintaining a cut sparsifier for $\beta$-balanced directed graphs, where the ratio between weighted incoming and outgoing edges of any cut is at most $\beta$. This algorithm explicitly maintains a cut sparsifier of size $O(\varepsilon^{-2}\beta n \cdot \text{polylog}(n))$ in worst-case update time $O(\varepsilon^{-2}\beta \cdot \text{polylog}(n))$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19632v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yibin Zhao</dc:creator>
    </item>
    <item>
      <title>Online Rounding Schemes for $ k $-Rental Problems</title>
      <link>https://arxiv.org/abs/2507.19649</link>
      <description>arXiv:2507.19649v1 Announce Type: new 
Abstract: We study two online resource-allocation problems with reusability in an adversarial setting, namely kRental-Fixed and kRental-Variable. In both problems, a decision-maker manages $k$ identical reusable units and faces a sequence of rental requests over time. We develop theoretically grounded relax-and-round algorithms with provable competitive-ratio guarantees for both settings. For kRental-Fixed, we present an optimal randomized algorithm that attains the best possible competitive ratio: it first computes an optimal fractional allocation via a price-based approach, then applies a novel lossless online rounding scheme to obtain an integral solution. For kRental-Variable, we prove that lossless online rounding is impossible. We introduce a limited-correlation rounding technique that treats each unit independently while introducing controlled dependencies across allocation decisions involving the same unit. Coupled with a carefully crafted price-based method for computing the fractional allocation, this yields an order-optimal competitive ratio for the variable-duration setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19649v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hossein Nekouyan, Bo Sun, Raouf Boutaba, Xiaoqi Tan</dc:creator>
    </item>
    <item>
      <title>Improved 2-Approximate Shortest Paths for close vertex pairs</title>
      <link>https://arxiv.org/abs/2507.19859</link>
      <description>arXiv:2507.19859v1 Announce Type: new 
Abstract: An influential result by Dor, Halperin, and Zwick (FOCS 1996, SICOMP 2000) implies an algorithm that can compute approximate shortest paths for all vertex pairs in $\tilde{O}(n^{2+O\left(\frac{1}{k}\right )})$ time, ensuring that the output distance is at most twice the actual shortest path, provided the pairs are at least $k$ apart, where $k \ge 2$. We present the first improvement on this result in over 25 years. Our algorithm achieves roughly same $\tilde{O}(n^{2+\frac{1}{k}})$ runtime but applies to vertex pairs merely $O(\log k)$ apart, where $\log k \ge 1$. When $k=\log n$, the running time of our algorithm is $\tilde{O}(n^2)$ and it works for all pairs at least $O(\log \log n)$ apart. Our algorithm is combinatorial, randomized, and returns correct results for all pairs with a high probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19859v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manoj Gupta</dc:creator>
    </item>
    <item>
      <title>Generating Satisfiable Benchmark Instances for Stable Roommates Problems with Optimization</title>
      <link>https://arxiv.org/abs/2507.20013</link>
      <description>arXiv:2507.20013v1 Announce Type: new 
Abstract: While the existence of a stable matching for the stable roommates problem possibly with incomplete preference lists (SRI) can be decided in polynomial time, SRI problems with some fairness criteria are intractable. Egalitarian SRI that tries to maximize the total satisfaction of agents if a stable matching exists, is such a hard variant of SRI. For experimental evaluations of methods to solve these hard variants of SRI, several well-known algorithms have been used to randomly generate benchmark instances. However, these benchmark instances are not always satisfiable, and usually have a small number of stable matchings if one exists. For such SRI instances, despite the NP-hardness of Egalitarian SRI, it is practical to find an egalitarian stable matching by enumerating all stable matchings. In this study, we introduce a novel algorithm to generate benchmark instances for SRI that have very large numbers of solutions, and for which it is hard to find an egalitarian stable matching by enumerating all stable matchings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20013v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Baturay Y{\i}lmaz, Esra Erdem</dc:creator>
    </item>
    <item>
      <title>Parallel Hierarchical Agglomerative Clustering in Low Dimensions</title>
      <link>https://arxiv.org/abs/2507.20047</link>
      <description>arXiv:2507.20047v1 Announce Type: new 
Abstract: Hierarchical Agglomerative Clustering (HAC) is an extensively studied and widely used method for hierarchical clustering in $\mathbb{R}^k$ based on repeatedly merging the closest pair of clusters according to an input linkage function $d$. Highly parallel (i.e., NC) algorithms are known for $(1+\epsilon)$-approximate HAC (where near-minimum rather than minimum pairs are merged) for certain linkage functions that monotonically increase as merges are performed. However, no such algorithms are known for many important but non-monotone linkage functions such as centroid and Ward's linkage.
  In this work, we show that a general class of non-monotone linkage functions -- which include centroid and Ward's distance -- admit efficient NC algorithms for $(1+\epsilon)$-approximate HAC in low dimensions. Our algorithms are based on a structural result which may be of independent interest: the height of the hierarchy resulting from any constant-approximate HAC on $n$ points for this class of linkage functions is at most $\operatorname{poly}(\log n)$ as long as $k = O(\log \log n / \log \log \log n)$. Complementing our upper bounds, we show that NC algorithms for HAC with these linkage functions in \emph{arbitrary} dimensions are unlikely to exist by showing that HAC is CC-hard when $d$ is centroid distance and $k = n$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20047v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DC</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>MohammadHossein Bateni, Laxman Dhulipala, Willem Fletcher, Kishen N Gowda, D Ellis Hershkowitz, Rajesh Jayaram, Jakub {\L}\k{a}cki</dc:creator>
    </item>
    <item>
      <title>Adaptive BSTs for Single-Source and All-to-All Requests: Algorithms and Lower Bounds</title>
      <link>https://arxiv.org/abs/2507.20228</link>
      <description>arXiv:2507.20228v1 Announce Type: new 
Abstract: Adaptive binary search trees are a fundamental data structure for organizing hierarchical information. Their ability to dynamically adjust to access patterns makes them particularly valuable for building responsive and efficient networked and distributed systems.
  We present a unified framework for adaptive binary search trees with fixed restructuring cost, analyzed under two models: the single-source model, where the cost of querying a node is proportional to its distance from a fixed source, and the all-to-all model, where the cost of serving a request depends on the distance between the source and destination nodes. We propose an offline algorithm for the single-source model and extend it to the all-to-all model. For both models, we prove upper bounds on the cost incurred by our algorithms. Furthermore, we show the existence of input sequences for which any offline algorithm must incur a cost comparable to ours.
  In the online setting, we develop a general mathematical framework for deterministic online adaptive binary search trees and propose a deterministic online strategy for the single-source case, which naturally extends to the all-to-all model. We also establish lower bounds on the competitive ratio of any deterministic online algorithm, highlighting fundamental limitations of online adaptivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20228v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maryam Shiran</dc:creator>
    </item>
    <item>
      <title>The Min Max Average Cycle Weight Problem</title>
      <link>https://arxiv.org/abs/2507.20253</link>
      <description>arXiv:2507.20253v1 Announce Type: new 
Abstract: When an old apartment building is demolished and rebuilt, how can we fairly redistribute the new apartments to minimize envy among residents? We reduce this question to a combinatorial optimization problem called the *Min Max Average Cycle Weight* problem. In that problem we seek to assign objects to agents in a way that minimizes the maximum average weight of directed cycles in an associated envy graph. While this problem reduces to maximum-weight matching when starting from a clean slate (achieving polynomial-time solvability), we show that this is not the case when we account for preexisting conditions, such as residents' satisfaction with their original apartments. Whether the problem is polynomial-time solvable in the general case remains an intriguing open problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20253v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noga Klein Elmalem, Rica Gonen, Erel Segal-Halevi</dc:creator>
    </item>
    <item>
      <title>Faster exact learning of k-term DNFs with membership and equivalence queries</title>
      <link>https://arxiv.org/abs/2507.20336</link>
      <description>arXiv:2507.20336v1 Announce Type: new 
Abstract: In 1992 Blum and Rudich [BR92] gave an algorithm that uses membership and equivalence queries to learn $k$-term DNF formulas over $\{0,1\}^n$ in time $\textsf{poly}(n,2^k)$, improving on the naive $O(n^k)$ running time that can be achieved without membership queries [Val84]. Since then, many alternative algorithms [Bsh95, Kus97, Bsh97, BBB+00] have been given which also achieve runtime $\textsf{poly}(n,2^k)$.
  We give an algorithm that uses membership and equivalence queries to learn $k$-term DNF formulas in time $\textsf{poly}(n) \cdot 2^{\tilde{O}(\sqrt{k})}$. This is the first improvement for this problem since the original work of Blum and Rudich [BR92].
  Our approach employs the Winnow2 algorithm for learning linear threshold functions over an enhanced feature space which is adaptively constructed using membership queries. It combines a strengthened version of a technique that effectively reduces the length of DNF terms from the original work of [BR92] with a range of additional algorithmic tools (attribute-efficient learning algorithms for low-weight linear threshold functions and techniques for finding relevant variables from junta testing) and analytic ingredients (extremal polynomials and noise operators) that are novel in the context of query-based DNF learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20336v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Josh Alman, Shivam Nadimpalli, Shyamal Patel, Rocco Servedio</dc:creator>
    </item>
    <item>
      <title>Deterministic Almost-Linear-Time Gomory-Hu Trees</title>
      <link>https://arxiv.org/abs/2507.20354</link>
      <description>arXiv:2507.20354v1 Announce Type: new 
Abstract: Given an $m$-edge, undirected, weighted graph $G=(V,E,w)$, a Gomory-Hu tree $T$ (Gomory and Hu, 1961) is a tree over the vertex set $V$ such that all-pairs mincuts in $G$ are preserved exactly in $T$.
  In this article, we give the first almost-optimal $m^{1+o(1)}$-time deterministic algorithm for constructing a Gomory-Hu tree. Prior to our work, the best deterministic algorithm for this problem dated back to the original algorithm of Gomory and Hu that runs in $nm^{1+o(1)}$ time (using current maxflow algorithms). In fact, this is the first almost-linear time deterministic algorithm for even simpler problems, such as finding the $k$-edge-connected components of a graph.
  Our new result hinges on two separate and novel components that each introduce a distinct set of de-randomization tools of independent interest:
  - a deterministic reduction from the all-pairs mincuts problem to the single-souce mincuts problem incurring only subpolynomial overhead, and
  - a deterministic almost-linear time algorithm for the single-source mincuts problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20354v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amir Abboud, Rasmus Kyng, Jason Li, Debmalya Panigrahi, Maximilian Probst Gutenberg, Thatchaphol Saranurak, Weixuan Yuan, Wuwei Yuan</dc:creator>
    </item>
    <item>
      <title>CleANN: Efficient Full Dynamism in Graph-based Approximate Nearest Neighbor Search</title>
      <link>https://arxiv.org/abs/2507.19802</link>
      <description>arXiv:2507.19802v1 Announce Type: cross 
Abstract: Approximate nearest neighbor search (ANNS) has become a quintessential algorithmic problem for various other foundational data tasks for AI workloads. Graph-based ANNS indexes have superb empirical trade-offs in indexing cost, query efficiency, and query approximation quality. Most existing graph-based indexes are designed for the static scenario, where there are no updates to the data after the index is constructed. However, full dynamism (insertions, deletions, and searches) is crucial to providing up-to-date responses in applications using vector databases. It is desirable that the index efficiently supports updates and search queries concurrently. Existing dynamic graph-based indexes suffer from at least one of the following problems: (1) the query quality degrades as updates happen; and (2) the graph structure updates used to maintain the index quality upon updates are global and thus expensive. To solve these problems, we propose the CleANN system which consists of three main components: (1) workload-aware linking of diverse search tree descendants to combat distribution shift; (2)query-adaptive on-the-fly neighborhood consolidation to efficiently handle deleted nodes; and (3) semi-lazy memory cleaning to clean up stale information in the data structure and reduce the work spent by the first two components. We evaluate CleANN on 7 diverse datasets on fully dynamic workloads and find that CleANN has query quality at least as good as if the index had been built statically using the corresponding data. In the in-memory setting using 56 hyper-threads, with all types of queries running concurrently, at the same recall level, CleANN achieves 7-1200x throughput improvement on million-scale real-world datasets. To the best of our knowledge, CleANN is the first concurrent ANNS index to achieve such efficiency while maintaining quality under full dynamism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19802v1</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ziyu Zhang, Yuanhao Wei, Joshua Engels, Julian Shun</dc:creator>
    </item>
    <item>
      <title>An Algorithm-to-Contract Framework without Demand Queries</title>
      <link>https://arxiv.org/abs/2507.20038</link>
      <description>arXiv:2507.20038v1 Announce Type: cross 
Abstract: Consider costly tasks that add up to the success of a project, and must be fitted by an agent into a given time-frame. This is an instance of the classic budgeted maximization problem, which admits an approximation scheme (FPTAS). Now assume the agent is performing these tasks on behalf of a principal, who is the one to reap the rewards if the project succeeds. The principal must design a contract to incentivize the agent. Is there still an approximation scheme? In this work, our ultimate goal is an algorithm-to-contract transformation, which transforms algorithms for combinatorial problems (like budgeted maximization) to tackle incentive constraints that arise in contract design. Our approach diverges from previous works on combinatorial contract design by avoiding an assumption of black-box access to a demand oracle.
  We first show how to "lift" the FPTAS for budgeted maximization to obtain the best-possible multiplicative and additive FPTAS for the contract design problem. We establish this through our "local-global" framework, in which the "local" step is to (approximately) solve a two-sided strengthened variant of the demand problem. The "global" step then utilizes the local one to find the approximately optimal contract. We apply our framework to a host of combinatorial constraints including multi-dimensional budgets, budgeted matroid, and budgeted matching constraints. In all cases we achieve an approximation essentially matching the best approximation for the purely algorithmic problem. We also develop a method to tackle multi-agent contract settings, where the team of working agents must abide to combinatorial feasibility constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20038v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilan Doron-Arad, Hadas Shachnai, Gilad Shmerler, Inbal Talgam-Cohen</dc:creator>
    </item>
    <item>
      <title>MTASet: A Tree-based Set for Efficient Range Queries in Update-heavy Workloads</title>
      <link>https://arxiv.org/abs/2507.20041</link>
      <description>arXiv:2507.20041v1 Announce Type: cross 
Abstract: In concurrent data structures, the efficiency of set operations can vary significantly depending on the workload characteristics. Numerous concurrent set implementations are optimized and fine-tuned to excel in scenarios characterized by predominant read operations. However, they often perform poorly when confronted with workloads that heavily prioritize updates. Additionally, current leading-edge concurrent sets optimized for update-heavy tasks typically lack efficiency in handling atomic range queries. This study introduces the MTASet, which leverages a concurrent (a,b)-tree implementation. Engineered to accommodate update-heavy workloads and facilitate atomic range queries, MTASet surpasses existing counterparts optimized for tasks in range query operations by up to 2x. Notably, MTASet ensures linearizability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20041v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Manor, Mor Perry, Moshe Sulamy</dc:creator>
    </item>
    <item>
      <title>Online Learning with Probing for Sequential User-Centric Selection</title>
      <link>https://arxiv.org/abs/2507.20112</link>
      <description>arXiv:2507.20112v1 Announce Type: cross 
Abstract: We formalize sequential decision-making with information acquisition as the probing-augmented user-centric selection (PUCS) framework, where a learner first probes a subset of arms to obtain side information on resources and rewards, and then assigns $K$ plays to $M$ arms. PUCS covers applications such as ridesharing, wireless scheduling, and content recommendation, in which both resources and payoffs are initially unknown and probing is costly. For the offline setting with known distributions, we present a greedy probing algorithm with a constant-factor approximation guarantee $\zeta = (e-1)/(2e-1)$. For the online setting with unknown distributions, we introduce OLPA, a stochastic combinatorial bandit algorithm that achieves a regret bound $\mathcal{O}(\sqrt{T} + \ln^{2} T)$. We also prove a lower bound $\Omega(\sqrt{T})$, showing that the upper bound is tight up to logarithmic factors. Experiments on real-world data demonstrate the effectiveness of our solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20112v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyi Xu, Yiting Chen, Henger Li, Zheyong Bian, Emiliano Dall'Anese, Zizhan Zheng</dc:creator>
    </item>
    <item>
      <title>TIMEST: Temporal Information Motif Estimator Using Sampling Trees</title>
      <link>https://arxiv.org/abs/2507.20441</link>
      <description>arXiv:2507.20441v1 Announce Type: cross 
Abstract: The mining of pattern subgraphs, known as motifs, is a core task in the field of graph mining. Edges in real-world networks often have timestamps, so there is a need for temporal motif mining. A temporal motif is a richer structure that imposes timing constraints on the edges of the motif. Temporal motifs have been used to analyze social networks, financial transactions, and biological networks.
  Motif counting in temporal graphs is particularly challenging. A graph with millions of edges can have trillions of temporal motifs, since the same edge can occur with multiple timestamps. There is a combinatorial explosion of possibilities, and state-of-the-art algorithms cannot manage motifs with more than four vertices.
  In this work, we present TIMEST: a general, fast, and accurate estimation algorithm to count temporal motifs of arbitrary sizes in temporal networks. Our approach introduces a temporal spanning tree sampler that leverages weighted sampling to generate substructures of target temporal motifs. This method carefully takes a subset of temporal constraints of the motif that can be jointly and efficiently sampled. TIMEST uses randomized estimation techniques to obtain accurate estimates of motif counts.
  We give theoretical guarantees on the running time and approximation guarantees of TIMEST. We perform an extensive experimental evaluation and show that TIMEST is both faster and more accurate than previous algorithms. Our CPU implementation exhibits an average speedup of 28x over state-of-the-art GPU implementation of the exact algorithm, and 6x speedup over SOTA approximate algorithms while consistently showcasing less than 5% error in most cases. For example, TIMEST can count the number of instances of a financial fraud temporal motif in four minutes with 0.6% error, while exact methods take more than two days.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20441v1</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yunjie Pan, Omkar Bhalerao, C. Seshadhri, Nishil Talati</dc:creator>
    </item>
    <item>
      <title>A Tale of Santa Claus, Hypergraphs and Matroids</title>
      <link>https://arxiv.org/abs/1807.07189</link>
      <description>arXiv:1807.07189v3 Announce Type: replace 
Abstract: A well-known problem in scheduling and approximation algorithms is the Santa Claus problem. Suppose that Santa Claus has a set of gifts, and he wants to distribute them among a set of children so that the least happy child is made as happy as possible. Here, the value that a child $i$ has for a present $j$ is of the form $p_{ij} \in \{ 0,p_j\}$. A polynomial time algorithm by Annamalai et al. gives a $12.33$-approximation and is based on a modification of Haxell's hypergraph matching argument.
  In this paper, we introduce a matroid version of the Santa Claus problem. Our algorithm is also based on Haxell's augmenting tree, but with the introduction of the matroid structure we solve a more general problem with cleaner methods. Our result can then be used as a blackbox to obtain a $(6+\varepsilon)$-approximation for Santa Claus. This factor also compares against a natural, compact LP for Santa Claus.</description>
      <guid isPermaLink="false">oai:arXiv.org:1807.07189v3</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sami Davies, Thomas Rothvoss, Yihao Zhang</dc:creator>
    </item>
    <item>
      <title>Searching in trees with k-up-modular weight functions</title>
      <link>https://arxiv.org/abs/2504.17887</link>
      <description>arXiv:2504.17887v2 Announce Type: replace 
Abstract: We consider the following generalization of the binary search problem: A searcher is required to find a hidden element $x$ in a tree $T$. To do so, they iteratively perform queries to an oracle about a chosen vertex $v$. After each such call, the oracle responds whether the target was found and if not, the searcher receives as a reply the neighbor of $v$ that lays on the shortest path towards $x$. Additionally, each vertex $v$ may have a different query cost $w(v)$. The goal is to find the optimal querying strategy for the searcher which minimizes the worst case query cost required to find $x$. The problem is known to be NP-hard even in restricted classes of trees such as bounded diameter spiders [Cicalese et al. 2016] and no constant factor approximation algorithm is known for the general case. Inspired by recent studies [Dereniowski et al. 2022, Dereniowski et al. 2024], instead of restricted classes of trees, we explore restrictions on the weight function. We introduce the concept of a heavy group set of a vertex $HG(v,w)$. We show that if for every $v\in T$: $|HG\br{v,w}|\leq k$ an $O(\log\log n)$-approximation can be found within $2^{O(\log^2k)}\cdot\text{poly}(n)$ time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17887v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Micha{\l} Szyfelbein</dc:creator>
    </item>
    <item>
      <title>PHast -- Perfect Hashing made fast</title>
      <link>https://arxiv.org/abs/2504.17918</link>
      <description>arXiv:2504.17918v4 Announce Type: replace 
Abstract: Perfect hash functions give unique "names" to arbitrary keys requiring only a few bits per key. This is an essential building block in applications like static hash tables, databases, or bioinformatics. This paper introduces the PHast approach that combines the fastest available queries, very fast construction, and good space consumption (below 2 bits per key). PHast improves bucket-placement which first hashes each key k to a bucket, and then looks for the bucket seed s such that a placement function maps pairs (s,k) in a collision-free way. PHast can use small-range hash functions with linear mapping, fixed-width encoding of seeds, and parallel construction. This is achieved using small overlapping slices of allowed values and bumping to handle unsuccessful seed assignment. A variant we called PHast+ uses additive placement, which enables bit-parallel seed searching, speeding up the construction by an order of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17918v4</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <category>cs.PF</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Piotr Beling, Peter Sanders</dc:creator>
    </item>
    <item>
      <title>RLZ-r and LZ-End-r: Enhancing Move-r</title>
      <link>https://arxiv.org/abs/2507.17300</link>
      <description>arXiv:2507.17300v2 Announce Type: replace 
Abstract: In pattern matching on strings, a locate query asks for an enumeration of all the occurrences of a given pattern in a given text. The r-index [Gagie et al., 2018] is a recently presented compressed self index that stores the text and auxiliary information in compressed space. With some modifications, locate queries can be answered in optimal time [Nishimoto &amp; Tabei, 2021], which has recently been proven relevant in practice in the form of Move-r [Bertram et al., 2024]. However, there remains the practical bottleneck of evaluating function $\Phi$ for every occurrence to report. This motivates enhancing the index by a compressed representation of the suffix array featuring efficient random access, trading off space for faster answering of locate queries [Puglisi &amp; Zhukova, 2021]. In this work, we build upon this idea considering two suitable compression schemes: Relative Lempel-Ziv [Kuruppu et al., 2010], improving the work by Puglisi and Zhukova, and LZ-End [Kreft &amp; Navarro, 2010], introducing a different trade-off where compression is better than for Relative Lempel-Ziv at the cost of slower access times. We enhance both the r-index and Move-r by the compressed suffix arrays and evaluate locate query performance in an experiment. We show that locate queries can be sped up considerably in both the r-index and Move-r, especially if the queried pattern has many occurrences. The choice between two different compression schemes offers new trade-offs regarding index size versus query performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17300v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Dinklage, Johannes Fischer, Lukas Nalbach, Jan Zumbrink</dc:creator>
    </item>
    <item>
      <title>String Consensus Problems with Swaps and Substitutions</title>
      <link>https://arxiv.org/abs/2507.19139</link>
      <description>arXiv:2507.19139v2 Announce Type: replace 
Abstract: String consensus problems aim at finding a string that minimizes some given distance with respect to an input set of strings. In particular, in the Closest string problem, we are given a set of strings of equal length and a radius $d$. The objective is to find a new string that differs from each input string by at most $d$ substitutions. We study a generalization of this problem where, in addition to substitutions, swaps of adjacent characters are also permitted, each operation incurring a unit cost. Amir et al. showed that this generalized problem is NP-hard, even when only swaps are allowed. In this paper, we show that it is FPT with respect to the parameter $d$. Moreover, we investigate a variant in which the goal is to minimize the sum of distances from the output string to all input strings. For this version, we present a polynomial-time algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19139v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Est\'eban Gabory, Laurent Bulteau, Gabriele Fici, Hilde Verbeek</dc:creator>
    </item>
    <item>
      <title>Parallel Point-to-Point Shortest Paths and Batch Queries</title>
      <link>https://arxiv.org/abs/2506.16488</link>
      <description>arXiv:2506.16488v2 Announce Type: replace-cross 
Abstract: We propose Orionet, efficient parallel implementations of Point-to-Point Shortest Paths (PPSP) queries using bidirectional search (BiDS) and other heuristics, with an additional focus on batch PPSP queries. We present a framework for parallel PPSP built on existing single-source shortest paths (SSSP) frameworks by incorporating pruning conditions. As a result, we develop efficient parallel PPSP algorithms based on early termination, bidirectional search, A$^*$ search, and bidirectional A$^*$ all with simple and efficient implementations.
  We extend our idea to batch PPSP queries, which are widely used in real-world scenarios. We first design a simple and flexible abstraction to represent the batch so PPSP can leverage the shared information of the batch. Orionet formalizes the batch as a query graph represented by edges between queried sources and targets. In this way, we directly extended our PPSP framework to batched queries in a simple and efficient way.
  We evaluate Orionet on both single and batch PPSP queries using various graph types and distance percentiles of queried pairs, and compare it against two baselines, GraphIt and MBQ. Both of them support parallel single PPSP and A$^*$ using unidirectional search. On 14 graphs we tested, on average, our bidirectional search is 2.9$\times$ faster than GraphIt, and 6.8$\times$ faster than MBQ. Our bidirectional A$^*$ is 4.4$\times$ and 6.2$\times$ faster than the A$^*$ in GraphIt and MBQ, respectively. For batched PPSP queries, we also provide in-depth experimental evaluation, and show that Orionet provides strong performance compared to the plain solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16488v2</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3694906.3743311</arxiv:DOI>
      <dc:creator>Xiaojun Dong, Andy Li, Yan Gu, Yihan Sun</dc:creator>
    </item>
  </channel>
</rss>
