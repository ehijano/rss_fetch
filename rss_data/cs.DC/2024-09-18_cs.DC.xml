<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Sep 2024 04:00:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Study of Performance Programming of CPU, GPU accelerated Computers and SIMD Architecture</title>
      <link>https://arxiv.org/abs/2409.10661</link>
      <description>arXiv:2409.10661v1 Announce Type: new 
Abstract: Parallel computing is a standard approach to achieving high-performance computing (HPC). Three commonly used methods to implement parallel computing include: 1) applying multithreading technology on single-core or multi-core CPUs; 2) incorporating powerful parallel computing devices such as GPUs, FPGAs, and other accelerators; and 3) utilizing special parallel architectures like Single Instruction/Multiple Data (SIMD).
  Many researchers have made efforts using different parallel technologies, including developing applications, conducting performance analyses, identifying performance bottlenecks, and proposing feasible solutions. However, balancing and optimizing parallel programs remain challenging due to the complexity of parallel algorithms and hardware architectures. Issues such as data transfer between hosts and devices in heterogeneous systems continue to be bottlenecks that limit performance.
  This work summarizes a vast amount of information on various parallel programming techniques, aiming to present the current state and future development trends of parallel programming, performance issues, and solutions. It seeks to give readers an overall picture and provide background knowledge to support subsequent research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10661v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyao Yi</dc:creator>
    </item>
    <item>
      <title>Deterministic Bounds in Committee Selection: Enhancing Decentralization and Scalability in Distributed Ledgers</title>
      <link>https://arxiv.org/abs/2409.10727</link>
      <description>arXiv:2409.10727v1 Announce Type: new 
Abstract: Consensus plays a crucial role in distributed ledger systems, impacting both scalability and decentralization. Many blockchain systems use a weighted lottery based on a scarce resource such as a stake, storage, memory, or computing power to select a committee whose members drive the consensus and are responsible for adding new information to the ledger. Therefore, ensuring a robust and fair committee selection process is essential for maintaining security, efficiency, and decentralization.
  There are two main approaches to randomized committee selection. In one approach, each validator candidate locally checks whether they are elected to the committee and reveals their proof during the consensus phase. In contrast, in the second approach, a sortition algorithm decides a fixed-sized committee that is globally verified. This paper focuses on the latter approach, with cryptographic sortition as a method for fair committee selection that guarantees a constant committee size. Our goal is to develop deterministic guarantees that strengthen decentralization. We introduce novel methods that provide deterministic bounds on the influence of adversaries within the committee, as evidenced by numerical experiments. This approach overcomes the limitations of existing protocols that only offer probabilistic guarantees, often providing large committees that are impractical for many quorum-based applications like atomic broadcast and randomness beacon protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10727v1</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Grigorii Melnikov, Sebastian M\"uller, Nikita Polyanskii, Yury Yanovich</dc:creator>
    </item>
    <item>
      <title>Advances in ArborX to support exascale applications</title>
      <link>https://arxiv.org/abs/2409.10743</link>
      <description>arXiv:2409.10743v1 Announce Type: new 
Abstract: ArborX is a performance portable geometric search library developed as part of the Exascale Computing Project (ECP). In this paper, we explore a collaboration between ArborX and a cosmological simulation code HACC. Large cosmological simulations on exascale platforms encounter a bottleneck due to the in-situ analysis requirements of halo finding, a problem of identifying dense clusters of dark matter (halos). This problem is solved by using a density-based DBSCAN clustering algorithm. With each MPI rank handling hundreds of millions of particles, it is imperative for the DBSCAN implementation to be efficient. In addition, the requirement to support exascale supercomputers from different vendors necessitates performance portability of the algorithm. We describe how this challenge problem guided ArborX development, and enhanced the performance and the scope of the library. We explore the improvements in the basic algorithms for the underlying search index to improve the performance, and describe several implementations of DBSCAN in ArborX. Further, we report the history of the changes in ArborX and their effect on the time to solve a representative benchmark problem, as well as demonstrate the real world impact on production end-to-end cosmology simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10743v1</guid>
      <category>cs.DC</category>
      <category>astro-ph.CO</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrey Prokopenko, Daniel Arndt, Damien Lebrun-Grandi\'e, Bruno Turcksin, Nicholas Frontiere, J. D. Emberson, Michael Buehlmann</dc:creator>
    </item>
    <item>
      <title>HPC with Enhanced User Separation</title>
      <link>https://arxiv.org/abs/2409.10770</link>
      <description>arXiv:2409.10770v1 Announce Type: new 
Abstract: HPC systems used for research run a wide variety of software and workflows. This software is often written or modified by users to meet the needs of their research projects, and rarely is built with security in mind. In this paper we explore several of the key techniques that MIT Lincoln Laboratory Supercomputing Center has deployed on its systems to manage the security implications of these workflows by providing enforced separation for processes, filesystem access, network traffic, and accelerators to make every user feel like they are running on a personal HPC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10770v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Prout, Albert Reuther, Michael Houle, Michael Jones, Peter Michaleas, LaToya Anderson, William Arcand, Bill Bergeron, David Bestor, Alex Bonn, Daniel Burrill, Chansup Byun, Vijay Gadepally, Matthew Hubbell, Hayden Jananthan, Piotr Luszczek, Lauren Milechin, Guillermo Morales, Julie Mullen, Antonio Rosa, Charles Yee, Jeremy Kepner</dc:creator>
    </item>
    <item>
      <title>Ladon: High-Performance Multi-BFT Consensus via Dynamic Global Ordering (Extended Version)</title>
      <link>https://arxiv.org/abs/2409.10954</link>
      <description>arXiv:2409.10954v1 Announce Type: new 
Abstract: Multi-BFT consensus runs multiple leader-based consensus instances in parallel, circumventing the leader bottleneck of a single instance. However, it contains an Achilles' heel: the need to globally order output blocks across instances. Deriving this global ordering is challenging because it must cope with different rates at which blocks are produced by instances. Prior Multi-BFT designs assign each block a global index before creation, leading to poor performance.
  We propose Ladon, a high-performance Multi-BFT protocol that allows varying instance block rates. Our key idea is to order blocks across instances dynamically, which eliminates blocking on slow instances. We achieve dynamic global ordering by assigning monotonic ranks to blocks. We pipeline rank coordination with the consensus process to reduce protocol overhead and combine aggregate signatures with rank information to reduce message complexity. Ladon's dynamic ordering enables blocks to be globally ordered according to their generation, which respects inter-block causality. We implemented and evaluated Ladon by integrating it with both PBFT and HotStuff protocols. Our evaluation shows that Ladon-PBFT (resp., Ladon-HotStuff) improves the peak throughput of the prior art by $\approx$8x (resp., 2x) and reduces latency by $\approx$62% (resp., 23%), when deployed with one straggling replica (out of 128 replicas) in a WAN setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10954v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3689031.3696102</arxiv:DOI>
      <dc:creator>Hanzheng Lyu, Shaokang Xie, Jianyu Niu, Chen Feng, Yinqian Zhang, Ivan Beschastnikh</dc:creator>
    </item>
    <item>
      <title>Delay Analysis of EIP-4844</title>
      <link>https://arxiv.org/abs/2409.11043</link>
      <description>arXiv:2409.11043v1 Announce Type: new 
Abstract: Proto-Danksharding, proposed in Ethereum Improvement Proposal 4844 (EIP-4844), aims to incrementally improve the scalability of the Ethereum blockchain by introducing a new type of transaction known as blob-carrying transactions. These transactions incorporate binary large objects (blobs) of data that are stored off-chain but referenced and verified on-chain to ensure data availability. By decoupling data availability from transaction execution, Proto-Danksharding alleviates network congestion and reduces gas fees, laying the groundwork for future, more advanced sharding solutions. This letter provides an analytical model to derive the delay for these new transactions. We model the system as an $\mathrm{M/D}^B/1$ queue which we then find its steady state distribution through embedding a Markov chain and use of supplementary variable method. We show that transactions with more blobs but less frequent impose higher delays on the system compared to lower blobs but more frequent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11043v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pourya Soltani, Farid Ashtiani</dc:creator>
    </item>
    <item>
      <title>ISO: Overlap of Computation and Communication within Seqenence For LLM Inference</title>
      <link>https://arxiv.org/abs/2409.11155</link>
      <description>arXiv:2409.11155v1 Announce Type: new 
Abstract: In the realm of Large Language Model (LLM) inference, the inherent structure of transformer models coupled with the multi-GPU tensor parallelism strategy leads to a sequential execution of computation and communication. This results in substantial underutilization of computing resources during the communication phase. To mitigate this inefficiency, various techniques have been developed to optimize the use of computational power throughout the communication process. These strategies primarily involve overlapping matrix computations and communications, as well as interleaving micro-batches across different requests. Nonetheless, these approaches either fall short of achieving ideal overlap or impose certain limitations on their application. To overcome these challenges, this paper introduces a novel strategy for computation-communication overlap that operates at the sequence level. This method not only enhances the degree of overlap but also minimizes the constraints on its applicability. Experimental evaluations conducted using 30b/70b models have demonstrated significant improvements in efficiency. Specifically, the proposed technique has been shown to reduce time consumption by approximately 35% on 4090 GPU and by roughly 15% on A800 GPU during the prefill stage of LLM inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11155v1</guid>
      <category>cs.DC</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bin Xiao, Lei Su</dc:creator>
    </item>
    <item>
      <title>Energy Efficiency Support for Software Defined Networks: a Serverless Computing Approach</title>
      <link>https://arxiv.org/abs/2409.11208</link>
      <description>arXiv:2409.11208v1 Announce Type: new 
Abstract: Automatic network management strategies have become paramount for meeting the needs of innovative real-time and data-intensive applications, such as in the Internet of Things. However, meeting the ever-growing and fluctuating demands for data and services in such applications requires more than ever an efficient and scalable network resource management approach. Such approach should enable the automated provisioning of services while incentivising energy-efficient resource usage that expands throughout the edge-to-cloud continuum. This paper is the first to realise the concept of modular Software-Defined Networks based on serverless functions in an energy-aware environment. By adopting Function as a Service, the approach enables on-demand deployment of network functions, resulting in cost reduction through fine resource provisioning granularity. An analytical model is presented to approximate the service delivery time and power consumption, as well as an open-source prototype implementation supported by an extensive experimental evaluation. The experiments demonstrate not only the practical applicability of the proposed approach but significant improvement in terms of energy efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11208v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fatemeh Banaie, Karim Djemame, Abdulaziz Alhindi, Vasilios Kelefouras</dc:creator>
    </item>
    <item>
      <title>Communication Lower Bounds and Optimal Algorithms for Symmetric Matrix Computations</title>
      <link>https://arxiv.org/abs/2409.11304</link>
      <description>arXiv:2409.11304v1 Announce Type: new 
Abstract: In this article, we focus on the communication costs of three symmetric matrix computations: i) multiplying a matrix with its transpose, known as a symmetric rank-k update (SYRK) ii) adding the result of the multiplication of a matrix with the transpose of another matrix and the transpose of that result, known as a symmetric rank-2k update (SYR2K) iii) performing matrix multiplication with a symmetric input matrix (SYMM). All three computations appear in the Level 3 Basic Linear Algebra Subroutines (BLAS) and have wide use in applications involving symmetric matrices. We establish communication lower bounds for these kernels using sequential and distributed-memory parallel computational models, and we show that our bounds are tight by presenting communication-optimal algorithms for each setting. Our lower bound proofs rely on applying a geometric inequality for symmetric computations and analytically solving constrained nonlinear optimization problems. The symmetric matrix and its corresponding computations are accessed and performed according to a triangular block partitioning scheme in the optimal algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11304v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hussam Al Daas (STFC, Scientific Computing Department, Rutherford Appleton Laboratory, Didcot, UK), Grey Ballard (Wake Forest University, Computer Science Department, Winston-Salem, NC, USA), Laura Grigori (EPFL, Institute of Mathematics, Lausanne, Switzerland,PSI, Center for Scientific Computing, Theory,Data, Villigen, Switzerland), Suraj Kumar (Institut national de recherche en sciences et technologies du num\'erique, Lyon, France), Kathryn Rouse (Inmar Intelligence, Winston-Salem, NC, USA), Mathieu Verite (EPFL, Institute of Mathematics, Lausanne, Switzerland)</dc:creator>
    </item>
    <item>
      <title>Dynamic DAG-Application Scheduling for Multi-Tier Edge Computing in Heterogeneous Networks</title>
      <link>https://arxiv.org/abs/2409.10839</link>
      <description>arXiv:2409.10839v1 Announce Type: cross 
Abstract: Edge computing is deemed a promising technique to execute latency-sensitive applications by offloading computation-intensive tasks to edge servers. Extensive research has been conducted in the field of end-device to edge server task offloading for several goals, including latency minimization, energy optimization, and resource optimization. However, few of them consider our mobile computing devices (smartphones, tablets, and laptops) to be edge devices. In this paper, we propose a novel multi-tier edge computing framework, which we refer to as M-TEC, that aims to optimize latency, reduce the probability of failure, and optimize cost while accounting for the sporadic failure of personally owned devices and the changing network conditions. We conduct experiments with a real testbed and a real commercial CBRS 4G network, and the results indicate that M-TEC is capable of reducing the end-to-end latency of applications by at least 8\% compared to the best baseline under a variety of network conditions, while providing reliable performance at an affordable cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10839v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiang Li, Mustafa Abdallah, Yuan-Yao Lou, Mung Chiang, Kwang Taik Kim, Saurabh Bagchi</dc:creator>
    </item>
    <item>
      <title>Skip TLB flushes for reused pages within mmap's</title>
      <link>https://arxiv.org/abs/2409.10946</link>
      <description>arXiv:2409.10946v1 Announce Type: cross 
Abstract: Memory access efficiency is significantly enhanced by caching recent address translations in the CPUs' Translation Lookaside Buffers (TLBs). However, since the operating system is not aware of which core is using a particular mapping, it flushes TLB entries across all cores where the application runs whenever addresses are unmapped, ensuring security and consistency. These TLB flushes, known as TLB shootdowns, are costly and create a performance and scalability bottleneck. A key contributor to TLB shootdowns is memory-mapped I/O, particularly during mmap-munmap cycles and page cache evictions. Often, the same physical pages are reassigned to the same process post-eviction, presenting an opportunity for the operating system to reduce the frequency of TLB shootdowns. We demonstrate, that by slightly extending the mmap function, TLB shootdowns for these "recycled pages" can be avoided.
  Therefore we introduce and implement the "fast page recycling" (FPR) feature within the mmap system call. FPR-mmaps maintain security by only triggering TLB shootdowns when a page exits its recycling cycle and is allocated to a different process. To ensure consistency when FPR-mmap pointers are used, we made minor adjustments to virtual memory management to avoid the ABA problem. Unlike previous methods to mitigate shootdown effects, our approach does not require any hardware modifications and operates transparently within the existing Linux virtual memory framework.
  Our evaluations across a variety of CPU, memory, and storage setups, including persistent memory and Optane SSDs, demonstrate that FPR delivers notable performance gains, with improvements of up to 28% in real-world applications and 92% in micro-benchmarks. Additionally, we show that TLB shootdowns are a significant source of bottlenecks, previously misattributed to other components of the Linux kernel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10946v1</guid>
      <category>cs.OS</category>
      <category>cs.DC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Frederic Schimmelpfennig, Andr\'e Brinkmann, Hossein Asadi, Reza Salkhordeh</dc:creator>
    </item>
    <item>
      <title>A Reinforcement Learning Environment for Automatic Code Optimization in the MLIR Compiler</title>
      <link>https://arxiv.org/abs/2409.11068</link>
      <description>arXiv:2409.11068v1 Announce Type: cross 
Abstract: Code optimization is a crucial task aimed at enhancing code performance. However, this process is often tedious and complex, highlighting the necessity for automatic code optimization techniques. Reinforcement Learning (RL), a machine learning technique, has emerged as a promising approach for tackling such complex optimization problems. In this project, we introduce the first RL environment for the MLIR compiler, dedicated to facilitating MLIR compiler research, and enabling automatic code optimization using Multi-Action Reinforcement Learning. We also propose a novel formulation of the action space as a Cartesian product of simpler action subspaces, enabling more efficient and effective optimizations. Experimental results demonstrate that our proposed environment allows for an effective optimization of MLIR operations, and yields comparable performance to TensorFlow, surpassing it in multiple cases, highlighting the potential of RL-based optimization in compiler frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11068v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.SE</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nazim Bendib, Iheb Nassim Aouadj, Riyadh Baghdadi</dc:creator>
    </item>
    <item>
      <title>Federated Learning with Integrated Sensing, Communication, and Computation: Frameworks and Performance Analysis</title>
      <link>https://arxiv.org/abs/2409.11240</link>
      <description>arXiv:2409.11240v1 Announce Type: cross 
Abstract: With the emergence of integrated sensing, communication, and computation (ISCC) in the upcoming 6G era, federated learning with ISCC (FL-ISCC), integrating sample collection, local training, and parameter exchange and aggregation, has garnered increasing interest for enhancing training efficiency. Currently, FL-ISCC primarily includes two algorithms: FedAVG-ISCC and FedSGD-ISCC. However, the theoretical understanding of the performance and advantages of these algorithms remains limited. To address this gap, we investigate a general FL-ISCC framework, implementing both FedAVG-ISCC and FedSGD-ISCC. We experimentally demonstrate the substantial potential of the ISCC framework in reducing latency and energy consumption in FL. Furthermore, we provide a theoretical analysis and comparison. The results reveal that:1) Both sample collection and communication errors negatively impact algorithm performance, highlighting the need for careful design to optimize FL-ISCC applications. 2) FedAVG-ISCC performs better than FedSGD-ISCC under IID data due to its advantage with multiple local updates. 3) FedSGD-ISCC is more robust than FedAVG-ISCC under non-IID data, where the multiple local updates in FedAVG-ISCC worsen performance as non-IID data increases. FedSGD-ISCC maintains performance levels similar to IID conditions. 4) FedSGD-ISCC is more resilient to communication errors than FedAVG-ISCC, which suffers from significant performance degradation as communication errors increase.Extensive simulations confirm the effectiveness of the FL-ISCC framework and validate our theoretical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11240v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yipeng Liang, Qimei Chen, Hao Jiang</dc:creator>
    </item>
    <item>
      <title>Temporal Load Imbalance on Ondes3D Seismic Simulator for Different Multicore Architectures</title>
      <link>https://arxiv.org/abs/2409.11392</link>
      <description>arXiv:2409.11392v1 Announce Type: cross 
Abstract: The variety of today's multicore architectures motivates researchers to explore parallel scientific applications on different platforms. Load imbalance is one performance issue that can prejudice parallel applications from exploiting the computational power of these platforms. Ondes3D is a scientific application for seismic wave simulation used to assess the geological impact of earthquakes. Its parallelism relies on applying a regular domain decomposition in the geological domain provided and distributing each sub-domain to MPI ranks. Previous works investigate the significant spatial and temporal imbalance in Ondes3D and suggest new parallelization and load balancing techniques to minimize them. However, none explored its execution on different architectures. Our paper evaluates the performance of Ondes3D for two earthquake scenarios on eight different multicore architectures, including Intel, AMD, and ARM processors. We measure the load distribution per MPI rank, evaluate the temporal load imbalance, and compare the execution of the application's kernels. Our results show that the temporal load imbalance in Ondes3D depends on the architecture chosen, with some platforms minimizing such imbalance more effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11392v1</guid>
      <category>cs.PF</category>
      <category>cs.DC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ana Luisa Veroneze Sol\'orzano, Philippe Olivier Alexandre Navaux, Lucas Mello Schnorr</dc:creator>
    </item>
    <item>
      <title>ACC Saturator: Automatic Kernel Optimization for Directive-Based GPU Code</title>
      <link>https://arxiv.org/abs/2306.13002</link>
      <description>arXiv:2306.13002v4 Announce Type: replace 
Abstract: Automatic code optimization is a complex process that typically involves the application of multiple discrete algorithms that modify the program structure irreversibly. However, the design of these algorithms is often monolithic, and they require repetitive implementation to perform similar analyses due to the lack of cooperation. To address this issue, modern optimization techniques, such as equality saturation, allow for exhaustive term rewriting at various levels of inputs, thereby simplifying compiler design.
  In this paper, we propose equality saturation to optimize sequential codes utilized in directive-based programming for GPUs. Our approach realizes less computation, less memory access, and high memory throughput simultaneously. Our fully-automated framework constructs single-assignment forms from inputs to be entirely rewritten while keeping dependencies and extracts optimal cases. Through practical benchmarks, we demonstrate a significant performance improvement on several compilers. Furthermore, we highlight the advantages of computational reordering and emphasize the significance of memory-access order for modern GPUs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.13002v4</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazuaki Matsumura, Simon Garcia De Gonzalo, Antonio J. Pe\~na</dc:creator>
    </item>
    <item>
      <title>Context Adaptive Cooperation</title>
      <link>https://arxiv.org/abs/2311.08776</link>
      <description>arXiv:2311.08776v2 Announce Type: replace 
Abstract: As shown by Reliable Broadcast and Consensus, cooperation among a set of independent computing entities (sequential processes) is a central issue in distributed computing. Considering $n$-process asynchronous message-passing systems where some processes can be Byzantine, this paper introduces a new cooperation abstraction denoted Context-Adaptive Cooperation (CAC). While Reliable Broadcast is a one-to-$n$ cooperation abstraction and Consensus is an $n$-to-$n$ cooperation abstraction, CAC is a $d$-to-$n$ cooperation abstraction where the parameter $d$ ($1\leq d\leq n$) depends on the run and remains unknown to the processes. Moreover, the correct processes accept the same set of $\ell$ pairs $\langle v,i\rangle$ ($v$ is the value proposed by $p_i$) from the $d$ proposer processes, where $1 \leq \ell \leq d$ and, as $d$, $\ell$ remains unknown to the processes (except in specific cases). Those $\ell$ values are accepted one at a time in different orders at each process. Furthermore, CAC provides the processes with an imperfect oracle that gives information about the values that they may accept in the future. In a very interesting way, the CAC abstraction is particularly efficient in favorable circumstances. To illustrate its practical use, the paper describes in detail two applications that benefit from the abstraction: a fast consensus implementation under low contention (named Cascading Consensus), and a novel naming problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.08776v2</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Timoth\'e Albouy, Davide Frey, Mathieu Gestin, Michel Raynal, Fran\c{c}ois Ta\"iani</dc:creator>
    </item>
    <item>
      <title>SABLE: Staging Blocked Evaluation of Sparse Matrix Computations</title>
      <link>https://arxiv.org/abs/2407.00829</link>
      <description>arXiv:2407.00829v2 Announce Type: replace 
Abstract: Sparse Matrices found in the real world often have some structure in their distribution of dense elements. While existing techniques specialize the generated code for the structure of matrices, their generality misses optimization opportunities. We propose a system that -- if the sparse matrix is stored in a blocked storage format -- can adapt its code generation strategy depending on the structure of the sparse matrix. Our system SABLE performs a specified computation over every element of {\em mostly} dense blocks instead of avoiding computing any sparse element and achieving regularity in generated code while having special treatment for hyper-sparse blocks (ie, blocks with very few dense elements). SABLE is extensible, providing a block iterator for users to express any computation over these non-empty blocks. We demonstrate that our approach can significantly accelerate SpMV and SpMM operations, surpassing the performance of state-of-the-art systems like Partially-Strided Codelets and Sparse Register Tiling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00829v2</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pratyush Das, Adhitha Dias, Anxhelo Xhebraj, Artem Pelenitsyn, Kirshanthan Sundararajah, Milind Kulkarni</dc:creator>
    </item>
  </channel>
</rss>
