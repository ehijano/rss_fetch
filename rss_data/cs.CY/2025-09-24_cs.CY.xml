<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Sep 2025 01:43:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Learning Progression-Guided AI Evaluation of Scientific Models To Support Diverse Multi-Modal Understanding in NGSS Classroom</title>
      <link>https://arxiv.org/abs/2509.18157</link>
      <description>arXiv:2509.18157v1 Announce Type: new 
Abstract: Learning Progressions (LPs) can help adjust instruction to individual learners needs if the LPs reflect diverse ways of thinking about a construct being measured, and if the LP-aligned assessments meaningfully measure this diversity. The process of doing science is inherently multi-modal with scientists utilizing drawings, writing and other modalities to explain phenomena. Thus, fostering deep science understanding requires supporting students in using multiple modalities when explaining phenomena. We build on a validated NGSS-aligned multi-modal LP reflecting diverse ways of modeling and explaining electrostatic phenomena and associated assessments. We focus on students modeling, an essential practice for building a deep science understanding. Supporting culturally and linguistically diverse students in building modeling skills provides them with an alternative mode of communicating their understanding, essential for equitable science assessment. Machine learning (ML) has been used to score open-ended modeling tasks (e.g., drawings), and short text-based constructed scientific explanations, both of which are time- consuming to score. We use ML to evaluate LP-aligned scientific models and the accompanying short text-based explanations reflecting multi-modal understanding of electrical interactions in high school Physical Science. We show how LP guides the design of personalized ML-driven feedback grounded in the diversity of student thinking on both assessment modes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18157v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonora Kaldaras, Tingting Li, Prudence Djagba, Kevin Haudek, Joseph Krajcik</dc:creator>
    </item>
    <item>
      <title>Deleuze's "Postscript on the Societies of Control" Updated for Big Data and Predictive Analytics</title>
      <link>https://arxiv.org/abs/2509.18194</link>
      <description>arXiv:2509.18194v1 Announce Type: new 
Abstract: In 1990, Gilles Deleuze published Postscript on the Societies of Control, an introduction to the potentially suffocating reality of the nascent control society. This thirty-year update details how Deleuze's conception has developed from a broad speculative vision into specific economic mechanisms clustering around personal information, big data, predictive analytics, and marketing. The central claim is that today's advancing control society coerces without prohibitions, and through incentives that are not grim but enjoyable, even euphoric because they compel individuals to obey their own personal information. The article concludes by delineating two strategies for living that are as unexplored as control society itself because they are revealed and then enabled by the particular method of oppression that is control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18194v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.3167/th.2020.6716401</arxiv:DOI>
      <arxiv:journal_reference>Theoria: A Journal of Social and Political Theory, Vol. 67, No. 3 (164) (September 2020), pp. 1-25 (25 pages) https://www.jstor.org/stable/48590393</arxiv:journal_reference>
      <dc:creator>James Brusseau (Philosophy,Computer Science, Pace University NYC,University of Trento, Italy)</dc:creator>
    </item>
    <item>
      <title>Algorithmic A-Legality: Shorting the Human Future through AI</title>
      <link>https://arxiv.org/abs/2509.18195</link>
      <description>arXiv:2509.18195v1 Announce Type: new 
Abstract: This article provides a necessary corrective to the belief that current legal and political concepts and institutions are capable of holding to account the power of new AI technologies. Drawing on jurisprudential analysis, it argues that while the current development of AI is dependent on the combination of economic and legal power, the technological forms that result increasingly exceed the capacity of even the most rigorous legal and political regimes. A situation of "a-legality" is emerging whereby the potential of AI to produce harms cannot be restrained by conventional legal or political institutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18195v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Scott Veitch</dc:creator>
    </item>
    <item>
      <title>Dark and Bright Patterns in Cookie Consent Requests</title>
      <link>https://arxiv.org/abs/2509.18210</link>
      <description>arXiv:2509.18210v1 Announce Type: new 
Abstract: Dark patterns are (evil) design nudges that steer people's behaviour through persuasive interface design. Increasingly found in cookie consent requests, they possibly undermine principles of EU privacy law. In two preregistered online experiments we investigated the effects of three common design nudges (default, aesthetic manipulation, obstruction) on users' consent decisions and their perception of control over their personal data in these situations. In the first experiment (N = 228) we explored the effects of design nudges towards the privacy-unfriendly option (dark patterns). The experiment revealed that most participants agreed to all consent requests regardless of dark design nudges. Unexpectedly, despite generally low levels of perceived control, obstructing the privacy-friendly option led to more rather than less perceived control. In the second experiment (N = 255) we reversed the direction of the design nudges towards the privacy-friendly option, which we title "bright patterns". This time the obstruction and default nudges swayed people effectively towards the privacy-friendly option, while the result regarding perceived control stayed the same compared to Experiment 1. Overall, our findings suggest that many current implementations of cookie consent requests do not enable meaningful choices by internet users, and are thus not in line with the intention of the EU policymakers. We also explore how policymakers could address the problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18210v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.33621/jdsr.v3i1.54</arxiv:DOI>
      <arxiv:journal_reference>Journal Of Digital Social Research, 3(1), 1-38, 2021</arxiv:journal_reference>
      <dc:creator>Paul Gra{\ss}l, Hanna Schraffenberger, Frederik Zuiderveen Borgesius, Moniek Buijzen</dc:creator>
    </item>
    <item>
      <title>Microtargeted propaganda by foreign actors: An interdisciplinary exploration</title>
      <link>https://arxiv.org/abs/2509.18211</link>
      <description>arXiv:2509.18211v1 Announce Type: new 
Abstract: This article discusses a problem that has received scant attention in literature: microtargeted propaganda by foreign actors. Microtargeting involves collecting information about people, and using that information to show them targeted political advertisements. Such microtargeting enables advertisers to target ads to specific groups of people, for instance people who visit certain websites, forums, or Facebook groups. This article focuses on one type of microtargeting: microtargeting by foreign actors. For example, Russia has targeted certain groups in the US with ads, aiming to sow discord. Foreign actors could also try to influence European elections, for instance by advertising in favour of a certain political party. Foreign propaganda possibilities existed before microtargeting. This article explores two questions. In what ways, if any, is microtargeted propaganda by foreign actors different from other foreign propaganda? What could lawmakers in Europe do to mitigate the risks of microtargeted propaganda?</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18211v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1177/1023263X211042471</arxiv:DOI>
      <arxiv:journal_reference>Maastricht Journal of European and Comparative Law 2021, Vol. 28(6) 856-877</arxiv:journal_reference>
      <dc:creator>Ronan \'O Fathaigh, Tom Dobber, Frederik Zuiderveen Borgesius, James Shires</dc:creator>
    </item>
    <item>
      <title>Personalised Pricing: The Demise of the Fixed Price?</title>
      <link>https://arxiv.org/abs/2509.18212</link>
      <description>arXiv:2509.18212v1 Announce Type: new 
Abstract: An online seller or platform is technically able to offer every consumer a different price for the same product, based on information it has about the customers. Such online price discrimination exacerbates concerns regarding the fairness and morality of price discrimination, and the possible need for regulation. In this chapter, we discuss the underlying basis of price discrimination in economic theory, and its popular perception. Our surveys show that consumers are critical and suspicious of online price discrimination. A majority consider it unacceptable and unfair, and are in favour of a ban. When stores apply online price discrimination, most consumers think they should be informed about it. We argue that the General Data Protection Regulation (GDPR) applies to the most controversial forms of online price discrimination, and not only requires companies to disclose their use of price discrimination, but also requires companies to ask customers for their prior consent. Industry practice, however, does not show any adoption of these two principles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18212v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Chapter 10 in: Kohl, U., &amp; Eisler, J. (Eds.). (2021). Data-Driven Personalisation in Markets, Politics and Law. Cambridge: Cambridge University Press</arxiv:journal_reference>
      <dc:creator>Joost Poort, Frederik Zuiderveen Borgesius</dc:creator>
    </item>
    <item>
      <title>Enhanced Interpretable Knowledge Tracing for Students Performance Prediction with Human understandable Feature Space</title>
      <link>https://arxiv.org/abs/2509.18231</link>
      <description>arXiv:2509.18231v1 Announce Type: new 
Abstract: Knowledge Tracing (KT) plays a central role in assessing students skill mastery and predicting their future performance. While deep learning based KT models achieve superior predictive accuracy compared to traditional methods, their complexity and opacity hinder their ability to provide psychologically meaningful explanations. This disconnect between model parameters and cognitive theory poses challenges for understanding and enhancing the learning process, limiting their trustworthiness in educational applications. To address these challenges, we enhance interpretable KT models by exploring human-understandable features derived from students interaction data. By incorporating additional features, particularly those reflecting students learning abilities, our enhanced approach improves predictive accuracy while maintaining alignment with cognitive theory. Our contributions aim to balance predictive power with interpretability, advancing the utility of adaptive learning systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18231v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sein Minn, Roger Nkambou</dc:creator>
    </item>
    <item>
      <title>Perceptions of AI Across Sectors: A Comparative Review of Public Attitudes</title>
      <link>https://arxiv.org/abs/2509.18233</link>
      <description>arXiv:2509.18233v1 Announce Type: new 
Abstract: This paper offers a domain-mediated comparative review of 251 studies on public attitudes toward AI, published between 2011 and 2025. Drawing on a systematic literature review, we analyse how different factors including perceived benefits and concerns (or risks) shape public acceptance of - or resistance to - artificial intelligence across domains and use-cases, including healthcare, education, security, public administration, generative AI, and autonomous vehicles. The analysis highlights recurring patterns in individual, contextual, and technical factors influencing perception, while also tracing variations in institutional trust, perceived fairness, and ethical concerns. We show that the public perception in AI is shaped not only by technical design or performance but also by sector-specific considerations as well as imaginaries, cultural narratives, and historical legacies. This comparative approach offers a foundation for developing more tailored and context-sensitive strategies for responsible AI governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18233v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Filip Bialy, Mark Elliot, Robert Meckin</dc:creator>
    </item>
    <item>
      <title>An Artificial Intelligence Value at Risk Approach: Metrics and Models</title>
      <link>https://arxiv.org/abs/2509.18394</link>
      <description>arXiv:2509.18394v1 Announce Type: new 
Abstract: Artificial intelligence risks are multidimensional in nature, as the same risk scenarios may have legal, operational, and financial risk dimensions. With the emergence of new AI regulations, the state of the art of artificial intelligence risk management seems to be highly immature due to upcoming AI regulations. Despite the appearance of several methodologies and generic criteria, it is rare to find guidelines with real implementation value, considering that the most important issue is customizing artificial intelligence risk metrics and risk models for specific AI risk scenarios. Furthermore, the financial departments, legal departments and Government Risk Compliance teams seem to remain unaware of many technical aspects of AI systems, in which data scientists and AI engineers emerge as the most appropriate implementers. It is crucial to decompose the problem of artificial intelligence risk in several dimensions: data protection, fairness, accuracy, robustness, and information security. Consequently, the main task is developing adequate metrics and risk models that manage to reduce uncertainty for decision-making in order to take informed decisions concerning the risk management of AI systems.
  The purpose of this paper is to orientate AI stakeholders about the depths of AI risk management. Although it is not extremely technical, it requires a basic knowledge of risk management, quantifying uncertainty, the FAIR model, machine learning, large language models and AI context engineering. The examples presented pretend to be very basic and understandable, providing simple ideas that can be developed regarding specific AI customized environments. There are many issues to solve in AI risk management, and this paper will present a holistic overview of the inter-dependencies of AI risks, and how to model them together, within risk scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18394v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>q-fin.RM</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luis Enriquez Alvarez</dc:creator>
    </item>
    <item>
      <title>Large-Scale, Longitudinal Study of Large Language Models During the 2024 US Election Season</title>
      <link>https://arxiv.org/abs/2509.18446</link>
      <description>arXiv:2509.18446v1 Announce Type: new 
Abstract: The 2024 US presidential election is the first major contest to occur in the US since the popularization of large language models (LLMs). Building on lessons from earlier shifts in media (most notably social media's well studied role in targeted messaging and political polarization) this moment raises urgent questions about how LLMs may shape the information ecosystem and influence political discourse. While platforms have announced some election safeguards, how well they work in practice remains unclear. Against this backdrop, we conduct a large-scale, longitudinal study of 12 models, queried using a structured survey with over 12,000 questions on a near-daily cadence from July through November 2024. Our design systematically varies content and format, resulting in a rich dataset that enables analyses of the models' behavior over time (e.g., across model updates), sensitivity to steering, responsiveness to instructions, and election-related knowledge and "beliefs." In the latter half of our work, we perform four analyses of the dataset that (i) study the longitudinal variation of model behavior during election season, (ii) illustrate the sensitivity of election-related responses to demographic steering, (iii) interrogate the models' beliefs about candidates' attributes, and (iv) reveal the models' implicit predictions of the election outcome. To facilitate future evaluations of LLMs in electoral contexts, we detail our methodology, from question generation to the querying pipeline and third-party tooling. We also publicly release our dataset at https://huggingface.co/datasets/sarahcen/llm-election-data-2024</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18446v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarah H. Cen, Andrew Ilyas, Hedi Driss, Charlotte Park, Aspen Hopkins, Chara Podimata, Aleksander M\k{a}dry</dc:creator>
    </item>
    <item>
      <title>Developing a Decolonial Mindset for Indigenising Computing Education (CE)</title>
      <link>https://arxiv.org/abs/2509.18509</link>
      <description>arXiv:2509.18509v1 Announce Type: new 
Abstract: The underrepresentation of First Peoples in computing education reflects colonial legacies embedded in curricula, pedagogies, and digital infrastructures. This paper introduces the \textbf{Decolonial Mindset Stack (DMS)}, a seven-layer framework for educator transformation: \textbf{Recognition, Reflection, Reframing, Reembedding, Reciprocity, Reclamation}, and \textbf{Resurgence}. Grounded in Freirean critical pedagogy and Indigenous methodologies, the DMS aligns with relational lenses of ``About Me,'' ``Between Us,'' and ``By Us.'' It fosters self-reflexivity, relational accountability, and Indigenous sovereignty in computing education, reframing underrepresentation as systemic exclusion. The DMS provides both theoretical grounding and pathways for practice, positioning indigenisation not as an endpoint but as a sustained ethical commitment to transformative justice and the co-creation of computing education with First Peoples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18509v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianhua Li, Yin Paradies, Trina Myers, Robin Doss, Armita Zarnegar, Jack Reis</dc:creator>
    </item>
    <item>
      <title>Automatic coherence-driven inference on arguments</title>
      <link>https://arxiv.org/abs/2509.18523</link>
      <description>arXiv:2509.18523v1 Announce Type: new 
Abstract: Inconsistencies are ubiquitous in law, administration, and jurisprudence. Though a cure is too much to hope for, we propose a technological remedy. Large language models (LLMs) can accurately extract propositions from arguments and compile them into natural data structures that enable coherence-driven inference (CDI) via combinatorial optimization. This neurosymbolic architecture naturally separates concerns and enables meaningful judgments about the coherence of arguments that can inform legislative and policy analysis and legal reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18523v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Steve Huntsman</dc:creator>
    </item>
    <item>
      <title>Judging Data: Critical Discourse and the Rise of Data Intellectual Property Rights in Chinese Courts</title>
      <link>https://arxiv.org/abs/2509.18605</link>
      <description>arXiv:2509.18605v1 Announce Type: new 
Abstract: This paper uses Critical Discourse Analysis (CDA) to show how Sino-judicial activism shapes Data Intellectual Property Rights (DIPR) in China. We identify two complementary judicial discourses. Local courts (exemplified by the Zhejiang High People's Court, HCZJ) use a judicial continuation discourse that extends intellectual property norms to data disputes. The Supreme People's Court (SPC) deploys a judicial linkage discourse that aligns adjudication with state policy and administrative governance. Their interaction forms a bidirectional conceptual coupling (BCC): an inside-out projection of local reasoning and an outside-in translation of policy into doctrine. The coupling both legitimizes and constrains courts and policymakers, balancing pressure for unified market standards with safeguards against platform monopolization. Through cases such as HCZJ's Taobao v. Meijing and the SPC's Anti-Unfair Competition Interpretation, the study presents DIPR as a testbed for doctrinal innovation and institutional coordination in China's evolving digital governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18605v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1515/ijdlg-2025-0009</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Digital Law and Governance, 2025</arxiv:journal_reference>
      <dc:creator>Chanhou Lou</dc:creator>
    </item>
    <item>
      <title>Purer than pure: how purity reshapes the upstream materiality of the semiconductor industry</title>
      <link>https://arxiv.org/abs/2509.18768</link>
      <description>arXiv:2509.18768v1 Announce Type: new 
Abstract: Growing attention is given to the environmental impacts of the digital sector, exacerbated by the increase of digital products and services in our globalized societies. The materiality of the digital sector is often presented through the environmental impacts of mining activities to point out that digitization does not mean dematerialization. Despite its importance, such a narrative is often restricted to a few minerals (e.g., cobalt, lithium) that have become the symbols of extractive industries. In this paper, we further explore the materiality of the digital sector with an approach based on the diversity of elements and their purity requirements in the semiconductor industry. Semiconductors are responsible for manufacturing the key building blocks of the digital sector, i.e., microchips. Given that the need for ultra-high purity materials is very specific to the semiconductor industry, a few companies around the world have been studied, revealing new critical actors in complex supply chains. This highlights strong dependencies towards other industrial sectors with mass production and the need for a deeper investigation of interactions with the chemical industry, complementary to the mining industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18768v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gauthier Roussilhe, Thibault Pirson, David Bol, Srinjoy Mitra</dc:creator>
    </item>
    <item>
      <title>The AI Literacy Heptagon: A Structured Approach to AI Literacy in Higher Education</title>
      <link>https://arxiv.org/abs/2509.18900</link>
      <description>arXiv:2509.18900v1 Announce Type: new 
Abstract: The integrative literature review addresses the conceptualization and implementation of AI Literacy (AIL) in Higher Education (HE) by examining recent research literature. Through an analysis of publications (2021-2024), we explore (1) how AIL is defined and conceptualized in current research, particularly in HE, and how it can be delineated from related concepts such as Data Literacy, Media Literacy, and Computational Literacy; (2) how various definitions can be synthesized into a comprehensive working definition, and (3) how scientific insights can be effectively translated into educational practice. Our analysis identifies seven central dimensions of AIL: technical, applicational, critical thinking, ethical, social, integrational, and legal. These are synthesized in the AI Literacy Heptagon, deepening conceptual understanding and supporting the structured development of AIL in HE. The study aims to bridge the gap between theoretical AIL conceptualizations and the practical implementation in academic curricula.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18900v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Veronika Hackl, Alexandra Mueller, Maximilian Sailer</dc:creator>
    </item>
    <item>
      <title>A Mega-Study of Digital Twins Reveals Strengths, Weaknesses and Opportunities for Further Improvement</title>
      <link>https://arxiv.org/abs/2509.19088</link>
      <description>arXiv:2509.19088v1 Announce Type: new 
Abstract: Do "digital twins" capture individual responses in surveys and experiments? We run 19 pre-registered studies on a national U.S. panel and their LLM-powered digital twins (constructed based on previously-collected extensive individual-level data) and compare twin and human answers across 164 outcomes. The correlation between twin and human answers is modest (approximately 0.2 on average) and twin responses are less variable than human responses. While constructing digital twins based on rich individual-level data improves our ability to capture heterogeneity across participants and predict relative differences between them, it does not substantially improve our ability to predict the exact answers given by specific participants or enhance predictions of population means. Twin performance varies by domain and is higher among more educated, higher-income, and ideologically moderate participants. These results suggest current digital twins can capture some degree of relative differences but are unreliable for individual-level predictions and sample mean and variance estimation, underscoring the need for careful validation before use. Our data and code are publicly available for researchers and practitioners interested in optimizing digital twin pipelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19088v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tiany Peng, George Gui, Daniel J. Merlau, Grace Jiarui Fan, Malek Ben Sliman, Melanie Brucks, Eric J. Johnson, Vicki Morwitz, Abdullah Althenayyan, Silvia Bellezza, Dante Donati, Hortense Fong, Elizabeth Friedman, Ariana Guevara, Mohamed Hussein, Kinshuk Jerath, Bruce Kogut, Kristen Lane, Hannah Li, Patryk Perkowski, Oded Netzer, Olivier Toubia</dc:creator>
    </item>
    <item>
      <title>Generative Propaganda</title>
      <link>https://arxiv.org/abs/2509.19147</link>
      <description>arXiv:2509.19147v1 Announce Type: new 
Abstract: Generative propaganda is the use of generative artificial intelligence (AI) to shape public opinion. To characterize its use in real-world settings, we conducted interviews with defenders (e.g., factcheckers, journalists, officials) in Taiwan and creators (e.g., influencers, political consultants, advertisers) as well as defenders in India, centering two places characterized by high levels of online propaganda. The term "deepfakes", we find, exerts outsized discursive power in shaping defenders' expectations of misuse and, in turn, the interventions that are prioritized. To better characterize the space of generative propaganda, we develop a taxonomy that distinguishes between obvious versus hidden and promotional versus derogatory use. Deception was neither the main driver nor the main impact vector of AI's use; instead, Indian creators sought to persuade rather than to deceive, often making AI's use obvious in order to reduce legal and reputational risks, while Taiwan's defenders saw deception as a subset of broader efforts to distort the prevalence of strategic narratives online. AI was useful and used, however, in producing efficiency gains in communicating across languages and modes, and in evading human and algorithmic detection. Security researchers should reconsider threat models to clearly differentiate deepfakes from promotional and obvious uses, to complement and bolster the social factors that constrain misuse by internal actors, and to counter efficiency gains globally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19147v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Madeleine I. G. Daepp, Alejandro Cuevas, Robert Osazuwa Ness, Vickie Yu-Ping Wang, Bharat Kumar Nayak, Dibyendu Mishra, Ti-Chung Cheng, Shaily Desai, Joyojeet Pal</dc:creator>
    </item>
    <item>
      <title>Introducing a novel Location-Assignment Algorithm for Activity-Based Transport Models: CARLA</title>
      <link>https://arxiv.org/abs/2509.18191</link>
      <description>arXiv:2509.18191v1 Announce Type: cross 
Abstract: This paper introduces CARLA (spatially Constrained Anchor-based Recursive Location Assignment), a recursive algorithm for assigning secondary or any activity locations in activity-based travel models. CARLA minimizes distance deviations while integrating location potentials, ensuring more realistic activity distributions. The algorithm decomposes trip chains into smaller subsegments, using geometric constraints and configurable heuristics to efficiently search the solution space. Compared to a state-of-the-art relaxation-discretization approach, CARLA achieves significantly lower mean deviations, even under limited runtimes. It is robust to real-world data inconsistencies, such as infeasible distances, and can flexibly adapt to various priorities, such as emphasizing location attractiveness or distance accuracy. CARLA's versatility and efficiency make it a valuable tool for improving the spatial accuracy of activity-based travel models and agent-based transport simulations. Our implementation is available at https://github.com/tnoud/carla.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18191v1</guid>
      <category>cs.OH</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <category>math.OC</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Felix Petre, Lasse Bienzeisler, Bernhard Friedrich</dc:creator>
    </item>
    <item>
      <title>Evaluating Large Language Models for Detecting Antisemitism</title>
      <link>https://arxiv.org/abs/2509.18293</link>
      <description>arXiv:2509.18293v1 Announce Type: cross 
Abstract: Detecting hateful content is a challenging and important problem. Automated tools, like machine-learning models, can help, but they require continuous training to adapt to the ever-changing landscape of social media. In this work, we evaluate eight open-source LLMs' capability to detect antisemitic content, specifically leveraging in-context definition as a policy guideline. We explore various prompting techniques and design a new CoT-like prompt, Guided-CoT. Guided-CoT handles the in-context policy well, increasing performance across all evaluated models, regardless of decoding configuration, model sizes, or reasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5. Additionally, we examine LLM errors and introduce metrics to quantify semantic divergence in model-generated rationales, revealing notable differences and paradoxical behaviors among LLMs. Our experiments highlight the differences observed across LLMs' utility, explainability, and reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18293v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jay Patel, Hrudayangam Mehta, Jeremy Blackburn</dc:creator>
    </item>
    <item>
      <title>Identifying Constructive Conflict in Online Discussions through Controversial yet Toxicity Resilient Posts</title>
      <link>https://arxiv.org/abs/2509.18303</link>
      <description>arXiv:2509.18303v1 Announce Type: cross 
Abstract: Bridging content that brings together individuals with opposing viewpoints on social media remains elusive, overshadowed by echo chambers and toxic exchanges. We propose that algorithmic curation could surface such content by considering constructive conflicts as a foundational criterion. We operationalize this criterion through controversiality to identify challenging dialogues and toxicity resilience to capture respectful conversations. We develop high-accuracy models to capture these dimensions. Analyses based on these models demonstrate that assessing resilience to toxic responses is not the same as identifying low-toxicity posts. We also find that political posts are often controversial and tend to attract more toxic responses. However, some posts, even the political ones, are resilient to toxicity despite being highly controversial, potentially sparking civil engagement. Toxicity resilient posts tend to use politeness cues, such as showing gratitude and hedging. These findings suggest the potential for framing the tone of posts to encourage constructive political discussions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18303v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ozgur Can Seckin, Bao Tran Truong, Alessandro Flammini, Filippo Menczer</dc:creator>
    </item>
    <item>
      <title>Fair Decisions through Plurality: Results from a Crowdfunding Platform</title>
      <link>https://arxiv.org/abs/2509.18343</link>
      <description>arXiv:2509.18343v1 Announce Type: cross 
Abstract: We discuss an algorithmic intervention aimed at increasing equity and economic efficiency at a crowdfunding platform that gives cash subsidies to grantees. Through a blend of technical and qualitative methods, we show that the previous algorithm used by the platform -- Quadratic Funding (QF) -- suffered problems because its design was rooted in a model of individuals as isolated and selfish. We present an alternative algorithm -- Connection-Oriented Quadratic Funding (CO-QF) -- rooted in a theory of plurality and prosocial utilities, and show that it qualitatively and quantitatively performs better than QF. CO-QF has achieved an 89% adoption rate at the platform and has distributed over $4 Million to date. In simulations we show that it provides better social welfare than QF. While our design for CO-QF was responsive to the needs of a specific community, we also extrapolate out of this context to show that CO-QF is a potentially helpful tool for general-purpose public decision making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18343v1</guid>
      <category>cs.GT</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3757887.3763019</arxiv:DOI>
      <dc:creator>Joel Miller, E. Glen Weyl, Chris Kanich</dc:creator>
    </item>
    <item>
      <title>When Ads Become Profiles: Large-Scale Audit of Algorithmic Biases and LLM Profiling Risks</title>
      <link>https://arxiv.org/abs/2509.18874</link>
      <description>arXiv:2509.18874v1 Announce Type: cross 
Abstract: Automated ad targeting on social media is opaque, creating risks of exploitation and invisibility to external scrutiny. Users may be steered toward harmful content while independent auditing of these processes remains blocked. Large Language Models (LLMs) raise a new concern: the potential to reverse-engineer sensitive user attributes from exposure alone. We introduce a multi-stage auditing framework to investigate these risks. First, a large-scale audit of over 435,000 ad impressions delivered to 891 Australian Facebook users reveals algorithmic biases, including disproportionate Gambling and Politics ads shown to socioeconomically vulnerable and politically aligned groups. Second, a multimodal LLM can reconstruct users' demographic profiles from ad streams, outperforming census-based baselines and matching or exceeding human performance. Our results provide the first empirical evidence that ad streams constitute rich digital footprints for public AI inference, highlighting urgent privacy risks and the need for content-level auditing and governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18874v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Baiyu Chen, Benjamin Tag, Hao Xue, Daniel Angus, Flora Salim</dc:creator>
    </item>
    <item>
      <title>Poster: The Internet Quality Barometer Framework</title>
      <link>https://arxiv.org/abs/2509.19034</link>
      <description>arXiv:2509.19034v1 Announce Type: cross 
Abstract: In this paper, we introduce the Internet Quality Barometer (IQB), a framework aiming to redefine Internet quality beyond ``speed''. IQB (i) defines Internet quality in a user-centric way by considering popular use cases, (ii) maps network requirements to use cases through a set of weights and quality thresholds, and (iii) leverages publicly available Internet performance datasets, to calculate the IQB score, a composite metric that reflects the quality of Internet experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19034v1</guid>
      <category>cs.NI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lai Yi Ohlsen, Pavlos Sermpezis, Melissa Newcomb</dc:creator>
    </item>
    <item>
      <title>Anecdoctoring: Automated Red-Teaming Across Language and Place</title>
      <link>https://arxiv.org/abs/2509.19143</link>
      <description>arXiv:2509.19143v1 Announce Type: cross 
Abstract: Disinformation is among the top risks of generative artificial intelligence (AI) misuse. Global adoption of generative AI necessitates red-teaming evaluations (i.e., systematic adversarial probing) that are robust across diverse languages and cultures, but red-teaming datasets are commonly US- and English-centric. To address this gap, we propose "anecdoctoring", a novel red-teaming approach that automatically generates adversarial prompts across languages and cultures. We collect misinformation claims from fact-checking websites in three languages (English, Spanish, and Hindi) and two geographies (US and India). We then cluster individual claims into broader narratives and characterize the resulting clusters with knowledge graphs, with which we augment an attacker LLM. Our method produces higher attack success rates and offers interpretability benefits relative to few-shot prompting. Results underscore the need for disinformation mitigations that scale globally and are grounded in real-world adversarial misuse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19143v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Cuevas, Saloni Dash, Bharat Kumar Nayak, Dan Vann, Madeleine I. G. Daepp</dc:creator>
    </item>
    <item>
      <title>A decentralized future for the open-science databases</title>
      <link>https://arxiv.org/abs/2509.19206</link>
      <description>arXiv:2509.19206v1 Announce Type: cross 
Abstract: Continuous and reliable access to curated biological data repositories is indispensable for accelerating rigorous scientific inquiry and fostering reproducible research. Centralized repositories, though widely used, are vulnerable to single points of failure arising from cyberattacks, technical faults, natural disasters, or funding and political uncertainties. This can lead to widespread data unavailability, data loss, integrity compromises, and substantial delays in critical research, ultimately impeding scientific progress. Centralizing essential scientific resources in a single geopolitical or institutional hub is inherently dangerous, as any disruption can paralyze diverse ongoing research. The rapid acceleration of data generation, combined with an increasingly volatile global landscape, necessitates a critical re-evaluation of the sustainability of centralized models. Implementing federated and decentralized architectures presents a compelling and future-oriented pathway to substantially strengthen the resilience of scientific data infrastructures, thereby mitigating vulnerabilities and ensuring the long-term integrity of data. Here, we examine the structural limitations of centralized repositories, evaluate federated and decentralized models, and propose a hybrid framework for resilient, FAIR, and sustainable scientific data stewardship. Such an approach offers a significant reduction in exposure to governance instability, infrastructural fragility, and funding volatility, and also fosters fairness and global accessibility. The future of open science depends on integrating these complementary approaches to establish a globally distributed, economically sustainable, and institutionally robust infrastructure that safeguards scientific data as a public good, further ensuring continued accessibility, interoperability, and preservation for generations to come.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19206v1</guid>
      <category>cs.DB</category>
      <category>cs.AR</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>q-bio.OT</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Gaurav Sharma, Viorel Munteanu, Nika Mansouri Ghiasi, Jineta Banerjee, Susheel Varma, Luca Foschini, Kyle Ellrott, Onur Mutlu, Dumitru Ciorb\u{a}, Roel A. Ophoff, Viorel Bostan, Christopher E Mason, Jason H. Moore, Despoina Sousoni, Arunkumar Krishnan, Christopher E. Mason, Mihai Dimian, Gustavo Stolovitzky, Fabio G. Liberante, Taras K. Oleksyk, Serghei Mangul</dc:creator>
    </item>
    <item>
      <title>A Stateless Transparent Voting Machine</title>
      <link>https://arxiv.org/abs/2509.19257</link>
      <description>arXiv:2509.19257v1 Announce Type: cross 
Abstract: Transparency and security are essential in our voting system, and voting machines. This paper describes an implementation of a stateless, transparent voting machine (STVM). The STVM is a ballot marking device (BMD) that uses a transparent, interactive printing interface where voters can verify their paper ballots as they fill out the ballot. The transparent interface turns the paper ballot into an interactive interface. In this architecture, stateless describes the machine's boot sequence, where no information is stored or passed forward between reboots. The machine does not have a hard drive. Instead, it boots and runs from read-only media. This STVM design utilizes a Blu-ray Disc ROM (BD-R) to boot the voting software. This system's statelessness and the transparent interactive printing interface make this design the most secure BMD for voting. Unlike other voting methods, this system incorporates high usability, accessibility, and security for all voters. The STVM uses an open-source voting system that has a universally designed interface, making the system accessible for all voters independent of their ability or disability. This system can make voting safer by simultaneously addressing the issue of voters noticing a vote flip and making it difficult for a hack to persist or go unmitigated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19257v1</guid>
      <category>cs.ET</category>
      <category>cs.CY</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Juan E. Gilbert, Jean D. Louis</dc:creator>
    </item>
    <item>
      <title>Unlocking the Potential of AI Researchers in Scientific Discovery: What Is Missing?</title>
      <link>https://arxiv.org/abs/2503.05822</link>
      <description>arXiv:2503.05822v3 Announce Type: replace 
Abstract: The potential of AI researchers in scientific discovery remains largely untapped. Over the past decade, AI for Science (AI4Science) publications in 145 Nature Index journals have increased fifteen-fold, yet they still account for less than 3% of the total publications. Drawing upon the Diffusion of Innovation theory, we project AI4Science's share of total publications to rise from 2.72% in 2024 to approximately 20% by 2050. Achieving this shift requires fully harnessing the potential of AI researchers, as nearly 95% of AI-driven research in these journals is led by experimental scientists. To facilitate this, we propose structured workflows and strategic interventions to position AI researchers at the forefront of scientific discovery. Specifically, we identify three critical pathways: equipping experimental scientists with accessible AI tools to amplify the impact of AI researchers, bridging cognitive and methodological gaps to enable more direct involvement in scientific discovery, and proactively fostering a thriving AI-driven scientific ecosystem. By addressing these challenges, we aim to empower AI researchers as key drivers of future scientific breakthroughs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05822v3</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hengjie Yu, Shuya Liu, Haiyun Yang, Yuping Yan, Maozhen Qu, Yaochu Jin</dc:creator>
    </item>
    <item>
      <title>Testing Fairness with Utility Tradeoffs: A Wasserstein Projection Approach</title>
      <link>https://arxiv.org/abs/2505.11678</link>
      <description>arXiv:2505.11678v3 Announce Type: replace 
Abstract: Ensuring fairness in data driven decision making has become a central concern across domains such as marketing, lending, and healthcare, but fairness constraints often come at the cost of utility. We propose a statistical hypothesis testing framework that jointly evaluates approximate fairness and utility, relaxing strict fairness requirements while ensuring that overall utility remains above a specified threshold. Our framework builds on the strong demographic parity (SDP) criterion and incorporates a utility measure motivated by the potential outcomes framework. The test statistic is constructed via Wasserstein projections, enabling auditors to assess whether observed fairness-utility tradeoffs are intrinsic to the algorithm or attributable to randomness in the data. We show that the test is computationally tractable, interpretable, broadly applicable across machine learning models, and extendable to more general settings. We apply our approach to multiple real-world datasets, offering new insights into the fairness-utility tradeoff through the perspective of statistical hypothesis testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11678v3</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yan Chen, Zheng Tan, Jose Blanchet, Hanzhang Qin</dc:creator>
    </item>
    <item>
      <title>Justice in Judgment: Unveiling (Hidden) Bias in LLM-assisted Peer Reviews</title>
      <link>https://arxiv.org/abs/2509.13400</link>
      <description>arXiv:2509.13400v3 Announce Type: replace 
Abstract: The adoption of large language models (LLMs) is transforming the peer review process, from assisting reviewers in writing more detailed evaluations to generating entire reviews automatically. While these capabilities offer exciting opportunities, they also raise critical concerns about fairness and reliability. In this paper, we investigate bias in LLM-generated peer reviews by conducting controlled experiments on sensitive metadata, including author affiliation and gender. Our analysis consistently shows affiliation bias favoring institutions highly ranked on common academic rankings. Additionally, we find some gender preferences, which, even though subtle in magnitude, have the potential to compound over time. Notably, we uncover implicit biases that become more evident with token-based soft ratings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13400v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sai Suresh Marchala Vasu, Ivaxi Sheth, Hui-Po Wang, Ruta Binkyte, Mario Fritz</dc:creator>
    </item>
    <item>
      <title>The Narcissus Hypothesis: Descending to the Rung of Illusion</title>
      <link>https://arxiv.org/abs/2509.17999</link>
      <description>arXiv:2509.17999v2 Announce Type: replace 
Abstract: Modern foundational models increasingly reflect not just world knowledge, but patterns of human preference embedded in their training data. We hypothesize that recursive alignment-via human feedback and model-generated corpora-induces a social desirability bias, nudging models to favor agreeable or flattering responses over objective reasoning. We refer to it as the Narcissus Hypothesis and test it across 31 models using standardized personality assessments and a novel Social Desirability Bias score. Results reveal a significant drift toward socially conforming traits, with profound implications for corpus integrity and the reliability of downstream inferences. We then offer a novel epistemological interpretation, tracing how recursive bias may collapse higher-order reasoning down Pearl's Ladder of Causality, culminating in what we refer to as the Rung of Illusion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17999v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riccardo Cadei, Christian Intern\`o</dc:creator>
    </item>
    <item>
      <title>Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation</title>
      <link>https://arxiv.org/abs/2410.05401</link>
      <description>arXiv:2410.05401v4 Announce Type: replace-cross 
Abstract: Climate change communication on social media increasingly employs microtargeting strategies to effectively reach and influence specific demographic groups. This study presents a post-hoc analysis of microtargeting practices within climate campaigns by leveraging large language models (LLMs) to examine Meta (previously known as Facebook) advertisements. Our analysis focuses on two key aspects: demographic targeting and fairness. We evaluate the ability of LLMs to accurately predict the intended demographic targets, such as gender and age group. Furthermore, we instruct the LLMs to generate explanations for their classifications, providing transparent reasoning behind each decision. These explanations reveal the specific thematic elements used to engage different demographic segments, highlighting distinct strategies tailored to various audiences. Our findings show that young adults are primarily targeted through messages emphasizing activism and environmental consciousness, while women are engaged through themes related to caregiving roles and social advocacy. Additionally, we conduct a comprehensive fairness analysis to uncover biases in model predictions. We assess disparities in accuracy and error rates across demographic groups using established fairness metrics such as Demographic Parity, Equal Opportunity, and Predictive Equality. Our findings indicate that while LLMs perform well overall, certain biases exist, particularly in the classification of male audiences. The analysis of thematic explanations uncovers recurring patterns in messaging strategies tailored to various demographic groups, while the fairness analysis underscores the need for more inclusive targeting methods. This study provides a valuable framework for future research aimed at enhancing transparency, accountability, and inclusivity in social media-driven climate campaigns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05401v4</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tunazzina Islam, Dan Goldwasser</dc:creator>
    </item>
    <item>
      <title>Homophily Within and Across Groups</title>
      <link>https://arxiv.org/abs/2412.07901</link>
      <description>arXiv:2412.07901v3 Announce Type: replace-cross 
Abstract: Homophily -- the tendency of individuals to interact with similar others -- shapes how networks form and function. Yet existing approaches typically collapse homophily to a single scale, either one parameter for the whole network or one per community, thereby detaching it from other structural features. Here, we introduce a maximum-entropy random graph model that moves beyond these limits, capturing homophily across all social scales in the network, with parameters for each group size. The framework decomposes homophily into within- and across-group contributions, recovering the stochastic block model as a special case. As an exponential-family model, it fits empirical data and enables inference of group-level variation of homophily that aggregate metrics miss. The group-dependence of homophily substantially impacts network percolation thresholds, altering predictions for epidemic spread, information diffusion, and the effectiveness of interventions. Ignoring such heterogeneity risks systematically misjudging connectivity and dynamics in complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07901v3</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <category>math.PR</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abbas K. Rizi, Riccardo Michielan, Clara Stegehuis, Mikko Kivel\"a</dc:creator>
    </item>
    <item>
      <title>Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections of Images</title>
      <link>https://arxiv.org/abs/2504.08727</link>
      <description>arXiv:2504.08727v3 Announce Type: replace-cross 
Abstract: We present a system using Multimodal LLMs (MLLMs) to analyze a large database with tens of millions of images captured at different times, with the aim of discovering patterns in temporal changes. Specifically, we aim to capture frequent co-occurring changes ("trends") across a city over a certain period. Unlike previous visual analyses, our analysis answers open-ended queries (e.g., "what are the frequent types of changes in the city?") without any predetermined target subjects or training labels. These properties cast prior learning-based or unsupervised visual analysis tools unsuitable. We identify MLLMs as a novel tool for their open-ended semantic understanding capabilities. Yet, our datasets are four orders of magnitude too large for an MLLM to ingest as context. So we introduce a bottom-up procedure that decomposes the massive visual analysis problem into more tractable sub-problems. We carefully design MLLM-based solutions to each sub-problem. During experiments and ablation studies with our system, we find it significantly outperforms baselines and is able to discover interesting trends from images captured in large cities (e.g., "addition of outdoor dining,", "overpass was painted blue," etc.). See more results and interactive demos at https://boyangdeng.com/visual-chronicles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08727v3</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Boyang Deng, Songyou Peng, Kyle Genova, Gordon Wetzstein, Noah Snavely, Leonidas Guibas, Thomas Funkhouser</dc:creator>
    </item>
    <item>
      <title>Early Prediction of In-Hospital ICU Mortality Using Innovative First-Day Data: A Review</title>
      <link>https://arxiv.org/abs/2505.12344</link>
      <description>arXiv:2505.12344v2 Announce Type: replace-cross 
Abstract: The intensive care unit (ICU) manages critically ill patients, many of whom face a high risk of mortality. Early and accurate prediction of in-hospital mortality within the first 24 hours of ICU admission is crucial for timely clinical interventions, resource optimization, and improved patient outcomes. Traditional scoring systems, while useful, often have limitations in predictive accuracy and adaptability. Objective: This review aims to systematically evaluate and benchmark innovative methodologies that leverage data available within the first day of ICU admission for predicting in-hospital mortality. We focus on advancements in machine learning, novel biomarker applications, and the integration of diverse data types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12344v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Baozhu Huang, Cheng Chen, Xuanhe Hou, Junmin Huang, Zihan Wei, Hongying Luo, Lu Chen, Yongzhi Xu, Hejiao Luo, Changqi Qin, Ziqian Bi, Junhao Song, Tianyang Wang, ChiaXin Liang, Zizhong Yu, Han Wang, Xiaotian Sun, Junfeng Hao, Chunjie Tian</dc:creator>
    </item>
    <item>
      <title>MetaFed: Advancing Privacy, Performance, and Sustainability in Federated Metaverse Systems</title>
      <link>https://arxiv.org/abs/2508.17341</link>
      <description>arXiv:2508.17341v2 Announce Type: replace-cross 
Abstract: The rapid expansion of immersive Metaverse applications introduces complex challenges at the intersection of performance, privacy, and environmental sustainability. Centralized architectures fall short in addressing these demands, often resulting in elevated energy consumption, latency, and privacy concerns. This paper proposes MetaFed, a decentralized federated learning (FL) framework that enables sustainable and intelligent resource orchestration for Metaverse environments. MetaFed integrates (i) multi-agent reinforcement learning for dynamic client selection, (ii) privacy-preserving FL using homomorphic encryption, and (iii) carbon-aware scheduling aligned with renewable energy availability. Evaluations on MNIST and CIFAR-10 using lightweight ResNet architectures demonstrate that MetaFed achieves up to 25% reduction in carbon emissions compared to conventional approaches, while maintaining high accuracy and minimal communication overhead. These results highlight MetaFed as a scalable solution for building environmentally responsible and privacy-compliant Metaverse infrastructures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17341v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammet Anil Yagiz, Zeynep Sude Cengiz, Polat Goktas</dc:creator>
    </item>
  </channel>
</rss>
