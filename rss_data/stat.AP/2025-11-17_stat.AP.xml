<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Nov 2025 04:07:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Modeling U.S. Mortality and Suicide Rates by Integrating Mental Health and Socio-Economic Indicators</title>
      <link>https://arxiv.org/abs/2511.10719</link>
      <description>arXiv:2511.10719v1 Announce Type: new 
Abstract: Accurate mortality modeling is central to actuarial science and public health, especially as mental health emerges as a significant factor in population outcomes. This paper develops and applies a Bayesian hierarchical model to analyze U.S. county-level mortality and suicide rates from 2010 to 2023. Applying a conditional autoregressive (CAR) structure to each combination of sex and age grouping, the model captures spatial and temporal trends while incorporating mental health surveillance data and socio-economic indicators. We first assess socio-economic covariates in predicting suicide. While the results vary considerably by age and sex, we find that the county-wide levels of educational attainment, housing prices, marriage rates, racial composition, household size, and poor mental health days all have significant relationships with suicide rates. We next consider the impact of various mental health indicators on all-cause and suicide-specific mortality and find that the strongest effects are observed in younger populations. The spatial and temporal correlation structures reveal substantial regional clustering and time-consistent trends in both all-cause mortality and suicide rates, supporting the use of spatio-temporal methods. Our findings highlight the value of integrating mental health surveillance data into mortality models to better identify emerging risk areas and vulnerable populations. This approach has the potential to inform public health policy, resource allocation, and targeted interventions aimed at reducing disparities in mortality and suicide across U.S. communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10719v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Brianne Weaver, Brigg Trendler, Chris Groendyke, Brian Hartman, Robert Richardson, Davey Erekson</dc:creator>
    </item>
    <item>
      <title>Bayesian Evaluation of Large Language Model Behavior</title>
      <link>https://arxiv.org/abs/2511.10661</link>
      <description>arXiv:2511.10661v1 Announce Type: cross 
Abstract: It is increasingly important to evaluate how text generation systems based on large language models (LLMs) behave, such as their tendency to produce harmful output or their sensitivity to adversarial inputs. Such evaluations often rely on a curated benchmark set of input prompts provided to the LLM, where the output for each prompt may be assessed in a binary fashion (e.g., harmful/non-harmful or does not leak/leaks sensitive information), and the aggregation of binary scores is used to evaluate the LLM. However, existing approaches to evaluation often neglect statistical uncertainty quantification. With an applied statistics audience in mind, we provide background on LLM text generation and evaluation, and then describe a Bayesian approach for quantifying uncertainty in binary evaluation metrics. We focus in particular on uncertainty that is induced by the probabilistic text generation strategies typically deployed in LLM-based systems. We present two case studies applying this approach: 1) evaluating refusal rates on a benchmark of adversarial inputs designed to elicit harmful responses, and 2) evaluating pairwise preferences of one LLM over another on a benchmark of open-ended interactive dialogue examples. We demonstrate how the Bayesian approach can provide useful uncertainty quantification about the behavior of LLM-based systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10661v1</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rachel Longjohn, Shang Wu, Saatvik Kher, Catarina Bel\'em, Padhraic Smyth</dc:creator>
    </item>
    <item>
      <title>Assaults on Judicial Independence under the Pretense of Modernization: Evidence from Venezuela</title>
      <link>https://arxiv.org/abs/2511.10681</link>
      <description>arXiv:2511.10681v1 Announce Type: cross 
Abstract: We investigate how government-orchestrated assaults on the judiciary, disguised as modernization efforts, undermine judicial independence. Our study focuses on Venezuela's constitutional overhaul in the early 2000s, initiated by Hugo Ch\'avez and implemented through a judicial emergency committee. We employ a hybrid synthetic control and difference-in-differences approach to estimate the impact of populist attacks on judicial independence trajectories. By comparing Venezuela to a stable pool of countries without radical constitutional changes, our identification strategy isolates the effect of populist assaults from unobservable confounders and common time trends. Our findings reveal that authoritarian interventions lead to an immediate and lasting breakdown of judicial independence. The deterioration in judicial independence vis-\'a-vis the estimated counterfactual is robust to variations in the donor pool composition. It does not appear to be driven by pre-existing judicial changes and withstands numerous temporal and spatial placebo checks across over nine million randomly sequenced donor samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10681v1</guid>
      <category>econ.GN</category>
      <category>econ.EM</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nuno Garoupa, Virginia Rosales, Rok Spruk</dc:creator>
    </item>
    <item>
      <title>Multivariate longitudinal modeling of cross-sectional and lagged associations between a continuous time-varying endogenous covariate and a non-Gaussian outcome</title>
      <link>https://arxiv.org/abs/2511.11114</link>
      <description>arXiv:2511.11114v1 Announce Type: cross 
Abstract: In longitudinal studies, time-varying covariates are often endogenous, meaning their values depend on both their own history and that of the outcome variable. This violates key assumptions of Generalized Linear Mixed Effects Models (GLMMs), leading to biased and inconsistent estimates. Additionally, missing data and non-concurrent measurements between covariates and outcomes further complicate analysis, especially in rare or degenerative diseases where data is limited. To address these challenges, we propose an alternative use of two well-known multivariate models, each assuming a different form of the association. One induces the association by jointly modeling the random effects, called Joint Mixed Model (JMM); the other quantifies the association using a scaling factor, called Joint Scaled Model (JSM). We extend these models to accommodate continuous endogenous covariates and a wide range of longitudinal outcome types. A limitation in both cases is that the interpretation of the association is neither straightforward nor easy to communicate to scientists. Hence, we have numerically derived an association coefficient that measures the marginal relation between the outcome and the endogenous covariate. The proposed method provides interpretable, population-level estimates of cross-sectional associations (capturing relationships between covariates and outcomes measured at the same time point) and lagged associations (quantifying how past covariate values influence future outcomes), enabling clearer clinical insights. We fitted the JMM and JSM using a flexible Bayesian estimation approach, known as Integrated Nested Laplace Approximation (INLA), to overcome computation burden problems. These models will be presented along with the results of a simulation study and a natural history study on patients with Duchenne Muscular Dystrophy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11114v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chiara Degan, Bart J. A. Mertens, Jelle Goeman, Nadine A. Ikelaar, Erik H. Niks, Pietro Spitali, Roula Tsonaka</dc:creator>
    </item>
    <item>
      <title>Estimating the Effects of Heatwaves on Health: A Causal Inference Framework</title>
      <link>https://arxiv.org/abs/2511.11433</link>
      <description>arXiv:2511.11433v1 Announce Type: cross 
Abstract: The harmful relationship between heatwaves and health has been extensively documented in medical and epidemiological literature. However, most evidence is associational and cannot be interpreted causally unless strong assumptions are made. In this paper, we first make explicit the assumptions underlying the statistical methods frequently used in the heatwave literature and demonstrate when these assumptions might break down in heatwave contexts. To address these shortcomings, we propose a causal inference framework that transparently elicits causal identification assumptions. Within this new framework, we first introduce synthetic controls (SC) for estimating heatwave effects, then propose a spatially augmented Bayesian synthetic control (SA-SC) method that accounts for spatial dependence and spillovers. Empirical Monte Carlo simulations show both methods perform well, with SA-SC reducing root mean squared error and improving posterior interval coverage under spillovers and spatial dependence. Finally, we apply the proposed methods to estimate the causal effects of heatwaves on Medicare heat-related hospitalizations among 13,753,273 beneficiaries residing in Northeastern U.S. from 2000 to 2019. This causal inference framework provides spatially coherent counterfactual outcomes and robust, interpretable, and transparent causal estimates while explicitly addressing the unexamined assumptions in existing methods that pervade the heatwave effect literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11433v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giulio Grossi, Leo Vanciu, Veronica Ballerini, Danielle Braun, Falco J. Bargagli Stoffi</dc:creator>
    </item>
    <item>
      <title>Some are observed, all leave traces: whole-population modeling of French elite civil servants' career paths</title>
      <link>https://arxiv.org/abs/2311.15257</link>
      <description>arXiv:2311.15257v3 Announce Type: replace 
Abstract: Elite civil servants may come and go between the public and private sectors throughout their career, a process of particular interest for the public and social scientists. However, data to document such processes are rarely completely available: we need inference tools that can account for many missing values. We consider public-private paths of elite French civil servants and introduce binary Markov switching models with Bayesian data augmentation. Our procedure relies on two complementary data sources: (1) detailed observations of some individual trajectories obtained from LinkedIn; (2) less informative ``traces'' left by all individuals in the administrative record, which we model for missing data imputation. This model class maintains the properties of hidden Markov models and enables a tailored sampler to target the posterior, yet allows for varying parameters across individuals and time. By integrating the two sources, we can consider the whole population rather than just a sample, and avoid the biases that would stem from using only a single source. We demonstrate this allows to properly test substantive hypotheses on career paths across a variety of public organizations. We notably show that the probability for ENA graduates to exit the public sector has not increased since 1990, but that the probability they return has increased. We identify three clusters of organizations, with distinct patterns of public-private behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15257v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Th\'eo Voldoire, Robin J. Ryder, Ryan Lahfa</dc:creator>
    </item>
    <item>
      <title>Automated lag-selection for multi-step univariate time series forecast using Bayesian Optimization: Forecast station-wise monthly rainfall of nine divisional cities of Bangladesh</title>
      <link>https://arxiv.org/abs/2401.08070</link>
      <description>arXiv:2401.08070v2 Announce Type: replace 
Abstract: Rainfall is an essential hydrological component, and most of the economic activities of an agrarian country like Bangladesh depend on rainfall. An accurate rainfall forecast can help make necessary decisions and reduce the damages caused by heavy or low to no rainfall. The monthly average rainfall is a time series data, and recently, long short-term memory (LSTM) neural networks are being used heavily for time series forecasting problems. One major challenge of forecasting using LSTMs is to select the appropriate number of lag values. In this research, we considered the number of lag values selected as a hyperparameter of LSTM; it, with the other hyperparameters determining LSTMs structure, has been optimized using Bayesian optimization. We used our proposed method to forecast rainfall for nine different weather stations of Bangladesh. Finally, the performance of the proposed model has been compared with some other LSTM with different lag-selection methods and some several popular machine learning and statistical forecasting models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08070v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rezoanoor Rahman, Fariha Taskin</dc:creator>
    </item>
    <item>
      <title>Hierarchical Probabilistic Conformal Prediction for Distributed Energy Resources Adoption</title>
      <link>https://arxiv.org/abs/2411.12193</link>
      <description>arXiv:2411.12193v3 Announce Type: replace 
Abstract: The rapid growth of distributed energy resources (DERs) presents both opportunities and operational challenges for electric grid management. Accurately predicting DER adoption is critical for proactive infrastructure planning, but the inherent uncertainty and spatial disparity of DER growth complicate traditional forecasting approaches. Moreover, the hierarchical structure of distribution grids demands that predictions satisfy statistical guarantees at both the circuit and substation levels, a non-trivial requirement for reliable decision-making. In this paper, we propose a novel uncertainty quantification framework for DER adoption predictions that ensures validity across hierarchical grid structures. Leveraging a multivariate Hawkes process to model DER adoption dynamics and a tailored split conformal prediction algorithm, we introduce a new nonconformity score that preserves statistical guarantees under aggregation while maintaining prediction efficiency. We establish theoretical validity under mild conditions and demonstrate through empirical evaluation on customer-level solar panel installation data from Indianapolis, Indiana that our method consistently outperforms existing baselines in both predictive accuracy and uncertainty calibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12193v3</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenbin Zhou, Shixiang Zhu</dc:creator>
    </item>
    <item>
      <title>Efficient Image Restoration via Latent Consistency Flow Matching</title>
      <link>https://arxiv.org/abs/2502.03500</link>
      <description>arXiv:2502.03500v2 Announce Type: replace-cross 
Abstract: Recent advances in generative image restoration (IR) have demonstrated impressive results. However, these methods are hindered by their substantial size and computational demands, rendering them unsuitable for deployment on edge devices. This work introduces ELIR, an Efficient Latent Image Restoration method. ELIR addresses the distortion-perception trade-off within the latent space and produces high-quality images using a latent consistency flow-based model. In addition, ELIR introduces an efficient and lightweight architecture. Consequently, ELIR is 4$\times$ smaller and faster than state-of-the-art diffusion and flow-based approaches for blind face restoration, enabling a deployment on resource-constrained devices. Comprehensive evaluations of various image restoration tasks and datasets show that ELIR achieves competitive performance compared to state-of-the-art methods, effectively balancing distortion and perceptual quality metrics while significantly reducing model size and computational cost. The code is available at: https://github.com/eladc-git/ELIR</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03500v2</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elad Cohen, Idan Achituve, Idit Diamant, Arnon Netzer, Hai Victor Habi</dc:creator>
    </item>
    <item>
      <title>Extending the Joint Probability Method to Compound Flooding: Statistical Delineation of Transition Zones and Design Event Selection</title>
      <link>https://arxiv.org/abs/2511.03871</link>
      <description>arXiv:2511.03871v2 Announce Type: replace-cross 
Abstract: Compound flooding from the combined effects of extreme storm surge, rainfall, and river flows poses significant risks to infrastructure and communities -- as demonstrated by hurricanes Isaac and Harvey. Yet, existing methods to quantify compound flood risk lack a unified probabilistic basis. Copula-based models capture the co-occurrence of flood drivers but not the likelihood of the flood response, while coupled hydrodynamic models simulate interactions but lack a probabilistic characterization of compound flood extremes. The Joint Probability Method (JPM), the foundation of coastal surge risk analysis, has never been formally extended to incorporate hydrologic drivers -- leaving a critical gap in quantifying compound flood risk and the statistical structure of compound flood transition zones (CFTZs). Here, we extend the JPM theory to hydrologic processes for quantifying the likelihood of compound flood depths across both tropical and non-tropical storms. This extended methodology incorporates rainfall fields, antecedent soil moisture, and baseflow alongside coastal storm surge, enabling: (1) a statistical description of the flood depth as the response to the joint distribution of hydrologic and coastal drivers, (2) a statistical delineation of the CFTZ based on exceedance probabilities, and (3) a systematic identification of design storms for specified return period flood depths, moving beyond design based solely on driver likelihoods. We demonstrate this method around Lake Maurepas, Louisiana. Results show a CFTZ more than double the area of prior event-specific delineations, with compound interactions increasing flood depths by up to 2.25 feet. This extended JPM provides a probabilistic foundation for compound flood risk assessment and planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03871v2</guid>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark S. Bartlett, Nathan Geldner, Zach Cobell, Luis Partida, Ovel Diaz, David R. Johnson, Hanbeen Kim, Brett McMann, Gabriele Villarini, Shubra Misra, Hugh J. Roberts, Muthukumar Narayanaswamy</dc:creator>
    </item>
    <item>
      <title>Data reuse enables cost-efficient randomized trials of medical AI models</title>
      <link>https://arxiv.org/abs/2511.08986</link>
      <description>arXiv:2511.08986v2 Announce Type: replace-cross 
Abstract: Randomized controlled trials (RCTs) are indispensable for establishing the clinical value of medical artificial-intelligence (AI) tools, yet their high cost and long timelines hinder timely validation as new models emerge rapidly. Here, we propose BRIDGE, a data-reuse RCT design for AI-based risk models. AI risk models support a broad range of interventions, including screening, treatment selection, and clinical alerts. BRIDGE trials recycle participant-level data from completed trials of AI models when legacy and updated models make concordant predictions, thereby reducing the enrollment requirement for subsequent trials. We provide a practical checklist for investigators to assess whether reusing data from previous trials allows for valid causal inference and preserves type I error. Using real-world datasets across breast cancer, cardiovascular disease, and sepsis, we demonstrate concordance between successive AI models, with up to 64.8% overlap in top 5% high-risk cohorts. We then simulate a series of breast cancer screening studies, where our design reduced required enrollment by 46.6%--saving over US$2.8 million--while maintaining 80% power. By transforming trials into adaptive, modular studies, our proposed design makes Level I evidence generation feasible for every model iteration, thereby accelerating cost-effective translation of AI into routine care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08986v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Nercessian, Wenxin Zhang, Alexander Schubert, Daphne Yang, Maggie Chung, Ahmed Alaa, Adam Yala</dc:creator>
    </item>
  </channel>
</rss>
