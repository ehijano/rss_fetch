<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Aug 2025 01:30:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Toward Optimal Statistical Inference in Noisy Linear Quadratic Reinforcement Learning over a Finite Horizon</title>
      <link>https://arxiv.org/abs/2508.08436</link>
      <description>arXiv:2508.08436v1 Announce Type: new 
Abstract: Recent developments in Reinforcement learning have significantly enhanced sequential decision-making in uncertain environments. Despite their strong performance guarantees, most existing work has focused primarily on improving the operational accuracy of learned control policies and the convergence rates of learning algorithms, with comparatively little attention to uncertainty quantification and statistical inference. Yet, these aspects are essential for assessing the reliability and variability of control policies, especially in high-stakes applications. In this paper, we study statistical inference for the policy gradient (PG) method for noisy Linear Quadratic Reinforcement learning (LQ RL) over a finite time horizon, where linear dynamics with both known and unknown drift parameters are controlled subject to a quadratic cost. We establish the theoretical foundations for statistical inference in LQ RL, deriving exact asymptotics for both the PG estimators and the corresponding objective loss. Furthermore, we introduce a principled inference framework that leverages online bootstrapping to construct confidence intervals for both the learned optimal policy and the corresponding objective losses. The method updates the PG estimates along with a set of randomly perturbed PG estimates as new observations arrive. We prove that the proposed bootstrapping procedure is distributionally consistent and that the resulting confidence intervals achieve both asymptotic and non-asymptotic validity. Notably, our results imply that the quantiles of the exact distribution can be approximated at a rate of $n^{-1/4}$, where $n$ is the number of samples used during the procedure. The proposed procedure is easy to implement and applicable to both offline and fully online settings. Numerical experiments illustrate the effectiveness of our approach across a range of noisy linear dynamical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08436v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bo Pan, Jianya Lu, Yafei Wang, Hao Li, Bei Jiang, Linglong Kong</dc:creator>
    </item>
    <item>
      <title>Covariance as a commutator</title>
      <link>https://arxiv.org/abs/2508.09102</link>
      <description>arXiv:2508.09102v1 Announce Type: new 
Abstract: The covariance between real finite variance random variables can be expressed as the commutator of taking expectations and multiplying, both viewed as operators extended to act jointly on pairs of functions. The efficient influence curve of the mean represents a centering operator which we demonstrate to interact with expectations and products through simple commutator identities. These expressions reveal an underlying Lie algebraic structure that endows the calculus of efficient influence curves with a natural differential geometric interpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09102v1</guid>
      <category>math.ST</category>
      <category>math.DG</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Garc\'ia Meixide</dc:creator>
    </item>
    <item>
      <title>On Experiments</title>
      <link>https://arxiv.org/abs/2508.08288</link>
      <description>arXiv:2508.08288v1 Announce Type: cross 
Abstract: The scientific process is a means for turning the results of experiments into knowledge about the world in which we live. Much research effort has been directed toward automating this process. To do this, one needs to formulate the scientific process in a precise mathematical language. This paper outlines one such language. What is presented here is hardly new. The material leans much on great thinkers of times past as well as more modern contributions. The novel contributions of this paper are: A new, general data processing inequality, a bias variance decomposition for canonical losses, Streamlined proofs of the Blackwell-Sherman-Stein and Randomization Theorems, and Means to calculate deficiency via linear programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08288v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brendan van Rooyen</dc:creator>
    </item>
    <item>
      <title>Consistent Bayesian Spatial Domain Partitioning Using Predictive Spanning Tree Methods</title>
      <link>https://arxiv.org/abs/2508.08324</link>
      <description>arXiv:2508.08324v1 Announce Type: cross 
Abstract: Bayesian model-based spatial clustering methods are widely used for their flexibility in estimating latent clusters with an unknown number of clusters while accounting for spatial proximity. Many existing methods are designed for clustering finite spatial units, limiting their ability to make predictions, or may impose restrictive geometric constraints on the shapes of subregions. Furthermore, the posterior clustering consistency theory of spatial clustering models remains largely unexplored in the literature. In this study, we propose a Spatial Domain Random Partition Model (Spat-RPM) and demonstrate its application for spatially clustered regression, which extends spanning tree-based Bayesian spatial clustering by partitioning the spatial domain into disjoint blocks and using spanning tree cuts to induce contiguous domain partitions. Under an infill-domain asymptotic framework, we introduce a new distance metric to study the posterior concentration of domain partitions. We show that Spat-RPM achieves a consistent estimation of domain partitions, including the number of clusters, and derive posterior concentration rates for partition, parameter, and prediction. We also establish conditions on the hyperparameters of priors and the number of blocks, offering important practical guidance for hyperparameter selection. Finally, we examine the asymptotic properties of our model through simulation studies and apply it to Atlantic Ocean data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08324v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kun Huang, Huiyan Sang</dc:creator>
    </item>
    <item>
      <title>Doubly robust pointwise confidence intervals for a monotonic continuous treatment effect curve</title>
      <link>https://arxiv.org/abs/2508.08415</link>
      <description>arXiv:2508.08415v1 Announce Type: cross 
Abstract: We study nonparametric inference for the causal dose-response (or treatment effect) curve when the treatment variable is continuous rather than binary or discrete. We do this by developing doubly robust confidence intervals for the continuous treatment effect curve (at a fixed point) under the assumption that it is monotonic, based on inverting a likelihood ratio-type test. Monotonicity of the treatment effect curve is often a very natural assumption, and this assumption removes the need to choose a smoothing or tuning parameter for the nonparametrically estimated curve. The likelihood ratio procedure is effective because it allows us to avoid estimating the curve's unknown bias, which is challenging to do. The test statistic is ``doubly robust'' in that a remainder term is the product of errors for the two so-called nuisance functions that naturally arise (the outcome regression and generalized propensity score functions), which allows one nuisance to be estimated poorly if the other is estimated well. Furthermore, we propose a version of our test or confidence interval that is adaptive to a range of the unknown curve's flatness level. We present versions with and without cross fitting. We illustrate the new methods via simulations and a study of a dataset relating the effect of nurse staffing hours on hospital performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08415v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles R. Doss</dc:creator>
    </item>
    <item>
      <title>Causal Geodesy: Counterfactual Estimation Along the Path Between Correlation and Causation</title>
      <link>https://arxiv.org/abs/2508.08499</link>
      <description>arXiv:2508.08499v1 Announce Type: cross 
Abstract: We introduce causal geodesy, a framework for studying the landscape of stochastic interventions that lie between the two extremes of performing no intervention, and performing a sharp intervention that sets an exposure equal to a specific value. We define this framework by constructing paths of distributions that smoothly interpolate between the treatment density and a point mass at the target intervention. Thus, each path starts at a purely observational (or correlational) quantity and moves into a counterfactual world. Of particular interest are paths that correspond to geodesics in some metric, i.e. the shortest path. We then consider the interpretation and estimation of the corresponding causal effects as we move along the path from correlation toward causation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08499v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Schindl, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Kernel Two-Sample Testing via Directional Components Analysis</title>
      <link>https://arxiv.org/abs/2508.08564</link>
      <description>arXiv:2508.08564v1 Announce Type: cross 
Abstract: We propose a novel kernel-based two-sample test that leverages the spectral decomposition of the maximum mean discrepancy (MMD) statistic to identify and utilize well-estimated directional components in reproducing kernel Hilbert space (RKHS). Our approach is motivated by the observation that the estimation quality of these components varies significantly, with leading eigen-directions being more reliably estimated in finite samples. By focusing on these directions and aggregating information across multiple kernels, the proposed test achieves higher power and improved robustness, especially in high-dimensional and unbalanced sample settings. We further develop a computationally efficient multiplier bootstrap procedure for approximating critical values, which is theoretically justified and significantly faster than permutation-based alternatives. Extensive simulations and empirical studies on microarray datasets demonstrate that our method maintains the nominal Type I error rate and delivers superior power compared to other existing MMD-based tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08564v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rui Cui, Yuhao Li, Xiaojun Song</dc:creator>
    </item>
    <item>
      <title>Bias correction for Chatterjee's graph-based correlation coefficient</title>
      <link>https://arxiv.org/abs/2508.09040</link>
      <description>arXiv:2508.09040v1 Announce Type: cross 
Abstract: Azadkia and Chatterjee (2021) recently introduced a simple nearest neighbor (NN) graph-based correlation coefficient that consistently detects both independence and functional dependence. Specifically, it approximates a measure of dependence that equals 0 if and only if the variables are independent, and 1 if and only if they are functionally dependent. However, this NN estimator includes a bias term that may vanish at a rate slower than root-$n$, preventing root-$n$ consistency in general. In this article, we propose a bias correction approach that overcomes this limitation, yielding an NN-based estimator that is both root-$n$ consistent and asymptotically normal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09040v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mona Azadkia, Leihao Chen, Fang Han</dc:creator>
    </item>
    <item>
      <title>Selecting the number of components in PCA via random signflips</title>
      <link>https://arxiv.org/abs/2012.02985</link>
      <description>arXiv:2012.02985v4 Announce Type: replace 
Abstract: Principal component analysis (PCA) is a foundational tool in modern data analysis, and a crucial step in PCA is selecting the number of components to keep. However, classical selection methods (e.g., scree plots, parallel analysis, etc.) lack statistical guarantees in the increasingly common setting of large-dimensional data with heterogeneous noise, i.e., where each entry may have a different noise variance. Moreover, it turns out that these methods, which are highly effective for homogeneous noise, can fail dramatically for data with heterogeneous noise. This paper proposes a new method called signflip parallel analysis (FlipPA) for the setting of approximately symmetric noise: it compares the data singular values to those of "empirical null" matrices generated by flipping the sign of each entry randomly with probability one-half. We develop a rigorous theory for FlipPA, showing that it has nonasymptotic type I error control and that it consistently selects the correct rank for signals rising above the noise floor in the large-dimensional limit (even when the noise is heterogeneous). We also rigorously explain why classical permutation-based parallel analysis degrades under heterogeneous noise. Finally, we illustrate that FlipPA compares favorably to state-of-the art methods via numerical simulations and an illustration on data coming from astronomy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2012.02985v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Hong, Yue Sheng, Edgar Dobriban</dc:creator>
    </item>
    <item>
      <title>Maximum Likelihood Degrees of Brownian Motion Tree Models: Star Trees and Root Invariance</title>
      <link>https://arxiv.org/abs/2402.10322</link>
      <description>arXiv:2402.10322v2 Announce Type: replace 
Abstract: A Brownian motion tree (BMT) model is a Gaussian model whose associated set of covariance matrices is linearly constrained according to common ancestry in a phylogenetic tree. We study the complexity of inferring the maximum likelihood (ML) estimator for a BMT model by computing its ML-degree. Our main result is that the ML-degree of the BMT model on a star tree with $n + 1$ leaves is $2^{n+1}-2n-3$, which was previously conjectured by Am\'endola and Zwiernik. We also prove that the ML-degree of a BMT model is independent of the choice of the root. The proofs rely on the toric geometry of concentration matrices in a BMT model. Toward this end, we produce a combinatorial formula for the determinant of the concentration matrix of a BMT model, which generalizes the Cayley-Pr\"ufer theorem to complete graphs with weights given by a tree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10322v2</guid>
      <category>math.ST</category>
      <category>math.AG</category>
      <category>q-bio.PE</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jsc.2025.102482</arxiv:DOI>
      <dc:creator>Jane Ivy Coons, Shelby Cox, Aida Maraj, Ikenna Nometa</dc:creator>
    </item>
    <item>
      <title>Optimal low-rank posterior covariance approximation in linear Gaussian inverse problems on Hilbert spaces</title>
      <link>https://arxiv.org/abs/2411.01112</link>
      <description>arXiv:2411.01112v5 Announce Type: replace 
Abstract: For linear inverse problems with Gaussian priors and Gaussian observation noise, the posterior is Gaussian, with mean and covariance determined by the conditioning formula. The covariance is the central object for uncertainty quantification, as it encodes the variability of the posterior distribution and thus the uncertainty in the posterior mean estimate. Using the Feldman-Hajek theorem, we analyse the prior-to-posterior update and its low-rank approximation for infinite-dimensional Hilbert parameter spaces and finite-dimensional observations. We show that the posterior distribution differs from the prior on a finite-dimensional subspace, and construct low-rank approximations to the posterior covariance, while keeping the mean fixed. Since in infinite dimensions, not all low-rank covariance approximations yield approximate posterior distributions which are equivalent to the posterior and prior distribution, we characterise the low-rank covariance approximations which do yield this equivalence, and their respective inverses, or `precisions'. For such approximations, a family of measure approximation problems is solved by identifying the low-rank approximations which are optimal for various losses simultaneously. These loss functions include the family of R\'enyi divergences, the Amari $\alpha$-divergences for $\alpha\in(0,1)$, the Hellinger metric and the Kullback-Leibler divergence. Our results extend those of Spantini et al. (SIAM J. Sci. Comput. 2015) to Hilbertian parameter spaces, and provide theoretical underpinning for the construction of low-rank approximations of discretised versions of the infinite-dimensional inverse problem, by formulating discretisation independent results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01112v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Carere, Han Cheng Lie</dc:creator>
    </item>
    <item>
      <title>Optimal low-rank posterior mean and distribution approximation in linear Gaussian inverse problems on Hilbert spaces</title>
      <link>https://arxiv.org/abs/2503.24209</link>
      <description>arXiv:2503.24209v3 Announce Type: replace 
Abstract: In this work, we construct optimal low-rank approximations for the Gaussian posterior distribution in linear Gaussian inverse problems with possibly infinite-dimensional separable Hilbert parameter spaces and finite-dimensional data spaces. We consider different approximation families for the posterior. We first consider approximate posteriors in which the means vary among a class of either structure-preserving or structure-ignoring low-rank transformations of the data, and in which the posterior covariance is kept fixed. We give necessary and sufficient conditions for these approximating posteriors to be equivalent to the exact posterior, for all possible realisations of the data simultaneously. For such approximations, we measure approximation error with the Kullback-Leibler, R\'enyi and Amari $\alpha$-divergences for $\alpha\in(0,1)$, and with the Hellinger distance, all averaged over the data distribution. With these losses, we find the optimal approximations and formulate an equivalent condition for their uniqueness, extending the work in finite dimensions of Spantini et al. (SIAM J. Sci. Comput. 2015). We then consider joint low-rank approximation of the mean and covariance. For the reverse Kullback-Leibler divergence, we show that the separate optimal approximations of the mean and of the covariance can be combined to yield an optimal joint approximation of the mean and covariance. In addition, we interpret the joint approximation with the optimal structure-ignoring approximate mean in terms of an optimal projector in parameter space, showing this approximation amounts to solving a Bayesian inverse problem with projected forward model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24209v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Carere, Han Cheng Lie</dc:creator>
    </item>
    <item>
      <title>Understanding Aggregations of Proper Learners in Multiclass Classification</title>
      <link>https://arxiv.org/abs/2410.22749</link>
      <description>arXiv:2410.22749v2 Announce Type: replace-cross 
Abstract: Multiclass learnability is known to exhibit a properness barrier: there are learnable classes which cannot be learned by any proper learner. Binary classification faces no such barrier for learnability, but a similar one for optimal learning, which can in general only be achieved by improper learners. Fortunately, recent advances in binary classification have demonstrated that this requirement can be satisfied using aggregations of proper learners, some of which are strikingly simple. This raises a natural question: to what extent can simple aggregations of proper learners overcome the properness barrier in multiclass classification?
  We give a positive answer to this question for classes which have finite Graph dimension, $d_G$. Namely, we demonstrate that the optimal binary learners of Hanneke, Larsen, and Aden-Ali et al. (appropriately generalized to the multiclass setting) achieve sample complexity $O\left(\frac{d_G + \ln(1 / \delta)}{\epsilon}\right)$. This forms a strict improvement upon the sample complexity of ERM. We complement this with a lower bound demonstrating that for certain classes of Graph dimension $d_G$, majorities of ERM learners require $\Omega \left( \frac{d_G + \ln(1 / \delta)}{\epsilon}\right)$ samples. Furthermore, we show that a single ERM requires $\Omega \left(\frac{d_G \ln(1 / \epsilon) + \ln(1 / \delta)}{\epsilon}\right)$ samples on such classes, exceeding the lower bound of Daniely et al. (2015) by a factor of $\ln(1 / \epsilon)$. For multiclass learning in full generality -- i.e., for classes of finite DS dimension but possibly infinite Graph dimension -- we give a strong refutation to these learning strategies, by exhibiting a learnable class which cannot be learned to constant error by any aggregation of a finite number of proper learners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22749v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Julian Asilis, Mikael M{\o}ller H{\o}gsgaard, Grigoris Velegkas</dc:creator>
    </item>
    <item>
      <title>Fast Tensor Completion via Approximate Richardson Iteration</title>
      <link>https://arxiv.org/abs/2502.09534</link>
      <description>arXiv:2502.09534v2 Announce Type: replace-cross 
Abstract: We study tensor completion (TC) through the lens of low-rank tensor decomposition (TD). Many TD algorithms use fast alternating minimization methods to solve highly structured linear regression problems at each step (e.g., for CP, Tucker, and tensor-train decompositions). However, such algebraic structure is often lost in TC regression problems, making direct extensions unclear. This work proposes a novel lifting method for approximately solving TC regression problems using structured TD regression algorithms as blackbox subroutines, enabling sublinear-time methods. We analyze the convergence rate of our approximate Richardson iteration-based algorithm, and our empirical study shows that it can be 100x faster than direct methods for CP completion on real-world tensors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09534v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 42nd International Conference on Machine Learning (ICML 2025)</arxiv:journal_reference>
      <dc:creator>Mehrdad Ghadiri, Matthew Fahrbach, Yunbum Kook, Ali Jadbabaie</dc:creator>
    </item>
    <item>
      <title>High-dimensional Longitudinal Inference via a De-sparsified Dantzig-Selector</title>
      <link>https://arxiv.org/abs/2508.07498</link>
      <description>arXiv:2508.07498v2 Announce Type: replace-cross 
Abstract: In this paper, we consider statistical inference with generalized linear models in high dimensions under a longitudinal clustered data framework. Specifically, we propose a de-sparsified version of an initial Dantzig-type regularized estimator in regression settings and provide theoretical justification for both linear and generalized linear models. We present extensive numerical simulations demonstrating the effectiveness of our method for continuous and binary data. For continuous outcomes under linear models, we show that our estimator asymptotically attains an appropriate efficiency bound when the correlation structure is correctly specified. We conclude with an application of our method to a well-established genetics dataset, with bacterial riboflavin production as the outcome of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07498v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Huey</dc:creator>
    </item>
  </channel>
</rss>
