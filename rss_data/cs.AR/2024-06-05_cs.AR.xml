<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Jun 2024 01:49:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Demystifying Platform Requirements for Diverse LLM Inference Use Cases</title>
      <link>https://arxiv.org/abs/2406.01698</link>
      <description>arXiv:2406.01698v1 Announce Type: new 
Abstract: Large language models (LLMs) have shown remarkable performance across a wide range of applications, often outperforming human experts. However, deploying these parameter-heavy models efficiently for diverse inference use cases requires carefully designed hardware platforms with ample computing, memory, and network resources. With LLM deployment scenarios and models evolving at breakneck speed, the hardware requirements to meet SLOs remains an open research question. In this work, we present an analytical tool, GenZ, to study the relationship between LLM inference performance and various platform design parameters. Our analysis provides insights into configuring platforms for different LLM workloads and use cases. We quantify the platform requirements to support SOTA LLMs models like LLaMA and GPT-4 under diverse serving settings. Furthermore, we project the hardware capabilities needed to enable future LLMs potentially exceeding hundreds of trillions of parameters. The trends and insights derived from GenZ can guide AI engineers deploying LLMs as well as computer architects designing next-generation hardware accelerators and platforms. Ultimately, this work sheds light on the platform design considerations for unlocking the full potential of large language models across a spectrum of applications. The source code is available at https://github.com/abhibambhaniya/GenZ-LLM-Analyzer .</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01698v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Abhimanyu Bambhaniya, Ritik Raj, Geonhwa Jeong, Souvik Kundu, Sudarshan Srinivasan, Midhilesh Elavazhagan, Madhu Kumar, Tushar Krishna</dc:creator>
    </item>
    <item>
      <title>LMB: Augmenting PCIe Devices with CXL-Linked Memory Buffer</title>
      <link>https://arxiv.org/abs/2406.02039</link>
      <description>arXiv:2406.02039v1 Announce Type: new 
Abstract: PCIe devices, such as SSDs and GPUs, are pivotal in modern data centers, and their value is set to grow amidst the emergence of AI and large models. However, these devices face onboard DRAM shortage issue due to internal space limitation, preventing accommodation of sufficient DRAM modules alongside flash or GPU processing chips. Current solutions either curb device-internal memory usage or supplement slower non-DRAM mediums, prove inadequate or performance-compromising. This paper introduces the Linked Memory Buffer (LMB), a scalable solution utilizing the CXL memory expander to tackle device onboard memory deficiencies. The low-latency of CXL enables LMB to utilize emerging DRAM memory expander to efficiently supplement device onboard DRAM with minimal impact on performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02039v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiapin Wang, Xiangping Zhang, Chenlei Tang, Xiang Chen, Tao Lu</dc:creator>
    </item>
    <item>
      <title>Fast and Practical Strassen's Matrix Multiplication using FPGAs</title>
      <link>https://arxiv.org/abs/2406.02088</link>
      <description>arXiv:2406.02088v1 Announce Type: new 
Abstract: Matrix multiplication is a cornerstone operation in a wide array of scientific fields, including machine learning and computer graphics. The standard algorithm for matrix multiplication has a complexity of $\mathcal{O}(n^3)$ for $n\times n$ matrices. Strassen's algorithm improves this to $\mathcal{O}(n^{2.807})$, but its practicality is limited for small to medium matrix sizes due to the large number of additions it introduces. This paper presents a novel FPGA-based implementation of Strassen's algorithm that achieves superior speed over an optimized General Matrix Multiply (GeMM) implementation for matrices as small as $n=256$. Our design, tested extensively on two high-performance FPGA accelerators (Alveo U50 and U280) across various data types, matches or surpasses the performance of a highly optimized baseline across a range of matrix sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02088v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Afzal Ahmad, Linfeng Du, Wei Zhang</dc:creator>
    </item>
    <item>
      <title>Resource Optimized Quantum Squaring Circuit</title>
      <link>https://arxiv.org/abs/2406.01875</link>
      <description>arXiv:2406.01875v1 Announce Type: cross 
Abstract: Quantum squaring operation is a useful building block in implementing quantum algorithms such as linear regression, regularized least squares algorithm, order-finding algorithm, quantum search algorithm, Newton Raphson division, Euclidean distance calculation, cryptography, and in finding roots and reciprocals. Quantum circuits could be made fault-tolerant by using error correcting codes and fault-tolerant quantum gates (such as the Clifford + T-gates). However, the T-gate is very costly to implement. Two qubit gates (such as the CNOT-gate) are more prone to noise errors than single qubit gates. Consequently, in order to realize reliable quantum algorithms, the quantum circuits should have a low T-count and CNOT-count. In this paper, we present a novel quantum integer squaring architecture optimized for T-count, CNOT-count, T-depth, CNOT-depth, and $KQ_T$ that produces no garbage outputs. To reduce costs, we use a novel approach for arranging the generated partial products that allows us to reduce the number of adders by 50%. We also use the resource efficient logical-AND gate and uncomputation gate shown in [1] to further save resources. The proposed quantum squaring circuit sees an asymptotic reduction of 66.67% in T-count, 50% in T-depth, 29.41% in CNOT-count, 42.86% in CNOT-depth, and 25% in KQ T with respect to Thapliyal et al. [2]. With respect to Nagamani et al. [3] the design sees an asymptotic reduction of 77.27% in T-count, 68.75% in T-depth, 50% in CNOT-count, 61.90% in CNOT-depth, and 6.25% in the $KQ_T$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01875v1</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Afrin Sultana, Edgard Mu\~noz-Coreas</dc:creator>
    </item>
  </channel>
</rss>
