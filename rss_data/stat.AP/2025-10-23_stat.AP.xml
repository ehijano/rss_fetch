<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 23 Oct 2025 04:00:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Multiple Imputation for Small, Extremely High Efficacy Clinical Trials with Binary Endpoints</title>
      <link>https://arxiv.org/abs/2510.19011</link>
      <description>arXiv:2510.19011v1 Announce Type: new 
Abstract: There has been an increasing interest in using cell and gene therapy (CGT) to treat/cure difficult diseases. The hallmark of CGT trials are the small sample size and extremely high efficacy. Due to the innovation and novelty of such therapies, when there is missing data, more scrutiny is exercised, and regulators often request for missing data handling strategy when missing data occurs. Often, multiple imputation (MI) will be used. MI for continuous endpoint is well established but literature of MI for binary endpoint is lacking. In this work, we compare and develop 3 new methods to handle missing data using MI for binary endpoints when the sample size is small and efficacy extremely high. The parameter of interest is population proportion of success. We show that our proposed methods performed well and produced good 95% coverage. We also applied our methods to an actual clinical study, the Clinical Islet Transplantation (CIT) Protocol 07, conducted by National Institutes of Health (NIH).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19011v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yaoyuan Vincent Tan, Gang Xu, Chenkun Wang</dc:creator>
    </item>
    <item>
      <title>Simulation-Guided Planning of a Target Trial Emulated Cluster Randomized Trial for Mass Small-Quantity Lipid Nutrient Supplementation Combined with Expanded Program on Immunization in Rural Niger</title>
      <link>https://arxiv.org/abs/2510.19077</link>
      <description>arXiv:2510.19077v1 Announce Type: new 
Abstract: While target trial emulation (TTE) is increasingly used to improve the analysis of non-randomized studies by applying trial design principles, TTE applications to emulate cluster randomized trials (RCTs) have been limited. We performed simulations to prospectively plan data collection of a non-randomized study intended to emulate a village-level cluster RCT when cluster-randomization was infeasible. The planned study will assess the impact of mass distribution of nutritional supplements embedded within an existing immunization program to improve pentavalent vaccination rates among children 12-24 months old in Niger. The design included covariate-constrained random selection of villages for outcome ascertainment at follow-up. Simulations used baseline census data on pentavalent vaccination rates and cluster-level covariates to compare the type I error rate and power of four statistical methods: beta-regression; quasi-binomial regression; inverse probability of treatment weighting (IPTW); and na\"ive Wald test. Of these methods, only IPTW and beta-regression controlled the type I error rate at 0.05, but IPTW yielded poor statistical power. Beta-regression, which showed adequate statistical power, was chosen as our primary analysis. Adopting simulation-guided design principles within TTE can enable robust planning of a group-level non-randomized study emulating a cluster RCT. Lessons from this study also apply to TTE planning of individually-RCTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19077v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rebecca K. Metcalfe, Nathaniel Dyrkton, Yichen Yan, Shomoita Alam, Susan Shepherd, Ibrahim Sana, Kevin Phelan, Jay JH Park</dc:creator>
    </item>
    <item>
      <title>Green Finance and Carbon Emissions: A Nonlinear and Interaction Analysis Using Bayesian Additive Regression Trees</title>
      <link>https://arxiv.org/abs/2510.19785</link>
      <description>arXiv:2510.19785v1 Announce Type: new 
Abstract: As a core policy tool for China in addressing climate risks, green finance plays a strategically important role in shaping carbon mitigation outcomes. This study investigates the nonlinear and interaction effects of green finance on carbon emission intensity (CEI) using Chinese provincial panel data from 2000 to 2022. The Climate Physical Risk Index (CPRI) is incorporated into the analytical framework to assess its potential role in shaping carbon outcomes. We employ Bayesian Additive Regression Trees (BART) to capture complex nonlinear relationships and interaction pathways, and use SHapley Additive exPlanations values to enhance model interpretability. Results show that the Green Finance Index (GFI) has a statistically significant inverted U-shaped effect on CEI, with notable regional heterogeneity. Contrary to expectations, CPRI does not show a significant impact on carbon emissions. Further analysis reveals that in high energy consumption scenarios, stronger green finance development contributes to lower CEI. These findings highlight the potential of green finance as an effective instrument for carbon intensity reduction, especially in energy-intensive contexts, and underscore the importance of accounting for nonlinear effects and regional disparities when designing and implementing green financial policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19785v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengxiang zhu, Riccardo Rastelli</dc:creator>
    </item>
    <item>
      <title>A Class of Markovian Self-Reinforcing Processes with Power-Law Distributions</title>
      <link>https://arxiv.org/abs/2510.19034</link>
      <description>arXiv:2510.19034v1 Announce Type: cross 
Abstract: Solar flares, email exchanges, and many natural or social systems exhibit bursty dynamics, with periods of intense activity separated by long inactivity. These patterns often follow power- law distributions in inter-event intervals or event rates. Existing models typically capture only one of these features and rely on non-local memory, which complicates analysis and mechanistic interpretation. We introduce a novel self-reinforcing point process whose event rates are governed by local, Markovian nonlinear dynamics and post-event resets. The model generates power-law tails for both inter-event intervals and event rates over a broad range of exponents observed empirically across natural and human phenomena. Compared to non-local models such as Hawkes processes, our approach is mechanistically simpler, highly analytically tractable, and also easier to simulate. We provide methods for model fitting and validation, establishing this framework as a versatile foundation for the study of bursty phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19034v1</guid>
      <category>physics.soc-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pavlo Bulanchuk, Sue Ann Koay, Sandro Romani</dc:creator>
    </item>
    <item>
      <title>Spatially Regularized Gaussian Mixtures for Clustering Spatial Transcriptomic Data</title>
      <link>https://arxiv.org/abs/2510.19108</link>
      <description>arXiv:2510.19108v1 Announce Type: cross 
Abstract: Spatial transcriptomics measures the expression of thousands of genes in a tissue sample while preserving its spatial structure. This class of technologies has enabled the investigation of the spatial variation of gene expressions and their impact on specific biological processes. Identifying genes with similar expression profiles is of utmost importance, thus motivating the development of flexible methods leveraging spatial data structure to cluster genes. Here, we propose a modeling framework for clustering observations measured over numerous spatial locations via Gaussian processes. Rather than specifying their covariance kernels as a function of the spatial structure, we use it to inform a generalized Cholesky decomposition of their precision matrices. This approach prevents issues with kernel misspecification and facilitates the estimation of a non-stationarity spatial covariance structure. Applied to spatial transcriptomic data, our model identifies gene clusters with distinctive spatial correlation patterns across tissue areas comprising different cell types, like tumoral and stromal areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19108v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrea Sottosanti, Davide Risso, Francesco Denti</dc:creator>
    </item>
    <item>
      <title>Signature Kernel Scoring Rule as Spatio-Temporal Diagnostic for Probabilistic Forecasting</title>
      <link>https://arxiv.org/abs/2510.19110</link>
      <description>arXiv:2510.19110v1 Announce Type: cross 
Abstract: Modern weather forecasting has increasingly transitioned from numerical weather prediction (NWP) to data-driven machine learning forecasting techniques. While these new models produce probabilistic forecasts to quantify uncertainty, their training and evaluation may remain hindered by conventional scoring rules, primarily MSE, which ignore the highly correlated data structures present in weather and atmospheric systems. This work introduces the signature kernel scoring rule, grounded in rough path theory, which reframes weather variables as continuous paths to encode temporal and spatial dependencies through iterated integrals. Validated as strictly proper through the use of path augmentations to guarantee uniqueness, the signature kernel provides a theoretically robust metric for forecast verification and model training. Empirical evaluations through weather scorecards on WeatherBench 2 models demonstrate the signature kernel scoring rule's high discriminative power and unique capacity to capture path-dependent interactions. Following previous demonstration of successful adversarial-free probabilistic training, we train sliding window generative neural networks using a predictive-sequential scoring rule on ERA5 reanalysis weather data. Using a lightweight model, we demonstrate that signature kernel based training outperforms climatology for forecast paths of up to fifteen timesteps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19110v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Archer Dodson, Ritabrata Dutta</dc:creator>
    </item>
    <item>
      <title>Efficient scenario analysis in real-time Bayesian election forecasting via sequential meta-posterior sampling</title>
      <link>https://arxiv.org/abs/2510.19133</link>
      <description>arXiv:2510.19133v1 Announce Type: cross 
Abstract: Bayesian aggregation lets election forecasters combine diverse sources of information, such as state polls and economic and political indicators: as in our collaboration with The Economist magazine. However, the demands of real-time posterior updating, model checking, and communication introduce practical methodological challenges. In particular, sensitivity and scenario analysis help trace forecast shifts to model assumptions and understand model behavior. Yet, under standard Markov chain Monte Carlo, even small tweaks to the model (e.g., in priors, data, hyperparameters) require full refitting, making such real-time analysis computationally expensive. To overcome the bottleneck, we introduce a meta-modeling strategy paired with a sequential sampling scheme; by traversing posterior meta-models, we enable real-time inference and structured scenario and sensitivity analysis without repeated refitting. In a back-test of the model, we show substantial computational gains and uncover non-trivial sensitivity patterns. For example, forecasts remain responsive to prior confidence in fundamentals-based forecasts, but less so to random walk scale; these help clarify the relative influence of polling data versus structural assumptions. Code is available at https://github.com/geonhee619/SMC-Sense.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19133v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geonhee Han, Andrew Gelman, Aki Vehtari</dc:creator>
    </item>
    <item>
      <title>Topology of Currencies: Persistent Homology for FX Co-movements: A Comparative Clustering Study</title>
      <link>https://arxiv.org/abs/2510.19306</link>
      <description>arXiv:2510.19306v1 Announce Type: cross 
Abstract: This study investigates whether Topological Data Analysis (TDA) can provide additional insights beyond traditional statistical methods in clustering currency behaviours. We focus on the foreign exchange (FX) market, which is a complex system often exhibiting non-linear and high-dimensional dynamics that classical techniques may not fully capture. We compare clustering results based on TDA-derived features versus classical statistical features using monthly logarithmic returns of 13 major currency exchange rates (all against the euro). Two widely-used clustering algorithms, \(k\)-means and Hierarchical clustering, are applied on both types of features, and cluster quality is evaluated via the Silhouette score and the Calinski-Harabasz index. Our findings show that TDA-based feature clustering produces more compact and well-separated clusters than clustering on traditional statistical features, particularly achieving substantially higher Calinski-Harabasz scores. However, all clustering approaches yield modest Silhouette scores, underscoring the inherent difficulty of grouping FX time series. The differing cluster compositions under TDA vs. classical features suggest that TDA captures structural patterns in currency co-movements that conventional methods might overlook. These results highlight TDA as a valuable complementary tool for analysing financial time series, with potential applications in risk management where understanding structural co-movements is crucial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19306v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pattravadee de Favereau de Jeneret, Ioannis Diamantis</dc:creator>
    </item>
    <item>
      <title>Robust Rank Estimation for Noisy Matrices</title>
      <link>https://arxiv.org/abs/2510.19583</link>
      <description>arXiv:2510.19583v1 Announce Type: cross 
Abstract: Estimating the true rank of a noisy data matrix is a fundamental problem underlying techniques such as principal component analysis, matrix completion, etc. Existing rank estimation criteria, including information-based and cross-validation methods, are either highly sensitive to outliers or computationally demanding when combined with robust estimators. This paper proposes a new criterion, the Divergence Information Criterion for Matrix Rank (DICMR), that achieves both robustness and computational simplicity. Derived from the density power divergence framework, DICMR inherits the robustness properties while being computationally very simple. We provide asymptotic bounds on its overestimation and underestimation probabilities, and demonstrate first-order B-robustness of the criteria. Extensive simulations show that DICMR delivers accuracy comparable to the robustified cross-validation methods, but with far lower computational cost. We also showcase a real-data application to microarray imputation to further demonstrate its practical utility, outperforming several state-of-the-art algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19583v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhrajyoty Roy, Abhik Ghosh, Ayanendranath Basu</dc:creator>
    </item>
    <item>
      <title>Network Contagion Dynamics in European Banking: A Navier-Stokes Framework for Systemic Risk Assessment</title>
      <link>https://arxiv.org/abs/2510.19630</link>
      <description>arXiv:2510.19630v1 Announce Type: cross 
Abstract: This paper develops a continuous functional framework for analyzing contagion dynamics in financial networks, extending the Navier-Stokes-based approach to network-structured spatial processes. We model financial distress propagation as a diffusion process on weighted networks, deriving a network diffusion equation from first principles that predicts contagion decay depends on the network's algebraic connectivity through the relation $\kappa = \sqrt{\lambda_2/D}$, where $\lambda_2$ is the second-smallest eigenvalue of the graph Laplacian and $D$ is the diffusion coefficient. Applying this framework to European banking data from the EBA stress tests (2018, 2021, 2023), we estimate interbank exposure networks using maximum entropy methods and track the evolution of systemic risk through the COVID-19 crisis. Our key finding is that network connectivity declined by 45\% from 2018 to 2023, implying a 26\% reduction in the contagion decay parameter. Difference-in-differences analysis reveals this structural change was driven by regulatory-induced deleveraging of systemically important banks, which experienced differential asset reductions of 17\% relative to smaller institutions. The networks exhibit lognormal rather than scale-free degree distributions, suggesting greater resilience than previously assumed in the literature. Extensive robustness checks across parametric and non-parametric estimation methods confirm declining systemic risk, with cross-method correlations exceeding 0.95. These findings demonstrate that post-COVID-19 regulatory reforms effectively reduced network interconnectedness and systemic vulnerability in the European banking system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19630v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Causal Feedback Discovery using Convergence Cross Mapping on Sea Ice Data</title>
      <link>https://arxiv.org/abs/2505.09001</link>
      <description>arXiv:2505.09001v5 Announce Type: replace 
Abstract: Identifying causal relationships in climate systems remains challenging due to nonlinear, coupled dynamics that limit the effectiveness of linear and stochastic causal discovery approaches. This study benchmarks Convergence Cross Mapping (CCM) against Granger causality, PCMCI, and VarLiNGAM using both synthetic datasets with ground truth causal links and 41 years of Arctic climate data (1979--2021). Unlike stochastic models that rely on autoregressive residual dependence, CCM leverages Takens' state-space reconstruction and delay-embedding to reconstruct attractor manifolds from time series. Cross mapping between reconstructed manifolds exploits deterministic signatures of causation, enabling the detection of weak and bidirectional causal links that linear models fail to resolve. Results demonstrate that CCM achieves higher specificity and fewer false positives on synthetic benchmarks, while maintaining robustness under observational noise and limited sample lengths. On Arctic data, CCM reveals significant causal interactions between sea ice extent and atmospheric variables like specific humidity, longwave radiation, and surface temperature with a $p$-value of $0.009$, supporting ice-albedo feedbacks and moisture-radiation couplings central to Arctic amplification. In contrast, stochastic approaches miss these nonlinear dependencies or infer spurious causal relations. This work establishes CCM as a robust causal inference tool for nonlinear climate dynamics and provides the first systematic benchmarking framework for method selection in climate research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09001v5</guid>
      <category>stat.AP</category>
      <category>physics.ao-ph</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francis Nji, Seraj Al Mahmud Mostafa, Jianwu Wang</dc:creator>
    </item>
    <item>
      <title>A systematic review of sample size determination in Bayesian randomized clinical trials: full Bayesian methods are rarely used</title>
      <link>https://arxiv.org/abs/2505.15735</link>
      <description>arXiv:2505.15735v2 Announce Type: replace 
Abstract: Utilizing Bayesian methods in clinical trials has become increasingly popular, as they can incorporate historical data and expert opinions into the design and allow for smaller sample sizes to reduce costs while providing reliable and robust statistical results. Sample size determination (SSD) is a key aspect of clinical trial design and various methods for Bayesian sample size determination are available. However, it is unclear how these methods are being used in practice. A systematic literature review was conducted to understand how sample sizes for Bayesian randomized clinical trials (RCTs) are determined and inform the design of future Bayesian trials. We searched five databases in May 2023, and updated in January 2025, including efficacy RCTs in humans which utilized a Bayesian framework for the primary data analysis, published in English, and enrolled participants between 2009 and 2024. The literature search produced 19,182 records, of which 105 studies were selected for data extraction. Results show that the most common method for SSD in Bayesian RCTs was a hybrid approach in which elements of Bayesian and frequentist theory are combined. Many RCTs did not provide a justification for SSD, while fully Bayesian methods were rarely used in practice, despite significant theoretical development. Our review also revealed a lack of standardized reporting, making it challenging to review the SSD. The CONSORT statement for reporting RCTs states that sample size calculations must be reported, which was poorly adhered to. Among RCTs that reported SSD, relevant information was frequently omitted from the reports and discussed in poorly structured supplementary materials. Thus, there is a critical need for greater transparency, standardization and translation of relevant methodology in Bayesian RCTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15735v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yanara Marks, Jessie Cunningham, Arlene Jiang, Linke Li, Yi-Shu Lin, Abigail McGrory, Yongdong Ouyang, Nam-Anh Tran, Yuning Wang, Anna Heath</dc:creator>
    </item>
    <item>
      <title>How to measure the uncertainty of a tournament draw: The case of European football's Champions League</title>
      <link>https://arxiv.org/abs/2507.15320</link>
      <description>arXiv:2507.15320v4 Announce Type: replace 
Abstract: According to recent empirical studies, the group draw of major sports tournaments can imply a high level of uncertainty, and some lucky teams enjoy an unfair advantage over the other teams. We propose a novel technique to quantify this draw uncertainty, which, arguably, has an optimal level of zero. Our simulation-based approach requires generating a high number of random draws to compute the variance of qualifying probabilities for each team. The method is applied to compare draw uncertainty in the former group stage and the current incomplete round-robin league phase of the UEFA Champions League. We also break down the impact of the 2024/25 reform into various components. The new format is found to decrease draw uncertainty; the reduction can mainly be attributed to the inaccurate seeding system used by UEFA. Our results reveal that the primary benefit of an incomplete round-robin tournament compared to the standard group stage lies in the robustness of its draw uncertainty to the seeding of the teams, which is a crucial aspect of fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15320v4</guid>
      <category>stat.AP</category>
      <category>math.OC</category>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o, Andr\'as Gyimesi, Dries Goossens, Karel Devriesere, Roel Lambers, Frits Spieksma</dc:creator>
    </item>
    <item>
      <title>Survey Data Integration for Distribution Function Estimation</title>
      <link>https://arxiv.org/abs/2409.14284</link>
      <description>arXiv:2409.14284v4 Announce Type: replace-cross 
Abstract: We propose a novel CDF estimator that integrates data from probability samples with data from, potentially big, nonprobability samples. Assuming that a set of shared covariates are observed in both, while the response variable is observed only in the latter, the proposed estimator uses a survey-weighted empirical CDF of regression residuals trained on the convenience sample to estimate the CDF of the response variable. Under some assumptions, we derive the asymptotic bias and variance of our CDF estimator and show that it is asymptotically unbiased for the finite population CDF if ignorability holds. Empirical results demonstrate that the estimator performs well under model misspecification when ignorability holds, and under nonignorable sampling when the outcome model is correctly specified. Even when both assumptions fail, the residual-based estimator continues to outperform its plug-in and na\"ive counterparts, albeit with noted decreases in efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14284v4</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy Flood, Sayed Mostafa</dc:creator>
    </item>
    <item>
      <title>Bayes factor functions for testing partial correlation coefficients</title>
      <link>https://arxiv.org/abs/2503.10787</link>
      <description>arXiv:2503.10787v3 Announce Type: replace-cross 
Abstract: Partial correlation coefficients are widely applied in the social sciences to evaluate the relationship between two variables after accounting for the influence of others. In this article, we present Bayes Factor Functions (BFFs) for assessing the presence of partial correlation. BFFs represent Bayes factors derived from test statistics and are expressed as functions of a standardized effect size. While traditional frequentist methods based on $p$-values have been criticized for their inability to provide cumulative evidence in favor of the true hypothesis, Bayesian approaches are often challenged due to their computational demands and sensitivity to prior distributions. BFFs overcome these limitations and offer summaries of hypothesis tests as alternative hypotheses are varied over a range of prior distributions on standardized effects. They also enable the integration of evidence across multiple studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10787v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saptati Datta</dc:creator>
    </item>
    <item>
      <title>Adaptive Designs in Fast-Track Registration Processes for Digital Health Applications</title>
      <link>https://arxiv.org/abs/2507.04092</link>
      <description>arXiv:2507.04092v3 Announce Type: replace-cross 
Abstract: Fast-track procedures play an important role in the context of conditional registration of medical devices, such as listing processes for digital health applications. They offer the potential for earlier patient access to innovative products and involve two registration steps. The applicants can apply first for conditional registration. A successful conditional registration provides a limited funding or approval period and time to prepare the application for permanent registration (the second registration step). For conditional registration, products have to fulfill only a part of the requirements necessary for permanent registration. There is interest in valid and efficient study designs for fast-track procedures. This will be addressed in this paper. A motivating example is the German fast-track registration process of digital health applications (DiGA) for reimbursement by statutory health insurances. The main focus of the paper is the systematic statistical investigation of the utility of adaptive designs in the context of fast-track registration processes like the DiGA fast-track. We demonstrate that, in most cases, such designs are much more efficient than the current standard of two separate studies. A careful statistical discussion of the registration requirements and their consequences is also included. The results are based on numerical calculations supported by mathematical arguments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04092v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Liane Kluge, Werner Brannath</dc:creator>
    </item>
    <item>
      <title>Loss Functions for Detecting Outliers in Panel Data</title>
      <link>https://arxiv.org/abs/2509.07014</link>
      <description>arXiv:2509.07014v2 Announce Type: replace-cross 
Abstract: The detection of outliers is of critical importance in the assurance of data quality. Outliers may exist in observed data or in data derived from these observed data, such as estimates and forecasts. An outlier may indicate a problem with its data generation process or may simply be a true, but unusual, statement about the world. Without making any distributional assumptions, we proposes the use of loss functions to detect these outliers in panel data.
  Part I covers nonnegative data. We axiomatically derive an unsigned loss function. We then develop a signed loss function ito account for positive and negative outliers separately. In the case of nominal time we obtain an exact parametrization of the loss function. A time-invariant loss function permits the comparison of data at multiple times on the same basis. We provide several examples, including an example in which the outliers are classified by another variable.
  Part II covers data of mixed sign. Similar to Part I, we axiomatically develop unsigned and signed loss functions. We search for optimal values of the loss function parameter using graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07014v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Charles D. Coleman, Thomas Bryan</dc:creator>
    </item>
  </channel>
</rss>
