<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Jun 2025 04:00:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Sensitivity Analysis of Priors in the Bayesian Dirichlet Auto-Regressive Moving Average Model</title>
      <link>https://arxiv.org/abs/2506.13973</link>
      <description>arXiv:2506.13973v1 Announce Type: new 
Abstract: Prior choice can strongly influence Bayesian Dirichlet ARMA (B-DARMA) inference for compositional time-series. Using simulations with (i) correct lag order, (ii) overfitting, and (iii) underfitting, we assess five priors: weakly-informative, horseshoe, Laplace, mixture-of-normals, and hierarchical. With the true lag order, all priors achieve comparable RMSE, though horseshoe and hierarchical slightly reduce bias. Under overfitting, aggressive shrinkage-especially the horseshoe-suppresses noise and improves forecasts, yet no prior rescues a model that omits essential VAR or VMA terms.
  We then fit B-DARMA to daily SP 500 sector weights using an intentionally large lag structure. Shrinkage priors curb spurious dynamics, whereas weakly-informative priors magnify errors in volatile sectors. Two lessons emerge: (1) match shrinkage strength to the degree of overparameterization, and (2) prioritize correct lag selection, because no prior repairs structural misspecification. These insights guide prior selection and model complexity management in high-dimensional compositional time-series applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13973v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison Katz, Liz Medina, Robert E. Weiss</dc:creator>
    </item>
    <item>
      <title>Causal Mediation Analysis with Multiple Mediators: A Simulation Approach</title>
      <link>https://arxiv.org/abs/2506.14019</link>
      <description>arXiv:2506.14019v1 Announce Type: new 
Abstract: Analyses of causal mediation often involve exposure-induced confounders or, relatedly, multiple mediators. In such applications, researchers aim to estimate a variety of different quantities, including interventional direct and indirect effects, multivariate natural direct and indirect effects, and/or path-specific effects. This study introduces a general approach to estimating all these quantities by simulating potential outcomes from a series of distribution models for each mediator and the outcome. Building on similar methods developed for analyses with only a single mediator (Imai et al. 2010), we first outline how to implement this approach with parametric models. The parametric implementation can accommodate linear and nonlinear relationships, both continuous and discrete mediators, and many different types of outcomes. However, it depends on correct specification of each model used to simulate the potential outcomes. To address the risk of misspecification, we also introduce an alternative implementation using a novel class of nonparametric models, which leverage deep neural networks to approximate the relevant distributions without relying on strict assumptions about functional form. We illustrate both methods by reanalyzing the effects of media framing on attitudes toward immigration (Brader et al. 2008) and the effects of prenatal care on preterm birth (VanderWeele et al. 2014).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14019v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesse Zhou, Geoffrey T. Wodtke</dc:creator>
    </item>
    <item>
      <title>A generalised discrete mixture model to better capture preference heterogeneity in discrete choice data</title>
      <link>https://arxiv.org/abs/2506.14094</link>
      <description>arXiv:2506.14094v1 Announce Type: new 
Abstract: Arguably the key issue in modelling discrete choice data is capturing preference heterogeneity. This can be through observed characteristics, and/or using techniques for capturing random heterogeneity across respondents. On the latter, in health economics, the two main approaches are the mixed multinomial logit (MMNL) and the latent class (LC) model. In this paper, we revisit the discrete mixture (DM) model as a third alternative to these. The DM model is similar to LC but allows for any combination of preferences across attributes, rather than grouping preferences as is the case in LC. We next develop a generalised discrete mixture (GDM) model. Additional boosting parameters in the class allocation component allow the model to collapse to a standard DM or LC structure as best fits the data at hand. This means that the model, by definition, performs at least as well as the best of a standard DM and a LC model; or better than both. Additional benefits include that it (a) allows the data to tell us the underlying correlations of preferences, (b) does not rely on distributions as is the case for mixed logit models, meaning estimation times are reduced and it does not require assumptions on the distribution of preferences. Exercises on simulated data show the unlikely conditions under which a LC model would be preferred to a DM. The convention of labelling latent classes, we believe, is questionable in many cases. The GDM is suitable in all cases. We show in empirical data that the GDM substantially outperforms LC models, granting a more detailed depiction of respondents' preferences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14094v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Thomas O. Hancock, John Buckell</dc:creator>
    </item>
    <item>
      <title>A Robust Nonparametric Framework for Detecting Repeated Spatial Patterns</title>
      <link>https://arxiv.org/abs/2506.14103</link>
      <description>arXiv:2506.14103v1 Announce Type: new 
Abstract: Identifying spatially contiguous clusters and repeated spatial patterns (RSP) characterized by similar underlying distributions that are spatially apart is a key challenge in modern spatial statistics. Existing constrained clustering methods enforce spatial contiguity but are limited in their ability to identify RSP. We propose a novel nonparametric framework that addresses this limitation by combining constrained clustering with a post-clustering reassigment step based on the maximum mean discrepancy (MMD) statistic. We employ a block permutation strategy within each cluster that preserves local attribute structure when approximating the null distribution of the MMD. We also show that the MMD$^2$ statistic is asymptotically consistent under second-order stationarity and spatial mixing conditions. This two-stage approach enables the detection of clusters that are both spatially distant and similar in distribution. Through simulation studies that vary spatial dependence, cluster sizes, shapes, and multivariate dimensionality, we demonstrate the robustness of our proposed framework in detecting RSP. We further illustrate its applicability through an analysis of spatial proteomics data from patients with triple-negative breast cancer. Overall, our framework presents a methodological advancement in spatial clustering, offering a flexible and robust solution for spatial datasets that exhibit repeated patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14103v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajitha Senanayake, Pratheepa Jeganathan</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for Initial Heat States with Gaussian Series Priors</title>
      <link>https://arxiv.org/abs/2506.14241</link>
      <description>arXiv:2506.14241v1 Announce Type: new 
Abstract: We consider the statistical linear inverse problem of recovering the unknown initial heat state from noisy interior measurements over an inhomogeneous domain of the solution to the heat equation at a fixed time instant. We employ nonparametric Bayesian procedures with Gaussian series priors defined on the Dirichlet-Laplacian eigenbasis, yielding convenient conjugate posterior distributions with explicit expressions for posterior inference. We review recent theoretical results that provide asymptotic performance guarantees (in the large sample size limit) for the resulting posterior-based point estimation and uncertainty quantification. We further provide an implementation of the approach, and illustrate it via a numerical simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14241v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Giordano</dc:creator>
    </item>
    <item>
      <title>Non-Parametric Goodness-of-Fit Tests Using Tsallis Entropy Measures</title>
      <link>https://arxiv.org/abs/2506.14242</link>
      <description>arXiv:2506.14242v1 Announce Type: new 
Abstract: In this paper, we investigate new procedures for statistical testing based on Tsallis entropy, a parametric generalization of Shannon entropy. Focusing on multivariate generalized Gaussian and $q$-Gaussian distributions, we develop entropy-based goodness-of-fit tests based on maximum entropy formulations and nearest neighbour entropy estimators. Furthermore, we propose a novel iterative approach for estimating the shape parameters of the distributions, which is crucial for practical inference. This method extends entropy estimation techniques beyond traditional approaches, improving precision in heavy-tailed and non-Gaussian contexts. The numerical experiments are demonstrative of the statistical properties and convergence behaviour of the proposed tests. These findings are important for disciplines that require robust distributional tests, such as machine learning, signal processing, and information theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14242v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mehmet S{\i}dd{\i}k \c{C}ad{\i}rc{\i}</dc:creator>
    </item>
    <item>
      <title>Network Cross-Validation for Nested Models by Edge-Sampling: Selection Consistency</title>
      <link>https://arxiv.org/abs/2506.14244</link>
      <description>arXiv:2506.14244v1 Announce Type: new 
Abstract: In the network literature, a wide range of statistical models have been proposed to exploit structural patterns in the data. Therefore, model selection between different models is a fundamental problem. Cross-validation is a powerful candidate to solve this problem, and Li et al. have already proposed an edge-sampling procedure to choose the number of communities in the block model framework. In this paper, we propose a penalized edge-sampling cross-validation framework for nested network model selection, adding a penalty term to deal with overfitting. We give a general framework applicable in various settings, giving a theoretical guarantee of consistency of the model selection procedure for distinguishing between several widely used models, including the stochastic block model (SBM), the degree-corrected stochastic block model (DCBM), and the graphon model. In summary, our work addresses the problem of model selection over a broad range of settings and fills a theoretical gap in the existing literature. Further numerical investigations will be reported in a subsequent version.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14244v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bokai Yang</dc:creator>
    </item>
    <item>
      <title>Environmental extreme risk modeling via sub-sampling block maxima</title>
      <link>https://arxiv.org/abs/2506.14556</link>
      <description>arXiv:2506.14556v1 Announce Type: new 
Abstract: This paper introduces a novel sub-sampling block maxima technique to model and characterize environmental extreme risks. We examine the relationships between block size and block maxima statistics derived from the Gaussian and generalized Pareto distributions. We introduce a weighted least square estimator for extreme value index (EVI) and evaluate its performance using simulated auto-correlated data. We employ the second moment of block maxima for plateau finding in EVI and extremal index (EI) estimation, and present the effect of EI on Kullback-Leibler divergence. The applicability of this approach is demonstrated across diverse environmental datasets, including meteorite landing mass, earthquake energy release, solar activity, and variations in Greenland's land snow cover and sea ice extent. Our method provides a sample-efficient framework, robust to temporal dependencies, that delivers actionable environmental extreme risk measures across different timescales. With its flexibility, sample efficiency, and limited reliance on subjective tuning, this approach emerges as a useful tool for environmental extreme risk assessment and management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14556v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tuoyuan Cheng, Xiao Peng, Achmad Choiruddin, Xiaogang He, Kan Chen</dc:creator>
    </item>
    <item>
      <title>Addressing Phase Discrepancies in Functional Data: A Bayesian Approach for Accurate Alignment and Smoothing</title>
      <link>https://arxiv.org/abs/2506.14650</link>
      <description>arXiv:2506.14650v1 Announce Type: new 
Abstract: In many real-world applications, functional data exhibit considerable variability in both amplitude and phase. This is especially true in biomechanical data such as the knee flexion angle dataset motivating our work, where timing differences across curves can obscure meaningful comparisons. Curves of this study also exhibit substantial variability from one another. These pronounced differences make the dataset particularly challenging to align properly without distorting or losing some of the individual curves characteristics. Our alignment model addresses these challenges by eliminating phase discrepancies while preserving the individual characteristics of each curve and avoiding distortion, thanks to its flexible smoothing component. Additionally, the model accommodates group structures through a dedicated parameter. By leveraging the Bayesian approach, the new prior on the warping parameters ensures that the resulting warping functions automatically satisfy all necessary validity conditions. We applied our model to the knee flexion dataset, demonstrating excellent performance in both smoothing and alignment, particularly in the presence of high inter-curve variability and complex group structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14650v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacopo Gardella, Raffaele Argiento, Alessandro Casa, Alessia Pini</dc:creator>
    </item>
    <item>
      <title>Quantifying Diagnostic Signal Decay in Dementia: A National Study of Medicare Hospitalization Data</title>
      <link>https://arxiv.org/abs/2506.14669</link>
      <description>arXiv:2506.14669v1 Announce Type: new 
Abstract: Background: Artificial intelligence (AI) models in healthcare depend on the fidelity of diagnostic data, yet the quality of such data is often compromised by variability in clinical documentation practices. In dementia, a condition already prone to diagnostic ambiguity, this variability may introduce systematic distortion into claims-based research and AI model development.
  Methods: We analyzed Medicare Part A hospitalization data from 2016-2018 to examine patterns of dementia-related ICD-10 code utilization across more than 3,000 U.S. counties. Using a clinically informed classification of 17 ICD-10 codes grouped into five diagnostic categories, we applied the transitive Sequential Pattern Mining (tSPM+) algorithm to model temporal usage structures. We then used matrix similarity methods to compare local diagnostic patterns to national norms and fit multivariable linear regressions to identify county-level demographic and structural correlates of divergence.
  Findings: We found substantial geographic and demographic variation in dementia-related diagnostic code usage. Non-specific codes were dominant nationwide, while Alzheimer's disease and vascular dementia codes showed pronounced variability. Temporal sequence analysis revealed consistent transitions from specific to non-specific codes, which suggest degradation of diagnostic specificity over time. Counties with higher proportions of rural residents, Medicaid-eligible patients, and Black or Hispanic dementia patients demonstrated significantly lower similarity to national usage patterns. Our model explained 38% of the variation in local-to-national diagnostic alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14669v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Federica Spoto, Jiazi Tian, Jonas H\"ugel, Daniel T. Ortega, Christine S. Ritchie, Deborah Blacker, Francesca Dominici, Chirag J. Patel, Daniel Mork, Hossein Estiri</dc:creator>
    </item>
    <item>
      <title>AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science</title>
      <link>https://arxiv.org/abs/2506.13992</link>
      <description>arXiv:2506.13992v1 Announce Type: cross 
Abstract: Large language models (LLMs) have advanced the automation of data science workflows. Yet it remains unclear whether they can critically leverage external domain knowledge as human data scientists do in practice. To answer this question, we introduce AssistedDS (Assisted Data Science), a benchmark designed to systematically evaluate how LLMs handle domain knowledge in tabular prediction tasks. AssistedDS features both synthetic datasets with explicitly known generative mechanisms and real-world Kaggle competitions, each accompanied by curated bundles of helpful and adversarial documents. These documents provide domain-specific insights into data cleaning, feature engineering, and model selection. We assess state-of-the-art LLMs on their ability to discern and apply beneficial versus harmful domain knowledge, evaluating submission validity, information recall, and predictive performance. Our results demonstrate three key findings: (1) LLMs frequently exhibit an uncritical adoption of provided information, significantly impairing their predictive performance when adversarial content is introduced, (2) helpful guidance is often insufficient to counteract the negative influence of adversarial information, and (3) in Kaggle datasets, LLMs often make errors in handling time-series data, applying consistent feature engineering across different folds, and interpreting categorical variables correctly. These findings highlight a substantial gap in current models' ability to critically evaluate and leverage expert knowledge, underscoring an essential research direction for developing more robust, knowledge-aware automated data science systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13992v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>An Luo, Xun Xian, Jin Du, Fangqiao Tian, Ganghua Wang, Ming Zhong, Shengchun Zhao, Xuan Bi, Zirui Liu, Jiawei Zhou, Jayanth Srinivasa, Ashish Kundu, Charles Fleming, Mingyi Hong, Jie Ding</dc:creator>
    </item>
    <item>
      <title>Estimation of Treatment Effects in Extreme and Unobserved Data</title>
      <link>https://arxiv.org/abs/2506.14051</link>
      <description>arXiv:2506.14051v1 Announce Type: cross 
Abstract: Causal effect estimation seeks to determine the impact of an intervention from observational data. However, the existing causal inference literature primarily addresses treatment effects on frequently occurring events. But what if we are interested in estimating the effects of a policy intervention whose benefits, while potentially important, can only be observed and measured in rare yet impactful events, such as extreme climate events? The standard causal inference methodology is not designed for this type of inference since the events of interest may be scarce in the observed data and some degree of extrapolation is necessary. Extreme Value Theory (EVT) provides methodologies for analyzing statistical phenomena in such extreme regimes. We introduce a novel framework for assessing treatment effects in extreme data to capture the causal effect at the occurrence of rare events of interest. In particular, we employ the theory of multivariate regular variation to model extremities. We develop a consistent estimator for extreme treatment effects and present a rigorous non-asymptotic analysis of its performance. We illustrate the performance of our estimator using both synthetic and semi-synthetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14051v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiyuan Tan, Jose Blanchet, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>$\beta$-integrated local depth and corresponding partitioned local depth representation</title>
      <link>https://arxiv.org/abs/2506.14108</link>
      <description>arXiv:2506.14108v1 Announce Type: cross 
Abstract: A novel local depth definition, $\beta$-integrated local depth ($\beta$-ILD), is proposed as a generalization of the local depth introduced by Paindaveine and Van Bever \cite{paindaveine2013depth}, designed to quantify the local centrality of data points. $\beta$-ILD inherits desirable properties from global data depth and remains robust across varying locality levels. A partitioning approach for $\beta$-ILD is introduced, leading to the construction of a matrix that quantifies the contribution of one point to another's local depth, providing a new interpretable measure of local centrality. These concepts are applied to classification and outlier detection tasks, demonstrating significant improvements in the performance of depth-based algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14108v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siyi Wang, Alexandre Leblanc, Paul D. McNicholas</dc:creator>
    </item>
    <item>
      <title>Adjustment for Confounding using Pre-Trained Representations</title>
      <link>https://arxiv.org/abs/2506.14329</link>
      <description>arXiv:2506.14329v1 Announce Type: cross 
Abstract: There is growing interest in extending average treatment effect (ATE) estimation to incorporate non-tabular data, such as images and text, which may act as sources of confounding. Neglecting these effects risks biased results and flawed scientific conclusions. However, incorporating non-tabular data necessitates sophisticated feature extractors, often in combination with ideas of transfer learning. In this work, we investigate how latent features from pre-trained neural networks can be leveraged to adjust for sources of confounding. We formalize conditions under which these latent features enable valid adjustment and statistical inference in ATE estimation, demonstrating results along the example of double machine learning. We discuss critical challenges inherent to latent feature learning and downstream parameter estimation arising from the high dimensionality and non-identifiability of representations. Common structural assumptions for obtaining fast convergence rates with additive or sparse linear models are shown to be unrealistic for latent features. We argue, however, that neural networks are largely insensitive to these issues. In particular, we show that neural networks can achieve fast convergence rates by adapting to intrinsic notions of sparsity and dimension of the learning problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14329v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rickmer Schulte, David R\"ugamer, Thomas Nagler</dc:creator>
    </item>
    <item>
      <title>A statistical framework for dynamic cognitive diagnosis in digital learning environments</title>
      <link>https://arxiv.org/abs/2506.14531</link>
      <description>arXiv:2506.14531v1 Announce Type: cross 
Abstract: Reading is foundational for educational, employment, and economic outcomes, but a persistent proportion of students globally struggle to develop adequate reading skills. Some countries promote digital tools to support reading development, alongside regular classroom instruction. Such tools generate rich log data capturing students' behaviour and performance. This study proposes a dynamic cognitive diagnostic modeling (CDM) framework based on restricted latent class models to trace students' time-varying skills mastery using log files from digital tools. Unlike traditional CDMs that require expert-defined skill-item mappings (Q-matrix), our approach jointly estimates the Q-matrix and latent skill profiles, integrates log-derived covariates (e.g., reattempts, response times, counts of mastered items) and individual characteristics, and models transitions in mastery using a Bayesian estimation approach. Applied to real-world data, the model demonstrates practical value in educational settings by effectively uncovering individual skill profiles and the skill-item mappings. Simulation studies confirm robust recovery of Q-matrix structures and latent profiles with high accuracy under varied sample sizes, item counts and different sparsity of Q-matrices. The framework offers a data-driven, time-dependent restricted latent class modeling approach to understanding early reading development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14531v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yawen Ma, Anastasia Ushakova, Kate Cain, Gabriel Wallin</dc:creator>
    </item>
    <item>
      <title>Simulation study to evaluate when Plasmode simulation is superior to parametric simulation in estimating the mean squared error of the least squares estimator in linear regression</title>
      <link>https://arxiv.org/abs/2312.04077</link>
      <description>arXiv:2312.04077v4 Announce Type: replace 
Abstract: Simulation is a crucial tool for the evaluation and comparison of statistical methods. How to design fair and neutral simulation studies is therefore of great interest for researchers developing new methods and practitioners confronted with the choice of the most suitable method. The term simulation usually refers to parametric simulation, that is, computer experiments using artificial data made up of pseudo-random numbers. Plasmode simulation, that is, computer experiments using the combination of resampling feature data from a real-life dataset and generating the target variable with a known user-selected outcome-generating model (OGM), is an alternative that is often claimed to produce more realistic data. We compare parametric and Plasmode simulation for the example of estimating the mean squared error (MSE) of the least squares estimator (LSE) in linear regression. If the true underlying data-generating process (DGP) and the OGM were known, parametric simulation would obviously be the best choice in terms of estimating the MSE well. However, in reality, both are usually unknown, so researchers have to make assumptions: in Plasmode simulation for the OGM, in parametric simulation for both DGP and OGM. Most likely, these assumptions do not exactly reflect the truth. Here, we aim to find out how assumptions deviating from the true DGP and the true OGM affect the performance of parametric and Plasmode simulations in the context of MSE estimation for the LSE and in which situations which simulation type is preferable. Our results suggest that the preferable simulation method depends on many factors, including the number of features, and on how and to what extent the assumptions of a parametric simulation differ from the true DGP. Also, the resampling strategy used for Plasmode influences the results. In particular, subsampling with a small sampling proportion can be recommended.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04077v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1371/journal.pone.0299989</arxiv:DOI>
      <arxiv:journal_reference>PLOS ONE (2024)</arxiv:journal_reference>
      <dc:creator>Marieke Stolte, Nicholas Schreck, Alla Slynko, Maral Saadati, Axel Benner, J\"org Rahnenf\"uhrer, Andrea Bommert</dc:creator>
    </item>
    <item>
      <title>Methods for Quantifying Dataset Similarity: a Review, Taxonomy and Comparison</title>
      <link>https://arxiv.org/abs/2312.04078</link>
      <description>arXiv:2312.04078v3 Announce Type: replace 
Abstract: Quantifying the similarity between datasets has widespread applications in statistics and machine learning. The performance of a predictive model on novel datasets, referred to as generalizability, depends on how similar the training and evaluation datasets are. Exploiting or transferring insights between similar datasets is a key aspect of meta-learning and transfer-learning. In simulation studies, the similarity between distributions of simulated datasets and real datasets, for which the performance of methods is assessed, is crucial. In two- or $k$-sample testing, it is checked, whether the underlying distributions of two or more datasets coincide.
  Extremely many approaches for quantifying dataset similarity have been proposed in the literature. We examine more than 100 methods and provide a taxonomy, classifying them into ten classes. In an extensive review of these methods the main underlying ideas, formal definitions, and important properties are introduced.
  We compare the 118 methods in terms of their applicability, interpretability, and theoretical properties, in order to provide recommendations for selecting an appropriate dataset similarity measure based on the specific goal of the dataset comparison and on the properties of the datasets at hand. An online tool facilitates the choice of the appropriate dataset similarity measure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04078v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1214/24-SS149</arxiv:DOI>
      <arxiv:journal_reference>Statist. Surv. 18, 163 - 298, 2024</arxiv:journal_reference>
      <dc:creator>Marieke Stolte, Franziska Kappenberg, J\"org Rahnenf\"uhrer, Andrea Bommert</dc:creator>
    </item>
    <item>
      <title>Root cause discovery via permutations and Cholesky decomposition</title>
      <link>https://arxiv.org/abs/2410.12151</link>
      <description>arXiv:2410.12151v4 Announce Type: replace 
Abstract: This work is motivated by the following problem: Can we identify the disease-causing gene in a patient affected by a monogenic disorder? This problem is an instance of root cause discovery. In particular, we aim to identify the intervened variable in one interventional sample using a set of observational samples as reference. We consider a linear structural equation model where the causal ordering is unknown. We begin by examining a simple method that uses squared z-scores and characterize the conditions under which this method succeeds and fails, showing that it generally cannot identify the root cause. We then prove, without additional assumptions, that the root cause is identifiable even if the causal ordering is not. Two key ingredients of this identifiability result are the use of permutations and the Cholesky decomposition, which allow us to exploit an invariant property across different permutations to discover the root cause. Furthermore, we characterize permutations that yield the correct root cause and, based on this, propose a valid method for root cause discovery. We also adapt this approach to high-dimensional settings. Finally, we evaluate the performance of our methods through simulations and apply the high-dimensional method to discover disease-causing genes in the gene expression dataset that motivates this work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12151v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jinzhou Li, Benjamin B. Chu, Ines F. Scheller, Julien Gagneur, Marloes H. Maathuis</dc:creator>
    </item>
    <item>
      <title>Assessing Risk Heterogeneity through Heavy-Tailed Frequency and Severity Mixtures</title>
      <link>https://arxiv.org/abs/2505.04795</link>
      <description>arXiv:2505.04795v2 Announce Type: replace 
Abstract: In operational risk management and actuarial finance, the analysis of risk often begins by dividing a random damage-generation process into its separate frequency and severity components. In the present article, we construct canonical families of mixture distributions for each of these components, based on a Negative Binomial kernel for frequency and a Gamma kernel for severity. The mixtures are employed to assess the heterogeneity of risk factors underlying an empirical distribution through the shape of the implied mixing distribution. From the duality of the Negative Binomial and Gamma distributions, we first derive necessary and sufficient conditions for heavy-tailed (i.e., inverse power-law) canonical mixtures. We then formulate flexible 4-parameter families of mixing distributions for Geometric and Exponential kernels to generate heavy-tailed 4-parameter mixture models, and extend these mixtures to arbitrary Negative Binomial and Gamma kernels, respectively, yielding 5-parameter mixtures for detecting and measuring risk heterogeneity. To check the robustness of such heterogeneity inferences, we show how a fitted 5-parameter model may be re-expressed in terms of alternative Negative Binomial or Gamma kernels whose associated mixing distributions form a "calibrated" family.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04795v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael R. Powers, Jiaxin Xu</dc:creator>
    </item>
    <item>
      <title>Local empirical Bayes correction for Bayesian modeling</title>
      <link>https://arxiv.org/abs/2506.11424</link>
      <description>arXiv:2506.11424v2 Announce Type: replace 
Abstract: The James-Stein estimator has attracted much interest as a shrinkage estimator that yields better estimates than the maximum likelihood estimator. The James-Stein estimator is also very useful as an argument in favor of empirical Bayesian methods. However, for problems involving large-scale data, such as differential gene expression data, the distribution is considered a mixture distribution with different means that cannot be considered sufficiently close. Therefore, it is not appropriate to apply the James-Stein estimator. Efron (2011) proposed a local empirical Bayes correction that attempted to correct a selection bias for large-scale data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11424v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.24644/keidaironshu.68.4_161</arxiv:DOI>
      <arxiv:journal_reference>Osaka Keidai Ronshu, vol.68, no.4, pp.161-172, 2017</arxiv:journal_reference>
      <dc:creator>Yoshiko Hayashi</dc:creator>
    </item>
    <item>
      <title>Generalized Random Forests using Fixed-Point Trees</title>
      <link>https://arxiv.org/abs/2306.11908</link>
      <description>arXiv:2306.11908v4 Announce Type: replace-cross 
Abstract: We propose a computationally efficient alternative to generalized random forests (GRFs) for estimating heterogeneous effects in large dimensions. While GRFs rely on a gradient-based splitting criterion, which in large dimensions is computationally expensive and unstable, our method introduces a fixed-point approximation that eliminates the need for Jacobian estimation. This gradient-free approach preserves GRF's theoretical guarantees of consistency and asymptotic normality while significantly improving computational efficiency. We demonstrate that our method achieves a speedup of multiple times over standard GRFs without compromising statistical accuracy. Experiments on both simulated and real-world data validate our approach. Our findings suggest that the proposed method is a scalable alternative for localized effect estimation in machine learning and causal inference applications</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11908v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Fleischer, David A. Stephens, Archer Y. Yang</dc:creator>
    </item>
    <item>
      <title>Variational Bayesian Bow tie Neural Networks with Shrinkage</title>
      <link>https://arxiv.org/abs/2411.11132</link>
      <description>arXiv:2411.11132v3 Announce Type: replace-cross 
Abstract: Despite the dominant role of deep models in machine learning, limitations persist, including overconfident predictions, susceptibility to adversarial attacks, and underestimation of variability in predictions. The Bayesian paradigm provides a natural framework to overcome such issues and has become the gold standard for uncertainty estimation with deep models, also providing improved accuracy and a framework for tuning critical hyperparameters. However, exact Bayesian inference is challenging, typically involving variational algorithms that impose strong independence and distributional assumptions. Moreover, existing methods are sensitive to the architectural choice of the network. We address these issues by focusing on a stochastic relaxation of the standard feed-forward rectified neural network and using sparsity-promoting priors on the weights of the neural network for increased robustness to architectural design. Thanks to Polya-Gamma data augmentation tricks, which render a conditionally linear and Gaussian model, we derive a fast, approximate variational inference algorithm that avoids distributional assumptions and independence across layers. Suitable strategies to further improve scalability and account for multimodality are considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11132v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alisa Sheinkman, Sara Wade</dc:creator>
    </item>
    <item>
      <title>Navigating Challenges in Spatio-temporal Modelling of Antarctic Krill Abundance: Addressing Zero-inflated Data and Misaligned Covariates</title>
      <link>https://arxiv.org/abs/2412.01399</link>
      <description>arXiv:2412.01399v2 Announce Type: replace-cross 
Abstract: Antarctic krill (Euphausia superba) are among the most abundant species on our planet and serve as a vital food source for many marine predators in the Southern Ocean. In this paper, we utilise statistical spatio-temporal methods to combine data from various sources and resolutions, aiming to model krill abundance. Our focus lies in fitting the model to a dataset comprising acoustic measurements of krill biomass. To achieve this, we integrate climate covariates obtained from satellite imagery and from drifting surface buoys (also known as drifters). Additionally, we use sparsely collected krill biomass data obtained from net fishing efforts (KRILLBASE) for validation. However, integrating these multiple heterogeneous data sources presents significant modelling challenges, including spatio-temporal misalignment and inflated zeros in the observed data. To address these challenges, we fit a Hurdle-Gamma model to jointly describe the occurrence of zeros and the krill biomass for the non-zero observations, while also accounting for misaligned and heterogeneous data sources, including drifters. Therefore, our work presents a comprehensive framework for analysing and predicting krill abundance in the Southern Ocean, leveraging information from various sources and formats. This is crucial due to the impact of krill fishing, as understanding their distribution is essential for informed management decisions and fishing regulations aimed at protecting the species.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01399v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andr\'e Victor Ribeiro Amaral, Adam M. Sykulski, Sophie Fielding, Emma Cavan</dc:creator>
    </item>
    <item>
      <title>Transductive Conformal Inference for Full Ranking</title>
      <link>https://arxiv.org/abs/2501.11384</link>
      <description>arXiv:2501.11384v2 Announce Type: replace-cross 
Abstract: We introduce a method based on Conformal Prediction (CP) to quantify the uncertainty of full ranking algorithms. We focus on a specific scenario where $n+m$ items are to be ranked by some ``black box'' algorithm. It is assumed that the relative (ground truth) ranking of $n$ of them is known. The objective is then to quantify the error made by the algorithm on the ranks of the $m$ new items among the total $(n+m)$. In such a setting, the true ranks of the $n$ original items in the total $(n+m)$ depend on the (unknown) true ranks of the $m$ new ones. Consequently, we have no direct access to a calibration set to apply a classical CP method. To address this challenge, we propose to construct distribution-free bounds of the unknown conformity scores using recent results on the distribution of conformal p-values. Using these scores upper bounds, we provide valid prediction sets for the rank of any item. We also control the false coverage proportion, a crucial quantity when dealing with multiple prediction sets. Finally, we empirically show on both synthetic and real data the efficiency of our CP method for state-of-the-art algorithms such as RankNet or LambdaMart.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11384v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean-Baptiste Fermanian (UM, IMAG, IROKO), Pierre Humbert (SU, LPSM), Gilles Blanchard (LMO, DATASHAPE)</dc:creator>
    </item>
    <item>
      <title>Understanding the Trade-offs in Accuracy and Uncertainty Quantification: Architecture and Inference Choices in Bayesian Neural Networks</title>
      <link>https://arxiv.org/abs/2503.11808</link>
      <description>arXiv:2503.11808v2 Announce Type: replace-cross 
Abstract: As modern neural networks get more complex, specifying a model with high predictive performance and sound uncertainty quantification becomes a more challenging task. Despite some promising theoretical results on the true posterior predictive distribution of Bayesian neural networks, the properties of even the most commonly used posterior approximations are often questioned. Computational burdens and intractable posteriors expose miscalibrated Bayesian neural networks to poor accuracy and unreliable uncertainty estimates. Approximate Bayesian inference aims to replace unknown and intractable posterior distributions with some simpler but feasible distributions. The dimensions of modern deep models, coupled with the lack of identifiability, make Markov chain Monte Carlo (MCMC) tremendously expensive and unable to fully explore the multimodal posterior. On the other hand, variational inference benefits from improved computational complexity but lacks the asymptotical guarantees of sampling-based inference and tends to concentrate around a single mode. The performance of both approaches heavily depends on architectural choices; this paper aims to shed some light on this by considering the computational costs, accuracy and uncertainty quantification in different scenarios including large width and out-of-sample data. To improve posterior exploration, different model averaging and ensembling techniques are studied, along with their benefits on predictive performance. In our experiments, variational inference overall provided better uncertainty quantification than MCMC; further, stacking and ensembles of variational approximations provided comparable accuracy to MCMC at a much-reduced cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11808v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alisa Sheinkman, Sara Wade</dc:creator>
    </item>
    <item>
      <title>Difference-in-Differences Designs: A Practitioner's Guide</title>
      <link>https://arxiv.org/abs/2503.13323</link>
      <description>arXiv:2503.13323v3 Announce Type: replace-cross 
Abstract: Difference-in-differences (DiD) is arguably the most popular quasi-experimental research design. Its canonical form, with two groups and two periods, is well-understood. However, empirical practices can be ad hoc when researchers go beyond that simple case. This article provides an organizing framework for discussing different types of DiD designs and their associated DiD estimators. It discusses covariates, weights, handling multiple periods, and staggered treatments. The organizational framework, however, applies to other extensions of DiD methods as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13323v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrew Baker, Brantly Callaway, Scott Cunningham, Andrew Goodman-Bacon, Pedro H. C. Sant'Anna</dc:creator>
    </item>
  </channel>
</rss>
