<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Jul 2024 04:00:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A concise proof of Benford's law</title>
      <link>https://arxiv.org/abs/2407.11076</link>
      <description>arXiv:2407.11076v1 Announce Type: new 
Abstract: This article presents a concise proof of the famous Benford's law when the distribution has a Riemann integrable probability density function and provides a criterion to judge whether a distribution obeys the law. The proof is intuitive and elegant, accessible to anyone with basic knowledge of calculus, revealing that the law originates from the basic property of the human number system. The criterion can bring great convenience to the field of fraud detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11076v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.fmre.2023.01.002</arxiv:DOI>
      <arxiv:journal_reference>Fundamental Research 4 (2024) 842-845</arxiv:journal_reference>
      <dc:creator>Luohan Wang, Bo-Qiang Ma</dc:creator>
    </item>
    <item>
      <title>On the convergence of entropy for K-th extreme</title>
      <link>https://arxiv.org/abs/2407.11264</link>
      <description>arXiv:2407.11264v1 Announce Type: new 
Abstract: Let recall that the term 'k-th extreme' was introduced in a limiting sense. That is, if $X_{r:n}$ denote the r-th order statistic then for fix k, as $n\to\infty$, $X_{n-k+1:n}$ is called the k-th extremes or k-th largest order statistics. In this paper, we study entropy limit theorems for k-th largest order statistics under linear normalization. We show the necessary and sufficient conditions which convergence entropy of k-th extreme holds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11264v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Saeb</dc:creator>
    </item>
    <item>
      <title>The Grouped Horseshoe distribution and its statistical properties</title>
      <link>https://arxiv.org/abs/2407.11423</link>
      <description>arXiv:2407.11423v1 Announce Type: new 
Abstract: The Grouped Horseshoe distribution arises from hierarchical structures in the recent Bayesian methodological literature aimed at selection of groups of regression coefficients. We isolate this distribution and study its properties concerning Bayesian statistical inference. Most, but not all, of the properties of the univariate Horseshoe distribution are seen to transfer to the grouped case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11423v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Virginia X. He, Matt P. Wand</dc:creator>
    </item>
    <item>
      <title>ERM-Lasso classification algorithm for Multivariate Hawkes Processes paths</title>
      <link>https://arxiv.org/abs/2407.11455</link>
      <description>arXiv:2407.11455v1 Announce Type: new 
Abstract: We are interested in the problem of classifying Multivariate Hawkes Processes (MHP) paths coming from several classes. MHP form a versatile family of point processes that models interactions between connected individuals within a network. In this paper, the classes are discriminated by the exogenous intensity vector and the adjacency matrix, which encodes the strength of the interactions. The observed learning data consist of labeled repeated and independent paths on a fixed time interval. Besides, we consider the high-dimensional setting, meaning the dimension of the network may be large {\it w.r.t.} the number of observations. We consequently require a sparsity assumption on the adjacency matrix. In this context, we propose a novel methodology with an initial interaction recovery step, by class, followed by a refitting step based on a suitable classification criterion. To recover the support of the adjacency matrix, a Lasso-type estimator is proposed, for which we establish rates of convergence. Then, leveraging the estimated support, we build a classification procedure based on the minimization of a $L_2$-risk. Notably, rates of convergence of our classification procedure are provided. An in-depth testing phase using synthetic data supports both theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11455v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charlotte Dion-Blanc (LPSM), Christophe Denis (LPSM), Laure Sansonnet (LAMA), Romain Edmond Lacoste (LAMA)</dc:creator>
    </item>
    <item>
      <title>Testing by Betting while Borrowing and Bargaining</title>
      <link>https://arxiv.org/abs/2407.11465</link>
      <description>arXiv:2407.11465v1 Announce Type: new 
Abstract: Testing by betting has been a cornerstone of the game-theoretic statistics literature. In this framework, a betting score (or more generally an e-process), as opposed to a traditional p-value, is used to quantify the evidence against a null hypothesis: the higher the betting score, the more money one has made betting against the null, and thus the larger the evidence that the null is false. A key ingredient assumed throughout past works is that one cannot bet more money than one currently has. In this paper, we ask what happens if the bettor is allowed to borrow money after going bankrupt, allowing further financial flexibility in this game of hypothesis testing. We propose various definitions of (adjusted) evidence relative to the wealth borrowed, indebted, and accumulated. We also ask what happens if the bettor can "bargain", in order to obtain odds bettor than specified by the null hypothesis. The adjustment of wealth in order to serve as evidence appeals to the characterization of arbitrage, interest rates, and num\'eraire-adjusted pricing in this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11465v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>q-fin.MF</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Wang, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Revisiting the Langlie procedure</title>
      <link>https://arxiv.org/abs/2407.11725</link>
      <description>arXiv:2407.11725v1 Announce Type: new 
Abstract: Introduced in 1962, the Langlie procedure is one of the most popular approaches to sensitivity testing. It aims to estimate an unknown sensitivity distribution based on the outcomes of binary trials. Officially recognized by the U.S. Department of Defense, the procedure is widely used both in civil and military industry. It first provides an experimental design for how the binary trials should be conducted, and then estimates the sensitivity distribution via maximum likelihood under a simple parametric model like logistic or probit regression. Despite its popularity and longevity, little is known about the statistical properties of the Langlie procedure, but it is well-established that the sequence of inputs tend to narrow in on the median of the sensitivity distribution. For this reason, the U.S. Department of Defense's protocol dictates that the procedure is only appropriate for estimating the median of the distribution, and no other quantiles. This begs the question of whether the parametric model assumption can be disposed of altogether, potentially making the Langlie procedure entirely nonparametric, much like the Robbins-Monro procedure. In this paper we answer this question in the negative by proving that when the Langlie procedure is employed, the sequence of inputs converges with probability zero.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11725v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dennis Christensen</dc:creator>
    </item>
    <item>
      <title>On the optimal prediction of extreme events in heavy-tailed time series with applications to solar flare forecasting</title>
      <link>https://arxiv.org/abs/2407.11887</link>
      <description>arXiv:2407.11887v1 Announce Type: new 
Abstract: The prediction of extreme events in time series is a fundamental problem arising in many financial, scientific, engineering, and other applications. We begin by establishing a general Neyman-Pearson-type characterization of optimal extreme event predictors in terms of density ratios. This yields new insights and several closed-form optimal extreme event predictors for additive models. These results naturally extend to time series, where we study optimal extreme event prediction for heavy-tailed autoregressive and moving average models. Using a uniform law of large numbers for ergodic time series, we establish the asymptotic optimality of an empirical version of the optimal predictor for autoregressive models. Using multivariate regular variation, we also obtain expressions for the optimal extremal precision in heavy-tailed infinite moving averages, which provide theoretical bounds on the ability to predict extremes in this general class of models. The developed theory and methodology is applied to the important problem of solar flare prediction based on the state-of-the-art GOES satellite flux measurements of the Sun. Our results demonstrate the success and limitations of long-memory autoregressive as well as long-range dependent heavy-tailed FARIMA models for the prediction of extreme solar flares.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11887v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Victor Verma, Stilian Stoev, Yang Chen</dc:creator>
    </item>
    <item>
      <title>Impossibility of latent inner product recovery via rate distortion</title>
      <link>https://arxiv.org/abs/2407.11932</link>
      <description>arXiv:2407.11932v1 Announce Type: new 
Abstract: In this largely expository note, we present an impossibility result for inner product recovery in a random geometric graph or latent space model using the rate-distortion theory. More precisely, suppose that we observe a graph $A$ on $n$ vertices with average edge density $p$ generated from Gaussian or spherical latent locations $z_1, \dots, z_n \in \mathbb{R}^d$ associated with the $n$ vertices. It is of interest to estimate the inner products $\langle z_i, z_j \rangle$ which represent the geometry of the latent points. We prove that it is impossible to recover the inner products if $d \gtrsim n h(p)$ where $h(p)$ is the binary entropy function. This matches the condition required for positive results on inner product recovery in the literature. The proof follows the well-established rate-distortion theory with the main technical ingredient being a lower bound on the rate-distortion function of the Wishart distribution which is interesting in its own right.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11932v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.SI</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cheng Mao, Shenduo Zhang</dc:creator>
    </item>
    <item>
      <title>Optimal estimators of cross-partial derivatives and surrogates of functions</title>
      <link>https://arxiv.org/abs/2407.11035</link>
      <description>arXiv:2407.11035v1 Announce Type: cross 
Abstract: Computing cross-partial derivatives using fewer model runs is relevant in modeling, such as stochastic approximation, derivative-based ANOVA, exploring complex models, and active subspaces. This paper introduces surrogates of all the cross-partial derivatives of functions by evaluating such functions at $N$ randomized points and using a set of $L$ constraints. Randomized points rely on independent, central, and symmetric variables. The associated estimators, based on $NL$ model runs, reach the optimal rates of convergence (i.e., $\mathcal{O}(N^{-1})$), and the biases of our approximations do not suffer from the curse of dimensionality for a wide class of functions. Such results are used for i) computing the main and upper-bounds of sensitivity indices, and ii) deriving emulators of simulators or surrogates of functions thanks to the derivative-based ANOVA. Simulations are presented to show the accuracy of our emulators and estimators of sensitivity indices. The plug-in estimates of indices using the U-statistics of one sample are numerically much stable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11035v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Matieyendou Lamboni</dc:creator>
    </item>
    <item>
      <title>Preconditioned Gradient Descent Finds Over-Parameterized Neural Networks with Sharp Generalization for Nonparametric Regression</title>
      <link>https://arxiv.org/abs/2407.11353</link>
      <description>arXiv:2407.11353v1 Announce Type: cross 
Abstract: We consider nonparametric regression by an over-parameterized two-layer neural network trained by gradient descent (GD) or its variant in this paper. We show that, if the neural network is trained with a novel Preconditioned Gradient Descent (PGD) with early stopping and the target function has spectral bias widely studied in the deep learning literature, the trained network renders a particularly sharp generalization bound with a minimax optimal rate of $\cO({1}/{n^{4\alpha/(4\alpha+1)}})$, which is sharper the current standard rate of $\cO({1}/{n^{2\alpha/(2\alpha+1)}})$ with $2\alpha = d/(d-1)$ when the data is distributed uniformly on the unit sphere in $\RR^d$ and $n$ is the size of the training data. When the target function has no spectral bias, we prove that neural network trained with regular GD with early stopping still enjoys minimax optimal rate, and in this case our results do not require distributional assumptions in contrast with the current known results. Our results are built upon two significant technical contributions. First, uniform convergence to the NTK is established during the training process by PGD or GD, so that we can have a nice decomposition of the neural network function at any step of GD or PGD into a function in the RKHS and an error function with a small $L^{\infty}$-norm. Second, local Rademacher complexity is employed to tightly bound the Rademacher complexity of the function class comprising all the possible neural network functions obtained by GD or PGD. Our results also indicate that PGD can be another way of avoiding the usual linear regime of NTK and obtaining sharper generalization bound, because PGD induces a different kernel with lower kernel complexity during the training than the regular NTK induced by the network architecture trained by regular GD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11353v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingzhen Yang</dc:creator>
    </item>
    <item>
      <title>Exponential tilting of subweibull distributions</title>
      <link>https://arxiv.org/abs/2407.11386</link>
      <description>arXiv:2407.11386v1 Announce Type: cross 
Abstract: The class of subweibull distributions has recently been shown to generalize the important properties of subexponential and subgaussian random variables. We describe alternative characterizations of subweibull distributions and detail the conditions under which their tail behavior is preserved after exponential tilting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11386v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>F. William Townes</dc:creator>
    </item>
    <item>
      <title>A goodness-of-fit test for testing exponentiality based on normalized dynamic survival extropy</title>
      <link>https://arxiv.org/abs/2407.11634</link>
      <description>arXiv:2407.11634v1 Announce Type: cross 
Abstract: The cumulative residual extropy (CRJ) is a measure of uncertainty that serves as an alternative to extropy. It replaces the probability density function with the survival function in the expression of extropy. This work introduces a new concept called normalized dynamic survival extropy (NDSE), a dynamic variation of CRJ. We observe that NDSE is equivalent to CRJ of the random variable of interest $X_{[t]}$ in the age replacement model at a fixed time $t$. Additionally, we have demonstrated that NDSE remains constant exclusively for exponential distribution at any time. We categorize two classes, INDSE and DNDSE, based on their increasing and decreasing NDSE values. Next, we present a non-parametric test to assess whether a distribution follows an exponential pattern against INDSE. We derive the exact and asymptotic distribution for the test statistic $\widehat{\Delta}^*$. Additionally, a test for asymptotic behavior is presented in the paper for right censoring data. Finally, we determine the critical values and power of our exact test through simulation. The simulation demonstrates that the suggested test is easy to compute and has significant statistical power, even with small sample sizes. We also conduct a power comparison analysis among other tests, which shows better power for the proposed test against other alternatives mentioned in this paper. Some numerical real-life examples validating the test are also included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11634v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gaurav Kandpal, Nitin Gupta</dc:creator>
    </item>
    <item>
      <title>Theoretical Insights into CycleGAN: Analyzing Approximation and Estimation Errors in Unpaired Data Generation</title>
      <link>https://arxiv.org/abs/2407.11678</link>
      <description>arXiv:2407.11678v1 Announce Type: cross 
Abstract: In this paper, we focus on analyzing the excess risk of the unpaired data generation model, called CycleGAN. Unlike classical GANs, CycleGAN not only transforms data between two unpaired distributions but also ensures the mappings are consistent, which is encouraged by the cycle-consistency term unique to CycleGAN. The increasing complexity of model structure and the addition of the cycle-consistency term in CycleGAN present new challenges for error analysis. By considering the impact of both the model architecture and training procedure, the risk is decomposed into two terms: approximation error and estimation error. These two error terms are analyzed separately and ultimately combined by considering the trade-off between them. Each component is rigorously analyzed; the approximation error through constructing approximations of the optimal transport maps, and the estimation error through establishing an upper bound using Rademacher complexity. Our analysis not only isolates these errors but also explores the trade-offs between them, which provides a theoretical insights of how CycleGAN's architecture and training procedures influence its performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11678v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luwei Sun, Dongrui Shen, Han Feng</dc:creator>
    </item>
    <item>
      <title>Frequentist Guarantees of Distributed (Non)-Bayesian Inference</title>
      <link>https://arxiv.org/abs/2311.08214</link>
      <description>arXiv:2311.08214v4 Announce Type: replace 
Abstract: Motivated by the need to analyze large, decentralized datasets, distributed Bayesian inference has become a critical research area across multiple fields, including statistics, electrical engineering, and economics. This paper establishes Frequentist properties, such as posterior consistency, asymptotic normality, and posterior contraction rates, for the distributed (non-)Bayes Inference problem among agents connected via a communication network. Our results show that, under appropriate assumptions on the communication graph, distributed Bayesian inference retains parametric efficiency while enhancing robustness in uncertainty quantification. We also explore the trade-off between statistical efficiency and communication efficiency by examining how the design and size of the communication graph impact the posterior contraction rate. Furthermore, We extend our analysis to time-varying graphs and apply our results to exponential family models, distributed logistic regression, and decentralized detection models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.08214v4</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bohan Wu, C\'esar A. Uribe</dc:creator>
    </item>
    <item>
      <title>Sharp Thresholds Imply Circuit Lower Bounds: from random 2-SAT to Planted Clique</title>
      <link>https://arxiv.org/abs/2311.04204</link>
      <description>arXiv:2311.04204v3 Announce Type: replace-cross 
Abstract: We show that sharp thresholds for Boolean functions directly imply average-case circuit lower bounds. More formally we show that any Boolean function exhibiting a sharp enough threshold at \emph{arbitrary} critical density cannot be computed by Boolean circuits of bounded depth and polynomial size. We also prove a partial converse: if a monotone graph invariant Boolean function does not have a sharp threshold then it can be computed on average by a Boolean circuit of bounded depth and polynomial size.
  Our general result also implies new average-case bounded depth circuit lower bounds in a variety of settings.
  (a) ($k$-cliques) For $k=\Theta(n)$, we prove that any circuit of depth $d$ deciding the presence of a size $k$ clique in a random graph requires exponential-in-$n^{\Theta(1/d)}$ size.
  (b)(random 2-SAT) We prove that any circuit of depth $d$ deciding the satisfiability of a random 2-SAT formula requires exponential-in-$n^{\Theta(1/d)}$ size. To the best of our knowledge, this is the first bounded depth circuit lower bound for random $k$-SAT for any value of $k \geq 2.$ Our results also provide the first rigorous lower bound in agreement with a conjectured, but debated, "computational hardness" of random $k$-SAT around its satisfiability threshold.
  (c)(Statistical estimation -- planted $k$-clique) Over the recent years, multiple statistical estimation problems have also been proven to exhibit a "statistical" sharp threshold, called the All-or-Nothing (AoN) phenomenon. We show that AoN also implies circuit lower bounds for statistical problems. As a simple corollary of that, we prove that any circuit of depth $d$ that solves to information-theoretic optimality a "dense" variant of the celebrated planted $k$-clique problem requires exponential-in-$n^{\Theta(1/d)}$ size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04204v3</guid>
      <category>cs.CC</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Gamarnik, Elchanan Mossel, Ilias Zadik</dc:creator>
    </item>
    <item>
      <title>Generalization Error Curves for Analytic Spectral Algorithms under Power-law Decay</title>
      <link>https://arxiv.org/abs/2401.01599</link>
      <description>arXiv:2401.01599v2 Announce Type: replace-cross 
Abstract: The generalization error curve of certain kernel regression method aims at determining the exact order of generalization error with various source condition, noise level and choice of the regularization parameter rather than the minimax rate. In this work, under mild assumptions, we rigorously provide a full characterization of the generalization error curves of the kernel gradient descent method (and a large class of analytic spectral algorithms) in kernel regression. Consequently, we could sharpen the near inconsistency of kernel interpolation and clarify the saturation effects of kernel regression algorithms with higher qualification, etc. Thanks to the neural tangent kernel theory, these results greatly improve our understanding of the generalization behavior of training the wide neural networks. A novel technical contribution, the analytic functional argument, might be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01599v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yicheng Li, Weiye Gan, Zuoqiang Shi, Qian Lin</dc:creator>
    </item>
    <item>
      <title>Characteristic Learning for Provable One Step Generation</title>
      <link>https://arxiv.org/abs/2405.05512</link>
      <description>arXiv:2405.05512v4 Announce Type: replace-cross 
Abstract: We propose the characteristic generator, a novel one-step generative model that combines the efficiency of sampling in Generative Adversarial Networks (GANs) with the stable performance of flow-based models. Our model is driven by characteristics, along which the probability density transport can be described by ordinary differential equations (ODEs). Specifically, We estimate the velocity field through nonparametric regression and utilize Euler method to solve the probability flow ODE, generating a series of discrete approximations to the characteristics. We then use a deep neural network to fit these characteristics, ensuring a one-step mapping that effectively pushes the prior distribution towards the target distribution. In the theoretical aspect, we analyze the errors in velocity matching, Euler discretization, and characteristic fitting to establish a non-asymptotic convergence rate for the characteristic generator in 2-Wasserstein distance. To the best of our knowledge, this is the first thorough analysis for simulation-free one step generative models. Additionally, our analysis refines the error analysis of flow-based generative models in prior works. We apply our method on both synthetic and real datasets, and the results demonstrate that the characteristic generator achieves high generation quality with just a single evaluation of neural network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05512v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhao Ding, Chenguang Duan, Yuling Jiao, Ruoxuan Li, Jerry Zhijian Yang, Pingwen Zhang</dc:creator>
    </item>
    <item>
      <title>FLIPHAT: Joint Differential Privacy for High Dimensional Sparse Linear Bandits</title>
      <link>https://arxiv.org/abs/2405.14038</link>
      <description>arXiv:2405.14038v2 Announce Type: replace-cross 
Abstract: High dimensional sparse linear bandits serve as an efficient model for sequential decision-making problems (e.g. personalized medicine), where high dimensional features (e.g. genomic data) on the users are available, but only a small subset of them are relevant. Motivated by data privacy concerns in these applications, we study the joint differentially private high dimensional sparse linear bandits, where both rewards and contexts are considered as private data. First, to quantify the cost of privacy, we derive a lower bound on the regret achievable in this setting. To further address the problem, we design a computationally efficient bandit algorithm, \textbf{F}orgetfu\textbf{L} \textbf{I}terative \textbf{P}rivate \textbf{HA}rd \textbf{T}hresholding (FLIPHAT). Along with doubling of episodes and episodic forgetting, FLIPHAT deploys a variant of Noisy Iterative Hard Thresholding (N-IHT) algorithm as a sparse linear regression oracle to ensure both privacy and regret-optimality. We show that FLIPHAT achieves optimal regret up to logarithmic factors. We analyze the regret by providing a novel refined analysis of the estimation error of N-IHT, which is of parallel interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14038v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sunrit Chakraborty, Saptarshi Roy, Debabrota Basu</dc:creator>
    </item>
  </channel>
</rss>
