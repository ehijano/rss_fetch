<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Apr 2025 04:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Training and synchronizing oscillator networks with Equilibrium Propagation</title>
      <link>https://arxiv.org/abs/2504.11884</link>
      <description>arXiv:2504.11884v1 Announce Type: cross 
Abstract: Oscillator networks represent a promising technology for unconventional computing and artificial intelligence. Thus far, these systems have primarily been demonstrated in small-scale implementations, such as Ising Machines for solving combinatorial problems and associative memories for image recognition, typically trained without state-of-the-art gradient-based algorithms. Scaling up oscillator-based systems requires advanced gradient-based training methods that also ensure robustness against frequency dispersion between individual oscillators. Here, we demonstrate through simulations that the Equilibrium Propagation algorithm enables effective gradient-based training of oscillator networks, facilitating synchronization even when initial oscillator frequencies are significantly dispersed. We specifically investigate two oscillator models: purely phase-coupled oscillators and oscillators coupled via both amplitude and phase interactions. Our results show that these oscillator networks can scale successfully to standard image recognition benchmarks, such as achieving nearly 98\% test accuracy on the MNIST dataset, despite noise introduced by imperfect synchronization. This work thus paves the way for practical hardware implementations of large-scale oscillator networks, such as those based on spintronic devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11884v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.mes-hall</category>
      <category>cs.ET</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Th\'eophile Rageau, Julie Grollier</dc:creator>
    </item>
    <item>
      <title>Noise-based Local Learning using Stochastic Magnetic Tunnel Junctions</title>
      <link>https://arxiv.org/abs/2412.12783</link>
      <description>arXiv:2412.12783v3 Announce Type: replace 
Abstract: Brain-inspired learning in physical hardware has enormous potential to learn fast at minimal energy expenditure. One of the characteristics of biological learning systems is their ability to learn in the presence of various noise sources. Inspired by this observation, we introduce a novel noise-based learning approach for physical systems implementing multi-layer neural networks. Simulation results show that our approach allows for effective learning whose performance approaches that of the conventional effective yet energy-costly backpropagation algorithm. Using a spintronics hardware implementation, we demonstrate experimentally that learning can be achieved in a small network composed of physical stochastic magnetic tunnel junctions. These results provide a path towards efficient learning in general physical systems which embraces rather than mitigates the noise inherent in physical devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12783v3</guid>
      <category>cs.ET</category>
      <category>cond-mat.mes-hall</category>
      <category>cs.LG</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kees Koenders, Leo Schnitzpan, Fabian Kammerbauer, Sinan Shu, Gerhard Jakob, Mathis Kl\"aui, Johan Mentink, Nasir Ahmad, Marcel van Gerven</dc:creator>
    </item>
    <item>
      <title>DelGrad: Exact event-based gradients for training delays and weights on spiking neuromorphic hardware</title>
      <link>https://arxiv.org/abs/2404.19165</link>
      <description>arXiv:2404.19165v3 Announce Type: replace-cross 
Abstract: Spiking neural networks (SNNs) inherently rely on the timing of signals for representing and processing information. Incorporating trainable transmission delays, alongside synaptic weights, is crucial for shaping these temporal dynamics. While recent methods have shown the benefits of training delays and weights in terms of accuracy and memory efficiency, they rely on discrete time, approximate gradients, and full access to internal variables like membrane potentials. This limits their precision, efficiency, and suitability for neuromorphic hardware due to increased memory requirements and I/O bandwidth demands. To address these challenges, we propose DelGrad, an analytical, event-based method to compute exact loss gradients for both synaptic weights and delays. The inclusion of delays in the training process emerges naturally within our proposed formalism, enriching the model's search space with a temporal dimension. Moreover, DelGrad, grounded purely in spike timing, eliminates the need to track additional variables such as membrane potentials. To showcase this key advantage, we demonstrate the functionality and benefits of DelGrad on the BrainScaleS-2 neuromorphic platform, by training SNNs in a chip-in-the-loop fashion. For the first time, we experimentally demonstrate the memory efficiency and accuracy benefits of adding delays to SNNs on noisy mixed-signal hardware. Additionally, these experiments also reveal the potential of delays for stabilizing networks against noise. DelGrad opens a new way for training SNNs with delays on neuromorphic hardware, which results in fewer required parameters, higher accuracy and ease of hardware training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19165v3</guid>
      <category>cs.NE</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julian G\"oltz, Jimmy Weber, Laura Kriener, Sebastian Billaudelle, Peter Lake, Johannes Schemmel, Melika Payvand, Mihai A. Petrovici</dc:creator>
    </item>
  </channel>
</rss>
