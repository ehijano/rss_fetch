<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 29 Aug 2025 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Comprehensive Survey of 5G URLLC and Challenges in the 6G Era</title>
      <link>https://arxiv.org/abs/2508.20205</link>
      <description>arXiv:2508.20205v1 Announce Type: new 
Abstract: As the wireless communication paradigm is being transformed from human centered communication services towards machine centered communication services, the requirements of rate, latency and reliability for these services have also been transformed drastically. Thus the concept of Ultra Reliable and Low Latency Communication (URLLC) has emerged as a dominant theme for 5G and 6G systems. Though the latency and reliability requirement varies from one use case to another, URLLC services generally aim to achieve very high reliability in the range of 99.999\% while ensuring the latency of up to 1 ms. These two targets are however inherently opposed to one another. Significant amounts of work have been carried out to meet these ambitious but conflicting targets. In this article a comprehensive survey of the URLLC approaches in 5G systems are analysed in detail. Effort has been made to trace the history and evolution of latency and reliability issues in wireless communication. A layered approach is taken where physical layer, Medium Access Control (MAC) layer as well as cross layer techniques are discussed in detail. It also covers the design consideration for various 5G and beyond verticals. Finally the article concludes by providing a detailed discussion on challenges and future outlook with particular focus on the emerging 6G paradigm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20205v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md. Emadul Haque, Faisal Tariq, Muhammad R A Khandaker, Md. Sakir Hossain, Muhammad Ali Imran, Kai-Kit Wong</dc:creator>
    </item>
    <item>
      <title>DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource Allocation and Markov Decision Process in Named Data Networking (NDN)</title>
      <link>https://arxiv.org/abs/2508.20272</link>
      <description>arXiv:2508.20272v1 Announce Type: new 
Abstract: Named Data Networking (NDN) represents a transformative shift in network architecture, prioritizing content names over host addresses to enhance data dissemination. Efficient queue and resource management are critical to NDN performance, especially under dynamic and high-traffic conditions. This paper introduces DRR-MDPF, a novel hybrid strategy that integrates the Markov Decision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR) algorithm. MDPF enables routers to intelligently predict optimal forwarding decisions based on key metrics such as bandwidth, delay, and the number of unsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation among competing data flows. The proposed method models each router as a learning agent capable of adjusting its strategies through continuous feedback and probabilistic updates. Simulation results using ndnSIM demonstrate that DRR-MDPF significantly outperforms state-of-the-art strategies including SAF, RFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest Satisfaction Rate (ISR), packet drop rate, content retrieval time, and load balancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and heavy traffic, offering enhanced adaptability and lower computational complexity due to its single-path routing design. Furthermore, its multi-metric decision-making capability enables more accurate interface selection, leading to optimized network performance. Overall, DRR-MDPF serves as an intelligent, adaptive, and scalable queue management solution for NDN, effectively addressing core challenges such as resource allocation, congestion control, and route optimization in dynamic networking environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20272v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fatemeh Roshanzadeh, Hamid Barati, Ali Barati</dc:creator>
    </item>
    <item>
      <title>Relay Selection in Wireless Networks as Restless Bandits</title>
      <link>https://arxiv.org/abs/2508.20625</link>
      <description>arXiv:2508.20625v1 Announce Type: new 
Abstract: We consider a wireless network in which a source node needs to transmit a large file to a destination node. The direct wireless link between the source and the destination is assumed to be blocked. Multiple candidate relays are available to forward packets from the source to the destination. A holding cost is incurred for each packet stored at every relay in each time slot. The objective is to design a policy for selecting a relay in each time slot to which the source attempts to send a packet, so as to minimize the expected long-run time-averaged total packet holding cost at the relays. This problem is an instance of the restless multi-armed bandit (RMAB) problem, which is provably hard to solve. We prove that this relay selection problem is Whittle-indexable, and propose a method to compute the Whittle index of each relay in every time slot. In each time slot, our relay selection policy transmits a packet to the relay with the smallest Whittle index. Using simulations, we show that the proposed policy outperforms the relay selection policies proposed in prior work in terms of average cost, delay, as well as throughput.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20625v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mandar R. Nalavade, Ravindra S. Tomar, Gaurav S. Kasbekar</dc:creator>
    </item>
    <item>
      <title>Digital Twin-Empowered Deep Reinforcement Learning for Intelligent VNF Migration in Edge-Core Networks</title>
      <link>https://arxiv.org/abs/2508.20957</link>
      <description>arXiv:2508.20957v1 Announce Type: new 
Abstract: The growing demand for services and the rapid deployment of virtualized network functions (VNFs) pose significant challenges for achieving low-latency and energy-efficient orchestration in modern edge-core network infrastructures. To address these challenges, this study proposes a Digital Twin (DT)-empowered Deep Reinforcement Learning framework for intelligent VNF migration that jointly minimizes average end-to-end (E2E) delay and energy consumption. By formulating the VNF migration problem as a Markov Decision Process and utilizing the Advantage Actor-Critic model, the proposed framework enables adaptive and real-time migration decisions. A key innovation of the proposed framework is the integration of a DT module composed of a multi-task Variational Autoencoder and a multi-task Long Short-Term Memory network. This combination collectively simulates environment dynamics and generates high-quality synthetic experiences, significantly enhancing training efficiency and accelerating policy convergence. Simulation results demonstrate substantial performance gains, such as significant reductions in both average E2E delay and energy consumption, thereby establishing new benchmarks for intelligent VNF migration in edge-core networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20957v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Faisal Ahmed, Suresh Subramaniam, Motoharu Matsuura, Hiroshi Hasegawa, Shih-Chun Lin</dc:creator>
    </item>
    <item>
      <title>RANGAN: GAN-empowered Anomaly Detection in 5G Cloud RAN</title>
      <link>https://arxiv.org/abs/2508.20985</link>
      <description>arXiv:2508.20985v1 Announce Type: new 
Abstract: Radio Access Network (RAN) systems are inherently complex, requiring continuous monitoring to prevent performance degradation and ensure optimal user experience. The RAN leverages numerous key performance indicators (KPIs) to evaluate system performance, generating vast amounts of data each second. This immense data volume can make troubleshooting and accurate diagnosis of performance anomalies more difficult. Furthermore, the highly dynamic nature of RAN performance demands adaptive methodologies capable of capturing temporal dependencies to detect anomalies reliably. In response to these challenges, we introduce \textbf{RANGAN}, an anomaly detection framework that integrates a Generative Adversarial Network (GAN) with a transformer architecture. To enhance the capability of capturing temporal dependencies within the data, RANGAN employs a sliding window approach during data preprocessing. We rigorously evaluated RANGAN using the publicly available RAN performance dataset from the Spotlight project \cite{sun-2024}. Experimental results demonstrate that RANGAN achieves promising detection accuracy, notably attaining an F1-score of up to $83\%$ in identifying network contention issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20985v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Douglas Liao, Jiping Luo, Jens Vevstad, Nikolaos Pappas</dc:creator>
    </item>
    <item>
      <title>DSROQ: Dynamic Scheduling and Routing for QoE Management in LEO Satellite Networks</title>
      <link>https://arxiv.org/abs/2508.21047</link>
      <description>arXiv:2508.21047v1 Announce Type: new 
Abstract: The modern Internet supports diverse applications with heterogeneous quality of service (QoS) requirements. Low Earth orbit (LEO) satellite constellations offer a promising solution to meet these needs, enhancing coverage in rural areas and complementing terrestrial networks in urban regions. Ensuring QoS in such networks requires joint optimization of routing, bandwidth allocation, and dynamic queue scheduling, as traffic handling is critical for maintaining service performance. This paper formulates a joint routing and bandwidth allocation problem where QoS requirements are treated as soft constraints, aiming to maximize user experience. An adaptive scheduling approach is introduced to prioritize flow-specific QoS needs. We propose a Monte Carlo tree search (MCTS)-inspired method to solve the NP-hard route and bandwidth allocation problem, with Lyapunov optimization-based scheduling applied during reward evaluation. Using the Starlink Phase 1 Version 2 constellation, we compare end-user experience and fairness between our proposed DSROQ algorithm and a benchmark scheme. Results show that DSROQ improves both performance metrics and demonstrates the advantage of joint routing and bandwidth decisions. Furthermore, we observe that the dominant performance factor shifts from scheduling to routing and bandwidth allocation as traffic sensitivity changes from latency-driven to bandwidth-driven.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21047v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhiraj Bhattacharjee, Pablo G. Madoery, Abhishek Naik, Halim Yanikomeroglu, Gunes Karabulut Kurt, Stephane Martel, Khaled Ahmed</dc:creator>
    </item>
    <item>
      <title>Enhancing Resilience for IoE: A Perspective of Networking-Level Safeguard</title>
      <link>https://arxiv.org/abs/2508.20504</link>
      <description>arXiv:2508.20504v1 Announce Type: cross 
Abstract: The Internet of Energy (IoE) integrates IoT-driven digital communication with power grids to enable efficient and sustainable energy systems. Still, its interconnectivity exposes critical infrastructure to sophisticated cyber threats, including adversarial attacks designed to bypass traditional safeguards. Unlike general IoT risks, IoE threats have heightened public safety consequences, demanding resilient solutions. From the networking-level safeguard perspective, we propose a Graph Structure Learning (GSL)-based safeguards framework that jointly optimizes graph topology and node representations to resist adversarial network model manipulation inherently. Through a conceptual overview, architectural discussion, and case study on a security dataset, we demonstrate GSL's superior robustness over representative methods, offering practitioners a viable path to secure IoE networks against evolving attacks. This work highlights the potential of GSL to enhance the resilience and reliability of future IoE networks for practitioners managing critical infrastructure. Lastly, we identify key open challenges and propose future research directions in this novel research area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20504v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guan-Yan Yang, Jui-Ning Chen, Farn Wang, Kuo-Hui Yeh</dc:creator>
    </item>
    <item>
      <title>Microarchitecture Design and Benchmarking of Custom SHA-3 Instruction for RISC-V</title>
      <link>https://arxiv.org/abs/2508.20653</link>
      <description>arXiv:2508.20653v1 Announce Type: cross 
Abstract: Integrating cryptographic accelerators into modern CPU architectures presents unique microarchitectural challenges, particularly when extending instruction sets with complex and multistage operations. Hardware-assisted cryptographic instructions, such as Intel's AES-NI and ARM's custom instructions for encryption workloads, have demonstrated substantial performance improvements. However, efficient SHA-3 acceleration remains an open problem due to its distinct permutation-based structure and memory access patterns. Existing solutions primarily rely on standalone coprocessors or software optimizations, often avoiding the complexities of direct microarchitectural integration. This study investigates the architectural challenges of embedding a SHA-3 permutation operation as a custom instruction within a general-purpose processor, focusing on pipelined simultaneous execution, storage utilization, and hardware cost. In this paper, we investigated and prototyped a SHA-3 custom instruction for the RISC-V CPU architecture. Using cycle-accurate GEM5 simulations and FPGA prototyping, our results demonstrate performance improvements of up to 8.02x for RISC-V optimized SHA-3 software workloads and up to 46.31x for Keccak-specific software workloads, with only a 15.09% increase in registers and a 11.51% increase in LUT utilization. These findings provide critical insights into the feasibility and impact of SHA-3 acceleration at the microarchitectural level, highlighting practical design considerations for future cryptographic instruction set extensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20653v1</guid>
      <category>cs.AR</category>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alperen Bolat, Sakir Sezer, Kieran McLaughlin, Henry Hui</dc:creator>
    </item>
    <item>
      <title>NetGPT: Generative Pretrained Transformer for Network Traffic</title>
      <link>https://arxiv.org/abs/2304.09513</link>
      <description>arXiv:2304.09513v3 Announce Type: replace 
Abstract: All data on the Internet are transferred by network traffic, thus accurately modeling network traffic can help improve network services quality and protect data privacy. Pretrained models for network traffic can utilize large-scale raw data to learn the essential characteristics of network traffic, and generate distinguishable results for input traffic without considering specific downstream tasks. Effective pretrained models can significantly optimize the training efficiency and effectiveness of downstream tasks, such as application classification, attack detection and traffic generation. Despite the great success of pretraining in natural language processing, there is no work in the network field. Considering the diverse demands and characteristics of network traffic and network tasks, it is non-trivial to build a pretrained model for network traffic and we face various challenges, especially the heterogeneous headers and payloads in the multi-pattern network traffic and the different dependencies for contexts of diverse downstream network tasks.
  To tackle these challenges, in this paper, we make the first attempt to provide a generative pretrained model NetGPT for both traffic understanding and generation tasks. We propose the multi-pattern network traffic modeling to construct unified text inputs and support both traffic understanding and generation tasks. We further optimize the adaptation effect of the pretrained model to diversified tasks by shuffling header fields, segmenting packets in flows, and incorporating diverse task labels with prompts. With diverse traffic datasets from encrypted software, DNS, private industrial protocols and cryptocurrency mining, expensive experiments demonstrate the effectiveness of our NetGPT in a range of traffic understanding and generation tasks on traffic datasets, and outperform state-of-the-art baselines by a wide margin.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.09513v3</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xuying Meng, Chungang Lin, Yequan Wang, Yujun Zhang</dc:creator>
    </item>
    <item>
      <title>Policy Design in Zero-Trust Distributed Networks: Challenges and Solutions</title>
      <link>https://arxiv.org/abs/2508.04526</link>
      <description>arXiv:2508.04526v2 Announce Type: replace 
Abstract: Traditional security architectures are becoming more vulnerable to distributed attacks due to significant dependence on trust. This will further escalate when implementing agentic AI within the systems, as more components must be secured over a similar distributed space. These scenarios can be observed in consumer technologies, such as the dense Internet of things (IoT). Here, zero-trust architecture (ZTA) can be seen as a potential solution, which relies on a key principle of not giving users explicit trust, instead always verifying their privileges whenever a request is made. However, the overall security in ZTA is managed through its policies, and unverified policies can lead to unauthorized access. Thus, this paper explores challenges and solutions for ZTA policy design in the context of distributed networks, which is referred to as zero-trust distributed networks (ZTDN). This is followed by a case-study on formal verification of policies using UPPAAL. Subsequently, the importance of accountability and responsibility in the system's security is discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04526v2</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fannya R. Sandjaja, Ayesha A. Majeed, Abdullah Abdullah, Gyan Wickremasinghe, Karen Rafferty, Vishal Sharma</dc:creator>
    </item>
    <item>
      <title>A Quantum Speedup in Localizing Transmission Loss Change in Optical Networks</title>
      <link>https://arxiv.org/abs/2504.10882</link>
      <description>arXiv:2504.10882v2 Announce Type: replace-cross 
Abstract: The ability to localize transmission loss change to a subset of links in optical networks is crucial for maintaining network reliability, performance and security. \emph{Quantum probes}, implemented by sending blocks of $n$ coherent-state pulses augmented with continuous-variable (CV) squeezing ($n=1$) or weak temporal-mode entanglement ($n&gt;1$) over a lossy channel to a receiver with homodyne detection capabilities, are known to be more sensitive than their quasi-classical counterparts in detecting a sudden increase in channel loss. The enhanced sensitivity can be characterized by the increased Kullback-Leibler (KL) divergence of the homodyne output, before and after the loss change occurs. When combined with the theory of quickest change detection (QCD), the increase in KL divergence translates into a decrease in detection latency.
  In this work, we first revisit quantum probes over a channel, generalizing previous results on $n=1$ (CV squeezed states) to arbitrary values of $n$. Assuming a subset of nodes in an optical network is capable of sending and receiving such probes through intermediate nodes with all-optical switching capabilities, we present a scheme for quickly detecting the links that have suffered a sudden drop in transmissivity. Since quantum probes lose their sensitivity with increasing loss in the channel, we first propose a probe construction algorithm that makes the set of links suffering transmission loss change identifiable, while minimizing the longest distance a probe traverses. We then introduce new cumulative sum (CUSUM) statistics with a stopping rule, which allows us to run the CUSUM algorithm to quickly localize the lossy links using our constructed probes. Finally, we show that the proposed scheme achieves a quantum speedup in decreasing the detection delay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10882v2</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yufei Zheng, Yu-Zhen Janice Chen, Prithwish Basu, Don Towsley</dc:creator>
    </item>
  </channel>
</rss>
