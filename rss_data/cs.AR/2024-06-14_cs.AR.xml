<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Jun 2024 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>ONNX-to-Hardware Design Flow for Adaptive Neural-Network Inference on FPGAs</title>
      <link>https://arxiv.org/abs/2406.09078</link>
      <description>arXiv:2406.09078v1 Announce Type: new 
Abstract: The challenges involved in executing neural networks (NNs) at the edge include providing diversity, flexibility, and sustainability. That implies, for instance, supporting evolving applications and algorithms energy-efficiently. Using hardware or software accelerators can deliver fast and efficient computation of the NNs, while flexibility can be exploited to support long-term adaptivity. Nonetheless, handcrafting an NN for a specific device, despite the possibility of leading to an optimal solution, takes time and experience, and that's why frameworks for hardware accelerators are being developed. This work, starting from a preliminary semi-integrated ONNX-to-hardware toolchain [21], focuses on enabling approximate computing leveraging the distinctive ability of the original toolchain to favor adaptivity. The goal is to allow lightweight adaptable NN inference on FPGAs at the edge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09078v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Federico Manca, Francesco Ratto, Francesca Palumbo</dc:creator>
    </item>
    <item>
      <title>A nA-Range Area-Efficient Sub-100-ppm/{\deg}C Peaking Current Reference Using Forward Body Biasing in 0.11-$\mu$m Bulk and 22-nm FD-SOI</title>
      <link>https://arxiv.org/abs/2406.09104</link>
      <description>arXiv:2406.09104v1 Announce Type: new 
Abstract: In recent years, the development of the Internet of Things (IoT) has prompted the search for nA-range current references that are simultaneously constrained to a small area and robust to process, voltage and temperature variations. Yet, such references have remained elusive, as existing architectures fail to reach a low temperature coefficient (TC) while minimizing silicon area. In this work, we propose a nA-range constant-with-temperature (CWT) peaking current reference, in which a resistor is biased by the threshold voltage difference between two transistors in weak inversion. This bias voltage is lower than in conventional architectures to cut down the silicon area occupied by the resistor and is obtained by forward body biasing one of the two transistors with an ultra-low-power voltage reference so as to reduce its threshold voltage. In addition, the proposed reference includes a circuit to suppress the leakage of parasitic diodes at high temperature, and two simple trimming mechanisms for the reference current and its TC. As the proposed design relies on the body effect, it has been validated in both 0.11-$\mu$m bulk and 22-nm fully-depleted silicon-on-insulator, to demonstrate feasibility across different technology types. In post-layout simulation, the 0.11-$\mu$m design generates a 5-nA current with a 65-ppm/{\deg}C TC and a 2.84-%/V line sensitivity (LS), while in measurement, the 22-nm design achieves a 1.5-nA current with an 89-ppm/{\deg}C TC and a 0.51-%/V LS. As a result of the low resistor bias voltage, the proposed references occupy a silicon area of 0.00954 mm$^2$ in 0.11 $\mu$m (resp. 0.00214 mm$^2$ in 22 nm) at least 1.8$\times$ (resp. 8.2$\times$) smaller than fabricated nA-range CWT references, but with a TC improved by 6.1$\times$ (resp. 4.4$\times$).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09104v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/JSSC.2024.3406423</arxiv:DOI>
      <dc:creator>Martin Lefebvre, David Bol</dc:creator>
    </item>
    <item>
      <title>Python-based DSL for generating Verilog model of Synchronous Digital Circuits</title>
      <link>https://arxiv.org/abs/2406.09208</link>
      <description>arXiv:2406.09208v1 Announce Type: new 
Abstract: We have designed a Python-based Domain Specific Language (DSL) for modeling synchronous digital circuits. In this DSL, hardware is modeled as a collection of transactions -- running in series, parallel, and loops. When the model is executed by a Python interpreter, synthesizable and behavioural Verilog is generated as output, which can be integrated with other RTL designs or directly used for FPGA and ASIC flows. In this paper, we describe - 1) the language (DSL), which allows users to express computation in series/parallel/loop constructs, with explicit cycle boundaries, 2) the internals of a simple Python implementation to produce synthesizable Verilog, and 3) several design examples and case studies for applications in post-quantum cryptography, stereo-vision, digital signal processing and optimization techniques. In the end, we list ideas to extend this framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09208v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mandar Datar (Indian Institute of Technology Bombay), Dhruva S. Hegde (Indian Institute of Technology Bombay), Vendra Durga Prasad (Indian Institute of Technology Bombay), Manish Prajapati (Indian Institute of Technology Bombay), Neralla Manikanta (Indian Institute of Technology Bombay), Devansh Gupta (Indian Institute of Technology Bombay), Janampalli Pavanija (Indian Institute of Technology Bombay), Pratyush Pare (Indian Institute of Technology Bombay),  Akash (Indian Institute of Technology Bombay), Shivam Gupta (Indian Institute of Technology Bombay), Sachin B. Patkar (Indian Institute of Technology Bombay)</dc:creator>
    </item>
    <item>
      <title>C2HLSC: Can LLMs Bridge the Software-to-Hardware Design Gap?</title>
      <link>https://arxiv.org/abs/2406.09233</link>
      <description>arXiv:2406.09233v1 Announce Type: new 
Abstract: High Level Synthesis (HLS) tools offer rapid hardware design from C code, but their compatibility is limited by code constructs. This paper investigates Large Language Models (LLMs) for refactoring C code into HLS-compatible formats. We present several case studies by using an LLM to rewrite C code for NIST 800-22 randomness tests, a QuickSort algorithm and AES-128 into HLS-synthesizable c. The LLM iteratively transforms the C code guided by user prompts, implementing functions like streaming data and hardware-specific signals. This evaluation demonstrates the LLM's potential to assist hardware design refactoring regular C code into HLS synthesizable C code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09233v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Collini, Siddharth Garg, Ramesh Karri</dc:creator>
    </item>
    <item>
      <title>3D-Carbon: An Analytical Carbon Modeling Tool for 3D and 2.5D Integrated Circuits</title>
      <link>https://arxiv.org/abs/2307.08060</link>
      <description>arXiv:2307.08060v4 Announce Type: replace 
Abstract: Environmental sustainability is crucial for Integrated Circuits (ICs) across their lifecycle, particularly in manufacturing and use. Meanwhile, ICs using 3D/2.5D integration technologies have emerged as promising solutions to meet the growing demands for computational power. However, there is a distinct lack of carbon modeling tools for 3D/2.5D ICs. Addressing this, we propose 3D-Carbon, an analytical carbon modeling tool designed to quantify the carbon emissions of 3D/2.5D ICs throughout their life cycle. 3D-Carbon factors in both potential savings and overheads from advanced integration technologies, considering practical deployment constraints like bandwidth. We validate 3D-Carbon's accuracy against established baselines and illustrate its utility through case studies in autonomous vehicles. We believe that 3D-Carbon lays the initial foundation for future innovations in developing environmentally sustainable 3D/2.5D ICs. Our open-source code is available at https://github.com/UMN-ZhaoLab/3D-Carbon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.08060v4</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujie Zhao, Yang Zhao, Cheng Wan, Yingyan Lin</dc:creator>
    </item>
    <item>
      <title>Scalable and Programmable Look-Up Table based Neural Acceleration (LUT-NA) for Extreme Energy Efficiency</title>
      <link>https://arxiv.org/abs/2406.05282</link>
      <description>arXiv:2406.05282v2 Announce Type: replace 
Abstract: Traditional digital implementations of neural accelerators are limited by high power and area overheads, while analog and non-CMOS implementations suffer from noise, device mismatch, and reliability issues. This paper introduces a CMOS Look-Up Table (LUT)-based Neural Accelerator (LUT-NA) framework that reduces the power, latency, and area consumption of traditional digital accelerators through pre-computed, faster look-ups while avoiding noise and mismatch of analog circuits. To solve the scalability issues of conventional LUT-based computation, we split the high-precision multiply and accumulate (MAC) operations into lower-precision MACs using a divide-and-conquer-based approach. We show that LUT-NA achieves up to $29.54\times$ lower area with $3.34\times$ lower energy per inference task than traditional LUT-based techniques and up to $1.23\times$ lower area with $1.80\times$ lower energy per inference task than conventional digital MAC-based techniques (Wallace Tree/Array Multipliers) without retraining and without affecting accuracy, even on lottery ticket pruned (LTP) models that already reduce the number of required MAC operations by up to 98%. Finally, we introduce mixed precision analysis in LUT-NA framework for various LTP models (VGG11, VGG19, Resnet18, Resnet34, GoogleNet) that achieved up to $32.22\times$-$50.95\times$ lower area across models with $3.68\times$-$6.25\times$ lower energy per inference than traditional LUT-based techniques, and up to $1.35\times$-$2.14\times$ lower area requirement with $1.99\times$-$3.38\times$ lower energy per inference across models as compared to conventional digital MAC-based techniques with $\sim$1% accuracy loss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05282v2</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ovishake Sen, Chukwufumnanya Ogbogu, Peyman Dehghanzadeh, Janardhan Rao Doppa, Swarup Bhunia, Partha Pratim Pande, Baibhab Chatterjee</dc:creator>
    </item>
  </channel>
</rss>
