<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Oct 2024 02:04:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Circuits and Systems for Embodied AI: Exploring uJ Multi-Modal Perception for Nano-UAVs on the Kraken Shield</title>
      <link>https://arxiv.org/abs/2410.09054</link>
      <description>arXiv:2410.09054v1 Announce Type: new 
Abstract: Embodied artificial intelligence (AI) requires pushing complex multi-modal models to the extreme edge for time-constrained tasks such as autonomous navigation of robots and vehicles. On small form-factor devices, e.g., nano-sized unmanned aerial vehicles (UAVs), such challenges are exacerbated by stringent constraints on energy efficiency and weight. In this paper, we explore embodied multi-modal AI-based perception for Nano-UAVs with the Kraken shield, a 7g multi-sensor (frame-based and event-based imagers) board based on Kraken, a 22 nm SoC featuring multiple acceleration engines for multi-modal event and frame-based inference based on spiking (SNN) and ternary (TNN) neural networks, respectively. Kraken can execute SNN real-time inference for depth estimation at 1.02k inf/s, 18 {\mu}J/inf, TNN real-time inference for object classification at 10k inf/s, 6 {\mu}J/inf, and real-time inference for obstacle avoidance at 221 frame/s, 750 {\mu}J/inf.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09054v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Viviane Potocnik, Alfio Di Mauro, Lorenzo Lamberti, Victor Kartsch, Moritz Scherer, Francesco Conti, Luca Benini</dc:creator>
    </item>
    <item>
      <title>GUST: Graph Edge-Coloring Utilization for Accelerating Sparse Matrix Vector Multiplication</title>
      <link>https://arxiv.org/abs/2410.09106</link>
      <description>arXiv:2410.09106v1 Announce Type: new 
Abstract: Sparse matrix-vector multiplication (SpMV) plays a vital role in various scientific and engineering fields, from scientific computing to machine learning. Traditional general-purpose processors often fall short of their peak performance with sparse data, leading to the development of domain-specific architectures to enhance SpMV. Yet, these specialized approaches, whether tailored explicitly for SpMV or adapted from matrix-matrix multiplication accelerators, still face challenges in fully utilizing hardware resources as a result of sparsity. To tackle this problem, we introduce GUST, a hardware/software co-design, the key insight of which lies in separating multipliers and adders in the hardware, thereby enabling resource sharing across multiple rows and columns, leading to efficient hardware utilization and ameliorating negative performance impacts from sparsity. Resource sharing, however, can lead to collisions, a problem we address through a specially devised edge-coloring scheduling algorithm. Our comparisons with various prior domain specific architectures using real-world datasets shows the effectiveness of GUST, with an average hardware utilization of $33.67\%$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09106v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Armin Gerami, Bahar Asgari</dc:creator>
    </item>
    <item>
      <title>M$^2$-ViT: Accelerating Hybrid Vision Transformers with Two-Level Mixed Quantization</title>
      <link>https://arxiv.org/abs/2410.09113</link>
      <description>arXiv:2410.09113v1 Announce Type: new 
Abstract: Although Vision Transformers (ViTs) have achieved significant success, their intensive computations and substantial memory overheads challenge their deployment on edge devices. To address this, efficient ViTs have emerged, typically featuring Convolution-Transformer hybrid architectures to enhance both accuracy and hardware efficiency. While prior work has explored quantization for efficient ViTs to marry the best of efficient hybrid ViT architectures and quantization, it focuses on uniform quantization and overlooks the potential advantages of mixed quantization. Meanwhile, although several works have studied mixed quantization for standard ViTs, they are not directly applicable to hybrid ViTs due to their distinct algorithmic and hardware characteristics. To bridge this gap, we present M$^2$-ViT to accelerate Convolution-Transformer hybrid efficient ViTs with two-level mixed quantization. Specifically, we introduce a hardware-friendly two-level mixed quantization (M$^2$Q) strategy, characterized by both mixed quantization precision and mixed quantization schemes (i.e., uniform and power-of-two), to exploit the architectural properties of efficient ViTs. We further build a dedicated accelerator with heterogeneous computing engines to transform our algorithmic benefits into real hardware improvements. Experimental results validate our effectiveness, showcasing an average of $80\%$ energy-delay product (EDP) saving with comparable quantization accuracy compared to the prior work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09113v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanbiao Liang, Huihong Shi, Zhongfeng Wang</dc:creator>
    </item>
    <item>
      <title>Energy-efficient SNN Architecture using 3nm FinFET Multiport SRAM-based CIM with Online Learning</title>
      <link>https://arxiv.org/abs/2410.09130</link>
      <description>arXiv:2410.09130v1 Announce Type: new 
Abstract: Current Artificial Intelligence (AI) computation systems face challenges, primarily from the memory-wall issue, limiting overall system-level performance, especially for Edge devices with constrained battery budgets, such as smartphones, wearables, and Internet-of-Things sensor systems. In this paper, we propose a new SRAM-based Compute-In-Memory (CIM) accelerator optimized for Spiking Neural Networks (SNNs) Inference. Our proposed architecture employs a multiport SRAM design with multiple decoupled Read ports to enhance the throughput and Transposable Read-Write ports to facilitate online learning. Furthermore, we develop an Arbiter circuit for efficient data-processing and port allocations during the computation. Results for a 128$\times$128 array in 3nm FinFET technology demonstrate a 3.1$\times$ improvement in speed and a 2.2$\times$ enhancement in energy efficiency with our proposed multiport SRAM design compared to the traditional single-port design. At system-level, a throughput of 44 MInf/s at 607 pJ/Inf and 29mW is achieved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09130v1</guid>
      <category>cs.AR</category>
      <category>cs.NE</category>
      <category>eess.IV</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Huijbregts, Liu Hsiao-Hsuan, Paul Detterer, Said Hamdioui, Amirreza Yousefzadeh, Rajendra Bishnoi</dc:creator>
    </item>
    <item>
      <title>MFIT: Multi-Fidelity Thermal Modeling for 2.5D and 3D Multi-Chiplet Architectures</title>
      <link>https://arxiv.org/abs/2410.09188</link>
      <description>arXiv:2410.09188v1 Announce Type: new 
Abstract: Rapidly evolving artificial intelligence and machine learning applications require ever-increasing computational capabilities, while monolithic 2D design technologies approach their limits. Heterogeneous integration of smaller chiplets using a 2.5D silicon interposer and 3D packaging has emerged as a promising paradigm to address this limit and meet performance demands. These approaches offer a significant cost reduction and higher manufacturing yield than monolithic 2D integrated circuits. However, the compact arrangement and high compute density exacerbate the thermal management challenges, potentially compromising performance. Addressing these thermal modeling challenges is critical, especially as system sizes grow and different design stages require varying levels of accuracy and speed. Since no single thermal modeling technique meets all these needs, this paper introduces MFIT, a range of multi-fidelity thermal models that effectively balance accuracy and speed. These multi-fidelity models can enable efficient design space exploration and runtime thermal management. Our extensive testing on systems with 16, 36, and 64 2.5D integrated chiplets and 16x3 3D integrated chiplets demonstrates that these models can reduce execution times from days to mere seconds and milliseconds with negligible loss in accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09188v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Pfromm, Alish Kanani, Harsh Sharma, Parth Solanki, Eric Tervo, Jaehyun Park, Janardhan Rao Doppa, Partha Pratim Pande, Umit Y. Ogras</dc:creator>
    </item>
    <item>
      <title>Tackling Coherent Noise in Quantum Computing via Cross-Layer Compiler Optimization</title>
      <link>https://arxiv.org/abs/2410.09664</link>
      <description>arXiv:2410.09664v1 Announce Type: new 
Abstract: Quantum computing hardware is affected by quantum noise that undermine the quality of results of an executed quantum program. Amongst other quantum noises, coherent error that caused by parameter drifting and miscalibration, remains critical. While coherent error mitigation has been studied before, studies focused either on gate-level or pulse-level -- missing cross-level optimization opportunities; And most of them only target single-qubit gates -- while multi-qubit gates are also used in practice.
  To address above limitations, this work proposes a cross-layer approach for coherent error mitigation that considers program-level, gate-level, and pulse-level compiler optimizations, by leveraging the hidden inverse theory, and exploiting the structure inside different quantum programs, while also considering multi-qubit gates. We implemented our approach as compiler optimization passes, and integrated into IBM Qiskit framework. We tested our technique on real quantum computer (IBM-Brisbane), and demonstrated up to 92% fidelity improvements (45% on average), on several benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09664v1</guid>
      <category>cs.AR</category>
      <category>quant-ph</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiangyu Ren, Junjie Wan, Zhiding Liang, Antonio Barbalace</dc:creator>
    </item>
    <item>
      <title>Messaging-based Intelligent Processing Unit (m-IPU) for next generation AI computing</title>
      <link>https://arxiv.org/abs/2410.09961</link>
      <description>arXiv:2410.09961v1 Announce Type: new 
Abstract: Recent advancements in Artificial Intelligence (AI) algorithms have sparked a race to enhance hardware capabilities for accelerated task processing. While significant strides have been made, particularly in areas like computer vision, the progress of AI algorithms appears to have outpaced hardware development, as specialized hardware struggles to keep up with the ever-expanding algorithmic landscape. To address this gap, we propose a new accelerator architecture, called messaging-based intelligent processing unit (m-IPU), capable of runtime configuration to cater to various AI tasks. Central to this hardware is a programmable interconnection mechanism, relying on message passing between compute elements termed Sites. While the messaging between compute elements is a known concept for Network-on-Chip or multi-core architectures, our hardware can be categorized as a new class of coarse-grained reconfigurable architecture (CGRA), specially optimized for AI workloads. In this paper, we highlight m-IPU's fundamental advantages for machine learning applications. We illustrate the efficacy through implementations of a neural network, matrix multiplications, and convolution operations, showcasing lower latency compared to the state-of-the-art. Our simulation-based experiments, conducted on the TSMC 28nm technology node, reveal minimal power consumption of 44.5 mW with 94,200 cells utilization. For 3D convolution operations on (32 x 128) images, each (256 x 256), using a (3 x 3) filter and 4,096 Sites at a frequency of 100 MHz, m-IPU achieves processing in just 503.3 milliseconds. These results underscore the potential of m-IPU as a unified, scalable, and high-performance hardware architecture tailored for future AI applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09961v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md. Rownak Hossain Chowdhury, Mostafizur Rahman</dc:creator>
    </item>
    <item>
      <title>Work-in-Progress: Real-Time Neural Network Inference on a Custom RISC-V Multicore Vector Processor</title>
      <link>https://arxiv.org/abs/2410.10340</link>
      <description>arXiv:2410.10340v1 Announce Type: new 
Abstract: Neural networks are increasingly used in real-time systems, such as automated driving applications. This requires high-performance hardware with predictable timing behavior. State-of-the-art real-time hardware is limited in memory and compute resources. On the other hand, modern accelerator systems lack the necessary predictability properties, mainly due to interference in the memory subsystem.
  We present a new hardware architecture with an accompanying compiler-based deployment toolchain to close this gap between performance and predictability. The hardware architecture consists of a multicore vector processor with predictable cores, each with local scratchpad memories. A central management core facilitates access to shared external memory through a static schedule calculated at compile-time. The presented compiler exploits the fixed data flow of neural networks and WCET estimates of subtasks running on individual cores to compute this schedule.
  Through this approach, the WCET estimate of the overall system can be obtained from the subtask WCET estimates, data transfer times, and access times of the shared memory in conjunction with the schedule calculated by the compiler.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10340v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maximilian Kirschner, Konstantin Dudzik, J\"urgen Becker</dc:creator>
    </item>
    <item>
      <title>Dynamic Power Control in a Hardware Neural Network with Error-Configurable MAC Units</title>
      <link>https://arxiv.org/abs/2410.10545</link>
      <description>arXiv:2410.10545v1 Announce Type: new 
Abstract: Multi-Layer Perceptrons (MLP) are powerful tools for representing complex, non-linear relationships, making them essential for diverse machine learning and AI applications. Efficient hardware implementation of MLPs can be achieved through many hardware and architectural design techniques. These networks excel at predictive modeling and classification tasks like image classification, making them a popular choice. Approximate computing techniques are increasingly used to optimize critical path delay, area, power, and overall hardware efficiency in high-performance computing systems through controlled error and related trade-offs. This study proposes a hardware MLP neural network implemented in 45nm CMOS technology, in which MAC units of the neurons incorporate error and power controllable approximate multipliers for classification of the MNIST dataset. The optimized network consists of 10 neurons within the hidden layers, occupying 0.026mm2 of area, with 5.55mW at 100MHz frequency in accurate mode and 4.81mW in lowest accuracy mode. The experiments indicate that the proposed design achieves a maximum rate of 13.33% decrease overall and 24.78% in each neuron's power consumption with only a 0.92% decrease in accuracy in comparison with accurate circuit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10545v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maedeh Ghaderi, Arvin Delavari, Faraz Ghoreishy, Sattar Mirzakuchaki</dc:creator>
    </item>
    <item>
      <title>Advancing Experimental Platforms for UAV Communications: Insights from AERPAW'S Digital Twin</title>
      <link>https://arxiv.org/abs/2410.09648</link>
      <description>arXiv:2410.09648v1 Announce Type: cross 
Abstract: The rapid evolution of 5G and beyond has advanced space-air-terrestrial networks, with unmanned aerial vehicles (UAVs) offering enhanced coverage, flexible configurations, and cost efficiency. However, deploying UAV-based systems presents challenges including varying propagation conditions and hardware limitations. While simulators and theoretical models have been developed, real-world experimentation is critically important to validate the research. Digital twins, virtual replicas of physical systems, enable emulation that bridge theory and practice. This paper presents our experimental results from AERPAW's digital twin, showcasing its ability to simulate UAV communication scenarios and providing insights into system performance and reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09648v1</guid>
      <category>eess.SY</category>
      <category>cs.AR</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Moore, Aly Sabri Abdalla, Charles Ueltschey, An{\i}l G\"urses, \"Ozg\"ur \"Ozdemir, Mihail L. Sichitiu, \.Ismail G\"uven\c{c}, Vuk Marojevic</dc:creator>
    </item>
    <item>
      <title>RISC-V Needs Secure 'Wheels': the MCU Initiator-Side Perspective</title>
      <link>https://arxiv.org/abs/2410.09839</link>
      <description>arXiv:2410.09839v1 Announce Type: cross 
Abstract: The automotive industry is experiencing a massive paradigm shift. Cars are becoming increasingly autonomous, connected, and computerized. Modern electrical/electronic (E/E) architectures are pushing for an unforeseen functionality integration density, resulting in physically separate Electronic Control Units (ECUs) becoming virtualized and mapped to logical partitions within a single physical microcontroller (MCU). While functional safety (FuSa) has been pivotal for vehicle certification for decades, the increasing connectivity and advances have opened the door for a number of car hacks and attacks. This development drives (cyber-)security requirements in cars, and has paved the way for the release of the new security certification standard ISO21434. RISC-V has great potential to transform automotive computing systems, but we argue that current ISA/extensions are not ready yet. This paper provides our critical perspective on the existing RISC-V limitations, particularly on the upcoming WorldGuard technology, to address virtualized MCU requirements in line with foreseen automotive applications and ISO21434 directives. We then present our proposal for the required ISA extensions to address such limitations, mainly targeting initiator-side protection. Finally, we explain our roadmap towards a full open-source proof-of-concept (PoC), which includes extending QEMU, an open-source RISC-V core, and building a complete software stack.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09839v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sandro Pinto, Jose Martins, Manuel Rodriguez, Luis Cunha, Georg Schmalz, Uwe Moslehner, Kai Dieffenbach, Thomas Roecker</dc:creator>
    </item>
    <item>
      <title>Tracing Human Stress from Physiological Signals using UWB Radar</title>
      <link>https://arxiv.org/abs/2410.10155</link>
      <description>arXiv:2410.10155v1 Announce Type: cross 
Abstract: Stress tracing is an important research domain that supports many applications, such as health care and stress management; and its closest related works are derived from stress detection. However, these existing works cannot well address two important challenges facing stress detection. First, most of these studies involve asking users to wear physiological sensors to detect their stress states, which has a negative impact on the user experience. Second, these studies have failed to effectively utilize multimodal physiological signals, which results in less satisfactory detection results. This paper formally defines the stress tracing problem, which emphasizes the continuous detection of human stress states. A novel deep stress tracing method, named DST, is presented. Note that DST proposes tracing human stress based on physiological signals collected by a noncontact ultrawideband radar, which is more friendly to users when collecting their physiological signals. In DST, a signal extraction module is carefully designed at first to robustly extract multimodal physiological signals from the raw RF data of the radar, even in the presence of body movement. Afterward, a multimodal fusion module is proposed in DST to ensure that the extracted multimodal physiological signals can be effectively fused and utilized. Extensive experiments are conducted on three real-world datasets, including one self-collected dataset and two publicity datasets. Experimental results show that the proposed DST method significantly outperforms all the baselines in terms of tracing human stress states. On average, DST averagely provides a 6.31% increase in detection accuracy on all datasets, compared with the best baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10155v1</guid>
      <category>cs.HC</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jia Xu, Teng Xiao, Pin Lv, Zhe Chen, Chao Cai, Yang Zhang, Zehui Xiong</dc:creator>
    </item>
    <item>
      <title>LoopTree: Exploring the Fused-layer Dataflow Accelerator Design Space</title>
      <link>https://arxiv.org/abs/2409.13625</link>
      <description>arXiv:2409.13625v4 Announce Type: replace 
Abstract: Latency and energy consumption are key metrics in the performance of deep neural network (DNN) accelerators. A significant factor contributing to latency and energy is data transfers. One method to reduce transfers or data is reusing data when multiple operations use the same data. Fused-layer accelerators reuse data across operations in different layers by retaining intermediate data in on-chip buffers, which has been shown to reduce energy consumption and latency. Moreover, the intermediate data is often tiled (i.e., broken into chunks) to reduce the on-chip buffer capacity required to reuse the data. Because on-chip buffer capacity is frequently more limited than computation units, fused-layer dataflow accelerators may also recompute certain parts of the intermediate data instead of retaining them in a buffer. Achieving efficient trade-offs between on-chip buffer capacity, off-chip transfers, and recomputation requires systematic exploration of the fused-layer dataflow design space. However, prior work only explored a subset of the design space, and more efficient designs are left unexplored.
  In this work, we propose (1) a more extensive design space that has more choices in terms of tiling, data retention, recomputation and, importantly, allows us to explore them in combination, (2) a taxonomy to systematically specify designs, and (3) a model, LoopTree, to evaluate the latency, energy consumption, buffer capacity requirements, and off-chip transfers of designs in this design space. We validate our model against a representative set of prior architectures, achieving a worst-case 4% error. Finally, we present case studies that show how exploring this larger space results in more efficient designs (e.g., up to a 10$\times$ buffer capacity reduction to achieve the same off-chip transfers).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13625v4</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TCASAI.2024.3461716)</arxiv:DOI>
      <dc:creator>Michael Gilbert, Yannan Nellie Wu, Joel S. Emer, Vivienne Sze</dc:creator>
    </item>
    <item>
      <title>In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs</title>
      <link>https://arxiv.org/abs/2409.14360</link>
      <description>arXiv:2409.14360v3 Announce Type: replace 
Abstract: Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage systems. To increase capacity, high bit-density cells, such as Triple-Level Cell (TLC), are utilized within 3D SSDs. However, due to the inferior performance of TLC, a portion of TLCs is configured to operate as Single-Level Cell (SLC) to provide high performance, with host data initially directed to the SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated as an SLC cache to achieve high SSD performance by writing host data at the SLC speed. Given the limited size of the SLC cache, block reclamation is necessary to free up the SLC cache during idle periods. However, our preliminary studies indicate that the SLC cache can lead to a performance cliff if filled rapidly and cause significant write amplification when data migration occurs during idle times.
  In this work, we propose leveraging a reprogram operation to address these challenges. Specifically, when the SLC cache is full or during idle periods, a reprogram operation is performed to switch used SLC pages to TLC pages in place (termed In-place Switch, IPS). Subsequently, other free TLC space is allocated as the new SLC cache. IPS can continuously provide sufficient SLC cache within SSDs, significantly improving write performance and reducing write amplification. Experimental results demonstrate that IPS can reduce write latency and write amplification by up to 0.75 times and 0.53 times, respectively, compared to state-of-the-art SLC cache technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14360v3</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xufeng Yang, Zhengjian Cong, Congming Gao</dc:creator>
    </item>
    <item>
      <title>Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective</title>
      <link>https://arxiv.org/abs/2410.04466</link>
      <description>arXiv:2410.04466v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various fields, from natural language understanding to text generation. Compared to non-generative LLMs like BERT and DeBERTa, generative LLMs like GPT series and Llama series are currently the main focus due to their superior algorithmic performance. The advancements in generative LLMs are closely intertwined with the development of hardware capabilities. Various hardware platforms exhibit distinct hardware characteristics, which can help improve LLM inference performance. Therefore, this paper comprehensively surveys efficient generative LLM inference on different hardware platforms. First, we provide an overview of the algorithm architecture of mainstream generative LLMs and delve into the inference process. Then, we summarize different optimization methods for different platforms such as CPU, GPU, FPGA, ASIC, and PIM/NDP, and provide inference results for generative LLMs. Furthermore, we perform a qualitative and quantitative comparison of inference performance with batch sizes 1 and 8 on different hardware platforms by considering hardware power consumption, absolute inference speed (tokens/s), and energy efficiency (tokens/J). We compare the performance of the same optimization methods across different hardware platforms, the performance across different hardware platforms, and the performance of different methods on the same hardware platform. This provides a systematic and comprehensive summary of existing inference acceleration work by integrating software optimization methods and hardware platforms, which can point to the future trends and potential developments of generative LLMs and hardware technology for edge-side scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04466v2</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinhao Li, Jiaming Xu, Shan Huang, Yonghua Chen, Wen Li, Jun Liu, Yaoxiu Lian, Jiayi Pan, Li Ding, Hao Zhou, Yu Wang, Guohao Dai</dc:creator>
    </item>
    <item>
      <title>Reducing the Barriers to Entry for Foundation Model Training</title>
      <link>https://arxiv.org/abs/2404.08811</link>
      <description>arXiv:2404.08811v2 Announce Type: replace-cross 
Abstract: The world has recently witnessed an unprecedented acceleration in demands for Machine Learning and Artificial Intelligence applications. This spike in demand has imposed tremendous strain on the underlying technology stack in supply chain, GPU-accelerated hardware, software, datacenter power density, and energy consumption. If left on the current technological trajectory, future demands show insurmountable spending trends, further limiting market players, stifling innovation, and widening the technology gap. To address these challenges, we propose a fundamental change in the AI training infrastructure throughout the technology ecosystem. The changes require advancements in supercomputing and novel AI training approaches, from high-end software to low-level hardware, microprocessor, and chip design, while advancing the energy efficiency required by a sustainable infrastructure. This paper presents the analytical framework that quantitatively highlights the challenges and points to the opportunities to reduce the barriers to entry for training large language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08811v2</guid>
      <category>cs.ET</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paolo Faraboschi, Ellis Giles, Justin Hotard, Konstanty Owczarek, Andrew Wheeler</dc:creator>
    </item>
  </channel>
</rss>
