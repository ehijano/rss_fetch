<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 Jan 2026 02:51:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The generalised balanced power diagram: flat sections, affine transformations and an improved rendering algorithm</title>
      <link>https://arxiv.org/abs/2601.18593</link>
      <description>arXiv:2601.18593v1 Announce Type: new 
Abstract: The generalised balanced power diagram (GBPD) is regarded in the literature as a suitable geometric model for describing polycrystalline microstructures with curved grain boundaries. This article compiles properties of GBPDs with regard to affine transformations and flat sections. Furthermore, it extends an algorithm known for power diagrams for generating digital images, which is more efficient than the usual brute force approach, on GBPDs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18593v1</guid>
      <category>stat.CO</category>
      <category>cond-mat.mtrl-sci</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felix Ballani</dc:creator>
    </item>
    <item>
      <title>A Unified Approach to Concurrent, Parallel Map-Reduce in R using Futures</title>
      <link>https://arxiv.org/abs/2601.17578</link>
      <description>arXiv:2601.17578v1 Announce Type: cross 
Abstract: The R ecosystem offers a rich variety of map-reduce application programming interfaces (APIs) for iterative computations, yet parallelizing code across these diverse frameworks requires learning multiple, often incompatible, parallel APIs. The futurize package addresses this challenge by providing a single function, futurize(), which transpiles sequential map-reduce expressions into their parallel equivalents in the future ecosystem, which performs all the heavy lifting. By leveraging R's native pipe operator, users can parallelize existing code with minimal refactoring -- often by simply appending `|&gt; futurize()' to an expression. The package supports classical map-reduce functions from base R, purrr, crossmap, foreach, plyr, BiocParallel, e.g., lapply(xs, fcn) |&gt; futurize() and map(xs, fcn) |&gt; futurize(), as well as a growing set of domain-specific packages, e.g., boot, caret, glmnet, lme4, mgcv, and tm. By abstracting away the underlying parallel machinery, and unifying handling of future options, the package enables developers to declare what to parallelize via futurize(), and end-users to choose how via plan(). This article describes the philosophy, design, and implementation of futurize, demonstrates its usage across various map-reduce paradigms, and discusses its role in simplifying parallel computing in R.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17578v1</guid>
      <category>cs.DC</category>
      <category>stat.CO</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henrik Bengtsson</dc:creator>
    </item>
    <item>
      <title>The effect of collinearity and sample size on linear regression results: a simulation study</title>
      <link>https://arxiv.org/abs/2601.18072</link>
      <description>arXiv:2601.18072v1 Announce Type: cross 
Abstract: Background: Multicollinearity inflates the variance of OLS coefficients, widening confidence intervals and reducing inferential reliability. Yet fixed variance inflation factor (VIF) cut-offs are often applied uniformly across studies with very different sample sizes, even though collinearity is a finite-sample problem. We quantify how collinearity and sample size jointly affect linear regression performance and provide practical guidance for interpreting VIFs.
  Methods: We simulated data across sample sizes N=100-100,000 and collinearity levels VIF=1-50. For each scenario we generated 1,000 datasets, fitted OLS models, and assessed coverage, mean absolute error (MAE), bias, traditional power (CI excludes 0), and precision assurance (probability the 95% CI lies within a prespecified margin around the true effect). We also evaluated a biased, misspecified setting by omitting a relevant predictor to study bias amplification.
  Results: Under correct specification, collinearity did not materially affect nominal coverage and did not introduce systematic bias, but it reduced precision in small samples: at N=100, even mild collinearity (VIF&lt;2) inflated MAE and markedly reduced both power metrics, whereas at N&gt;=50,000 estimates were robust even at VIF=50. Under misspecification, collinearity strongly amplified bias, increasing errors, reducing coverage, and sharply degrading both precision assurance and traditional power even at low VIF.
  Conclusion: VIF thresholds should not be applied mechanically. Collinearity must be interpreted in relation to sample size and potential sources of bias; removing predictors solely to reduce VIF can worsen inference via omitted-variable bias. The accompanying heatmaps provide a practical reference across study sizes and modelling assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18072v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephanie CC van der Lubbe, Jose M Valderas, Evangelos Kontopantelis</dc:creator>
    </item>
    <item>
      <title>Exact Minimum-Volume Confidence Set Intersection for Multinomial Outcomes</title>
      <link>https://arxiv.org/abs/2601.18145</link>
      <description>arXiv:2601.18145v1 Announce Type: cross 
Abstract: Computation of confidence sets is central to data science and machine learning, serving as the workhorse of A/B testing and underpinning the operation and analysis of reinforcement learning algorithms. Among all valid confidence sets for the multinomial parameter, minimum-volume confidence sets (MVCs) are optimal in that they minimize average volume, but they are defined as level sets of an exact p-value that is discontinuous and difficult to compute. Rather than attempting to characterize the geometry of MVCs directly, this paper studies a practically motivated decision problem: given two observed multinomial outcomes, can one certify whether their MVCs intersect? We present a certified, tolerance-aware algorithm for this intersection problem. The method exploits the fact that likelihood ordering induces halfspace constraints in log-odds coordinates, enabling adaptive geometric partitioning of parameter space and computable lower and upper bounds on p-values over each cell. For three categories, this yields an efficient and provably sound algorithm that either certifies intersection, certifies disjointness, or returns an indeterminate result when the decision lies within a prescribed margin. We further show how the approach extends to higher dimensions. The results demonstrate that, despite their irregular geometry, MVCs admit reliable certified decision procedures for core tasks in A/B testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18145v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heguang Lin, Binhao Chen, Mengze Li, Daniel Pimentel-Alarc\'on, Matthew L. Malloy</dc:creator>
    </item>
    <item>
      <title>A label-switching algorithm for fast core-periphery identification</title>
      <link>https://arxiv.org/abs/2506.02069</link>
      <description>arXiv:2506.02069v3 Announce Type: replace 
Abstract: Core-periphery (CP) structure is frequently observed in networks where the nodes form two distinct groups: a small, densely interconnected core and a sparse periphery. Borgatti and Everett (2000) proposed one of the most popular methods to identify and quantify CP structure by comparing the observed network with an ``ideal'' CP structure. While this metric has been widely used, an improved algorithm is still needed. In this work, we detail a greedy, label-switching algorithm to identify CP structure that is both fast and accurate. By leveraging a mathematical reformulation of the CP metric, our proposed heuristic offers an order-of-magnitude improvement on the number of operations compared to a naive implementation. We prove that the algorithm monotonically ascends to a local maximum while consistently yielding solutions within 90% of the global optimum on small toy networks. On synthetic networks, our algorithm exhibits superior classification accuracies and run-times compared to a popular competing method, and on one-real world network, it is 340 times faster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02069v3</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Yanchenko, Srijan Sengupta</dc:creator>
    </item>
    <item>
      <title>Streaming Sliced Optimal Transport</title>
      <link>https://arxiv.org/abs/2505.06835</link>
      <description>arXiv:2505.06835v3 Announce Type: replace-cross 
Abstract: Sliced optimal transport (SOT), or sliced Wasserstein (SW) distance, is widely recognized for its statistical and computational scalability. In this work, we further enhance computational scalability by proposing the first method for estimating SW from sample streams, called \emph{streaming sliced Wasserstein} (Stream-SW). To define Stream-SW, we first introduce a streaming estimator of the one-dimensional Wasserstein distance (1DW). Since the 1DW has a closed-form expression, given by the absolute difference between the quantile functions of the compared distributions, we leverage quantile approximation techniques for sample streams to define a streaming 1DW estimator. By applying the streaming 1DW to all projections, we obtain Stream-SW. The key advantage of Stream-SW is its low memory complexity while providing theoretical guarantees on the approximation error. We demonstrate that Stream-SW achieves a more accurate approximation of SW than random subsampling, with lower memory consumption, when comparing Gaussian distributions and mixtures of Gaussians from streaming samples. Additionally, we conduct experiments on point cloud classification, point cloud gradient flows, and streaming change point detection to further highlight the favorable performance of the proposed Stream-SW</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06835v3</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen</dc:creator>
    </item>
    <item>
      <title>Assessing the impact of variance heterogeneity and misspecification in mixed-effects location-scale models</title>
      <link>https://arxiv.org/abs/2505.18038</link>
      <description>arXiv:2505.18038v2 Announce Type: replace-cross 
Abstract: Linear Mixed Model (LMM) is a common statistical approach to model the relation between exposure and outcome while capturing individual variability through random effects. However, this model assumes the homogeneity of the error term's variance. Breaking this assumption, known as homoscedasticity, can bias estimates and, consequently, may change a study's conclusions. If this assumption is unmet, the mixed-effect location-scale model (MELSM) offers a solution to account for within-individual variability. Our work explores how LMMs and MELSMs behave when the homoscedasticity assumption is not met. Further, we study how misspecification affects inference for MELSM. To this aim, we propose a simulation study with longitudinal data and evaluate the estimates' bias and coverage. Our simulations show that neglecting heteroscedasticity in LMMs leads to loss of coverage for the estimated coefficients and biases the estimates of the standard deviations of the random effects. In MELSMs, scale misspecification does not bias the location model, but location misspecification alters the scale estimates. Our simulation study illustrates the importance of modelling heteroscedasticity, with potential implications beyond mixed effect models, for generalised linear mixed models for non-normal outcomes and joint models with survival data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18038v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1186/s12874-025-02755-3</arxiv:DOI>
      <dc:creator>Vincent Jeanselme, Marco Palma, Jessica K Barrett</dc:creator>
    </item>
    <item>
      <title>Improving the Accuracy of Amortized Model Comparison with Self-Consistency</title>
      <link>https://arxiv.org/abs/2512.14308</link>
      <description>arXiv:2512.14308v3 Announce Type: replace-cross 
Abstract: Amortized Bayesian inference (ABI) offers fast, scalable approximations to posterior densities by training neural surrogates on data simulated from the statistical model. However, ABI methods are highly sensitive to model misspecification: when observed data fall outside the training distribution (generative scope of the statistical models), neural surrogates can behave unpredictably. This makes it a challenge in a model comparison setting, where multiple statistical models are considered, of which at least some are misspecified. Recent work on self-consistency (SC) provides a promising remedy to this issue, accessible even for empirical data (without ground-truth labels). In this work, we investigate how SC can improve amortized model comparison conceptualized in four different ways. Across two synthetic and two real-world case studies, we find that approaches for model comparison that estimate marginal likelihoods through approximate parameter posteriors consistently outperform methods that directly approximate model evidence or posterior model probabilities. SC training improves robustness when the likelihood is available, even under severe model misspecification. The benefits of SC for methods without access of analytic likelihoods are more limited and inconsistent. Our results suggest practical guidance for reliable amortized Bayesian model comparison: prefer parameter posterior-based methods and augment them with SC training on empirical datasets to mitigate extrapolation bias under model misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14308v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>\v{S}imon Kucharsk\'y, Aayush Mishra, Daniel Habermann, Stefan T. Radev, Paul-Christian B\"urkner</dc:creator>
    </item>
  </channel>
</rss>
