<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Dec 2025 03:45:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Spatial Two-Stage Hierarchical Optimization Analysis for Site Selection of Bitcoin Mining in South Korea</title>
      <link>https://arxiv.org/abs/2511.21773</link>
      <description>arXiv:2511.21773v1 Announce Type: new 
Abstract: South Korea faces the dual challenge of managing growing distributed solar energy surpluses and the high energy demand of industries like Bitcoin mining. Leveraging mining operations as a flexible load to monetize this `net-metering surplus' presents a viable synergy, but requires a robust site selection methodology. Traditional GIS-based Multi-Criteria Decision Analysis (MCDA) struggles with subjective weighting and integrating heterogeneous spatial data (areal-level and lattice-level). This thesis develops and implements a Two-Stage Hierarchical Optimization framework to overcome these limitations. Stage 1 (Areal-Level) employs a cost-benefit optimization to determine the optimal number ($K^*$) and combination of regions, maximizing a final adjusted net profit by balancing surplus power revenue against detailed land and non-linear infrastructure costs. Stage 2 (Point-Level) then uses a GIS-based sliding window search within these selected regions, applying topographic (slope $&lt; 6.0^\circ$) and land-use constraints at a 30m resolution to identify physically constructible `unit sites'. The model identified an optimal configuration of $K^*=3$ regions (Yongin, Damyang, Miryang) yielding a maximum potential net profit of approximately \$307 million. Crucially, the Stage 2 screening revealed that Yongin, the most profitable region, was also the most physically constrained, 87\% of sites filtered out. This research contributes a scalable, objective framework for energy infrastructure siting that effectively integrates multi-scale spatial data. It provides a data-driven strategy for policymakers and grid operators (like Korea Electric Power Corporation) to monetize curtailed renewables and enhance grid stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21773v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yoonseul Choi, Jungsoon Choi</dc:creator>
    </item>
    <item>
      <title>Survey-Based Estimation of Probe Group Sizes in the Network Scale-up Method: A Case Study from Jordan</title>
      <link>https://arxiv.org/abs/2511.21938</link>
      <description>arXiv:2511.21938v1 Announce Type: new 
Abstract: Estimating the size of marginalized populations is a persistent challenge in survey statistics and public health, especially where stigma and legal restrictions exclude such groups from census and administrative data. Migrant domestic workers in Jordan represent one such population. We employ the Network Scale-up Method using the direct probe group method, estimating probe group sizes from survey respondents' own membership rather than relying on external counts. Using data from a nationally representative household survey in Jordan, we combine the direct probe group method with Bayesian logistic mixed-effects models to stabilize small-area estimates at the Governorate level. We validate the method against census data, demonstrating that direct probe group estimates yield reliable inference and provide a practical alternative where known probe group sizes are unavailable. Our results highlight regional variation in social network size and connectivity to migrant domestic workers. We argue that the direct probe group method is more likely to satisfy the conditions required for unbiased estimation than relying on official record sizes. This work provides the first systematic validation of the direct probe group method in a small-area setting and offers guidance for adapting the Network Scale-up Method to surveys with limited sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21938v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ian Laga</dc:creator>
    </item>
    <item>
      <title>A two-parameter, minimal-data model to predict dengue cases: the 2022-2023 outbreak in Florida, USA</title>
      <link>https://arxiv.org/abs/2511.22040</link>
      <description>arXiv:2511.22040v1 Announce Type: new 
Abstract: Reliable and timely dengue predictions provide actionable lead time for targeted vector control and clinical preparedness, reducing preventable diseases and health-system costs in at-risk communities. Dengue forecasting often relies on site-specific covariates and entomological data, limiting generalizability in data-sparse settings. We propose a data-parsimonious (DP) framework based on the incidence versus cumulative cases (ICC) curve, extending it from a basic SIR to a two-population SEIR model for dengue. Our DP model uses only the target season's incidence time series and estimates only two parameters, reducing noise and computational burden. A Bayesian extension quantifies the case reporting and fitting uncertainty to produce calibrated predictive intervals. We evaluated the performance of the DP model in the 2022-2023 outbreaks in Florida, where standardized clinical tests and reporting support accurate case determination. The DP framework demonstrates competitive predictive performance at substantially lower computational cost than more elaborate models, making it suitable for dengue early detection where dense surveillance or long historical records are unavailable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22040v1</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saman Hosseini, Lee W. Cohnstaedt, Caterina Scoglio</dc:creator>
    </item>
    <item>
      <title>Spatial constraints improve filtering of measurement noise from animal tracks</title>
      <link>https://arxiv.org/abs/2511.22430</link>
      <description>arXiv:2511.22430v1 Announce Type: new 
Abstract: Advances in tracking technologies for animal movement require new statistical tools to better exploit the increasing amount of data. Animal positions are usually calculated using the GPS or Argos satellite system and include potentially complex non-Gaussian and heavy-tailed measurement error patterns. Errors are usually handled through a Kalman filter algorithm, which can be sensitive to non-Gaussian error distributions.
  In this paper, we introduce a realistic latent movement model through an underdamped Langevin stochastic differential equation (SDE) that includes an additional drift term to ensure that the animal remains in a known spatial domain of interest. This can be applied to aquatic animals moving in water or terrestrial animals moving in a restricted zone delimited by fences or natural barriers. We demonstrate that the incorporation of these spatial constraints into the latent movement model improves the accuracy of filtering for noisy observations of the positions. We implement an Extended Kalman Filter as well as a particle filter adapted to non-Gaussian error distributions. Our filters are based on solving the SDE through splitting schemes to approximate the latent dynamic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22430v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Delporte, Susanne Ditlevsen, Adeline Samson</dc:creator>
    </item>
    <item>
      <title>Mapping urban air quality using mobile and fixed low cost sensors: a model comparison</title>
      <link>https://arxiv.org/abs/2511.22550</link>
      <description>arXiv:2511.22550v1 Announce Type: new 
Abstract: This study addresses the critical challenge of modeling and mapping urban air quality to ascertain pollutant concentrations in unmonitored locations. The advent of low-cost sensors, particularly those deployed in vehicular networks, presents novel datasets that hold the potential to enhance air quality modeling. This research conducts a comprehensive review of ten statistical models drawn from existing literature, using both fixed and mobile low-cost sensor data, alongside ancillary variables, within the urban confines of Nantes, France.
  Employing a methodology that includes cross-validation of data from low-cost sensors and validation on fixed air quality monitoring stations, this paper evaluates the models' performance in scenarios of temporal interpolation and prediction. Our findings reveal a pronounced bias in the model outputs when reliant on low-cost sensor data compared to the verification data obtained from fixed stations. Furthermore, machine learning models demonstrated superior performance in predictive scenarios, suggesting their enhanced suitability for forecasting tasks.
  The study conclusively indicates that reliance solely on data from low-cost mobile sensors compromises the reliability of air quality models, due to significant accuracy deficiencies. Consequently, we advocate for a directed focus towards the integration and calibration of low-cost sensor data with information from fixed monitoring stations. This approach, rather than an exclusive emphasis on the complexity of statistical modeling techniques, is pivotal for achieving the precision required for effective air quality management and policy-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22550v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yacine Mohamed Idir, Olivier Orfila, Patrice Chatellier, Vincent Judalet, Valentin Guaffre</dc:creator>
    </item>
    <item>
      <title>The Causal Uncertainty Principle</title>
      <link>https://arxiv.org/abs/2511.22649</link>
      <description>arXiv:2511.22649v1 Announce Type: new 
Abstract: This paper explains why internal and external validity cannot be simultaneously maximised. It introduces "evidential states" to represent the information available for causal inference and shows that routine study operations (restriction, conditioning, and intervention) transform these states in ways that do not commute. Because each operation removes or reorganises information differently, changing their order yields evidential states that support different causal claims. This non-commutativity creates a structural trade-off: the steps that secure precise causal identification also eliminate the heterogeneity required for generalisation. Small model, observational and experimental examples illustrate how familiar failures of transportability arise from this order dependence. The result is a concise structural account of why increasing causal precision necessarily narrows the world to which findings apply.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22649v1</guid>
      <category>stat.AP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Daniel D. Reidpath</dc:creator>
    </item>
    <item>
      <title>A novel method to analyze pattern shifts in rainfall using cluster analysis and probability models</title>
      <link>https://arxiv.org/abs/2511.23067</link>
      <description>arXiv:2511.23067v1 Announce Type: new 
Abstract: : One of the prominent challenges being faced by agricultural sciences is the onset of climate change which is adversely affecting every aspect of cropping. Modelling of climate change at macro level have been carried out at large scale and there is ample amount of research publications available for that. But at micro level like at state level or district level there are lesser studies. District level studies can help in preparing specific plans for the mitigation of adverse effects of climate change at local level. An attempt has been made in this paper to model the monthly rainfall of Varanasi district of the state of Uttar Pradesh with the help of probability models. Firstly, the pattern of the climate change over 122 years has been unveiled by using exploratory analysis and using multivariate techniques like cluster analysis and then probability models have been fitted for selected months</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23067v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhishek Singh, Aaditya Jadhav, Abha Goyal, Jesma V, Vyshna I C</dc:creator>
    </item>
    <item>
      <title>Multi-fidelity Bayesian Optimization Framework for CFD-Based Non-Premixed Burner Design</title>
      <link>https://arxiv.org/abs/2511.23140</link>
      <description>arXiv:2511.23140v1 Announce Type: new 
Abstract: We propose a multi-fidelity Bayesian optimization (MF-BO) framework that integrates computational fluid dynamics (CFD) evaluations with Gaussian-process surrogates to efficiently navigate the accuracy-cost trade-off induced by mesh resolution. The design vector x = [h, l, s] (height, length, and mesh element size) defines a continuous fidelity index Z(h, l, s), enabling the optimizer to adaptively combine low- and high-resolution simulations. This framework is applied to a non-premixed burner configuration targeting improved thermal efficiency under hydrogen-enriched fuels. A calibrated runtime model t_hat(h, l, s) penalizes computationally expensive queries, while a constrained noisy expected improvement (qNEI) guides sampling under an emissions cap of 2e-6 for NOx.
  Surrogates trained on CFD data exhibit stable hyperparameters and physically consistent sensitivities: mean temperature increases with reactor length and fidelity and is weakly negative with height; NOx grows with temperature yet tends to decrease with length. The best design achieves T_bar approx 2.0e3 K while satisfying the NOx limit.
  Relative to a hypothetical single-fidelity campaign (Z = 1), the MF-BO achieves comparable convergence with about 57 percent lower total wall time by learning the design landscape through fast low-Z evaluations and reserving high-Z CFD for promising candidates. Overall, the methodology offers a generalizable and computationally affordable path for optimizing reacting-flow systems in which mesh-driven fidelity inherently couples accuracy, cost, and emissions. This highlights its potential to accelerate design cycles and reduce resource requirements in industrial burner development and other high-cost CFD-driven applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23140v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Patrick Souza Lima, Paulo Roberto Santana dos Reis, Alex \'Alisson Bandeira Santos, Ehecatl Antonio del R\'io Chanona, Idelfonso Bessa dos Reis Nogueira</dc:creator>
    </item>
    <item>
      <title>Design loads for wave impacts -- introducing the Probabilistic Adaptive Screening (PAS) method for predicting extreme non-linear loads on maritime structures</title>
      <link>https://arxiv.org/abs/2511.23156</link>
      <description>arXiv:2511.23156v2 Announce Type: new 
Abstract: Wave impact loads on a maritime structure can cause casualties, damage, pollution of the sea and operational delays. Their extreme (or design) values should therefore be considered in the design of these structures. However, this is challenging because these events are both rare and complex, requiring high-fidelity computation and long analysis durations to obtain such design loads. Existing extreme value prediction methods are not tailored or validated for wave impacts. We therefore introduce the new Probabilistic Adaptive Screening (PAS) method for predicting extreme non-linear loads on maritime structures. The method introduces a probabilistic approach to multi-fidelity screening, allowing efficient linear potential flow indicators to be used in the low-fidelity stage, even for strongly non-linear load cases. The method is validated against a range of cases, including non-linear waves, ship vertical bending moments, green water impact loads, and slamming loads. It can be concluded that PAS accurately estimates both the short-term distributions and extreme values in these test cases, with most probable maximum (MPM) values within 10% of the available full brute-force Monte-Carlo Simulation (MCS) results. In addition, PAS achieves this performance very efficiently, requiring less than 4% of the high-fidelity simulation time needed for conventional MCS. These results demonstrate that PAS can reliably reproduce the statistics of both weakly and strongly non-linear extreme load problems, while significantly reducing the associated computational cost. The present study validates the statistical PAS framework; further work should focus on validating the full procedure including CFD load simulations, and on validating it for long-term extremes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23156v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanne M. van Essen, Harleigh C. Seyffert</dc:creator>
    </item>
    <item>
      <title>From 'Individual Scientist' to 'Integrated Scientist': The Evolution of Scientific Organizational panels and Their Impact on the Scientific System</title>
      <link>https://arxiv.org/abs/2511.21771</link>
      <description>arXiv:2511.21771v1 Announce Type: cross 
Abstract: This article aims to propose and elucidate the analytical concepts of "individual scientist" and "integrated scientist" to depict the fundamental transformation in the modes of scientific research actors throughout the history of science. The "individual scientist" represents an early modern scientific research panel characterized by independence, egalitarian collaboration, and personal recognition, while the "integrated scientist" emerged in the context of "big science," marked by hierarchical teams, division of labor, collaboration, and the concentration of recognition on team leaders. Through historical review and case analysis, this article explores the underlying drivers of this transformation and focuses on its challenges and reconstructions concerning the name-based scientific reward system, aiming to provide a reflective perspective for contemporary scientific governance and research evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21771v1</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zekai Zhang</dc:creator>
    </item>
    <item>
      <title>Design-based nested instrumental variable analysis</title>
      <link>https://arxiv.org/abs/2511.21992</link>
      <description>arXiv:2511.21992v1 Announce Type: cross 
Abstract: Two binary instrumental variables (IVs) are nested if individuals who comply under one binary IV also comply under the other. This situation often arises when the two IVs represent different intensities of encouragement or discouragement to take the treatment--one stronger than the other. In a nested IV structure, treatment effects can be identified for two latent subgroups: always-compliers and switchers. Always-compliers are individuals who comply even under the weaker IV, while switchers are those who do not comply under the weaker IV but do under the stronger IV. We introduce a novel pair-of-pairs nested IV design, where each matched stratum consists of four units organized in two pairs. Under this design, we develop design-based inference for estimating the always-complier sample average treatment effect (ACO-SATE) and switcher sample average treatment effect (SW-SATE). In a nested IV analysis, IV assignment is randomized within each IV pair; however, whether a study unit receives the weaker or stronger IV may not be randomized. To address this complication, we then propose a novel partly biased randomization scheme and study design-based inference under this new scheme. Using extensive simulation studies, we demonstrate the validity of the proposed method and assess its power under different scenarios. Applying the nested IV framework, we estimated that 52.2% (95% CI: 50.4%-53.9%) of participants enrolled at the Henry Ford Health System in the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial were always-compliers, while 26.7% (95% CI: 24.5%-28.9%) were switchers. Among always-compliers, flexible sigmoidoscopy was associated with a trend toward a decreased colorectal cancer rate. No effect was detected among switchers. This offers a richer interpretation of why no increase in the intention-to-treat effect was observed after 1997, even though the compliance rate rose.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21992v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhe Chen, Xinran Li, Michael O. Harhay, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>Density-based Neural Temporal Point Processes for Heartbeat Dynamics</title>
      <link>https://arxiv.org/abs/2511.22096</link>
      <description>arXiv:2511.22096v1 Announce Type: cross 
Abstract: Temporal point processes (TPPs) provide a natural mathematical framework for modeling heartbeats due to capturing underlying physiological inductive biases. In this work, we apply density-based neural TPPs to model heartbeat dynamics from 18 subjects. We adapt a goodness-of-fit framework from classical point process literature to Neural TPPs and use it to optimize hyperparameters, identify appropriate training sequence lengths to capture temporal dependencies, and demonstrate zero-shot predictive capability on heartbeat data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22096v1</guid>
      <category>q-bio.TO</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sandya Subramanian, Bharath Ramsundar</dc:creator>
    </item>
    <item>
      <title>Probabilistic Digital Twin for Misspecified Structural Dynamical Systems via Latent Force Modeling and Bayesian Neural Networks</title>
      <link>https://arxiv.org/abs/2511.22133</link>
      <description>arXiv:2511.22133v1 Announce Type: cross 
Abstract: This work presents a probabilistic digital twin framework for response prediction in dynamical systems governed by misspecified physics. The approach integrates Gaussian Process Latent Force Models (GPLFM) and Bayesian Neural Networks (BNNs) to enable end-to-end uncertainty-aware inference and prediction. In the diagnosis phase, model-form errors (MFEs) are treated as latent input forces to a nominal linear dynamical system and jointly estimated with system states using GPLFM from sensor measurements. A BNN is then trained on posterior samples to learn a probabilistic nonlinear mapping from system states to MFEs, while capturing diagnostic uncertainty. For prognosis, this mapping is used to generate pseudo-measurements, enabling state prediction via Kalman filtering. The framework allows for systematic propagation of uncertainty from diagnosis to prediction, a key capability for trustworthy digital twins. The framework is demonstrated using four nonlinear examples: a single degree of freedom (DOF) oscillator, a multi-DOF system, and two established benchmarks -- the Bouc-Wen hysteretic system and the Silverbox experimental dataset -- highlighting its predictive accuracy and robustness to model misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22133v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sahil Kashyap, Rajdip Nayek</dc:creator>
    </item>
    <item>
      <title>The Hidden AI Race: Tracking Environmental Costs of Innovation</title>
      <link>https://arxiv.org/abs/2511.22781</link>
      <description>arXiv:2511.22781v1 Announce Type: cross 
Abstract: The past decade has seen a massive rise in the popularity of AI systems, mainly owing to the developments in Gen AI, which has revolutionized numerous industries and applications. However, this progress comes at a considerable cost to the environment as training and deploying these models consume significant computational resources and energy and are responsible for large carbon footprints in the atmosphere. In this paper, we study the amount of carbon dioxide released by models across different domains over varying time periods. By examining parameters such as model size, repository activity (e.g., commits and repository age), task type, and organizational affiliation, we identify key factors influencing the environmental impact of AI development. Our findings reveal that model size and versioning frequency are strongly correlated with higher emissions, while domain-specific trends show that NLP models tend to have lower carbon footprints compared to audio-based systems. Organizational context also plays a significant role, with university-driven projects exhibiting the highest emissions, followed by non-profits and companies, while community-driven projects show a reduction in emissions. These results highlight the critical need for green AI practices, including the adoption of energy-efficient architectures, optimizing development workflows, and leveraging renewable energy sources. We also discuss a few practices that can lead to a more sustainable future with AI, and we end this paper with some future research directions that could be motivated by our work. This work not only provides actionable insights to mitigate the environmental impact of AI but also poses new research questions for the community to explore. By emphasizing the interplay between sustainability and innovation, our study aims to guide future efforts toward building a more ecologically responsible AI ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22781v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shyam Agarwal, Mahasweta Chakraborti</dc:creator>
    </item>
    <item>
      <title>What If They Took the Shot? A Hierarchical Bayesian Framework for Counterfactual Expected Goals</title>
      <link>https://arxiv.org/abs/2511.23072</link>
      <description>arXiv:2511.23072v1 Announce Type: cross 
Abstract: This study develops a hierarchical Bayesian framework that integrates expert domain knowledge to quantify player-specific effects in expected goals (xG) estimation, addressing a limitation of standard models that treat all players as identical finishers. Using 9,970 shots from StatsBomb's 2015-16 data and Football Manager 2017 ratings, we combine Bayesian logistic regression with informed priors to stabilise player-level estimates, especially for players with few shots. The hierarchical model reduces posterior uncertainty relative to weak priors and achieves strong external validity: hierarchical and baseline predictions correlate at R2 = 0.75, while an XGBoost benchmark validated against StatsBomb xG reaches R2 = 0.833. The model uncovers interpretable specialisation profiles, including one-on-one finishing (Aguero, Suarez, Belotti, Immobile, Martial), long-range shooting (Pogba), and first-touch execution (Insigne, Salah, Gameiro). It also identifies latent ability in underperforming players such as Immobile and Belotti. The framework supports counterfactual "what-if" analysis by reallocating shots between players under identical contexts. Case studies show that Sansone would generate +2.2 xG from Berardi's chances, driven largely by high-pressure situations, while Vardy-Giroud substitutions reveal strong asymmetry: replacing Vardy with Giroud results in a large decline (about -7 xG), whereas the reverse substitution has only a small effect (about -1 xG). This work provides an uncertainty-aware tool for player evaluation, recruitment, and tactical planning, and offers a general approach for domains where individual skill and contextual factors jointly shape performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23072v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikayil Mahmudlu, Oktay Karaku\c{s}, Hasan Arkada\c{s}</dc:creator>
    </item>
    <item>
      <title>The Bayesian optimal two-stage design for clinical phase II trials based on Bayes factors</title>
      <link>https://arxiv.org/abs/2511.23144</link>
      <description>arXiv:2511.23144v1 Announce Type: cross 
Abstract: Sequential trial design is an important statistical approach to increase the efficiency of clinical trials. Bayesian sequential trial design relies primarily on conducting a Monte Carlo simulation under the hypotheses of interest and investigating the resulting design characteristics via Monte Carlo estimates. This approach has several drawbacks, namely that replicating the calibration of a Bayesian design requires repeating a possibly complex Monte Carlo simulation. Furthermore, Monte Carlo standard errors are required to judge the reliability of the simulation. All of this is due to a lack of closed-form or numerical approaches to calibrate a Bayesian design which uses Bayes factors. In this paper, we propose the Bayesian optimal two-stage design for clinical phase II trials based on Bayes factors. The optimal two-stage Bayes factor design is a sequential clinical trial design that is built on the idea of trinomial tree branching, a method we propose to correct the resulting design characteristics for introducing a single interim analysis. We build upon this idea to invent a calibration algorithm which yields the optimal Bayesian design that minimizes the expected sample size under the null hypothesis. Examples show that our design recovers Simon's two-stage optimal design as a special case, improves upon non-sequential Bayesian design based on Bayes factors, and can be calibrated quickly, as it makes use only of standard numerical techniques instead of time-consuming Monte Carlo simulations. Furthermore, the design allows to ensure a minimum probability on compelling evidence in favour of the null hypothesis, which is not possible with other designs. As the idea of trinomial tree branching is neither dependent on the endpoint, nor on the use of Bayes factors, the design can therefore be generalized to other settings, too.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23144v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Riko Kelter, Samuel Pawel</dc:creator>
    </item>
    <item>
      <title>A Bayesian Network Method for Deaggregation: Identification of Tropical Cyclones Driving Coastal Hazards</title>
      <link>https://arxiv.org/abs/2505.14374</link>
      <description>arXiv:2505.14374v2 Announce Type: replace 
Abstract: Bayesian networks (BN) have advantages in visualizing causal relationships and performing probabilistic inference analysis, making them ideal tools for coastal hazard analysis and characterizing the compound mechanisms of coastal hazards. Meanwhile, the Joint Probability Method (JPM) has served as the primary probabilistic assessment approach used to develop hazard curves for tropical cyclone (TC) induced coastal hazards in the past decades. To develop hazard curves that can capture the breadth of TC-induced coastal hazards, a large number of synthetic TCs need to be simulated, which is computationally expensive. Given that low exceedance probability (LEP) coastal hazards are likely to result in the most significant damage to coastal communities, it is practical to focus efforts on identifying and understanding TC scenarios that are dominant contributors to LEP coastal hazards. This study developed a BN-based framework incorporating existing JPM for multiple TC-induced coastal hazards deaggregation. Copula-based models capture dependence among TC atmospheric parameters and generate CPTs for corresponding BN nodes. Machine learning surrogates model the relationship between TC parameters and coastal hazards, providing conditional probability tables (CPTs) for hazard nodes. Case studies are applied to the Greater New Orleans region in Louisiana (USA). Deaggregation is a method for identifying dominant scenarios for a given hazard, which was first established in the field of probabilistic seismic hazard analysis. The objective of this study is to leverage BN to develop a deaggregation method of multiple LEP coastal hazards to better understand the dominant drivers of coastal hazards to refine storm parameter set selection to more comprehensively represent multiple forcings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14374v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyue Liu, Meredith L. Carr, Norberto C. Nadal-Caraballo, Madison C. Yawn, Michelle T. Bensi</dc:creator>
    </item>
    <item>
      <title>Optimized Supergeo Design: A Scalable Framework for Geographic Marketing Experiments</title>
      <link>https://arxiv.org/abs/2506.20499</link>
      <description>arXiv:2506.20499v4 Announce Type: replace 
Abstract: Geographic experiments are a widely-used methodology for measuring incremental return on ad spend (iROAS) at scale, yet their design presents significant challenges. The unit count is small, heterogeneity is large, and the optimal Supergeo partitioning problem is NP-hard. We introduce Optimized Supergeo Design (OSD), a two-stage framework that renders Supergeo designs practical for large-scale markets. Principal Component Analysis (PCA) first reduces the covariate space to create interpretable geo-embeddings. A Mixed-Integer Linear Programming (MILP) solver then selects a partition that balances both baseline outcomes and pre-treatment covariates. We provide theoretical arguments that OSD's objective value is within $(1+\varepsilon)$ of the global optimum under community-structure assumptions. Rigorous ablation analysis on synthetic data shows that PCA- and random-embedding Supergeo designs match unit-level randomisation in estimation error while delivering tighter covariate balance, whereas spectral embeddings substantially worsen both RMSE and balance. Crucially, OSD solves the scalability bottleneck. For $N=210$ markets, OSD completes in a fraction of a second, while exact Supergeo covering MIPs described in prior work are projected to require orders of magnitude longer, on the order of weeks. Scalability experiments up to $N=1\,000$ units show that OSD remains fast without trimming markets. In our main synthetic setting with $N=200$ units, PCA- and random-embedding designs keep covariate imbalance at only a few percentage points while preserving every media dollar, establishing a scalable framework that matches the statistical efficiency of randomisation with the operational practicality of Supergeos.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20499v4</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles Shaw</dc:creator>
    </item>
    <item>
      <title>Network Dynamics and Spatial Shifts in Civilian Targeting: A Stochastic Block Model Analysis of the Colombian Armed Conflict</title>
      <link>https://arxiv.org/abs/2508.09051</link>
      <description>arXiv:2508.09051v2 Announce Type: replace 
Abstract: In this article, we explore how the escalating victimization of civilians during civil wars is mirrored in the fragmented distribution of territorial control, focusing on the Colombian armed conflict. Through an exhaustive characterization of the topology of bipartite and projected networks of municipalities, we describe changes in territorial configurations across different periods between 1978 and 2007. By employing stochastic block models for count data, we show that, during periods dominated by a small set of actors, the networks adopt a centralized node periphery structure, whereas during times of widespread conflict, areas of influence overlap in complex ways. Our findings also suggest the existence of cohesive municipal communities shaped by both geographic proximity and affinities between armed structures, as well as internally dispersed groups with a high likelihood of interaction. As the spatial distribution shifts toward a more fragmented arrangement, the average interaction intensity between communities predicted by the stochastic block model approaches that within communities, indicating a weakening of modular structure and increased inter community connectivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09051v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Natalia Perdomo-Londo\~no, Juan Sosa, Emma J. Camargo-D\'iaz</dc:creator>
    </item>
    <item>
      <title>Transportability of Prognostic Markers: Rethinking Common Practices through a Sufficient-Component-Cause Perspective</title>
      <link>https://arxiv.org/abs/2511.04065</link>
      <description>arXiv:2511.04065v3 Announce Type: replace 
Abstract: Transportability, the ability to maintain performance across populations, is a desirable property of markers of clinical outcomes. However, empirical findings indicate that markers often exhibit varying performances across populations. For prognostic markers that are advertised as predictive risk equations, oftentimes a form of updating is required when the equation is transported to populations with different disease prevalences. Here, we revisit transportability of prognostic markers through the lens of the foundational framework of sufficient component causes (SCC). We argue that transporting a marker 'as is' implicitly assumes predictive values are transportable, whereas conventional prevalence adjustment shifts the locus of transportability to accuracy metrics (sensitivity and specificity). Using a minimalist SCC framework that decomposes risk prediction into its causal constituents, we show that both approaches rely on strong assumptions about the stability of cause distributions. A SCC framework instead invites making transparent assumptions about how different causes vary across populations, leading to different transportation methods. For example, in the absence of any external information other than disease prevalence, a cause-neutral perspective can assume all causes are responsible for change in prevalence, leading to a new form of marker transportation. Numerical experiments demonstrate that different transportability assumptions lead to varying degrees of information loss, depending on the distribution of causes. A SCC perspective challenges common assumptions and practices for marker transportability, and proposes transportation algorithms that reflect our knowledge or assumptions about how causes vary across populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04065v3</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Sadatsafavi, Gavin Pereira, Wenjia Chen</dc:creator>
    </item>
    <item>
      <title>Classical JAK2V617F+ Myeloproliferative Neoplasms emergence and development based on real life incidence and mathematical modeling</title>
      <link>https://arxiv.org/abs/2406.06765</link>
      <description>arXiv:2406.06765v2 Announce Type: replace-cross 
Abstract: Mathematical modeling allows us to better understand myeloproliferative neoplasms (MPN) emergence and development. We tested different mathematical models on an initial cohort to determine the emergence and evolution times before diagnosis of JAK2V617F classical MPN (Polycythemia Vera (PV) and Essential Thrombocythemia (ET)). We considered the time before diagnosis as the sum of two independent periods: the time (from embryonic development) for the JAK2V617F mutation to occur, not disappear and enter proliferation, and a second time corresponding to the expansion of the clonal population until diagnosis. We demonstrate using progressively complexified models that the rate of active mutation occurrence cannot be constant, but rather increases exponentially with age following the Gompertz model. We found that the first tumorous cell takes an average time of $63.1 \pm 13$ years to appear and start proliferation. On the other hand, the expansion time is constant: $8.8$ years once the mutation has emerged. These results were validated in an external cohort. Using this model, we analyzed JAK2V167F ET versus PV, and obtained that the time of active mutation occurrence for PV takes approximately $1.5$ years more than for ET to develop, while the expansion time was similar. In conclusion, our multi-step approach and the ultimate age-dependent model for the emergence and development of MPN demonstrates that the emergence of a JAKV617F mutation should be linked to an aging mechanism, and indicates a $8-9$ years period of time to develop a full MPN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06765v2</guid>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ana Fern\'andez Baranda, Vincent Bansaye, Evelyne Lauret, Morgane Mounier, Val\'erie Ugo, Sylvie M\'el\'eard, St\'ephane Giraudier</dc:creator>
    </item>
    <item>
      <title>Scenario Analysis with Multivariate Bayesian Machine Learning Models</title>
      <link>https://arxiv.org/abs/2502.08440</link>
      <description>arXiv:2502.08440v4 Announce Type: replace-cross 
Abstract: We present an econometric framework that adapts tools for scenario analysis, such as variants of conditional forecasts and generalized impulse responses, for use with dynamic nonparametric models. The proposed algorithms are based on predictive simulation and sequential Monte Carlo methods. Their utility is demonstrated with three applications: (1) conditional forecasts based on stress test scenarios, measuring (2) macroeconomic risk under varying financial stress, and estimating the (3) asymmetric effects of financial shocks in the US and their international spillovers. Our empirical results indicate the importance of nonlinearities and asymmetries in relationships between macroeconomic and financial variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08440v4</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Pfarrhofer, Anna Stelzer</dc:creator>
    </item>
    <item>
      <title>Joint Modelling of Line and Point Data on Metric Graphs</title>
      <link>https://arxiv.org/abs/2505.01175</link>
      <description>arXiv:2505.01175v2 Announce Type: replace-cross 
Abstract: Metric graphs are useful tools for describing spatial domains like road and river networks, where spatial dependence act along the network. We take advantage of recent developments for such Gaussian Random Fields (GRFs), and consider joint spatial modelling of observations with different spatial supports. Motivated by an application to traffic state modelling in Trondheim, Norway, we consider line-referenced data, which can be described by an integral of the GRF along a line segment on the metric graph, and point-referenced data. Through a simulation study inspired by the application, we investigate the number of replicates that are needed to estimate parameters and to predict unobserved locations. The former is assessed using bias and variability, and the latter is assessed through root mean square error (RMSE), continuous rank probability scores (CRPSs), and coverage. Joint modelling is contrasted with a simplified approach that treat line-referenced observations as point-referenced observations. The results suggest joint modelling leads to strong improvements. The application to Trondheim, Norway, combines point-referenced induction loop data and line-referenced public transportation data. To ensure positive speeds, we use a non-linear link function, which requires integrals of non-linear combinations of the linear predictor. This is made computationally feasible by a combination of the R packages inlabru and MetricGraph, and new code for processing geographical line data to work with existing graph representations and fmesher methods for dealing with line support in inlabru on objects from MetricGraph. We fit the model to two datasets where we expect different spatial dependency and compare the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01175v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.spasta.2025.100946</arxiv:DOI>
      <dc:creator>Karina Lilleborge, Sara Martino, Geir-Arne Fuglstad, Finn Lindgren, Rikke Ingebrigtsen</dc:creator>
    </item>
    <item>
      <title>The Malaysian Election Corpus (MECo): Federal and State-Level Election Results from 1955 to 2025</title>
      <link>https://arxiv.org/abs/2505.06564</link>
      <description>arXiv:2505.06564v2 Announce Type: replace-cross 
Abstract: Empirical research and public knowledge on Malaysia's elections have long been constrained by a lack of high-quality open data, particularly in the absence of a Freedom of Information framework. This paper introduces the Malaysian Election Corpus (MECo), an open-access panel database covering all federal and state general elections since 1955, as well as by-elections since 2008. MECo includes candidate- and constituency-level data for 9,704 electoral contests across seven decades, standardised with unique identifiers for candidates, parties, and coalitions. The database also provides summary statistics for each contest (electorate size, voter turnout, majority size, rejected ballots, unreturned ballots), key demographic data for candidates (age, gender, ethnicity), and lineage data for political parties. MECo is the most well-curated open database on Malaysian elections to date, and will unlock new opportunities for research, data journalism, and civic engagement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06564v2</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Thevesh Thevananthan</dc:creator>
    </item>
    <item>
      <title>A thermoinformational formulation for the description of neuropsychological systems</title>
      <link>https://arxiv.org/abs/2511.09506</link>
      <description>arXiv:2511.09506v2 Announce Type: replace-cross 
Abstract: Complex systems produce high-dimensional signals that lack macroscopic variables analogous to entropy, temperature, or free energy. This work introduces a thermoinformational formulation that derives entropy, internal energy, temperature, and Helmholtz free energy directly from empirical microstate distributions of arbitrary datasets. The approach provides a data-driven description of how a system reorganizes, exchanges information, and moves between stable and unstable states. Applied to dual-EEG recordings from mother-infant dyads performing the A-not-B task, the formulation captures increases in informational heat during switches and errors, and reveals that correct choices arise from more stable, low-temperature states. In an independent optogenetic dam-pup experiment, the same variables separate stimulation conditions and trace coherent trajectories in thermodynamic state space. Across both human and rodent systems, this thermoinformational formulation yields compact and physically interpretable macroscopic variables that generalize across species, modalities, and experimental paradigms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09506v2</guid>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>George-Rafael Domenikos, Victoria Leong</dc:creator>
    </item>
  </channel>
</rss>
