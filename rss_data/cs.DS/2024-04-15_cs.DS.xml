<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Apr 2024 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 15 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Matrix Multiplication Reductions</title>
      <link>https://arxiv.org/abs/2404.08085</link>
      <description>arXiv:2404.08085v1 Announce Type: new 
Abstract: In this paper we study a worst case to average case reduction for the problem of matrix multiplication over finite fields. Suppose we have an efficient average case algorithm, that given two random matrices $A,B$ outputs a matrix that has a non-trivial correlation with their product $A \cdot B$. Can we transform it into a worst case algorithm, that outputs the correct answer for all inputs without incurring a significant overhead in the running time? We present two results in this direction.
  (1) Two-sided error in the high agreement regime: We begin with a brief remark about a reduction for high agreement algorithms, i.e., an algorithm which agrees with the correct output on a large (say $&gt;0.9$) fraction of entries, and show that the standard self-correction of linearity allows us to transform such algorithms into algorithms that work in worst case.
  (2) One-sided error in the low agreement regime: Focusing on average case algorithms with one-sided error, we show that over $\mathbb{F}_2$ there is a reduction that gets an $O(T)$ time average case algorithm that given a random input $A,B$ outputs a matrix that agrees with $A \cdot B$ on at least $51\%$ of the entries (i.e., has only a slight advantage over the trivial algorithm), and transforms it into an $\widetilde{O}(T)$ time worst case algorithm, that outputs the correct answer for all inputs with high probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08085v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashish Gola (Simon Fraser University), Igor Shinkar (Simon Fraser University), Harsimran Singh (Simon Fraser University)</dc:creator>
    </item>
    <item>
      <title>Naively Sorting Evolving Data is Optimal and Robust</title>
      <link>https://arxiv.org/abs/2404.08162</link>
      <description>arXiv:2404.08162v1 Announce Type: new 
Abstract: We study comparison sorting in the evolving data model [AKMU11], where the true total order changes while the sorting algorithm is processing the input. More precisely, each comparison operation of the algorithm is followed by a sequence of evolution steps, where an evolution step perturbs the rank of a random item by a "small" random value. The goal is to maintain an ordering that remains close to the true order over time. Previous works have analyzed adaptations of classic sorting algorithms, assuming that an evolution step changes the rank of an item by just one, and that a fixed constant number $b$ of evolution steps take place between two comparisons. In fact, the only previous result achieving optimal $O(n)$ total deviation from the true order, where $n$ is the number of items, applies just for $b=1$ [BDEGJ18].
  We analyze a very simple sorting algorithm suggested in [M14], which samples a random pair of adjacent items in each step and swaps them if they are out of order. We show that the algorithm achieves and maintains, w.h.p., optimal total deviation, $O(n)$, and optimal maximum deviation, $O(\log n)$, under very general model settings. Namely, the perturbation introduced by each evolution step follows a distribution of bounded moment generating function, and over a linear number of steps, on average the number of evolution steps between two sorting steps is bounded by an arbitrary constant.
  Our proof consists of a novel potential function argument that inserts "gaps" in the list of items, and a general framework which separates the analysis of sorting from that of the evolution steps, and is applicable to a variety of settings for which previous approaches do not apply. Our results settle conjectures by [AKMU11] and [M14], and provide theoretical support for the empirical evidence that simple quadratic algorithms are optimal and robust for sorting evolving data [BDEGJ18].</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08162v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcos Kiwi, George Giakkoupis, Dimitrios Los</dc:creator>
    </item>
    <item>
      <title>Anarchy in the APSP: Algorithm and Hardness for Incorrect Implementation of Floyd-Warshall</title>
      <link>https://arxiv.org/abs/2404.08173</link>
      <description>arXiv:2404.08173v1 Announce Type: new 
Abstract: The celebrated Floyd-Warshall algorithm efficiently computes the all-pairs shortest path, and its simplicity made it a staple in computer science classes. Frequently, students discover a variant of this Floyd-Warshall algorithm by mixing up the loop order, ending up with the incorrect APSP matrix. This paper considers a computational problem of computing this incorrect APSP matrix. We will propose efficient algorithms for this problem and prove that this incorrect variant is APSP-complete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08173v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.FUN.2024.11</arxiv:DOI>
      <dc:creator>Jaehyun Koo</dc:creator>
    </item>
    <item>
      <title>Bottom-up Rebalancing Binary Search Trees by Flipping a Coin</title>
      <link>https://arxiv.org/abs/2404.08287</link>
      <description>arXiv:2404.08287v1 Announce Type: new 
Abstract: Rebalancing schemes for dynamic binary search trees are numerous in the literature, where the goal is to maintain trees of low height, either in the worst-case or expected sense. In this paper we study randomized rebalancing schemes for sequences of $n$ insertions into an initially empty binary search tree, under the assumption that a tree only stores the elements and the tree structure without any additional balance information. Seidel~(2009) presented a top-down randomized insertion algorithm, where insertions take expected $O\big(\lg^2 n\big)$ time, and the resulting trees have the same distribution as inserting a uniform random permutation into a binary search tree without rebalancing. Seidel states as an open problem if a similar result can be achieved with bottom-up insertions. In this paper we fail to answer this question.
  We consider two simple canonical randomized bottom-up insertion algorithms on binary search trees, assuming that an insertion is given the position where to insert the next element. The subsequent rebalancing is performed bottom-up in expected $O(1)$ time, uses expected $O(1)$ random bits, performs at most two rotations, and the rotations appear with geometrically decreasing probability in the distance from the leaf. For some insertion sequences the expected depth of each node is proved to be $O(\lg n)$. On the negative side, we prove for both algorithms that there exist simple insertion sequences where the expected depth is $\Omega(n)$, i.e., the studied rebalancing schemes are \emph{not} competitive with (most) other rebalancing schemes in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08287v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gerth St{\o}lting Brodal</dc:creator>
    </item>
    <item>
      <title>Tight Bounds for Sorting Under Partial Information</title>
      <link>https://arxiv.org/abs/2404.08468</link>
      <description>arXiv:2404.08468v1 Announce Type: new 
Abstract: Sorting has a natural generalization where the input consists of: (1) a ground set $X$ of size $n$, (2) a partial oracle $O_P$ specifying some fixed partial order $P$ on $X$ and (3) a linear oracle $O_L$ specifying a linear order $L$ that extends $P$. The goal is to recover the linear order $L$ on $X$ using the fewest number of linear oracle queries.
  In this problem, we measure algorithmic complexity through three metrics: oracle queries to $O_L$, oracle queries to $O_P$, and the time spent. Any algorithm requires worst-case $\log_2 e(P)$ linear oracle queries to recover the linear order on $X$.
  Kahn and Saks presented the first algorithm that uses $\Theta(\log e(P))$ linear oracle queries (using $O(n^2)$ partial oracle queries and exponential time). The state-of-the-art for the general problem is by Cardinal, Fiorini, Joret, Jungers and Munro who at STOC'10 manage to separate the linear and partial oracle queries into a preprocessing and query phase. They can preprocess $P$ using $O(n^2)$ partial oracle queries and $O(n^{2.5})$ time. Then, given $O_L$, they uncover the linear order on $X$ in $\Theta(\log e(P))$ linear oracle queries and $O(n + \log e(P))$ time -- which is worst-case optimal in the number of linear oracle queries but not in the time spent.
  For $c \geq 1$, our algorithm can preprocess $O_P$ using $O(n^{1 + \frac{1}{c}})$ queries and time. Given $O_L$, we uncover $L$ using $\Theta(c \log e(P))$ queries and time. We show a matching lower bound, as there exist positive constants $(\alpha, \beta)$ where for any constant $c \geq 1$, any algorithm that uses at most $\alpha \cdot n^{1 + \frac{1}{c}}$ preprocessing must use worst-case at least $\beta \cdot c \log e(P)$ linear oracle queries. Thus, we solve the problem of sorting under partial information through an algorithm that is asymptotically tight across all three metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08468v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ivor van der Hoog, Daniel Rutschmann</dc:creator>
    </item>
    <item>
      <title>Destroying Densest Subgraphs is Hard</title>
      <link>https://arxiv.org/abs/2404.08599</link>
      <description>arXiv:2404.08599v1 Announce Type: new 
Abstract: We analyze the computational complexity of the following computational problems called Bounded-Density Edge Deletion and Bounded-Density Vertex Deletion: Given a graph $G$, a budget $k$ and a target density $\tau_\rho$, are there $k$ edges ($k$ vertices) whose removal from $G$ results in a graph where the densest subgraph has density at most $\tau_\rho$? Here, the density of a graph is the number of its edges divided by the number of its vertices. We prove that both problems are polynomial-time solvable on trees and cliques but are NP-complete on planar bipartite graphs and split graphs. From a parameterized point of view, we show that both problems are fixed-parameter tractable with respect to the vertex cover number but W[1]-hard with respect to the solution size. Furthermore, we prove that Bounded-Density Edge Deletion is W[1]-hard with respect to the feedback edge number, demonstrating that the problem remains hard on very sparse graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08599v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cristina Bazgan, Andr\'e Nichterlein, Sofia Vazquez Alferez</dc:creator>
    </item>
    <item>
      <title>On the Power of Interactive Proofs for Learning</title>
      <link>https://arxiv.org/abs/2404.08158</link>
      <description>arXiv:2404.08158v1 Announce Type: cross 
Abstract: We continue the study of doubly-efficient proof systems for verifying agnostic PAC learning, for which we obtain the following results.
  - We construct an interactive protocol for learning the $t$ largest Fourier characters of a given function $f \colon \{0,1\}^n \to \{0,1\}$ up to an arbitrarily small error, wherein the verifier uses $\mathsf{poly}(t)$ random examples. This improves upon the Interactive Goldreich-Levin protocol of Goldwasser, Rothblum, Shafer, and Yehudayoff (ITCS 2021) whose sample complexity is $\mathsf{poly}(t,n)$.
  - For agnostically learning the class $\mathsf{AC}^0[2]$ under the uniform distribution, we build on the work of Carmosino, Impagliazzo, Kabanets, and Kolokolova (APPROX/RANDOM 2017) and design an interactive protocol, where given a function $f \colon \{0,1\}^n \to \{0,1\}$, the verifier learns the closest hypothesis up to $\mathsf{polylog}(n)$ multiplicative factor, using quasi-polynomially many random examples. In contrast, this class has been notoriously resistant even for constructing realisable learners (without a prover) using random examples.
  - For agnostically learning $k$-juntas under the uniform distribution, we obtain an interactive protocol, where the verifier uses $O(2^k)$ random examples to a given function $f \colon \{0,1\}^n \to \{0,1\}$. Crucially, the sample complexity of the verifier is independent of $n$.
  We also show that if we do not insist on doubly-efficient proof systems, then the model becomes trivial. Specifically, we show a protocol for an arbitrary class $\mathcal{C}$ of Boolean functions in the distribution-free setting, where the verifier uses $O(1)$ labeled examples to learn $f$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08158v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3618260.3649784</arxiv:DOI>
      <dc:creator>Tom Gur, Mohammad Mahdi Jahanara, Mohammad Mahdi Khodabandeh, Ninad Rajgopal, Bahar Salamatian, Igor Shinkar</dc:creator>
    </item>
    <item>
      <title>Asymptotics of relaxed $k$-ary trees</title>
      <link>https://arxiv.org/abs/2404.08415</link>
      <description>arXiv:2404.08415v1 Announce Type: cross 
Abstract: A relaxed $k$-ary tree is an ordered directed acyclic graph with a unique source and sink in which every node has out-degree $k$. These objects arise in the compression of trees in which some repeated subtrees are factored and repeated appearances are replaced by pointers. We prove an asymptotic theta-result for the number of relaxed $k$-ary tree with $n$ nodes for $n \to \infty$. This generalizes the previously proved binary case to arbitrary finite arity, and shows that the seldom observed phenomenon of a stretched exponential term $e^{c n^{1/3}}$ appears in all these cases. We also derive the recurrences for compacted $k$-ary trees in which all subtrees are unique and minimal deterministic finite automata accepting a finite language over a finite alphabet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08415v1</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manosij Ghosh Dastidar, Michael Wallner</dc:creator>
    </item>
    <item>
      <title>An improved spectral lower bound of treewidth</title>
      <link>https://arxiv.org/abs/2404.08520</link>
      <description>arXiv:2404.08520v1 Announce Type: cross 
Abstract: We show that for every $n$-vertex graph with at least one edge, its treewidth is greater than or equal to $n \lambda_{2} / (\Delta + \lambda_{2}) - 1$, where $\Delta$ and $\lambda_{2}$ are the maximum degree and the second smallest Laplacian eigenvalue of the graph, respectively. This lower bound improves the one by Chandran and Subramanian [Inf. Process. Lett., 2003] and the subsequent one by the authors of the present paper [IEICE Trans. Inf. Syst., 2024]. The new lower bound is almost tight in the sense that there is an infinite family of graphs such that the lower bound is only $1$ less than the treewidth for each graph in the family. Additionally, using similar techniques, we also present a lower bound of treewidth in terms of the largest and the second smallest Laplacian eigenvalues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08520v1</guid>
      <category>math.CO</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tatsuya Gima, Tesshu Hanaka, Kohei Noro, Hirotaka Ono, Yota Otachi</dc:creator>
    </item>
    <item>
      <title>Approximating the volume of a truncated relaxation of the independence polytope</title>
      <link>https://arxiv.org/abs/2404.08577</link>
      <description>arXiv:2404.08577v1 Announce Type: cross 
Abstract: Answering a question of Gamarnik and Smedira, we give a polynomial time algorithm that approximately computes the volume of a truncation of a relaxation of the independent set polytope, improving on their quasi-polynomial time algorithm. Our algorithm is obtained by viewing the volume as an evaluation of a graph polynomial and we approximate this evaluation using Barvinok's interpolation method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08577v1</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ferenc Bencs, Guus Regts</dc:creator>
    </item>
    <item>
      <title>Composing dynamic programming tree-decomposition-based algorithms</title>
      <link>https://arxiv.org/abs/1904.12500</link>
      <description>arXiv:1904.12500v4 Announce Type: replace 
Abstract: Given two integers $\ell$ and $p$ as well as $\ell$ graph classes $\mathcal{H}_1,\ldots,\mathcal{H}_\ell$, the problems $\mathsf{GraphPart}(\mathcal{H}_1, \ldots, \mathcal{H}_\ell,p)$, \break $\mathsf{VertPart}(\mathcal{H}_1, \ldots, \mathcal{H}_\ell)$, and $\mathsf{EdgePart}(\mathcal{H}_1, \ldots, \mathcal{H}_\ell)$ ask, given graph $G$ as input, whether $V(G)$, $V(G)$, $E(G)$ respectively can be partitioned into $\ell$ sets $S_1, \ldots, S_\ell$ such that, for each $i$ between $1$ and $\ell$, $G[S_i] \in \mathcal{H}_i$, $G[S_i] \in \mathcal{H}_i$, $(V(G),S_i) \in \mathcal{H}_i$ respectively. Moreover in $\mathsf{GraphPart}(\mathcal{H}_1, \ldots, \mathcal{H}_\ell,p)$, we request that the number of edges with endpoints in different sets of the partition is bounded by $p$. We show that if there exist dynamic programming tree-decomposition-based algorithms for recognizing the graph classes $\mathcal{H}_i$, for each $i$, then we can constructively create a dynamic programming tree-decomposition-based algorithms for $\mathsf{GraphPart}(\mathcal{H}_1, \ldots, \mathcal{H}_\ell,p)$, $\mathsf{VertPart}(\mathcal{H}_1, \ldots, \mathcal{H}_\ell)$, and $\mathsf{EdgePart}(\mathcal{H}_1, \ldots, \mathcal{H}_\ell)$. We apply this approach to known problems. For well-studied problems, like VERTEX COVER and GRAPH $q$-COLORING, we obtain running times that are comparable to those of the best known problem-specific algorithms. For an exotic problem from bioinformatics, called DISPLAYGRAPH, this approach improves the known algorithm parameterized by treewidth.</description>
      <guid isPermaLink="false">oai:arXiv.org:1904.12500v4</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.46298/dmtcs.11069</arxiv:DOI>
      <dc:creator>Julien Baste</dc:creator>
    </item>
    <item>
      <title>Algorithmic Extensions of Dirac's Theorem</title>
      <link>https://arxiv.org/abs/2011.03619</link>
      <description>arXiv:2011.03619v5 Announce Type: replace 
Abstract: In 1952, Dirac proved the following theorem about long cycles in graphs with large minimum vertex degrees: Every $n$-vertex $2$-connected graph $G$ with minimum vertex degree $\delta\geq 2$ contains a cycle with at least $\min\{2\delta,n\}$ vertices. In particular, if $\delta\geq n/2$, then $G$ is Hamiltonian. The proof of Dirac's theorem is constructive, and it yields an algorithm computing the corresponding cycle in polynomial time. The combinatorial bound of Dirac's theorem is tight in the following sense. There are 2-connected graphs that do not contain cycles of length more than $2\delta+1$. Also, there are non-Hamiltonian graphs with all vertices but one of degree at least $n/2$. This prompts naturally to the following algorithmic questions. For $k\geq 1$,
  (A) How difficult is to decide whether a 2-connected graph contains a cycle of length at least $\min\{2\delta+k,n\}$?
  (B) How difficult is to decide whether a graph $G$ is Hamiltonian, when at least $n - k$ vertices of $G$ are of degrees at least $n/2-k$?
  The first question was asked by Fomin, Golovach, Lokshtanov, Panolan, Saurabh, and Zehavi. The second question is due to Jansen, Kozma, and Nederlof. Even for a very special case of $k=1$, the existence of a polynomial-time algorithm deciding whether $G$ contains a cycle of length at least $\min\{2\delta+1,n\}$ was open. We resolve both questions by proving the following algorithmic generalization of Dirac's theorem: If all but $k$ vertices of a $2$-connected graph $G$ are of degree at least $\delta$, then deciding whether $G$ has a cycle of length at least $\min\{2\delta +k, n\}$ can be done in time $2^{\mathcal{O}(k)}\cdot n^{\mathcal{O}(1)}$.
  The proof of the algorithmic generalization of Dirac's theorem builds on new graph-theoretical results that are interesting on their own.</description>
      <guid isPermaLink="false">oai:arXiv.org:2011.03619v5</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fedor V. Fomin, Petr A. Golovach, Danil Sagunov, Kirill Simonov</dc:creator>
    </item>
    <item>
      <title>On 2-strong connectivity orientations of mixed graphs and related problems</title>
      <link>https://arxiv.org/abs/2302.02215</link>
      <description>arXiv:2302.02215v3 Announce Type: replace 
Abstract: A mixed graph $G$ is a graph that consists of both undirected and directed edges. An orientation of $G$ is formed by orienting all the undirected edges of $G$, i.e., converting each undirected edge $\{u,v\}$ into a directed edge that is either $(u,v)$ or $(v,u)$. The problem of finding an orientation of a mixed graph that makes it strongly connected is well understood and can be solved in linear time. Here we introduce the following orientation problem in mixed graphs. Given a mixed graph $G$, we wish to compute its maximal sets of vertices $C_1,C_2,\ldots,C_k$ with the property that by removing any edge $e$ from $G$ (directed or undirected), there is an orientation $R_i$ of $G\setminus{e}$ such that all vertices in $C_i$ are strongly connected in $R_i$. We discuss properties of those sets, and we show how to solve this problem in linear time by reducing it to the computation of the $2$-edge twinless strongly connected components of a directed graph. A directed graph $G=(V,E)$ is twinless strongly connected if it contains a strongly connected spanning subgraph without any pair of antiparallel (or twin) edges. The twinless strongly connected components (TSCCs) of a directed graph $G$ are its maximal twinless strongly connected subgraphs. A $2$-edge twinless strongly connected component (2eTSCC) of $G$ is a maximal subset of vertices $C$ such that any two vertices $u, v \in C$ are in the same twinless strongly connected component of $G \setminus e$, for any edge $e$. These concepts are motivated by several diverse applications, such as the design of road and telecommunication networks, and the structural stability of buildings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.02215v3</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Loukas Georgiadis, Dionysios Kefallinos, Evangelos Kosinas</dc:creator>
    </item>
    <item>
      <title>On Approximating Cutwidth and Pathwidth</title>
      <link>https://arxiv.org/abs/2311.15639</link>
      <description>arXiv:2311.15639v2 Announce Type: replace 
Abstract: We study graph ordering problems with a min-max objective. A classical problem of this type is cutwidth, where given a graph we want to order its vertices such that the number of edges crossing any point is minimized. We give a $ \log^{1+o(1)}(n)$ approximation for the problem, substantially improving upon the previous poly-logarithmic guarantees based on the standard recursive balanced partitioning approach of Leighton and Rao (FOCS'88). Our key idea is a new metric decomposition procedure that is suitable for handling min-max objectives, which could be of independent interest. We also use this to show other results, including an improved $ \log^{1+o(1)}(n)$ approximation for computing the pathwidth of a graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15639v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nikhil Bansal, Dor Katzelnick, Roy Schwartz</dc:creator>
    </item>
    <item>
      <title>Fully Dynamic Correlation Clustering: Breaking 3-Approximation</title>
      <link>https://arxiv.org/abs/2404.06797</link>
      <description>arXiv:2404.06797v2 Announce Type: replace 
Abstract: We study the classic correlation clustering in the dynamic setting. Given $n$ objects and a complete labeling of the object-pairs as either similar or dissimilar, the goal is to partition the objects into arbitrarily many clusters while minimizing disagreements with the labels. In the dynamic setting, an update consists of a flip of a label of an edge. In a breakthrough result, [BDHSS, FOCS'19] showed how to maintain a 3-approximation with polylogarithmic update time by providing a dynamic implementation of the Pivot algorithm of [ACN, STOC'05]. Since then, it has been a major open problem to determine whether the 3-approximation barrier can be broken in the fully dynamic setting. In this paper, we resolve this problem. Our algorithm, Modified Pivot, locally improves the output of Pivot by moving some vertices to other existing clusters or new singleton clusters. We present an analysis showing that this modification does indeed improve the approximation to below 3. We also show that its output can be maintained in polylogarithmic time per update.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06797v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soheil Behnezhad, Moses Charikar, Vincent Cohen-Addad, Alma Ghafari, Weiyun Ma</dc:creator>
    </item>
    <item>
      <title>On the complexity of isomorphism problems for tensors, groups, and polynomials IV: linear-length reductions and their applications</title>
      <link>https://arxiv.org/abs/2306.16317</link>
      <description>arXiv:2306.16317v2 Announce Type: replace-cross 
Abstract: Many isomorphism problems for tensors, groups, algebras, and polynomials were recently shown to be equivalent to one another under polynomial-time reductions, prompting the introduction of the complexity class TI (Grochow &amp; Qiao, ITCS '21; SIAM J. Comp., '23). Using the tensorial viewpoint, Grochow &amp; Qiao (CCC '21) then gave moderately exponential-time search- and counting-to-decision reductions for a class of $p$-groups. A significant issue was that the reductions usually incurred a quadratic increase in the length of the tensors involved. When the tensors represent $p$-groups, this corresponds to an increase in the order of the group of the form $|G|^{\Theta(\log |G|)}$, negating any asymptotic gains in the Cayley table model.
  In this paper, we present a new kind of tensor gadget that allows us to replace those quadratic-length reductions with linear-length ones, yielding the following consequences:
  1. If Graph Isomorphism is in P, then testing equivalence of cubic forms in $n$ variables over $F_q$, and testing isomorphism of $n$-dimensional algebras over $F_q$, can both be solved in time $q^{O(n)}$, improving from the brute-force upper bound $q^{O(n^2)}$ for both of these.
  2. Combined with the $|G|^{O((\log |G|)^{5/6})}$-time isomorphism-test for $p$-groups of class 2 and exponent $p$ (Sun, STOC '23), our reductions extend this runtime to $p$-groups of class $c$ and exponent $p$ where $c&lt;p$, and yield algorithms in time $q^{O(n^{1.8}\cdot \log q)}$ for cubic form equivalence and algebra isomorphism.
  3. Polynomial-time search- and counting-to-decision reduction for testing isomorphism of $p$-groups of class $2$ and exponent $p$ when Cayley tables are given. This answers questions of Arvind and T\'oran (Bull. EATCS, 2005) for this group class, thought to be one of the hardest cases of Group Isomorphism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16317v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.AG</category>
      <category>math.GR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua A. Grochow, Youming Qiao</dc:creator>
    </item>
    <item>
      <title>Hardness of circuit and monotone diameters of polytopes</title>
      <link>https://arxiv.org/abs/2404.04158</link>
      <description>arXiv:2404.04158v2 Announce Type: replace-cross 
Abstract: The Circuit diameter of polytopes was introduced by Borgwardt, Finhold and Hemmecke as a fundamental tool for the study of circuit augmentation schemes for linear programming and for estimating combinatorial diameters. Determining the complexity of computing the circuit diameter of polytopes was posed as an open problem by Sanit\`a as well as by Kafer, and was recently reiterated by Borgwardt, Grewe, Kafer, Lee and Sanit\`a.
  In this paper, we solve this problem by showing that computing the circuit diameter of a polytope given in halfspace-description is strongly NP-hard. To prove this result, we show that computing the combinatorial diameter of the perfect matching polytope of a bipartite graph is NP-hard. This complements a result by Sanit\`a (FOCS 2018) on the NP-hardness of computing the diameter of fractional matching polytopes and implies the new result that computing the diameter of a $\{0,1\}$-polytope is strongly NP-hard, which may be of independent interest. In our second main result, we give a precise graph-theoretic description of the monotone diameter of perfect matching polytopes and use this description to prove that computing the monotone (circuit) diameter of a given input polytope is strongly NP-hard as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04158v2</guid>
      <category>math.OC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian N\"obel, Raphael Steiner</dc:creator>
    </item>
  </channel>
</rss>
