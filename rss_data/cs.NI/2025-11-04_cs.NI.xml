<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Nov 2025 05:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Towards Sub-millisecond Latency and Guaranteed Bit Rates in 5G User Plane</title>
      <link>https://arxiv.org/abs/2511.00196</link>
      <description>arXiv:2511.00196v1 Announce Type: new 
Abstract: Next-generation services demand stringent Quality of Service (QoS) guarantees, such as per-flow bandwidth assurance, ultra-low latency, and traffic prioritization, posing significant challenges to 5G and beyond networks. As 5G network functions increasingly migrate to edge and central clouds, the transport layer becomes a critical enabler of end-to-end QoS compliance. However, traditional fixed-function infrastructure lacks the flexibility to support the diverse and dynamic QoS profiles standardized by 3GPP.
  This paper presents a QoS-aware data plane model for programmable transport networks, designed to provide predictable behavior and fine-grained service differentiation. The model supports all 3GPP QoS resource types and integrates per-flow metering, classification, strict priority scheduling, and delay-aware queuing. Implemented on off-the-shelf programmable hardware using P4 and evaluated on an Intel Tofino switch, our approach ensures per-flow bandwidth guarantees, sub-millisecond delay for delay-critical traffic, and resilience under congestion. Experimental results demonstrate that the model achieves microsecond-level latencies and near-zero packet loss for mission-critical flows, validating its suitability for future QoS-sensitive applications in 5G and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00196v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonardo Alberro, Noura Limam, Raouf Boutaba</dc:creator>
    </item>
    <item>
      <title>Toward Hybrid COTS-based LiFi/WiFi Networks with QoS Requirements in Mobile Environments</title>
      <link>https://arxiv.org/abs/2511.00210</link>
      <description>arXiv:2511.00210v1 Announce Type: new 
Abstract: We consider a hybrid LiFi/WiFi network consisting of commercially available equipment, for mobile scenarios, where WiFi backs up communications, through vertical handovers, in case of insufficient LiFi QoS. When QoS requirements in terms of goodput are defined, tools are needed to anticipate the vertical handover relative to what is possible with standard basic mechanisms, which are only based on a complete loss of connectivity. We introduce two such mechanisms, based on signal power level readings and CRC-based packet failure ratio, and evaluate their performance in terms of QoS-outage duration, considering as a benchmark an existing baseline solution based on the detection of a connectivity loss. In doing this, we provide insights into the interplay between such mechanisms and the LiFi protocol channel adaptation capabilities. Our experimental results are obtained using a lab-scale testbed equipped with a conveyor belt, which allows us to accurately replicate experiments with devices in motion. With the proposed methods, we achieve QoS outages below one second for a QoS level of 20 Mbps, compared to outage durations of a few seconds obtained with the baseline solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00210v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Emilio Ancillotti, Loreto Pescosolido, Andrea Passarella</dc:creator>
    </item>
    <item>
      <title>Mist-Assisted Federated Learning for Intrusion Detection in Heterogeneous IoT Networks</title>
      <link>https://arxiv.org/abs/2511.00271</link>
      <description>arXiv:2511.00271v1 Announce Type: new 
Abstract: The rapid growth of the Internet of Things (IoT) offers new opportunities but also expands the attack surface of distributed, resource-limited devices. Intrusion detection in such environments is difficult due to data heterogeneity from diverse sensing modalities and the non-IID distribution of samples across clients. Federated Learning (FL) provides a privacy-preserving alternative to centralized training, yet conventional frameworks struggle under these conditions. To address this, we propose a Mist-assisted hierarchical framework for IoT intrusion detection. The architecture spans four layers: (i) Mist, where raw data are abstracted into a unified feature space and lightweight models detect anomalies; (ii) Edge, which applies utility-based client selection; (iii) Fog, where multiple regional aggregators use FedProx to stabilize training; and (iv) Cloud, which consolidates and disseminates global models. Evaluations on the TON-IoT dataset show the framework achieves 98-99% accuracy, PR-AUC&gt; 0.97, and stable convergence under heterogeneous and large-scale settings, while maintaining efficiency and preserving privacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00271v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saadat Izadi, Shakib Komasi, Ali Salimi, Alireza Rezaei, Mahmood Ahmadi</dc:creator>
    </item>
    <item>
      <title>Reinforcement Learning for Resource Allocation in Vehicular Multi-Fog Computing</title>
      <link>https://arxiv.org/abs/2511.00276</link>
      <description>arXiv:2511.00276v1 Announce Type: new 
Abstract: The exponential growth of Internet of Things (IoT) devices, smart vehicles, and latency-sensitive applications has created an urgent demand for efficient distributed computing paradigms. Multi-Fog Computing (MFC), as an extension of fog and edge computing, deploys multiple fog nodes near end users to reduce latency, enhance scalability, and ensure Quality of Service (QoS). However, resource allocation in MFC environments is highly challenging due to dynamic vehicular mobility, heterogeneous resources, and fluctuating workloads. Traditional optimization-based methods often fail to adapt to such dynamics. Reinforcement Learning (RL), as a model-free decision-making framework, enables adaptive task allocation by continuously interacting with the environment. This paper formulates the resource allocation problem in MFC as a Markov Decision Process (MDP) and investigates the application of RL algorithms such as Q-learning, Deep Q-Networks (DQN), and Actor-Critic. We present experimental results demonstrating improvements in latency, workload balance, and task success rate. The contributions and novelty of this study are also discussed, highlighting the role of RL in addressing emerging vehicular computing challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00276v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohammad Hadi Akbarzadeh, Mahmood Ahmadi, Mohammad Saeed Jahangiry, Jae Young Hur</dc:creator>
    </item>
    <item>
      <title>COHERE - Congestion-aware Offloading and Handover via Empirical RAT Evaluation for Multi-RAT Networks</title>
      <link>https://arxiv.org/abs/2511.00439</link>
      <description>arXiv:2511.00439v1 Announce Type: new 
Abstract: The evolution of wireless networks and radio access technologies (RATs) has transformed communication from user-driven traffic into a dynamic ecosystem of autonomous systems, including IoT devices, edge nodes, autonomous vehicles, AR/XR clients, and AI-powered agents. These systems exhibit diverse traffic patterns, latency requirements, and mobility behaviors, increasingly operating across overlapping heterogeneous RATs such as 5G, WiFi, satellite, NB-IoT, LoRaWAN, Zigbee, etc. This multi-RAT coexistence creates opportunities for intelligent access, mobility, and routing strategies. However, most mobility decisions still rely heavily on RSSI, which neglects RAT-specific features, congestion, queuing delays, and application needs, favoring high-power links over optimal ones. To address this gap, we propose chrome (Congestion-aware Offloading and Handover via Empirical RAT Evaluation), a multi criteria framework for dense multi-RAT networks. chrome enhances RSSI with multiple criteria and applies the Technique for Order of Preference by Similarity to the Ideal Solution (TOPSIS) to rank available RATs. Criteria weights are determined using both subjective (operator-driven) and objective (measurement-based) approaches. Based on this ranking, chrome performs intelligent cross-RAT offloading to reduce congestion on over-utilized links. We evaluate chrome in a dense SDN-controlled 5G/WiFi Multi-RAT environment using Mininet WiFi. Compared to RSSI-only handover, COHERE reduces the load on the congested RAT by up to 32%, reduces total handovers by 25%, lowers handovers to the congested RAT by 55%, and improves link delay by up to 166%, while maintaining comparable or up to 11% higher throughput. These results demonstrate that guarded, multi-criteria decision-making can exploit RAT coexistence to deliver robust, congestion-aware performance across heterogeneous deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00439v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pavan K. Mangipudi, Sharon Boamah, Lorenz Carvajal, Janise Mcnair</dc:creator>
    </item>
    <item>
      <title>Impact of Antenna Arrays Misalignment on the Near Field Distance in Terahertz Communications</title>
      <link>https://arxiv.org/abs/2511.00502</link>
      <description>arXiv:2511.00502v1 Announce Type: new 
Abstract: The extremely short wavelength of terahertz (THz) communications leads to an extended radiative near-field region, in which some canonical far-field assumptions fail. Existing near-field boundary formulations (Fraunhofer distance) for uniform linear/planar array (ULA/UPA) configurations assume ideal alignment between transceivers, overlooking practical misalignments caused by mobility or mechanical imperfections. This paper addresses this critical gap by analyzing the impact of spatial misalignment on near-field distance calculations in THz systems. We derive exact analytical expressions and simplified approximations for the near-field boundary in both ULA--ULA and UPA--UPA configurations under arbitrary misalignment offsets. Through numerical simulations, we validate our theoretical models and quantify how misalignment reshapes the near-field region. These findings provide essential guidelines for optimizing THz system deployment in realistic scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00502v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peng Zhang, Vitaly Petrov, Emil Bj\"ornson</dc:creator>
    </item>
    <item>
      <title>Advancing Fluid Antenna-Assisted Non-Terrestrial Networks in 6G and Beyond: Fundamentals, State of the Art, and Future Directions</title>
      <link>https://arxiv.org/abs/2511.00569</link>
      <description>arXiv:2511.00569v1 Announce Type: new 
Abstract: With the surging demand for ultra-reliable, low-latency, and ubiquitous connectivity in Sixth-Generation (6G) networks, Non-Terrestrial Networks (NTNs) emerge as a key complement to terrestrial networks by offering flexible access and global coverage. Despite the significant potential, NTNs still face critical challenges, including dynamic propagation environments, energy constraints, and dense interference. As a key 6G technology, Fluid Antennas (FAs) can reshape wireless channels by reconfiguring radiating elements within a limited space, such as their positions and rotations, to provide higher channel diversity and multiplexing gains. Compared to fixed-position antennas, FAs can present a promising integration path for NTNs to mitigate dynamic channel fading and optimize resource allocation. This paper provides a comprehensive review of FA-assisted NTNs. We begin with a brief overview of the classical structure and limitations of existing NTNs, the fundamentals and advantages of FAs, and the basic principles of FA-assisted NTNs. We then investigate the joint optimization solutions, detailing the adjustments of FA configurations, NTN platform motion modes, and resource allocations. We also discuss the combination with other emerging technologies and explore FA-assisted NTNs as a novel network architecture for intelligent function integrations. Furthermore, we delve into the physical layer security and covert communication in FA-assisted NTNs. Finally, we highlight the potential future directions to empower broader applications of FA-assisted NTNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00569v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianheng Xu, Runke Fan, Jie Zhu, Pei Peng, Xianfu Chen, Qingqing Wu, Ming Jiang, Celimuge Wu, Dusit Niyato, Kai-Kit Wong</dc:creator>
    </item>
    <item>
      <title>Power Control Based on Multi-Agent Deep Q Network for D2D Communication</title>
      <link>https://arxiv.org/abs/2511.00767</link>
      <description>arXiv:2511.00767v1 Announce Type: new 
Abstract: In device-to-device (D2D) communication under a cell with resource sharing mode the spectrum resource utilization of the system will be improved. However, if the interference generated by the D2D user is not controlled, the performance of the entire system and the quality of service (QOS) of the cellular user may be degraded. Power control is important because it helps to reduce interference in the system. In this paper, we propose a reinforcement learning algorithm for adaptive power control that helps reduce interference to increase system throughput. Simulation results show the proposed algorithm has better performance than traditional algorithm in LTE (Long Term Evolution).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00767v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shi Gengtian, Takashi Koshimizu, Megumi Saito, Pan Zhenni, Liu Jiang, Shigeru Shimamoto</dc:creator>
    </item>
    <item>
      <title>TINC: Trusted Intelligent NetChain</title>
      <link>https://arxiv.org/abs/2511.00823</link>
      <description>arXiv:2511.00823v1 Announce Type: new 
Abstract: Blockchain technology facilitates the development of decentralized systems that ensure trust and transparency without the need for expensive centralized intermediaries. However, existing blockchain architectures particularly consortium blockchains face critical challenges related to scalability and efficiency. State sharding has emerged as a promising approach to enhance blockchain scalability and performance. However, current shard-based solutions often struggle to guarantee fair participation and a balanced workload distribution among consortium members. To address these limitations, we propose Trusted Intelligent NetChain (TINC), a multi-plane sharding architecture specifically designed for consortium blockchains. TINC incorporates intelligent mechanisms for adaptive node assignment and dynamic workload balancing, enabling the system to respond effectively to changing network conditions while maintaining equitable shard utilization. By decoupling the control and data planes, TINC allows control nodes to focus on consensus operations, while data nodes handle large-scale storage, thus improving overall resource efficiency. Extensive experimental evaluation and formal analysis demonstrate that TINC significantly outperforms existing shard-based blockchain frameworks. It achieves higher throughput, lower latency, balanced node and transaction distributions, and reduced transaction failure rates. Furthermore, TINC maintains essential blockchain security guarantees, exhibiting resilience against Byzantine faults and dynamic network environments. The integration of Dynamic Decentralized Identifiers (DDIDs) further strengthens trust and security management within the consortium network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00823v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qi Xia, Hu Xia, Isaac Amankona Obiri, Adjei-Arthur Bonsu, Grace Mupoyi Ntuala, Ansu Badjie, Tienin Bole Wilfried, Jiaqin Liu, Lan Ma, Jianbin Gao, Feng Yao</dc:creator>
    </item>
    <item>
      <title>DPMon: a Differentially-Private Query Engine for Passive Measurements</title>
      <link>https://arxiv.org/abs/2511.00906</link>
      <description>arXiv:2511.00906v1 Announce Type: new 
Abstract: Passive monitoring is a network measurement technique which analyzes the traffic carried by an operational network. It has several applications for traffic engineering, Quality of Experience monitoring and cyber security. However, it entails the processing of personal information, thus, threatening users' privacy. In this work, we propose DPMon, a tool to run privacy-preserving queries to a dataset of passive network measurements. It exploits differential privacy to perturb the output of the query to preserve users' privacy. DPMon can exploit big data infrastructures running Apache Spark and operate on different data formats. We show that DPMon allows extracting meaningful insights from the data, while at the same time controlling the amount of disclosed information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00906v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Martino Trevisan</dc:creator>
    </item>
    <item>
      <title>Optimizing Energy and Latency in 6G Smart Cities with Edge CyberTwins</title>
      <link>https://arxiv.org/abs/2511.00955</link>
      <description>arXiv:2511.00955v1 Announce Type: new 
Abstract: The proliferation of IoT devices in smart cities challenges 6G networks with conflicting energy-latency requirements across heterogeneous slices. Existing approaches struggle with the energy-latency trade-off, particularly for massive scale deployments exceeding 50,000 devices km. This paper proposes an edge-aware CyberTwin framework integrating hybrid federated learning for energy-latency co-optimization in 6G network slicing. Our approach combines centralized Artificial Intelligence scheduling for latency-sensitive slices with distributed federated learning for non-critical slices, enhanced by compressive sensing-based digital twins and renewable energy-aware resource allocation. The hybrid scheduler leverages a three-tier architecture with Physical Unclonable Function (PUF) based security attestation achieving 99.7% attack detection accuracy. Comprehensive simulations demonstrate 52% energy reduction for non-real-time slices compared to Diffusion-Reinforcement Learning baselines while maintaining 0.9ms latency for URLLC applications with 99.1% SLA compliance. The framework scales to 50,000 devices km with CPU overhead below 25%, validated through NS-3 hybrid simulations across realistic smart city scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00955v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator> Abouaomar, Badr Ben Elallid, Nabil Benamar</dc:creator>
    </item>
    <item>
      <title>Detecting Coverage Holes in Wireless Sensor Networks Using Connected Component Labeling and Force-Directed Algorithms</title>
      <link>https://arxiv.org/abs/2511.00965</link>
      <description>arXiv:2511.00965v1 Announce Type: new 
Abstract: Contour detection in Wireless Sensor Networks (WSNs) is crucial for tasks like energy saving and network optimization, especially in security and surveillance applications. Coverage holes, where data transmission is not achievable, are a significant issue caused by factors such as energy depletion and physical damage. Traditional methods for detecting these holes often suffer from inaccuracy, low processing speed, and high energy consumption, relying heavily on physical information like node coordinates and sensing range. To address these challenges, we propose a novel, coordinate-free coverage hole detection method using Connected Component Labeling (CCL) and Force-Directed (FD) algorithms, termed FD-CCL. This method does not require node coordinates or sensing range information. We also investigate Suzuki's Contour Tracing (CT) algorithm and compare its performance with CCL on various FD graphs. Our experiments demonstrate the effectiveness of FD-CCL in terms of processing time and accuracy. Simulation results confirm the superiority of FD-CCL in detecting and locating coverage holes in WSNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00965v1</guid>
      <category>cs.NI</category>
      <category>cs.GR</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiacheng Xu, Xiongfei Zhao, Hou-Wan Long, Cheong Se-Hang, Yain-Whar Si</dc:creator>
    </item>
    <item>
      <title>Quantum Reinforcement Learning for 6G and Beyond Wireless Networks</title>
      <link>https://arxiv.org/abs/2511.01070</link>
      <description>arXiv:2511.01070v1 Announce Type: new 
Abstract: While 5G is being deployed worldwide, 6G is receiving increasing attention from researchers to meet the growing demand for higher data rates, lower latency, higher density, and seamless communications worldwide. To meet the stringent requirements of 6G wireless communications networks, AI-integrated communications have become an indispensable part of supporting 6G systems with intelligence, automation, and big data training capabilities. However, traditional artificial intelligence (AI) systems are difficult to meet the stringent latency and high throughput requirements of 6G with limited resources. In this article, we summarize, analyze, discuss the potential, and benefits of Quantum Reinforcement Learning (QRL) in 6G. As an example, we show the superiority of QRL in dynamic spectrum access compared to the conventional Deep Reinforcement Learning (DRL) approach. In addition, we provide an overview of what DRL has accomplished in 6G and its challenges and limitations. From there, we introduce QRL and potential research directions that should continue to be of interest in 6G. To the best of our knowledge, this is the first review and vision article on QRL for 6G wireless communication networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01070v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dinh-Hieu Tran, Thai Duong Nguyen, Thanh-Dao Nguyen, Ngoc-Tan Nguyen, Van Nhan Vo, Hung Tran, Mouhamad Chehaitly, Yan Kyaw Tun, Cedomir Stefanovic, Tu Ho Dac, Eva Lagunas, Symeon Chatzinotas, Nguyen Van Huynh</dc:creator>
    </item>
    <item>
      <title>Quantum Network Tomography for General Topology with SPAM Errors</title>
      <link>https://arxiv.org/abs/2511.01074</link>
      <description>arXiv:2511.01074v1 Announce Type: new 
Abstract: The goal of quantum network tomography (QNT) is the characterization of internal quantum channels in a quantum network from external peripheral operations. Prior research has primarily focused on star networks featuring bit-flip and depolarizing channels, leaving the broader problem -- such as QNT for networks with arbitrary topologies and general Pauli channels -- largely unexplored. Moreover, establishing channel identifiability remains a significant challenge even in simplified quantum star networks.
  In the first part of this paper, we introduce a novel network tomography method, termed Mergecast, in quantum networks. We demonstrate that Mergecast, together with a progressive etching procedure, enables the unique identification of all internal quantum channels in networks characterized by arbitrary topologies and Pauli channels. As a side contribution, we introduce a subclass of Pauli channels, termed bypassable Pauli channels, and propose a more efficient unicast-based tomography method, called BypassUnicast, for networks exclusively comprising these channels. In the second part, we extend our investigation to a more realistic QNT scenario that incorporates state preparation and measurement (SPAM) errors. We rigorously formulate SPAM errors in QNT, propose estimation protocols for such errors within QNT, and subsequently adapt our Mergecast approaches to handle networks affected by SPAM errors. Lastly, we conduct NetSquid-based simulations to corroborate the effectiveness of our proposed protocols in identifying internal quantum channels and estimating SPAM errors in quantum networks. In particular, we demonstrate that Mergecast maintains good performance under realistic conditions, such as photon loss and quantum memory decoherence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01074v1</guid>
      <category>cs.NI</category>
      <category>quant-ph</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuchuang Wang, Matheus Guedes De Andrade, Guus Avis, Yu-zhen Janice Chen, Mohammad Hajiesmaili, Don Towsley</dc:creator>
    </item>
    <item>
      <title>Joint Computation Offloading and Resource Allocation for Maritime MEC with Energy Harvesting</title>
      <link>https://arxiv.org/abs/2511.01160</link>
      <description>arXiv:2511.01160v1 Announce Type: new 
Abstract: In this paper, we establish a multi-access edge computing (MEC)-enabled sea lane monitoring network (MSLMN) architecture with energy harvesting (EH) to support dynamic ship tracking, accident forensics, and anti-fouling through real-time maritime traffic scene monitoring. Under this architecture, the computation offloading and resource allocation are jointly optimized to maximize the long-term average throughput of MSLMN. Due to the dynamic environment and unavailable future network information, we employ the Lyapunov optimization technique to tackle the optimization problem with large state and action spaces and formulate a stochastic optimization program subject to queue stability and energy consumption constraints. We transform the formulated problem into a deterministic one and decouple the temporal and spatial variables to obtain asymptotically optimal solutions. Under the premise of queue stability, we develop a joint computation offloading and resource allocation (JCORA) algorithm to maximize the long-term average throughput by optimizing task offloading, subchannel allocation, computing resource allocation, and task migration decisions. Simulation results demonstrate the effectiveness of the proposed scheme over existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01160v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhen Wang, Bin Lin, Qiang Ye, Yuguang Fang, Xiaoling Han</dc:creator>
    </item>
    <item>
      <title>3D Gaussian Radiation Field Modeling for Integrated RIS-FAS Systems: Analysis and Optimization</title>
      <link>https://arxiv.org/abs/2511.01373</link>
      <description>arXiv:2511.01373v1 Announce Type: new 
Abstract: The integration of reconfigurable intelligent surfaces (RIS) and fluid antenna systems (FAS) has attracted considerable attention due to its tremendous potential in enhancing wireless communication performance. However, under fast-fading channel conditions, rapidly and effectively performing joint optimization of the antenna positions in an FAS system and the RIS phase configuration remains a critical challenge. Traditional optimization methods typically rely on complex iterative computations, thus making it challenging to obtain optimal solutions in real time within dynamic channel environments. To address this issue, this paper introduces a field information-driven optimization method based on three-dimensional Gaussian radiation-field modeling for real-time optimization of integrated FAS-RIS systems. In the proposed approach, obstacles are treated as virtual transmitters and, by separately learning the amplitude and phase variations, the model can quickly generate high-precision channel information based on the transmitter's position. This design eliminates the need for extensive pilot overhead and cumbersome computations. On this framework, an alternating optimization scheme is presented to jointly optimize the FAS position and the RIS phase configuration. Simulation results demonstrate that the proposed method significantly outperforms existing approaches in terms of spectrum prediction accuracy, convergence speed, and minimum achievable rate, validating its effectiveness and practicality in fast-fading scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01373v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaining Wang, Bo Yang, Yusheng Lei, Zhiwen Yu, Xuelin Cao, Liang Wang, Bin Guo, George C. Alexandropoulos, M\'erouane Debbah, Zhu Han</dc:creator>
    </item>
    <item>
      <title>Learning a Network Digital Twin as a Hybrid System</title>
      <link>https://arxiv.org/abs/2511.00291</link>
      <description>arXiv:2511.00291v1 Announce Type: cross 
Abstract: Network digital twin (NDT) models are virtual models that replicate the behavior of physical communication networks and are considered a key technology component to enable novel features and capabilities in future 6G networks. In this work, we focus on NDTs that model the communication quality properties of a multi-cell, dynamically changing wireless network over a workspace populated with multiple moving users. We propose an NDT modeled as a hybrid system, where each mode corresponds to a different base station and comprises sub-modes that correspond to areas of the workspace with similar network characteristics. The proposed hybrid NDT is identified and continuously improved through an annealing optimization-based learning algorithm, driven by online data measurements collected by the users. The advantages of the proposed hybrid NDT are studied with respect to memory and computational efficiency, data consumption, and the ability to timely adapt to network changes. Finally, we validate the proposed methodology on real experimental data collected from a two-cell 5G testbed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00291v1</guid>
      <category>eess.SY</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christos Mavridis, Fernando S. Barbosa, Hamed Farhadi, Karl H. Johansson</dc:creator>
    </item>
    <item>
      <title>Tetris: An SLA-aware Application Placement Strategy in the Edge-Cloud Continuum</title>
      <link>https://arxiv.org/abs/2511.00294</link>
      <description>arXiv:2511.00294v1 Announce Type: cross 
Abstract: An Edge-Cloud Continuum integrates edge and cloud resources to provide a flexible and scalable infrastructure. This paradigm can minimize latency by processing data closer to the source at the edge while leveraging the vast computational power of the cloud for more intensive tasks. In this context, module application placement requires strategic allocation plans that align user demands with infrastructure constraints, aiming for efficient resource use. Therefore, we propose Tetris, an application placement strategy that utilizes a heuristic algorithm to distribute computational services across edge and cloud resources efficiently. Tetris prioritizes services based on SLA urgencies and resource efficiency to avoid system overloading. Our results demonstrate that Tetris reduces SLA violations by approximately 76% compared to the baseline method, which serves as a reference point for benchmarking performance in this scenario. Therefore, Tetris offers an effective placement approach for managing latency-sensitive applications in Edge-Cloud Continuum environments, enhancing Quality of Service (QoS) for users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00294v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Almeida, Maycon Peixoto</dc:creator>
    </item>
    <item>
      <title>Fast Networks for High-Performance Distributed Trust</title>
      <link>https://arxiv.org/abs/2511.00363</link>
      <description>arXiv:2511.00363v1 Announce Type: cross 
Abstract: Organizations increasingly need to collaborate by performing a computation on their combined dataset, while keeping their data hidden from each other. Certain kinds of collaboration, such as collaborative data analytics and AI, require a level of performance beyond what current cryptographic techniques for distributed trust can provide. This is because the organizations run software in different trust domains, which can require them to communicate over WANs or the public Internet. In this paper, we explore how to instead run such applications using fast datacenter-type LANs. We show that, by carefully redesigning distributed trust frameworks for LANs, we can achieve up to order-of-magnitude better performance than na\"ively using a LAN. Then, we develop deployment models for Distributed But Proximate Trust (DBPT) that allow parties to use a LAN while remaining physically and logically distinct. These developments make secure collaborative data analytics and AI significantly more practical and set new research directions for developing systems and cryptographic theory for high-performance distributed trust.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00363v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <category>cs.OS</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yicheng Liu, Rafail Ostrovsky, Scott Shenker, Sam Kumar</dc:creator>
    </item>
    <item>
      <title>EPARA: Parallelizing Categorized AI Inference in Edge Clouds</title>
      <link>https://arxiv.org/abs/2511.00603</link>
      <description>arXiv:2511.00603v1 Announce Type: cross 
Abstract: With the increasing adoption of AI applications such as large language models and computer vision AI, the computational demands on AI inference systems are continuously rising, making the enhancement of task processing capacity using existing hardware a primary objective in edge clouds. We propose EPARA, an end-to-end AI parallel inference framework in edge, aimed at enhancing the edge AI serving capability. Our key idea is to categorize tasks based on their sensitivity to latency/frequency and requirement for GPU resources, thereby achieving both request-level and service-level task-resource allocation. EPARA consists of three core components: 1) a task-categorized parallelism allocator that decides the parallel mode of each task, 2) a distributed request handler that performs the calculation for the specific request, and 3) a state-aware scheduler that periodically updates service placement in edge clouds. We implement a EPARA prototype and conduct a case study on the EPARA operation for LLMs and segmentation tasks. Evaluation through testbed experiments involving edge servers, embedded devices, and microcomputers shows that EPARA achieves up to 2.1$\times$ higher goodput in production workloads compared to prior frameworks, while adapting to various edge AI inference tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00603v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yubo Wang, Yubo Cui, Tuo Shi, Danyang Li, Wenxin Li, Lide Suo, Tao Wang, Xin Xie</dc:creator>
    </item>
    <item>
      <title>Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting</title>
      <link>https://arxiv.org/abs/2511.00651</link>
      <description>arXiv:2511.00651v1 Announce Type: cross 
Abstract: Telecom networks are rapidly growing in scale and complexity, making effective management, operation, and optimization increasingly challenging. Although Artificial Intelligence (AI) has been applied to many telecom tasks, existing models are often narrow in scope, require large amounts of labeled data, and struggle to generalize across heterogeneous deployments. Consequently, network troubleshooting continues to rely heavily on Subject Matter Experts (SMEs) to manually correlate various data sources to identify root causes and corrective actions. To address these limitations, we propose a Multi-Agent System (MAS) that employs an agentic workflow, with Large Language Models (LLMs) coordinating multiple specialized tools for fully automated network troubleshooting. Once faults are detected by AI/ML-based monitors, the framework dynamically activates agents such as an orchestrator, solution planner, executor, data retriever, and root-cause analyzer to diagnose issues and recommend remediation strategies within a short time frame. A key component of this system is the solution planner, which generates appropriate remediation plans based on internal documentation. To enable this, we fine-tuned a Small Language Model (SLM) on proprietary troubleshooting documents to produce domain-grounded solution plans. Experimental results demonstrate that the proposed framework significantly accelerates troubleshooting automation across both Radio Access Network (RAN) and Core network domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00651v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.IT</category>
      <category>cs.MA</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenhua Shi, Bhavika Jalli, Gregor Macdonald, John Zou, Wanlu Lei, Mridul Jain, Joji Philip</dc:creator>
    </item>
    <item>
      <title>HyRES: A Hybrid Replication and Erasure Coding Approach to Data Storage</title>
      <link>https://arxiv.org/abs/2511.00896</link>
      <description>arXiv:2511.00896v1 Announce Type: cross 
Abstract: Reliability in distributed storage systems has typically focused on the design and deployment of data replication or erasure coding techniques. Although some scenarios have considered the use of replication for hot data and erasure coding for cold data in the same system, each is designed in isolation. We propose HyRES, a hybrid scheme incorporates the best characteristics of each scheme, thus, resulting in additional design flexibility and better potential performance for the system. We show that HyRES generalizes previously proposed hybrid schemes. We characterize the theoretical performance of HyRES as well as that of replication and erasure coding considering the effects of the size of the storage networks. We validate our theoretical results using simulations. These results show that HyRES can yield simultaneously lower storage costs than replication, lower probabilities of file loss than replication and erasure coding with similar worst case performance, and even lower effective repair traffic than replication when considering the network size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00896v1</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel E. Lucani, Marcell Feh\'er</dc:creator>
    </item>
    <item>
      <title>Experimental Demonstration of Software-Orchestrated Quantum Network Applications over a Campus-Scale Testbed</title>
      <link>https://arxiv.org/abs/2511.01247</link>
      <description>arXiv:2511.01247v1 Announce Type: cross 
Abstract: To fulfill their promise, quantum networks must transform from isolated testbeds into scalable infrastructures for distributed quantum applications. In this paper, we present a prototype orchestrator for the Argonne Quantum Network (ArQNet) testbed that leverages design principles of software-defined networking (SDN) to automate typical quantum communication experiments across buildings in the Argonne campus connected over deployed, telecom fiber. Our implementation validates a scalable architecture supporting service-level abstraction of quantum networking tasks, distributed time synchronization, and entanglement verification across remote nodes. We present a prototype service of continuous, stable entanglement distribution between remote sites that ran for 12 hours, which defines a promising path towards scalable quantum networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01247v1</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md. Shariful Islam (Argonne National Laboratory), Joaquin Chung (Argonne National Laboratory), Ely Marcus Eastman (Argonne National Laboratory, Northwestern University), Robert J. Hayek (Argonne National Laboratory), Prem Kumar (Northwestern University), Rajkumar Kettimuthu (Argonne National Laboratory)</dc:creator>
    </item>
    <item>
      <title>DeepSpecs: Expert-Level Questions Answering in 5G</title>
      <link>https://arxiv.org/abs/2511.01305</link>
      <description>arXiv:2511.01305v1 Announce Type: cross 
Abstract: 5G technology enables mobile Internet access for billions of users. Answering expert-level questions about 5G specifications requires navigating thousands of pages of cross-referenced standards that evolve across releases. Existing retrieval-augmented generation (RAG) frameworks, including telecom-specific approaches, rely on semantic similarity and cannot reliably resolve cross-references or reason about specification evolution. We present DeepSpecs, a RAG system enhanced by structural and temporal reasoning via three metadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB (line-level version diffs), and TDocDB (standardization meeting documents). DeepSpecs explicitly resolves cross-references by recursively retrieving referenced clauses through metadata lookup, and traces specification evolution by mining changes and linking them to Change Requests that document design rationale. We curate two 5G QA datasets: 573 expert-annotated real-world questions from practitioner forums and educational resources, and 350 evolution-focused questions derived from approved Change Requests. Across multiple LLM backends, DeepSpecs outperforms base models and state-of-the-art telecom RAG systems; ablations confirm that explicit cross-reference resolution and evolution-aware retrieval substantially improve answer quality, underscoring the value of modeling the structural and temporal properties of 5G standards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01305v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aman Ganapathy Manvattira, Yifei Xu, Ziyue Dang, Songwu Lu</dc:creator>
    </item>
    <item>
      <title>Beyond Static Thresholds: Adaptive RRC Signaling Storm Detection with Extreme Value Theory</title>
      <link>https://arxiv.org/abs/2511.01391</link>
      <description>arXiv:2511.01391v1 Announce Type: cross 
Abstract: In 5G and beyond networks, the radio communication between a User Equipment (UE) and a base station (gNodeB or gNB), also known as the air interface, is a critical component of network access and connectivity. During the connection establishment procedure, the Radio Resource Control (RRC) layer can be vulnerable to signaling storms, which threaten the availability of the radio access control plane. These attacks may occur when one or more UEs send a large number of connection requests to the gNB, preventing new UEs from establishing connections. In this paper, we investigate the detection of such threats and propose an adaptive threshold-based detection system based on Extreme Value Theory (EVT). The proposed solution is evaluated numerically by applying simulated attack scenarios based on a realistic threat model on top of real-world RRC traffic data from an operator network. We show that, by leveraging features from the RRC layer only, the detection system can not only identify the attacks but also differentiate them from legitimate high-traffic situations. The adaptive threshold calculated using EVT ensures that the system can work under diverse traffic conditions. The results show high accuracy, precision, and recall values (above 93%), and a low detection latency even under complex conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01391v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dang Kien Nguyen, Rim El Malki, Filippo Rebecchi, Raymond Knopp, Melek \"Onen</dc:creator>
    </item>
    <item>
      <title>Towards Efficient Federated Learning of Networked Mixture-of-Experts for Mobile Edge Computing</title>
      <link>https://arxiv.org/abs/2511.01743</link>
      <description>arXiv:2511.01743v1 Announce Type: cross 
Abstract: Recent advancements in large artificial intelligence models (LAMs) are driving significant innovations in mobile edge computing within next-generation wireless networks. However, the substantial demands for computational resources and large-scale training data required to train LAMs conflict with the limited storage and computational capacity of edge devices, posing significant challenges to training and deploying LAMs at the edge. In this work, we introduce the Networked Mixture-of-Experts (NMoE) system, in which clients infer collaboratively by distributing tasks to suitable neighbors based on their expertise and aggregate the returned results. For training the NMoE, we propose a federated learning framework that integrates both supervised and self-supervised learning to balance personalization and generalization, while preserving communication efficiency and data privacy. We conduct extensive experiments to demonstrate the efficacy of the proposed NMoE system, providing insights and benchmarks for the NMoE training algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01743v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Song Gao, Shusen Jing, Shuai Zhang, Yue Wang, Xiangwei Zhou, Songyang Zhang</dc:creator>
    </item>
    <item>
      <title>Waiting for QUIC: Passive Measurements to Understand QUIC Deployments</title>
      <link>https://arxiv.org/abs/2209.00965</link>
      <description>arXiv:2209.00965v2 Announce Type: replace 
Abstract: QUIC experiences a rapid adoption since its standardization in 2021, and hypergiants configure their infrastructure to optimize for QUIC performance. In this paper, we introduce a passive measurement method to study both the progressive rollout and individual hypergiant configurations during the last five years. By analyzing backscatter traffic of the UCSD network telescope, we are able to make the following observations. First, Meta, Google, and Cloudflare configure significantly different maximal retransmission numbers and timeouts. Second, we can identify different off-net deployments of hypergiants, using packet features, such as QUIC connection IDs, packet coalescence, and packet lengths. Third, we observe changing hypergiant deployment configurations during our different measurement periods. Fourth, connection IDs can allow further insights into load balancer deployments, such as the number of servers. We bolster our results using two orthogonal measurements: passive recording of QUIC flows and active probing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.00965v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3768988</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the ACM on Networking (PACMNET) 3, CoNEXT4, Article 41 (December 2025)</arxiv:journal_reference>
      <dc:creator>Jonas M\"ucke, Marcin Nawrocki, Raphael Hiesgen, Patrick Sattler, Johannes Zirngibl, Georg Carle, Jan Luxemburk, Thomas C. Schmidt, Matthias W\"ahlisch</dc:creator>
    </item>
    <item>
      <title>Hybrid-Task Meta-Learning: A GNN Approach for Scalable and Transferable Bandwidth Allocation</title>
      <link>https://arxiv.org/abs/2401.10253</link>
      <description>arXiv:2401.10253v3 Announce Type: replace 
Abstract: In this paper, we develop a deep learning-based bandwidth allocation policy that is: 1) scalable with the number of users and 2) transferable to different communication scenarios, such as non-stationary wireless channels, different quality-of-service (QoS) requirements, and dynamically available resources. To support scalability, the bandwidth allocation policy is represented by a graph neural network (GNN), with which the number of training parameters does not change with the number of users. To enable the generalization of the GNN, we develop a hybrid-task meta-learning (HML) algorithm that trains the initial parameters of the GNN with different communication scenarios during meta-training. Next, during meta-testing, a few samples are used to fine-tune the GNN with unseen communication scenarios. Simulation results demonstrate that our HML approach can improve the initial performance by 8.79%, and sample efficiency by 73%, compared with existing benchmarks. After fine-tuning, our near-optimal GNN-based policy can achieve close to the same reward with much lower inference complexity compared to the optimal policy obtained using iterative optimization. Numerical results validate that our HML can reduce the computation time by approximately 200 to 2000 times than the optimal iterative algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10253v3</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Hao, Changyang She, Phee Lep Yeoh, Yuhong Liu, Branka Vucetic, Yonghui Li</dc:creator>
    </item>
    <item>
      <title>Task-Oriented Multimodal Token Transmission in Resource-Constrained Multiuser Networks</title>
      <link>https://arxiv.org/abs/2505.07841</link>
      <description>arXiv:2505.07841v3 Announce Type: replace 
Abstract: With the emergence of large model-based agents, widely adopted transformer-based architectures inevitably produce excessively long token embeddings for transmission, which may result in high bandwidth overhead, increased power consumption and latency. In this letter, we propose a task-oriented multimodal token transmission scheme for efficient multimodal information fusion and utilization. To improve the efficiency of token transmission, we design a two-stage training algotithm, including cross-modal alignment and task-oriented fine-tuning, for large model-based token communication. Meanwhile, token compression is performed using a sliding window pooling operation to save communication resources. To balance the trade-off between latency and model performance caused by compression, we formulate a weighted-sum optimization problem over latency and validation loss. We jointly optimizes bandwidth, power allocation, and token length across users by using an alternating optimization method. Simulation results demonstrate that the proposed algorithm outperforms the baseline under different bandwidth and power budgets. Moreover, the two-stage training algorithm achieves higher accuracy across various signal-to-noise ratios than the method without cross-modal alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07841v3</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/LWC.2025.3628928</arxiv:DOI>
      <dc:creator>Junhe Zhang, Wanli Ni, Pengwei Wang, Dongyu Wang</dc:creator>
    </item>
    <item>
      <title>Video Streaming Over QUIC: A Comprehensive Study</title>
      <link>https://arxiv.org/abs/2505.21769</link>
      <description>arXiv:2505.21769v2 Announce Type: replace 
Abstract: The QUIC transport protocol represents a significant evolution in web transport technologies, offering improved performance and reduced latency compared to traditional protocols like TCP. Given the growing number of QUIC implementations, understanding their performance, particularly in video streaming contexts, is essential. This paper presents a comprehensive analysis of various QUIC implementations, focusing on their transport-layer congestion control (CC) performance and its impact on HTTP Adaptive Streaming (HAS) in single-server, multi-client environments. Through extensive trace-driven experiments, we explore how different QUIC CCs impact adaptive bitrate (ABR) algorithms in two video streaming scenarios: video-on- demand (VoD) and low-latency live streaming (LLL). Our study aims to shed light on the impact of QUIC CC implementations, queuing strategies, and cooperative versus competitive dynamics of QUIC streams on user QoE under diverse network conditions. Our results demonstrate that identical CC algorithms across different QUIC implementations can lead to significant performance variations, directly impacting the QoE of video streaming sessions. These findings offer valuable insights into the effectiveness of various QUIC implementations and their implications for optimizing QoE, underscoring the need for intelligent cross-layer designs that integrate QUIC CC and ABR schemes to enhance overall streaming performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21769v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jashanjot Singh Sidhu, Abdelhak Bentaleb</dc:creator>
    </item>
    <item>
      <title>Distributionally Robust Wireless Semantic Communication with Large AI Models</title>
      <link>https://arxiv.org/abs/2506.03167</link>
      <description>arXiv:2506.03167v2 Announce Type: replace 
Abstract: Semantic communication (SemCom) has emerged as a promising paradigm for 6G wireless systems by transmitting task-relevant information rather than raw bits, yet existing approaches remain vulnerable to dual sources of uncertainty: semantic misinterpretation arising from imperfect feature extraction and transmission-level perturbations from channel noise. Current deep learning based SemCom systems typically employ domain-specific architectures that lack robustness guarantees and fail to generalize across diverse noise conditions, adversarial attacks, and out-of-distribution data. In this paper, a novel and generalized semantic communication framework called WaSeCom is proposed to systematically address uncertainty and enhance robustness. In particular, Wasserstein distributionally robust optimization is employed to provide resilience against semantic misinterpretation and channel perturbations. A rigorous theoretical analysis is performed to establish the robust generalization guarantees of the proposed framework. Experimental results on image and text transmission demonstrate that WaSeCom achieves improved robustness under noise and adversarial perturbations. These results highlight its effectiveness in preserving semantic fidelity across varying wireless conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03167v2</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Long Tan Le, Senura Hansaja Wanasekara, Zerun Niu, Nguyen H. Tran, Phuong Vo, Walid Saad, Dusit Niyato, Zhu Han, Choong Seon Hong, H. Vincent Poor</dc:creator>
    </item>
    <item>
      <title>Automatic Network Planning with Digital Radio Twin</title>
      <link>https://arxiv.org/abs/2509.12441</link>
      <description>arXiv:2509.12441v2 Announce Type: replace 
Abstract: Network planning seeks to determine base station parameters that maximize coverage and capacity in cellular networks. However, achieving optimal planning remains challenging due to the diversity of deployment scenarios and the significant simulation-to-reality discrepancy. In this paper, we propose \emph{AutoPlan}, a new automatic network planning framework by leveraging digital radio twin (DRT) techniques. We derive the DRT by finetuning the parameters of building materials to reduce the sim-to-real discrepancy based on crowdsource real-world user data. Leveraging the DRT, we design a Bayesian optimization based algorithm to optimize the deployment parameters of base stations efficiently. Using the field measurement from Husker-Net, we extensively evaluate \emph{AutoPlan} under various deployment scenarios, in terms of both coverage and capacity. The evaluation results show that \emph{AutoPlan} flexibly adapts to different scenarios and achieves performance comparable to exhaustive search, while requiring less than 2\% of its computation time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12441v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaomeng Li, Yuru Zhang, Qiang Liu, Mehmet Can Vuran, Nathan Huynh, Li Zhao, Mizan Rahman, Eren Erman Ozguven</dc:creator>
    </item>
    <item>
      <title>Rethinking HTTP API Rate Limiting: A Client-Side Approach</title>
      <link>https://arxiv.org/abs/2510.04516</link>
      <description>arXiv:2510.04516v3 Announce Type: replace 
Abstract: HTTP underpins modern Internet services, and providers enforce quotas to regulate HTTP API traffic for scalability and reliability. When requests exceed quotas, clients are throttled and must retry. Server-side enforcement protects the service. However, when independent clients' usage counts toward a shared quota, server-only controls are inefficient; clients lack visibility into others' load, causing their retry attempts to potentially fail. Indeed, retry timing is important since each attempt incurs costs and yields no benefit unless admitted. While centralized coordination could address this, practical limitations have led to widespread adoption of simple client-side strategies like exponential backoff. As we show, these simple strategies cause excessive retries and significant costs. We design adaptive client-side mechanisms requiring no central control, relying only on minimal feedback. We present two algorithms: ATB, an offline method deployable via service workers, and AATB, which enhances retry behavior using aggregated telemetry data. Both algorithms infer system congestion to schedule retries. Through emulations with real-world traces and synthetic datasets with up to 100 clients, we demonstrate that our algorithms reduce HTTP 429 errors by up to 97.3% compared to exponential backoff, while the modest increase in completion time is outweighed by the reduction in errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04516v3</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Behrooz Farkiani, Fan Liu, Patrick Crowley</dc:creator>
    </item>
    <item>
      <title>Beyond Lamport, Towards Probabilistic Fair Ordering</title>
      <link>https://arxiv.org/abs/2510.13664</link>
      <description>arXiv:2510.13664v3 Announce Type: replace 
Abstract: A growing class of applications demands \emph{fair ordering} of events, which ensures that events generated earlier are processed before later events. However, achieving such sequencing is challenging due to the inherent errors in clock synchronization: two events at two clients generated close together may have timestamps that cannot be compared confidently. We advocate for an approach that embraces, rather than eliminates, clock synchronization errors. Instead of attempting to remove the error from a timestamp, \systemname{}, our proposed system, leverages a statistical model to compare two noisy timestamps probabilistically by learning per-clock synchronization error distributions. Our preliminary statistical model computes the probability that one event precedes another by only relying on local clocks of clients. This serves as a foundation for a new relation: \emph{likely-happened-before} denoted by $\xrightarrow{p}$ where $p$ represents the probability that an event happened before another. The $\xrightarrow{p}$ relation provides a basis for ordering multiple events which are otherwise considered \emph{concurrent} by Lamport's \emph{happened-before} ($\rightarrow$) relation. We highlight various related challenges including the intransitivity of the $\xrightarrow{p}$ relation as opposed to the transitive $\rightarrow$ relation. We outline several research directions: online fair sequencing, stochastically fair total ordering, and handling byzantine clients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13664v3</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Haseeb, Jinkun Geng, Radhika Mittal, Aurojit Panda, Srinivas Narayana, Anirudh Sivaraman</dc:creator>
    </item>
    <item>
      <title>Passive Indoor Localization with WiFi Fingerprints</title>
      <link>https://arxiv.org/abs/2111.14281</link>
      <description>arXiv:2111.14281v2 Announce Type: replace-cross 
Abstract: This paper proposes passive WiFi indoor localization. Instead of using WiFi signals received by mobile devices as fingerprints, we use signals received by routers to locate the mobile carrier. Consequently, software installation on the mobile device is not required. To resolve the data insufficiency problem, flow control signals such as request to send (RTS) and clear to send (CTS) are utilized. In our model, received signal strength indicator (RSSI) and channel state information (CSI) are used as fingerprints for several algorithms, including deterministic, probabilistic and neural networks localization algorithms. We further investigated localization algorithms performance through extensive on-site experiments with various models of phones at hundreds of testing locations. We demonstrate that our passive scheme achieves an average localization error of 0.8 m when the phone is actively transmitting data frames and 1.5 m when it is not transmitting data frames.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.14281v2</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Minh Tu Hoang, Brosnan Yuen, Kai Ren, Ahmed Elmoogy, Xiaodai Dong, Tao Lu, Hung Le Nguyen Robert Westendorp, Kishore Reddy Tarimala</dc:creator>
    </item>
    <item>
      <title>OrbitChain: Orchestrating In-orbit Real-time Analytics of Earth Observation Data</title>
      <link>https://arxiv.org/abs/2508.13374</link>
      <description>arXiv:2508.13374v2 Announce Type: replace-cross 
Abstract: Earth observation analytics have the potential to serve many time-sensitive applications. However, due to limited bandwidth and duration of ground-satellite connections, it takes hours or even days to download and analyze data from existing Earth observation satellites, making real-time demands like timely disaster response impossible. Toward real-time analytics, we introduce OrbitChain, a collaborative analytics framework that orchestrates computational resources across multiple satellites in an Earth observation constellation. OrbitChain decomposes analytics applications into microservices and allocates computational resources for time-constrained analysis. A traffic routing algorithm is devised to minimize the inter-satellite communication overhead. OrbitChain adopts a pipeline workflow that completes Earth observation tasks in real-time, facilitates time-sensitive applications and inter-constellation collaborations such as tip-and-cue. To evaluate OrbitChain, we implement a hardware-in-the-loop orbital computing testbed. Experiments show that our system can complete up to 60% analytics workload than existing Earth observation analytics framework while reducing the communication overhead by up to 72%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13374v2</guid>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhouyu Li, Zhijin Yang, Huayue Gu, Xiaojian Wang, Yuchen Liu, Ruozhou Yu</dc:creator>
    </item>
  </channel>
</rss>
