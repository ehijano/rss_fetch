<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Apr 2025 04:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Tracy-Widom, Gaussian, and Bootstrap: Approximations for Leading Eigenvalues in High-Dimensional PCA</title>
      <link>https://arxiv.org/abs/2503.23097</link>
      <description>arXiv:2503.23097v1 Announce Type: new 
Abstract: Under certain conditions, the largest eigenvalue of a sample covariance matrix undergoes a well-known phase transition when the sample size $n$ and data dimension $p$ diverge proportionally. In the subcritical regime, this eigenvalue has fluctuations of order $n^{-2/3}$ that can be approximated by a Tracy-Widom distribution, while in the supercritical regime, it has fluctuations of order $n^{-1/2}$ that can be approximated with a Gaussian distribution. However, the statistical problem of determining which regime underlies a given dataset is far from resolved. We develop a new testing framework and procedure to address this problem. In particular, we demonstrate that the procedure has an asymptotically controlled level, and that it is power consistent for certain alternatives. Also, this testing procedure enables the design a new bootstrap method for approximating the distributions of functionals of the leading sample eigenvalues within the subcritical regime -- which is the first such method that is supported by theoretical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23097v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nina D\"ornemann, Miles E. Lopes</dc:creator>
    </item>
    <item>
      <title>On Finite Time Span Estimators of Parameters for Ornstein-Uhlenbeck Processes</title>
      <link>https://arxiv.org/abs/2503.23677</link>
      <description>arXiv:2503.23677v1 Announce Type: new 
Abstract: We study the bias and the mean-squared error of the maximum likelihood estimators (MLE) of parameters associated with a two-parameter mean-reverting process for a finite time $T$. Using the likelihood ratio process, we derive the expressions for MLEs, then compute the bias and the MSE via the change of measure and Ito's formula. We apply the derived expressions to the general Ornstein-Uhlenbeck process, where the bias and the MSE are numerically computed through a joint moment-generating function of key functionals of the O-U process. A numerical study is provided to illustrate the behaviour of bias and the MSE for the MLE of the mean-reverting speed parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23677v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jun S. Han, Nino Kordzakhia</dc:creator>
    </item>
    <item>
      <title>Finite sample valid confidence sets of mode</title>
      <link>https://arxiv.org/abs/2503.23711</link>
      <description>arXiv:2503.23711v1 Announce Type: new 
Abstract: Estimating the mode of a unimodal distribution is a classical problem in statistics. Although there are several approaches for point-estimation of mode in the literature, very little has been explored about the interval-estimation of mode. Our work proposes a collection of novel methods of obtaining finite sample valid confidence set of the mode of a unimodal distribution. We analyze the behaviour of the width of the proposed confidence sets under some regularity assumptions of the density about the mode and show that the width of these confidence sets shrink to zero near optimally. Simply put, we show that it is possible to build finite sample valid confidence sets for the mode that shrink to a singleton as sample size increases. We support the theoretical results by showing the performance of the proposed methods on some synthetic data-sets. We believe that our confidence sets can be improved both in construction and in terms of rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23711v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manit Paul, Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>Distributional regression with reject option</title>
      <link>https://arxiv.org/abs/2503.23782</link>
      <description>arXiv:2503.23782v1 Announce Type: new 
Abstract: Selective prediction, where a model has the option to abstain from making a decision, is crucial for machine learning applications in which mistakes are costly. In this work, we focus on distributional regression and introduce a framework that enables the model to abstain from estimation in situations of high uncertainty. We refer to this approach as distributional regression with reject option, inspired by similar concepts in classification and regression with reject option. We study the scenario where the rejection rate is fixed. We derive a closed-form expression for the optimal rule, which relies on thresholding the entropy function of the Continuous Ranked Probability Score (CRPS). We propose a semi-supervised estimation procedure for the optimal rule, using two datasets: the first, labeled, is used to estimate both the conditional distribution function and the entropy function of the CRPS, while the second, unlabeled, is employed to calibrate the desired rejection rate. Notably, the control of the rejection rate is distribution-free. Under mild conditions, we show that our procedure is asymptotically as effective as the optimal rule, both in terms of error rate and rejection rate. Additionally, we establish rates of convergence for our approach based on distributional k-nearest neighbor. A numerical analysis on real-world datasets demonstrates the strong performance of our procedure</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23782v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmed Zaoui (UMLP, LMB), Cl\'ement Dombry (UMLP, LMB)</dc:creator>
    </item>
    <item>
      <title>Multivariate Species Sampling Models</title>
      <link>https://arxiv.org/abs/2503.24004</link>
      <description>arXiv:2503.24004v1 Announce Type: new 
Abstract: Species sampling processes have long served as the framework for studying random discrete distributions. However, their statistical applicability is limited when partial exchangeability is assumed as probabilistic invariance for the observables. Despite numerous discrete models for partially exchangeable observations, a unifying framework is currently missing, leaving many questions about the induced learning mechanisms unanswered in this setting. To fill this gap, we consider the natural extension of species sampling models to a multivariate framework, obtaining a general class of models characterized by their partially exchangeable partition probability function. A notable subclass, named regular multivariate species sampling models, exists among these models. In the subclass, dependence across processes is accurately captured by the correlation among them: a correlation of one equals full exchangeability and a null correlation corresponds to independence. Regular multivariate species sampling models encompass discrete processes for partial exchangeable data used in Bayesian models, thereby highlighting their core distributional properties and providing a means for developing new models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24004v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Beatrice Franzolini, Antonio Lijoi, Igor Pr\"unster, Giovanni Rebaudo</dc:creator>
    </item>
    <item>
      <title>Optimal low-rank approximations for linear Gaussian inverse problems on Hilbert spaces, Part I: posterior covariance approximation</title>
      <link>https://arxiv.org/abs/2503.24020</link>
      <description>arXiv:2503.24020v1 Announce Type: new 
Abstract: For linear inverse problems with Gaussian priors and Gaussian observation noise, the posterior is Gaussian, with mean and covariance determined by the conditioning formula. Using the Feldman-Hajek theorem, we analyse the prior-to-posterior update and its low-rank approximation for infinite-dimensional Hilbert parameter spaces and finite-dimensional observations. We show that the posterior distribution differs from the prior on a finite-dimensional subspace, and construct low-rank approximations to the posterior covariance, while keeping the mean fixed. Since in infinite dimensions, not all low-rank covariance approximations yield approximate posterior distributions which are equivalent to the posterior and prior distribution, we characterise the low-rank covariance approximations which do yield this equivalence, and their respective inverses, or `precisions'. For such approximations, a family of measure approximation problems is solved by identifying the low-rank approximations which are optimal for various losses simultaneously. These loss functions include the family of R\'enyi divergences, the Amari $\alpha$-divergences for $\alpha\in(0,1)$, the Hellinger metric and the Kullback-Leibler divergence. Our results extend those of Spantini et al. (SIAM J. Sci. Comput. 2015) to Hilbertian parameter spaces, and provide theoretical underpinning for the construction of low-rank approximations of discretised versions of the infinite-dimensional inverse problem, by formulating discretization independent results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24020v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Carere, Han Cheng Lie</dc:creator>
    </item>
    <item>
      <title>Wasserstein KL-divergence for Gaussian distributions</title>
      <link>https://arxiv.org/abs/2503.24022</link>
      <description>arXiv:2503.24022v1 Announce Type: new 
Abstract: We introduce a new version of the KL-divergence for Gaussian distributions which is based on Wasserstein geometry and referred to as WKL-divergence. We show that this version is consistent with the geometry of the sample space ${\Bbb R}^n$. In particular, we can evaluate the WKL-divergence of the Dirac measures concentrated in two points which turns out to be proportional to the squared distance between these points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24022v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adwait Datar, Nihat Ay</dc:creator>
    </item>
    <item>
      <title>Smooth and rough paths in mean derivative estimation for functional data</title>
      <link>https://arxiv.org/abs/2503.24066</link>
      <description>arXiv:2503.24066v1 Announce Type: new 
Abstract: In this paper, in a multivariate setting we derive near optimal rates of convergence in the minimax sense for estimating partial derivatives of the mean function for functional data observed under a fixed synchronous design over H\"older smoothness classes. We focus on the supremum norm since it corresponds to the visualisation of the estimation error, and is closely related to the construction of uniform confidence bands. In contrast to mean function estimation, for derivative estimation the smoothness of the paths of the processes is crucial for the rates of convergence. On the one hand, if the paths have higher-order smoothness than the order of the partial derivative to be estimated, the parametric $\sqrt n$ rate can be achieved under sufficiently dense design. On the other hand, for processes with rough paths of lower-order smoothness, we show that the rates of convergence are necessarily slower than the parametric rate, and determine a near-optimal rate at which estimation is still possible. We implement a multivariate local polynomial derivative estimator and illustrate its finite-sample performance in a simulation as well as for two real-data sets. To assess the smoothness of the sample paths in the applications we further discuss a method based on comparing restricted estimates of the partial derivatives of the covariance kernel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24066v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Berger, Hajo Holzmann</dc:creator>
    </item>
    <item>
      <title>Asymptotically distribution-free goodness-of-fit testing for point processes</title>
      <link>https://arxiv.org/abs/2503.24197</link>
      <description>arXiv:2503.24197v1 Announce Type: new 
Abstract: Consider an observation of a multivariate temporal point process $N$ with law $\mathcal P$ on the time interval $[0,T]$. To test the null hypothesis that $\mathcal P$ belongs to a given parametric family, we construct a convergent compensated counting process to which we apply an innovation martingale transformation. We prove that the resulting process converges weakly to a standard Wiener process. Consequently, taking a suitable functional of this process yields an asymptotically distribution-free goodness-of-fit test for point processes. For several standard tests based on the increments of this transformed process, we establish consistency under alternative hypotheses. Finally, we assess the performance of the proposed testing procedure through a Monte Carlo simulation study and illustrate its practical utility with two real-data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24197v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Justin Baars, Sami Umut Can, Roger J. A. Laeven</dc:creator>
    </item>
    <item>
      <title>Optimal low-rank approximations for linear Gaussian inverse problems on Hilbert spaces, Part II: posterior mean approximation</title>
      <link>https://arxiv.org/abs/2503.24209</link>
      <description>arXiv:2503.24209v1 Announce Type: new 
Abstract: In this work, we construct optimal low-rank approximations for the Gaussian posterior distribution in linear Gaussian inverse problems. The parameter space is a separable Hilbert space of possibly infinite dimension, and the data space is assumed to be finite-dimensional. We consider various types of approximation families for the posterior. We first consider approximate posteriors in which the means vary among a class of either structure-preserving or structure-ignoring low-rank transformations of the data, and in which the posterior covariance is kept fixed. We give necessary and sufficient conditions for these approximating posteriors to be equivalent to the exact posterior, for all possible realisations of the data simultaneously. For such approximations, we measure approximation error with the Kullback-Leibler, R\'enyi and Amari $\alpha$-divergences for $\alpha\in(0,1)$, and with the Hellinger distance, all averaged over the data distribution. With these losses, we find the optimal approximations and formulate an equivalent condition for their uniqueness, extending the work in finite dimensions of Spantini et al. (SIAM J. Sci. Comput. 2015). We then consider joint approximation of the mean and covariance, by also varying the posterior covariance over the low-rank updates considered in Part I of this work. For the reverse Kullback-Leibler divergence, we show that the separate optimal approximations of the mean and of the covariance can be combined to yield an optimal joint approximation of the mean and covariance. In addition, we interpret the joint approximation with the optimal structure-ignoring approximate mean in terms of an optimal projector in parameter space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24209v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Carere, Han Cheng Lie</dc:creator>
    </item>
    <item>
      <title>Optimal Change Point Detection and Inference in the Spectral Density of General Time Series Models</title>
      <link>https://arxiv.org/abs/2503.23211</link>
      <description>arXiv:2503.23211v1 Announce Type: cross 
Abstract: This paper addresses the problem of detecting change points in the spectral density of time series, motivated by EEG analysis of seizure patients. Seizures disrupt coherence and functional connectivity, necessitating precise detection. Departing from traditional parametric approaches, we utilize the Wold decomposition, representing general time series as autoregressive processes with infinite lags, which are truncated and estimated around the change point. Our detection procedure employs an initial estimator that systematically searches across time points. We examine the localization error and its dependence on time series properties and sample size. To enhance accuracy, we introduce an optimal rate method with an asymptotic distribution, facilitating the construction of confidence intervals. The proposed method effectively identifies seizure onset in EEG data and extends to event detection in video data. Comprehensive numerical experiments demonstrate its superior performance compared to existing techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23211v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sepideh Mosaferi, Abolfazl Safikhani, Peiliang Bai</dc:creator>
    </item>
    <item>
      <title>Modeling Maximum drawdown Records with Piecewise Deterministic Markov Processe in Capital Markets</title>
      <link>https://arxiv.org/abs/2503.23221</link>
      <description>arXiv:2503.23221v1 Announce Type: cross 
Abstract: We propose to model the records of the maximum Drawdown in capital markets by means a Piecewise Deterministic Markov Process (PDMP). We derive statistical results such as the mean and variance that describes the sequence of maximum Drawdown records. In addition, we developed a simulation study and techniques for estimating the parameters governing the stochastic process, using a practical example in the capital market to illustrate the procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23221v1</guid>
      <category>q-fin.RM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rolando Rubilar-Torrealba, Lisandro Fermin, Soledad Torres</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for High-dimensional Time Series with a Directed Acyclic Graphical Structure</title>
      <link>https://arxiv.org/abs/2503.23563</link>
      <description>arXiv:2503.23563v1 Announce Type: cross 
Abstract: In multivariate time series analysis, understanding the underlying causal relationships among variables is often of interest for various applications. Directed acyclic graphs (DAGs) provide a powerful framework for representing causal dependencies. This paper proposes a novel Bayesian approach for modeling multivariate time series where conditional independencies and causal structure are encoded by a DAG. The proposed model allows structural properties such as stationarity to be easily accommodated. Given the application, we further extend the model for matrix-variate time series. We take a Bayesian approach to inference, and a ``projection-posterior'' based efficient computational algorithm is developed. The posterior convergence properties of the proposed method are established along with two identifiability results for the unrestricted structural equation models. The utility of the proposed method is demonstrated through simulation studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23563v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arkaprava Roy, Anindya Roy, Subhashis Ghosal</dc:creator>
    </item>
    <item>
      <title>Operator limit of Wigner matrices I</title>
      <link>https://arxiv.org/abs/2503.23940</link>
      <description>arXiv:2503.23940v1 Announce Type: cross 
Abstract: We consider the Wigner matrix $W_{n}$ of dimension $n \times n$ as $n \to \infty$. The objective of this paper is two folds: first we construct an operator $\mathcal{W}$ on a suitable Hilbert space $\mathcal{H}$ and then define a suitable notion of convergence such that the matrices $W_{n}$ converge in that notion of convergence to $\mathcal{W}$. We further investigate some properties of $\mathcal{W}$ and $\mathcal{H}$. We show that $\mathcal{H}$ is a nontrivial extension of $L^{2}[0,1]$ with respect to the Lebesgue measure and the spectral measure of $\mathcal{W}$ at any function $f \in L^{2}[0,1]$ is almost surely the semicircular law.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23940v1</guid>
      <category>math.PR</category>
      <category>math-ph</category>
      <category>math.FA</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Debapratim Banerjee</dc:creator>
    </item>
    <item>
      <title>Estimating a graph's spectrum via random Kirchhoff forests</title>
      <link>https://arxiv.org/abs/2503.24236</link>
      <description>arXiv:2503.24236v1 Announce Type: cross 
Abstract: Exact eigendecomposition of large matrices is very expensive, and it is practically impossible to compute exact eigenvalues. Instead, one may set a more modest goal of approaching the empirical distribution of the eigenvalues, recovering the overall shape of the eigenspectrum. Current approaches to spectral estimation typically work with \emph{moments} of the spectral distribution. These moments are first estimated using Monte Carlo trace estimators, then the estimates are combined to approximate the spectral density. In this article we show how \emph{Kirchhoff forests}, which are random forests on graphs, can be used to estimate certain non-linear moments of very large graph Laplacians. We show how to combine these moments into an estimate of the spectral density. If the estimate's desired precision isn't too high, our approach paves the way to the estimation of a graph's spectrum in time sublinear in the number of links.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24236v1</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Barthelm\'e, Fabienne Castell, Alexandre Gaudilli\`ere, Clothilde Melot, Matteo Quattropani, Nicolas Tremblay</dc:creator>
    </item>
    <item>
      <title>Phase-type frailty models: A flexible approach to modeling unobserved heterogeneity in survival analysis</title>
      <link>https://arxiv.org/abs/2103.13142</link>
      <description>arXiv:2103.13142v2 Announce Type: replace 
Abstract: Frailty models are essential tools in survival analysis for addressing unobserved heterogeneity and random effects in the data. These models incorporate a random effect, the frailty, which is assumed to impact the hazard rate multiplicatively. In this paper, we introduce a novel class of frailty models in both univariate and multivariate settings, using phase-type distributions as the underlying frailty specification. We investigate the properties of these phase-type frailty models and develop expectation-maximization algorithms for their maximum-likelihood estimation. In particular, we show that the resulting model shares similarities with the Gamma frailty model, has closed-form expressions for its functionals, and can approximate any other frailty model. Through a series of simulated and real-life numerical examples, we demonstrate the effectiveness and versatility of the proposed models in addressing unobserved heterogeneity in survival analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2103.13142v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jorge Yslas</dc:creator>
    </item>
    <item>
      <title>Asymptotic theory for Bayesian inference and prediction: from the ordinary to a conditional Peaks-Over-Threshold method</title>
      <link>https://arxiv.org/abs/2310.06720</link>
      <description>arXiv:2310.06720v2 Announce Type: replace 
Abstract: The Peaks Over Threshold (POT) method is the most popular statistical method for the analysis of univariate extremes. Even though there is a rich applied literature on Bayesian inference for the POT, the asymptotic theory for such proposals is missing. Even more importantly, the ambitious and challenging problem of predicting future extreme events according to a proper predictive statistical approach has received no attention to date. In this paper we fill this gap by developing the asymptotic theory of posterior distributions (consistency, contraction rates, asymptotic normality and asymptotic coverage of credible intervals) and prediction within the Bayesian framework in the POT context. We extend this asymptotic theory to account for cases where the focus is on the tail properties of the conditional distribution of a response variable given a vector of random covariates. To enable accurate predictions of extreme events more severe than those previously observed, we derive the posterior predictive distribution as an estimator of the conditional distribution of an out-of-sample random variable, given that it exceeds a sufficiently high threshold. We establish Wasserstein consistency of the posterior predictive distribution under both the unconditional and covariate-conditional approaches and derive its contraction rates. Simulations show the good performances of the proposed Bayesian inferential methods. The analysis of the change in the frequency of financial crises over time shows the utility of our methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06720v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cl\'ement Dombry, Simone A. Padoan, Stefano Rizzelli</dc:creator>
    </item>
    <item>
      <title>Unsupervised domain adaptation under hidden confounding</title>
      <link>https://arxiv.org/abs/2402.15502</link>
      <description>arXiv:2402.15502v3 Announce Type: replace 
Abstract: We introduce a new predictive mechanism that operates in the presence of hidden confounding across distributionally diverse data sources while ensuring consistent estimation of causal parameters-despite their recognized suboptimality for prediction in the literature. Our method is based on a novel estimand that captures the dependence structure between response noise and covariates, incorporating causal parameters into a generative model that adaptively replicates the conditional distribution of the test environment. Identifiability is achieved under a straightforward, empirically verifiable assumption. Our approach ensures probabilistic alignment with test distributions uniformly across arbitrary interventions, enabling valid predictions without requiring worst-case optimization or assumptions about the strength of perturbations at test time. Through extensive simulations, we demonstrate that our method outperforms state-of-the-art invariance-based and domain adaptation approaches. Additionally, we validate its practical applicability and superior target risk performance on a cardiovascular disease dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15502v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Garc\'ia Meixide, David R\'ios Insua</dc:creator>
    </item>
    <item>
      <title>On universal inference in Gaussian mixture models</title>
      <link>https://arxiv.org/abs/2407.19361</link>
      <description>arXiv:2407.19361v2 Announce Type: replace 
Abstract: A recent line of work provides new statistical tools based on game-theory and achieves safe anytime-valid inference without assuming regularity conditions. In particular, the framework of universal inference proposed by Wasserman, Ramdas and Balakrishnan [78] offers new solutions to testing problems by modifying the likelihood ratio test in a data-splitting scheme. In this paper, we study the performance of the resulting split likelihood ratio test under Gaussian mixture models, which are canonical examples for models in which classical regularity conditions fail to hold. We establish that under the null hypothesis, the split likelihood ratio statistic is asymptotically normal with increasing mean and variance. Contradicting the usual belief that the flexibility of universal inference comes at the price of a significant loss of power, we prove that universal inference surprisingly achieves the same detection rate $(n^{-1}\log\log n)^{1/2}$ as the classical likelihood ratio test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19361v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Shi, Mathias Drton</dc:creator>
    </item>
    <item>
      <title>Optimal low-rank approximations for linear Gaussian inverse problems on Hilbert spaces, Part I: posterior covariance approximation</title>
      <link>https://arxiv.org/abs/2411.01112</link>
      <description>arXiv:2411.01112v2 Announce Type: replace 
Abstract: For linear inverse problems with Gaussian priors and Gaussian observation noise, the posterior is Gaussian, with mean and covariance determined by the conditioning formula. Using the Feldman-Hajek theorem, we analyse the prior-to-posterior update and its low-rank approximation for infinite-dimensional Hilbert parameter spaces and finite-dimensional observations. We show that the posterior distribution differs from the prior on a finite-dimensional subspace, and construct low-rank approximations to the posterior covariance, while keeping the mean fixed. Since in infinite dimensions, not all low-rank covariance approximations yield approximate posterior distributions which are equivalent to the posterior and prior distribution, we characterise the low-rank covariance approximations which do yield this equivalence, and their respective inverses, or `precisions'. For such approximations, a family of measure approximation problems is solved by identifying the low-rank approximations which are optimal for various losses simultaneously. These loss functions include the family of R\'enyi divergences, the Amari $\alpha$-divergences for $\alpha\in(0,1)$, the Hellinger metric and the Kullback-Leibler divergence. Our results extend those of Spantini et al. (SIAM J. Sci. Comput. 2015) to Hilbertian parameter spaces, and provide theoretical underpinning for the construction of low-rank approximations of discretised versions of the infinite-dimensional inverse problem, by formulating discretisation independent results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01112v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Carere, Han Cheng Lie</dc:creator>
    </item>
    <item>
      <title>Theoretical Foundations of Conformal Prediction</title>
      <link>https://arxiv.org/abs/2411.11824</link>
      <description>arXiv:2411.11824v2 Announce Type: replace 
Abstract: This book is about conformal prediction and related inferential techniques that build on permutation tests and exchangeability. These techniques are useful in a diverse array of tasks, including hypothesis testing and providing uncertainty quantification guarantees for machine learning systems. Much of the current interest in conformal prediction is due to its ability to integrate into complex machine learning workflows, solving the problem of forming prediction sets without any assumptions on the form of the data generating distribution. Since contemporary machine learning algorithms have generally proven difficult to analyze directly, conformal prediction's main appeal is its ability to provide formal, finite-sample guarantees when paired with such methods.
  The goal of this book is to teach the reader about the fundamental technical arguments that arise when researching conformal prediction and related questions in distribution-free inference. Many of these proof strategies, especially the more recent ones, are scattered among research papers, making it difficult for researchers to understand where to look, which results are important, and how exactly the proofs work. We hope to bridge this gap by curating what we believe to be some of the most important results in the literature and presenting their proofs in a unified language, with illustrations, and with an eye towards pedagogy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11824v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasios N. Angelopoulos, Rina Foygel Barber, Stephen Bates</dc:creator>
    </item>
    <item>
      <title>Diffusion on the circle and a stochastic correlation model</title>
      <link>https://arxiv.org/abs/2412.06343</link>
      <description>arXiv:2412.06343v3 Announce Type: replace 
Abstract: We propose analytically tractable SDE models for correlation in financial markets. We study diffusions on the circle, namely the Brownian motion on the circle and the von Mises process, and consider these as models for correlation. The von Mises process was proposed in Kent (1975) as a probabilistic justification for the von Mises distribution which is widely used in Circular statistics. The transition density of the von Mises process has been unknown, we identify an approximate analytic transition density for the von Mises process. We discuss the estimation of these diffusion models and a stochastic correlation model in finance. We illustrate the application of the proposed model on real-data of equity-currency pairs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06343v3</guid>
      <category>math.ST</category>
      <category>q-fin.MF</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sourav Majumdar, Arnab Kumar Laha</dc:creator>
    </item>
    <item>
      <title>Highest Posterior Density Intervals As Analogues to Profile Likelihood Ratio Confidence Intervals for Modes of Unimodal Distributions</title>
      <link>https://arxiv.org/abs/2412.06528</link>
      <description>arXiv:2412.06528v3 Announce Type: replace 
Abstract: In Bayesian statistics, the highest posterior density (HPD) interval is often used to describe properties of a posterior distribution. As a method for estimating confidence intervals (CIs), the HPD has two main desirable properties. Firstly, it is the shortest interval to have a specified coverage probability. Secondly, every point inside the HPD interval has a density greater than every point outside the interval. However, it is sometimes criticized for being transformation invariant.
  We make the case that the HPD interval is a natural analog to the frequentist profile likelihood ratio confidence interval (LRCI). First we provide background on the HPD interval as well as the Likelihood Ratio Test statistic and its inversion to generate asymptotically-correct CIs. Our main result is to show that the HPD interval has similar desirable properties as the profile LRCI, such as transformation invariance with respect to the mode for monotonic functions. We then discuss an application of the main result, an example case which compares the profile LRCI for the binomial probability parameter p with the Bayesian HPD interval for the beta distribution density function, both of which are used to estimate population proportions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06528v3</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. X. Venu</dc:creator>
    </item>
    <item>
      <title>To Study Properties of a Known Procedure in Adaptive Sequential Sampling Design</title>
      <link>https://arxiv.org/abs/2412.17791</link>
      <description>arXiv:2412.17791v2 Announce Type: replace 
Abstract: We consider the procedure proposed by Bhandari et al. (2009) in the context of two-treatment clinical trials, with the objective of minimizing the applications of the less effective drug to the least number of patients. Our focus is on an adaptive sequential procedure that is both simple and intuitive. Our findings show that the number of applications of the less effective drug is a finite random variable whose all moments are also finite. In contrast, Bhandari et al. (2009) observed that this number increases logarithmically with the total sample size. We attribute this discrepancy to differences in their choice of starting sample size and the method of analysis employed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17791v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sampurna Kundu, Jayant Jha, Subir Kumar Bhandari</dc:creator>
    </item>
    <item>
      <title>Improving variable selection properties by leveraging external data</title>
      <link>https://arxiv.org/abs/2502.15584</link>
      <description>arXiv:2502.15584v2 Announce Type: replace 
Abstract: Sparse high-dimensional signal recovery is only possible under certain conditions on the number of parameters, sample size, signal strength and underlying sparsity. We show that leveraging external information, as possible with data integration or transfer learning, allows to push these mathematical limits. Specifically, we consider external information that allows splitting parameters into blocks, first in a simplified case, the Gaussian sequence model, and then in the general linear regression setting. We show how external information dependent, block-based, $\ell_0$ penalties attain model selection consistency under milder conditions than standard $\ell_0$ penalties, and they also attain faster model recovery rates. We first provide results for oracle-based $\ell_0$ penalties that have access to perfect sparsity and signal strength information. Subsequently, we propose an empirical Bayes data analysis method that does not require oracle information and for which efficient computation is possible via standard MCMC techniques. Our results provide a mathematical basis to justify the use of data integration methods in high-dimensional structural learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15584v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Rognon-Vael, David Rossell, Piotr Zwiernik</dc:creator>
    </item>
    <item>
      <title>Geometric Ergodicity of a Gibbs Algorithm for a Normal Model With a Horseshoe Prior</title>
      <link>https://arxiv.org/abs/2503.00538</link>
      <description>arXiv:2503.00538v2 Announce Type: replace 
Abstract: In this paper, we consider a two-stage Gibbs sampler for a normal linear regression model with a horseshoe prior. Under some assumptions, we show that it produces a geometrically ergodic Markov chain. In particular, we prove geometric ergodicity under some three-parameter beta global prior which does not have a finite $(p / 5)$-th negative moment, where $p$ is the number of regression coefficients. This is in contrast to the case of a known general result which is applicable if the global parameter has a finite approximately $(p / 2)$-th negative moment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00538v2</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasuyuki Hamura</dc:creator>
    </item>
    <item>
      <title>Adaptive monotonicity testing in sublinear time</title>
      <link>https://arxiv.org/abs/2503.03020</link>
      <description>arXiv:2503.03020v2 Announce Type: replace 
Abstract: Modern large-scale data analysis increasingly faces the challenge of achieving computational efficiency as well as statistical accuracy, as classical statistically efficient methods often fall short in the first regard. In the context of testing monotonicity of a regression function, we propose FOMT (Fast and Optimal Monotonicity Test), a novel methodology tailored to meet these dual demands. FOMT employs a sparse collection of local tests, strategically generated at random, to detect violations of monotonicity scattered throughout the domain of the regression function. This sparsity enables significant computational efficiency, achieving sublinear runtime in most cases, and quasilinear runtime (i.e., linear up to a log factor) in the worst case. In contrast, existing statistically optimal tests typically require at least quadratic runtime. FOMT's statistical accuracy is achieved through the precise calibration of these local tests and their effective combination, ensuring both sensitivity to violations and control over false positives. More precisely, we show that FOMT separates the null and alternative hypotheses at minimax optimal rates over H\"older function classes of smoothness order in $(0,2]$. Further, when the smoothness is unknown, we introduce an adaptive version of FOMT, based on a modified Lepskii principle, which attains statistical optimality and meanwhile maintains the same computational complexity as if the intrinsic smoothness were known. Extensive simulations confirm the competitiveness and effectiveness of both FOMT and its adaptive variant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03020v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Housen Li, Zhi Liu, Axel Munk</dc:creator>
    </item>
    <item>
      <title>General reproducing properties in RKHS with application to derivative and integral operators</title>
      <link>https://arxiv.org/abs/2503.15922</link>
      <description>arXiv:2503.15922v2 Announce Type: replace 
Abstract: In this paper, we consider the reproducing property in Reproducing Kernel Hilbert Spaces (RKHS). We establish a reproducing property for the closure of the class of combinations of composition operators under minimal conditions. This allows to revisit the sufficient conditions for the reproducing property to hold for the derivative operator, as well as for the existence of the mean embedding function. These results provide a framework of application of the representer theorem for regularized learning algorithms that involve data for function values, gradients, or any other operator from the considered class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15922v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fatima-Zahrae El-Boukkouri (INSA Toulouse, IMT), Josselin Garnier (CMAP, ASCII), Olivier Roustant (INSA Toulouse, IMT, RT-UQ)</dc:creator>
    </item>
    <item>
      <title>Graphical Negative Multinomial and Multinomial Models with Dirichlet-type priors</title>
      <link>https://arxiv.org/abs/2301.06058</link>
      <description>arXiv:2301.06058v5 Announce Type: replace-cross 
Abstract: Bayesian statistical graphical models are typically classified as either continuous and parametric (Gaussian, parameterized by the graph-dependent precision matrix with Wishart-type priors) or discrete and non-parametric (with graph-dependent structure of probabilities of cells and Dirichlet-type priors). We propose to break this dichotomy by introducing two discrete parametric graphical models on finite decomposable graphs: the graph negative multinomial and the graph multinomial distributions (the former related to the Cartier-Foata theorem for the graph genereted free quotient monoid). These models interpolate between the product of univariate negative binomial laws and the negative multinomial distribution, and between the product of binomial laws and the multinomial distribution, respectively. We derive their Markov decompositions and provide related probabilistic representations.
  We also introduce graphical versions of the Dirichlet and inverted Dirichlet distributions, which serve as conjugate priors for the two discrete graphical Markov models. We derive explicit normalizing constants for both graphical Dirichlet laws and establish their independence structure (a graphical version of neutrality), which yields a strong hyper Markov property for both Bayesian models. We also provide characterization theorems for graphical Dirichlet laws via respective graphical versions of neutrality, which extends previously known results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.06058v5</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iza Danielewska, Bartosz Ko{\l}odziejek, Jacek Weso{\l}owski, Xiaolin Zeng</dc:creator>
    </item>
    <item>
      <title>Incorporating Auxiliary Variables to Improve the Efficiency of Time-Varying Treatment Effect Estimation</title>
      <link>https://arxiv.org/abs/2306.17260</link>
      <description>arXiv:2306.17260v3 Announce Type: replace-cross 
Abstract: Contextual sensing and delivery of digital interventions to improve health outcomes have gained significant traction in behavioral and psychiatric studies. Micro-randomized trials (MRTs) are a common experimental design for obtaining data-driven evidence on the effectiveness of digital interventions where each individual is repeatedly randomized to receive treatments over numerous time points. Throughout the study, individual characteristics and contextual factors around randomization are collected, with some prespecified as moderators for assessing time-varying causal effect moderation. However, many additional measurements beyond these moderators often go underutilized. Some of these may influence treatment randomization or known to strongly moderate the treatment effect. Incorporating such auxiliary information into the estimation procedure can reduce chance imbalances and improve asymptotic estimation efficiency. In this work, we propose a method to adjust for auxiliary variables in consistently estimating time-varying intervention effects. The approach can also be extended to include post-treatment auxiliary variables when evaluating lagged treatment effects. Under specific conditions, local efficiency gains are guaranteed. We demonstrate the method's utility through simulation studies and an analysis of data from the Intern Health Study (NeCamp et al., 2020).</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.17260v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jieru Shi, Zhenke Wu, Walter Dempsey</dc:creator>
    </item>
    <item>
      <title>Identification by non-Gaussianity in structural threshold and smooth transition vector autoregressive models</title>
      <link>https://arxiv.org/abs/2404.19707</link>
      <description>arXiv:2404.19707v4 Announce Type: replace-cross 
Abstract: Linear structural vector autoregressive models can be identified statistically without imposing restrictions on the model if the shocks are mutually independent and at most one of them is Gaussian. We show that this result extends to structural threshold and smooth transition vector autoregressive models incorporating a time-varying impact matrix defined as a weighted sum of the impact matrices of the regimes. We also discuss the problem of labelling the shocks, estimation of the parameters, and stationarity the model. The introduced methods are implemented to the accompanying R package sstvars. Our empirical application studies the effects of the climate policy uncertainty shock on the U.S. macroeconomy. In a structural logistic smooth transition vector autoregressive model with two regimes, we find that a positive climate policy uncertainty shock decreases production and increases inflation in times of both low and high economic policy uncertainty, but its inflationary effects are stronger in the periods of high economic policy uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19707v4</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Savi Virolainen</dc:creator>
    </item>
    <item>
      <title>Is Algorithmic Stability Testable? A Unified Framework under Computational Constraints</title>
      <link>https://arxiv.org/abs/2405.15107</link>
      <description>arXiv:2405.15107v2 Announce Type: replace-cross 
Abstract: Algorithmic stability is a central notion in learning theory that quantifies the sensitivity of an algorithm to small changes in the training data. If a learning algorithm satisfies certain stability properties, this leads to many important downstream implications, such as generalization, robustness, and reliable predictive inference. Verifying that stability holds for a particular algorithm is therefore an important and practical question. However, recent results establish that testing the stability of a black-box algorithm is impossible, given limited data from an unknown distribution, in settings where the data lies in an uncountably infinite space (such as real-valued data). In this work, we extend this question to examine a far broader range of settings, where the data may lie in any space -- for example, categorical data. We develop a unified framework for quantifying the hardness of testing algorithmic stability, which establishes that across all settings, if the available data is limited then exhaustive search is essentially the only universally valid mechanism for certifying algorithmic stability. Since in practice, any test of stability would naturally be subject to computational constraints, exhaustive search is impossible and so this implies fundamental limits on our ability to test the stability property for a black-box algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15107v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuetian Luo, Rina Foygel Barber</dc:creator>
    </item>
    <item>
      <title>Coupled Input-Output Dimension Reduction: Application to Goal-oriented Bayesian Experimental Design and Global Sensitivity Analysis</title>
      <link>https://arxiv.org/abs/2406.13425</link>
      <description>arXiv:2406.13425v2 Announce Type: replace-cross 
Abstract: We introduce a new method to jointly reduce the dimension of the input and output space of a function between high-dimensional spaces. Choosing a reduced input subspace influences which output subspace is relevant and vice versa. Conventional methods focus on reducing either the input or output space, even though both are often reduced simultaneously in practice. Our coupled approach naturally supports goal-oriented dimension reduction, where either an input or output quantity of interest is prescribed. We consider, in particular, goal-oriented sensor placement and goal-oriented sensitivity analysis, which can be viewed as dimension reduction where the most important output or, respectively, input components are chosen. Both applications present difficult combinatorial optimization problems with expensive objectives such as the expected information gain and Sobol' indices. By optimizing gradient-based bounds, we can determine the most informative sensors and most influential parameters as the largest diagonal entries of some diagnostic matrices, thus bypassing the combinatorial optimization and objective evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13425v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiao Chen, Elise Arnaud, Ricardo Baptista, Olivier Zahm</dc:creator>
    </item>
    <item>
      <title>Uncertainty quantification for intervals</title>
      <link>https://arxiv.org/abs/2408.16381</link>
      <description>arXiv:2408.16381v2 Announce Type: replace-cross 
Abstract: Data following an interval structure are increasingly prevalent in many scientific applications. In medicine, clinical events are often monitored between two clinical visits, making the exact time of the event unknown and generating outcomes with a range format. As interest in automating healthcare decisions grows, uncertainty quantification via predictive regions becomes essential for developing reliable and trustworthy predictive algorithms. However, the statistical literature currently lacks a general methodology for interval targets, especially when these outcomes are incomplete due to censoring. We propose an uncertainty quantification algorithm for interval responses and establish its theoretical properties using empirical process arguments based on a newly developed class of functions specifically designed for these interval data structures. Although this paper primarily focuses on deriving predictive regions for interval-censored data, the approach can also be applied to other statistical modeling tasks, such as goodness-of-fit assessments. Finally, the applicability of the method is demonstrated through simulations, showing up to a 60\% improvement in conditional coverage. Our new algorithm is also applied to various biomedical contexts, including two clinical examples: i) sleep duration and its association with cardiovascular diseases, and ii) survival time in relation to physical activity levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16381v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Carlos Garc\'ia Meixide, Michael R. Kosorok, Marcos Matabuena</dc:creator>
    </item>
    <item>
      <title>Relational Graph in Vector Autoregression: A Case Study on the Effect of the Great Recession on Connectivity of Economic Indicators</title>
      <link>https://arxiv.org/abs/2410.22617</link>
      <description>arXiv:2410.22617v2 Announce Type: replace-cross 
Abstract: Under a high-dimensional vector autoregressive (VAR) model, we propose a way of efficiently estimating both the stationary graph structure between the nodal time series and their temporal dynamics. The framework is then used to make inferences on the change in interdependencies between several economic indicators due to the impact of the Great Recession, the financial crisis that lasted from 2007 through 2009. There are several key advantages of the proposed framework; (1) it develops a reparametrized VAR likelihood that can be used in general high-dimensional VAR problems, (2) it strictly maintains causality of the estimated process, making inference on stationary features more meaningful and (3) it is computationally efficient due to the reduced rank structure of the parameterization. We apply the methodology to the seasonally adjusted quarterly economic indicators available in the FRED-QD database of the Federal Reserve. The analysis essentially confirms much of the prevailing knowledge about the impact of the Great Recession on different economic indicators. At the same time, it provides deeper insight into the nature and extent of the impact on the interplay of the different indicators. We also contribute to the theory of Bayesian VAR by showing the consistency of the posterior under sparse priors for the parameters of the reduced rank formulation of the VAR process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22617v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arkaprava Roy, Anindya Roy, Subhashis Ghosal</dc:creator>
    </item>
  </channel>
</rss>
