<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Dec 2025 02:12:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Deconvolution with parametric rate of convergence in unlinked linear models</title>
      <link>https://arxiv.org/abs/2512.19929</link>
      <description>arXiv:2512.19929v1 Announce Type: new 
Abstract: Unlinked regression, in which covariates and responses are observed separately without known correspondence, has recently gained increasing attention. Deconvolution, on the other hand, is a fundamental and challenging problem in nonparametric statistics with the aim of estimating the distribution of a latent random variable $Z$ based on observations contaminated by some additive noise. The complexity of this task is heavily influenced by the smoothness of the noise distribution and often leads to slow estimation rates. In this paper, we combine the recent unlinked linear regression problem with the classical deconvolution framework. Specifically, we study nonparametric deconvolution under the assumption that $Z$ is a linear function of an observable multidimensional covariate. This structural constraint allows us to introduce a nonparametric estimator of the distribution of $Z$ which achieves the parametric convergence rate in the Wasserstein distance of order 1, and this independently of the smoothness of the noise distribution. Furthermore, and conditionally on an observed response, we consider the problem of estimating the value of the latent linear predictor, whose link to the observed response is not accessible. Through several simulations, we illustrate the fast convergence rate of our deconvolution estimator and the performance of the proposed conditional estimators of the latent predictor in different simulation scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19929v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fadoua Balabdaoui, Antonio Di Noia, C\'ecile Durot</dc:creator>
    </item>
    <item>
      <title>CoLaS: Copula-Seeded Sparse Local Graphs with Tunable Assortativity, Persistent Clustering, and a Degree-Tail Dichotomy</title>
      <link>https://arxiv.org/abs/2512.20019</link>
      <description>arXiv:2512.20019v1 Announce Type: new 
Abstract: Empirical networks are typically sparse yet display pronounced degree variation, persistent transitivity, and systematic degree mixing. Most sparse generators control at most two of these features, and assortativity is often achieved by degree-preserving rewiring, which obscures the mechanism-parameter link. We introduce CoLaS (copula-seeded local latent-space graphs), a modular latent-variable model that separates marginal specifications from dependence. Each node has a popularity variable governing degree heterogeneity and a latent geometric location governing locality. A low-dimensional copula couples popularity and location, providing an interpretable dependence parameter that tunes degree mixing while leaving the chosen marginals unchanged. Under shrinking-range locality, edges are conditionally independent, the graph remains sparse, and clustering does not vanish. We develop sparse-limit theory for degrees, transitivity, and assortativity. Degrees converge to mixed-Poisson limits and we establish a degree-tail dichotomy: with fixed-range local kernels, degree tails are necessarily light, even under heavy-ailed popularity. To recover power-law degrees without sacrificing sparsity or locality, we propose CoLaS-HT, a minimal tail-inheriting extension in which effective connection ranges grow with popularity. Finally, under an identifiability condition, we provide a consistent one-graph calibration method based on jointly matching transitivity and assortativity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20019v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marios Papamichalis, Regina Ruane</dc:creator>
    </item>
    <item>
      <title>Optimal Anytime-Valid Tests for Composite Nulls</title>
      <link>https://arxiv.org/abs/2512.20039</link>
      <description>arXiv:2512.20039v1 Announce Type: new 
Abstract: We consider the problem of designing optimal level-$\alpha$ power-one tests for composite nulls. Given a parameter $\alpha \in (0,1)$ and a stream of $\mathcal{X}$-valued observations $\{X_n: n \geq 1\} \overset{i.i.d.}{\sim} P$, the goal is to design a level-$\alpha$ power-one test $\tau_\alpha$ for the null $H_0: P \in \mathcal{P}_0 \subset \mathcal{P}(\mathcal{X})$. Prior works have shown that any such $\tau_\alpha$ must satisfy $\mathbb{E}_P[\tau_\alpha] \geq \tfrac{\log(1/\alpha)}{\gamma^*(P, \mathcal{P}_0)}$, where $\gamma^*(P, \mathcal{P}_0)$ is the so-called $\mathrm{KL}_{\inf}$ or minimum divergence of $P$ to the null class. In this paper, our objective is to develop and analyze constructive schemes that match this lower bound as $\alpha \downarrow 0$.
  We first consider the finite-alphabet case~($|\mathcal{X}| = m &lt; \infty$), and show that a test based on \emph{universal} $e$-process~(formed by the ratio of a universal predictor and the running null MLE) is optimal in the above sense. The proof relies on a Donsker-Varadhan~(DV) based saddle-point representation of $\mathrm{KL}_{\inf}$, and an application of Sion's minimax theorem. This characterization motivates a general method for arbitrary $\mathcal{X}$: construct an $e$-process based on the empirical solutions to the saddle-point representation over a sufficiently rich class of test functions. We give sufficient conditions for the optimality of this test for compact convex nulls, and verify them for H\"older smooth density models. We end the paper with a discussion on the computational aspects of implementing our proposed tests in some practical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20039v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shubhanshu Shekhar</dc:creator>
    </item>
    <item>
      <title>Convergence analysis of data augmentation algorithms in Bayesian lasso models with log-concave likelihoods</title>
      <link>https://arxiv.org/abs/2512.20041</link>
      <description>arXiv:2512.20041v1 Announce Type: new 
Abstract: We study the convergence properties of a class of data augmentation algorithms targeting posterior distributions of Bayesian lasso models with log-concave likelihoods. Leveraging isoperimetric inequalities, we derive a generic convergence bound for this class of algorithms and apply it to Bayesian probit, logistic, and heteroskedastic Gaussian linear lasso models. Under feasible initializations, the mixing times for the probit and logistic models are of order $O[(p+n)^3 (pn^{1-c} + n)]$, up to logarithmic factors, where $n$ is the sample size, $p$ is the dimension of the regression coefficients, and $c \in [0,1]$ is determined by the lasso penalty parameter. The mixing time for the heteroskedastic Gaussian model is $O[n(n+p)^3 (p n^{1-c} + n)]$, up to logarithmic factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20041v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingkai Cui, Qian Qin</dc:creator>
    </item>
    <item>
      <title>Structure-Preserving Nonlinear Sufficient Dimension Reduction for Tensors</title>
      <link>https://arxiv.org/abs/2512.20057</link>
      <description>arXiv:2512.20057v1 Announce Type: new 
Abstract: We introduce two nonlinear sufficient dimension reduction methods for regressions with tensor-valued predictors. Our goal is two-fold: the first is to preserve the tensor structure when performing dimension reduction, particularly the meaning of the tensor modes, for improved interpretation; the second is to substantially reduce the number of parameters in dimension reduction, thereby achieving model parsimony and enhancing estimation accuracy. Our two tensor dimension reduction methods echo the two commonly used tensor decomposition mechanisms: one is the Tucker decomposition, which reduces a larger tensor to a smaller one; the other is the CP-decomposition, which represents an arbitrary tensor as a sequence of rank-one tensors. We developed the Fisher consistency of our methods at the population level and established their consistency and convergence rates. Both methods are easy to implement numerically: the Tucker-form can be implemented through a sequence of least-squares steps, and the CP-form can be implemented through a sequence of singular value decompositions. We investigated the finite-sample performance of our methods and showed substantial improvement in accuracy over existing methods in simulations and two data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20057v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dianjun Lin, Bing Li, Lingzhou Xue</dc:creator>
    </item>
    <item>
      <title>$L^2-$posterior contraction rates for Gaussian process and random series priors in Bayesian nonparametric regression models</title>
      <link>https://arxiv.org/abs/2512.20503</link>
      <description>arXiv:2512.20503v1 Announce Type: new 
Abstract: The nonparametric regression model with normal errors has been extensively studied, both from the frequentist and Bayesian viewpoint. A central result in Bayesian nonparametrics is that under assumptions on the prior, the data-generating distribution (assuming a true frequentist model) and a semi-metric $\rho(.,.)$ on the space of regression functions that satisfy the so called testing condition, the posterior contracts around the true distribution with respect to $\rho(.,.)$, and the rate of contraction can be estimated. In the regression setting, the semi-metric $\rho(.,.)$ is often taken to be the Hellinger distance or the empirical $L^2$ norm (i.e., the $L^2$ norm with respect to the empirical distribution of the design) in the present regression context. However, extending contraction rates to the ``integrated" $L^2$ norm usually requires more work, and has previously been done for instance under sufficient smoothness or boundedness assumptions, which may not necessarily hold. In this work we show that, for classes of priors based on random basis expansions or Gaussian processes with RKHS of Sobolev type and in the random design setting, such $L^2$ posterior contraction rates can be obtained under substantially weaker assumptions than those currently used in the literature. Importantly we do not require a known a priori upper bound on its supremum norm or that its smoothness is larger than $d/2$, where $d$ is the dimension of the covariates. Our proof crucially relies on an application of the matrix Bernstein concentration inequality to empirical inner product matrices, which require explicit upper bounds on the basis functions at hand that we prove in several cases of interest. In particular we obtain upper bounds on the supremum norm of Mercer eigenfunctions of several reproducing kernels (including several Mat\'ern kernels) which are of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20503v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Rosa</dc:creator>
    </item>
    <item>
      <title>Numerical Analysis of Test Optimality</title>
      <link>https://arxiv.org/abs/2512.19843</link>
      <description>arXiv:2512.19843v1 Announce Type: cross 
Abstract: In nonstandard testing environments, researchers often derive ad hoc tests with correct (asymptotic) size, but their optimality properties are typically unknown a priori and difficult to assess. This paper develops a numerical framework for determining whether an ad hoc test is effectively optimal - approximately maximizing a weighted average power criterion for some weights over the alternative and attaining a power envelope generated by a single weighted average power-maximizing test. Our approach uses nested optimization algorithms to approximate the weight function that makes an ad hoc test's weighted average power as close as possible to that of a true weighted average power-maximizing test, and we show the surprising result that the rejection probabilities corresponding to the latter form an approximate power envelope for the former. We provide convergence guarantees, discuss practical implementation and apply the method to the weak instrument-robust conditional likelihood ratio test and a recently-proposed test for when a nuisance parameter may be on or near its boundary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19843v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Ketz, Adam McCloskey, Jan Scherer</dc:creator>
    </item>
    <item>
      <title>Assumption-lean covariate adjustment under covariate adaptive randomization when $p = o (n)$</title>
      <link>https://arxiv.org/abs/2512.20046</link>
      <description>arXiv:2512.20046v1 Announce Type: cross 
Abstract: Adjusting for (baseline) covariates with working regression models becomes standard practice in the analysis of randomized clinical trials (RCT). When the dimension $p$ of the covariates is large relative to the sample size $n$, specifically $p = o (n)$, adjusting for covariates even in a linear working model by ordinary least squares can yield overly large bias, defeating the purpose of improving efficiency. This issue arises when no structural assumptions are imposed on the outcome model, a scenario that we refer to as the assumption-lean setting. Several new estimators have been proposed to address this issue. However, they focus mainly on simple randomization under the finite-population model, not covering covariate adaptive randomization (CAR) schemes under the superpopulation model. Due to improved covariate balance between treatment groups, CAR is more widely adopted in RCT; and the superpopulation model fits better when subjects are enrolled sequentially or when generalizing to a larger population is of interest. Thus, there is an urgent need to develop procedures in these settings, as the current regulatory guidance provides little concrete direction. In this paper, we fill this gap by demonstrating that an adjusted estimator based on second-order $U$-statistics can almost unbiasedly estimate the average treatment effect and enjoy a guaranteed efficiency gain if $p = o (n)$. In our analysis, we generalize the coupling technique commonly used in the CAR literature to $U$-statistics and also obtain several useful results for analyzing inverse sample Gram matrices by a delicate leave-$m$-out analysis, which may be of independent interest. Both synthetic and semi-synthetic experiments are conducted to demonstrate the superior finite-sample performance of our new estimator compared to popular benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20046v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujia Gu, Lin Liu, Wei Ma</dc:creator>
    </item>
    <item>
      <title>Change Point Detection and Mean-Field Dynamics of Variable Productivity Hawkes Processes</title>
      <link>https://arxiv.org/abs/2512.20068</link>
      <description>arXiv:2512.20068v1 Announce Type: cross 
Abstract: Many self-exciting systems change because endogenous amplification, as opposed to exogenous forcing, varies. We study a Hawkes process with fixed background rate and kernel, but piecewise time-varying productivity. For exponential kernels we derive closed-form mean-field relaxation after a change and a deterministic surrogate for post-change Fisher information, revealing a boundary layer in which change time information localises and saturates, while post-change level information grows linearly beyond a short transient. These results motivate a Bayesian change point procedure that stabilizes inference on finite windows. We illustrate the method on invasive pneumococcal disease incidence in The Gambia, identifying a decline in productivity aligned with pneumococcal conjugate vaccine rollout.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20068v1</guid>
      <category>stat.OT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Conor Kresin, Boris Baumer, Sophie Phillips</dc:creator>
    </item>
    <item>
      <title>Avoiding the Price of Adaptivity: Inference in Linear Contextual Bandits via Stability</title>
      <link>https://arxiv.org/abs/2512.20368</link>
      <description>arXiv:2512.20368v1 Announce Type: cross 
Abstract: Statistical inference in contextual bandits is complicated by the adaptive, non-i.i.d. nature of the data. A growing body of work has shown that classical least-squares inference may fail under adaptive sampling, and that constructing valid confidence intervals for linear functionals of the model parameter typically requires paying an unavoidable inflation of order $\sqrt{d \log T}$. This phenomenon -- often referred to as the price of adaptivity -- highlights the inherent difficulty of reliable inference under general contextual bandit policies.
  A key structural property that circumvents this limitation is the \emph{stability} condition of Lai and Wei, which requires the empirical feature covariance to concentrate around a deterministic limit. When stability holds, the ordinary least-squares estimator satisfies a central limit theorem, and classical Wald-type confidence intervals -- designed for i.i.d. data -- become asymptotically valid even under adaptation, \emph{without} incurring the $\sqrt{d \log T}$ price of adaptivity.
  In this paper, we propose and analyze a penalized EXP4 algorithm for linear contextual bandits. Our first main result shows that this procedure satisfies the Lai--Wei stability condition and therefore admits valid Wald-type confidence intervals for linear functionals. Our second result establishes that the same algorithm achieves regret guarantees that are minimax optimal up to logarithmic factors, demonstrating that stability and statistical efficiency can coexist within a single contextual bandit method. Finally, we complement our theory with simulations illustrating the empirical normality of the resulting estimators and the sharpness of the corresponding confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20368v1</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samya Praharaj, Koulik Khamaru</dc:creator>
    </item>
    <item>
      <title>ScoreMatchingRiesz: Auto-DML with Infinitesimal Classification</title>
      <link>https://arxiv.org/abs/2512.20523</link>
      <description>arXiv:2512.20523v1 Announce Type: cross 
Abstract: This study proposes Riesz representer estimation methods based on score matching. The Riesz representer is a key component in debiased machine learning for constructing $\sqrt{n}$-consistent and efficient estimators in causal inference and structural parameter estimation. To estimate the Riesz representer, direct approaches have garnered attention, such as Riesz regression and the covariate balancing propensity score. These approaches can also be interpreted as variants of direct density ratio estimation (DRE) in several applications such as average treatment effect estimation. In DRE, it is well known that flexible models can easily overfit the observed data due to the estimand and the form of the loss function. To address this issue, recent work has proposed modeling the density ratio as a product of multiple intermediate density ratios and estimating it using score-matching techniques, which are often used in the diffusion model literature. We extend score-matching-based DRE methods to Riesz representer estimation. Our proposed method not only mitigates overfitting but also provides insights for causal inference by bridging marginal effects and average policy effects through time score functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20523v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Estimating Graph Dimension with Cross-validated Eigenvalues</title>
      <link>https://arxiv.org/abs/2108.03336</link>
      <description>arXiv:2108.03336v2 Announce Type: replace-cross 
Abstract: In applied multivariate statistics, estimating the number of latent dimensions or the number of clusters, $k$, is a fundamental and recurring problem. We study a sequence of statistics called "cross-validated eigenvalues." Under a large class of random graph models, including both Poisson and Bernoulli edges, without parametric assumptions, we provide a $p$-value for each cross-validated eigenvalue. It tests the null hypothesis that the sample eigenvector is orthogonal to (i.e., uncorrelated with) the true latent dimensions. This approach naturally adapts to problems where some dimensions are not statistically detectable. In scenarios where all $k$ dimensions can be estimated, we show that our procedure consistently estimates $k$. In simulations and data example, the proposed estimator compares favorably to alternative approaches in both computational and statistical performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.03336v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fan Chen, Sebastien Roch, Karl Rohe, Shuqi Yu</dc:creator>
    </item>
    <item>
      <title>Robust Max Statistics for High-Dimensional Inference</title>
      <link>https://arxiv.org/abs/2409.16683</link>
      <description>arXiv:2409.16683v2 Announce Type: replace-cross 
Abstract: Although much progress has been made in the theory and application of bootstrap approximations for max statistics in high dimensions, the literature has largely been restricted to cases involving light-tailed data. To address this issue, we propose an approach to inference based on robust max statistics, and we show that their distributions can be accurately approximated via bootstrapping when the data are both high-dimensional and heavy-tailed. In particular, the data are assumed to satisfy an extended version of the well-established $L^{4}$-$L^2$ moment equivalence condition, as well as a weak variance decay condition. In this setting, we show that near-parametric rates of bootstrap approximation can be achieved in the Kolmogorov metric, independently of the data dimension. Moreover, this theoretical result is complemented by encouraging empirical results involving both Euclidean and functional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16683v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingshuo Liu, Miles E. Lopes</dc:creator>
    </item>
    <item>
      <title>Optimization-centric cutting feedback for semiparametric models</title>
      <link>https://arxiv.org/abs/2509.18708</link>
      <description>arXiv:2509.18708v2 Announce Type: replace-cross 
Abstract: Complex statistical models are often built by combining multiple submodels, called modules. Here, we consider modular inference where the modules contain both parametric and nonparametric components. In such cases, standard Bayesian inference can be highly sensitive to misspecification in any module, and common priors for the nonparametric components may compromise inference for the parametric components, and vice versa. We propose a novel ``optimization-centric'' approach to cutting feedback for semiparametric modular inference, which can address misspecification and prior-data conflicts. Proposed cut posteriors are defined via a variational optimization problem like other generalized posteriors, but regularization is based on R\'{e}nyi divergence, instead of Kullback-Leibler divergence (KLD). We show empirically that defining the cut posterior using R\'{e}nyi divergence delivers more robust inference than KLD, and R\'{e}nyi divergence reduces the tendency of uncertainty underestimation when the variational approximations impose strong parametric or independence assumptions. Novel posterior concentration results that accommodate the R\'{e}nyi divergence and allow for semiparametric components are derived, extending existing results for cut posteriors that only apply to KLD and parametric models. These new methods are demonstrated in a benchmark example and two real examples: Gaussian process adjustments for confounding in causal inference and misspecified copula models with nonparametric marginals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18708v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linda S. L. Tan, David J. Nott, David T. Frazier</dc:creator>
    </item>
    <item>
      <title>Iterated sampling importance resampling with adaptive number of proposals</title>
      <link>https://arxiv.org/abs/2512.00220</link>
      <description>arXiv:2512.00220v2 Announce Type: replace-cross 
Abstract: Iterated sampling importance resampling (i-SIR) is a Markov chain Monte Carlo (MCMC) algorithm which is based on $N$ independent proposals. As $N$ grows, its samples become nearly independent, but with an increased computational cost. We discuss a method which finds an approximately optimal number of proposals $N$ in terms of the asymptotic efficiency. The optimal $N$ depends on both the mixing properties of the i-SIR chain and the (parallel) computing costs. Our method for finding an appropriate $N$ is based on an approximate asymptotic variance of the i-SIR, which has similar properties as the i-SIR asymptotic variance, and a generalised i-SIR transition having fractional `number of proposals.' These lead to an adaptive i-SIR algorithm, which tunes the number of proposals automatically during sampling. Our experiments demonstrate that our approximate efficiency and the adaptive i-SIR algorithm have promising empirical behaviour. We also present new theoretical results regarding the i-SIR, such as the convexity of asymptotic variance in the number of proposals, which can be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00220v2</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pietari Laitinen, Matti Vihola</dc:creator>
    </item>
  </channel>
</rss>
