<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Mar 2025 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Prediction of Permissioned Blockchain Performance for Resource Scaling Configurations</title>
      <link>https://arxiv.org/abs/2503.15769</link>
      <description>arXiv:2503.15769v1 Announce Type: new 
Abstract: Blockchain is increasingly offered as blockchain-as-a-service (BaaS) by cloud service providers. However, configuring BaaS appropriately for optimal performance and reliability resorts to try-and-error. A key challenge is that BaaS is often perceived as a ``black-box,'' leading to uncertainties in performance and resource provisioning. Previous studies attempted to address this challenge; however, the impacts of both vertical and horizontal scaling remain elusive. To this end, we present machine learning-based models to predict network reliability and throughput based on scaling configurations. In our evaluation, the models exhibit prediction errors of ~1.9%, which is highly accurate and can be applied in the real-world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15769v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.icte.2024.09.003</arxiv:DOI>
      <arxiv:journal_reference>ICT Express, Volume 10, Issue 6, December 2024, Pages 1253-1258</arxiv:journal_reference>
      <dc:creator>Seungwoo Jung, Yeonho Yoo, Gyeongsik Yang, Chuck Yoo</dc:creator>
    </item>
    <item>
      <title>SPIN: Accelerating Large Language Model Inference with Heterogeneous Speculative Models</title>
      <link>https://arxiv.org/abs/2503.15921</link>
      <description>arXiv:2503.15921v1 Announce Type: new 
Abstract: Speculative decoding has been shown as an effective way to accelerate Large Language Model (LLM) inference by using a Small Speculative Model (SSM) to generate candidate tokens in a so-called speculation phase, which are subsequently verified by the LLM in a verification phase. However, current state-of-the-art speculative decoding approaches have three key limitations: handling requests with varying difficulty using homogeneous SSMs, lack of robust support for batch processing, and insufficient holistic optimization for both speculation and verification phases. In this paper, we introduce SPIN, an efficient LLM inference serving system based on speculative decoding, designed to address these challenges through three main innovations. First, SPIN improves token speculation by using multiple heterogeneous SSMs, with a learning-based algorithm for SSM selection that operates without prior knowledge of request difficulty. Second, SPIN employs a request decomposition method to minimize batching overhead during LLM verification. Finally, SPIN orchestrates speculation and verification phases by pipelining their executions on GPUs to achieve further acceleration. Experimental results demonstrate that SPIN significantly outperforms state-of-the-art methods, achieving a performance increase of approximately 2.28X.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15921v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fahao Chen, Peng Li, Tom H. Luan, Zhou Su, Jing Deng</dc:creator>
    </item>
    <item>
      <title>A Controllable and Realistic Framework for Evaluating Microservice Scheduling in Cloud-Edge Continuum</title>
      <link>https://arxiv.org/abs/2503.16029</link>
      <description>arXiv:2503.16029v1 Announce Type: new 
Abstract: The transition from traditional architectures to containerized microservices within the cloud-edge computing continuum introduces significant challenges, particularly in the efficient scheduling of microservices under dynamic conditions. Complex and fluctuating call-graph dependencies, varying cross-node communication latencies, and unpredictable bandwidth conditions substantially impact the performance and reliability of deployed microservices. Consequently, accurately evaluating scheduling policies in such dynamic environments remains essential yet challenging due to the lack of realistic and controllable evaluation frameworks.
  In this paper, we propose iDynamics, a novel evaluation framework designed explicitly to address these challenges. iDynamics provides comprehensive and controllable evaluation capabilities by emulating realistic dynamics, including configurable call-graph topologies, cross-node communication delays, and bandwidth variability. The framework is composed of modular components, such as the Graph Dynamics Analyzer, Networking Dynamics Manager, and Scheduling Policy Extender, enabling fine-grained environmental control and facilitating systematic comparisons of different scheduling strategies. Extensive experiments on a real cloud-edge testbed demonstrate that iDynamics effectively captures diverse dynamic scenarios encountered in microservice deployments, offering a robust solution for evaluating and optimizing policy performance under realistic and controllable conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16029v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming Chen, Muhammed Tawfiqul Islam, Maria Rodriguez Read, Rajkumar Buyya</dc:creator>
    </item>
    <item>
      <title>The Merit of Simple Policies: Buying Performance With Parallelism and System Architecture</title>
      <link>https://arxiv.org/abs/2503.16166</link>
      <description>arXiv:2503.16166v1 Announce Type: new 
Abstract: While scheduling and dispatching of computational workloads is a well-investigated subject, only recently has Google provided publicly a vast high-resolution measurement dataset of its cloud workloads. We revisit dispatching and scheduling algorithms fed by traffic workloads derived from those measurements. The main finding is that mean job response time attains a minimum as the number of servers of the computing cluster is varied, under the constraint that the overall computational budget is kept constant. Moreover, simple policies, such as Join Idle Queue, appear to attain the same performance as more complex, size-based policies for suitably high degrees of parallelism. Further, better performance, definitely outperforming size-based dispatching policies, is obtained by using multi-stage server clusters, even using very simple policies such as Round Robin. The takeaway is that parallelism and architecture of computing systems might be powerful knobs to control performance, even more than policies, under realistic workload traffic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16166v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mert Yildiz, Alexey Rolich, Andrea Baiocchi</dc:creator>
    </item>
    <item>
      <title>Dispersion is (Almost) Optimal under (A)synchrony</title>
      <link>https://arxiv.org/abs/2503.16216</link>
      <description>arXiv:2503.16216v1 Announce Type: new 
Abstract: The dispersion problem has received much attention recently in the distributed computing literature. In this problem, $k\leq n$ agents placed initially arbitrarily on the nodes of an $n$-node, $m$-edge anonymous graph of maximum degree $\Delta$ have to reposition autonomously to reach a configuration in which each agent is on a distinct node of the graph. Dispersion is interesting as well as important due to its connections to many fundamental coordination problems by mobile agents on graphs, such as exploration, scattering, load balancing, relocation of self-driven electric cars (robots) to recharge stations (nodes), etc. The objective has been to provide a solution that optimizes simultaneously time and memory complexities. There exist graphs for which the lower bound on time complexity is $\Omega(k)$. Memory complexity is $\Omega(\log k)$ per agent independent of graph topology. The state-of-the-art algorithms have (i) time complexity $O(k\log^2k)$ and memory complexity $O(\log(k+\Delta))$ under the synchronous setting [DISC'24] and (ii) time complexity $O(\min\{m,k\Delta\})$ and memory complexity $O(\log(k+\Delta))$ under the asynchronous setting [OPODIS'21]. In this paper, we improve substantially on this state-of-the-art. Under the synchronous setting as in [DISC'24], we present the first optimal $O(k)$ time algorithm keeping memory complexity $O(\log (k+\Delta))$. Under the asynchronous setting as in [OPODIS'21], we present the first algorithm with time complexity $O(k\log k)$ keeping memory complexity $O(\log (k+\Delta))$, which is time-optimal within an $O(\log k)$ factor despite asynchrony. Both results were obtained through novel techniques to quickly find empty nodes to settle agents, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16216v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>cs.MA</category>
      <category>cs.RO</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ajay D. Kshemkalyani, Manish Kumar, Anisur Rahaman Molla, Gokarna Sharma</dc:creator>
    </item>
    <item>
      <title>Omnichain Web: The Universal Framework for Streamlined Chain Abstraction and Cross-Layer Interaction</title>
      <link>https://arxiv.org/abs/2411.10132</link>
      <description>arXiv:2411.10132v2 Announce Type: cross 
Abstract: The Web3 ecosystem is highly fragmented, making seamless integration difficult for over a billion Web2 businesses, enterprises, and AI protocols. As blockchains, rollups, and app-specific chains expand, cross-chain interactions remain inefficient, and liquidity is deeply fragmented. AI systems lack standardized blockchain access, limiting autonomous functionality. Intent-based interactions, crucial for AI-driven automation, face scalability issues due to the absence of robust execution platforms. Meanwhile, the current solver ecosystem is centralized, as liquidity rebalancing remains a challenge due to a lack of developer-friendly tools. Dojima's Omnichain Web introduces a universal framework that abstracts blockchain complexity, bridging Web2, Web3, and AI. At its core, OmniRollups facilitate scalable execution across chains, while the Omni Sequencer ensures atomic, secure intent processing. Linera microchains enable AI-driven transaction automation, seamlessly integrating with Web3 data streams. Ragno Network decentralizes L1 infrastructure, optimizing cross-chain liquidity flows, while the Proof Network enhances cryptographic security for omnichain transactions. Finally, the Builder Marketplace introduces a solver-driven execution layer, allowing developers to build and monetize intent-based applications without liquidity constraints. By fostering a composable marketplace at the intersection of Web2 and Web3, Omnichain Web enables the seamless flow of data, value, and computation. This framework mirrors the internet, bridging Web3 decentralization with Web2 scale to drive the next wave of adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10132v2</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hardik Gajera, Akhil Reddy, Bhagath Reddy</dc:creator>
    </item>
    <item>
      <title>Capturing a Moving Target by Two Robots in the F2F Model</title>
      <link>https://arxiv.org/abs/2503.15688</link>
      <description>arXiv:2503.15688v1 Announce Type: cross 
Abstract: We study a search problem on capturing a moving target on an infinite real line. Two autonomous mobile robots (which can move with a maximum speed of 1) are initially placed at the origin, while an oblivious moving target is initially placed at a distance $d$ away from the origin. The robots can move along the line in any direction, but the target is oblivious, cannot change direction, and moves either away from or toward the origin at a constant speed $v$. Our aim is to design efficient algorithms for the two robots to capture the target. The target is captured only when both robots are co-located with it. The robots communicate with each other only face-to-face (F2F), meaning they can exchange information only when co-located, while the target remains oblivious and has no communication capabilities.
  We design algorithms under various knowledge scenarios, which take into account the prior knowledge the robots have about the starting distance $d$, the direction of movement (either toward or away from the origin), and the speed $v$ of the target. As a measure of the efficiency of the algorithms, we use the competitive ratio, which is the ratio of the capture time of an algorithm with limited knowledge to the capture time in the full-knowledge model. In our analysis, we are mindful of the cost of changing direction of movement, and show how to accomplish the capture of the target with at most three direction changes (turns).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15688v1</guid>
      <category>cs.RO</category>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Khaled Jawhar, Evangelos Kranakis</dc:creator>
    </item>
    <item>
      <title>ATTENTION2D: Communication Efficient Distributed Self-Attention Mechanism</title>
      <link>https://arxiv.org/abs/2503.15758</link>
      <description>arXiv:2503.15758v1 Announce Type: cross 
Abstract: Transformer-based models have emerged as a leading architecture for natural language processing, natural language generation, and image generation tasks. A fundamental element of the transformer architecture is self-attention, which allows the model to capture intricate dependencies within the data. However, the self-attention mechanism also incurs significant computational and memory costs, particularly for long sequences.
  In this paper, we introduce ATTENTION2D, a novel approach that exploits parallelism along two dimensions - query and key/value - of the self-attention operation. This method enables efficient distribution and parallelization of computations across multiple devices. Our approach facilitates asymptotically faster training and inference phases compared to previous methods, without relying on approximations or incurring additional computational or memory overheads. Furthermore, unlike existing techniques that struggle to scale with an increasing number of processing units, our approach effectively scales with additional processing units.
  Our experimental results confirm the effectiveness of our method in improving communication efficiency and scalability. Compared to Ring Attention, our approach demonstrated up to a 5x performance boost on a GPT-3-like model using 64 NVIDIA A100 GPUs across 16 nodes, and up to a 9.4x performance boost on 64 NVIDIA H100 GPUs across 64 nodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15758v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Venmugil Elango</dc:creator>
    </item>
    <item>
      <title>Empirical Analysis of Privacy-Fairness-Accuracy Trade-offs in Federated Learning: A Step Towards Responsible AI</title>
      <link>https://arxiv.org/abs/2503.16233</link>
      <description>arXiv:2503.16233v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables collaborative machine learning while preserving data privacy but struggles to balance privacy preservation (PP) and fairness. Techniques like Differential Privacy (DP), Homomorphic Encryption (HE), and Secure Multi-Party Computation (SMC) protect sensitive data but introduce trade-offs. DP enhances privacy but can disproportionately impact underrepresented groups, while HE and SMC mitigate fairness concerns at the cost of computational overhead. This work explores the privacy-fairness trade-offs in FL under IID (Independent and Identically Distributed) and non-IID data distributions, benchmarking q-FedAvg, q-MAML, and Ditto on diverse datasets. Our findings highlight context-dependent trade-offs and offer guidelines for designing FL systems that uphold responsible AI principles, ensuring fairness, privacy, and equitable real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16233v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dawood Wasif, Dian Chen, Sindhuja Madabushi, Nithin Alluru, Terrence J. Moore, Jin-Hee Cho</dc:creator>
    </item>
    <item>
      <title>RESFL: An Uncertainty-Aware Framework for Responsible Federated Learning by Balancing Privacy, Fairness and Utility in Autonomous Vehicles</title>
      <link>https://arxiv.org/abs/2503.16251</link>
      <description>arXiv:2503.16251v1 Announce Type: cross 
Abstract: Autonomous vehicles (AVs) increasingly rely on Federated Learning (FL) to enhance perception models while preserving privacy. However, existing FL frameworks struggle to balance privacy, fairness, and robustness, leading to performance disparities across demographic groups. Privacy-preserving techniques like differential privacy mitigate data leakage risks but worsen fairness by restricting access to sensitive attributes needed for bias correction. This work explores the trade-off between privacy and fairness in FL-based object detection for AVs and introduces RESFL, an integrated solution optimizing both. RESFL incorporates adversarial privacy disentanglement and uncertainty-guided fairness-aware aggregation. The adversarial component uses a gradient reversal layer to remove sensitive attributes, reducing privacy risks while maintaining fairness. The uncertainty-aware aggregation employs an evidential neural network to weight client updates adaptively, prioritizing contributions with lower fairness disparities and higher confidence. This ensures robust and equitable FL model updates. We evaluate RESFL on the FACET dataset and CARLA simulator, assessing accuracy, fairness, privacy resilience, and robustness under varying conditions. RESFL improves detection accuracy, reduces fairness disparities, and lowers privacy attack success rates while demonstrating superior robustness to adversarial conditions compared to other approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16251v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dawood Wasif, Terrence J. Moore, Jin-Hee Cho</dc:creator>
    </item>
    <item>
      <title>A parallel algorithm for the odd two-face shortest k-disjoint path problem</title>
      <link>https://arxiv.org/abs/2503.16336</link>
      <description>arXiv:2503.16336v1 Announce Type: cross 
Abstract: The shortest Disjoint Path problem (SDPP) requires us to find pairwise vertex disjoint paths between
  k designated pairs of terminal vertices such that the sum of the path lengths is minimum. The
  focus here is on SDPP restricted to planar graphs where all terminals are arbitrarily partitioned
  over two distinct faces with the additional restriction that each face is required to contain an odd
  number of terminals. We call this problem the Odd two-face planar SDPP. It is shown that this
  problem is solvable in randomized polynomial time and even in RNC. This is the first parallel (or
  even polynomial time) solution for the problem.
  Our algorithm combines ideas from the randomized solution for 2-SDPP by Bj\"orklund and
  Huslfeldt with its parallelization by Datta and Jaiswal along with the deterministic algorithm for
  One-face planar SDPP by Datta, Iyer, Kulkarni and Mukherjee.
  The proof uses a combination of two involutions to reduce a system of linear equations modulo a
  power of 2 to a system of triangular form that is, therefore, invertible. This, in turn, is proved by
  showing that the matrix of the equations, can be interpreted as (the adjacency matrix of) a directed
  acyclic graph (DAG). While our algorithm is primarily algebraic the proof remains combinatorial.
  We also give a parallel algorithm for the (A + B)-SDPP introduced by Hirai and Namba.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16336v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Srijan Chakraborty, Samir Datta</dc:creator>
    </item>
    <item>
      <title>Graph of Effort: Quantifying Risk of AI Usage for Vulnerability Assessment</title>
      <link>https://arxiv.org/abs/2503.16392</link>
      <description>arXiv:2503.16392v1 Announce Type: cross 
Abstract: With AI-based software becoming widely available, the risk of exploiting its capabilities, such as high automation and complex pattern recognition, could significantly increase. An AI used offensively to attack non-AI assets is referred to as offensive AI.
  Current research explores how offensive AI can be utilized and how its usage can be classified. Additionally, methods for threat modeling are being developed for AI-based assets within organizations. However, there are gaps that need to be addressed. Firstly, there is a need to quantify the factors contributing to the AI threat. Secondly, there is a requirement to create threat models that analyze the risk of being attacked by AI for vulnerability assessment across all assets of an organization. This is particularly crucial and challenging in cloud environments, where sophisticated infrastructure and access control landscapes are prevalent. The ability to quantify and further analyze the threat posed by offensive AI enables analysts to rank vulnerabilities and prioritize the implementation of proactive countermeasures.
  To address these gaps, this paper introduces the Graph of Effort, an intuitive, flexible, and effective threat modeling method for analyzing the effort required to use offensive AI for vulnerability exploitation by an adversary. While the threat model is functional and provides valuable support, its design choices need further empirical validation in future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16392v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anket Mehra, Andreas A{\ss}muth, Malte Prie{\ss}</dc:creator>
    </item>
    <item>
      <title>AMReX and pyAMReX: Looking Beyond ECP</title>
      <link>https://arxiv.org/abs/2403.12179</link>
      <description>arXiv:2403.12179v2 Announce Type: replace 
Abstract: AMReX is a software framework for the development of block-structured mesh applications with adaptive mesh refinement (AMR). AMReX was initially developed and supported by the AMReX Co-Design Center as part of the U.S. DOE Exascale Computing Project, and is continuing to grow post-ECP. In addition to adding new functionality and performance improvements to the core AMReX framework, we have also developed a Python binding, pyAMReX, that provides a bridge between AMReX-based application codes and the data science ecosystem. pyAMReX provides zero-copy application GPU data access for AI/ML, in situ analysis and application coupling, and enables rapid, massively parallel prototyping. In this paper we review the overall functionality of AMReX and pyAMReX, focusing on new developments, new functionality, and optimizations of key operations. We also summarize capabilities of ECP projects that used AMReX and provide an overview of new, non-ECP applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12179v2</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1177/10943420241271017</arxiv:DOI>
      <arxiv:journal_reference>The International Journal of High Performance Computing Applications. 2024;38(6):599-611</arxiv:journal_reference>
      <dc:creator>Andrew Myers, Weiqun Zhang, Ann Almgren, Thierry Antoun, John Bell, Axel Huebl, Alexander Sinn</dc:creator>
    </item>
    <item>
      <title>AMReX: Block-Structured Adaptive Mesh Refinement for Multiphysics Applications</title>
      <link>https://arxiv.org/abs/2009.12009</link>
      <description>arXiv:2009.12009v2 Announce Type: replace-cross 
Abstract: Block-structured adaptive mesh refinement (AMR) provides the basis for the temporal and spatial discretization strategy for a number of ECP applications in the areas of accelerator design, additive manufacturing, astrophysics, combustion, cosmology, multiphase flow, and wind plant modelling. AMReX is a software framework that provides a unified infrastructure with the functionality needed for these and other AMR applications to be able to effectively and efficiently utilize machines from laptops to exascale architectures. AMR reduces the computational cost and memory footprint compared to a uniform mesh while preserving accurate descriptions of different physical processes in complex multi-physics algorithms. AMReX supports algorithms that solve systems of partial differential equations (PDEs) in simple or complex geometries, and those that use particles and/or particle-mesh operations to represent component physical processes. In this paper, we will discuss the core elements of the AMReX framework such as data containers and iterators as well as several specialized operations to meet the needs of the application projects. In addition we will highlight the strategy that the AMReX team is pursuing to achieve highly performant code across a range of accelerator-based architectures for a variety of different applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2009.12009v2</guid>
      <category>cs.MS</category>
      <category>cs.CE</category>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1177/10943420211022811</arxiv:DOI>
      <arxiv:journal_reference>The International Journal of High Performance Computing Applications. 2021;35(6):508-526</arxiv:journal_reference>
      <dc:creator>Weiqun Zhang, Andrew Myers, Kevin Gott, Ann Almgren, John Bell</dc:creator>
    </item>
    <item>
      <title>Federated Learning for Traffic Flow Prediction with Synthetic Data Augmentation</title>
      <link>https://arxiv.org/abs/2412.08460</link>
      <description>arXiv:2412.08460v2 Announce Type: replace-cross 
Abstract: Deep-learning based traffic prediction models require vast amounts of data to learn embedded spatial and temporal dependencies. The inherent privacy and commercial sensitivity of such data has encouraged a shift towards decentralised data-driven methods, such as Federated Learning (FL). Under a traditional Machine Learning paradigm, traffic flow prediction models can capture spatial and temporal relationships within centralised data. In reality, traffic data is likely distributed across separate data silos owned by multiple stakeholders. In this work, a cross-silo FL setting is motivated to facilitate stakeholder collaboration for optimal traffic flow prediction applications. This work introduces an FL framework, referred to as FedTPS, to generate synthetic data to augment each client's local dataset by training a diffusion-based trajectory generation model through FL. The proposed framework is evaluated on a large-scale real world ride-sharing dataset using various FL methods and Traffic Flow Prediction models, including a novel prediction model we introduce, which leverages Temporal and Graph Attention mechanisms to learn the Spatio-Temporal dependencies embedded within regional traffic flow data. Experimental results show that FedTPS outperforms multiple other FL baselines with respect to global model performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08460v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fermin Orozco, Pedro Porto Buarque de Gusm\~ao, Hongkai Wen, Johan Wahlstr\"om, Man Luo</dc:creator>
    </item>
    <item>
      <title>GC-Fed: Gradient Centralized Federated Learning with Partial Client Participation</title>
      <link>https://arxiv.org/abs/2503.13180</link>
      <description>arXiv:2503.13180v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) enables privacy-preserving multi-source information fusion (MSIF) but is challenged by client drift in highly heterogeneous data settings. Many existing drift-mitigation strategies rely on reference-based techniques--such as gradient adjustments or proximal loss--that use historical snapshots (e.g., past gradients or previous global models) as reference points. When only a subset of clients participates in each training round, these historical references may not accurately capture the overall data distribution, leading to unstable training. In contrast, our proposed Gradient Centralized Federated Learning (GC-Fed) employs a hyperplane as a historically independent reference point to guide local training and enhance inter-client alignment. GC-Fed comprises two complementary components: Local GC, which centralizes gradients during local training, and Global GC, which centralizes updates during server aggregation. In our hybrid design, Local GC is applied to feature-extraction layers to harmonize client contributions, while Global GC refines classifier layers to stabilize round-wise performance. Theoretical analysis and extensive experiments on benchmark FL tasks demonstrate that GC-Fed effectively mitigates client drift and achieves up to a 20% improvement in accuracy under heterogeneous and partial participation conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13180v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jungwon Seo, Ferhat Ozgur Catak, Chunming Rong, Kibeom Hong, Minhoe Kim</dc:creator>
    </item>
  </channel>
</rss>
