<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Oct 2025 04:00:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Switching Network System Identification via Convex Optimizations</title>
      <link>https://arxiv.org/abs/2510.23721</link>
      <description>arXiv:2510.23721v1 Announce Type: new 
Abstract: This paper introduces a convex optimization framework for identifying switched network systems, in which both the node dynamics and the underlying graph topology switch between a finite number of configurations. Building on our recent convex identification method for general switching systems, we extend the formulation to structured network systems where each mode corresponds to a distinct adjacency matrix. We show that both the continuous node dynamics and binary network topologies can be identified from sampled state-velocity data by solving a sequence of convex programs. The proposed framework provides a unified and scalable way to recover piecewise network structures from data without a prior knowledge of mode labels at each state. Numerical results on diffusively coupled oscillators demonstrate accurate recovery of both mode dynamics and switching graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23721v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaito Iwasaki, Anthony Bloch, Maani Ghaffari</dc:creator>
    </item>
    <item>
      <title>A Neural Network Framework for Discovering Closed-form Solutions to Quadratic Programs with Linear Constraints</title>
      <link>https://arxiv.org/abs/2510.23737</link>
      <description>arXiv:2510.23737v1 Announce Type: new 
Abstract: Deep neural networks (DNNs) have been used to model complex optimization problems in many applications, yet have difficulty guaranteeing solution optimality and feasibility, despite training on large datasets. Training a NN as a surrogate optimization solver amounts to estimating a global solution function that maps varying problem input parameters to the corresponding optimal solutions. Work in multiparametric programming (mp) has shown that solutions to quadratic programs (QP) are piece-wise linear functions of the parameters, and researchers have suggested leveraging this property to model mp-QP using NN with ReLU activation functions, which also exhibit piecewise linear behaviour. This paper proposes a NN modeling approach and learning algorithm that discovers the exact closed-form solution to QP with linear constraints, by analytically deriving NN model parameters directly from the problem coefficients without training. Whereas generic DNN cannot guarantee accuracy outside the training distribution, the closed-form NN model produces exact solutions for every discovered critical region of the solution function. To evaluate the closed-form NN model, it was applied to DC optimal power flow problems in electricity management. In terms of Karush-Kuhn-Tucker (KKT) optimality and feasibility of solutions, it outperformed a classically trained DNN and was competitive with, or outperformed, a commercial analytic solver (Gurobi) at far less computational cost. For a long-range energy planning problem, it was able to produce optimal and feasible solutions for millions of input parameters within seconds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23737v1</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fuat Can Beylunioglu, P. Robert Duimering, Mehrdad Pirnia</dc:creator>
    </item>
    <item>
      <title>A Family of Convex Models to Achieve Fairness through Dispersion Control</title>
      <link>https://arxiv.org/abs/2510.23791</link>
      <description>arXiv:2510.23791v1 Announce Type: new 
Abstract: Controlling the dispersion of a subset of decision variables in an optimization problem is crucial for enforcing fairness or load-balancing across a wide range of applications. Building on the well-known equivalence of finite-dimensional norms, the note develops a family of parameterized convex models that regulate the dispersion of a vector of decision-variable values through its coefficient of variation. Each model contains a single parameter that takes a value in the interval [0,1]. When the parameter is set to zero, the model imposes only a trivial constraint on the optimization problem; when set to one, it enforces equality of all the decision variables. As the parameter varies, the coefficient of variation is provably bounded above by a monotonic function of that parameter. The note also presents theoretical results that relate the space of feasible solutions to all the models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23791v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhay Singh Bhadoriya, Deepjyoti Deka, Kaarthik Sundar</dc:creator>
    </item>
    <item>
      <title>On the Sampling-based Computation of Nash Equilibria under Uncertainty via the Nikaido-Isoda Function</title>
      <link>https://arxiv.org/abs/2510.23851</link>
      <description>arXiv:2510.23851v1 Announce Type: new 
Abstract: We consider the computation of an equilibrium of a stochastic Nash equilibrium problem, where the player objectives are assumed to be $L_0$-Lipschitz continuous and convex given rival decisions with convex and closed player-specific feasibility sets. To address this problem, we consider minimizing a suitably defined value function associated with the Nikaido-Isoda function. Such an avenue does not necessitate either monotonicity properties of the concatenated gradient map or potentiality requirements on the game but does require a suitable regularity requirement under which a stationary point is a Nash equilibrium. We design and analyze a sampling-enabled projected gradient descent-type method, reliant on inexact resolution of a player-level best-response subproblem. By deriving suitable Lipschitzian guarantees on the value function, we derive both asymptotic guarantees for the sequence of iterates as well as rate and complexity guarantees for computing a stationary point by appropriate choices of the sampling rate and inexactness sequence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23851v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luke Marrinan, Farzad Yousefian, Uday V. Shanbhag</dc:creator>
    </item>
    <item>
      <title>Stabilizability with bounded feedback for analytic linear control systems</title>
      <link>https://arxiv.org/abs/2510.23953</link>
      <description>arXiv:2510.23953v1 Announce Type: new 
Abstract: In this paper, we give sufficient conditions under which linear abstract control systems for which the semigroup is analytic are stabilizable with a bounded feedback. We obtain various characterizations of that property, which extend some earlier works. We illustrate our findings with several examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23953v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yaxing Ma, Emmanuel Tr\'elat, Lijuan Wang, Huaiqiang Yu</dc:creator>
    </item>
    <item>
      <title>Dynamics of linear control systems and stabilization</title>
      <link>https://arxiv.org/abs/2510.23973</link>
      <description>arXiv:2510.23973v1 Announce Type: new 
Abstract: In this paper, we study linear control systems with positive bounded orbits. We show that the existence of positive bounded orbits imposes strong algebraic and topological constraints on the state space. In fact, a linear control system has bounded positive orbits if and only if it can be decomposed as the product of the stable and central subgroups of the drift, with the central subgroup being compact. In particular, systems with bounded positive orbits admit a compact control set, and if the system is controllable, the entire state space is a compact group. As a byproduct, we obtain a complete characterization of the internal and BIBO stability of linear control systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23973v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Ayala, Adriano Da Silva</dc:creator>
    </item>
    <item>
      <title>A kernel-based stochastic approximation framework for contextual optimization</title>
      <link>https://arxiv.org/abs/2510.24033</link>
      <description>arXiv:2510.24033v1 Announce Type: new 
Abstract: We present a kernel-based stochastic approximation (KBSA) framework for solving contextual stochastic optimization problems with differentiable objective functions. The framework only relies on system output estimates and can be applied to address a large class of contextual measures, including conditional expectations, conditional quantiles, CoVaR, and conditional expected shortfalls.Under appropriate conditions, we show the strong convergence of KBSA and characterize its finite-time performance in terms of bounds on the mean squared errors of the sequences of iterates produced. In addition, we discuss variants of the framework, including a version based on high-order kernels for further enhancing the convergence rate of the method and an extension of KBSA for handling contextual measures involving multiple conditioning events.Simulation experiments are also carried out to illustrate the framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24033v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Cao, Jian-Qiang Hu, Jiaqiao Hu</dc:creator>
    </item>
    <item>
      <title>A Novel Virus Diffusion Optimization (VDO) Algorithm for Global Optimization</title>
      <link>https://arxiv.org/abs/2510.24083</link>
      <description>arXiv:2510.24083v1 Announce Type: new 
Abstract: Meta-heuristic algorithms are widely used to tackle complex optimization problems, including nonlinear, multimodal, and high-dimensional tasks. However, many existing methods suffer from premature convergence, limited exploration, and performance degradation in large-scale search spaces. To overcome these limitations, this paper introduces a novel Virus Diffusion Optimizer (VDO), inspired by the life-cycle and propagation dynamics of herpes-type viruses. VDO integrates four biologically motivated strategies, including viral tropism exploration, viral replication step regulation, virion diffusion propagation, and latency reactivation mechanism, to achieve a balanced trade-off between global exploration and local exploitation. Experiments on standard benchmark problems, including CEC 2017 and CEC 2022, demonstrate that VDO consistently surpasses state-of-the-art metaheuristics in terms of convergence speed, solution quality, and scalability. These results highlight the effectiveness of viral-inspired strategies in optimization and position VDO as a promising tool for addressing large-scale, complex problems in engineering and computational intelligence.To ensure reproducibility and foster further research, the source code of VDO is made publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24083v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaoqi Sun, Qingsong Wang</dc:creator>
    </item>
    <item>
      <title>Extended HJB Equation for Mean-Variance Stopping Problem: Vanishing Regularization Method</title>
      <link>https://arxiv.org/abs/2510.24128</link>
      <description>arXiv:2510.24128v1 Announce Type: new 
Abstract: This paper studies the time-inconsistent MV optimal stopping problem via a game-theoretic approach to find equilibrium strategies. To overcome the mathematical intractability of direct equilibrium analysis, we propose a vanishing regularization method: first, we introduce an entropy-based regularization term to the MV objective, modeling mixed-strategy stopping times using the intensity of a Cox process. For this regularized problem, we derive a coupled extended Hamilton-Jacobi-Bellman (HJB) equation system, prove a verification theorem linking its solutions to equilibrium intensities, and establish the existence of classical solutions for small time horizons via a contraction mapping argument. By letting the regularization term tend to zero, we formally recover a system of parabolic variational inequalities that characterizes equilibrium stopping times for the original MV problem. This system includes an additional key quadratic term--a distinction from classical optimal stopping, where stopping conditions depend only on comparing the value function to the instantaneous reward.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24128v1</guid>
      <category>math.OC</category>
      <category>q-fin.MF</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuchao Dong, Harry Zheng</dc:creator>
    </item>
    <item>
      <title>Variable Projected Augmented Lagrangian Methods for Generalized Lasso Problems</title>
      <link>https://arxiv.org/abs/2510.24140</link>
      <description>arXiv:2510.24140v1 Announce Type: new 
Abstract: We introduce variable projected augmented Lagrangian (VPAL) methods for solving generalized nonlinear Lasso problems with improved speed and accuracy. By eliminating the nonsmooth variable via soft-thresholding, VPAL transforms the problem into a smooth reduced formulation. For linear models, we develop a preconditioned variant that mimics Newton-type updates and yields significant acceleration. We prove convergence guarantees for both standard and preconditioned VPAL under mild assumptions and show that variable projection leads to sharper convergence and higher solution quality. The method seamlessly extends to nonlinear inverse problems, where it outperforms traditional approaches in applications such as phase retrieval and contrast enhanced MRI (LIP-CAR). Across tasks including deblurring, inpainting, and sparse-view tomography, VPAL consistently delivers state-of-the-art reconstructions, positioning variable projection as a powerful tool for modern large-scale inverse problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24140v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefano Aleotti, Davide Bianchi, Florian Bossmann, Riley Yizhou Chen, Matthias Chung</dc:creator>
    </item>
    <item>
      <title>Distributed Stochastic Momentum Tracking with Local Updates: Achieving Optimal Communication and Iteration Complexities</title>
      <link>https://arxiv.org/abs/2510.24155</link>
      <description>arXiv:2510.24155v1 Announce Type: new 
Abstract: We propose Local Momentum Tracking (LMT), a novel distributed stochastic gradient method for solving distributed optimization problems over networks. To reduce communication overhead, LMT enables each agent to perform multiple local updates between consecutive communication rounds. Specifically, LMT integrates local updates with the momentum tracking strategy and the Loopless Chebyshev Acceleration (LCA) technique. We demonstrate that LMT achieves linear speedup with respect to the number of local updates as well as the number of agents for minimizing smooth objective functions. Moreover, with sufficiently many local updates ($Q\geq Q^*$), LMT attains the optimal communication complexity. For a moderate number of local updates ($Q\in[1,Q^*]$), it achieves the optimal iteration complexity. To our knowledge, LMT is the first method that enjoys such properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24155v1</guid>
      <category>math.OC</category>
      <category>cs.DC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Huang, Shi Pu</dc:creator>
    </item>
    <item>
      <title>Feedback Enhancement of Time Series Aggregation for Power System Expansion Planning</title>
      <link>https://arxiv.org/abs/2510.24249</link>
      <description>arXiv:2510.24249v1 Announce Type: new 
Abstract: As a consequence of the high variability of load demand and renewable generation, long-term and high-resolution inputs are required for power system expansion planning, making the problem intractable in real-world applications. Time series aggregation (TSA), which captures representative patterns, reduces temporal complexity while providing similar planning outputs. However, purely statistical clustering, even when enhanced with predefined ``extremes'', can overlook system-specific critical operating conditions, making it unreliable across real-world systems. Therefore, this paper links TSA accuracy on specific system operation and final solution quality, which becomes a practical bound with mean-based TSA approaches. It is observed that the distribution of operational errors is highly imbalanced, such that a few representatives dominate the total error. This paper proposes an adaptive clustering strategy based on feedback enhancement of TSA that iteratively identifies poor-performing representatives with high operational error and re-clusters only their associated periods. A study shows that the feedback enhancement improves the decision error and tighten the bound significantly compared with the plain mean-based clustering method, offering a diagnostic for TSA quality, while balancing the computational effort with solution accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24249v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiqi Zhang, Ensieh Sharifnia, Simon H. Tindemans</dc:creator>
    </item>
    <item>
      <title>A convex reformulation for speed planning of a vehicle under the travel time and energy consumption objectives</title>
      <link>https://arxiv.org/abs/2510.24286</link>
      <description>arXiv:2510.24286v1 Announce Type: new 
Abstract: In this paper we address the speed planning problem for a vehicle along a predefined path. A weighted sum of two conflicting objectives, energy consumption and travel time, is minimized. After deriving a non-convex mathematical model of the problem, we prove that the feasible region of this problem is a lattice. Moreover, we introduce a feasibility-based bound-tightening technique which allows us to derive the minimum and maximum element of the lattice, or establish that the feasible region is empty. We prove the exactness of a convex relaxation of the non-convex problem, obtained by replacing all constraints with the lower and upper bounds for the variables corresponding to the minimum and maximum elements of the lattice, respectively. After proving some properties of optimal solutions of the convex relaxation, we exploit them to develop a dynamic programming approach returning an approximate solution to the convex relaxation, and with time complexity $O(n^2)$, where $n$ is the number of points into which the continuous path is discretized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24286v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luca Consolini, Mattia Laurini, Marco Locatelli</dc:creator>
    </item>
    <item>
      <title>Problem-Parameter-Free Decentralized Bilevel Optimization</title>
      <link>https://arxiv.org/abs/2510.24288</link>
      <description>arXiv:2510.24288v1 Announce Type: new 
Abstract: Decentralized bilevel optimization has garnered significant attention due to its critical role in solving large-scale machine learning problems. However, existing methods often rely on prior knowledge of problem parameters-such as smoothness, convexity, or communication network topologies-to determine appropriate stepsizes. In practice, these problem parameters are typically unavailable, leading to substantial manual effort for hyperparameter tuning. In this paper, we propose AdaSDBO, a fully problem-parameter-free algorithm for decentralized bilevel optimization with a single-loop structure. AdaSDBO leverages adaptive stepsizes based on cumulative gradient norms to update all variables simultaneously, dynamically adjusting its progress and eliminating the need for problem-specific hyperparameter tuning. Through rigorous theoretical analysis, we establish that AdaSDBO achieves a convergence rate of $\widetilde{\mathcal{O}}\left(\frac{1}{T}\right)$, matching the performance of well-tuned state-of-the-art methods up to polylogarithmic factors. Extensive numerical experiments demonstrate that AdaSDBO delivers competitive performance compared to existing decentralized bilevel optimization methods while exhibiting remarkable robustness across diverse stepsize configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24288v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiwei Zhai, Wenjing Yan, Ying-Jun Angela Zhang</dc:creator>
    </item>
    <item>
      <title>Optimal Unmanned Aerial Vehicle Deployment for Macro-Micro Traffic Monitoring Fused with Connected Vehicles</title>
      <link>https://arxiv.org/abs/2510.24384</link>
      <description>arXiv:2510.24384v1 Announce Type: new 
Abstract: Reliable estimation of macro and micro traffic states is essential for urban traffic management. Unmanned Aerial Vehicles, with their airborne full-sample continuous trajectory observation, bring new opportunities for macro- and micro-traffic state estimation. In this study, we will explore the optimal UAV deployment problem in road networks in conjunction with sampled connected vehicle data to achieve more reliable estimation of macroscopic path flow as well as microscopic arrival rates and queue lengths. Oriented towards macro-micro traffic states, we propose entropy-based and area-based uncertainty measures, respectively, and transform the optimal UAV deployment problem into minimizing the uncertainty of macro-micro traffic states. A quantum genetic algorithm that integrates the thoughts of metaheuristic algorithms and quantum computation is then proposed to solve the large-scale nonlinear problem efficiently. Evaluation results on a network with 18 intersections have demonstrated that by deploying UAV detection at specific locations, the uncertainty reduction of macro-micro traffic state estimation ranges from 15.28\% to 75.69\%. A total of 5 UAVs with optimal location schemes would be sufficient to detect over 95\% of the paths in the network considering both microscopic uncertainty regarding the intersection operation efficiency and the macroscopic uncertainty regarding the route choice of road users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24384v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaopeng Tan, Jiarong Yao, Meng Wang</dc:creator>
    </item>
    <item>
      <title>Concurrent Crossover for PDHG</title>
      <link>https://arxiv.org/abs/2510.24429</link>
      <description>arXiv:2510.24429v1 Announce Type: new 
Abstract: First-order methods based on the PDHG algorithm have recently emerged as a viable option for efficiently solving large-scale linear programming problems. One highly desirable property of these methods is that they can make effective use of GPUs. One undesirable property is that, as first-order methods, their convergence can be extremely slow. This property forces one to decide how much accuracy is truly necessary when solving an LP problem. This paper looks at whether a parallel, concurrent crossover scheme can help to obtain highly accurate solutions without sacrificing the benefits of these new approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24429v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edward Rothberg</dc:creator>
    </item>
    <item>
      <title>Efficient Network Reconfiguration by Randomized Switching</title>
      <link>https://arxiv.org/abs/2510.24458</link>
      <description>arXiv:2510.24458v1 Announce Type: new 
Abstract: We present an algorithm that efficiently computes nearly-optimal solutions to a class of combinatorial reconfiguration problems on weighted, undirected graphs. Inspired by societally relevant applications in networked infrastructure systems, these problems consist of simultaneously finding an unreweighted sparsified graph and nodal potentials that satisfy fixed demands, where the objective is to minimize some congestion criterion, e.g., a Laplacian quadratic form. These are mixed-integer nonlinear programming problems that are NP-hard in general. To circumvent these challenges, instead of solving for a single best configuration, the proposed randomized switching algorithm seeks to design a distribution of configurations that, when sampled, ensures that congestion concentrates around its optimum. We show that the proposed congestion metric is a generalized self-concordant function in the space of switching probabilities, which enables the use of efficient and simple conditional gradient methods. We implement our algorithm and show that it outperforms a state-of-the-art commercial mixed-integer second-order cone programming (MISOCP) solver by orders of magnitude over a large range of problem sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24458v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Talkington, Dmitrii M. Ostrovskii, Daniel K. Molzahn</dc:creator>
    </item>
    <item>
      <title>Collaborating Unmanned Aerial Vehicle and Ground Sensors for Urban Signalized Network Traffic Monitoring</title>
      <link>https://arxiv.org/abs/2510.24460</link>
      <description>arXiv:2510.24460v1 Announce Type: new 
Abstract: Reliable estimation of network-wide traffic states is essential for urban traffic management. Unmanned Aerial Vehicles (UAVs), with their airborne full-sample continuous trajectory observation, bring new opportunities for traffic state estimation. In this study, we will explore the optimal UAV deployment problem in road networks in conjunction with ground sensors, including connected vehicle (CV) and loop detectors, to achieve more reliable estimation of vehicle path reconstruction as well as movement-based arrival rates and queue lengths. Oriented towards reliable estimation of traffic states, we propose an index, feasible domain size, as the uncertainty measurement, and transform the optimal UAV deployment problem into minimizing the observation uncertainty of network-wide traffic states. Given the large-scale and nonlinear nature of the problem, an improved quantum genetic algorithm (IQGA) that integrates two customized operators is proposed to enhance neighbor searching and solution refinement, thereby improving the observability of UAV pairs. Evaluation was conducted on an empirical network with 18 intersections. Results demonstrated that a UAV fleet size of 7 is sufficient for traffic monitoring, with more than 60\% of network-wide observation uncertainty reduced. Through horizontal comparison with three baselines, the optimal UAV location scheme obtained by the proposed method can reach an improvement of up to 7.23\% and 5.02\% in the estimation accuracy of arrival rate and queue length, respectively. The proposed IQGA is also shown to be faster in solution convergence than the classic QGA by about 9.22\% with better exploration ability in optimum searching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24460v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiarong Yao, Chaopeng Tan, Meng Wang, Wei Ma</dc:creator>
    </item>
    <item>
      <title>Deterministic classical limit of the optimal control problem of quantum particles with spin</title>
      <link>https://arxiv.org/abs/2510.24462</link>
      <description>arXiv:2510.24462v1 Announce Type: new 
Abstract: We study the optimal control problem applied to a gas of particles with spin confined in a material with Rashba spin-orbit coupling effect, in the presence of an external magnetic field. The evolution of the particle gas is described in the Wigner formalism. We investigate the classical limit of the optimal control problem, and we prove the convergence of the solution of the quantum problem toward the solution of a simplified optimal control model, based on an ODE description of the particle gas in terms of a single spin vector traveling along a classical trajectory in the phase-space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24462v1</guid>
      <category>math.OC</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Omar Morandi</dc:creator>
    </item>
    <item>
      <title>Non-Singularity of the Gradient Descent map for Neural Networks with Piecewise Analytic Activations</title>
      <link>https://arxiv.org/abs/2510.24466</link>
      <description>arXiv:2510.24466v1 Announce Type: new 
Abstract: The theory of training deep networks has become a central question of modern machine learning and has inspired many practical advancements. In particular, the gradient descent (GD) optimization algorithm has been extensively studied in recent years. A key assumption about GD has appeared in several recent works: the \emph{GD map is non-singular} -- it preserves sets of measure zero under preimages. Crucially, this assumption has been used to prove that GD avoids saddle points and maxima, and to establish the existence of a computable quantity that determines the convergence to global minima (both for GD and stochastic GD). However, the current literature either assumes the non-singularity of the GD map or imposes restrictive assumptions, such as Lipschitz smoothness of the loss (for example, Lipschitzness does not hold for deep ReLU networks with the cross-entropy loss) and restricts the analysis to GD with small step-sizes. In this paper, we investigate the neural network map as a function on the space of weights and biases. We also prove, for the first time, the non-singularity of the gradient descent (GD) map on the loss landscape of realistic neural network architectures (with fully connected, convolutional, or softmax attention layers) and piecewise analytic activations (which includes sigmoid, ReLU, leaky ReLU, etc.) for almost all step-sizes. Our work significantly extends the existing results on the convergence of GD and SGD by guaranteeing that they apply to practical neural network settings and has the potential to unlock further exploration of learning dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24466v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandru Cr\u{a}ciun, Debarghya Ghoshdastidar</dc:creator>
    </item>
    <item>
      <title>Nonlinear forward-backward-half forward splitting with momentum for monotone inclusions</title>
      <link>https://arxiv.org/abs/2510.24489</link>
      <description>arXiv:2510.24489v1 Announce Type: new 
Abstract: In this work, we propose a new splitting algorithm for solving structured monotone inclusion problems composed of a maximally monotone operator, a maximally monotone and Lipschitz continuous operator and a cocoercive operator. Our method augments the forward-backward-half forward splitting algorithm with a nonlinear momentum term. Under appropriate conditions on the step-size, we prove the weak convergence of the proposed algorithm. A linear convergence rate is also obtained under the strong monotonicity assumption. Furthermore, we investigate a stochastic variance-reduced forward-backward-half forward splitting algorithm with momentum for solving finite-sum monotone inclusion problems. Weak almost sure convergence and linear convergence are also established under standard condition. Preliminary numerical experiments on synthetic datasets and real-world quadratic programming problems in portfolio optimization demonstrate the effectiveness and superiority of the proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24489v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liqian Qin, Yuchao Tang, Jigen Peng</dc:creator>
    </item>
    <item>
      <title>Linear-Quadratic Zero-Sum Stochastic Differential Game with Partial Observation</title>
      <link>https://arxiv.org/abs/2510.24493</link>
      <description>arXiv:2510.24493v1 Announce Type: new 
Abstract: This paper is concerned with a kind of linear-quadratic (LQ, for short) two-person zero-sum stochastic differential game problems with partial observation. We propose the notions of explicit and implicit feedback laws under partial observation. With the help of a class of conditional mean-field stochastic differential equations (CMF-SDEs, for short), the separation principle, filtering techniques, and the method of completion of squares, we construct a saddle point in the form of feedback laws for the two players. Finally, the theoretical results are applied to investigate a duopoly competition problem with partial observation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24493v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyong Yu, Wanying Yue</dc:creator>
    </item>
    <item>
      <title>A Two-step Krasnosel'skii-Mann Algorithm with Adaptive Momentum and Its Applications to Image Denoising and Matrix Completion</title>
      <link>https://arxiv.org/abs/2510.24544</link>
      <description>arXiv:2510.24544v1 Announce Type: new 
Abstract: In this paper, we propose a Two-step Krasnosel'skii-Mann (KM) Algorithm (TKMA) with adaptive momentum for solving convex optimization problems arising in image processing. Such optimization problems can often be reformulated as fixed-point problems for certain operators, which are then solved using iterative methods based on the same operator, including the KM iteration, to ultimately obtain the solution to the original optimization problem. Prior to developing TKMA, we first introduce a KM iteration enhanced with adaptive momentum, derived from geometric properties of an averaged nonexpansive operator T, KM acceleration technique, and information from the composite operator T^2. The proposed TKMA is constructed as a convex combination of this adaptive-momentum KM iteration and the Picard iteration of T^2. We establish the convergence of the sequence generated by TKMA to a fixed point of T. Moreover, under specific assumptions on the adaptive momentum parameters, we prove that the algorithm achieves an o(1/k^{1/2}) convergence rate in terms of the distance between successive iterates. Numerical experiments demonstrate that TKMA outperforms the FPPA, PGA, Fast KM algorithm, and Halpern algorithm on tasks such as image denoising and low-rank matrix completion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24544v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongxin He, Jingyuan Li, Yizun Lin, Deren Han</dc:creator>
    </item>
    <item>
      <title>Sum of Squares Submodularity</title>
      <link>https://arxiv.org/abs/2510.24550</link>
      <description>arXiv:2510.24550v1 Announce Type: new 
Abstract: We introduce the notion of $t$-sum of squares (sos) submodularity, which is a hierarchy, indexed by $t$, of sufficient algebraic conditions for certifying submodularity of set functions. We show that, for fixed $t$, each level of the hierarchy can be verified via a semidefinite program of size polynomial in $n$, the size of the ground set of the set function. This is particularly relevant given existing hardness results around testing whether a set function is submodular (Crama, 1989). We derive several equivalent algebraic characterizations of $t$-sos submodularity and identify submodularity-preserving operations that also preserve $t$-sos submodularity. We further present a complete classification of the cases for which submodularity and $t$-sos submodularity coincide, as well as examples of $t$-sos-submodular functions. We demonstrate the usefulness of $t$-sos submodularity through three applications: (i) a new convex approach to submodular regression, involving minimal manual tuning; (ii) a systematic procedure to derive lower bounds on the submodularity ratio in approximate submodular maximization, and (iii) improved difference-of-submodular decompositions for difference-of-submodular optimization. Overall, our work builds a new bridge between discrete optimization and real algebraic geometry by connecting sum of squares-based algebraic certificates to a fundamental discrete structure, submodularity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24550v1</guid>
      <category>math.OC</category>
      <category>math.AG</category>
      <category>math.FA</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Deza, Georgina Hall</dc:creator>
    </item>
    <item>
      <title>A Single-Loop First-Order Algorithm for Linearly Constrained Bilevel Optimization</title>
      <link>https://arxiv.org/abs/2510.24710</link>
      <description>arXiv:2510.24710v1 Announce Type: new 
Abstract: We study bilevel optimization problems where the lower-level problems are strongly convex and have coupled linear constraints. To overcome the potential non-smoothness of the hyper-objective and the computational challenges associated with the Hessian matrix, we utilize penalty and augmented Lagrangian methods to reformulate the original problem as a single-level one. Especially, we establish a strong theoretical connection between the reformulated function and the original hyper-objective by characterizing the closeness of their values and derivatives. Based on this reformulation, we propose a single-loop, first-order algorithm for linearly constrained bilevel optimization (SFLCB). We provide rigorous analyses of its non-asymptotic convergence rates, showing an improvement over prior double-loop algorithms -- form $O(\epsilon^{-3}\log(\epsilon^{-1}))$ to $O(\epsilon^{-3})$. The experiments corroborate our theoretical findings and demonstrate the practical efficiency of the proposed SFLCB algorithm. Simulation code is provided at https://github.com/ShenGroup/SFLCB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24710v1</guid>
      <category>math.OC</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Shen, Jiawei Zhang, Minhui Huang, Cong Shen</dc:creator>
    </item>
    <item>
      <title>Optimized Loudspeaker Panning for Adaptive Sound-Field Correction and Non-stationary Listening Areas</title>
      <link>https://arxiv.org/abs/2510.23937</link>
      <description>arXiv:2510.23937v1 Announce Type: cross 
Abstract: Surround sound systems commonly distribute loudspeakers along standardized layouts for multichannel audio reproduction. However in less controlled environments, practical layouts vary in loudspeaker quantity, placement, and listening locations / areas. Deviations from standard layouts introduce sound-field errors that degrade acoustic timbre, imaging, and clarity of audio content reproduction. This work introduces both Bayesian loudspeaker normalization and content panning optimization methods for sound-field correction. Conjugate prior distributions over loudspeaker-listener directions update estimated layouts for non-stationary listening locations; digital filters adapt loudspeaker acoustic responses to a common reference target at the estimated listening area without acoustic measurements. Frequency-domain panning coefficients are then optimized via sensitivity / efficiency objectives subject to spatial, electrical, and acoustic domain constraints; normalized and panned loudspeakers form virtual loudspeakers in standardized layouts for accurate multichannel reproduction. Experiments investigate robustness of Bayesian adaptation, and panning optimizations in practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23937v1</guid>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>eess.SP</category>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Luo, Yuancheng; Optimized Loudspeaker Panning for Adaptive Sound-Field Correction and Non-stationary Listening Areas; AES Long Beach: 159th Audio Engineering Society Convention 2025; Paper 385</arxiv:journal_reference>
      <dc:creator>Yuancheng Luo</dc:creator>
    </item>
    <item>
      <title>Computing intrinsic volumes of sublevel sets and applications</title>
      <link>https://arxiv.org/abs/2510.24001</link>
      <description>arXiv:2510.24001v1 Announce Type: cross 
Abstract: Intrinsic volumes are fundamental geometric invariants generalizing volume, surface area, and mean width for convex bodies. We establish a unified Laplace-Grassmannian representation for intrinsic and dual volumes of convex polynomial sublevel sets. More precisely, let $f$ be a convex $d$-homogeneous polynomial of even degree $d \ge 2$ which is positive except at the origin. We show that the intrinsic and dual volumes of the sublevel set $[f \le 1]$ admit Laplace-type integral formulas obtained by averaging the infimal projection and restriction of $f$ over the Grassmannian. This explicit representation yields three main consequences: (1) L\"owner--John-type existence and uniqueness results extending beyond the classical volume case; (2) a block decomposition principle describing factorization of intrinsic volumes under direct-sum splitting; (3) a coordinate-free formulation of Lipschitz-type lattice discrepancy bounds. These formulas enable analytic treatment of a broad class of geometric quantities, providing direct access to variational and arithmetic applications as well as new structural insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24001v1</guid>
      <category>math.MG</category>
      <category>math.NT</category>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tr\'i Minh L\^e, Khai-Hoan Nguyen-Dang</dc:creator>
    </item>
    <item>
      <title>Discovering Heuristics with Large Language Models (LLMs) for Mixed-Integer Programs: Single-Machine Scheduling</title>
      <link>https://arxiv.org/abs/2510.24013</link>
      <description>arXiv:2510.24013v1 Announce Type: cross 
Abstract: Our study contributes to the scheduling and combinatorial optimization literature with new heuristics discovered by leveraging the power of Large Language Models (LLMs). We focus on the single-machine total tardiness (SMTT) problem, which aims to minimize total tardiness by sequencing n jobs on a single processor without preemption, given processing times and due dates. We develop and benchmark two novel LLM-discovered heuristics, the EDD Challenger (EDDC) and MDD Challenger (MDDC), inspired by the well-known Earliest Due Date (EDD) and Modified Due Date (MDD) rules. In contrast to prior studies that employed simpler rule-based heuristics, we evaluate our LLM-discovered algorithms using rigorous criteria, including optimality gaps and solution time derived from a mixed-integer programming (MIP) formulation of SMTT. We compare their performance against state-of-the-art heuristics and exact methods across various job sizes (20, 100, 200, and 500 jobs). For instances with more than 100 jobs, exact methods such as MIP and dynamic programming become computationally intractable. Up to 500 jobs, EDDC improves upon the classic EDD rule and another widely used algorithm in the literature. MDDC consistently outperforms traditional heuristics and remains competitive with exact approaches, particularly on larger and more complex instances. This study shows that human-LLM collaboration can produce scalable, high-performing heuristics for NP-hard constrained combinatorial optimization, even under limited resources when effectively configured.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24013v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>math.CO</category>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\.Ibrahim O\u{g}uz \c{C}etinkaya, \.I. Esra B\"uy\"uktahtak{\i}n, Parshin Shojaee, Chandan K. Reddy</dc:creator>
    </item>
    <item>
      <title>Structural Vulnerability Assessment in Urban Transport Networks: A Network-Wide Geometric Approach Using Gromov-Wasserstein</title>
      <link>https://arxiv.org/abs/2510.24306</link>
      <description>arXiv:2510.24306v1 Announce Type: cross 
Abstract: Urban transportation networks are inherently vulnerable to disruptions that affect connectivity and passenger mobility. Traditional graph_theoretic metrics, such as betweenness and degree centrality, offer insights into local network structure but often fail to capture global structural distortions resulting from link failures. On the other hand, global indices, such as those based on spectral analysis of the networks graph, fail in identifying critical elements. This study proposes to quantify the structural modifications implied by the disruption of single elements in a transportation network through the Gromov-Wasserstein distance. Specifically, we iteratively remove one single edge from the original network to simulate a disruptive event and then compute the Gromov-Wasserstein distance between the original network and the disrupted one. Finally, edges are ranked depending on the observed Gromov-Wasserstein distance: the higher the value of the distance, the more critical the edge is in terms. Two transportation networks from Berlin are considered in the experiments, namely Berlin Friedrichshain Center (BFC) and Berlin Tiergarten (BT). Results reveal that Gromov-Wasserstein is largely uncorrelated with edge betweenness (rho&lt;0.1), proving its ability to capture vulnerability aspects overlooked by local network measures. Moreover, Gromov-Wasserstein exhibits an almost perfect correlation (rho=0.9999) against a proxy measure of the transportation service level, that is, the increase in the maximum shortest path. As a result, the Gromov-Wasserstein distance can be used to rank edges depending on their criticality with respect to their individual impact on the overall infrastructure and level, allowing for prioritizing maintenance, emergency planning, and enhancing the resilience of the urban transport network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24306v1</guid>
      <category>physics.soc-ph</category>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iman Seyedi, Antonio Candelieri, Enza Messina, Francesco Archetti</dc:creator>
    </item>
    <item>
      <title>The Omniscient, yet Lazy, Investor</title>
      <link>https://arxiv.org/abs/2510.24467</link>
      <description>arXiv:2510.24467v1 Announce Type: cross 
Abstract: We formalize the paradox of an omniscient yet lazy investor - a perfectly informed agent who trades infrequently due to execution or computational frictions. Starting from a deterministic geometric construction, we derive a closed-form expected profit function linking trading frequency, execution cost, and path roughness. We prove existence and uniqueness of the optimal trading frequency and show that this optimum can be interpreted through the fractal dimension of the price path. A stochastic extension under fractional Brownian motion provides analytical expressions for the optimal interval and comparative statics with respect to the Hurst exponent. Empirical illustrations on equity data confirm the theoretical scaling behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24467v1</guid>
      <category>q-fin.TR</category>
      <category>math.OC</category>
      <category>q-fin.MF</category>
      <category>q-fin.ST</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stanis{\l}aw M. S. Halkiewicz</dc:creator>
    </item>
    <item>
      <title>Resolvent bounds imply observability from measurable time sets for Schr\"odinger equations</title>
      <link>https://arxiv.org/abs/2510.24517</link>
      <description>arXiv:2510.24517v1 Announce Type: cross 
Abstract: We prove that on a compact Riemannian manifold, resolvent bounds for the Laplace--Beltrami operator imply observability, and thus controllability, for the Schr\"odinger propagator from time sets of positive Lebesgue measure. Applications include almost all cases where observability and controllability hold from time intervals, particularly when the geometric control condition is satisfied or when the manifold is a compact surface of negative curvature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24517v1</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Burq, Hui Zhu</dc:creator>
    </item>
    <item>
      <title>Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures</title>
      <link>https://arxiv.org/abs/2510.24607</link>
      <description>arXiv:2510.24607v1 Announce Type: cross 
Abstract: We develop \emph{Entropy-Guided Multiplicative Updates} (EGMU), a convex optimization framework for constructing multi-factor target-exposure portfolios by minimizing Kullback--Leibler (KL) divergence from a benchmark subject to linear factor constraints. Our contributions are theoretical and algorithmic. (\emph{i}) We formalize feasibility and uniqueness: with strictly positive benchmark and feasible targets in the convex hull of exposures, the solution is unique and strictly positive. (\emph{ii}) We derive the dual concave program with gradient $t-\E_{w(\theta)}[x]$ and Hessian $-\Cov_{w(\theta)}(x)$, and give a precise sensitivity formula $\partial\theta^*/\partial t=\Cov_{w^*}(x)^{-1}$ and $\partial w^*/\partial t=\mathrm{diag}(w^*) (X-\1\mu^\top)\Cov_{w^*}(x)^{-1}$. (\emph{iii}) We present two provably convergent solvers: a damped \emph{dual Newton} method with global convergence and local quadratic rate, and a \emph{KL-projection} scheme based on IPF/Bregman--Dykstra for equalities and inequalities. (\emph{iv}) We further \textbf{generalize EGMU} with \emph{elastic targets} (strongly concave dual) and \emph{robust target sets} (support-function dual), and introduce a \emph{path-following ODE} for solution trajectories, all reusing the same dual-moment structure and solved via Newton or proximal-gradient schemes. (\emph{v}) We detail numerically stable and scalable implementations (LogSumExp, covariance regularization, half-space KL-projections). We emphasize theory and reproducible algorithms; empirical benchmarking is optional.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24607v1</guid>
      <category>q-fin.PM</category>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yimeng Qiu</dc:creator>
    </item>
    <item>
      <title>Fast algorithms enabling optimization and deep learning for photoacoustic tomography in a circular detection geometry</title>
      <link>https://arxiv.org/abs/2510.24687</link>
      <description>arXiv:2510.24687v1 Announce Type: cross 
Abstract: The inverse source problem arising in photoacoustic tomography and in several other coupled-physics modalities is frequently solved by iterative algorithms. Such algorithms are based on the minimization of a certain cost functional. In addition, novel deep learning techniques are currently being investigated to further improve such optimization approaches. All such methods require multiple applications of the operator defining the forward problem, and of its adjoint. In this paper, we present new asymptotically fast algorithms for numerical evaluation of the forward and adjoint operators, applicable in the circular acquisition geometry. For an $(n \times n)$ image, our algorithms compute these operators in $\mathcal{O}(n^2 \log n)$ floating point operations. We demonstrate the performance of our algorithms in numerical simulations, where they are used as an integral part of several iterative image reconstruction techniques: classic variational methods, such as non-negative least squares and total variation regularized least squares, as well as deep learning methods, such as learned primal dual. A Python implementation of our algorithms and computational examples is available to the general public.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24687v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Hauptmann, Leonid Kunyansky, Jenni Poimala</dc:creator>
    </item>
    <item>
      <title>Transport densities and congested optimal transport problem in the Heisenberg group</title>
      <link>https://arxiv.org/abs/2303.16052</link>
      <description>arXiv:2303.16052v3 Announce Type: replace 
Abstract: We adapt the problem of continuous congested optimal transport to the Heisenberg group, equipped with a sub-Riemannian metric. Originally introduced in the Euclidean setting by Carlier, Jimenez, and Santambrogio as a path-dependent variant of the Monge-Kantorovich problem, we significantly restrict the set of admissible curves to horizontal ones. We establish the existence of equilibrium configurations as solutions to a convex minimization problem over a suitable set of measures on horizontal curves. This result is achieved through the notions of horizontal transport density and horizontal traffic intensity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.16052v3</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jmaa.2025.130148</arxiv:DOI>
      <arxiv:journal_reference>Journal of Mathematical Analysis and Applications, vol. 555, no. 2, 2026</arxiv:journal_reference>
      <dc:creator>Michele Circelli, Giovanna Citti</dc:creator>
    </item>
    <item>
      <title>A cutting-surface consensus approach for distributed robust optimization of multi-agent systems</title>
      <link>https://arxiv.org/abs/2309.03519</link>
      <description>arXiv:2309.03519v3 Announce Type: replace 
Abstract: A novel and fully distributed optimization method is proposed for the distributed robust convex program (DRCP) over a time-varying unbalanced directed network under the uniformly jointly strongly connected (UJSC) assumption. Firstly, an approximated DRCP (ADRCP) is introduced by discretizing the semi-infinite constraints into a finite number of inequality constraints to ensure tractability and restricting the right-hand side of the constraints with a positive parameter to ensure a feasible solution for (DRCP) can be obtained. This problem is iteratively solved by a distributed projected gradient algorithm proposed in this paper, which is based on epigraphic reformulation and gradient projected operations. Secondly, a cutting-surface consensus approach is proposed for locating an approximately optimal consensus solution of the DRCP with guaranteed local feasibility for each agent. This approach is based on iteratively approximating the DRCP by successively reducing the restriction parameter of the right-hand constraints and adding the cutting-surfaces into the existing finite set of constraints. Thirdly, to ensure finite-time termination of the distributed optimization, a distributed termination algorithm is developed based on consensus and zeroth-order stopping conditions under UJSC graphs. Fourthly, it is proved that the cutting-surface consensus approach terminates finitely and yields a feasible and approximate optimal solution for each agent. Finally, the effectiveness of the approach is illustrated through a numerical example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.03519v3</guid>
      <category>math.OC</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TAC.2025.3573763</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Automatic Control, vol. 70, no. 11, November 2025</arxiv:journal_reference>
      <dc:creator>Jun Fu, Xunhao Wu</dc:creator>
    </item>
    <item>
      <title>The Chambolle--Pock method converges weakly with $\theta&gt;1/2$ and $\tau \sigma \|L\|^2&lt;4/(1+2\theta)$</title>
      <link>https://arxiv.org/abs/2309.03998</link>
      <description>arXiv:2309.03998v2 Announce Type: replace 
Abstract: The Chambolle--Pock method is a versatile three-parameter algorithm designed to solve a broad class of composite convex optimization problems, which encompass two proper, lower semicontinuous, and convex functions, along with a linear operator $L$. The functions are accessed via their proximal operators, while the linear operator is evaluated in a forward manner. Among the three algorithm parameters $\tau $, $\sigma $, and $\theta$; $\tau,\sigma &gt;0$ serve as step sizes for the proximal operators, and $\theta$ is an extrapolation step parameter. Previous convergence results have been based on the assumption that $\theta=1$. We demonstrate that weak convergence is achievable whenever $\theta&gt; 1/2$ and $\tau \sigma \|L\|^2&lt;4/(1+2\theta)$. Moreover, we establish tightness of the step size bound by providing an example that is nonconvergent whenever the second bound is violated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.03998v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11590-025-02250-0</arxiv:DOI>
      <dc:creator>Sebastian Banert, Manu Upadhyaya, Pontus Giselsson</dc:creator>
    </item>
    <item>
      <title>Auto-Calibration and Biconvex Compressive Sensing with Applications to Parallel MRI</title>
      <link>https://arxiv.org/abs/2401.10400</link>
      <description>arXiv:2401.10400v3 Announce Type: replace 
Abstract: We study an auto-calibration problem in which a transform-sparse signal is acquired via compressive sensing by multiple sensors in parallel, but with unknown calibration parameters of the sensors. This inverse problem has an important application in pMRI reconstruction, where the calibration parameters of the receiver coils are often difficult and costly to obtain explicitly, but nonetheless are a fundamental requirement for high-precision reconstructions. Most auto-calibration strategies for this problem involve solving a challenging biconvex optimization problem, which lacks reconstruction guarantees. In this work, we transform the auto-calibrated parallel compressive sensing problem to a convex optimization problem using the idea of `lifting'. By exploiting sparsity structures in the signal and the redundancy introduced by multiple sensors, we solve a mixed-norm minimization problem to recover the underlying signal and the sensing parameters simultaneously. Our method provides robust and stable recovery guarantees that take into account the presence of noise and sparsity deficiencies in the signals. As such, it offers a theoretically guaranteed approach to auto-calibrated parallel imaging in MRI under appropriate assumptions. Applications in compressive sensing pMRI are discussed, and numerical experiments using real and simulated MRI data are presented to support our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10400v3</guid>
      <category>math.OC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Ni, Thomas Strohmer</dc:creator>
    </item>
    <item>
      <title>Complexity results and active-set identification of a derivative-free method for bound-constrained problems</title>
      <link>https://arxiv.org/abs/2402.10801</link>
      <description>arXiv:2402.10801v4 Announce Type: replace 
Abstract: In this paper, we analyze a derivative-free line search method designed for bound-constrained problems. Our analysis demonstrates that this method exhibits a worst-case complexity comparable to other derivative-free methods for unconstrained and linearly constrained problems. In particular, when minimizing a function with $n$ variables, we prove that at most ${\cal O(n\epsilon^{-2})}$ iterations are needed to drive a criticality measure below a predefined threshold $\epsilon$, requiring at most ${\cal O(n^2\epsilon^{-2})}$ function evaluations. We also show that the total number of iterations where the criticality measure is not below $\epsilon$ is upper bounded by ${\cal O(n^2\epsilon^{-2})}$. Moreover, we investigate the method capability to identify active constraints at the final solutions. We show that, after a finite number of iterations, all the active constraints satisfying the strict complementarity condition are correctly identified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10801v4</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Brilli, Andrea Cristofari, Giampaolo Liuzzi, Stefano Lucidi</dc:creator>
    </item>
    <item>
      <title>Beyond Convexity: Proximal-Perturbed Lagrangian Methods for Efficient Functional Constrained Optimization</title>
      <link>https://arxiv.org/abs/2406.17107</link>
      <description>arXiv:2406.17107v2 Announce Type: replace 
Abstract: Non-convex functional constrained optimization problems have gained substantial attention in machine learning and data science, addressing broad requirements that typically go beyond the often performance-centric objectives. An influential class of algorithms for functional constrained problems is the class of primal-dual methods which has been extensively analyzed for convex problems. Nonetheless, the investigation of their efficacy for non-convex problems is under-explored. This paper develops a primal-dual algorithmic framework for solving such non-convex problems. This framework is built upon a novel form of the Lagrangian function, termed the {\em Proximal-Perturbed Augmented Lagrangian}, which enables the development of simple first-order algorithms that converge to a stationary solution under mild conditions. Notably, we study this framework under both non-smoothness and smoothness of the constraint function and provide three key contributions: (i) a simple algorithm that does not require the continuous adjustment of the penalty parameter; (ii) a non-asymptotic iteration complexity of $\widetilde{\mathcal{O}}(1/\epsilon^2)$; and (iii) extensive experimental results demonstrating the effectiveness of the proposed framework in terms of computational cost and performance, outperforming related approaches that use regularization (penalization) techniques and/or standard Lagrangian relaxation across diverse non-convex problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17107v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sang Bin Moon, Jong Gwang Kim, Ashish Chandra, Christopher Brinton, Abolfazl Hashemi</dc:creator>
    </item>
    <item>
      <title>Learning Firmly Nonexpansive Operators</title>
      <link>https://arxiv.org/abs/2407.14156</link>
      <description>arXiv:2407.14156v3 Announce Type: replace 
Abstract: This paper proposes a data-driven approach for constructing firmly nonexpansive operators. We demonstrate its applicability in Plug-and-Play (PnP) methods, where classical algorithms such as Forward-Backward splitting, Chambolle-Pock primal-dual iteration, Douglas-Rachford iteration or alternating directions method of multipliers (ADMM), are modified by replacing one proximal map by a learned firmly nonexpansive operator. We provide sound mathematical background to the problem of learning such an operator via expected and empirical risk minimization. We prove that, as the number of training points increases, the empirical risk minimization problem converges (in the sense of Gamma-convergence) to the expected risk minimization problem. Further, we derive a solution strategy that ensures firmly nonexpansive and piecewise affine operators within the convex envelope of the training set. We show that this operator converges to the best empirical solution as the number of points in the envelope increases in an appropriate way. Finally, the experimental section details practical implementations of the method and presents an application in image denoising, where we consider a novel, interpretable PnP Chambolle-Pock primal-dual iteration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14156v3</guid>
      <category>math.OC</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristian Bredies, Jonathan Chirinos-Rodriguez, Emanuele Naldi</dc:creator>
    </item>
    <item>
      <title>Global Optimization of Gaussian Process Acquisition Functions Using a Piecewise-Linear Kernel Approximation</title>
      <link>https://arxiv.org/abs/2410.16893</link>
      <description>arXiv:2410.16893v2 Announce Type: replace 
Abstract: Bayesian optimization relies on iteratively constructing and optimizing an acquisition function. The latter turns out to be a challenging, non-convex optimization problem itself. Despite the relative importance of this step, most algorithms employ sampling- or gradient-based methods, which do not provably converge to global optima. This work investigates mixed-integer programming (MIP) as a paradigm for global acquisition function optimization. Specifically, our Piecewise-linear Kernel Mixed Integer Quadratic Programming (PK-MIQP) formulation introduces a piecewise-linear approximation for Gaussian process kernels and admits a corresponding MIQP representation for acquisition functions. The proposed method is applicable to uncertainty-based acquisition functions for any stationary or dot-product kernel. We analyze the theoretical regret bounds of the proposed approximation, and empirically demonstrate the framework on synthetic functions, constrained benchmarks, and a hyperparameter tuning task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16893v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yilin Xie, Shiqiang Zhang, Joel A. Paulson, Calvin Tsay</dc:creator>
    </item>
    <item>
      <title>Forecasting Outside the Box: Application-Driven Optimal Pointwise Forecasts for Stochastic Optimization</title>
      <link>https://arxiv.org/abs/2411.03520</link>
      <description>arXiv:2411.03520v3 Announce Type: replace 
Abstract: We study a class of two-stage stochastic programs, namely, those with fixed recourse matrix and fixed costs, and linear second stage. We show that, under mild assumptions, the problem can be solved with just one scenario, which we call an ``optimal scenario.'' Such a scenario does not have to be unique and may fall outside the support of the underlying distribution. Although finding an optimal scenario in general might be hard, we show that the result can be particularly useful in the case of stochastic optimization problems with contextual information, where the goal is to optimize the expected value of a certain function given some contextual information (e.g., previous demand, customer type, etc.) that accompany the main data of interest. The contextual information allows for a better estimation of the quantity of interest via machine learning methods. We focus on a class of learning methods -- sometimes called in the literature decision-focused learning -- that integrate the learning and optimization procedures by means of a bilevel optimization formulation, which determines the parameters for pointwise forecasts. By using the optimal scenario result, we prove that when such models are applied to the class of contextual two-stage problems considered in this paper, the pointwise forecasts computed from the bilevel optimization formulation actually yield asymptotically the best approximation of an optimal scenario within the modeler's pre-specified set of parameterized forecast functions. Numerical results conducted with inventory problems from the literature (with synthetic data) as well as a bike-sharing problem with real data demonstrate that the proposed approach performs well when compared to benchmark methods from the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03520v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tito Homem-de-Mello, Juan Valencia, Felipe Lagos, Guido Lagos</dc:creator>
    </item>
    <item>
      <title>Eckstein-Ferris-Pennanen-Robinson duality revisited: paramonotonicity, total Fenchel-Rockafellar duality, and the Chambolle-Pock operator</title>
      <link>https://arxiv.org/abs/2412.11880</link>
      <description>arXiv:2412.11880v2 Announce Type: replace 
Abstract: Finding zeros of the sum of two maximally monotone operators involving a continuous linear operator is a central problem in optimization and monotone operator theory. We revisit the duality framework proposed by Eckstein, Ferris, Pennanen, and Robinson from a quarter of a century ago. Paramonotonicity is identified as a broad condition ensuring that saddle points coincide with the closed convex rectangle formed by the primal and dual solutions. Additionally, we characterize total duality in the subdifferential setting and derive projection formulas for sets that arise in the analysis of the Chambolle-Pock algorithm within the recent framework developed by Bredies, Chenchene, Lorenz, and Naldi.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11880v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/02331934.2025.2577413</arxiv:DOI>
      <arxiv:journal_reference>Bauschke, H. H., Moursi, W. M., &amp; Singh, S. (2025). Eckstein-Ferris-Pennanen-Robinson duality revisited: paramonotonicity, total Fenchel-Rockallar duality, and the Chambolle-Pock operator. Optimization, 1-23</arxiv:journal_reference>
      <dc:creator>Heinz H. Bauschke, Walaa M. Moursi, Shambhavi Singh</dc:creator>
    </item>
    <item>
      <title>Taylor polynomial-based constrained solver for fuel-optimal low-thrust trajectory optimization</title>
      <link>https://arxiv.org/abs/2502.00398</link>
      <description>arXiv:2502.00398v2 Announce Type: replace 
Abstract: This paper presents the differential algebra-based differential dynamic programming (DADDy) solver, a publicly available C++ framework for constrained, fuel-optimal low-thrust trajectory optimization. The method exploits differential algebra (DA) to perform automatic differentiation and provides high-order Taylor polynomial expansions of the dynamics. These expansions replace repeated numerical propagation with polynomial evaluations, significantly reducing computational cost while maintaining accuracy. The solver combines two complementary modules: a fast Differential Dynamic Programming or iterative Linear Quadratic Regulator (DDP/iLQR) scheme that generates an almost-feasible trajectory from arbitrary initial guesses, and a polynomial-based Newton solver that enforces full feasibility with quadratic convergence. The solver accommodates equality and inequality constraints efficiently, while a pseudo-Huber cost function and homotopy continuation enhance convergence robustness for fuel-optimal objectives. The performances of the DADDy solver are assessed through several benchmark cases, including Sun-centered, Earth-Moon, and Earth-centered transfers. Results show that the solver achieves accuracy comparable to state-of-the-art methods while providing substantial computational savings. The most robust configuration (iLQRDyn) converged in all cases, reducing run times by 70% for Sun-centered, 51-88% for Earth-Moon, and 41-55% for Earth-centered problems. When convergence is achieved, the DDP variant attains even faster solutions. These results demonstrate that DA enables a favorable trade-off between robustness and efficiency in second-order optimal control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00398v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Thomas Caleb, Roberto Armellin, Spencer Boone, St\'ephanie Lizy-Destrez</dc:creator>
    </item>
    <item>
      <title>Nonlinear systems and passivity: feedback control, model reduction, and time discretization</title>
      <link>https://arxiv.org/abs/2502.04987</link>
      <description>arXiv:2502.04987v3 Announce Type: replace 
Abstract: Dynamical systems can be used to model a broad class of physical processes, and conservation laws give rise to system properties like passivity or port-Hamiltonian structure. An important problem in practical applications is to steer dynamical systems to prescribed target states, and feedback controllers combining a regulator and an observer are a powerful tool to do so. However, controllers designed using classical methods do not necessarily obey energy principles, which makes it difficult to model the controller-plant interaction in a structured manner. In this paper, we show that the combination of an optimal feedback law characterized by the Hamilton-Jacobi-Bellman equation and output feedback gives rise to passivity properties of the controller that are independent of the plant structure. Furthermore, we state conditions for the controller to have a port-Hamiltonian realization and show that a model order reduction scheme can be deduced using the framework of nonlinear balanced truncation. To illustrate our results, we numerically realize the controller using the policy iteration and computationally verify passivity via a custom passivity-preserving discrete gradient scheme suitable for a wide class of passive systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04987v3</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tobias Breiten, Attila Karsai</dc:creator>
    </item>
    <item>
      <title>An iterative algorithm for the square-root Lasso</title>
      <link>https://arxiv.org/abs/2503.22523</link>
      <description>arXiv:2503.22523v2 Announce Type: replace 
Abstract: In the framework of sparsity-enforcing regularisation for linear inverse problems, we consider the minimisation of a square-root Lasso cost function. To solve this problem we devise a simple modification (called SQRT-ISTA) of the Iterative Soft-Thresholding Algorithm (ISTA) for the Lasso problem and we prove convergence for this algorithm. Under some additional assumptions, we derive an upper bound on the convergence rate of the cost function. We also generalise these results to the case of the group square-root Lasso, where sparsity is enforced for groups of variables instead of individual ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22523v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrizia Boccacci, Christine De Mol, Ignace Loris</dc:creator>
    </item>
    <item>
      <title>Prox-PINNs: A Deep Learning Algorithmic Framework for Elliptic Variational Inequalities</title>
      <link>https://arxiv.org/abs/2505.14430</link>
      <description>arXiv:2505.14430v2 Announce Type: replace 
Abstract: Elliptic variational inequalities (EVIs) present significant challenges in numerical computation due to their inherent non-smoothness, nonlinearity, and inequality formulations. Traditional mesh-based methods often struggle with complex geometries and high computational costs, while existing deep learning approaches lack generality for diverse EVIs. To alleviate these issues, this paper introduces Prox-PINNs, a novel deep learning algorithmic framework that integrates proximal operators with physics-informed neural networks (PINNs) to solve a broad class of EVIs. The Prox-PINNs reformulate EVIs as nonlinear equations using proximal operators and then approximate the solutions via neural networks that enforce boundary conditions as hard constraints. Then the neural networks are trained by minimizing physics-informed residuals. The Prox-PINNs framework advances the state-of-the-art by unifying the treatment of diverse EVIs within a mesh-free and scalable computational architecture. The framework is demonstrated on several prototypical applications, including obstacle problems, elasto-plastic torsion, Bingham visco-plastic flows, and simplified friction problems. Numerical experiments validate the method's accuracy, efficiency, robustness, and flexibility across benchmark examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14430v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Gao, Yongcun Song, Zhiyu Tan, Hangrui Yue, Shangzhi Zeng</dc:creator>
    </item>
    <item>
      <title>Efficient Stochastic BFGS methods Inspired by Bayesian Principles</title>
      <link>https://arxiv.org/abs/2507.07729</link>
      <description>arXiv:2507.07729v2 Announce Type: replace 
Abstract: Quasi-Newton methods are ubiquitous in deterministic local search due to their efficiency and low computational cost. This class of methods uses the history of gradient evaluations to approximate second-order derivatives. However, only noisy gradient observations are accessible in stochastic optimization; thus, deriving quasi-Newton methods in this setting is challenging. Although most existing quasi-Newton methods for stochastic optimization rely on deterministic equations that are modified to circumvent noise, we propose a new approach inspired by Bayesian inference to assimilate noisy gradient information and derive the stochastic counterparts to standard quasi-Newton methods. We focus on the derivations of stochastic BFGS and L-BFGS, but our methodology can also be employed to derive stochastic analogs of other quasi-Newton methods. The resulting stochastic BFGS (S-BFGS) and stochastic L-BFGS (L-S-BFGS) can effectively learn an inverse Hessian approximation even with small batch sizes. For a problem of dimension $d$, the iteration cost of S-BFGS is $\mathcal{O}(d^2)$, and the cost of L-S-BFGS is $\mathcal{O}(d)$. Numerical experiments with a dimensionality of up to $30,720$ demonstrate the efficiency and robustness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07729v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andr\'e Carlon, Luis Espath, Ra\'ul Tempone</dc:creator>
    </item>
    <item>
      <title>Frequency-Domain Analysis of the Euler-Bernoulli and Timoshenko Beams with Attached Masses</title>
      <link>https://arxiv.org/abs/2507.22147</link>
      <description>arXiv:2507.22147v2 Announce Type: replace 
Abstract: This work is focused on the frequency-domain modeling of a simply supported flexible beam with an attached mass in the presence of dissipation. The considered system is equipped with a spring-loaded control actuator and possesses local damping effect. With the help of Hamilton's variational principle, the equations of motion are derived in the state space form with account of interface conditions involving lumped control and local damping. The transfer functions are obtained for Timoshenko and Euler--Bernoulli beam models with the output measurements provided by a sensor. Comparative Bode plots are presented for the two beam models with different choices of damping coefficients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22147v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Zuyev, Julia Kalosha</dc:creator>
    </item>
    <item>
      <title>A Threshold Phenomenon for the Shortest Lattice Vector Problem in the Infinity Norm</title>
      <link>https://arxiv.org/abs/2508.02249</link>
      <description>arXiv:2508.02249v2 Announce Type: replace 
Abstract: One important question in the theory of lattices is to detect a shortest vector: given a norm and a lattice, what is the smallest norm attained by a non-zero vector contained in the lattice? We focus on the infinity norm and work with lattices of the form $A\mathbb{Z}^n$, where $A$ has integer entries and is of full column rank. Finding a shortest vector is NP-hard. We show that this task is fixed parameter tractable in the parameter $\Delta$, the largest absolute value of the determinant of a full rank submatrix of $A$. The algorithm is based on a structural result that can be interpreted as a threshold phenomenon: whenever the dimension $n$ exceeds a certain value determined only by $\Delta$, then a shortest lattice vector attains an infinity norm value of one. This threshold phenomenon has several applications. In particular, it reveals that integer optimal solutions lie on faces of the given polyhedron whose dimensions are bounded only in terms of $\Delta$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02249v2</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefan Kuhlmann, Robert Weismantel</dc:creator>
    </item>
    <item>
      <title>Decentralized Disturbance Rejection Control of Triangularly Coupled Loop Thermosyphon System</title>
      <link>https://arxiv.org/abs/2510.15197</link>
      <description>arXiv:2510.15197v2 Announce Type: replace 
Abstract: In this paper, we investigate the stability of a triangularly coupled triple loop thermosyphon system with momentum and heat exchange at the coupling point as well as the existence of disturbances. The controller consists of a single, local state feedback. From the stability analysis, we obtain explicit bounds on the feedback gains, which depend on the Rayleigh numbers and the momentum coupling parameter, but independent of the thermal coupling parameter. The existence of the stability bounds allows us to design decentralized adaptive controllers to automatically search for the feasible gains when the system parameters are unknown. In the case of existing disturbances in the system, we approximate the disturbances via an extended state observer for the purpose of disturbance rejection. Numerical results are given to demonstrate the performance of the proposed decentralized disturbance rejection controller design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15197v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Novel Kumar Dey, Yan Wu</dc:creator>
    </item>
    <item>
      <title>Online (Non-)Convex Learning via Tempered Optimism</title>
      <link>https://arxiv.org/abs/2301.07530</link>
      <description>arXiv:2301.07530v3 Announce Type: replace-cross 
Abstract: Optimistic Online Learning aims to exploit experts conveying reliable information to predict the future. However, such implicit optimism may be challenged when it comes to practical crafting of such experts. A fundamental example consists in approximating a minimiser of the current problem and use it as expert. In the context of dynamic environments, such an expert only conveys partially relevant information as it may lead to overfitting. To tackle this issue, we introduce in this work the \emph{optimistically tempered} (OT) online learning framework designed to handle such imperfect experts. As a first contribution, we show that tempered optimism is a fruitful paradigm for Online Non-Convex Learning by proposing simple, yet powerful modification of Online Gradient and Mirror Descent. Second, we derive a second OT algorithm for convex losses and third, evaluate the practical efficiency of tempered optimism on real-life datasets and a toy experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.07530v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Maxime Haddouche, Olivier Wintenberger, Benjamin Guedj</dc:creator>
    </item>
    <item>
      <title>A continuous model of transportation in the Heisenberg group</title>
      <link>https://arxiv.org/abs/2406.09380</link>
      <description>arXiv:2406.09380v3 Announce Type: replace-cross 
Abstract: We present a minimization problem with a horizontal divergence-type constraint in the Heisenberg group. Our study explores its dual formulation and examines its relationship with the congested optimal transport problem, for $1 &lt; p &lt; +\infty$, as well as the Monge-Kantorovich problem, in the limite case $p=1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09380v3</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1515/acv-2024-0119</arxiv:DOI>
      <arxiv:journal_reference>Advances in Calculus of Variations, vol. 18, no. 4, 2025, pp. 1223-1251</arxiv:journal_reference>
      <dc:creator>Michele Circelli, Albert Clop</dc:creator>
    </item>
    <item>
      <title>Autonomous Horizon-based Asteroid Navigation With Observability-constrained Maneuvers</title>
      <link>https://arxiv.org/abs/2501.15806</link>
      <description>arXiv:2501.15806v2 Announce Type: replace-cross 
Abstract: Small body exploration is a pertinent challenge due to low gravity environments and strong sensitivity to perturbations like Solar Radiation Pressure (SRP). Thus, autonomous methods are being developed to enable safe navigation and control around small bodies. These methods often involve using Optical Navigation (OpNav) to determine the spacecraft's location. Ensuring OpNav reliability would allow the spacecraft to maintain an accurate state estimate throughout its mission. This research presents an observability-constrained Lyapunov controller that steers a spacecraft to a desired target orbit while guaranteeing continuous OpNav observability. We design observability path constraints to avoid regions where horizon-based OpNav methods exhibit poor performance, ensuring control input that maintains good observability. This controller is implemented with a framework that simulates small body dynamics, synthetic image generation, edge detection, horizon-based OpNav, and filtering. We evaluate the approach in two representative scenarios, orbit maintenance and approach with circularization, around spherical and ellipsoidal target bodies. In Monte Carlo simulations, the proposed approach improves the rate of attaining target orbits without observability violations by up to 94% compared to an unconstrained Lyapunov baseline, demonstrating improved robustness over conventional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15806v2</guid>
      <category>cs.RO</category>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s40295-025-00536-0</arxiv:DOI>
      <arxiv:journal_reference>J Astronaut Sci 72, 56 (2025)</arxiv:journal_reference>
      <dc:creator>Aditya Arjun Anibha, Kenshiro Oguri</dc:creator>
    </item>
    <item>
      <title>Learning Provably Improves the Convergence of Gradient Descent</title>
      <link>https://arxiv.org/abs/2501.18092</link>
      <description>arXiv:2501.18092v5 Announce Type: replace-cross 
Abstract: Learn to Optimize (L2O) trains deep neural network-based solvers for optimization, achieving success in accelerating convex problems and improving non-convex solutions. However, L2O lacks rigorous theoretical backing for its own training convergence, as existing analyses often use unrealistic assumptions -- a gap this work highlights empirically. We bridge this gap by proving the training convergence of L2O models that learn Gradient Descent (GD) hyperparameters for quadratic programming, leveraging the Neural Tangent Kernel (NTK) theory. We propose a deterministic initialization strategy to support our theoretical results and promote stable training over extended optimization horizons by mitigating gradient explosion. Our L2O framework demonstrates over 50% better optimality than GD and superior robustness over state-of-the-art L2O methods on synthetic datasets. The code of our method can be found from https://github.com/NetX-lab/MathL2OProof-Official.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18092v5</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qingyu Song, Wei Lin, Hong Xu</dc:creator>
    </item>
    <item>
      <title>Federated Structured Sparse PCA for Anomaly Detection in IoT Networks</title>
      <link>https://arxiv.org/abs/2503.23981</link>
      <description>arXiv:2503.23981v3 Announce Type: replace-cross 
Abstract: Although federated learning has gained prominence as a privacy-preserving framework tailored for distributed Internet of Things (IoT) environments, current federated principal component analysis (PCA) methods lack integration of sparsity, a critical feature for robust anomaly detection. To address this limitation, we propose a novel federated structured sparse PCA (FedSSP) approach for anomaly detection in IoT networks. The proposed model uniquely integrates double sparsity regularization: (1) row-wise sparsity governed by $\ell_{2,p}$-norm with $p\in [0,1)$ to eliminate redundant feature dimensions, and (2) element-wise sparsity via $\ell_{q}$-norm with $q\in [0,1)$ to suppress noise-sensitive components. To solve this nonconvex problem in a distributed setting, we devise an efficient optimization algorithm based on the proximal alternating minimization (PAM). Numerical experiments validate that incorporating structured sparsity enhances both model interpretability and detection accuracy. Our code is available at https://github.com/xianchaoxiu/FedSSP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23981v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenyi Huang, Xianchao Xiu</dc:creator>
    </item>
    <item>
      <title>Turbocharging Gaussian Process Inference with Approximate Sketch-and-Project</title>
      <link>https://arxiv.org/abs/2505.13723</link>
      <description>arXiv:2505.13723v2 Announce Type: replace-cross 
Abstract: Gaussian processes (GPs) play an essential role in biostatistics, scientific machine learning, and Bayesian optimization for their ability to provide probabilistic predictions and model uncertainty. However, GP inference struggles to scale to large datasets (which are common in modern applications), since it requires the solution of a linear system whose size scales quadratically with the number of samples in the dataset. We propose an approximate, distributed, accelerated sketch-and-project algorithm ($\texttt{ADASAP}$) for solving these linear systems, which improves scalability. We use the theory of determinantal point processes to show that the posterior mean induced by sketch-and-project rapidly converges to the true posterior mean. In particular, this yields the first efficient, condition number-free algorithm for estimating the posterior mean along the top spectral basis functions, showing that our approach is principled for GP inference. $\texttt{ADASAP}$ outperforms state-of-the-art solvers based on conjugate gradient and coordinate descent across several benchmark datasets and a large-scale Bayesian optimization task. Moreover, $\texttt{ADASAP}$ scales to a dataset with $&gt; 3 \cdot 10^8$ samples, a feat which has not been accomplished in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13723v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pratik Rathore, Zachary Frangella, Sachin Garg, Shaghayegh Fazliani, Micha{\l} Derezi\'nski, Madeleine Udell</dc:creator>
    </item>
    <item>
      <title>Structured Reinforcement Learning for Combinatorial Decision-Making</title>
      <link>https://arxiv.org/abs/2505.19053</link>
      <description>arXiv:2505.19053v2 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) is increasingly applied to real-world problems involving complex and structured decisions, such as routing, scheduling, and assortment planning. These settings challenge standard RL algorithms, which struggle to scale, generalize, and exploit structure in the presence of combinatorial action spaces. We propose Structured Reinforcement Learning (SRL), a novel actor-critic paradigm that embeds combinatorial optimization-layers into the actor neural network. We enable end-to-end learning of the actor via Fenchel-Young losses and provide a geometric interpretation of SRL as a primal-dual algorithm in the dual of the moment polytope. Across six environments with exogenous and endogenous uncertainty, SRL matches or surpasses the performance of unstructured RL and imitation learning on static tasks and improves over these baselines by up to 92% on dynamic problems, with improved stability and convergence speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19053v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heiko Hoppe, L\'eo Baty, Louis Bouvier, Axel Parmentier, Maximilian Schiffer</dc:creator>
    </item>
    <item>
      <title>Linear regression with overparameterized linear neural networks: Tight upper and lower bounds for implicit $\ell^1$-regularization</title>
      <link>https://arxiv.org/abs/2506.01143</link>
      <description>arXiv:2506.01143v2 Announce Type: replace-cross 
Abstract: Modern machine learning models are often trained in a setting where the number of parameters exceeds the number of training samples. To understand the implicit bias of gradient descent in such overparameterized models, prior work has studied diagonal linear neural networks in the regression setting. These studies have shown that, when initialized with small weights, gradient descent tends to favor solutions with minimal $\ell^1$-norm - an effect known as implicit regularization. In this paper, we investigate implicit regularization in diagonal linear neural networks of depth $D\ge 2$ for overparameterized linear regression problems. We focus on analyzing the approximation error between the limit point of gradient flow trajectories and the solution to the $\ell^1$-minimization problem. By deriving tight upper and lower bounds on the approximation error, we precisely characterize how the approximation error depends on the scale of initialization $\alpha$. Our results reveal a qualitative difference between depths: for $D \ge 3$, the error decreases linearly with $\alpha$, whereas for $D=2$, it decreases at rate $\alpha^{1-\varrho}$, where the parameter $\varrho \in [0,1)$ can be explicitly characterized. Interestingly, this parameter is closely linked to so-called null space property constants studied in the sparse recovery literature. We demonstrate the asymptotic tightness of our bounds through explicit examples. Numerical experiments corroborate our theoretical findings and suggest that deeper networks, i.e., $D \ge 3$, may lead to better generalization, particularly for realistic initialization scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01143v2</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hannes Matt, Dominik St\"oger</dc:creator>
    </item>
    <item>
      <title>Integral bases, perfect matchings, and the Petersen graph</title>
      <link>https://arxiv.org/abs/2508.15602</link>
      <description>arXiv:2508.15602v2 Announce Type: replace-cross 
Abstract: Let $G=(V,E)$ be a matching-covered graph, denote by $P$ its perfect matching polytope, and by $L$ the integer lattice generated by the integral points in $P$. In this paper, we give short, polyhedral proofs for two difficult results established by Lov\'{a}sz (1987), and by Carvalho, Lucchesi, and Murty (2002) in a series of three papers totaling over 120 pages. More specifically, we prove that $L$ has a lattice basis consisting solely of incidence vectors of some perfect matchings of $G$, $2x\in L$ for all $x\in \mathrm{lin}(P)\cap \mathbb{Z}^E$, and if $G$ has no Petersen brick then $L = \mathrm{lin}(P)\cap \mathbb{Z}^E$. Our proof avoids major technical aspects of the previous proofs, the most important of these being a characterization of the dual lattice, and a `Petersen-brick-sensitive' ear-decomposition result for matching-covered graphs. This is achieved by a novel study of the facial structure of the polytope $P$ and its relationship with the lattice $L$. Along the way, we give a new polyhedral characterization of the Petersen graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15602v2</guid>
      <category>math.CO</category>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ahmad Abdi, Olha Silina</dc:creator>
    </item>
    <item>
      <title>Schr\"odinger bridge for generative AI: Soft-constrained formulation and convergence analysis</title>
      <link>https://arxiv.org/abs/2510.11829</link>
      <description>arXiv:2510.11829v2 Announce Type: replace-cross 
Abstract: Generative AI can be framed as the problem of learning a model that maps simple reference measures into complex data distributions, and it has recently found a strong connection to the classical theory of the Schr\"odinger bridge problems (SBPs) due partly to their common nature of interpolating between prescribed marginals via entropy-regularized stochastic dynamics. However, the classical SBP enforces hard terminal constraints, which often leads to instability in practical implementations, especially in high-dimensional or data-scarce regimes. To address this challenge, we follow the idea of the so-called soft-constrained Schr\"odinger bridge problem (SCSBP), in which the terminal constraint is replaced by a general penalty function. This relaxation leads to a more flexible stochastic control formulation of McKean-Vlasov type.
  We establish the existence of optimal solutions for all penalty levels and prove that, as the penalty grows, both the controls and value functions converge to those of the classical SBP at a linear rate. Our analysis builds on Doob's h-transform representations, the stability results of Schr\"odinger potentials, Gamma-convergence, and a novel fixed-point argument that couples an optimization problem over the space of measures with an auxiliary entropic optimal transport problem. These results not only provide the first quantitative convergence guarantees for soft-constrained bridges but also shed light on how penalty regularization enables robust generative modeling, fine-tuning, and transfer learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11829v2</guid>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <category>q-fin.MF</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jin Ma, Ying Tan, Renyuan Xu</dc:creator>
    </item>
    <item>
      <title>An Augmented Lagrangian Method-Based Framework in the Adjoint Space for Sparse Reconstruction of Acoustic Sources</title>
      <link>https://arxiv.org/abs/2510.14805</link>
      <description>arXiv:2510.14805v3 Announce Type: replace-cross 
Abstract: We propose a semismooth Newton-based augmented Lagrangian framework for reconstructing sparse sources in inverse acoustic scattering problems. Rather than working in the unknown source space, our semismooth Newton updates operate in the measurement (adjoint) space, which is especially efficient when the number of measurements is much smaller than the discretized source dimension. The source is then recovered via Fenchel-Rockafellar duality. Our approach substantially accelerates computation and reduces costs. Numerical experiments in two and three dimensions demonstrate the high efficiency of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14805v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nirui Tan, Hongpeng Sun</dc:creator>
    </item>
    <item>
      <title>Assignment-Routing Optimization with Cutting-Plane Subtour Elimination: Solver and Benchmark Dataset</title>
      <link>https://arxiv.org/abs/2510.17888</link>
      <description>arXiv:2510.17888v2 Announce Type: replace-cross 
Abstract: We study a joint routing-assignment optimization problem in which a set of items must be paired one-to-one with a set of placeholders while simultaneously determining a Hamiltonian cycle that visits every node exactly once. Both the assignment and routing decisions are optimized jointly to minimize the total travel cost. In this work, we propose a method to solve this problem using an exact MIP formulation with Gurobi, including cutting-plane subtour elimination. With analysis of the computational complexity and through extensive experiments, we analyze the computational limitations of this approach as the problem size grows and reveal the challenges associated with the need for more efficient algorithms for larger instances. The dataset, formulations, and experimental results provided here can serve as benchmarks for future studies in this research area. GitHub repository: https://github.com/QL-YUAN/Joint-Assignment-Routing-Optimization</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17888v2</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qilong Yuan</dc:creator>
    </item>
    <item>
      <title>MARS-M: When Variance Reduction Meets Matrices</title>
      <link>https://arxiv.org/abs/2510.21800</link>
      <description>arXiv:2510.21800v2 Announce Type: replace-cross 
Abstract: Matrix-based preconditioned optimizers, such as Muon, have recently been shown to be more efficient than scalar-based optimizers for training large-scale neural networks, including large language models (LLMs). On the other hand, recent benchmarks on optimizers for LLM pre-training have demonstrated that variance-reduction techniques such as MARS can achieve substantial speedups over standard optimizers that do not employ variance reduction. In this paper, to achieve the best of both worlds, we introduce MARS-M, a new optimizer that integrates the variance reduction technique in MARS with Muon. Under standard regularity conditions, we prove that Muon-M converges to a first-order stationary point at a rate of $\tilde{\mathcal{O}}(T^{-1/3})$, which improves upon $\tilde{\mathcal{O}}(T^{-1/4})$ rate attained by Muon. Our empirical results on language modeling and computer vision tasks demonstrate that MARS-M consistently yields lower losses and improved performance across various downstream benchmarks. The implementation of MARS-M is available at https://github.com/AGI-Arena/MARS/tree/main/MARS_M.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21800v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifeng Liu, Angela Yuan, Quanquan Gu</dc:creator>
    </item>
  </channel>
</rss>
