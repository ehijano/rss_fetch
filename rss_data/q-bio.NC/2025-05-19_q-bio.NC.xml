<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 May 2025 02:51:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>From Artificial Intelligence to Active Inference: The Key to True AI and 6G World Brain [Invited]</title>
      <link>https://arxiv.org/abs/2505.10569</link>
      <description>arXiv:2505.10569v1 Announce Type: new 
Abstract: In his opening OFC plenary talk back in 2021, Alibaba Group's Yiqun Cai notably added in the follow-up Q&amp;A that today's complex networks are more than computer science - they grow, they are life. This entails that future networks may be better viewed as techno-social systems that resemble biological superorganisms with brain-like cognitive capabilities. Fast-forwarding, there is now growing awareness that we have to completely change our networks from being static into being a living entity that would act as an AI-powered network `brain', as recently stated by Bruno Zerbib, Chief Technology and Innovation Officer of France's Orange, at the Mobile World Congress (MWC) 2025. Even though AI was front and center at both MWC and OFC 2025 and has been widely studied in the context of optical networks, there are currently no publications on active inference in optical (and less so mobile) networks available. Active inference is an ideal methodology for developing more advanced AI systems by biomimicking the way living intelligent systems work, while overcoming the limitations of today's AI related to training, learning, and explainability. Active inference is considered the key to true AI: Less artificial, more intelligent. The goal of this paper is twofold. First, we aim at enabling optical network researchers to conceptualize new research lines for future optical networks with human-AI interaction capabilities by introducing them to the main mathematical concepts of the active inference framework. Second, we demonstrate how to move AI research beyond the human brain toward the 6G world brain by exploring the role of mycorrhizal networks, the largest living organism on planet Earth, in the AI vision and R&amp;D roadmap for the next decade and beyond laid out by Karl Friston, the father of active inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10569v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Maier</dc:creator>
    </item>
    <item>
      <title>LLMs Do Not Have Human-Like Working Memory</title>
      <link>https://arxiv.org/abs/2505.10571</link>
      <description>arXiv:2505.10571v1 Announce Type: new 
Abstract: Human working memory is an active cognitive system that enables not only the temporary storage of information but also its processing and utilization. Without working memory, individuals may produce unreal conversations, exhibit self-contradiction, and struggle with tasks requiring mental reasoning. In this paper, we demonstrate that Large Language Models (LLMs) lack this human-like cognitive ability, posing a significant challenge to achieving artificial general intelligence. We validate this claim through three experiments: (1) Number Guessing Game, (2) Yes or No Game, and (3) Math Magic. Experimental results on several model families indicate that current LLMs fail to exhibit human-like cognitive behaviors in these scenarios. By highlighting this limitation, we aim to encourage further research in developing LLMs with improved working memory capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10571v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jen-tse Huang, Kaiser Sun, Wenxuan Wang, Mark Dredze</dc:creator>
    </item>
    <item>
      <title>Decomposing stimulus-specific sensory neural information via diffusion models</title>
      <link>https://arxiv.org/abs/2505.11309</link>
      <description>arXiv:2505.11309v1 Announce Type: new 
Abstract: To understand sensory coding, we must ask not only how much information neurons encode, but also what that information is about. This requires decomposing mutual information into contributions from individual stimuli and stimulus features fundamentally ill-posed problem with infinitely many possible solutions. We address this by introducing three core axioms, additivity, positivity, and locality that any meaningful stimulus-wise decomposition should satisfy. We then derive a decomposition that meets all three criteria and remains tractable for high-dimensional stimuli. Our decomposition can be efficiently estimated using diffusion models, allowing for scaling up to complex, structured and naturalistic stimuli. Applied to a model of visual neurons, our method quantifies how specific stimuli and features contribute to encoded information. Our approach provides a scalable, interpretable tool for probing representations in both biological and artificial neural systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11309v1</guid>
      <category>q-bio.NC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Steeve Laquitaine, Simone Azeglio, Carlo Paris, Ulisse Ferrari, Matthew Chalk</dc:creator>
    </item>
    <item>
      <title>Evolution imposes an inductive bias that alters and accelerates learning dynamics</title>
      <link>https://arxiv.org/abs/2505.10651</link>
      <description>arXiv:2505.10651v1 Announce Type: cross 
Abstract: The learning dynamics of biological brains and artificial neural networks are of interest to both neuroscience and machine learning. A key difference between them is that neural networks are often trained from a randomly initialized state whereas each brain is the product of generations of evolutionary optimization, yielding innate structures that enable few-shot learning and inbuilt reflexes. Artificial neural networks, by contrast, require non-ethological quantities of training data to attain comparable performance. To investigate the effect of evolutionary optimization on the learning dynamics of neural networks, we combined algorithms simulating natural selection and online learning to produce a method for evolutionarily conditioning artificial neural networks, and applied it to both reinforcement and supervised learning contexts. We found the evolutionary conditioning algorithm, by itself, performs comparably to an unoptimized baseline. However, evolutionarily conditioned networks show signs of unique and latent learning dynamics, and can be rapidly fine-tuned to optimal performance. These results suggest evolution constitutes an inductive bias that tunes neural systems to enable rapid learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10651v1</guid>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Midler, Alejandro Pan Vazquez</dc:creator>
    </item>
    <item>
      <title>ROIsGAN: A Region Guided Generative Adversarial Framework for Murine Hippocampal Subregion Segmentation</title>
      <link>https://arxiv.org/abs/2505.10687</link>
      <description>arXiv:2505.10687v1 Announce Type: cross 
Abstract: The hippocampus, a critical brain structure involved in memory processing and various neurodegenerative and psychiatric disorders, comprises three key subregions: the dentate gyrus (DG), Cornu Ammonis 1 (CA1), and Cornu Ammonis 3 (CA3). Accurate segmentation of these subregions from histological tissue images is essential for advancing our understanding of disease mechanisms, developmental dynamics, and therapeutic interventions. However, no existing methods address the automated segmentation of hippocampal subregions from tissue images, particularly from immunohistochemistry (IHC) images. To bridge this gap, we introduce a novel set of four comprehensive murine hippocampal IHC datasets featuring distinct staining modalities: cFos, NeuN, and multiplexed stains combining cFos, NeuN, and either {\Delta}FosB or GAD67, capturing structural, neuronal activity, and plasticity associated information. Additionally, we propose ROIsGAN, a region-guided U-Net-based generative adversarial network tailored for hippocampal subregion segmentation. By leveraging adversarial learning, ROIsGAN enhances boundary delineation and structural detail refinement through a novel region-guided discriminator loss combining Dice and binary cross-entropy loss. Evaluated across DG, CA1, and CA3 subregions, ROIsGAN consistently outperforms conventional segmentation models, achieving performance gains ranging from 1-10% in Dice score and up to 11% in Intersection over Union (IoU), particularly under challenging staining conditions. Our work establishes foundational datasets and methods for automated hippocampal segmentation, enabling scalable, high-precision analysis of tissue images in neuroscience research. Our generated datasets, proposed model as a standalone tool, and its corresponding source code are publicly available at: https://github.com/MehediAzim/ROIsGAN</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10687v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sayed Mehedi Azim, Brian Corbett, Iman Dehzangi</dc:creator>
    </item>
    <item>
      <title>The Way We Prompt: Conceptual Blending, Neural Dynamics, and Prompt-Induced Transitions in LLMs</title>
      <link>https://arxiv.org/abs/2505.10948</link>
      <description>arXiv:2505.10948v1 Announce Type: cross 
Abstract: Large language models (LLMs), inspired by neuroscience, exhibit behaviors that often evoke a sense of personality and intelligence-yet the mechanisms behind these effects remain elusive. Here, we operationalize Conceptual Blending Theory (CBT) as an experimental framework, using prompt-based methods to reveal how LLMs blend and compress meaning. By systematically investigating Prompt-Induced Transitions (PIT) and Prompt-Induced Hallucinations (PIH), we uncover structural parallels and divergences between artificial and biological cognition. Our approach bridges linguistics, neuroscience, and empirical AI research, demonstrating that human-AI collaboration can serve as a living prototype for the future of cognitive science. This work proposes prompt engineering not just as a technical tool, but as a scientific method for probing the deep structure of meaning itself.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10948v1</guid>
      <category>cs.CL</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Makoto Sato</dc:creator>
    </item>
    <item>
      <title>Fractal geometry predicts dynamic differences in structural and functional connectomes</title>
      <link>https://arxiv.org/abs/2505.11477</link>
      <description>arXiv:2505.11477v1 Announce Type: cross 
Abstract: Understanding the intricate architecture of brain networks and its connection to brain function is essential for deciphering the underlying principles of cognition and disease.
  While traditional graph-theoretical measures have been widely used to characterize these networks, they often fail to fully capture the emergent properties of large-scale neural dynamics. Here, we introduce an alternative approach to quantify brain networks that is rooted in complex dynamics, fractal geometry, and asymptotic analysis.
  We apply these concepts to brain connectomes and demonstrate how quadratic iterations and geometric properties of Mandelbrot-like sets can provide novel insights into structural and functional network dynamics. Our findings reveal fundamental distinctions between structural (positive) and functional (signed) connectomes, such as the shift of cusp orientation and the variability in equi-M set geometry. Notably, structural connectomes exhibit more robust, predictable features, while functional connectomes show increased variability for non-trivial tasks. We further demonstrate that traditional graph-theoretical measures, when applied separately to the positive and negative sub-networks of functional connectomes, fail to fully capture their dynamic complexity. Instead, size and shape-based invariants of the equi-M set effectively differentiate between rest and emotional task states, which highlights their potential as superior markers of emergent network dynamics. These results suggest that incorporating fractal-based methods into network neuroscience provides a powerful tool for understanding how information flows in natural systems beyond static connectivity measures, while maintaining their simplicity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11477v1</guid>
      <category>nlin.CD</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anca Radulescu, Eva Kaslik, Alexandry Fikl, Johan Nakuci, Sarah Muldoon, Michael Anderson</dc:creator>
    </item>
    <item>
      <title>Discriminating image representations with principal distortions</title>
      <link>https://arxiv.org/abs/2410.15433</link>
      <description>arXiv:2410.15433v2 Announce Type: replace 
Abstract: Image representations (artificial or biological) are often compared in terms of their global geometric structure; however, representations with similar global structure can have strikingly different local geometries. Here, we propose a framework for comparing a set of image representations in terms of their local geometries. We quantify the local geometry of a representation using the Fisher information matrix, a standard statistical tool for characterizing the sensitivity to local stimulus distortions, and use this as a substrate for a metric on the local geometry in the vicinity of a base image. This metric may then be used to optimally differentiate a set of models, by finding a pair of "principal distortions" that maximize the variance of the models under this metric. As an example, we use this framework to compare a set of simple models of the early visual system, identifying a novel set of image distortions that allow immediate comparison of the models by visual inspection. In a second example, we apply our method to a set of deep neural network models and reveal differences in the local geometry that arise due to architecture and training types. These examples demonstrate how our framework can be used to probe for informative differences in local sensitivities between complex models, and suggest how it could be used to compare model representations with human perception.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15433v2</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Int'l Conf on Learning Representations (ICLR), vol 13, Singapore, May 2025</arxiv:journal_reference>
      <dc:creator>Jenelle Feather, David Lipshutz, Sarah E. Harvey, Alex H. Williams, Eero P. Simoncelli</dc:creator>
    </item>
    <item>
      <title>Orientation selectivity properties for integrated affine quasi quadrature models of complex cells</title>
      <link>https://arxiv.org/abs/2503.21611</link>
      <description>arXiv:2503.21611v5 Announce Type: replace 
Abstract: This paper presents an analysis of the orientation selectivity properties of idealized models of complex cells in terms of affine quasi quadrature measures, which combine the responses of idealized models of simple cells in terms of affine Gaussian derivatives by (i) pointwise squaring, (ii) summation of responses for different orders of spatial derivation and (iii) spatial integration. Specifically, this paper explores the consequences of assuming that the family of spatial receptive fields should be covariant under spatial affine transformations, thereby implying that the receptive fields ought to span a variability over the degree of elongation.
  We investigate the theoretical properties of three main ways of defining idealized models of complex cells and compare the predictions from these models to neurophysiologically obtained receptive field histograms over the resultant of biological orientation selectivity curves. It is shown that the extended modelling mechanism lead to more uniform behaviour and a wider span over the values of the resultat that are covered, compared to earlier presented idealized models of complex cells without spatial integration.
  More generally, we propose (i) to include a variability over the degree of elongation of the receptive fields in functional models of complex cells, and that (ii) the presented methodology with comparisons to biological orientation selectivity curves and orientation selectivity histograms could be used as a new tool to evaluate other computational models of complex cells in relation to biological measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21611v5</guid>
      <category>q-bio.NC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tony Lindeberg</dc:creator>
    </item>
    <item>
      <title>Wavelet Analysis of Noninvasive EEG Signals Discriminates Complex and Natural Grasp Types</title>
      <link>https://arxiv.org/abs/2402.09447</link>
      <description>arXiv:2402.09447v2 Announce Type: replace-cross 
Abstract: This research aims to decode hand grasps from Electroencephalograms (EEGs) for dexterous neuroprosthetic development and Brain-Computer Interface (BCI) applications, especially for patients with motor disorders. Particularly, it focuses on distinguishing two complex natural power and precision grasps in addition to a neutral condition as a no-movement condition using a new EEG-based BCI platform and wavelet signal processing. Wavelet analysis involved generating time-frequency and topographic maps from wavelet power coefficients. Then, by using machine learning techniques with novel wavelet features, we achieved high average accuracies: 85.16% for multiclass, 95.37% for No-Movement vs Power, 95.40% for No-Movement vs Precision, and 88.07% for Power vs Precision, demonstrating the effectiveness of these features in EEG-based grasp differentiation. In contrast to previous studies, a critical part of our study was permutation feature importance analysis, which highlighted key features for grasp classification. It revealed that the most crucial brain activities during grasping occur in the motor cortex, within the alpha and beta frequency bands. These insights demonstrate the potential of wavelet features in real-time neuroprosthetic technology and BCI applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09447v2</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/EMBC53108.2024.10782674</arxiv:DOI>
      <dc:creator>Ali Rabiee, Sima Ghafoori, Anna Cetera, Reza Abiri</dc:creator>
    </item>
    <item>
      <title>Brain-like variational inference</title>
      <link>https://arxiv.org/abs/2410.19315</link>
      <description>arXiv:2410.19315v2 Announce Type: replace-cross 
Abstract: Inference in both brains and machines can be formalized by optimizing a shared objective: maximizing the evidence lower bound (ELBO) in machine learning, or minimizing variational free energy (F) in neuroscience (ELBO = -F). While this equivalence suggests a unifying framework, it leaves open how inference is implemented in neural systems. Here, we show that online natural gradient descent on F, under Poisson assumptions, leads to a recurrent spiking neural network that performs variational inference via membrane potential dynamics. The resulting model -- the iterative Poisson variational autoencoder (iP-VAE) -- replaces the encoder network with local updates derived from natural gradient descent on F. Theoretically, iP-VAE yields a number of desirable features such as emergent normalization via lateral competition, and hardware-efficient integer spike count representations. Empirically, iP-VAE outperforms both standard VAEs and Gaussian-based predictive coding models in sparsity, reconstruction, and biological plausibility. iP-VAE also exhibits strong generalization to out-of-distribution inputs, exceeding hybrid iterative-amortized VAEs. These results demonstrate how deriving inference algorithms from first principles can yield concrete architectures that are simultaneously biologically plausible and empirically effective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19315v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hadi Vafaii, Dekel Galor, Jacob L. Yates</dc:creator>
    </item>
  </channel>
</rss>
