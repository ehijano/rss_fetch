<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 Jan 2026 02:34:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A $O^*((2 + \epsilon)^k)$ Time Algorithm for Cograph Deletion Using Unavoidable Subgraphs in Large Prime Graphs</title>
      <link>https://arxiv.org/abs/2601.02532</link>
      <description>arXiv:2601.02532v1 Announce Type: new 
Abstract: We study the parameterized complexity of the Cograph Deletion problem, which asks whether one can delete at most $k$ edges from a graph to make it $P_4$-free. This is a well-known graph modification problem with applications in computation biology and social network analysis.
  All current parameterized algorithms use a similar strategy, which is to find a $P_4$ and explore the local structure around it to perform an efficient recursive branching.
  The best known algorithm achieves running time $O^*(2.303^k)$ and requires an automated search of the branching cases due to their complexity.
  Since it appears difficult to further improve the current strategy, we devise a new approach using modular decompositions. We solve each module and the quotient graph independently, with the latter being the core problem. This reduces the problem to solving on a prime graph, in which all modules are trivial. We then use a characterization of Chudnovsky et al. stating that any large enough prime graph has one of seven structures as an induced subgraph. These all have many $P_4$s, with the quantity growing linearly with the graph size, and we show that these allow a recursive branch tree algorithm to achieve running time $O^*((2 + \epsilon)^k)$ for any $\epsilon &gt; 0$.
  This appears to be the first algorithmic application of the prime graph characterization and it could be applicable to other modification problems. Towards this goal, we provide the exact set of graph classes $\H$ for which the $\H$-free editing problem can make use of our reduction to a prime graph, opening the door to improvements for other modification problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02532v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuel Lafond, Francis Sarrazin</dc:creator>
    </item>
    <item>
      <title>A Practical 73/50 Approximation for Contiguous Monotone Moldable Job Scheduling</title>
      <link>https://arxiv.org/abs/2601.02836</link>
      <description>arXiv:2601.02836v1 Announce Type: new 
Abstract: In moldable job scheduling, we are provided $m$ identical machines and $n$ jobs that can be executed on a variable number of machines. The execution time of each job depends on the number of machines assigned to execute that job. For the specific problem of monotone moldable job scheduling, jobs are assumed to have a processing time that is non-increasing in the number of machines.
  The previous best-known algorithms are: (1) a polynomial-time approximation scheme with time complexity $\Omega(n^{g(1/\varepsilon)})$, where $g(\cdot)$ is a super-exponential function [Jansen and Th\"ole '08; Jansen and Land '18], (2) a fully polynomial approximation scheme for the case of $m \geq 8\frac{n}{\varepsilon}$ [Jansen and Land '18], and (3) a $\frac{3}{2}$ approximation with time complexity $O(nm\log(mn))$ [Wu, Zhang, and Chen '23].
  We present a new practically efficient algorithm with an approximation ratio of $\approx (1.4593 + \varepsilon)$ and a time complexity of $O(nm \log \frac{1}{\varepsilon})$. Our result also applies to the contiguous variant of the problem. In addition to our theoretical results, we implement the presented algorithm and show that the practical performance is significantly better than the theoretical worst-case approximation ratio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02836v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Klaus Jansen, Felix Ohnesorge</dc:creator>
    </item>
    <item>
      <title>Hardness of Regular Expression Matching with Extensions</title>
      <link>https://arxiv.org/abs/2601.03020</link>
      <description>arXiv:2601.03020v1 Announce Type: new 
Abstract: The regular expression matching problem asks whether a given regular expression of length $m$ matches a given string of length $n$. As is well known, the problem can be solved in $O(nm)$ time using Thompson's algorithm. Moreover, recent studies have shown that the matching problem for regular expressions extended with a practical extension called lookaround can be solved in the same time complexity. In this work, we consider three well-known extensions to regular expressions called backreference, intersection and complement, and we show that, unlike in the case of lookaround, the matching problem for regular expressions extended with any of the three (for backreference, even when restricted to one capturing group) cannot be solved in $O(n^{2-\varepsilon} \mathrm{poly}(m))$ time for any constant $\varepsilon &gt; 0$ under the Orthogonal Vectors Conjecture. Moreover, we study the matching problem for regular expressions extended with complement in more detail, which is also known as extended regular expression (ERE) matching. We show that there is no ERE matching algorithm that runs in $O(n^{\omega-\varepsilon} \mathrm{poly}(m))$ time ($2 \le \omega &lt; 2.3716$ is the exponent of square matrix multiplication) for any constant $\varepsilon &gt; 0$ under the $k$-Clique Hypothesis, and there is no combinatorial ERE matching algorithm that runs in $O(n^{3-\varepsilon} \mathrm{poly}(m))$ time for any constant $\varepsilon &gt; 0$ under the Combinatorial $k$-Clique Hypothesis. This shows that the $O(n^3 m)$-time algorithm introduced by Hopcroft and Ullman in 1979 and recently improved by Bille et al. to run in $O(n^\omega m)$ time using fast matrix multiplication was already optimal in a sense, and sheds light on why the theoretical computer science community has struggled to improve the time complexity of ERE matching with respect to $n$ and $m$ for more than 45 years.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03020v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.FL</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taisei Nogami, Tachio Terauchi</dc:creator>
    </item>
    <item>
      <title>Density Matters: A Complexity Dichotomy of Deleting Edges to Bound Subgraph Density</title>
      <link>https://arxiv.org/abs/2601.03129</link>
      <description>arXiv:2601.03129v1 Announce Type: new 
Abstract: We study $\tau$-Bounded-Density Edge Deletion ($\tau$-BDED), where given an undirected graph $G$, the task is to remove as few edges as possible to obtain a graph $G'$ where no subgraph of $G'$ has density more than $\tau$. The density of a (sub)graph is the number of edges divided by the number of vertices. This problem was recently introduced and shown to be NP-hard for $\tau \in \{2/3, 3/4, 1 + 1/25\}$, but polynomial-time solvable for $\tau \in \{0,1/2,1\}$ [Bazgan et al., JCSS 2025]. We provide a complete dichotomy with respect to the target density $\tau$:
  1. If $2\tau \in \mathbb{N}$ (half-integral target density) or $\tau &lt; 2/3$, then $\tau$-BDED is polynomial-time solvable.
  2. Otherwise, $\tau$-BDED is NP-hard.
  We complement the NP-hardness with fixed-parameter tractability with respect to the treewidth of $G$. Moreover, for integral target density $\tau \in \mathbb{N}$, we show $\tau$-BDED to be solvable in randomized $O(m^{1 + o(1)})$ time. Our algorithmic results are based on a reduction to a new general flow problem on restricted networks that, depending on $\tau$, can be solved via Maximum s-t-Flow or General Factors. We believe this connection between these variants of flow and matching to be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03129v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Bentert, Tom-Lukas Breitkopf, Vincent Froese, Anton Herrmann, Andr\'e Nichterlein</dc:creator>
    </item>
    <item>
      <title>Scalable Tree Ensemble Proximities in Python</title>
      <link>https://arxiv.org/abs/2601.02735</link>
      <description>arXiv:2601.02735v1 Announce Type: cross 
Abstract: Tree ensemble methods such as Random Forests naturally induce supervised similarity measures through their decision tree structure, but existing implementations of proximities derived from tree ensembles typically suffer from quadratic time or memory complexity, limiting their scalability. In this work, we introduce a general framework for efficient proximity computation by defining a family of Separable Weighted Leaf-Collision Proximities. We show that any proximity measure in this family admits an exact sparse matrix factorization, restricting computation to leaf-level collisions and avoiding explicit pairwise comparisons. This formulation enables low-memory, scalable proximity computation using sparse linear algebra in Python. Empirical benchmarks demonstrate substantial runtime and memory improvements over traditional approaches, allowing tree ensemble proximities to scale efficiently to datasets with hundreds of thousands of samples on standard CPU hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02735v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>cs.PF</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Adrien Aumon, Guy Wolf, Kevin R. Moon, Jake S. Rhodes</dc:creator>
    </item>
    <item>
      <title>Constant-Factor Algorithms for Revenue Management with Consecutive Stays</title>
      <link>https://arxiv.org/abs/2506.00909</link>
      <description>arXiv:2506.00909v2 Announce Type: replace-cross 
Abstract: We study network revenue management problems motivated by applications such as railway ticket sales and hotel room bookings. Requests, each requiring a resource for a consecutive stay, arrive sequentially with known arrival probabilities. We investigate two scenarios: the accept-or-reject scenario, where a request can be fulfilled by assigning any available resource; and the BAM-based scenario, which generalizes the former by incorporating customer preferences through the basic attraction model (BAM), allowing the platform to offer an assortment of available resources from which the customer may choose. We develop polynomial-time policies and evaluate their performance using approximation ratios, defined as the ratio between the expected revenue of our policy and that of the optimal online algorithm. When each arrival has a fixed request type (e.g., the interval of the stay is fixed), we establish constant-factor guarantees: a ratio of 1 - 1/e for the accept-or-reject scenario and 0.25 for the BAM-based scenario. We further extend these results to the case where the request type is random (e.g., the interval of the stay is random). In this setting, the approximation ratios incur an additional multiplicative factor of 1 - 1/e, resulting in guarantees of at least 0.399 for the accept-or-reject scenario and 0.156 for the BAM-based scenario. These constant-factor guarantees stand in sharp contrast to the prior nonconstant competitive ratios that are benchmarked against the offline optimum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00909v2</guid>
      <category>econ.TH</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ming Hu, Tongwen Wu</dc:creator>
    </item>
    <item>
      <title>Constructive l2-Discrepancy Minimization with Additive Deviations</title>
      <link>https://arxiv.org/abs/2508.21423</link>
      <description>arXiv:2508.21423v3 Announce Type: replace-cross 
Abstract: The \emph{signed series} problem in the $\ell_2$ norm asks, given set of vectors $v_1,\ldots,v_n\in \mathbf{R}^d$ having at most unit $\ell_2$ norm, does there always exist a series $(\varepsilon_i)_{i\in [n]}$ of $\pm 1$ signs such that for all $i\in [n]$, $\max_{i\in [n]} \|\sum_{j=1}^i \varepsilon_i v_i\|_2 = O(\sqrt{d})$. A result of Banaszczyk [2012, \emph{Rand. Struct. Alg.}] states that there exist signs $\varepsilon_i\in \{-1,1\},\; i\in [n]$ such that $\max_{i\in [n]} \|\sum_{j=1}^i \varepsilon_i v_i\|_2 = O(\sqrt{d+\log n})$. The best constructive bound known so far is of $O(\sqrt{d\log n})$, by Bansal and Garg [2017, \emph{STOC.}, 2019, \emph{SIAM J. Comput.}]. We give a polynomial-time randomized algorithm to find signs $x(i) \in \{-1,1\},\; i\in [n]$ such that \[ \max_{i\in [n]} \|\sum_{j=1}^i x(i)v_i\|_2 = O(\sqrt{d + \log^2 n}) = O(\sqrt{d}+\log n).\] By the constructive reduction of Harvey and Samadi [\emph{COLT}, 2014], this also yields a constructive bound of $O(\sqrt{d}+\log n)$ for the Steinitz problem in the $\ell_2$-norm. Thus, we algorithmically achieve Banaszczyk's bounds for both problems when $d \geq \log^2n$, which also matches the conjectured bounds. Our algorithm is based on the framework on Bansal and Garg, together with a new analysis involving $(i)$ additional linear and spectral orthogonality constraints during the construction of the covariance matrix of the random walk steps, which allow us to control the quadratic variation in the linear as well as the quadratic components of the discrepancy increment vector, alongwith $(ii)$ a ``Freedman-like" version of the Hanson-Wright concentration inequality, for filtration-dependent sums of subgaussian chaoses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21423v3</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kunal Dutta</dc:creator>
    </item>
  </channel>
</rss>
