<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Jan 2025 05:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Adaptive stratified Monte Carlo using decision trees</title>
      <link>https://arxiv.org/abs/2501.04842</link>
      <description>arXiv:2501.04842v1 Announce Type: new 
Abstract: It has been known for a long time that stratification is one possible strategy to obtain higher convergence rates for the Monte Carlo estimation of integrals over the hyper-cube $[0, 1]^s$ of dimension $s$. However, stratified estimators such as Haber's are not practical as $s$ grows, as they require $\mathcal{O}(k^s)$ evaluations for some $k\geq 2$. We propose an adaptive stratification strategy, where the strata are derived from a a decision tree applied to a preliminary sample. We show that this strategy leads to higher convergence rates, that is, the corresponding estimators converge at rate $\mathcal{O}(N^{-1/2-r})$ for some $r&gt;0$ for certain classes of functions. Empirically, we show through numerical experiments that the method may improve on standard Monte Carlo even when $s$ is large.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04842v1</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Chopin, Hejin Wang, Mathieu Gerber</dc:creator>
    </item>
    <item>
      <title>Separable Geodesic Lagrangian Monte Carlo for Inference in 2-Way Covariance Models</title>
      <link>https://arxiv.org/abs/2501.04913</link>
      <description>arXiv:2501.04913v1 Announce Type: new 
Abstract: Matrix normal models have an associated 4-tensor for their covariance representation. The covariance array associated with a matrix normal model is naturally represented as a Kronecker-product structured covariance associated with the vector normal, also known as separable covariance matrices. Separable covariance matrices have been studied extensively in the context of multiway data, but little work has been done within the scope of MCMC beyond Gibbs sampling. This paper aims to fill this gap by considering the pullback geometry induced from the Kronecker structure of the parameter space to develop a geodesic Hamiltonian Monte Carlo sampler.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04913v1</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Quinn Simonis, Martin T. Wells</dc:creator>
    </item>
    <item>
      <title>Geodesic Variational Bayes for Multiway Covariances</title>
      <link>https://arxiv.org/abs/2501.04935</link>
      <description>arXiv:2501.04935v1 Announce Type: new 
Abstract: This article explores the optimization of variational approximations for posterior covariances of Gaussian multiway arrays. To achieve this, we establish a natural differential geometric optimization framework on the space using the pullback of the affine-invariant metric. In the case of a truly separable covariance, we demonstrate a joint approximation in the multiway space outperforms a mean-field approximation in optimization efficiency and provides a superior approximation to an unstructured Inverse-Wishart posterior under the average Mahalanobis distance of the data while maintaining a multiway interpretation. We moreover establish efficient expressions for the Euclidean and Riemannian gradients in both cases of the joint and mean-field approximation. We end with an analysis of commodity trade data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04935v1</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Quinn Simonis, Martin T. Wells</dc:creator>
    </item>
    <item>
      <title>Randomized Spectral Clustering for Large-Scale Multi-Layer Networks</title>
      <link>https://arxiv.org/abs/2501.05326</link>
      <description>arXiv:2501.05326v1 Announce Type: new 
Abstract: Large-scale multi-layer networks with large numbers of nodes, edges, and layers arise across various domains, which poses a great computational challenge for the downstream analysis. In this paper, we develop an efficient randomized spectral clustering algorithm for community detection of multi-layer networks. We first utilize the random sampling strategy to sparsify the adjacency matrix of each layer. Then we use the random projection strategy to accelerate the eigen-decomposition of the sum-of-squared sparsified adjacency matrices of all layers. The communities are finally obtained via the k-means of the eigenvectors. The algorithm not only has low time complexity but also saves the storage space. Theoretically, we study the misclassification error rate of the proposed algorithm under the multi-layer stochastic block models, which shows that the randomization does not deteriorate the error bound under certain conditions. Numerical studies on multi-layer networks with millions of nodes show the superior efficiency of the proposed algorithm, which achieves clustering results rapidly. A new R package called MLRclust is developed and made available to the public.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05326v1</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenqing Su, Xiao Guo, Xiangyu Chang, Ying Yang</dc:creator>
    </item>
    <item>
      <title>DisSim-FinBERT: Text Simplification for Core Message Extraction in Complex Financial Texts</title>
      <link>https://arxiv.org/abs/2501.04959</link>
      <description>arXiv:2501.04959v1 Announce Type: cross 
Abstract: This research integrates Discourse Simplification (DisSim) into aspect-based sentiment analysis (ABSA) to improve aspect selection and sentiment prediction in complex financial texts, particularly central bank communications. The study focuses on decomposing Federal Open Market Committee (FOMC) minutes into simple, canonical structures to identify key sentences encapsulating the core messages of intricate financial narratives. The investigation examines whether hierarchical segmenting of financial texts can enhance ABSA performance using a pre-trained Financial BERT model. Results indicate that DisSim methods enhance aspect selection accuracy in simplified texts compared to untreated counterparts and show empirical improvement in sentiment prediction. The study concludes that decomposing complex financial texts into shorter segments with Discourse Simplification can lead to more precise aspect selection, thereby facilitating more accurate economic analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04959v1</guid>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wonseong Kim, Christina Niklaus, Choong Lyol Lee, Siegfried Handschuh</dc:creator>
    </item>
    <item>
      <title>Improved MCMC with active subspaces</title>
      <link>https://arxiv.org/abs/2501.05144</link>
      <description>arXiv:2501.05144v1 Announce Type: cross 
Abstract: Constantine et al. (2016) introduced a Metropolis-Hastings (MH) approach that target the active subspace of a posterior distribution: a linearly projected subspace that is informed by the likelihood. Schuster et al. (2017) refined this approach to introduce a pseudo-marginal Metropolis-Hastings, integrating out inactive variables through estimating a marginal likelihood at every MH iteration. In this paper we show empirically that the effectiveness of these approaches is limited in the case where the linearity assumption is violated, and suggest a particle marginal Metropolis-Hastings algorithm as an alternative for this situation. Finally, the high computational cost of these approaches leads us to consider alternative approaches to using active subspaces in MCMC that avoid the need to estimate a marginal likelihood: we introduce Metropolis-within-Gibbs and Metropolis-within-particle Gibbs methods that provide a more computationally efficient use of the active subspace.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05144v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonardo Ripoli, Richard G. Everitt</dc:creator>
    </item>
    <item>
      <title>Entropy Adjusted Graphical Lasso for Sparse Precision Matrix Estimation</title>
      <link>https://arxiv.org/abs/2501.05308</link>
      <description>arXiv:2501.05308v1 Announce Type: cross 
Abstract: The estimation of a precision matrix is a crucial problem in various research fields, particularly when working with high dimensional data. In such settings, the most common approach is to use the penalized maximum likelihood. The literature typically employs Lasso, Ridge and Elastic Net norms, which effectively shrink the entries of the estimated precision matrix. Although these shrinkage approaches provide well-conditioned precision matrix estimates, they do not explicitly address the uncertainty associated with these estimated matrices. In fact, as the matrix becomes sparser, the precision matrix imposes fewer restrictions, leading to greater variability in the distribution, and thus, to higher entropy. In this paper, we introduce an entropy-adjusted extension of widely used Graphical Lasso using an additional log-determinant penalty term. The objective of the proposed technique is to impose sparsity on the precision matrix estimate and adjust the uncertainty through the log-determinant term. The advantage of the proposed method compared to the existing ones in the literature is evaluated through comprehensive numerical analyses, including both simulated and real-world datasets. The results demonstrate its benefits compared to existing approaches in the literature, with respect to several evaluation metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05308v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vahe Avagyan</dc:creator>
    </item>
    <item>
      <title>Bayesian Joint Additive Factor Models for Multiview Learning</title>
      <link>https://arxiv.org/abs/2406.00778</link>
      <description>arXiv:2406.00778v2 Announce Type: replace-cross 
Abstract: It is increasingly common in a wide variety of applied settings to collect data of multiple different types on the same set of samples. Our particular focus in this article is on studying relationships between such multiview features and responses. A motivating application arises in the context of precision medicine where multi-omics data are collected to correlate with clinical outcomes. It is of interest to infer dependence within and across views while combining multimodal information to improve the prediction of outcomes. The signal-to-noise ratio can vary substantially across views, motivating more nuanced statistical tools beyond standard late and early fusion. This challenge comes with the need to preserve interpretability, select features, and obtain accurate uncertainty quantification. We propose a joint additive factor regression model (JAFAR) with a structured additive design, accounting for shared and view-specific components. We ensure identifiability via a novel dependent cumulative shrinkage process (D-CUSP) prior. We provide an efficient implementation via a partially collapsed Gibbs sampler and extend our approach to allow flexible feature and outcome distributions. Prediction of time-to-labor onset from immunome, metabolome, and proteome data illustrates performance gains against state-of-the-art competitors. Our open-source software (R package) is available at https://github.com/niccoloanceschi/jafar.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00778v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niccolo Anceschi, Federico Ferrari, David B. Dunson, Himel Mallick</dc:creator>
    </item>
    <item>
      <title>singleRcapture: An R Package for Single-Source Capture-Recapture Models</title>
      <link>https://arxiv.org/abs/2411.11032</link>
      <description>arXiv:2411.11032v2 Announce Type: replace-cross 
Abstract: Population size estimation is a major challenge in official statistics, social sciences, and natural sciences. The problem can be tackled by applying capture-recapture methods, which vary depending on the number of sources used, particularly on whether a single or multiple sources are involved. This paper focuses on the first group of methods and introduces a novel R package: singleRcapture. The package implements state-of-the-art single-source capture-recapture (SSCR) models (e.g.~zero-truncated one-inflated regression) together with new developments proposed by the authors, and provides a user-friendly application programming interface (API). This self-contained package can be used to produce point estimates and their variance and implements several bootstrap variance estimators or diagnostics to assess quality and conduct sensitivity analysis. It is intended for users interested in estimating the size of populations, particularly those that are difficult to reach or measure, for which information is available only from one source and dual/multiple system estimation is not applicable. Our package serves to bridge a significant gap, as the SSCR methods are either not available at all or are only partially implemented in existing R packages and other open-source software.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11032v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Piotr Chlebicki, Maciej Ber\k{e}sewicz</dc:creator>
    </item>
  </channel>
</rss>
