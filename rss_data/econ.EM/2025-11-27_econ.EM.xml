<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Nov 2025 05:00:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Estimation in high-dimensional linear regression: Post-Double-Autometrics as an alternative to Post-Double-Lasso</title>
      <link>https://arxiv.org/abs/2511.21257</link>
      <description>arXiv:2511.21257v1 Announce Type: new 
Abstract: Post-Double-Lasso is becoming the most popular method for estimating linear regression models with many covariates when the purpose is to obtain an accurate estimate of a parameter of interest, such as an average treatment effect. However, this method can suffer from substantial omitted variable bias in finite sample. We propose a new method called Post-Double-Autometrics, which is based on Autometrics, and show that this method outperforms Post-Double-Lasso. Its use in a standard application of economic growth sheds new light on the hypothesis of convergence from poor to rich economies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21257v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sullivan Hu\'e, S\'ebastien Laurent, Ulrich Aiounou, Emmanuel Flachaire</dc:creator>
    </item>
    <item>
      <title>Discrete Choice with Endogenous Peer Selection</title>
      <link>https://arxiv.org/abs/2511.21446</link>
      <description>arXiv:2511.21446v1 Announce Type: new 
Abstract: We develop a continuous-time peer-effect discrete choice model where peers that affect the preferences of a given agent are randomly selected based on their previous choices. We characterize the equilibrium behavior and study the empirical content of the model. In the model, changes in the choices of peers affect both the set of peers the agent pays attention to and her preferences over the alternatives. We exploit variation in choices coupled with variation in the size of the set of potential peers to recover agents' preferences and the peer selection mechanism. These nonparametric identification results do not rely on exogenous variation of covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21446v1</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nail Kashaev, Natalia Lazzati</dc:creator>
    </item>
    <item>
      <title>A Generalized Control Function Approach to Production Function Estimation</title>
      <link>https://arxiv.org/abs/2511.21578</link>
      <description>arXiv:2511.21578v1 Announce Type: new 
Abstract: We generalize the control function approach to production function estimation. Our generalization accommodates scenarios in which productivity evolves jointly with other unobservable factors such as latent demand shocks and the invertibility assumption underpinning the traditional proxy variable approach fails. We provide conditions under which the output elasticity of the variable input -- and hence the markup -- is nonparametrically point-identified. A Neyman orthogonal moment condition ensures oracle efficiency of our GMM estimator. A Monte Carlo exercise shows a large bias for the traditional proxy variable approach that decreases rapidly and nearly vanishes for our generalized control function approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21578v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ulrich Doraszelski, Lixiong Li</dc:creator>
    </item>
    <item>
      <title>On Evolution-Based Models for Experimentation Under Interference</title>
      <link>https://arxiv.org/abs/2511.21675</link>
      <description>arXiv:2511.21675v1 Announce Type: cross 
Abstract: Causal effect estimation in networked systems is central to data-driven decision making. In such settings, interventions on one unit can spill over to others, and in complex physical or social systems, the interaction pathways driving these interference structures remain largely unobserved. We argue that for identifying population-level causal effects, it is not necessary to recover the exact network structure; instead, it suffices to characterize how those interactions contribute to the evolution of outcomes. Building on this principle, we study an evolution-based approach that investigates how outcomes change across observation rounds in response to interventions, hence compensating for missing network information. Using an exposure-mapping perspective, we give an axiomatic characterization of when the empirical distribution of outcomes follows a low-dimensional recursive equation, and identify minimal structural conditions under which such evolution mappings exist. We frame this as a distributional counterpart to difference-in-differences. Rather than assuming parallel paths for individual units, it exploits parallel evolution patterns across treatment scenarios to estimate counterfactual trajectories. A key insight is that treatment randomization plays a role beyond eliminating latent confounding; it induces an implicit sampling from hidden interference channels, enabling consistent learning about heterogeneous spillover effects. We highlight causal message passing as an instantiation of this method in dense networks while extending to more general interference structures, including influencer networks where a small set of units drives most spillovers. Finally, we discuss the limits of this approach, showing that strong temporal trends or endogenous interference can undermine identification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21675v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>econ.EM</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sadegh Shirani, Mohsen Bayati</dc:creator>
    </item>
    <item>
      <title>Projection Inference for set-identified SVARs</title>
      <link>https://arxiv.org/abs/2504.14106</link>
      <description>arXiv:2504.14106v3 Announce Type: replace 
Abstract: We study the properties of the classical \emph{projection} method to conduct simultaneous inference about the coefficients of the structural impulse-response function and their identified set in Structural Vector Autoregressions. We show that -- as the sample size grows large -- projection inference produces regions for the structural parameters and their identified set with both frequentist coverage and robust Bayesian credibility of at least $1-\alpha$. We then calibrate the radius of the Wald ellipsoid to guarantee that -- for a given posterior on the reduced-form parameters -- the robust Bayesian credibility of the projection method is exactly $1-\alpha$. If the bounds of the identified set are differentiable, our calibrated projection also covers the product of the identified sets for each structural parameter of interest with probability $1-\alpha$. We illustrate the main results of the paper using a demand/supply-model of the U.S.~labor market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14106v3</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bulat Gafarov, Matthias Meier, Jos\'e Luis Montiel Olea</dc:creator>
    </item>
    <item>
      <title>Does Residuals-on-Residuals Regression Produce Representative Estimates of Causal Effects?</title>
      <link>https://arxiv.org/abs/2506.07462</link>
      <description>arXiv:2506.07462v2 Announce Type: replace 
Abstract: Double Machine Learning is widely used to estimate causal treatment effects in large-scale observational data. The ``residuals-on-residuals'' regression estimator (RORR) is especially popular for its simplicity and computational tractability. However, when treatment effects are heterogeneous, the proper interpretation of RORR may not be widely understood. We show that for many-valued treatments with continuous dose-response functions, RORR converges to a conditional variance-weighted average of derivatives evaluated at points not in the observed dataset. This estimand does not in general equal the Average Causal Derivative (ACD). Hence, even if all units share the same dose-response function, RORR may not converge to an average treatment effect in the population represented by the sample. We propose an alternative estimator for the ACD that is suitable for large datasets. We demonstrate the pitfalls of RORR and the favorable properties of the proposed estimator through an illustrative numerical example and with real-world data from Netflix. Our methodology is deployed in Netflix's internal observational causal inference platform, where it regularly powers causal research and decision-making at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07462v2</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Apoorva Lal, Winston Chou</dc:creator>
    </item>
    <item>
      <title>Uniform Quasi ML based inference for the panel AR(1) model</title>
      <link>https://arxiv.org/abs/2508.20855</link>
      <description>arXiv:2508.20855v3 Announce Type: replace 
Abstract: Maximum Likelihood (ML) offers attractive alternatives to Generalized Method of Moments (GMM) estimators for dynamic panel data models. However, to date no identification-robust inference methods exist that can be used in conjunction with the ML estimators for these models. In this paper we propose ML based inference methods for panel AR(1) models with arbitrary initial conditions and heteroskedasticity that are robust to the strength of identification. We show that (Quasi) Lagrange Multiplier (LM) tests and confidence sets (CSs) that use the expected Hessian rather than the observed Hessian of the log-likelihood function have correct asymptotic size and coverage probability in a uniform sense, respectively. Such Quasi LM tests and CSs are also robust to misspecification of the distribution of the data and to heterogeneity, including heteroskedasticity. We derive the power envelope of a Fixed Effects version of such an LM test for hypotheses involving the autoregressive parameter when the average information matrix is estimated by a centered OPG estimator and the model is only second-order identified, and show that it coincides with the maximal attainable power curve in the worst-case setting. We also study the empirical size and power properties of these (Quasi) LM tests and find that the hypothesis that the (Quasi) LM test has correct size cannot be rejected.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20855v3</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Kruiniger</dc:creator>
    </item>
    <item>
      <title>Training and Testing with Multiple Splits: A Central Limit Theorem for Split-Sample Estimators</title>
      <link>https://arxiv.org/abs/2511.04957</link>
      <description>arXiv:2511.04957v2 Announce Type: replace 
Abstract: As predictive algorithms grow in popularity, using the same dataset to both train and test a new model has become routine across research, policy, and industry. Sample-splitting attains valid inference on model properties by using separate subsamples to estimate the model and to evaluate it. However, this approach has two drawbacks, since each task uses only part of the data, and different splits can lead to widely different estimates. Averaging across multiple splits, I develop an inference approach that uses more data for training, uses the entire sample for testing, and improves reproducibility. I address the statistical dependence from reusing observations across splits by proving a new central limit theorem for a large class of split-sample estimators under arguably mild and general conditions. Importantly, I make no restrictions on model complexity or convergence rates. I show that confidence intervals based on the normal approximation are valid for many applications, but may undercover in important cases of interest, such as comparing the performance between two models. I develop a new inference approach for such cases, explicitly accounting for the dependence across splits. Moreover, I provide a measure of reproducibility for p-values obtained from split-sample estimators. Finally, I apply my results to two important problems in development and public economics: predicting poverty and learning heterogeneous treatment effects in randomized experiments. I show that my inference approach with repeated cross-fitting achieves better power than existing alternatives, often enough to reveal statistical significance that would otherwise be missed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04957v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bruno Fava</dc:creator>
    </item>
    <item>
      <title>Multiple Randomization Designs: Estimation and Inference with Interference</title>
      <link>https://arxiv.org/abs/2112.13495</link>
      <description>arXiv:2112.13495v3 Announce Type: replace-cross 
Abstract: In this study we introduce a new class of experimental designs. In a classical randomized controlled trial (RCT), or A/B test, a randomly selected subset of a population of units (e.g., individuals, plots of land, or experiences) is assigned to a treatment (treatment A), and the remainder of the population is assigned to the control treatment (treatment B). The difference in average outcome by treatment group is an estimate of the average effect of the treatment. However, motivating our study, the setting for modern experiments is often different, with the outcomes and treatment assignments indexed by multiple populations. For example, outcomes may be indexed by buyers and sellers, by content creators and subscribers, by drivers and riders, or by travelers and airlines and travel agents, with treatments potentially varying across these indices. Spillovers or interference can arise from interactions between units across populations. For example, sellers' behavior may depend on buyers' treatment assignment, or vice versa. This can invalidate the simple comparison of means as an estimator for the average effect of the treatment in classical RCTs. We propose new experiment designs for settings in which multiple populations interact. We show how these designs allow us to study questions about interference that cannot be answered by classical randomized experiments. Finally, we develop new statistical methods for analyzing these Multiple Randomization Designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.13495v3</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Masoero, Suhas Vijaykumar, Thomas Richardson, James McQueen, Ido Rosen, Brian Burdick, Pat Bajari, Guido Imbens</dc:creator>
    </item>
    <item>
      <title>Learning Individual Behavior in Agent-Based Models with Graph Diffusion Networks</title>
      <link>https://arxiv.org/abs/2505.21426</link>
      <description>arXiv:2505.21426v2 Announce Type: replace-cross 
Abstract: Agent-Based Models (ABMs) are powerful tools for studying emergent properties in complex systems. In ABMs, agent behaviors are governed by local interactions and stochastic rules. However, these rules are, in general, non-differentiable, limiting the use of gradient-based methods for optimization, and thus integration with real-world data. We propose a novel framework to learn a differentiable surrogate of any ABM by observing its generated data. Our method combines diffusion models to capture behavioral stochasticity and graph neural networks to model agent interactions. Distinct from prior surrogate approaches, our method introduces a fundamental shift: rather than approximating system-level outputs, it models individual agent behavior directly, preserving the decentralized, bottom-up dynamics that define ABMs. We validate our approach on two ABMs (Schelling's segregation model and a Predator-Prey ecosystem) showing that it replicates individual-level patterns and accurately forecasts emergent dynamics beyond training. Our results demonstrate the potential of combining diffusion models and graph learning for data-driven ABM simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21426v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>econ.EM</category>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Cozzi, Marco Pangallo, Alan Perotti, Andr\'e Panisson, Corrado Monti</dc:creator>
    </item>
  </channel>
</rss>
