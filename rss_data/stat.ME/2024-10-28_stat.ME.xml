<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 Oct 2024 02:57:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Median Based Unit Weibull (MBUW): a new unit distribution Properties</title>
      <link>https://arxiv.org/abs/2410.19019</link>
      <description>arXiv:2410.19019v2 Announce Type: new 
Abstract: A new 2 parameter unit Weibull distribution is defined on the unit interval (0,1). The methodology of deducing its PDF, some of its properties and related functions are discussed. The paper is supplied by many figures illustrating the new distribution and how this can make it illegible to fit a wide range of skewed data. The new distribution holds a name (Attia) as a nickname.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19019v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Mohammed Attia</dc:creator>
    </item>
    <item>
      <title>Enhancing Approximate Modular Bayesian Inference by Emulating the Conditional Posterior</title>
      <link>https://arxiv.org/abs/2410.19028</link>
      <description>arXiv:2410.19028v1 Announce Type: new 
Abstract: In modular Bayesian analyses, complex models are composed of distinct modules, each representing different aspects of the data or prior information. In this context, fully Bayesian approaches can sometimes lead to undesirable feedback between modules, compromising the integrity of the inference. This paper focuses on the "cut-distribution" which prevents unwanted influence between modules by "cutting" feedback. The multiple imputation (DS) algorithm is standard practice for approximating the cut-distribution, but it can be computationally intensive, especially when the number of imputations required is large. An enhanced method is proposed, the Emulating the Conditional Posterior (ECP) algorithm, which leverages emulation to increase the number of imputations. Through numerical experiment it is demonstrated that the ECP algorithm outperforms the traditional DS approach in terms of accuracy and computational efficiency, particularly when resources are constrained. It is also shown how the DS algorithm can be improved using ideas from design of experiments. This work also provides practical recommendations on algorithm choice based on the computational demands of sampling from the prior and cut-distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19028v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grant Hutchings, Kellin Rumsey, Derek Bingham, Gabriel Huerta</dc:creator>
    </item>
    <item>
      <title>Model-free Variable Selection and Inference for High-dimensional Data</title>
      <link>https://arxiv.org/abs/2410.19031</link>
      <description>arXiv:2410.19031v1 Announce Type: new 
Abstract: Statistical inference is challenging in high-dimensional data analysis. Existing post-selection inference requires an explicitly specified regression model as well as sparsity in the regression model. The performance of such procedures can be poor under either misspecified nonlinear models or a violation of the sparsity assumption. In this paper, we propose a sufficient dimension association (SDA) technique that measures the association between each predictor and the response variable conditioning on other predictors. Our proposed SDA method requires neither a specific form of regression model nor sparsity in the regression. Alternatively, our method assumes normalized or Gaussian-distributed predictors with a Markov blanket property. We propose an estimator for the SDA and prove asymptotic properties for the estimator. For simultaneous hypothesis testing and variable selection, we construct test statistics based on the Kolmogorov-Smirnov principle and the Cram{\"e}r-von-Mises principle. A multiplier bootstrap approach is used for computing critical values and $p$-values. Extensive simulation studies have been conducted to show the validity and superiority of our SDA method. Gene expression data from the Alzheimer Disease Neuroimaging Initiative are used to demonstrate a real application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19031v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shangyuan Ye, Shauna Rakshe, Ye Liang</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Nonparametric Efficient Estimation for Provider Evaluation</title>
      <link>https://arxiv.org/abs/2410.19073</link>
      <description>arXiv:2410.19073v1 Announce Type: new 
Abstract: Provider profiling has the goal of identifying healthcare providers with exceptional patient outcomes. When evaluating providers, adjustment is necessary to control for differences in case-mix between different providers. Direct and indirect standardization are two popular risk adjustment methods. In causal terms, direct standardization examines a counterfactual in which the entire target population is treated by one provider. Indirect standardization, commonly expressed as a standardized outcome ratio, examines the counterfactual in which the population treated by a provider had instead been randomly assigned to another provider. Our first contribution is to present nonparametric efficiency bound for direct and indirectly standardized provider metrics by deriving their efficient influence functions. Our second contribution is to propose fully nonparametric estimators based on targeted minimum loss-based estimation that achieve the efficiency bounds. The finite-sample performance of the estimator is investigated through simulation studies. We apply our methods to evaluate dialysis facilities in New York State in terms of unplanned readmission rates using a large Medicare claims dataset. A software implementation of our methods is available in the R package TargetedRisk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19073v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Herbert Susmann, Yiting Li, Mara A. McAdams-DeMarco, Iv\'an D\'iaz, Wenbo Wu</dc:creator>
    </item>
    <item>
      <title>Enhancing Spatial Functional Linear Regression with Robust Dimension Reduction Methods</title>
      <link>https://arxiv.org/abs/2410.19140</link>
      <description>arXiv:2410.19140v1 Announce Type: new 
Abstract: This paper introduces a robust estimation strategy for the spatial functional linear regression model using dimension reduction methods, specifically functional principal component analysis (FPCA) and functional partial least squares (FPLS). These techniques are designed to address challenges associated with spatially correlated functional data, particularly the impact of outliers on parameter estimation. By projecting the infinite-dimensional functional predictor onto a finite-dimensional space defined by orthonormal basis functions and employing M-estimation to mitigate outlier effects, our approach improves the accuracy and reliability of parameter estimates in the spatial functional linear regression context. Simulation studies and empirical data analysis substantiate the effectiveness of our methods, while an appendix explores the Fisher consistency and influence function of the FPCA-based approach. The rfsac package in R implements these robust estimation strategies, ensuring practical applicability for researchers and practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19140v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ufuk Beyaztas, Abhijit Mandal, Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>Cross Spline Net and a Unified World</title>
      <link>https://arxiv.org/abs/2410.19154</link>
      <description>arXiv:2410.19154v1 Announce Type: new 
Abstract: In today's machine learning world for tabular data, XGBoost and fully connected neural network (FCNN) are two most popular methods due to their good model performance and convenience to use. However, they are highly complicated, hard to interpret, and can be overfitted. In this paper, we propose a new modeling framework called cross spline net (CSN) that is based on a combination of spline transformation and cross-network (Wang et al. 2017, 2021). We will show CSN is as performant and convenient to use, and is less complicated, more interpretable and robust. Moreover, the CSN framework is flexible, as the spline layer can be configured differently to yield different models. With different choices of the spline layer, we can reproduce or approximate a set of non-neural network models, including linear and spline-based statistical models, tree, rule-fit, tree-ensembles (gradient boosting trees, random forest), oblique tree/forests, multi-variate adaptive regression spline (MARS), SVM with polynomial kernel, etc. Therefore, CSN provides a unified modeling framework that puts the above set of non-neural network models under the same neural network framework. By using scalable and powerful gradient descent algorithms available in neural network libraries, CSN avoids some pitfalls (such as being ad-hoc, greedy or non-scalable) in the case-specific optimization methods used in the above non-neural network models. We will use a special type of CSN, TreeNet, to illustrate our point. We will compare TreeNet with XGBoost and FCNN to show the benefits of TreeNet. We believe CSN will provide a flexible and convenient framework for practitioners to build performant, robust and more interpretable models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19154v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linwei Hu, Ye Jin Choi, Vijayan N. Nair</dc:creator>
    </item>
    <item>
      <title>A novel longitudinal rank-sum test for multiple primary endpoints in clinical trials: Applications to neurodegenerative disorders</title>
      <link>https://arxiv.org/abs/2410.19190</link>
      <description>arXiv:2410.19190v1 Announce Type: new 
Abstract: Neurodegenerative disorders such as Alzheimer's disease (AD) present a significant global health challenge, characterized by cognitive decline, functional impairment, and other debilitating effects. Current AD clinical trials often assess multiple longitudinal primary endpoints to comprehensively evaluate treatment efficacy. Traditional methods, however, may fail to capture global treatment effects, require larger sample sizes due to multiplicity adjustments, and may not fully exploit multivariate longitudinal data. To address these limitations, we introduce the Longitudinal Rank Sum Test (LRST), a novel nonparametric rank-based omnibus test statistic. The LRST enables a comprehensive assessment of treatment efficacy across multiple endpoints and time points without multiplicity adjustments, effectively controlling Type I error while enhancing statistical power. It offers flexibility against various data distributions encountered in AD research and maximizes the utilization of longitudinal data. Extensive simulations and real-data applications demonstrate the LRST's performance, underscoring its potential as a valuable tool in AD clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19190v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoming Xu, Dhrubajyoti Ghosh, Sheng Luo</dc:creator>
    </item>
    <item>
      <title>Deep Transformation Model</title>
      <link>https://arxiv.org/abs/2410.19226</link>
      <description>arXiv:2410.19226v1 Announce Type: new 
Abstract: There has been a significant recent surge in deep neural network (DNN) techniques. Most of the existing DNN techniques have restricted model formats/assumptions. To overcome their limitations, we propose the nonparametric transformation model, which encompasses many popular models as special cases and hence is less sensitive to model mis-specification. This model also has the potential of accommodating heavy-tailed errors, a robustness property not broadly shared. Accordingly, a new loss function, which fundamentally differs from the existing ones, is developed. For computational feasibility, we further develop a double rectified linear unit (DReLU)-based estimator. To accommodate the scenario with a diverging number of input variables and/or noises, we propose variable selection based on group penalization. We further expand the scope to coherently accommodate censored survival data. The estimation and variable selection properties are rigorously established. Extensive numerical studies, including simulations and data analyses, establish the satisfactory practical utility of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19226v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tong Wang, Shunqin Zhang, Sanguo Zhang, Jian Huang, Shuangge Ma</dc:creator>
    </item>
    <item>
      <title>Feed-Forward Panel Estimation for Discrete-time Survival Analysis of Recurrent Events with Frailty</title>
      <link>https://arxiv.org/abs/2410.19271</link>
      <description>arXiv:2410.19271v1 Announce Type: new 
Abstract: In recurrent survival analysis where the event of interest can occur multiple times for each subject, frailty models play a crucial role by capturing unobserved heterogeneity at the subject level within a population. Frailty models traditionally face challenges due to the lack of a closed-form solution for the maximum likelihood estimation that is unconditional on frailty. In this paper, we propose a novel method: Feed-Forward Panel estimation for discrete-time Survival Analysis (FFPSurv). Our model uses variational Bayesian inference to sequentially update the posterior distribution of frailty as recurrent events are observed, and derives a closed form for the panel likelihood, effectively addressing the limitation of existing frailty models. We demonstrate the efficacy of our method through extensive experiments on numerical examples and real-world recurrent survival data. Furthermore, we mathematically prove that our model is identifiable under minor assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19271v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Borna Bateni, Peyman Bateni, Bishwadeep Bhattacharyya, Devin Reeh</dc:creator>
    </item>
    <item>
      <title>Insights into regression-based cross-temporal forecast reconciliation</title>
      <link>https://arxiv.org/abs/2410.19407</link>
      <description>arXiv:2410.19407v1 Announce Type: new 
Abstract: Cross-temporal forecast reconciliation aims to ensure consistency across forecasts made at different temporal and cross-sectional levels. We explore the relationships between sequential, iterative, and optimal combination approaches, and discuss the conditions under which a sequential reconciliation approach (either first-cross-sectional-then-temporal, or first-temporal-then-cross-sectional) is equivalent to a fully (i.e., cross-temporally) coherent iterative heuristic. Furthermore, we show that for specific patterns of the error covariance matrix in the regression model on which the optimal combination approach grounds, iterative reconciliation naturally converges to the optimal combination solution, regardless the order of application of the uni-dimensional cross-sectional and temporal reconciliation approaches. Theoretical and empirical properties of the proposed approaches are investigated through a forecasting experiment using a dataset of hourly photovoltaic power generation. The study presents a comprehensive framework for understanding and enhancing cross-temporal forecast reconciliation, considering both forecast accuracy and the often overlooked computational aspects, showing that significant improvement can be achieved in terms of memory space and computation time, two particularly important aspects in the high-dimensional contexts that usually arise in cross-temporal forecast reconciliation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19407v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Girolimetto, Tommaso Di Fonzo</dc:creator>
    </item>
    <item>
      <title>Unified Causality Analysis Based on the Degrees of Freedom</title>
      <link>https://arxiv.org/abs/2410.19469</link>
      <description>arXiv:2410.19469v1 Announce Type: new 
Abstract: Temporally evolving systems are typically modeled by dynamic equations. A key challenge in accurate modeling is understanding the causal relationships between subsystems, as well as identifying the presence and influence of unobserved hidden drivers on the observed dynamics. This paper presents a unified method capable of identifying fundamental causal relationships between pairs of systems, whether deterministic or stochastic. Notably, the method also uncovers hidden common causes beyond the observed variables. By analyzing the degrees of freedom in the system, our approach provides a more comprehensive understanding of both causal influence and hidden confounders. This unified framework is validated through theoretical models and simulations, demonstrating its robustness and potential for broader application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19469v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andr\'as Telcs, Marcell T. Kurbucz, Antal Jakov\'ac</dc:creator>
    </item>
    <item>
      <title>OCEAN: Flexible Feature Set Aggregation for Analysis of Multi-omics Data</title>
      <link>https://arxiv.org/abs/2410.19523</link>
      <description>arXiv:2410.19523v1 Announce Type: new 
Abstract: Integrated analysis of multi-omics datasets holds great promise for uncovering complex biological processes. However, the large dimension of omics data poses significant interpretability and multiple testing challenges. Simultaneous Enrichment Analysis (SEA) was introduced to address these issues in single-omics analysis, providing an in-built multiple testing correction and enabling simultaneous feature set testing. In this paper, we introduce OCEAN, an extension of SEA to multi-omics data. OCEAN is a flexible approach to analyze potentially all possible two-way feature sets from any pair of genomics datasets. We also propose two new error rates which are in line with the two-way structure of the data and facilitate interpretation of the results. The power and utility of OCEAN is demonstrated by analyzing copy number and gene expression data for breast and colon cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19523v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mitra Ebrahimpoor, Renee Menezes, Ningning Xu, Jelle J. Goeman</dc:creator>
    </item>
    <item>
      <title>trajmsm: An R package for Trajectory Analysis and Causal Modeling</title>
      <link>https://arxiv.org/abs/2410.19682</link>
      <description>arXiv:2410.19682v1 Announce Type: new 
Abstract: The R package trajmsm provides functions designed to simplify the estimation of the parameters of a model combining latent class growth analysis (LCGA), a trajectory analysis technique, and marginal structural models (MSMs) called LCGA-MSM. LCGA summarizes similar patterns of change over time into a few distinct categories called trajectory groups, which are then included as "treatments" in the MSM. MSMs are a class of causal models that correctly handle treatment-confounder feedback. The parameters of LCGA-MSMs can be consistently estimated using different estimators, such as inverse probability weighting (IPW), g-computation, and pooled longitudinal targeted maximum likelihood estimation (pooled LTMLE). These three estimators of the parameters of LCGA-MSMs are currently implemented in our package. In the context of a time-dependent outcome, we previously proposed a combination of LCGA and history-restricted MSMs (LCGA-HRMSMs). Our package provides additional functions to estimate the parameters of such models. Version 0.1.3 of the package is currently available on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19682v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Awa Diop, Caroline Sirois, Jason R. Guertin, Mireille E. Schnitzer, James M. Brophy, Denis Talbot</dc:creator>
    </item>
    <item>
      <title>Deterministic Fokker-Planck Transport -- With Applications to Sampling, Variational Inference, Kernel Mean Embeddings &amp; Sequential Monte Carlo</title>
      <link>https://arxiv.org/abs/2410.18993</link>
      <description>arXiv:2410.18993v1 Announce Type: cross 
Abstract: The Fokker-Planck equation can be reformulated as a continuity equation, which naturally suggests using the associated velocity field in particle flow methods. While the resulting probability flow ODE offers appealing properties - such as defining a gradient flow of the Kullback-Leibler divergence between the current and target densities with respect to the 2-Wasserstein distance - it relies on evaluating the current probability density, which is intractable in most practical applications. By closely examining the drawbacks of approximating this density via kernel density estimation, we uncover opportunities to turn these limitations into advantages in contexts such as variational inference, kernel mean embeddings, and sequential Monte Carlo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18993v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilja Klebanov</dc:creator>
    </item>
    <item>
      <title>A spectral method for multi-view subspace learning using the product of projections</title>
      <link>https://arxiv.org/abs/2410.19125</link>
      <description>arXiv:2410.19125v1 Announce Type: cross 
Abstract: Multi-view data provides complementary information on the same set of observations, with multi-omics and multimodal sensor data being common examples. Analyzing such data typically requires distinguishing between shared (joint) and unique (individual) signal subspaces from noisy, high-dimensional measurements. Despite many proposed methods, the conditions for reliably identifying joint and individual subspaces remain unclear. We rigorously quantify these conditions, which depend on the ratio of the signal rank to the ambient dimension, principal angles between true subspaces, and noise levels. Our approach characterizes how spectrum perturbations of the product of projection matrices, derived from each view's estimated subspaces, affect subspace separation. Using these insights, we provide an easy-to-use and scalable estimation algorithm. In particular, we employ rotational bootstrap and random matrix theory to partition the observed spectrum into joint, individual, and noise subspaces. Diagnostic plots visualize this partitioning, providing practical and interpretable insights into the estimation performance. In simulations, our method estimates joint and individual subspaces more accurately than existing approaches. Applications to multi-omics data from colorectal cancer patients and nutrigenomic study of mice demonstrate improved performance in downstream predictive tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19125v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renat Sergazinov, Armeen Taeb, Irina Gaynanova</dc:creator>
    </item>
    <item>
      <title>Golden Ratio-Based Sufficient Dimension Reduction</title>
      <link>https://arxiv.org/abs/2410.19300</link>
      <description>arXiv:2410.19300v1 Announce Type: cross 
Abstract: Many machine learning applications deal with high dimensional data. To make computations feasible and learning more efficient, it is often desirable to reduce the dimensionality of the input variables by finding linear combinations of the predictors that can retain as much original information as possible in the relationship between the response and the original predictors. We propose a neural network based sufficient dimension reduction method that not only identifies the structural dimension effectively, but also estimates the central space well. It takes advantages of approximation capabilities of neural networks for functions in Barron classes and leads to reduced computation cost compared to other dimension reduction methods in the literature. Additionally, the framework can be extended to fit practical dimension reduction, making the methodology more applicable in practical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19300v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenjing Yang, Yuhong Yang</dc:creator>
    </item>
    <item>
      <title>On the robustness of semi-discrete optimal transport</title>
      <link>https://arxiv.org/abs/2410.19596</link>
      <description>arXiv:2410.19596v1 Announce Type: cross 
Abstract: We derive the breakdown point for solutions of semi-discrete optimal transport problems, which characterizes the robustness of the multivariate quantiles based on optimal transport proposed in Ghosal and Sen (2022). We do so under very mild assumptions: the absolutely continuous reference measure is only assumed to have a support that is compact and convex, whereas the target measure is a general discrete measure on a finite number, $n$ say, of atoms. The breakdown point depends on the target measure only through its probability weights (hence not on the location of the atoms) and involves the geometry of the reference measure through the Tukey (1975) concept of halfspace depth. Remarkably, depending on this geometry, the breakdown point of the optimal transport median can be strictly smaller than the breakdown point of the univariate median or the breakdown point of the spatial median, namely~$\lceil n/2\rceil /2$. In the context of robust location estimation, our results provide a subtle insight on how to perform multivariate trimming when constructing trimmed means based on optimal transport.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19596v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davy Paindaveine, Riccardo Passeggeri</dc:creator>
    </item>
    <item>
      <title>Semi-Parametric Sensitivity Analysis for Trials with Irregular and Informative Assessment Times</title>
      <link>https://arxiv.org/abs/2204.11979</link>
      <description>arXiv:2204.11979v4 Announce Type: replace 
Abstract: Many trials are designed to collect outcomes at or around pre-specified times after randomization. If there is variability in the times when participants are actually assessed, this can pose a challenge to learning the effect of treatment, since not all participants have outcome assessments at the times of interest. Furthermore, observed outcome values may not be representative of all participants' outcomes at a given time. Methods have been developed that account for some types of such irregular and informative assessment times; however, since these methods rely on untestable assumptions, sensitivity analyses are needed. We develop a methodology that is benchmarked at the explainable assessmen (EA) assumption, under which assessment and outcomes at each time are related only through data collected prior to that time. Our method uses an exponential tilting assumption, governed by a sensitivity analysis parameter, that posits deviations from the EA assumption. Our inferential strategy is based on a new influence function-based, augmented inverse intensity-weighted estimator. Our approach allows for flexible semiparametric modeling of the observed data, which is separated from specification of the sensitivity parameter. We apply our method to a randomized trial of low-income individuals with uncontrolled asthma, and we illustrate implementation of our estimation procedure in detail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.11979v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bonnie B. Smith, Yujing Gao, Shu Yang, Ravi Varadhan, Andrea J. Apter, Daniel O. Scharfstein</dc:creator>
    </item>
    <item>
      <title>Two-Stage Pseudo Maximum Likelihood Estimation of Semiparametric Copula-based Regression Models for Semi-Competing Risks Data</title>
      <link>https://arxiv.org/abs/2312.14013</link>
      <description>arXiv:2312.14013v2 Announce Type: replace 
Abstract: We propose a two-stage estimation procedure for a copula-based model with semi-competing risks data, where the non-terminal event is subject to dependent censoring by the terminal event, and both events are subject to independent censoring. With a copula-based model, the marginal survival functions of individual event times are specified by semiparametric transformation models, and the dependence between the bivariate event times is specified by a parametric copula function. For the estimation procedure, in the first stage, the parameters associated with the marginal of the terminal event are estimated using only the corresponding observed outcomes, and in the second stage, the marginal parameters for the non-terminal event time and the copula parameter are estimated together via maximizing a pseudo-likelihood function based on the joint distribution of the bivariate event times. We derived the asymptotic properties of the proposed estimator and provided an analytic variance estimator for inference. Through simulation studies, we showed that our approach leads to consistent estimates with less computational cost and more robustness than the one-stage procedure developed in Chen (2012), where all parameters were estimated simultaneously. In addition, our approach demonstrates more desirable finite-sample performances over another existing two-stage estimation method proposed in Zhu et al. (2021). An R package PMLE4SCR is developed to implement our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14013v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10985-024-09640-z</arxiv:DOI>
      <arxiv:journal_reference>Lifetime Data Analysis (2024)</arxiv:journal_reference>
      <dc:creator>Sakie J. Arachchige, Xinyuan Chen, Qian M. Zhou</dc:creator>
    </item>
    <item>
      <title>Dependent Random Partitions by Shrinking Toward an Anchor</title>
      <link>https://arxiv.org/abs/2312.17716</link>
      <description>arXiv:2312.17716v3 Announce Type: replace 
Abstract: Although exchangeable processes from Bayesian nonparametrics have been used as a generating mechanism for random partition models, we deviate from this paradigm to explicitly incorporate clustering information in the formulation of our random partition model. Our shrinkage partition distribution takes any partition distribution and shrinks its probability mass toward an anchor partition. We show how this provides a framework to model hierarchically-dependent and temporally-dependent random partitions. The shrinkage parameter controls the degree of dependence, accommodating at its extremes both independence and complete equality. Since a priori knowledge of items may vary, our formulation allows the degree of shrinkage toward the anchor to be item-specific. Our random partition model has a tractable normalizing constant which allows for standard Markov chain Monte Carlo algorithms for posterior sampling. We prove intuitive theoretical properties for our distribution and compare it to related partition distributions. We show that our model provides better out-of-sample fit in a real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17716v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David B. Dahl, Richard L. Warr, Thomas P. Jensen</dc:creator>
    </item>
    <item>
      <title>Optimal Survival Analyses With Prevalent and Incident Patients</title>
      <link>https://arxiv.org/abs/2403.15302</link>
      <description>arXiv:2403.15302v2 Announce Type: replace 
Abstract: Period-prevalent cohorts are often used for their cost-saving potential in epidemiological studies of survival outcomes. Under this design, prevalent patients allow for evaluations of long-term survival outcomes without the need for long follow-up, whereas incident patients allow for evaluations of short-term survival outcomes without the issue of left-truncation. In most period-prevalent survival analyses from the existing literature, patients have been recruited to achieve an overall sample size, with little attention given to the relative frequencies of prevalent and incident patients and their statistical implications. Furthermore, there are no existing methods available to rigorously quantify the impact of these relative frequencies on estimation and inference and incorporate this information into study design strategies. To address these gaps, we develop an approach to identify the optimal mix of prevalent and incident patients that maximizes precision over the entire estimated survival curve, subject to a flexible weighting scheme. In addition, we prove that inference based on the weighted log-rank test or Cox proportional hazards model is most powerful with an entirely prevalent or incident cohort, and we derive theoretical formulas to determine the optimal choice. Simulations confirm the validity of the proposed optimization criteria and show that substantial efficiency gains can be achieved by recruiting the optimal mix of prevalent and incident patients. The proposed methods are applied to assess waitlist outcomes among kidney transplant candidates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15302v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas Hartman</dc:creator>
    </item>
    <item>
      <title>Longitudinal Generalizations of the Average Treatment Effect on the Treated for Multi-valued and Continuous Treatments</title>
      <link>https://arxiv.org/abs/2405.06135</link>
      <description>arXiv:2405.06135v2 Announce Type: replace 
Abstract: The Average Treatment Effect on the Treated (ATT) is a common causal parameter defined as the average effect of a binary treatment among the subset of the population receiving treatment. We propose a novel family of parameters, Generalized ATTs (GATTs), that generalize the concept of the ATT to longitudinal data structures, multi-valued or continuous treatments, and conditioning on arbitrary treatment subsets. We provide a formal causal identification result that expresses the GATT in terms of sequential regressions, and derive the efficient influence function of the parameter, which defines its semi-parametric efficiency bound. Efficient semi-parametric inference of the GATT requires estimating the ratios of functions of conditional probabilities (or densities); we propose directly estimating these ratios via empirical loss minimization, drawing on the theory of Riesz representers. Simulations suggest that estimation of the density ratios using Riesz representation have better stability in finite samples. Lastly, we illustrate the use of our methods to evaluate the effect of chronic pain management strategies on the development of opioid use disorder among Medicare patients with chronic pain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06135v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Herbert Susmann, Nicholas T. Williams, Kara E. Rudolph, Iv\'an D\'iaz</dc:creator>
    </item>
    <item>
      <title>Functional Clustering for Longitudinal Associations between Social Determinants of Health and Stroke Mortality in the US</title>
      <link>https://arxiv.org/abs/2406.10499</link>
      <description>arXiv:2406.10499v5 Announce Type: replace 
Abstract: Understanding the longitudinally changing associations between Social Determinants of Health (SDOH) and stroke mortality is essential for effective stroke management. Previous studies have uncovered significant regional disparities in the relationships between SDOH and stroke mortality. However, existing studies have not utilized longitudinal associations to develop data-driven methods for regional division in stroke control. To fill this gap, we propose a novel clustering method to analyze SDOH -- stroke mortality associations in US counties. To enhance the interpretability of the clustering outcomes, we introduce a novel regularized expectation-maximization algorithm equipped with various sparsity-and-smoothness-pursued penalties, aiming at simultaneous clustering and variable selection in longitudinal associations. As a result, we can identify crucial SDOH that contribute to longitudinal changes in stroke mortality. This facilitates the clustering of US counties into different regions based on the relationships between these SDOH and stroke mortality. The effectiveness of our proposed method is demonstrated through extensive numerical studies. By applying our method to longitudinal data on SDOH and stroke mortality at the county level, we identify 18 important SDOH for stroke mortality and divide the US counties into two clusters based on these selected SDOH. Our findings unveil complex regional heterogeneity in the longitudinal associations between SDOH and stroke mortality, providing valuable insights into region-specific SDOH adjustments for mitigating stroke mortality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10499v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fangzhi Luo, Jianbin Tan, Donglan Zhang, Hui Huang, Ye Shen</dc:creator>
    </item>
    <item>
      <title>Ablation Studies for Novel Treatment Effect Estimation Models</title>
      <link>https://arxiv.org/abs/2410.15560</link>
      <description>arXiv:2410.15560v2 Announce Type: replace 
Abstract: Ablation studies are essential for understanding the contribution of individual components within complex models, yet their application in nonparametric treatment effect estimation remains limited. This paper emphasizes the importance of ablation studies by examining the Bayesian Causal Forest (BCF) model, particularly the inclusion of the estimated propensity score $\hat{\pi}(x_i)$ intended to mitigate regularization-induced confounding (RIC). Through a partial ablation study utilizing a total of nine synthetic, we demonstrate that excluding $\hat{\pi}(x_i)$ does not diminish the model's performance in estimating average and conditional average treatment effects or in uncertainty quantification. Moreover, omitting $\hat{\pi}(x_i)$ reduces computational time by approximately 21%. These findings could suggest that the BCF model's inherent flexibility suffices in adjusting for confounding without explicitly incorporating the propensity score. The study advocates for the routine use of ablation studies in treatment effect estimation to ensure model components are essential and to prevent unnecessary complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15560v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hugo Gobato Souto, Francisco Louzada</dc:creator>
    </item>
    <item>
      <title>Studentized Tests of Independence: Random-Lifter approach</title>
      <link>https://arxiv.org/abs/2410.18437</link>
      <description>arXiv:2410.18437v2 Announce Type: replace 
Abstract: The exploration of associations between random objects with complex geometric structures has catalyzed the development of various novel statistical tests encompassing distance-based and kernel-based statistics. These methods have various strengths and limitations. One problem is that their test statistics tend to converge to asymptotic null distributions involving second-order Wiener chaos, which are hard to compute and need approximation or permutation techniques that use much computing power to build rejection regions. In this work, we take an entirely different and novel strategy by using the so-called ``Random-Lifter''. This method is engineered to yield test statistics with the standard normal limit under null distributions without the need for sample splitting. In other words, we set our sights on having simple limiting distributions and finding the proper statistics through reverse engineering. We use the Central Limit Theorems (CLTs) for degenerate U-statistics derived from our novel association measures to do this. As a result, the asymptotic distributions of our proposed tests are straightforward to compute. Our test statistics also have the minimax property. We further substantiate that our method maintains competitive power against existing methods with minimal adjustments to constant factors. Both numerical simulations and real-data analysis corroborate the efficacy of the Random-Lifter method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18437v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhe Gao, Roulin Wang, Xueqin Wang, Heping Zhang</dc:creator>
    </item>
    <item>
      <title>Rate-optimal estimation of mixed semimartingales</title>
      <link>https://arxiv.org/abs/2207.10464</link>
      <description>arXiv:2207.10464v2 Announce Type: replace-cross 
Abstract: Consider the sum $Y=B+B(H)$ of a Brownian motion $B$ and an independent fractional Brownian motion $B(H)$ with Hurst parameter $H\in(0,1)$. Even though $B(H)$ is not a semimartingale, it was shown in [\textit{Bernoulli} \textbf{7} (2001) 913--934] that $Y$ is a semimartingale if $H&gt;3/4$. Moreover, $Y$ is locally equivalent to $B$ in this case, so $H$ cannot be consistently estimated from local observations of $Y$. This paper pivots on another unexpected feature in this model: if $B$ and $B(H)$ become correlated, then $Y$ will never be a semimartingale, and $H$ can be identified, regardless of its value. This and other results will follow from a detailed statistical analysis of a more general class of processes called \emph{mixed semimartingales}, which are semiparametric extensions of $Y$ with stochastic volatility in both the martingale and the fractional component. In particular, we derive consistent estimators and feasible central limit theorems for all parameters and processes that can be identified from high-frequency observations. We further show that our estimators achieve optimal rates in a minimax sense.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.10464v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carsten H. Chong, Thomas Delerue, Fabian Mies</dc:creator>
    </item>
    <item>
      <title>Generalized logistic model for $r$ largest order statistics, with hydrological application</title>
      <link>https://arxiv.org/abs/2408.08764</link>
      <description>arXiv:2408.08764v2 Announce Type: replace-cross 
Abstract: The effective use of available information in extreme value analysis is critical because extreme values are scarce. Thus, using the $r$ largest order statistics (rLOS) instead of the block maxima is encouraged. Based on the four-parameter kappa model for the rLOS (rK4D), we introduce a new distribution for the rLOS as a special case of the rK4D. That is the generalized logistic model for rLOS (rGLO). This distribution can be useful when the generalized extreme value model for rLOS is no longer efficient to capture the variability of extreme values. Moreover, the rGLO enriches a pool of candidate distributions to determine the best model to yield accurate and robust quantile estimates. We derive a joint probability density function, the marginal and conditional distribution functions of new model. The maximum likelihood estimation, delta method, profile likelihood, order selection by the entropy difference test, cross-validated likelihood criteria, and model averaging were considered for inferences. The usefulness and practical effectiveness of the rGLO are illustrated by the Monte Carlo simulation and an application to extreme streamflow data in Bevern Stream, UK.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08764v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s00477-023-02642-7</arxiv:DOI>
      <arxiv:journal_reference>Stoch Environ Res Risk Assess 38 (2024) 1567-1581</arxiv:journal_reference>
      <dc:creator>Yire Shin, Jeong-Soo Park</dc:creator>
    </item>
    <item>
      <title>Vecchia Gaussian Processes: Probabilistic Properties, Minimax Rates and Methodological Developments</title>
      <link>https://arxiv.org/abs/2410.10649</link>
      <description>arXiv:2410.10649v2 Announce Type: replace-cross 
Abstract: Gaussian Processes (GPs) are widely used to model dependency in spatial statistics and machine learning, yet the exact computation suffers an intractable time complexity of $O(n^3)$. Vecchia approximation allows scalable Bayesian inference of GPs in $O(n)$ time by introducing sparsity in the spatial dependency structure that is characterized by a directed acyclic graph (DAG). Despite the popularity in practice, it is still unclear how to choose the DAG structure and there are still no theoretical guarantees in nonparametric settings. In this paper, we systematically study the Vecchia GPs as standalone stochastic processes and uncover important probabilistic properties and statistical results in methodology and theory. For probabilistic properties, we prove that the conditional distributions of the Mat\'{e}rn GPs, as well as the Vecchia approximations of the Mat\'{e}rn GPs, can be characterized by polynomials. This allows us to prove a series of results regarding the small ball probabilities and RKHSs of Vecchia GPs. For statistical methodology, we provide a principled guideline to choose parent sets as norming sets with fixed cardinality and provide detailed algorithms following such guidelines. For statistical theory, we prove posterior contraction rates for applying Vecchia GPs to regression problems, where minimax optimality is achieved by optimally tuned GPs via either oracle rescaling or hierarchical Bayesian methods. Our theory and methodology are demonstrated with numerical studies, where we also provide efficient implementation of our methods in C++ with R interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10649v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Botond Szabo, Yichen Zhu</dc:creator>
    </item>
  </channel>
</rss>
