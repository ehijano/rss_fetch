<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 15 Oct 2025 04:01:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On weak convergence of Gaussian conditional distributions</title>
      <link>https://arxiv.org/abs/2510.12412</link>
      <description>arXiv:2510.12412v1 Announce Type: new 
Abstract: Weak convergence of joint distributions generally does not imply convergence of conditional distributions. In particular, conditional distributions need not converge when joint Gaussian distributions converge to a singular Gaussian limit. Algebraically, this is due to the fact that at singular covariance matrices, Schur complements are not continuous functions of the matrix entries. Our results lay out special conditions under which convergence of Gaussian conditional distributions nevertheless occurs, and we exemplify how this allows one to reason about conditional independence in a new class of graphical models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12412v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarah Lumpp, Mathias Drton</dc:creator>
    </item>
    <item>
      <title>On estimation of weighted cumulative residual Tsallis entropy</title>
      <link>https://arxiv.org/abs/2510.12442</link>
      <description>arXiv:2510.12442v1 Announce Type: new 
Abstract: Recently, weighted cumulative residual Tsallis entropy has been introduced in the literature as a generalization of weighted cumulative residual entropy. We study some new properties of weighted cumulative residual Tsallis entropy measure. Next, we propose some non-parametric estimators of this measure. Asymptotic properties of these estimators are discussed. Performance of these estimators are compared by mean squared error. Non-parametric estimators for weighted cumulative residual entropy measure are also discussed. Two uniformity tests are proposed based on an estimator of these two measures and power of the tests are compared with some popular tests. The tests perform reasonably well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12442v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siddhartha Chakraborty, Asok K. Nanda</dc:creator>
    </item>
    <item>
      <title>Universal behaviors of the multi-time correlation functions of random processes with renewal: the step noise case (the random velocity of a L\'evy walk)</title>
      <link>https://arxiv.org/abs/2510.11747</link>
      <description>arXiv:2510.11747v1 Announce Type: cross 
Abstract: Stochastic processes with renewal properties are powerful tools for modeling systems where memory effects and long-time correlations play a significant role. In this work, we study a broad class of renewal processes where a variable's value changes according to a prescribed Probability Density Function (PDF), $p(\xi)$, after random waiting times $\theta$. This model is relevant across many fields, including classical chaos, nonlinear hydrodynamics, quantum dots, cold atom dynamics, biological motion, foraging, and finance. We derive a general analytical expression for the $n$-time correlation function by averaging over process realizations. Our analysis identifies the conditions for stationarity, aging, and long-range correlations based on the waiting time and jump distributions. Among the many consequences of our analysis, two new key results emerge. First, for Poissonian waiting times, the correlation function quickly approaches that of telegraphic noise. Second, for power-law waiting times with $\mu&gt;2$, , \emph{any $n$-time correlation function asymptotically reduces to the two-time correlation evaluated at the earliest and latest time points}. This second result reveals a universal long-time behavior where the system's full statistical structure becomes effectively two-time reducible. Furthermore, if the jump PDF $p(\xi)$ has fat tails, this convergence becomes independent of the waiting time PDF and is significantly accelerated, requiring only modest increases in either the number of realizations or the trajectory lengths. Building upon earlier work that established the universality of the two-point correlation function (i.e., a unique formal expression depending solely on the variance of $\xi$ and on the waiting-time PDF), the present study extends that universality to the full statistical description of a broad class of renewal-type stochastic processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11747v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Bianucci, Mauro Bologna, Daniele Lagomarsino-Oneto, Riccardo Mannella</dc:creator>
    </item>
    <item>
      <title>Dimension-Free Minimax Rates for Learning Pairwise Interactions in Attention-Style Models</title>
      <link>https://arxiv.org/abs/2510.11789</link>
      <description>arXiv:2510.11789v1 Announce Type: cross 
Abstract: We study the convergence rate of learning pairwise interactions in single-layer attention-style models, where tokens interact through a weight matrix and a non-linear activation function. We prove that the minimax rate is $M^{-\frac{2\beta}{2\beta+1}}$ with $M$ being the sample size, depending only on the smoothness $\beta$ of the activation, and crucially independent of token count, ambient dimension, or rank of the weight matrix. These results highlight a fundamental dimension-free statistical efficiency of attention-style nonlocal models, even when the weight matrix and activation are not separately identifiable and provide a theoretical understanding of the attention mechanism and its training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11789v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shai Zucker, Xiong Wang, Fei Lu, Inbar Seroussi</dc:creator>
    </item>
    <item>
      <title>Contrastive Dimension Reduction: A Systematic Review</title>
      <link>https://arxiv.org/abs/2510.11847</link>
      <description>arXiv:2510.11847v1 Announce Type: cross 
Abstract: Contrastive dimension reduction (CDR) methods aim to extract signal unique to or enriched in a treatment (foreground) group relative to a control (background) group. This setting arises in many scientific domains, such as genomics, imaging, and time series analysis, where traditional dimension reduction techniques such as Principal Component Analysis (PCA) may fail to isolate the signal of interest. In this review, we provide a systematic overview of existing CDR methods. We propose a pipeline for analyzing case-control studies together with a taxonomy of CDR methods based on their assumptions, objectives, and mathematical formulations, unifying disparate approaches under a shared conceptual framework. We highlight key applications and challenges in existing CDR methods, and identify open questions and future directions. By providing a clear framework for CDR and its applications, we aim to facilitate broader adoption and motivate further developments in this emerging field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11847v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sam Hawke, Eric Zhang, Jiawen Chen, Didong Li</dc:creator>
    </item>
    <item>
      <title>A Martingale Kernel Two-Sample Test</title>
      <link>https://arxiv.org/abs/2510.11853</link>
      <description>arXiv:2510.11853v1 Announce Type: cross 
Abstract: The Maximum Mean Discrepancy (MMD) is a widely used multivariate distance metric for two-sample testing. The standard MMD test statistic has an intractable null distribution typically requiring costly resampling or permutation approaches for calibration. In this work we leverage a martingale interpretation of the estimated squared MMD to propose martingale MMD (mMMD), a quadratic-time statistic which has a limiting standard Gaussian distribution under the null. Moreover we show that the test is consistent against any fixed alternative and for large sample sizes, mMMD offers substantial computational savings over the standard MMD test, with only a minor loss in power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11853v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anirban Chatterjee, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Optimal break tests for large linear time series models</title>
      <link>https://arxiv.org/abs/2510.12262</link>
      <description>arXiv:2510.12262v1 Announce Type: cross 
Abstract: We develop a class of optimal tests for a structural break occurring at an unknown date in infinite and growing-order time series regression models, such as AR($\infty$), linear regression with increasingly many covariates, and nonparametric regression. Under an auxiliary i.i.d. Gaussian error assumption, we derive an average power optimal test, establishing a growing-dimensional analog of the exponential tests of Andrews and Ploberger (1994) to handle identification failure under the null hypothesis of no break. Relaxing the i.i.d. Gaussian assumption to a more general dependence structure, we establish a functional central limit theorem for the underlying stochastic processes, which features an extra high-order serial dependence term due to the growing dimension. We robustify our test both against this term and finite sample bias and illustrate its excellent performance and practical relevance in a Monte Carlo study and a real data empirical example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12262v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhimanyu Gupta, Myung Hwan Seo</dc:creator>
    </item>
    <item>
      <title>Optimal Treatment Rules under Missing Predictive Covariates: A Covariate-Balancing Doubly Robust Approach</title>
      <link>https://arxiv.org/abs/2510.12321</link>
      <description>arXiv:2510.12321v1 Announce Type: cross 
Abstract: In precision medicine, one of the most important problems is estimating the optimal individualized treatment rules (ITR), which typically involves recommending treatment decisions based on fully observed individual characteristics of patients to maximize overall clinical benefit. In practice, however, there may be missing covariates that are not necessarily confounders, and it remains uncertain whether these missing covariates should be included for learning optimal ITRs. In this paper, we propose a covariate-balancing doubly robust estimator for constructing optimal ITRs, which is particularly suitable for situations with additional predictive covariates. The proposed method is based on two main steps: First, the propensity scores are estimated by solving the covariate-balancing equation. Second, an objective function is minimized to estimate the outcome model, with the function defined by the asymptotic variance under the correctly specified propensity score. The method has three significant advantages: (i) It is doubly robust, ensuring consistency when either the propensity score or outcome model is correctly specified. (ii) It minimizes variance within the class of augmented inverse probability weighted estimators. (iii) When applied to partially observed covariates related to the outcome, the method may further improve estimation efficiency. We demonstrate the proposed method through extensive numerical simulations and two real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12321v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Zhang, Shanshan Luo, Zhi Geng, Yangbo He</dc:creator>
    </item>
    <item>
      <title>Improved Central Limit Theorem and Bootstrap Approximations for Linear Stochastic Approximation</title>
      <link>https://arxiv.org/abs/2510.12375</link>
      <description>arXiv:2510.12375v1 Announce Type: cross 
Abstract: In this paper, we refine the Berry-Esseen bounds for the multivariate normal approximation of Polyak-Ruppert averaged iterates arising from the linear stochastic approximation (LSA) algorithm with decreasing step size. We consider the normal approximation by the Gaussian distribution with covariance matrix predicted by the Polyak-Juditsky central limit theorem and establish the rate up to order $n^{-1/3}$ in convex distance, where $n$ is the number of samples used in the algorithm. We also prove a non-asymptotic validity of the multiplier bootstrap procedure for approximating the distribution of the rescaled error of the averaged LSA estimator. We establish approximation rates of order up to $1/\sqrt{n}$ for the latter distribution, which significantly improves upon the previous results obtained by Samsonov et al. (2024).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12375v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bogdan Butyrin, Eric Moulines, Alexey Naumov, Sergey Samsonov, Qi-Man Shao, Zhuo-Song Zhang</dc:creator>
    </item>
    <item>
      <title>Dendrograms of Mixing Measures for Softmax-Gated Gaussian Mixture of Experts: Consistency without Model Sweeps</title>
      <link>https://arxiv.org/abs/2510.12744</link>
      <description>arXiv:2510.12744v1 Announce Type: cross 
Abstract: We develop a unified statistical framework for softmax-gated Gaussian mixture of experts (SGMoE) that addresses three long-standing obstacles in parameter estimation and model selection: (i) non-identifiability of gating parameters up to common translations, (ii) intrinsic gate-expert interactions that induce coupled differential relations in the likelihood, and (iii) the tight numerator-denominator coupling in the softmax-induced conditional density. Our approach introduces Voronoi-type loss functions aligned with the gate-partition geometry and establishes finite-sample convergence rates for the maximum likelihood estimator (MLE). In over-specified models, we reveal a link between the MLE's convergence rate and the solvability of an associated system of polynomial equations characterizing near-nonidentifiable directions. For model selection, we adapt dendrograms of mixing measures to SGMoE, yielding a consistent, sweep-free selector of the number of experts that attains pointwise-optimal parameter rates under overfitting while avoiding multi-size training. Simulations on synthetic data corroborate the theory, accurately recovering the expert count and achieving the predicted rates for parameter estimation while closely approximating the regression function. Under model misspecification (e.g., $\epsilon$-contamination), the dendrogram selection criterion is robust, recovering the true number of mixture components, while the Akaike information criterion, the Bayesian information criterion, and the integrated completed likelihood tend to overselect as sample size grows. On a maize proteomics dataset of drought-responsive traits, our dendrogram-guided SGMoE selects two experts, exposes a clear mixing-measure hierarchy, stabilizes the likelihood early, and yields interpretable genotype-phenotype maps, outperforming standard criteria without multi-size training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12744v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Do Tien Hai, Trung Nguyen Mai, TrungTin Nguyen, Nhat Ho, Binh T. Nguyen, Christopher Drovandi</dc:creator>
    </item>
    <item>
      <title>Identifiability and Falsifiability: Two Challenges for Bayesian Model Expansion</title>
      <link>https://arxiv.org/abs/2307.14545</link>
      <description>arXiv:2307.14545v2 Announce Type: replace 
Abstract: We study the identifiability of parameters and falsifiability of predictions under the process of model expansion in a Bayesian setting. Identifiability is represented by the closeness of the posterior to the prior distribution and falsifiability by the power of posterior predictive tests against alternatives. To study these two concepts formally, we develop information-theoretic proxies, which we term the identifiability and falsifiability mutual information. We argue that these are useful indicators, with lower values indicating a risk of poor parameter inference and underpowered model checks, respectively. Our main result establishes that a sufficiently complex expansion of a base statistical model forces a trade-off between these two mutual information quantities -- at least one of the two must decrease relative to the base model. We illustrate our result in three worked examples and extract implications for model expansion in practice. In particular, we show as an implication of our result that the negative impacts of model expansion can be limited by offsetting complexity in the likelihood with sufficiently constraining prior distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.14545v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Collin Cademartori</dc:creator>
    </item>
    <item>
      <title>Characterizing extremal dependence on a hyperplane</title>
      <link>https://arxiv.org/abs/2411.00573</link>
      <description>arXiv:2411.00573v3 Announce Type: replace 
Abstract: In this paper, we characterize the extremal dependence of $d$ asymptotically dependent variables by a class of random vectors on the $(d-1)$-dimensional hyperplane perpendicular to the diagonal vector $\mathbf1=(1,\ldots,1)$. This translates analyses of multivariate extremes to that on a linear vector space, opening up possibilities for applying existing statistical techniques that are based on linear operations. As an example, we demonstrate obtaining lower-dimensional approximations of the tail dependence through principal component analysis. Additionally, we show that the widely used H\"usler-Reiss family is characterized by a Gaussian family residing on the hyperplane.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00573v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Phyllis Wan</dc:creator>
    </item>
    <item>
      <title>Power comparison of sequential testing by betting procedures</title>
      <link>https://arxiv.org/abs/2504.00593</link>
      <description>arXiv:2504.00593v2 Announce Type: replace 
Abstract: In this paper, we derive power guarantees of some sequential tests for bounded mean under general alternatives. We focus on testing procedures using nonnegative supermartingales which are anytime valid and consider alternatives which coincide asymptotically with the null (e.g. vanishing mean) while still allowing to reject in finite time. Introducing variance constraints, we show that the alternative can be broaden while keeping power guarantees for certain second-order testing procedures. We also compare different test procedures in multidimensional setting using characteristics of the rejection times. Finally, we extend our analysis to other functionals as well as testing and comparing forecasters. Our results are illustrated with numerical simulations including bounded mean testing and comparison of forecasters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00593v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amaury Durand (EDF R\&amp;D OSIRIS), Olivier Wintenberger (SU)</dc:creator>
    </item>
    <item>
      <title>Gaussian Approximation for High-Dimensional $U$-statistics with Size-Dependent Kernels</title>
      <link>https://arxiv.org/abs/2504.10866</link>
      <description>arXiv:2504.10866v2 Announce Type: replace 
Abstract: Motivated by small bandwidth asymptotics for kernel-based semiparametric estimators in econometrics, this paper establishes Gaussian approximation results for high-dimensional fixed-order $U$-statistics whose kernels depend on the sample size. Our results allow for a situation where the dominant component of the Hoeffding decomposition is absent or unknown, including cases with known degrees of degeneracy as special forms. The obtained error bounds for Gaussian approximations are sharp enough to almost recover the weakest bandwidth condition of small bandwidth asymptotics in the fixed-dimensional setting when applied to a canonical semiparametric estimation problem. We also present an application to an adaptive goodness-of-fit testing and the simultaneous inference on high-dimensional density weighted averaged derivatives, along with discussions about several potential applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10866v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunsuke Imai, Yuta Koike</dc:creator>
    </item>
    <item>
      <title>Effective regions and kernels in continuous sparse regularisation, with application to sketched mixtures</title>
      <link>https://arxiv.org/abs/2507.08444</link>
      <description>arXiv:2507.08444v3 Announce Type: replace 
Abstract: This paper advances the general theory of continuous sparse regularisation on measures with the Beurling-LASSO (BLASSO). This TV-regularised convex program on the space of measures allows to recover a sparse measure using a noisy observation from a measurement operator. While previous works have uncovered the central role played by this operator and its associated kernel in order to get estimation error bounds, the latter requires a technical local positive curvature (LPC) assumption to be verified on a case-by-case basis. In practice, this yields only few LPC-kernels for which this condition is proved. In this paper, we prove that the ``sinc-4'' kernel, used for signal recovery and mixture problems, does satisfy the LPC assumption. Furthermore, we introduce the kernel switch analysis, which allows to leverage on a known LPC-kernel as a pivot kernel to prove error bounds. Together, these results provide easy-to-check conditions to get error bounds for a large family of translation-invariant model kernels. Besides, we also show that known BLASSO guarantees can be made adaptive to the noise level. This improves on known results where this error is fixed with some parameters depending on the model kernel. We illustrate the interest of our results in the case of mixture model estimation, using band-limiting smoothing and sketching techniques to reduce the computational burden of BLASSO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08444v3</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yohann De Castro, R\'emi Gribonval, Nicolas Jouvin</dc:creator>
    </item>
    <item>
      <title>Gaussian Sequence Model: Sample Complexities of Testing, Estimation and LFHT</title>
      <link>https://arxiv.org/abs/2507.16734</link>
      <description>arXiv:2507.16734v3 Announce Type: replace 
Abstract: We study the Gaussian sequence model, i.e. $X \sim N(\mathbf{\theta}, I_\infty)$, where $\mathbf{\theta} \in \Gamma \subset \ell_2$ is assumed to be convex and compact. We show that goodness-of-fit testing sample complexity is lower bounded by the square-root of the estimation complexity, whenever $\Gamma$ is orthosymmetric. This lower bound is tight when $\Gamma$ is also quadratically convex (as shown by [Donoho et al. 1990, Neykov 2023]). We also completely characterize likelihood-free hypothesis testing (LFHT) complexity for $\ell_p$-bodies, discovering new types of tradeoff between the numbers of simulation and observation samples, compared to the case of ellipsoids (p = 2) studied in [Gerber and Polyanskiy 2024].</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16734v3</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeyu Jia, Yury Polyanskiy</dc:creator>
    </item>
    <item>
      <title>Structured covariance estimation via tensor-train decomposition</title>
      <link>https://arxiv.org/abs/2510.08174</link>
      <description>arXiv:2510.08174v2 Announce Type: replace 
Abstract: We consider a problem of covariance estimation from a sample of i.i.d. high-dimensional random vectors. To avoid the curse of dimensionality we impose an additional assumption on the structure of the covariance matrix $\Sigma$. To be more precise we study the case when $\Sigma$ can be approximated by a sum of double Kronecker products of smaller matrices in a tensor train (TT) format. Our setup naturally extends widely known Kronecker sum and CANDECOMP/PARAFAC models but admits richer interaction across modes. We suggest an iterative polynomial time algorithm based on TT-SVD and higher-order orthogonal iteration (HOOI) adapted to Tucker-2 hybrid structure. We derive non-asymptotic dimension-free bounds on the accuracy of covariance estimation taking into account hidden Kronecker product and tensor train structures. The efficiency of our approach is illustrated with numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08174v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Artsiom Patarusau, Nikita Puchkin, Maxim Rakhuba, Fedor Noskov</dc:creator>
    </item>
    <item>
      <title>Beyond the noise: intrinsic dimension estimation with optimal neighbourhood identification</title>
      <link>https://arxiv.org/abs/2405.15132</link>
      <description>arXiv:2405.15132v3 Announce Type: replace-cross 
Abstract: The Intrinsic Dimension (ID) is a key concept in unsupervised learning and feature selection, as it is a lower bound to the number of variables which are necessary to describe a system. However, in almost any real-world dataset the ID depends on the scale at which the data are analysed. Quite typically at a small scale, the ID is very large, as the data are affected by measurement errors. At large scale, the ID can also be erroneously large, due to the curvature and the topology of the manifold containing the data. In this work, we introduce an automatic protocol to select the sweet spot, namely the correct range of scales in which the ID is meaningful and useful. This protocol is based on imposing that for distances smaller than the correct scale the density of the data is constant. In the presented framework, to estimate the density it is necessary to know the ID, therefore, this condition is imposed self-consistently. We illustrate the usefulness and robustness of this procedure by benchmarks on artificial and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15132v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonio Di Noia, Iuri Macocco, Aldo Glielmo, Alessandro Laio, Antonietta Mira</dc:creator>
    </item>
    <item>
      <title>Bayesian Calibration for Prediction in a Multi-Output Transposition Context</title>
      <link>https://arxiv.org/abs/2410.00116</link>
      <description>arXiv:2410.00116v4 Announce Type: replace-cross 
Abstract: Numerical simulations are widely used to predict the behavior of physical systems, with Bayesian approaches being particularly well suited for this purpose. However, experimental observations are necessary to calibrate certain simulator parameters for the prediction. In this work, we use a multi-output simulator to predict all its outputs, including those that have never been experimentally observed. This situation is referred to as the transposition context. To accurately quantify the discrepancy between model outputs and real data in this context, conventional methods cannot be applied, and the Bayesian calibration must be augmented by incorporating a joint model error across all outputs. To achieve this, the proposed method is to consider additional numerical input parameters within a hierarchical Bayesian model, which includes hyperparameters for the prior distribution of the calibration variables. This approach is applied on a computer code with three outputs that models the Taylor cylinder impact test with a small number of observations. The outputs are considered as the observed variables one at a time, to work with three different transposition situations. The proposed method is compared with other approaches that embed model errors to demonstrate the significance of the hierarchical formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00116v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1615/Int.J.UncertaintyQuantification.2025056586</arxiv:DOI>
      <arxiv:journal_reference>International Journal for Uncertainty Quantification, Volume 15, Issue 6, 2025, pp. 37-59</arxiv:journal_reference>
      <dc:creator>Charlie Sire, Josselin Garnier, C\'edric Durantin, Baptiste Kerleguer, Gilles Defaux, Guillaume Perrin</dc:creator>
    </item>
    <item>
      <title>Bootstrap tests for almost goodness-of-fit</title>
      <link>https://arxiv.org/abs/2410.20918</link>
      <description>arXiv:2410.20918v2 Announce Type: replace-cross 
Abstract: We introduce the \textit{almost goodness-of-fit} test, a procedure to assess whether a (parametric) model provides a good representation of the probability distribution generating the observed sample. Specifically, given a distribution function $F$ and a parametric family $\mathcal{G}=\{ G(\boldsymbol{\theta}) : \boldsymbol{\theta} \in \Theta\}$, we consider the testing problem \[ H_0: \| F - G(\boldsymbol{\theta}_F) \|_p \geq \epsilon \quad \text{vs} \quad H_1: \| F - G(\boldsymbol{\theta}_F) \|_p &lt; \epsilon, \] where $\epsilon&gt;0$ is a margin of error and $G(\boldsymbol{\theta}_F)$ denotes a representative of $F$ within the parametric class. The approximate model is determined via an M-estimator of the parameters. %The objective is the approximate validation of a distribution or an entire parametric family up to a pre-specified threshold value. The methodology also quantifies the percentage improvement of the proposed model relative to a non-informative (constant) benchmark. The test statistic is the $\mathrm{L}^p$-distance between the empirical distribution function and that of the estimated model. We present two consistent, easy-to-implement, and flexible bootstrap schemes to carry out the test. The performance of the proposal is illustrated through simulation studies and analysis and real-data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20918v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Statistics and Computing, 2025</arxiv:journal_reference>
      <dc:creator>Amparo Ba\'illo, Javier C\'arcamo</dc:creator>
    </item>
    <item>
      <title>A Statistical Theory of Contrastive Learning via Approximate Sufficient Statistics</title>
      <link>https://arxiv.org/abs/2503.17538</link>
      <description>arXiv:2503.17538v2 Announce Type: replace-cross 
Abstract: Contrastive learning -- a modern approach to extract useful representations from unlabeled data by training models to distinguish similar samples from dissimilar ones -- has driven significant progress in foundation models. In this work, we develop a new theoretical framework for analyzing data augmentation-based contrastive learning, with a focus on SimCLR as a representative example. Our approach is based on the concept of \emph{approximate sufficient statistics}, which we extend beyond its original definition in \cite{oko2025statistical} for contrastive language-image pretraining (CLIP) using KL-divergence. We generalize it to equivalent forms and general f-divergences, and show that minimizing SimCLR and other contrastive losses yields encoders that are approximately sufficient. Furthermore, we demonstrate that these near-sufficient encoders can be effectively adapted to downstream regression and classification tasks, with performance depending on their sufficiency and the error induced by data augmentation in contrastive learning. Concrete examples in linear regression and topic classification are provided to illustrate the broad applicability of our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17538v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Licong Lin, Song Mei</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Variable Selection in Model-Based Clustering with Missing Not at Random</title>
      <link>https://arxiv.org/abs/2505.19093</link>
      <description>arXiv:2505.19093v2 Announce Type: replace-cross 
Abstract: Model-based clustering integrated with variable selection is a powerful tool for uncovering latent structures within complex data. However, its effectiveness is often hindered by challenges such as identifying relevant variables that define heterogeneous subgroups and handling data that are missing not at random, a prevalent issue in fields like transcriptomics. While several notable methods have been proposed to address these problems, they typically tackle each issue in isolation, thereby limiting their flexibility and adaptability. This paper introduces a unified framework designed to address these challenges simultaneously. Our approach incorporates a data-driven penalty matrix into penalized clustering to enable more flexible variable selection, along with a mechanism that explicitly models the relationship between missingness and latent class membership. We demonstrate that, under certain regularity conditions, the proposed framework achieves both asymptotic consistency and selection consistency, even in the presence of missing data. This unified strategy significantly enhances the capability and efficiency of model-based clustering, advancing methodologies for identifying informative variables that define homogeneous subgroups in the presence of complex missing data patterns. The performance of the framework, including its computational efficiency, is evaluated through simulations and demonstrated using both synthetic and real-world transcriptomic datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19093v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>39th Conference on Neural Information Processing Systems (NeurIPS 2025)</arxiv:journal_reference>
      <dc:creator>Binh H. Ho, Long Nguyen Chi, TrungTin Nguyen, Binh T. Nguyen, Van Ha Hoang, Christopher Drovandi</dc:creator>
    </item>
  </channel>
</rss>
