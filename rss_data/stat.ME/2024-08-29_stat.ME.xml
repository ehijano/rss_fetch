<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 Aug 2024 04:00:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>ROMI: A Randomized Two-Stage Basket Trial Design to Optimize Doses for Multiple Indications</title>
      <link>https://arxiv.org/abs/2408.15502</link>
      <description>arXiv:2408.15502v1 Announce Type: new 
Abstract: Optimizing doses for multiple indications is challenging. The pooled approach of finding a single optimal biological dose (OBD) for all indications ignores that dose-response or dose-toxicity curves may differ between indications, resulting in varying OBDs. Conversely, indication-specific dose optimization often requires a large sample size. To address this challenge, we propose a Randomized two-stage basket trial design that Optimizes doses in Multiple Indications (ROMI). In stage 1, for each indication, response and toxicity are evaluated for a high dose, which may be a previously obtained MTD, with a rule that stops accrual to indications where the high dose is unsafe or ineffective. Indications not terminated proceed to stage 2, where patients are randomized between the high dose and a specified lower dose. A latent-cluster Bayesian hierarchical model is employed to borrow information between indications, while considering the potential heterogeneity of OBD across indications. Indication-specific utilities are used to quantify response-toxicity trade-offs. At the end of stage 2, for each indication with at least one acceptable dose, the dose with highest posterior mean utility is selected as optimal. Two versions of ROMI are presented, one using only stage 2 data for dose optimization and the other optimizing doses using data from both stages. Simulations show that both versions have desirable operating characteristics compared to designs that either ignore indications or optimize dose independently for each indication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15502v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuqi Wang, Peter F. Thall, Kentaro Takeda, Ying Yuan</dc:creator>
    </item>
    <item>
      <title>Comparing restricted mean survival times in small sample clinical trials using pseudo-observations</title>
      <link>https://arxiv.org/abs/2408.15607</link>
      <description>arXiv:2408.15607v1 Announce Type: new 
Abstract: The widely used proportional hazard assumption cannot be assessed reliably in small-scale clinical trials and might often in fact be unjustified, e.g. due to delayed treatment effects. An alternative to the hazard ratio as effect measure is the difference in restricted mean survival time (RMST) that does not rely on model assumptions. Although an asymptotic test for two-sample comparisons of the RMST exists, it has been shown to suffer from an inflated type I error rate in samples of small or moderate sizes. Recently, permutation tests, including the studentized permutation test, have been introduced to address this issue. In this paper, we propose two methods based on pseudo-observations (PO) regression models as alternatives for such scenarios and assess their properties in comparison to previously proposed approaches in an extensive simulation study. Furthermore, we apply the proposed PO methods to data from a clinical trail and, by doing so, point out some extension that might be very useful for practical applications such as covariate adjustments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15607v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Jesse, Cynthia Huber, Tim Friede</dc:creator>
    </item>
    <item>
      <title>Network Representation of Higher-Order Interactions Based on Information Dynamics</title>
      <link>https://arxiv.org/abs/2408.15617</link>
      <description>arXiv:2408.15617v1 Announce Type: new 
Abstract: Many complex systems in science and engineering are modeled as networks whose nodes and links depict the temporal evolution of each system unit and the dynamic interaction between pairs of units, which are assessed respectively using measures of auto- and cross-correlation or variants thereof. However, a growing body of work is documenting that this standard network representation can neglect potentially crucial information shared by three or more dynamic processes in the form of higher-order interactions (HOIs). While several measures, mostly derived from information theory, are available to assess HOIs in network systems mapped by multivariate time series, none of them is able to provide a compact and detailed representation of higher-order interdependencies. In this work, we fill this gap by introducing a framework for the assessment of HOIs in dynamic network systems at different levels of resolution. The framework is grounded on the dynamic implementation of the O-information, a new measure assessing HOIs in dynamic networks, which is here used together with its local counterpart and its gradient to quantify HOIs respectively for the network as a whole, for each link, and for each node. The integration of these measures into the conventional network representation results in a tool for the representation of HOIs as networks, which is defined formally using measures of information dynamics, implemented in its linear version by using vector regression models and statistical validation techniques, illustrated in simulated network systems, and finally applied to an illustrative example in the field of network physiology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15617v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gorana Mijatovic, Yuri Antonacci, Michal Javorka, Daniele Marinazzo, Sebastiano Stramaglia, Luca Faes</dc:creator>
    </item>
    <item>
      <title>Correlation-Adjusted Simultaneous Testing for Ultra High-dimensional Grouped Data</title>
      <link>https://arxiv.org/abs/2408.15623</link>
      <description>arXiv:2408.15623v1 Announce Type: new 
Abstract: Epigenetics plays a crucial role in understanding the underlying molecular processes of several types of cancer as well as the determination of innovative therapeutic tools. To investigate the complex interplay between genetics and environment, we develop a novel procedure to identify differentially methylated probes (DMPs) among cases and controls. Statistically, this translates to an ultra high-dimensional testing problem with sparse signals and an inherent grouping structure. When the total number of variables being tested is massive and typically exhibits some degree of dependence, existing group-wise multiple comparisons adjustment methods lead to inflated false discoveries. We propose a class of Correlation-Adjusted Simultaneous Testing (CAST) procedures incorporating the general dependence among probes within and between genes to control the false discovery rate (FDR). Simulations demonstrate that CASTs have superior empirical power while maintaining the FDR compared to the benchmark group-wise. Moreover, while the benchmark fails to control FDR for small-sized grouped correlated data, CAST exhibits robustness in controlling FDR across varying group sizes. In bladder cancer data, the proposed CAST method confirms some existing differentially methylated probes implicated with the disease (Langevin, et. al., 2014). However, CAST was able to detect novel DMPs that the previous study (Langevin, et. al., 2014) failed to identify. The CAST method can accurately identify significant potential biomarkers and facilitates informed decision-making aligned with precision medicine in the context of complex data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15623v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iris Ivy Gauran, Patrick Wincy Reyes, Erniel Barrios, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Adaptive Weighted Random Isolation (AWRI): a simple design to estimate causal effects under network interference</title>
      <link>https://arxiv.org/abs/2408.15670</link>
      <description>arXiv:2408.15670v1 Announce Type: new 
Abstract: Recently, causal inference under interference has gained increasing attention in the literature. In this paper, we focus on randomized designs for estimating the total treatment effect (TTE), defined as the average difference in potential outcomes between fully treated and fully controlled groups. We propose a simple design called weighted random isolation (WRI) along with a restricted difference-in-means estimator (RDIM) for TTE estimation. Additionally, we derive a novel mean squared error surrogate for the RDIM estimator, supported by a network-adaptive weight selection algorithm. This can help us determine a fair weight for the WRI design, thereby effectively reducing the bias. Our method accommodates directed networks, extending previous frameworks. Extensive simulations demonstrate that the proposed method outperforms nine established methods across a wide range of scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15670v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changhao Shi, Haoyu Yang, Yichen Qin, Yang Li</dc:creator>
    </item>
    <item>
      <title>Robust discriminant analysis</title>
      <link>https://arxiv.org/abs/2408.15701</link>
      <description>arXiv:2408.15701v1 Announce Type: new 
Abstract: Discriminant analysis (DA) is one of the most popular methods for classification due to its conceptual simplicity, low computational cost, and often solid performance. In its standard form, DA uses the arithmetic mean and sample covariance matrix to estimate the center and scatter of each class. We discuss and illustrate how this makes standard DA very sensitive to suspicious data points, such as outliers and mislabeled cases. We then present an overview of techniques for robust DA, which are more reliable in the presence of deviating cases. In particular, we review DA based on robust estimates of location and scatter, along with graphical diagnostic tools for visualizing the results of DA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15701v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mia Hubert, Jakob Raymaekers, Peter J. Rousseeuw</dc:creator>
    </item>
    <item>
      <title>Bayesian analysis of product feature allocation models</title>
      <link>https://arxiv.org/abs/2408.15806</link>
      <description>arXiv:2408.15806v1 Announce Type: new 
Abstract: Feature allocation models are an extension of Bayesian nonparametric clustering models, where individuals can share multiple features. We study a broad class of models whose probability distribution has a product form, which includes the popular Indian buffet process. This class plays a prominent role among existing priors, and it shares structural characteristics with Gibbs-type priors in the species sampling framework. We develop a general theory for the entire class, obtaining closed form expressions for the predictive structure and the posterior law of the underlying stochastic process. Additionally, we describe the distribution for the number of features and the number of hitherto unseen features in a future sample, leading to the $\alpha$-diversity for feature models. We also examine notable novel examples, such as mixtures of Indian buffet processes and beta Bernoulli models, where the latter entails a finite random number of features. This methodology finds significant applications in ecology, allowing the estimation of species richness for incidence data, as we demonstrate by analyzing plant diversity in Danish forests and trees in Barro Colorado Island.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15806v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Ghilotti, Federico Camerlenghi, Tommaso Rigon</dc:creator>
    </item>
    <item>
      <title>On harmonic oscillator hazard functions</title>
      <link>https://arxiv.org/abs/2408.15964</link>
      <description>arXiv:2408.15964v1 Announce Type: new 
Abstract: We propose a parametric hazard model obtained by enforcing positivity in the damped harmonic oscillator. The resulting model has closed-form hazard and cumulative hazard functions, facilitating likelihood and Bayesian inference on the parameters. We show that this model can capture a range of hazard shapes, such as increasing, decreasing, unimodal, bathtub, and oscillatory patterns, and characterize the tails of the corresponding survival function. We illustrate the use of this model in survival analysis using real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15964v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J. A. Christen, F. J. Rubio</dc:creator>
    </item>
    <item>
      <title>Comparing the Pearson and Spearman Correlation Coefficients Across Distributions and Sample Sizes: A Tutorial Using Simulations and Empirical Data</title>
      <link>https://arxiv.org/abs/2408.15979</link>
      <description>arXiv:2408.15979v1 Announce Type: new 
Abstract: The Pearson product-moment correlation coefficient (rp) and the Spearman rank correlation coefficient (rs) are widely used in psychological research. We compare rp and rs on 3 criteria: variability, bias with respect to the population value, and robustness to an outlier. Using simulations across low (N = 5) to high (N = 1,000) sample sizes we show that, for normally distributed variables, rp and rs have similar expected values but rs is more variable, especially when the correlation is strong. However, when the variables have high kurtosis, rp is more variable than rs. Next, we conducted a sampling study of a psychometric dataset featuring symmetrically distributed data with light tails, and of 2 Likert-type survey datasets, 1 with light-tailed and the other with heavy-tailed distributions. Consistent with the simulations, rp had lower variability than rs in the psychometric dataset. In the survey datasets with heavy-tailed variables in particular, rs had lower variability than rp, and often corresponded more accurately to the population Pearson correlation coefficient (Rp) than rp did. The simulations and the sampling studies showed that variability in terms of standard deviations can be reduced by about 20% by choosing rs instead of rp. In comparison, increasing the sample size by a factor of 2 results in a 41% reduction of the standard deviations of rs and rp. In conclusion, rp is suitable for light-tailed distributions, whereas rs is preferable when variables feature heavy-tailed distributions or when outliers are present, as is often the case in psychological research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15979v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1037/met0000079</arxiv:DOI>
      <arxiv:journal_reference>Psychological Methods 21 (2016) 273-290</arxiv:journal_reference>
      <dc:creator>J. C. F. de Winter, S. D. Gosling, J. Potter</dc:creator>
    </item>
    <item>
      <title>Artificial Data, Real Insights: Evaluating Opportunities and Risks of Expanding the Data Ecosystem with Synthetic Data</title>
      <link>https://arxiv.org/abs/2408.15260</link>
      <description>arXiv:2408.15260v1 Announce Type: cross 
Abstract: Synthetic Data is not new, but recent advances in Generative AI have raised interest in expanding the research toolbox, creating new opportunities and risks. This article provides a taxonomy of the full breadth of the Synthetic Data domain. We discuss its place in the research ecosystem by linking the advances in computational social science with the idea of the Fourth Paradigm of scientific discovery that integrates the elements of the evolution from empirical to theoretic to computational models. Further, leveraging the framework of Truth, Beauty, and Justice, we discuss how evaluation criteria vary across use cases as the information is used to add value and draw insights. Building a framework to organize different types of synthetic data, we end by describing the opportunities and challenges with detailed examples of using Generative AI to create synthetic quantitative and qualitative datasets and discuss the broader spectrum including synthetic populations, expert systems, survey data replacement, and personabots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15260v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richard Timpone, Yongwei Yang</dc:creator>
    </item>
    <item>
      <title>Semiparametric Modelling of Cancer Mortality Trends in Colombia</title>
      <link>https://arxiv.org/abs/2408.15387</link>
      <description>arXiv:2408.15387v1 Announce Type: cross 
Abstract: In this paper, we compare semiparametric and parametric model adjustments for cancer mortality in breast and cervical cancer and prostate and lung cancer in men, according to age and period of death. Semiparametric models were adjusted for the number of deaths from the two localizations of greatest mortality by sex: breast and cervix in women; prostate and lungs in men. Adjustments in different semiparametric models were compared; which included making adjustments with different distributions and variable combinations in the parametric and non-parametric part, for localization as well as for scale. Finally, the semiparametric model with best adjustment was selected and compared to traditional model; that is, to the generalized lineal model with Poisson response and logarithmic link. Best results for the four kinds of cancer were obtained for the selected semiparametric model by comparing it to the traditional Poisson model based upon AIC, envelope correlation between estimated logarithm rate and real rate logarithm. In general, we observe that in estimation, rate increases with age; however, with respect to period, breast cancer and stomach cancer in men show a tendency to rise over time; on the other hand, for cervical cancer, it remains virtually constant, but for lung cancer in men, as of 2007, it tends to decrease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15387v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lina Buitrago, Juan Sosa, Cristian Gonz\'ales</dc:creator>
    </item>
    <item>
      <title>Certified Causal Defense with Generalizable Robustness</title>
      <link>https://arxiv.org/abs/2408.15451</link>
      <description>arXiv:2408.15451v1 Announce Type: cross 
Abstract: While machine learning models have proven effective across various scenarios, it is widely acknowledged that many models are vulnerable to adversarial attacks. Recently, there have emerged numerous efforts in adversarial defense. Among them, certified defense is well known for its theoretical guarantees against arbitrary adversarial perturbations on input within a certain range (e.g., $l_2$ ball). However, most existing works in this line struggle to generalize their certified robustness in other data domains with distribution shifts. This issue is rooted in the difficulty of eliminating the negative impact of spurious correlations on robustness in different domains. To address this problem, in this work, we propose a novel certified defense framework GLEAN, which incorporates a causal perspective into the generalization problem in certified defense. More specifically, our framework integrates a certifiable causal factor learning component to disentangle the causal relations and spurious correlations between input and label, and thereby exclude the negative effect of spurious correlations on defense. On top of that, we design a causally certified defense strategy to handle adversarial attacks on latent causal factors. In this way, our framework is not only robust against malicious noises on data in the training distribution but also can generalize its robustness across domains with distribution shifts. Extensive experiments on benchmark datasets validate the superiority of our framework in certified robustness generalization in different data domains. Code is available in the supplementary materials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15451v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Qiao, Yu Yin, Chen Chen, Jing Ma</dc:creator>
    </item>
    <item>
      <title>Cellwise robust and sparse principal component analysis</title>
      <link>https://arxiv.org/abs/2408.15612</link>
      <description>arXiv:2408.15612v1 Announce Type: cross 
Abstract: A first proposal of a sparse and cellwise robust PCA method is presented. Robustness to single outlying cells in the data matrix is achieved by substituting the squared loss function for the approximation error by a robust version. The integration of a sparsity-inducing $L_1$ or elastic net penalty offers additional modeling flexibility. For the resulting challenging optimization problem, an algorithm based on Riemannian stochastic gradient descent is developed, with the advantage of being scalable to high-dimensional data, both in terms of many variables as well as observations. The resulting method is called SCRAMBLE (Sparse Cellwise Robust Algorithm for Manifold-based Learning and Estimation). Simulations reveal the superiority of this approach in comparison to established methods, both in the casewise and cellwise robustness paradigms. Two applications from the field of tribology underline the advantages of a cellwise robust and sparse PCA method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15612v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pia Pfeiffer, Laura Vana-G\"ur, Peter Filzmoser</dc:creator>
    </item>
    <item>
      <title>Visibility graph-based covariance functions for scalable spatial analysis in non-convex domains</title>
      <link>https://arxiv.org/abs/2307.11941</link>
      <description>arXiv:2307.11941v5 Announce Type: replace 
Abstract: We present a new method for constructing valid covariance functions of Gaussian processes for spatial analysis in irregular, non-convex domains such as bodies of water. Standard covariance functions based on geodesic distances are not guaranteed to be positive definite on such domains, while existing non-Euclidean approaches fail to respect the partially Euclidean nature of these domains where the geodesic distance agrees with the Euclidean distances for some pairs of points. Using a visibility graph on the domain, we propose a class of covariance functions that preserve Euclidean-based covariances between points that are connected in the domain while incorporating the non-convex geometry of the domain via conditional independence relationships. We show that the proposed method preserves the partially Euclidean nature of the intrinsic geometry on the domain while maintaining validity (positive definiteness) and marginal stationarity of the covariance function over the entire parameter space, properties which are not always fulfilled by existing approaches to construct covariance functions on non-convex domains. We provide useful approximations to improve computational efficiency, resulting in a scalable algorithm. We compare the performance of our method with those of competing state-of-the-art methods using simulation studies on synthetic non-convex domains. The method is applied to data regarding acidity levels in the Chesapeake Bay, showing its potential for ecological monitoring in real-world spatial applications on irregular domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11941v5</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian Gilbert, Abhirup Datta</dc:creator>
    </item>
    <item>
      <title>Robust estimation for number of factors in high dimensional factor modeling via Spearman correlation matrix</title>
      <link>https://arxiv.org/abs/2309.00870</link>
      <description>arXiv:2309.00870v2 Announce Type: replace 
Abstract: Determining the number of factors in high-dimensional factor modeling is essential but challenging, especially when the data are heavy-tailed. In this paper, we introduce a new estimator based on the spectral properties of Spearman sample correlation matrix under the high-dimensional setting, where both dimension and sample size tend to infinity proportionally. Our estimator is robust against heavy tails in either the common factors or idiosyncratic errors. The consistency of our estimator is established under mild conditions. Numerical experiments demonstrate the superiority of our estimator compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.00870v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaxin Qiu, Zeng Li, Jianfeng Yao</dc:creator>
    </item>
    <item>
      <title>Intervention effects based on potential benefit</title>
      <link>https://arxiv.org/abs/2405.08727</link>
      <description>arXiv:2405.08727v2 Announce Type: replace 
Abstract: Optimal treatment rules are mappings from individual patient characteristics to tailored treatment assignments that maximize mean outcomes. In this work, we introduce a conditional potential benefit (CPB) metric that measures the expected improvement under an optimally chosen treatment compared to the status quo, within covariate strata. The potential benefit combines (i) the magnitude of the treatment effect, and (ii) the propensity for subjects to naturally select a suboptimal treatment. As a consequence, heterogeneity in the CPB can provide key insights into the mechanism by which a treatment acts and/or highlight potential barriers to treatment access or adverse effects. Moreover, we demonstrate that CPB is the natural prioritization score for individualized treatment policies when intervention capacity is constrained. That is, in the resource-limited setting where treatment options are freely accessible, but the ability to intervene on a portion of the target population is constrained (e.g., if the population is large, and follow-up and encouragement of treatment uptake is labor-intensive), targeting subjects with highest CPB maximizes the mean outcome. Focusing on this resource-limited setting, we derive formulas for optimal constrained treatment rules, and for any given budget, quantify the loss compared to the optimal unconstrained rule. We describe sufficient identification assumptions, and propose nonparametric, robust, and efficient estimators of the proposed quantities emerging from our framework. Finally, we illustrate our methodology using data from a prospective cohort study in which we assess the impact of intensive care unit transfer on mortality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08727v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander W. Levis, Eli Ben-Michael, Edward H. Kennedy</dc:creator>
    </item>
    <item>
      <title>Heterogeneous Clinical Trial Outcomes via Multi-Output Gaussian Processes</title>
      <link>https://arxiv.org/abs/2407.13283</link>
      <description>arXiv:2407.13283v2 Announce Type: replace 
Abstract: We make use of Kronecker structure for scaling Gaussian Process models to large-scale, heterogeneous, clinical data sets. Repeated measures, commonly performed in clinical research, facilitate computational acceleration for nonlinear Bayesian nonparametric models and enable exact sampling for non-conjugate inference, when combinations of continuous and discrete endpoints are observed. Model inference is performed in Stan, and comparisons are made with brms on simulated data and two real clinical data sets, following a radiological image quality theme. Scalable Gaussian Process models compare favourably with parametric models on real data sets with 17,460 observations. Different GP model specifications are explored, with components analogous to random effects, and their theoretical properties are described.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13283v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Owen Thomas, Leiv R{\o}nneberg</dc:creator>
    </item>
    <item>
      <title>Misspecification-robust likelihood-free inference in high dimensions</title>
      <link>https://arxiv.org/abs/2002.09377</link>
      <description>arXiv:2002.09377v4 Announce Type: replace-cross 
Abstract: Likelihood-free inference for simulator-based statistical models has developed rapidly from its infancy to a useful tool for practitioners. However, models with more than a handful of parameters still generally remain a challenge for the Approximate Bayesian Computation (ABC) based inference. To advance the possibilities for performing likelihood-free inference in higher dimensional parameter spaces, we introduce an extension of the popular Bayesian optimisation based approach to approximate discrepancy functions in a probabilistic manner which lends itself to an efficient exploration of the parameter space. Our approach achieves computational scalability for higher dimensional parameter spaces by using separate acquisition functions and discrepancies for each parameter. The efficient additive acquisition structure is combined with exponentiated loss -likelihood to provide a misspecification-robust characterisation of the marginal posterior distribution for all model parameters. The method successfully performs computationally efficient inference in a 100-dimensional space on canonical examples and compares favourably to existing modularised ABC methods. We further illustrate the potential of this approach by fitting a bacterial transmission dynamics model to a real data set, which provides biologically coherent results on strain competition in a 30-dimensional parameter space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2002.09377v4</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Owen Thomas, Raquel S\'a-Le\~ao, Herm\'inia de Lencastre, Samuel Kaski, Jukka Corander, Henri Pesonen</dc:creator>
    </item>
    <item>
      <title>Adapting to Misspecification</title>
      <link>https://arxiv.org/abs/2305.14265</link>
      <description>arXiv:2305.14265v4 Announce Type: replace-cross 
Abstract: Empirical research typically involves a robustness-efficiency tradeoff. A researcher seeking to estimate a scalar parameter can invoke strong assumptions to motivate a restricted estimator that is precise but may be heavily biased, or they can relax some of these assumptions to motivate a more robust, but variable, unrestricted estimator. When a bound on the bias of the restricted estimator is available, it is optimal to shrink the unrestricted estimator towards the restricted estimator. For settings where a bound on the bias of the restricted estimator is unknown, we propose adaptive estimators that minimize the percentage increase in worst case risk relative to an oracle that knows the bound. We show that adaptive estimators solve a weighted convex minimax problem and provide lookup tables facilitating their rapid computation. Revisiting some well known empirical studies where questions of model specification arise, we examine the advantages of adapting to -- rather than testing for -- misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.14265v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timothy B. Armstrong, Patrick Kline, Liyang Sun</dc:creator>
    </item>
    <item>
      <title>Uniform error bound for PCA matrix denoising</title>
      <link>https://arxiv.org/abs/2306.12690</link>
      <description>arXiv:2306.12690v3 Announce Type: replace-cross 
Abstract: Principal component analysis (PCA) is a simple and popular tool for processing high-dimensional data. We investigate its effectiveness for matrix denoising.
  We consider the clean data are generated from a low-dimensional subspace, but masked by independent high-dimensional sub-Gaussian noises with standard deviation $\sigma$. Under the low-rank assumption on the clean data with a mild spectral gap assumption, we prove that the distance between each pair of PCA-denoised data point and the clean data point is uniformly bounded by $O(\sigma \log n)$. To illustrate the spectral gap assumption, we show it can be satisfied when the clean data are independently generated with a non-degenerate covariance matrix. We then provide a general lower bound for the error of the denoised data matrix, which indicates PCA denoising gives a uniform error bound that is rate-optimal. Furthermore, we examine how the error bound impacts downstream applications such as clustering and manifold learning. Numerical results validate our theoretical findings and reveal the importance of the uniform error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.12690v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin T. Tong, Wanjie Wang, Yuguan Wang</dc:creator>
    </item>
  </channel>
</rss>
