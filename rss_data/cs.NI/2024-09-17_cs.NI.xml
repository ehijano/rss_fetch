<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Sep 2024 04:01:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Throughput-Optimal Scheduling via Rate Learning</title>
      <link>https://arxiv.org/abs/2409.09198</link>
      <description>arXiv:2409.09198v1 Announce Type: new 
Abstract: We study the problem of designing scheduling policies for communication networks. This problem is often addressed with max-weight-type approaches since they are throughput-optimal. However, max-weight policies make scheduling decisions based on the network congestion, which can be sometimes unnecessarily restrictive. In this paper, we present a ``schedule as you learn'' (SYL) approach, where we learn an average rate, and then select schedules that generate such a rate in expectation. This approach is interesting because scheduling decisions do not depend on the size of the queue backlogs, and so it provides increased flexibility to select schedules based on other criteria or rules, such as serving high-priority queues. We illustrate the results with numerical experiments for a cross-bar switch and show that, compared to max-weight, SYL can achieve lower latency to certain flows without compromising throughput optimality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09198v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Panagiotis Promponas, V\'ictor Valls, Konstantinos Nikolakakis, Dionysis Kalogerias, Leandros Tassiulas</dc:creator>
    </item>
    <item>
      <title>Evaluating the Impact of Inter-cluster Communications in Edge Computing</title>
      <link>https://arxiv.org/abs/2409.09278</link>
      <description>arXiv:2409.09278v1 Announce Type: new 
Abstract: Distributed applications based on micro-services in edge computing are becoming increasingly popular due to the rapid evolution of mobile networks. While Kubernetes is the default framework when it comes to orchestrating and managing micro-service-based applications in mobile networks, the requirement to run applications between multiple sites at cloud and edge poses new challenges. Since Kubernetes does not natively provide tools to abstract inter-cluster communications at the application level, inter-cluster communication in edge computing is becoming increasingly critical to the application performance. In this paper, we evaluate for the first time the impact of inter-cluster communication on edge computing performance by using three prominent, open source inter-cluster communication projects and tools, i.e., Submariner, ClusterLink and Skupper. We develop a fully open-source testbed that integrates these tools in a modular fashion, and experimentally benchmark sample applications, including the ML class of applications, on their performance running in the multi-cluster edge computing system under varying networking conditions. We experimentally analyze two classes of envisioned mobile applications, i.e., a) industrial automation, b) vehicle decision drive assist. Our results show that Submariner performs best out of the three tools in scenarios with small payloads, regardless of the underlying networking conditions or transmission direction between clusters. When sending larger data to a service, ClusterLink outperforms Submariner once the inter-node networking conditions deteriorate, which may be the case in highly mobile scenarios in edge computing. Finally, Skupper significantly outperforms others in a variety of scenarios with larger payloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09278v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Michalke, Iulisloi Zacarias, Admela Jukan</dc:creator>
    </item>
    <item>
      <title>Reputation-Driven Peer-to-Peer Live Streaming Architecture for Preventing Free-Riding</title>
      <link>https://arxiv.org/abs/2409.09329</link>
      <description>arXiv:2409.09329v1 Announce Type: new 
Abstract: We present a peer-to-peer (P2P) live-streaming architecture designed to address challenges such as free-riding, malicious peers, churn, and network instability through the integration of a reputation system. The proposed algorithm incentivizes active peer participation while discouraging opportunistic behaviors, with a reputation mechanism that rewards altruistic peers and penalizes free riders and malicious actors. To manage peer dynamics, the algorithm continuously updates the strategies and adjusts to changing neighbors. It also implements a request-to-join mechanism for flash crowd scenarios, allowing the source node to delegate requests to child nodes, forming an interconnected tree structure that efficiently handles high demand and maintains system stability. The decentralized reputation mechanism promotes long-term sustainability in the P2P live streaming system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09329v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rashmi Kushwaha, Rahul Bhattacharyya, Yatindra Nath Singh</dc:creator>
    </item>
    <item>
      <title>VOMTC: Vision Objects for Millimeter and Terahertz Communications</title>
      <link>https://arxiv.org/abs/2409.09330</link>
      <description>arXiv:2409.09330v1 Announce Type: new 
Abstract: Recent advances in sensing and computer vision (CV) technologies have opened the door for the application of deep learning (DL)-based CV technologies in the realm of 6G wireless communications. For the successful application of this emerging technology, it is crucial to have a qualified vision dataset tailored for wireless applications (e.g., RGB images containing wireless devices such as laptops and cell phones). An aim of this paper is to propose a large-scale vision dataset referred to as Vision Objects for Millimeter and Terahertz Communications (VOMTC). The VOMTC dataset consists of 20,232 pairs of RGB and depth images obtained from a camera attached to the base station (BS), with each pair labeled with three representative object categories (person, cell phone, and laptop) and bounding boxes of the objects. Through experimental studies of the VOMTC datasets, we show that the beamforming technique exploiting the VOMTC-trained object detector outperforms conventional beamforming techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09330v1</guid>
      <category>cs.NI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TCCN.2024.3435909</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Cognitive Communications and Networking, 2024</arxiv:journal_reference>
      <dc:creator>Sunwoo Kim, Yongjun Ahn, Daeyoung Park, Byonghyo Shim</dc:creator>
    </item>
    <item>
      <title>Generative AI in Data Center Networking: Fundamentals, Perspectives, and Case Study</title>
      <link>https://arxiv.org/abs/2409.09343</link>
      <description>arXiv:2409.09343v1 Announce Type: new 
Abstract: Generative AI (GenAI), exemplified by Large Language Models (LLMs) such as OpenAI's ChatGPT, is revolutionizing various fields. Central to this transformation is Data Center Networking (DCN), which not only provides the computational power necessary for GenAI training and inference but also delivers GenAI-driven services to users. This article examines an interplay between GenAI and DCNs, highlighting their symbiotic relationship and mutual advancements. We begin by reviewing current challenges within DCNs and discuss how GenAI contributes to enhancing DCN capabilities through innovations, such as data augmentation, process automation, and domain transfer. We then focus on analyzing the distinctive characteristics of GenAI workloads on DCNs, gaining insights that catalyze the evolution of DCNs to more effectively support GenAI and LLMs. Moreover, to illustrate the seamless integration of GenAI with DCNs, we present a case study on full-lifecycle DCN digital twins. In this study, we employ LLMs equipped with Retrieval Augmented Generation (RAG) to formulate optimization problems for DCNs and adopt Diffusion-Deep Reinforcement Learning (DRL) for optimizing the RAG knowledge placement strategy. This approach not only demonstrates the application of advanced GenAI methods within DCNs but also positions the digital twin as a pivotal GenAI service operating on DCNs. We anticipate that this article can promote further research into enhancing the virtuous interaction between GenAI and DCNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09343v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yinqiu Liu, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Yonggang Wen, Dong In Kim</dc:creator>
    </item>
    <item>
      <title>Resources on the Move for Smart City: A Disruptive Perspective on the Grand Convergence of Sensing, Communications, Computing, Storage, and Intelligence</title>
      <link>https://arxiv.org/abs/2409.09417</link>
      <description>arXiv:2409.09417v1 Announce Type: new 
Abstract: The most commonly seen things on streets in any city are vehicles. However, most of them are used to transport people or goods. What if they also carry resources and capabilities for sensing, communications, computing, storage, and intelligence (SCCSI)? We will have a web of sensors to monitor the city, a network of powerful communicators to transport data around, a grid of computing power to conduct data analytics and machine learning (ML), a network of distributed storage to buffer/cache data/job for optimization, and a set of movable AI/ML toolboxes made available for specialized smart applications. This perspective article presents how to leverage SCCSI-empowered vehicles to design such a service network, simply called SCCSI network, to help build a smart city with a cost-effective and sustainable solution. It showcases how multi-dimensional technologies, namely, sensing, communications, computing, storage, and intelligence, converge to a unifying technology to solve grand challenges for resource demands from emerging large-scale applications. Thus, with SCCSI-empowered vehicles on the ground, over the air, and on the sea, SCCSI network can make resources and capabilities on the move, practically pushing SCCSI services to the edge! We hope this article serves as a spark to stimulate more disruptive thinking to address grand challenges of paramount importance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09417v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuguang Fang, Yiqin Deng, Xianhao Chen</dc:creator>
    </item>
    <item>
      <title>CSQF-based Time-Sensitive Flow Scheduling in Long-distance Industrial IoT Networks</title>
      <link>https://arxiv.org/abs/2409.09585</link>
      <description>arXiv:2409.09585v1 Announce Type: new 
Abstract: Booming time-critical services, such as automated manufacturing and remote operations, stipulate increasing demands for facilitating large-scale Industrial Internet of Things (IoT). Recently, a cycle specified queuing and forwarding (CSQF) scheme has been advocated to enhance the Ethernet. However, CSQF only outlines a foundational equipment-level primitive, while how to attain network-wide flow scheduling is not yet determined. Prior endeavors primarily focus on the range of a local area, rendering them unsuitable for long-distance factory interconnection. This paper devises the cycle tags planning (CTP) mechanism, the first integer programming model for the CSQF, which makes the CSQF practical for efficient global flow scheduling. In the CTP model, the per-hop cycle alignment problem is solved by decoupling the long-distance link delay from cyclic queuing time. To avoid queue overflows, we discretize the underlying network resources into cycle-related queue resource blocks and detail the core constraints within multiple periods. Then, two heuristic algorithms named flow offset and cycle shift (FO-CS) and Tabu FO-CS are designed to calculate the flows' cycle tags and maximize the number of schedulable flows, respectively. Evaluation results show that FO-CS increases the number of scheduled flows by 31.2%. The Tabu FO-CS algorithm can schedule 94.45% of flows at the level of 2000 flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09585v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yudong Huang, Tao Huang, Xinyuan Zhang, Shuo Wang, Hongyang Du, Dusit Niyato, Fei Richard Yu</dc:creator>
    </item>
    <item>
      <title>Programmable Cycle-Specified Queue for Long-Distance Industrial Deterministic Packet Scheduling</title>
      <link>https://arxiv.org/abs/2409.09592</link>
      <description>arXiv:2409.09592v1 Announce Type: new 
Abstract: The time-critical industrial applications pose intense demands for enabling long-distance deterministic networks. However, previous priority-based and weight-based scheduling methods focus on probabilistically reducing average delay, which ignores strictly guaranteeing task-oriented on-time packet delivery with bounded worst-case delay and jitter.
  This paper proposes a new Programmable Cycle-Specified Queue (PCSQ) for long-distance industrial deterministic packet scheduling. By implementing the first high-precision rotation dequeuing, PCSQ enables microsecond-level time slot resource reservation (noted as T) and especially jitter control of up to 2T. Then, we propose the cycle tags computation to approximate cyclic scheduling algorithms, which allows packets to actively pick and lock their favorite queue in a sequence of nodes. Accordingly, PCSQ can precisely defer packets to any desired time. Further, the queue coordination and cycle mapping mechanisms are delicately designed to solve the cycle-queue mismatch problem. Evaluation results show that PCSQ can schedule tens of thousands of time-sensitive flows and strictly guarantee $ms$-level delay and us-level jitter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09592v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yudong Huang, Shuo Wang, Shiyin Zhu, Guoyu Peng, Xinyuan Zhang, Tao Huang, Xinmin Liu</dc:creator>
    </item>
    <item>
      <title>Active RIS-Aided Terahertz Communications with Phase Error and Beam Misalignment</title>
      <link>https://arxiv.org/abs/2409.09713</link>
      <description>arXiv:2409.09713v1 Announce Type: new 
Abstract: Terahertz (THz) communications will be pivotal in sixth-generation (6G) wireless networks, offering significantly wider bandwidths and higher data rates. However, the unique propagation characteristics of the THz frequency band, such as high path loss and sensitivity to blockages, pose substantial challenges. Reconfigurable intelligent surfaces (RISs) present a promising solution for enhancing THz communications by dynamically shaping the propagation environment to address these issues. Active RISs, in particular, can amplify reflected signals, effectively mitigating the multiplicative fading effects in RIS-aided links. Given the highly directional nature of THz signals, beam misalignment is a significant concern, while discrete phase shifting is more practical for real-world RIS deployment compared to continuous adjustments. This paper investigates the performance of active-RIS-aided THz communication systems, focusing on discrete phase shifts and beam misalignment. An expression for the ergodic capacity is derived, incorporating critical system parameters to assess performance. Numerical results offer insights into optimizing active-RIS-aided THz communication systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09713v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Waqas Khalid, Heejung Yu, Farman Ali, Huiping Huang</dc:creator>
    </item>
    <item>
      <title>Optimal Operation of Active RIS-Aided Wireless Powered Communications in IoT Networks</title>
      <link>https://arxiv.org/abs/2409.09719</link>
      <description>arXiv:2409.09719v1 Announce Type: new 
Abstract: Wireless-powered communications (WPCs) are increasingly crucial for extending the lifespan of low-power Internet of Things (IoT) devices. Furthermore, reconfigurable intelligent surfaces (RISs) can create favorable electromagnetic environments by providing alternative signal paths to counteract blockages. The strategic integration of WPC and RIS technologies can significantly enhance energy transfer and data transmission efficiency. However, passive RISs suffer from double-fading attenuation over RIS-aided cascaded links. In this article, we propose the application of an active RIS within WPC-enabled IoT networks. The enhanced flexibility of the active RIS in terms of energy transfer and information transmission is investigated using adjustable parameters. We derive novel closed-form expressions for the ergodic rate and outage probability by incorporating key parameters, including signal amplification, active noise, power consumption, and phase quantization errors. Additionally, we explore the optimization of WPC scenarios, focusing on the time-switching factor and power consumption of the active RIS. The results validate our analysis, demonstrating that an active RIS significantly enhances WPC performance compared to a passive RIS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09719v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Waqas Khalid, A. -A. A. Boulogeorgos, Trinh Van Chien, Junse Lee, Howon Lee, Heejung Yu</dc:creator>
    </item>
    <item>
      <title>A Global Perspective on the Past, Present, and Future of Video Streaming over Starlink</title>
      <link>https://arxiv.org/abs/2409.09846</link>
      <description>arXiv:2409.09846v1 Announce Type: new 
Abstract: This study presents the first global analysis of on-demand video streaming over Low Earth Orbit (LEO) satellite networks, using data from over one million households across 85 countries. We highlight Starlink's role as a major LEO provider, enhancing connectivity in underserved regions. Our findings reveal that while overall video quality on Starlink matches that of traditional networks, the inherent variability in LEO conditions -- such as throughput fluctuations and packet loss -- leads to an increase in bitrate switches and rebuffers. To further improve the quality of experience for the LEO community, we manipulate existing congestion control and adaptive bitrate streaming algorithms using simulation and real A/B tests deployed on over one million households. Our results underscore the need for video streaming and congestion control algorithms to adapt to rapidly evolving network landscapes, ensuring high-quality service across diverse and dynamic network types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09846v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <category>cs.MM</category>
      <category>cs.PF</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liz Izhikevich, Reese Enghardt, Te-Yuan Huang, Renata Teixeira</dc:creator>
    </item>
    <item>
      <title>Li-MSD: A lightweight mitigation solution for DAO insider attack in RPL-based IoT</title>
      <link>https://arxiv.org/abs/2409.10020</link>
      <description>arXiv:2409.10020v1 Announce Type: new 
Abstract: Many IoT applications run on a wireless infrastructure supported by resource-constrained nodes which is popularly known as Low-Power and Lossy Networks (LLNs). Currently, LLNs play a vital role in digital transformation of industries. The resource limitations of LLNs restrict the usage of traditional routing protocols and therefore require an energy-efficient routing solution. IETF's Routing Protocol for Low-power Lossy Networks (RPL, pronounced 'ripple') is one of the most popular energy-efficient protocols for LLNs, specified in RFC 6550. In RPL, Destination Advertisement Object (DAO) control message is transmitted by a child node to pass on its reachability information to its immediate parent or root node. An attacker may exploit the insecure DAO sending mechanism of RPL to perform 'DAO insider attack' by transmitting DAO multiple times. This paper shows that an aggressive DAO insider attacker can drastically degrade network performance. We propose a Lightweight Mitigation Solution for DAO insider attack, which is termed as 'Li-MSD'. Li-MSD uses a blacklisting strategy to mitigate the attack and restore RPL performance, significantly. By using simulations, it is shown that Li-MSD outperforms the existing solution in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10020v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.future.2024.05.032</arxiv:DOI>
      <arxiv:journal_reference>Future Generation Computer Systems, 159, 327-339 (2024)</arxiv:journal_reference>
      <dc:creator>Abhishek Verma, Sachin Kumar Verma, Avinash Chandra Pandey, Jyoti Grover, Girish Sharma</dc:creator>
    </item>
    <item>
      <title>Cross: A Delay Based Congestion Control Method for RTP Media</title>
      <link>https://arxiv.org/abs/2409.10042</link>
      <description>arXiv:2409.10042v1 Announce Type: new 
Abstract: After more than a decade of development, real time communication (RTC) for video telephony has made significantly progress. However, emerging high-quality RTC applications with high definition and high frame rate requires sufficient bandwidth. The default congestion control mechanism specifically tuned for video telephony leaves plenty of room for optimization under high-rate scenarios. It is necessary to develop new rate control solutions to utilize bandwidth efficiently and to provide better experience for such services. A delay-based congestion control method called Cross is proposed, which regulates rate based on queue load with a multiplicative increase and multiplicative decrease fashion. A simulation module is developed to validate the effectiveness of these congestion control algorithms for RTC services. The module is released with the hope to provide convenience for RTC research community. Simulation results demonstrate that Cross can achieve low queuing delay and maintain high channel utilization under random loss environments. Online deployment shows that Cross can reduce the video freezing ratio by up to 58.45\% on average when compared with a benchmark algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10042v1</guid>
      <category>cs.NI</category>
      <category>cs.MM</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Songyang Zhang, Changpeng Yang</dc:creator>
    </item>
    <item>
      <title>Decoupling DNS Update Timing from TTL Values</title>
      <link>https://arxiv.org/abs/2409.10207</link>
      <description>arXiv:2409.10207v1 Announce Type: new 
Abstract: A relatively simple safety-belt mechanism for improving DNS system availability and efficiency is proposed here. While it may seem ambitious, a careful examination shows it is both feasible and beneficial for the DNS system. The mechanism called "DNS Real-time Update" (DNSRU), a service that facilitates real-time and secure updates of cached domain records in DNS resolvers worldwide, even before the expiration of the corresponding Time To Live (TTL) values. This service allows Internet domain owners to quickly rectify any erroneous global IP address distribution, even if a long TTL value is associated with it. By addressing this critical DNS high availability issue, DNSRU eliminates the need for short TTL values and their associated drawbacks. Therefore, DNSRU DNSRU reduces the traffic load on authoritative servers while enhancing the system's fault tolerance. In this paper we show that our DNSRU design is backward compatible, supports gradual deployment, secure, efficient, and feasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10207v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yehuda Afek, Ariel Litmanovich</dc:creator>
    </item>
    <item>
      <title>Enhancing Video Transmission with Machine Learning based Routing in Software-Defined Networks</title>
      <link>https://arxiv.org/abs/2409.10512</link>
      <description>arXiv:2409.10512v1 Announce Type: new 
Abstract: Our study uses the centralized, flexible, dynamic, and programmable structure of Software-Defined networks (SDN) to overcome the problems. Although SDN effectively addresses the challenges present in traditional networks, it still requires further enhancements to achieve a more optimized network architecture. The Floodlight controller utilized in this study employs metrics such as hop count, which provides limited information for routing. In scenarios such as video transmission, this situation is insufficient and the need for optimization arises. For this purpose, an artificial intelligence (AI) based routing algorithm is proposed between the server and the client in the scenario based on NSFNET topology. The topology designed with the Floodlight controller in the Mininet simulation environment includes a client, a server, and 14 switches. A realistic network environment is provided by adding different receivers and creating TCP traffic between these receivers using the iperf3 tool. In three scenarios, video streaming is performed using the FFmpeg tool, and 49 path metrics such as RTT, throughput, and loss are recorded. In these scenarios, PSNR and SSIM calculations are made to observe the differences between the transmitted and the original video in congested and uncongested environments. Due to the lack of a dataset suitable for the proposed network environment in the literature, a new dataset consisting of 876 records is created using continuously transmitted video traffic. Low and high traffic levels are created within the dataset, and different machine learning techniques such as KNN, Random Forest, SVM, AdaBoost, Logistic Regression and XGBoost are applied using the features that affect the traffic levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10512v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>An{\i}l Dursun \.Ipek, Murtaza Cicio\u{g}lu, Ali \c{C}alhan</dc:creator>
    </item>
    <item>
      <title>Security Testbed for Preempting Attacks against Supercomputing Infrastructure</title>
      <link>https://arxiv.org/abs/2409.09602</link>
      <description>arXiv:2409.09602v1 Announce Type: cross 
Abstract: Preempting attacks targeting supercomputing systems before damage remains the top security priority. The main challenge is that noisy attack attempts and unreliable alerts often mask real attacks, causing permanent damages such as system integrity violations and data breaches. This paper describes a security testbed embedded in live traffic of a supercomputer at the National Center for Supercomputing Applications (NCSA). The objective is to demonstrate attack preemption, i.e., stopping system compromise and data breaches at petascale supercomputers. Deployment of our testbed at NCSA enables the following key contributions:
  1) Insights from characterizing unique attack patterns found in real security logs of over 200 security incidents curated in the past two decades at NCSA.
  2) Deployment of an attack visualization tool to illustrate the challenges of identifying real attacks in HPC environments and to support security operators in interactive attack analyses.
  3) Demonstrate the testbed's utility by running novel models, such as Factor Graph-Based models, to preempt a real-world ransomware family.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09602v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Phuong Cao, Zbigniew Kalbarczyk, Ravishankar Iyer</dc:creator>
    </item>
    <item>
      <title>Context-Conditioned Spatio-Temporal Predictive Learning for Reliable V2V Channel Prediction</title>
      <link>https://arxiv.org/abs/2409.09978</link>
      <description>arXiv:2409.09978v1 Announce Type: cross 
Abstract: Achieving reliable multidimensional Vehicle-to-Vehicle (V2V) channel state information (CSI) prediction is both challenging and crucial for optimizing downstream tasks that depend on instantaneous CSI. This work extends traditional prediction approaches by focusing on four-dimensional (4D) CSI, which includes predictions over time, bandwidth, and antenna (TX and RX) space. Such a comprehensive framework is essential for addressing the dynamic nature of mobility environments within intelligent transportation systems, necessitating the capture of both temporal and spatial dependencies across diverse domains. To address this complexity, we propose a novel context-conditioned spatiotemporal predictive learning method. This method leverages causal convolutional long short-term memory (CA-ConvLSTM) to effectively capture dependencies within 4D CSI data, and incorporates context-conditioned attention mechanisms to enhance the efficiency of spatiotemporal memory updates. Additionally, we introduce an adaptive meta-learning scheme tailored for recurrent networks to mitigate the issue of accumulative prediction errors. We validate the proposed method through empirical studies conducted across three different geometric configurations and mobility scenarios. Our results demonstrate that the proposed approach outperforms existing state-of-the-art predictive models, achieving superior performance across various geometries. Moreover, we show that the meta-learning framework significantly enhances the performance of recurrent-based predictive models in highly challenging cross-geometry settings, thus highlighting its robustness and adaptability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09978v1</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lei Chu, Daoud Burghal, Michael Neuman, Andreas F. Molisch</dc:creator>
    </item>
    <item>
      <title>Security, Trust and Privacy challenges in AI-driven 6G Networks</title>
      <link>https://arxiv.org/abs/2409.10337</link>
      <description>arXiv:2409.10337v1 Announce Type: cross 
Abstract: The advent of 6G networks promises unprecedented advancements in wireless communication, offering wider bandwidth and lower latency compared to its predecessors. This article explores the evolving infrastructure of 6G networks, emphasizing the transition towards a more disaggregated structure and the integration of artificial intelligence (AI) technologies. Furthermore, it explores the security, trust and privacy challenges and attacks in 6G networks, particularly those related to the use of AI. It presents a classification of network attacks stemming from its AI-centric architecture and explores technologies designed to detect or mitigate these emerging threats. The paper concludes by examining the implications and risks linked to the utilization of AI in ensuring a robust network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10337v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.5121/csit.2024.141408</arxiv:DOI>
      <arxiv:journal_reference>CS &amp; IT Conference Proceedings (CSCP 2024) pp.95-113</arxiv:journal_reference>
      <dc:creator>Helena Rifa-Pous, Victor Garcia-Font, Carlos Nunez-Gomez, Julian Salas</dc:creator>
    </item>
    <item>
      <title>Dataset of Pathloss and ToA Radio Maps With Localization Application</title>
      <link>https://arxiv.org/abs/2212.11777</link>
      <description>arXiv:2212.11777v4 Announce Type: replace 
Abstract: In this article, we present a collection of radio map datasets in dense urban setting, which we generated and made publicly available. The datasets include simulated pathloss/received signal strength (RSS) and time of arrival (ToA) radio maps over a large collection of realistic dense urban setting in real city maps. The two main applications of the presented dataset are 1) learning methods that predict the pathloss from input city maps (namely, deep learning-based simulations), and, 2) wireless localization. The fact that the RSS and ToA maps are computed by the same simulations over the same city maps allows for a fair comparison of the RSS and ToA-based localization methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.11777v4</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\c{C}a\u{g}kan Yapar, Ron Levie, Gitta Kutyniok, Giuseppe Caire</dc:creator>
    </item>
    <item>
      <title>Rail-only: A Low-Cost High-Performance Network for Training LLMs with Trillion Parameters</title>
      <link>https://arxiv.org/abs/2307.12169</link>
      <description>arXiv:2307.12169v5 Announce Type: replace 
Abstract: This paper presents a low-cost network architecture for training large language models (LLMs) at hyperscale. We study the optimal parallelization strategy of LLMs and propose a novel datacenter network design tailored to LLM's unique communication pattern. We show that LLM training generates sparse communication patterns in the network and, therefore, does not require any-to-any full-bisection network to complete efficiently. As a result, our design eliminates the spine layer in traditional GPU clusters. We name this design a Rail-only network and demonstrate that it achieves the same training performance while reducing the network cost by 38% to 77% and network power consumption by 37% to 75% compared to a conventional GPU datacenter. Our architecture also supports Mixture-of-Expert (MoE) models with all-to-all communication through forwarding, with only 8.2% to 11.2% completion time overhead for all-to-all traffic. We study the failure robustness of Rail-only networks and provide insights into the performance impact of different network and training parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.12169v5</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weiyang Wang, Manya Ghobadi, Kayvon Shakeri, Ying Zhang, Naader Hasani</dc:creator>
    </item>
    <item>
      <title>Optimal Slicing and Scheduling with Service Guarantees in Multi-Hop Wireless Networks</title>
      <link>https://arxiv.org/abs/2404.08637</link>
      <description>arXiv:2404.08637v3 Announce Type: replace 
Abstract: We analyze the problem of scheduling in wireless networks to meet end-to-end service guarantees. Using network slicing to decouple the queueing dynamics between flows, we show that the network's ability to meet hard throughput and deadline requirements is largely influenced by the scheduling policy. We characterize the feasible throughput/deadline region for a flow under a fixed route and set of slices, and find throughput- and deadline-optimal policies for a solitary flow. We formulate the feasibility problem for multiple flows in a general topology, and show its equivalence to finding a bounded-cost cycle on an exponentially large graph, which is unsolvable in polynomial time by the best-known algorithm. Using a novel concept called delay deficit, we develop a sufficient condition for meeting deadlines as a function of inter-scheduling times, and show that regular schedules are optimal for satisfying this condition. Motivated by this, we design a polynomial-time algorithm that returns an (almost) regular schedule, optimized to meet service guarantees for all flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08637v3</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas Jones, Eytan Modiano</dc:creator>
    </item>
    <item>
      <title>Early Detection of Network Service Degradation: An Intra-Flow Approach</title>
      <link>https://arxiv.org/abs/2407.06637</link>
      <description>arXiv:2407.06637v2 Announce Type: replace 
Abstract: This research presents a novel method for predicting service degradation (SD) in computer networks by leveraging early flow features. Our approach focuses on the observable (O) segments of network flows, particularly analyzing Packet Inter-Arrival Time (PIAT) values and other derived metrics, to infer the behavior of non-observable (NO) segments. Through a comprehensive evaluation, we identify an optimal O/NO split threshold of 10 observed delay samples, balancing prediction accuracy and resource utilization. Evaluating models including Logistic Regression, XGBoost, and Multi-Layer Perceptron, we find XGBoost outperforms others, achieving an F1-score of 0.74, balanced accuracy of 0.84, and AUROC of 0.97. Our findings highlight the effectiveness of incorporating comprehensive early flow features and the potential of our method to offer a practical solution for monitoring network traffic in resource-constrained environments. This approach ensures enhanced user experience and network performance by preemptively addressing potential SD, providing the basis for a robust framework for maintaining high-quality network services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06637v2</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Balint Bicski, Adrian Pekar</dc:creator>
    </item>
    <item>
      <title>Towards Enabling 5G-NTN Satellite Communications for Manned and Unmanned Rotary Wing Aircraft</title>
      <link>https://arxiv.org/abs/2407.10177</link>
      <description>arXiv:2407.10177v3 Announce Type: replace 
Abstract: Satellite Communications (SatCom) are a backbone of worldwide development. In contrast with the past, when the GEO satellites were the only means for such connectivity, nowadays the multi-orbital connectivity is emerging, especially with the use of satellite constellations. Simultaneously, SatCom enabled the so-called In-Flight Connectivity, while with the advent of 5G-NTN, the development of this market is being accelerated. However, there are still various missing points before such a technology becomes mainstream, especially in the case of Rotary Wing Aircraft (RWA). Indeed, due to their particular characteristics, such as the low altitude flights and the blade interference, there are still open challenges. In this work, an End-to-End (E2E) analysis for the performance of SatCom under 5G-NTN for manned and unmanned RWA is performed. Various scenarios are examined, and related requirements are shown. The effects of blades and other characteristics of the RWA are established, and simulations for these cases are developed. Results along with related discussion are presented, while future directions for development are suggested. This work is part of the ESA ACROSS-AIR project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10177v3</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vasileios Leon, Ilias Christofilos, Athanasios Nesiadis, Iosif Paraskevas, Juan Perrela, Georgios Ioannopoulos, Alexandros Tasoulis-Nonikas, Mathieu Bernou, Jacques Reading</dc:creator>
    </item>
    <item>
      <title>Predictability of Performance in Communication Networks Under Markovian Dynamics</title>
      <link>https://arxiv.org/abs/2408.13196</link>
      <description>arXiv:2408.13196v3 Announce Type: replace 
Abstract: With the emergence of time-critical applications in modern communication networks, there is a growing demand for proactive network adaptation and quality of service (QoS) prediction. However, a fundamental question remains largely unexplored: how can we quantify and achieve more predictable communication systems in terms of performance? To address this gap, this paper introduces a theoretical framework for defining and analyzing predictability in communication systems, with a focus on the impact of observations for performance forecasting. We establish a mathematical definition of predictability based on the total variation distance between forecast and marginal performance distributions. A system is deemed unpredictable when the forecast distribution, providing the most comprehensive characterization of future states using all accessible information, is indistinguishable from the marginal distribution, which depicts the system's behavior without any observational input. This framework is applied to multi-hop systems under Markovian conditions, with a detailed analysis of Geo/Geo/1 queuing models in both single-hop and multi-hop scenarios. We derive exact and approximate expressions for predictability in these systems, as well as upper bounds based on spectral analysis of the underlying Markov chains. Our results have implications for the design of efficient monitoring and prediction mechanisms in future communication networks aiming to provide deterministic services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13196v3</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samie Mostafavi, Simon Egger, Gy\"orgy D\'an, James Gross</dc:creator>
    </item>
    <item>
      <title>A MAC Protocol with Time Reversal for Wireless Networks within Computing Packages</title>
      <link>https://arxiv.org/abs/2408.07421</link>
      <description>arXiv:2408.07421v2 Announce Type: replace-cross 
Abstract: Wireless Network-on-Chip (WNoC) is a promising concept which provides a solution to overcome the scalability issues in prevailing networks-in-package for many-core processors. However, the electromagnetic propagation inside the chip package leads to energy reverberation, resulting in Inter-Symbol Interference (ISI) with high delay spreads. Time Reversal (TR) is a technique that benefits the unique time-invariant channel with rich multipath effects to focus the energy to the desired transceiver. TR mitigates both ISI and co-channel interference, hence providing parallel communications in both space and time. Thus, TR is a versatile candidate to improve the aggregate bandwidth of wireless on-chip networks provided that a Medium Access Control (MAC) is used to efficiently share the wireless medium. In this paper, we explore a simple yet resilient TR-based MAC protocol (TR-MAC) design for WNoC. We propose to manage multiple parallel transmissions with simultaneous spatial channels in the same time slot with TR precoding and focused energy detection at the transceiver. Our results show that TR-MAC can be employed in massive computing architectures with improved latency and throughput while matching with the stringent requirements of the physical layer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07421v2</guid>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ama Bandara, Abhijit Das, F\'atima Rodr\'iguez-Gal\'an, Eduard Alarc\'on, Sergi Abadal</dc:creator>
    </item>
    <item>
      <title>LLM Honeypot: Leveraging Large Language Models as Advanced Interactive Honeypot Systems</title>
      <link>https://arxiv.org/abs/2409.08234</link>
      <description>arXiv:2409.08234v2 Announce Type: replace-cross 
Abstract: The rapid evolution of cyber threats necessitates innovative solutions for detecting and analyzing malicious activity. Honeypots, which are decoy systems designed to lure and interact with attackers, have emerged as a critical component in cybersecurity. In this paper, we present a novel approach to creating realistic and interactive honeypot systems using Large Language Models (LLMs). By fine-tuning a pre-trained open-source language model on a diverse dataset of attacker-generated commands and responses, we developed a honeypot capable of sophisticated engagement with attackers. Our methodology involved several key steps: data collection and processing, prompt engineering, model selection, and supervised fine-tuning to optimize the model's performance. Evaluation through similarity metrics and live deployment demonstrated that our approach effectively generates accurate and informative responses. The results highlight the potential of LLMs to revolutionize honeypot technology, providing cybersecurity professionals with a powerful tool to detect and analyze malicious activity, thereby enhancing overall security infrastructure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08234v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hakan T. Otal, M. Abdullah Canbaz</dc:creator>
    </item>
  </channel>
</rss>
