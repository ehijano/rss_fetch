<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Jul 2025 02:26:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Expansive Natural Neural Gradient Flows for Energy Minimization</title>
      <link>https://arxiv.org/abs/2507.13475</link>
      <description>arXiv:2507.13475v1 Announce Type: new 
Abstract: This paper develops expansive gradient dynamics in deep neural network-induced mapping spaces. Specifically, we generate tools and concepts for minimizing a class of energy functionals in an abstract Hilbert space setting covering a wide scope of applications such as PDEs-based inverse problems and supervised learning. The approach hinges on a Hilbert space metric in the full diffeomorphism mapping space, which could be viewed as a generalized Wasserstein-2 metric. We then study a projection gradient descent method within deep neural network parameterized sets. More importantly, we develop an adaptation and expanding strategy to step-by-step enlarge the deep neural network structures. In particular, the expansion mechanism aims to enhance the alignment of the neural manifold induced natural gradient direction as well as possible with the ideal Hilbert space gradient descent direction leveraging the fact that we can evaluate projections of the Hilbert space gradient. We demonstrate the efficacy of the proposed strategy for several simple model problems for energies arising in the context of supervised learning, model reduction, or inverse problems. In particular, we highlight the importance of assembling the neural flow matrix based on the inner product for the ambient Hilbert space. The actual algorithms are the simplest specifications of a broader spectrum based on a correspondingly wider discussion, postponing a detailed analysis to forthcoming work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13475v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wolfgang Dahmen, Wuchen Li, Yuankai Teng, Zhu Wang</dc:creator>
    </item>
    <item>
      <title>A Specialized Simplex Algorithm for Budget-Constrained Total Variation-Regularized Problems</title>
      <link>https://arxiv.org/abs/2507.13493</link>
      <description>arXiv:2507.13493v1 Announce Type: new 
Abstract: We consider a class of linear programs on graphs with total variation regularization and a budgetary constraint. For these programs, we give a characterization of basic solutions in terms of rooted spanning forests with orientation on the underlying graph. This leads to an interpretation of the simplex method in terms of simple graph operations on these underlying forests. We exploit this structure to produce an accelerated simplex method and empirically show that such improvements can lead to an order of magnitude improvement in time when compared to state-of-the-art solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13493v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dominic Yang</dc:creator>
    </item>
    <item>
      <title>Single spin exact gradients for the optimization of complex pulses and pulse sequences</title>
      <link>https://arxiv.org/abs/2507.13557</link>
      <description>arXiv:2507.13557v1 Announce Type: new 
Abstract: The efficient computer optimization of magnetic resonance pulses and pulse sequences involves the calculation of a problem-adapted cost function as well as its gradients with respect to all controls applied. The gradients generally can be calculated as a finite difference approximation, as a GRAPE approximation, or as an exact function, e.g. by the use of the augmented matrix exponentiation, where the exact gradient should lead to best optimization convergence. However, calculation of exact gradients is computationally expensive and analytical exact solutions to the problem would be highly desirable. As the majority of todays pulse optimizations involve a single spin 1/2, which can be represented by simple rotation matrices in the Bloch space or by their corresponding Cayley-Klein/quaternion parameters, the derivations of analytical exact gradient functions appear to be feasible. Taking two optimization types, the optimization of point-to-point pulses using 3D-rotations and the optimization of universal rotation pulses using quaternions, analytical solutions for gradients with respect to controls have been derived. Controls in this case can be conventional $x$ and $y$ pulses, but also $z$-controls, as well as gradients with respect to amplitude and phase of a pulse shape. In addition, analytical solutions with respect to pseudo controls, involving holonomic constraints to maximum rf-amplitudes, maximum rf-power, or maximum rf-energy, are introduced. Using the hyperbolic tangent function, maximum values are imposed in a fully continuous and differentiable way. The obtained analytical gradients allow the calculation two orders of magnitude faster than the augmented matrix exponential approach. The exact gradients for different controls are finally compared in a number of optimizations involving broadband pulses for $^{15}$N, $^{13}$C, and $^{19}$F applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13557v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>physics.chem-ph</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stella Slad, Burkhard Luy</dc:creator>
    </item>
    <item>
      <title>Dynamic Transmission Line Switching Amidst Wildfire-Prone Weather Under Decision-Dependent Uncertainty</title>
      <link>https://arxiv.org/abs/2507.13611</link>
      <description>arXiv:2507.13611v1 Announce Type: new 
Abstract: During dry and windy seasons, environmental conditions significantly increase the risk of wildfires, exposing power grids to disruptions caused by transmission line failures. Wildfire propagation exacerbates grid vulnerability, potentially leading to prolonged power outages. To address this challenge, we propose a multi-stage optimization model that dynamically adjusts transmission grid topology in response to wildfire propagation, aiming to develop an optimal response policy. By accounting for decision-dependent uncertainty, where line survival probabilities depend on usage, we employ distributionally robust optimization to model uncertainty in line survival distributions. We adapt the stochastic nested decomposition algorithm and derive a deterministic upper bound for its finite convergence. To enhance computational efficiency, we exploit the Lagrangian dual problem structure for a faster generation of Lagrangian cuts. Using realistic data from the California transmission grid, we demonstrate the superior performance of dynamic response policies against two-stage alternatives through a comprehensive case study. In addition, we construct easy-to-implement policies that significantly reduce computational burden while maintaining good performance in real-time deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13611v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan-Alberto Estrada-Garcia, Ruiwei Jiang, Alexandre Moreira</dc:creator>
    </item>
    <item>
      <title>Conformal Contraction for Robust Nonlinear Control with Distribution-Free Uncertainty Quantification</title>
      <link>https://arxiv.org/abs/2507.13613</link>
      <description>arXiv:2507.13613v1 Announce Type: new 
Abstract: We present a novel robust control framework for continuous-time, perturbed nonlinear dynamical systems with uncertainty that depends nonlinearly on both the state and control inputs. Unlike conventional approaches that impose structural assumptions on the uncertainty, our framework enhances contraction-based robust control with data-driven uncertainty prediction, remaining agnostic to the models of the uncertainty and predictor. We statistically quantify how reliably the contraction conditions are satisfied under dynamics with uncertainty via conformal prediction, thereby obtaining a distribution-free and finite-time probabilistic guarantee for exponential boundedness of the trajectory tracking error. We further propose the probabilistically robust control invariant (PRCI) tube for distributionally robust motion planning, within which the perturbed system trajectories are guaranteed to stay with a finite probability, without explicit knowledge of the uncertainty model. Numerical simulations validate the effectiveness of the proposed robust control framework and the performance of the PRCI tube.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13613v1</guid>
      <category>math.OC</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sihang Wei, Melkior Ornik, Hiroyasu Tsukamoto</dc:creator>
    </item>
    <item>
      <title>RiNNAL+: a Riemannian ALM Solver for SDP-RLT Relaxations of Mixed-Binary Quadratic Programs</title>
      <link>https://arxiv.org/abs/2507.13776</link>
      <description>arXiv:2507.13776v1 Announce Type: new 
Abstract: Doubly nonnegative (DNN) relaxation usually provides a tight lower bound for a mixed-binary quadratic program (MBQP). However, solving DNN problems is challenging because: (1) the problem size is $\Omega((n+l)^2)$ for an MBQP with $n$ variables and $l$ inequality constraints, and (2) the rank of optimal solutions cannot be estimated a priori due to the absence of theoretical bounds. In this work, we propose RiNNAL+, a Riemannian augmented Lagrangian method (ALM) for solving DNN problems. We prove that the DNN relaxation of an MBQP, with matrix dimension $(n+l+1)$, is equivalent to the SDP-RLT relaxation (based on the reformulation-linearization technique) with a smaller matrix dimension $(n+1)$. In addition, we develop a hybrid method that alternates between two phases to solve the ALM subproblems. In phase one, we apply low-rank matrix factorization and random perturbation to transform the feasible region into a lower-dimensional manifold so that we can use the Riemannian gradient descent method. In phase two, we apply a single projected gradient step to update the rank of the underlying variable and escape from spurious local minima arising in the first phase if necessary. To reduce the computation cost of the projected gradient step, we develop pre-processing and warm-start techniques for acceleration. Unlike traditional rank-adaptive methods that require extensive parameter tuning, our hybrid method requires minimal tuning. Extensive experiments confirm the efficiency and robustness of RiNNAL+ in solving various classes of large-scale DNN problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13776v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Di Hou, Tianyun Tang, Kim-Chuan Toh</dc:creator>
    </item>
    <item>
      <title>Gradient descent avoids strict saddles with a simple line-search method too</title>
      <link>https://arxiv.org/abs/2507.13804</link>
      <description>arXiv:2507.13804v1 Announce Type: new 
Abstract: It is known that gradient descent (GD) on a $C^2$ cost function generically avoids strict saddle points when using a small, constant step size. However, no such guarantee existed for GD with a line-search method. We provide one for a modified version of the standard Armijo backtracking method with generic, arbitrarily large initial step size. In contrast to previous works, our analysis does not require a globally Lipschitz gradient.
  We extend this to the Riemannian setting (RGD), assuming the retraction is real analytic (though the cost function still only needs to be $C^2$). In closing, we also improve guarantees for RGD with a constant step size in some scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13804v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreea-Alexandra Mu\c{s}at, Nicolas Boumal</dc:creator>
    </item>
    <item>
      <title>A Multi-Objective Optimization framework for Decentralized Learning with coordination constraints</title>
      <link>https://arxiv.org/abs/2507.13983</link>
      <description>arXiv:2507.13983v1 Announce Type: new 
Abstract: This article introduces a generalized framework for Decentralized Learning formulated as a Multi-Objective Optimization problem, in which both distributed agents and a central coordinator contribute independent, potentially conflicting objectives over a shared model parameter space. Unlike traditional approaches that merge local losses under a common goal, our formulation explicitly incorporates coordinator-side criteria, enabling more flexible and structured training dynamics. To navigate the resulting trade-offs, we explore scalarization strategies, particularly weighted sums, to construct tractable surrogate problems. These yield solutions that are provably Pareto optimal under standard convexity and smoothness assumptions, while embedding global preferences directly into local updates. We propose a decentralized optimization algorithm with convergence guarantees, and demonstrate its empirical performance through simulations, highlighting the impact of the coordinator's influence on local agent behavior. The proposed approach offers a principled and customizable strategy for balancing personalization, fairness, and coordination in decentralized learning systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13983v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberto Morales, Umberto Biccari</dc:creator>
    </item>
    <item>
      <title>cuPDLP+: A Further Enhanced GPU-Based First-Order Solver for Linear Programming</title>
      <link>https://arxiv.org/abs/2507.14051</link>
      <description>arXiv:2507.14051v1 Announce Type: new 
Abstract: We introduce cuPDLP+, a further enhanced GPU-based first-order solver for linear programming. Building on the predecessor cuPDLP, cuPDLP+ incorporates recent algorithmic advances, including the restarted Halpern PDHG method with reflection, a novel restart criterion, and a PID-controlled primal weight update. These innovations are carefully tailored for GPU architectures and deliver substantial empirical gains. On a comprehensive benchmark of MIPLIB LP relaxations, cuPDLP+ achieves 2x - 4x speedup over cuPDLP, with particularly strong improvements in high-accuracy and presolve-enabled settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14051v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haihao Lu, Zedong Peng, Jinwen Yang</dc:creator>
    </item>
    <item>
      <title>Last-Iterate Complexity of SGD for Convex and Smooth Stochastic Problems</title>
      <link>https://arxiv.org/abs/2507.14122</link>
      <description>arXiv:2507.14122v1 Announce Type: new 
Abstract: Most results on Stochastic Gradient Descent (SGD) in the convex and smooth setting are presented under the form of bounds on the ergodic function value gap. It is an open question whether bounds can be derived directly on the last iterate of SGD in this context. Recent advances suggest that it should be possible. For instance, it can be achieved by making the additional, yet unverifiable, assumption that the variance of the stochastic gradients is uniformly bounded. In this paper, we show that there is no need of such an assumption, and that SGD enjoys a $\tilde O \left( T^{-1/2} \right)$ last-iterate complexity rate for convex smooth stochastic problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14122v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillaume Garrigos, Daniel Cortild, Lucas Ketels, Juan Peypouquet</dc:creator>
    </item>
    <item>
      <title>Total Generalized Variation of the Normal Vector Field and Applications to Mesh Denoising</title>
      <link>https://arxiv.org/abs/2507.13530</link>
      <description>arXiv:2507.13530v1 Announce Type: cross 
Abstract: We propose a novel formulation for the second-order total generalized variation (TGV) of the normal vector on an oriented, triangular mesh embedded in $\mathbb{R}^3$. The normal vector is considered as a manifold-valued function, taking values on the unit sphere. Our formulation extends previous discrete TGV models for piecewise constant scalar data that utilize a Raviart-Thomas function space. To exctend this formulation to the manifold setting, a tailor-made tangential Raviart-Thomas type finite element space is constructed in this work. The new regularizer is compared to existing methods in mesh denoising experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13530v1</guid>
      <category>cs.CV</category>
      <category>math.DG</category>
      <category>math.OC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lukas Baumg\"artner, Ronny Bergmann, Roland Herzog, Stephan Schmidt, Manuel Wei{\ss}</dc:creator>
    </item>
    <item>
      <title>The periodic KdV with control on space-time measurable sets</title>
      <link>https://arxiv.org/abs/2507.13740</link>
      <description>arXiv:2507.13740v1 Announce Type: cross 
Abstract: In this paper, we establish the local exact controllability of the KdV equation on torus around equilibrium states, where both the spatial control region and the temporal control region are sets of positive measure. The proof is based on a novel strategy for proving observability inequalities on space-time measurable sets. This approach is applicable to a broad class of dispersive equations on torus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13740v1</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingrui Niu, Ming Wang, Shengquan Xiang</dc:creator>
    </item>
    <item>
      <title>Search-Optimized Quantization in Biomedical Ontology Alignment</title>
      <link>https://arxiv.org/abs/2507.13742</link>
      <description>arXiv:2507.13742v1 Announce Type: cross 
Abstract: In the fast-moving world of AI, as organizations and researchers develop more advanced models, they face challenges due to their sheer size and computational demands. Deploying such models on edge devices or in resource-constrained environments adds further challenges related to energy consumption, memory usage and latency. To address these challenges, emerging trends are shaping the future of efficient model optimization techniques. From this premise, by employing supervised state-of-the-art transformer-based models, this research introduces a systematic method for ontology alignment, grounded in cosine-based semantic similarity between a biomedical layman vocabulary and the Unified Medical Language System (UMLS) Metathesaurus. It leverages Microsoft Olive to search for target optimizations among different Execution Providers (EPs) using the ONNX Runtime backend, followed by an assembled process of dynamic quantization employing Intel Neural Compressor and IPEX (Intel Extension for PyTorch). Through our optimization process, we conduct extensive assessments on the two tasks from the DEFT 2020 Evaluation Campaign, achieving a new state-of-the-art in both. We retain performance metrics intact, while attaining an average inference speed-up of 20x and reducing memory usage by approximately 70%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13742v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oussama Bouaggad, Natalia Grabar</dc:creator>
    </item>
    <item>
      <title>Convex computation of regions of attraction from data using Sums-of-Squares programming</title>
      <link>https://arxiv.org/abs/2507.14073</link>
      <description>arXiv:2507.14073v1 Announce Type: cross 
Abstract: The paper concentrates on the analysis of the region of attraction (ROA) for unknown autonomous dynamical systems. The aim is to explore a data-driven approach based on moment-sum-of-squares (SoS) hierarchy, which enables novel RoA outer approximations despite the reduced information on the structure of the dynamics. The main contribution of this work is bypassing the system model and, consequently, the recurring constraint on its polynomial structure. Numerical experimentation showcases the influence of data on learned approximating sets, offering a promising outlook on the potential of this method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14073v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Oumayma Khattabi, Matteo Tacchi-B\'enard, Sorin Olaru</dc:creator>
    </item>
    <item>
      <title>Stochastic Primal-Dual Q-Learning</title>
      <link>https://arxiv.org/abs/1810.08298</link>
      <description>arXiv:1810.08298v3 Announce Type: replace 
Abstract: In this work, we present a new model-free and off-policy reinforcement learning (RL) algorithm, that is capable of finding a near-optimal policy with state-action observations from arbitrary behavior policies. Our algorithm, called the stochastic primal-dual Q-learning (SPD Q-learning), hinges upon a new linear programming formulation and a dual perspective of the standard Q-learning. In contrast to previous primal-dual RL algorithms, the SPD Q-learning includes a Q-function estimation step, thus allowing to recover an approximate policy from the primal solution as well as the dual solution. We prove a first-of-its-kind result that the SPD Q-learning guarantees a certain convergence rate, even when the state-action distribution is time-varying but sub-linearly converges to a stationary distribution. Numerical experiments are provided to demonstrate the off-policy learning abilities of the proposed algorithm in comparison to the standard Q-learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:1810.08298v3</guid>
      <category>math.OC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Narim Jeong, Donghwan Lee, Niao He</dc:creator>
    </item>
    <item>
      <title>How does the contraction property fail for convex functions on normed spaces?</title>
      <link>https://arxiv.org/abs/2311.15152</link>
      <description>arXiv:2311.15152v2 Announce Type: replace 
Abstract: On Euclidean and Hilbert spaces, Riemannian manifolds, and CAT$(0)$-spaces, gradient flows of convex functions are known to satisfy the contraction property, which plays a fundamental role in optimization theory and possesses fruitful analytic and geometric applications. On (non-inner product) normed spaces, however, gradient flows of convex functions do not satisfy the contraction property. We give a detailed proof of this characterization of inner products, and discuss a possible form of a weaker contraction property on normed spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15152v2</guid>
      <category>math.OC</category>
      <category>math.MG</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shin-ichi Ohta</dc:creator>
    </item>
    <item>
      <title>The Riemannian Convex Bundle Method</title>
      <link>https://arxiv.org/abs/2402.13670</link>
      <description>arXiv:2402.13670v3 Announce Type: replace 
Abstract: We introduce the convex bundle method to solve convex, non-smooth optimization problems on Riemannian manifolds of bounded sectional curvature. Each step of our method is based on a model that involves the convex hull of previously collected subgradients, parallelly transported into the current serious iterate. This approach generalizes the dual form of classical bundle subproblems in Euclidean space. We prove that, under mild conditions, the convex bundle method converges to a minimizer. Several numerical examples implemented using Manopt$.$jl illustrate the performance of the proposed method and compare it to the subgradient method, the cyclic proximal point algorithm, as well as the proximal bundle method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13670v3</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.DG</category>
      <category>math.NA</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ronny Bergmann, Roland Herzog, Hajg Jasa</dc:creator>
    </item>
    <item>
      <title>Sharper Exponential Convergence Rates for Sinkhorn's Algorithm in Continuous Settings</title>
      <link>https://arxiv.org/abs/2407.01202</link>
      <description>arXiv:2407.01202v2 Announce Type: replace 
Abstract: We study the convergence rate of Sinkhorn's algorithm for solving entropy-regularized optimal transport problems when at least one of the probability measures, $\mu$, admits a density over $\mathbb{R}^d$. For a semi-concave cost function bounded by $c_{\infty}$ and a regularization parameter $\lambda &gt; 0$, we obtain exponential convergence guarantees on the dual sub-optimality gap with contraction rate polynomial in $\lambda/c_{\infty}$. This represents an exponential improvement over the known contraction rate $1 - \Theta(\exp(-c_{\infty}/\lambda))$ achievable via Hilbert's projective metric. Specifically, we prove a contraction rate value of $1-\Theta(\lambda^2/c_\infty^2)$ when $\mu$ has a bounded log-density. In some cases, such as when $\mu$ is log-concave and the cost function is $c(x,y)=-\langle x, y \rangle$, this rate improves to $1-\Theta(\lambda/c_\infty)$. The latter rate matches the one that we derive for the transport between isotropic Gaussian measures, indicating tightness in the dependency in $\lambda/c_\infty$. Our results are fully non-asymptotic and explicit in all the parameters of the problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01202v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>L\'ena\"ic Chizat, Alex Delalande, Tomas Va\v{s}kevi\v{c}ius</dc:creator>
    </item>
    <item>
      <title>Mixed Markov-Perfect Equilibria in the Continuous-Time War of Attrition</title>
      <link>https://arxiv.org/abs/2407.04878</link>
      <description>arXiv:2407.04878v2 Announce Type: replace 
Abstract: We prove the existence of a Markov-perfect equilibrium in randomized stopping times for a model of the war of attrition in which the underlying state variable follows a homogenous linear diffusion. The proof uses the fact that the space of Markovian randomized stopping times can be topologized as a compact absolute retract, which in turn enables us to use a powerful fixed-point theorem by Eilenberg and Montgomery. We illustrate our results with an example of a war of attrition that admits a mixed-strategy Markov-perfect equilibrium but no pure-strategy Markov-perfect equilibrium.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04878v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean-Paul D\'ecamps, Fabien Gensbittel, Thomas Mariotti</dc:creator>
    </item>
    <item>
      <title>Optimization via Strategic Law of Large Numbers</title>
      <link>https://arxiv.org/abs/2412.05604</link>
      <description>arXiv:2412.05604v3 Announce Type: replace 
Abstract: This paper proposes a unified framework for the global optimization of a continuous function in a bounded rectangular domain. Specifically, we show that: (1) under the optimal strategy for a two-armed decision model, the sample mean converges to a global optimizer under the Strategic Law of Large Numbers, and (2) a sign-based strategy built upon the solution of a parabolic PDE is asymptotically optimal. Motivated by this result, we propose a class of {\bf S}trategic {\bf M}onte {\bf C}arlo {\bf O}ptimization (SMCO) algorithms, which uses a simple strategy that makes coordinate-wise two-armed decisions based on the signs of the partial gradient of the original function being optimized over (without the need of solving PDEs). While this simple strategy is not generally optimal, we show that it is sufficient for our SMCO algorithm to converge to local optimizer(s) from a single starting point, and to global optimizers under a growing set of starting points. Numerical studies demonstrate the suitability of our SMCO algorithms for global optimization, and illustrate the promise of our theoretical framework and practical approach. For a wide range of test functions with challenging optimization landscapes (including ReLU neural networks with square and hinge loss), our SMCO algorithms converge to the global maximum accurately and robustly, using only a small set of starting points (at most 100 for dimensions up to 1000) and a small maximum number of iterations (200). In fact, our algorithms outperform many state-of-the-art global optimizers, as well as local algorithms augmented with the same set of starting points as ours.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05604v3</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaohong Chen, Zengjing Chen, Wayne Yuan Gao, Xiaodong Yan, Guodong Zhang</dc:creator>
    </item>
    <item>
      <title>A Bayesian Composite Risk Approach for Stochastic Optimal Control and Markov Decision Processes</title>
      <link>https://arxiv.org/abs/2412.16488</link>
      <description>arXiv:2412.16488v2 Announce Type: replace 
Abstract: Inspired by Shapiro et al.~\cite{shapiro2023episodic}, we consider a stochastic optimal control (SOC) and Markov decision process (MDP) where the risks arising from epistemic and aleatoric uncertainties are assessed using Bayesian composite risk (BCR) measures (Qian et al.~\cite{qian2019composite}). The time dependence of the risk measures allows us to capture the decision maker's (DM) dynamic risk preferences opportunely as increasing information about both uncertainties is obtained. This makes the new BCR-SOC/MDP model more flexible than conventional risk-averse SOC/MDP models. Unlike \cite{shapiro2023episodic} where the control/action at each episode is based on the current state alone, the new model allows the control to depend on the probability distribution of the epistemic uncertainty, which reflects the fact that in many practical instances the cumulative information about epistemic uncertainty often affects the DM's belief about the future aleatoric uncertainty and hence the DM's action \cite{strens2000bayesian}. The new modeling paradigm incorporates several existing SOC/MDP models including distributionally robust SOC/MDP models and Bayes-adaptive MDP models and generates so-called preference robust SOC/MDP models. Moreover, we derive conditions under which the BCR-SOC/MDP model is well-defined, demonstrate that finite-horizon BCR-SOC/MDP models can be solved using dynamic programming techniques, and extend the discussion to the infinite-horizon case. By using Bellman equations, we show that under some standard conditions, asymptotic convergence of the optimal values and optimal actions as the episodic variable goes to infinity is achieved. Finally, we carry out numerical tests on a finite horizon spread betting problem and an inventory control problem and show the effectiveness of the proposed model and numerical schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16488v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wentao Ma, Zhiping Chen, Huifu Xu</dc:creator>
    </item>
    <item>
      <title>Monge-Kantorovich's duality for separable Baire measures in completely regular Hausdorff spaces</title>
      <link>https://arxiv.org/abs/2503.03929</link>
      <description>arXiv:2503.03929v4 Announce Type: replace 
Abstract: We generalize the classical Monge-Kantorovich duality--typically established for tight (Radon) probability measures--to separable Baire probability measures, which are strictly more general than tight measures on completely regular Hausdorff spaces. Within this broader framework, we also demonstrate the existence of solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03929v4</guid>
      <category>math.OC</category>
      <category>math.FA</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammed Bachir</dc:creator>
    </item>
    <item>
      <title>Structure Learning via ADMM in Networks obeying Conservation Laws</title>
      <link>https://arxiv.org/abs/2504.03189</link>
      <description>arXiv:2504.03189v2 Announce Type: replace 
Abstract: Learning the edge connectivity structure of networked systems from limited data is a fundamental challenge in many critical infrastructure domains, including power, traffic, and finance. Such systems obey steady-state conservation laws: x = L*y, where x and y represent injected flows (inputs) and potentials (outputs), respectively. The sparsity pattern of the pxp Laplacian L* encodes the underlying edge structure. In a stochastic setting, the goal is to infer this sparsity pattern from zero-mean i.i.d. samples of y.
  Recent work by \cite{rayas2022learning} has established statistical consistency results for this learning problem by considering an $\ell_1$-regularized maximum likelihood estimator. However, their approach did not develop a scalable algorithm but relies on solving a convex program via the CVX package. To address this gap, we propose an alternating direction method of multipliers (ADMM), which is transparent and fast. A key contribution is to demonstrate the role of an algebraic matrix Riccati equation in the primal update step of ADMM. Numerical experiments on a host of synthetic and benchmark networks, including power and water systems, show the efficiency of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03189v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rohith Reddy Mada, Rajasekhar Anguluri</dc:creator>
    </item>
    <item>
      <title>Reducing real-time complexity via sub-control Lyapunov functions: from theory to experiments</title>
      <link>https://arxiv.org/abs/2501.09143</link>
      <description>arXiv:2501.09143v3 Announce Type: replace-cross 
Abstract: The techniques to design control Lyapunov functions (CLF), along with a proper stabilizing feedback, possibly in the presence of constraints, often provide control laws that are too complex for proper implementation online, especially when an optimization problem is involved. In this work, we show how to acquire an alternative, computationally attractive feedback. Given a nominal CLF and a nominal state feedback, we say that a different positive definite function is a Sub-control Lyapunov function (SCLF) if its Lyapunov derivative is negative-definite and bounded above by the Lyapunov derivative of the nominal function with the nominal control. It turns out that if we consider a family of basis functions, then a SCLF can be computed by linear programming, with an infinite number of constraints. The idea is that although the offline computational burden to achieve the new controller and solve the linear program is considerable, the online computational burden is drastically reduced. Comprehensive simulations and experiments on drone control are conducted to demonstrate the effectiveness of the study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09143v3</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Huu-Thinh Do, Franco Blanchini, Stefano Miani, Ionela Prodan</dc:creator>
    </item>
    <item>
      <title>Fractional Sobolev processes on Wasserstein spaces and their energy-minimizing particle representations with applications</title>
      <link>https://arxiv.org/abs/2503.10859</link>
      <description>arXiv:2503.10859v3 Announce Type: replace-cross 
Abstract: Given a probability-measure-valued process $(\mu_t)$, we aim to find, among all path-continuous stochastic processes whose one-dimensional time marginals coincide almost surely with $(\mu_t)$ (if there is any), a process that minimizes a given energy in expectation. Building on our recent study (arXiv:2502.12068), where the minimization of fractional Sobolev energy was investigated for deterministic paths on Wasserstein spaces, we now extend the results to the stochastic setting to address some applications that originally motivated our study. Two applications are given. We construct minimizing particle representations for processes on Wasserstein spaces on $\mathbb{R}$ with H\"{o}lder regularity, using optimal transportation. We prove the existence of minimizing particle representations for solutions to stochastic Fokker--Planck--Kolmogorov equations on $\mathbb{R}^\mathrm{d}$ satisfying an integrability condition, using the stochastic superposition principle of Lacker--Shkolnikov--Zhang (J. Eur. Math. Soc. 25, 3229--3288 (2023)).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10859v3</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ehsan Abedi</dc:creator>
    </item>
    <item>
      <title>Improving DAPO from a Mixed-Policy Perspective</title>
      <link>https://arxiv.org/abs/2507.12931</link>
      <description>arXiv:2507.12931v2 Announce Type: replace-cross 
Abstract: This paper introduces two novel modifications to the Dynamic sAmpling Policy Optimization (DAPO) algorithm [1], approached from a mixed-policy perspective. Standard policy gradient methods can suffer from instability and sample inefficiency, particularly in sparse reward settings. To address this, we first propose a method that incorporates a pre-trained, stable guiding policy ($\piphi$) to provide off-policy experience, thereby regularizing the training of the target policy ($\pion$). This approach improves training stability and convergence speed by adaptively adjusting the learning step size. Secondly, we extend this idea to re-utilize zero-reward samples, which are often discarded by dynamic sampling strategies like DAPO's. By treating these samples as a distinct batch guided by the expert policy, we further enhance sample efficiency. We provide a theoretical analysis for both methods, demonstrating that their objective functions converge to the optimal solution within the established theoretical framework of reinforcement learning. The proposed mixed-policy framework effectively balances exploration and exploitation, promising more stable and efficient policy optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12931v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongze Tan</dc:creator>
    </item>
  </channel>
</rss>
