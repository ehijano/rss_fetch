<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 May 2024 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Oil &amp; Water? Diffusion of AI Within and Across Scientific Fields</title>
      <link>https://arxiv.org/abs/2405.15828</link>
      <description>arXiv:2405.15828v1 Announce Type: new 
Abstract: This study empirically investigates claims of the increasing ubiquity of artificial intelligence (AI) within roughly 80 million research publications across 20 diverse scientific fields, by examining the change in scholarly engagement with AI from 1985 through 2022. We observe exponential growth, with AI-engaged publications increasing approximately thirteenfold (13x) across all fields, suggesting a dramatic shift from niche to mainstream. Moreover, we provide the first empirical examination of the distribution of AI-engaged publications across publication venues within individual fields, with results that reveal a broadening of AI engagement within disciplines. While this broadening engagement suggests a move toward greater disciplinary integration in every field, increased ubiquity is associated with a semantic tension between AI-engaged research and more traditional disciplinary research. Through an analysis of tens of millions of document embeddings, we observe a complex interplay between AI-engaged and non-AI-engaged research within and across fields, suggesting that increasing ubiquity is something of an oil-and-water phenomenon -- AI-engaged work is spreading out over fields, but not mixing well with non-AI-engaged work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15828v1</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eamon Duede, William Dolan, Andr\'e Bauer, Ian Foster, Karim Lakhani</dc:creator>
    </item>
    <item>
      <title>Past, Present, and Future of Citation Practices in HCI</title>
      <link>https://arxiv.org/abs/2405.16526</link>
      <description>arXiv:2405.16526v1 Announce Type: cross 
Abstract: Science is a complex system comprised of many scientists who individually make collective decisions that, due to the size and nature of the academic system, largely do not affect the system as a whole. However, certain decisions at the meso-level of research communities, such as the Human-Computer Interaction (HCI) community, may result in deep and long-lasting behavioral changes in scientists. In this article, we provide evidence on how a change in editorial policies introduced at the ACM CHI Conference in 2016 launched the CHI community on an expansive path, denoted by a year-by-year increase in the mean number of references included in CHI articles. If this near-linear trend continues undisrupted, an article in CHI 2030 will include on average almost 130 references. Our meta-research provides insights into how the nature and meaning of citation practices in HCI have changed, influenced by factors such as digital accessibility of resources and academic pressures. The observed trend towards more citations reflects a citation culture where quantity is prioritized over quality, contributing to both author and peer reviewer fatigue. This article underscores the value of meta-research for research communities and the profound impact that meso-level policy adjustments have on the evolution of scientific fields and disciplines, urging stakeholders to carefully consider the broader implications of such changes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16526v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonas Oppenlaender</dc:creator>
    </item>
    <item>
      <title>Generation and human-expert evaluation of interesting research ideas using knowledge graphs and large language models</title>
      <link>https://arxiv.org/abs/2405.17044</link>
      <description>arXiv:2405.17044v1 Announce Type: cross 
Abstract: Advanced artificial intelligence (AI) systems with access to millions of research papers could inspire new research ideas that may not be conceived by humans alone. However, how interesting are these AI-generated ideas, and how can we improve their quality? Here, we introduce SciMuse, a system that uses an evolving knowledge graph built from more than 58 million scientific papers to generate personalized research ideas via an interface to GPT-4. We conducted a large-scale human evaluation with over 100 research group leaders from the Max Planck Society, who ranked more than 4,000 personalized research ideas based on their level of interest. This evaluation allows us to understand the relationships between scientific interest and the core properties of the knowledge graph. We find that data-efficient machine learning can predict research interest with high precision, allowing us to optimize the interest-level of generated research ideas. This work represents a step towards an artificial scientific muse that could catalyze unforeseen collaborations and suggest interesting avenues for scientists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17044v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuemei Gu, Mario Krenn</dc:creator>
    </item>
    <item>
      <title>Auditing citation polarization during the early COVID-19 pandemic</title>
      <link>https://arxiv.org/abs/2301.01926</link>
      <description>arXiv:2301.01926v2 Announce Type: replace 
Abstract: The recent pandemic stimulated scientists to publish a significant amount of research that created a surge of citations of COVID-19-related publications in a short time, leading to an abrupt inflation of the journal impact factor (IF). By auditing the complete set of COVID-19-related publications in the Web of Science, we reveal here that COVID-19-related research worsened the polarization of academic journals: the IF before the pandemic was proportional to the increment of IF, which had the effect of increasing inequality while retaining the journal rankings. We also found that the most highly cited studies related to COVID-19 were published in prestigious journals at the onset of the epidemic. Through the present quantitative investigation, our findings caution against the belief that quantitative metrics, particularly IF, can indicate the significance of individual papers. Rather, such metrics reflect the social attention given to a particular study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.01926v2</guid>
      <category>cs.DL</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taekho You, Jinseo Park, June Young Lee, Jinhyuk Yun</dc:creator>
    </item>
  </channel>
</rss>
