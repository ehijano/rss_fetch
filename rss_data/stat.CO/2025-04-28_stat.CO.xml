<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 Apr 2025 03:07:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Numerical Generalized Randomized Hamiltonian Monte Carlo for piecewise smooth target densities</title>
      <link>https://arxiv.org/abs/2504.18210</link>
      <description>arXiv:2504.18210v1 Announce Type: new 
Abstract: Traditional gradient-based sampling methods, like standard Hamiltonian Monte Carlo, require that the desired target distribution is continuous and differentiable. This limits the types of models one can define, although the presented models capture the reality in the observations better. In this project, Generalized Randomized Hamiltonian Monte Carlo (GRHMC) processes for sampling continuous densities with discontinuous gradient and piecewise smooth targets are proposed. The methods combine the advantages of Hamiltonian Monte Carlo methods with the nature of continuous time processes in the form of piecewise deterministic Markov processes to sample from such distributions. It is argued that the techniques lead to GRHMC processes that admit the desired target distribution as the invariant distribution in both scenarios. Simulation experiments verifying this fact and several relevant real-life models are presented, including a new parameterization of the spike and slab prior for regularized linear regression that returns sparse coefficient estimates and a regime switching volatility model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18210v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jimmy Huy Tran, Tore Selland Kleppe</dc:creator>
    </item>
    <item>
      <title>Statistical Disaggregation -- a Monte Carlo Approach for Imputation under Constraints</title>
      <link>https://arxiv.org/abs/2504.18377</link>
      <description>arXiv:2504.18377v1 Announce Type: new 
Abstract: Equality-constrained models naturally arise in problems in which measurements are taken at different levels of resolution. The challenge in this setting is that the models usually induce a joint distribution which is intractable. Resorting to instead sampling from the joint distribution by means of a Monte Carlo approach is also challenging. For example, a naive rejection sampling does not work when the probability mass of the constraint is zero. A typical example of such constrained problems is to learn energy consumption for a higher resolution level based on data at a lower resolution, e.g., to decompose a daily reading into readings at a finer level. We introduce a novel Monte Carlo sampling algorithm based on Langevin diffusions and rejection sampling to solve the problem of sampling from equality-constrained models. Our method has the advantage of being exact for linear constraints and naturally deals with multimodal distributions on arbitrary constraints. We test our method on statistical disaggregation problems for electricity consumption datasets, and our approach provides better uncertainty estimation and accuracy in data imputation compared with other naive/unconstrained methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18377v1</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shenggang Hu, Hongsheng Dai, Fanlin Meng, Louis Aslett, Murray Pollock, Gareth O. Roberts</dc:creator>
    </item>
    <item>
      <title>Analysis of Multiple-try Metropolis via Poincar\'e inequalities</title>
      <link>https://arxiv.org/abs/2504.18409</link>
      <description>arXiv:2504.18409v1 Announce Type: new 
Abstract: We study the Multiple-try Metropolis algorithm using the framework of Poincar\'e inequalities. We describe the Multiple-try Metropolis as an auxiliary variable implementation of a resampling approximation to an ideal Metropolis--Hastings algorithm. Under suitable moment conditions on the importance weights, we derive explicit Poincar\'e comparison results between the Multiple-try algorithm and the ideal algorithm. We characterize the spectral gap of the latter, and finally in the Gaussian case prove explicit non-asymptotic convergence bounds for Multiple-try Metropolis by comparison.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18409v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rocco Caprio, Sam Power, Andi Wang</dc:creator>
    </item>
    <item>
      <title>Structured Bayesian Regression Tree Models for Estimating Distributed Lag Effects: The R Package dlmtree</title>
      <link>https://arxiv.org/abs/2504.18452</link>
      <description>arXiv:2504.18452v1 Announce Type: cross 
Abstract: When examining the relationship between an exposure and an outcome, there is often a time lag between exposure and the observed effect on the outcome. A common statistical approach for estimating the relationship between the outcome and lagged measurements of exposure is a distributed lag model (DLM). Because repeated measurements are often autocorrelated, the lagged effects are typically constrained to vary smoothly over time. A recent statistical development on the smoothing constraint is a tree structured DLM framework. We present an R package dlmtree, available on CRAN, that integrates tree structured DLM and extensions into a comprehensive software package with user-friendly implementation. A conceptual background on tree structured DLMs and demonstration of the fitting process of each model using simulated data are provided. We also demonstrate inference and interpretation using the fitted models, including summary and visualization. Additionally, a built-in shiny app for heterogeneity analysis is included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18452v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seongwon Im, Ander Wilson, Daniel Mork</dc:creator>
    </item>
    <item>
      <title>Symmetry-driven embedding of networks in hyperbolic space</title>
      <link>https://arxiv.org/abs/2406.10711</link>
      <description>arXiv:2406.10711v2 Announce Type: replace 
Abstract: Hyperbolic models are known to produce networks with properties observed empirically in most network datasets, including heavy-tailed degree distribution, high clustering, and hierarchical structures. As a result, several embeddings algorithms have been proposed to invert these models and assign hyperbolic coordinates to network data. Current algorithms for finding these coordinates, however, do not quantify uncertainty in the inferred coordinates. We present BIGUE, a Markov chain Monte Carlo (MCMC) algorithm that samples the posterior distribution of a Bayesian hyperbolic random graph model. We show that the samples are consistent with current algorithms while providing added credible intervals for the coordinates and all network properties. We also show that some networks admit two or more plausible embeddings, a feature that an optimization algorithm can easily overlook.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10711v2</guid>
      <category>stat.CO</category>
      <category>cs.SI</category>
      <category>stat.ML</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Lizotte, Jean-Gabriel Young, Antoine Allard</dc:creator>
    </item>
    <item>
      <title>RandALO: Out-of-sample risk estimation in no time flat</title>
      <link>https://arxiv.org/abs/2409.09781</link>
      <description>arXiv:2409.09781v2 Announce Type: replace-cross 
Abstract: Estimating out-of-sample risk for models trained on large high-dimensional datasets is an expensive but essential part of the machine learning process, enabling practitioners to optimally tune hyperparameters. Cross-validation (CV) serves as the de facto standard for risk estimation but poorly trades off high bias ($K$-fold CV) for computational cost (leave-one-out CV). We propose a randomized approximate leave-one-out (RandALO) risk estimator that is not only a consistent estimator of risk in high dimensions but also less computationally expensive than $K$-fold CV. We support our claims with extensive simulations on synthetic and real data and provide a user-friendly Python package implementing RandALO available on PyPI as randalo and at https://github.com/cvxgrp/randalo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09781v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Parth Nobel, Daniel LeJeune, Emmanuel J. Cand\`es</dc:creator>
    </item>
    <item>
      <title>Integrative Learning of Quantum Dot Intensity Fluctuations under Excitation via Tailored Dynamic Mixture Modeling</title>
      <link>https://arxiv.org/abs/2501.01292</link>
      <description>arXiv:2501.01292v2 Announce Type: replace-cross 
Abstract: Semiconductor nano-crystals, known as quantum dots (QDs), have attracted significant attention for their unique fluorescence properties. Under continuous excitation, QDs emit photons with intricate intensity fluctuation: the intensity of photon emission fluctuates during the excitation, and such a fluctuation pattern can vary across different QDs even under the same experimental conditions. What adding to the complication is that the processed intensity series are non-Gaussian and truncated due to necessary thresholding and normalization. Conventional normality-based single-dot analysis fall short of addressing these complexities. In collaboration with chemists, we develop an integrative learning approach to simultaneously analyzing intensity series from multiple QDs. Motivated by the unique data structure and the hypothesized behaviors of the QDs, our approach leverages the celebrated hidden Markov model as its structural backbone to characterize individual dot intensity fluctuations, while assuming that, in each state the normalized intensity follows a 0/1 inflated Beta distribution, the state/emission distributions are shared across the QDs, and the state transition dynamics can vary among a few QD clusters. This framework allows for a precise, collective characterization of intensity fluctuation patterns and have the potential to transform current practice in chemistry. Applying our method to experimental data from 128 QDs, we reveal three shared intensity states and capture several distinct intensity transition patterns, underscoring the effectiveness of our approach in providing deeper insights into QD behaviors and their design and application potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01292v2</guid>
      <category>physics.app-ph</category>
      <category>cond-mat.mes-hall</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Yang, Hawi Nyiera, Yonglei Sun, Jing Zhao, Kun Chen</dc:creator>
    </item>
    <item>
      <title>Here Be Dragons: Bimodal posteriors arise from numerical integration error in longitudinal models</title>
      <link>https://arxiv.org/abs/2502.11510</link>
      <description>arXiv:2502.11510v2 Announce Type: replace-cross 
Abstract: Longitudinal models with dynamics governed by differential equations may require numerical integration alongside parameter estimation. We have identified a situation where the numerical integration introduces error in such a way that it becomes a novel source of non-uniqueness in estimation. We obtain two very different sets of parameters, one of which is a good estimate of the true values and the other a very poor one. The two estimates have forward numerical projections statistically indistinguishable from each other because of numerical error. In such cases, the posterior distribution for parameters is bimodal, with a dominant mode closer to the true parameter value, and a second cluster around the errant value. We demonstrate that multi-modality exists both theoretically and empirically for an affine first order differential equation, that a simulation workflow can test for evidence of the issue more generally, and that Markov Chain Monte Carlo sampling with a suitable solution can avoid bimodality. The issue of multi-modal posteriors arising from numerical error has consequences for Bayesian inverse methods that rely on numerical integration more broadly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11510v2</guid>
      <category>stat.OT</category>
      <category>stat.CO</category>
      <pubDate>Mon, 28 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tess O'Brien, Matt Moores, David Warton, Daniel Falster</dc:creator>
    </item>
  </channel>
</rss>
