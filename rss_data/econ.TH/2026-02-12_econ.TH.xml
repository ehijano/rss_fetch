<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.TH</link>
    <description>econ.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Feb 2026 05:01:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Filling Positions Without Transfers: Screening on Outside Options</title>
      <link>https://arxiv.org/abs/2602.10428</link>
      <description>arXiv:2602.10428v1 Announce Type: new 
Abstract: A designer offers vertically-differentiated positions to agents in the absence of transfers. Agents have private outside options and may reject their offers ex-post. The designer has preferences over the quantity of agents who accept each position. We show that under a general condition on the distribution of outside options, an optimal mechanism for the designer offers all agents an identical lottery, and we characterize this mechanism. When our condition does not hold, the optimal mechanism may require screening agents by offering a menu of distinct lotteries. Our results follow from a decomposition of agents' participation probabilities in any feasible mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10428v1</guid>
      <category>econ.TH</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Morteza Honarvar, Joanna Krysta, Eric Tang</dc:creator>
    </item>
    <item>
      <title>How to Ask for Belief Statistics without Distortion?</title>
      <link>https://arxiv.org/abs/2602.10474</link>
      <description>arXiv:2602.10474v1 Announce Type: new 
Abstract: Belief elicitation is ubiquitous in experiments but can distort behavior in the main tasks. We study when, and how, an experimenter can ask for a series of action-dependent belief statistics after a subject chooses an action, while incentivize truthful reports without distorting the subject's optimal action in the main experimental tasks. We first propose a novel mechanism called the Counterfactual Scoring Rule (CSR), which achieves such nondistortionary elicitation of any single belief statistic by decomposing it into supplemental action-independent statistics. In contrast, when eliciting a fixed set of belief statistics without such decomposition, we show that robust nondistortionary elicitation is achievable if and only if the questions satisfy a joint alignment condition with the task payoff. The necessity of joint alignment is established through a graph theoretical approach, while its sufficiency follows from invoking an adaptation of the Becker-DeGroot-Marschak mechanism. Our characterization applies to experiments with general task-payoff structures and belief elicitation questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10474v1</guid>
      <category>econ.TH</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yi-Chun Chen, Ruoyu Wang, Xinhan Zhang</dc:creator>
    </item>
    <item>
      <title>Identifying Behavioral Types</title>
      <link>https://arxiv.org/abs/2602.10756</link>
      <description>arXiv:2602.10756v1 Announce Type: new 
Abstract: We study identification in models of aggregate choice generated by unobserved behavioral types. An analyst observes only aggregate choice behavior, while the population distribution of types and their type-level choice patterns are latent. Assuming only minimal and purely qualitative prior knowledge of the process generating type-level choice probabilities, we characterize necessary and sufficient conditions for identifiability. Identification obtains if and only if the data exhibit sufficient cross-type behavioral heterogeneity, which we characterize equivalently through combinatorial matching conditions between types and alternatives, and through algebraic properties of the matrices mapping type-level to aggregate choice behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10756v1</guid>
      <category>econ.TH</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher Kops, Paola Manzini, Marco Mariotti, Illia Pasichnichenko</dc:creator>
    </item>
    <item>
      <title>Bayesian Persuasion under Bias Management</title>
      <link>https://arxiv.org/abs/2602.10821</link>
      <description>arXiv:2602.10821v1 Announce Type: new 
Abstract: A principal delegates choice to an agent whose decision depends on both beliefs and tastes. The principal can steer the delegated decision using two costly instruments: (i) an information policy that determines a Bayes--plausible distribution of posteriors, and (ii) a bias-management policy that shifts the agent's effective taste. We study a binary-state, two-action, convex hull of two benchmark tastes specialization with posterior-separable information costs. The analysis admits an inner--outer decomposition: optimal bias management is bang--bang (either no intervention or the minimal intervention needed to flip the agent's action), while the optimal information policy is characterized by concavification of an endogenous posterior value function that already incorporates optimal management and information costs. This structure clarifies how information acquisition and bias management interact; they can be complements, substitutes, or both depending on the primitives of the model. Information changes which posteriors are realized and hence where management is used; management reshapes the curvature and kinks of the posterior value function and hence the marginal value of information. The model delivers regime classifications for pooling vs. informativeness and for management at different posteriors within informative signals, and highlights how comparative statics can be monotone or non-monotone depending on how concavification contact points move with costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10821v1</guid>
      <category>econ.TH</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kemal Ozbek</dc:creator>
    </item>
    <item>
      <title>Metric geometry for ranking-based voting: Tools for learning electoral structure</title>
      <link>https://arxiv.org/abs/2602.10293</link>
      <description>arXiv:2602.10293v1 Announce Type: cross 
Abstract: In this paper, we develop the metric geometry of ranking statistics, proving that the two major permutation distances in the statistics literature -- Kendall tau and Spearman footrule -- extend naturally to incomplete rankings with both coordinate embeddings and graph realizations. This gives us a unifying framework that allows us to connect popular topics in computational social choice: metric preferences (and metric distortion), polarization, and proportionality.
  As an important application, the metric structure enables efficient identification of blocs of voters and slates of their preferred candidates. Since the definitions work for partial ballots, we can execute the methods not only on synthetic elections, but on a suite of real-world elections. This gives us robust clustering methods that often produce an identical grouping of voters -- even though one family of methods is based on a Condorcet-consistent ranking rule while the other is not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10293v1</guid>
      <category>math.MG</category>
      <category>cs.CY</category>
      <category>econ.TH</category>
      <category>math.CO</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moon Duchin, Kristopher Tapp</dc:creator>
    </item>
    <item>
      <title>Informal and Privatized Transit: Incentives, Efficiency and Coordination</title>
      <link>https://arxiv.org/abs/2602.10456</link>
      <description>arXiv:2602.10456v1 Announce Type: cross 
Abstract: Informal and privatized transit services, such as minibuses and shared auto-rickshaws, are integral to daily travel in large urban metropolises, providing affordable commutes where a formal public transport system is inadequate and other options are unaffordable. Despite the crucial role that these services play in meeting mobility needs, governments often do not account for these services or their underlying incentives when planning transit systems, which can significantly compromise system efficiency.
  Against this backdrop, we develop a framework to analyze the incentives underlying informal and privatized transit systems, while proposing mechanisms to guide public transit operation and incentive design when a substantial share of mobility is provided by such profit-driven private operators. We introduce a novel, analytically tractable game-theoretic model of a fully privatized informal transit system with a fixed menu of routes, in which profit-maximizing informal operators (drivers) decide where to provide service and cost-minimizing commuters (riders) decide whether to use these services. Within this framework, we establish tight price of anarchy bounds which demonstrate that decentralized, profit-maximizing driver behavior can lead to bounded yet substantial losses in cumulative driver profit and rider demand served. We further show that these performance losses can be mitigated through targeted interventions, including Stackelberg routing mechanisms in which a modest share of drivers are centrally controlled, reflecting environments where informal operators coexist with public transit, and cross-subsidization schemes that use route-specific tolls or subsidies to incentivize drivers to operate on particular routes. Finally, we reinforce these findings through numerical experiments based on a real-world informal transit system in Nalasopara, India.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10456v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <category>math.OC</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Devansh Jalota, Matthew Tsao</dc:creator>
    </item>
    <item>
      <title>Smart Lotteries in School Choice: Ex-ante Pareto-Improvement with Ex-post Stability</title>
      <link>https://arxiv.org/abs/2602.10679</link>
      <description>arXiv:2602.10679v1 Announce Type: cross 
Abstract: In a typical school choice application, the students have strict preferences over the schools while the schools have coarse priorities over the students based on their distance and their enrolled siblings. The outcome of a centralized admission mechanism is then usually obtained by the Deferred Acceptance (DA) algorithm with random tie-breaking. Therefore, every possible outcome of this mechanism is a stable solution for the coarse priorities that will arise with certain probability. This implies a probabilistic assignment, where the admission probability for each student-school pair is specified. In this paper, we propose a new efficiency-improving stable `smart lottery' mechanism. We aim to improve the probabilistic assignment ex-ante in a stochastic dominance sense, while ensuring that the improved random matching is still ex-post stable, meaning that it can be decomposed into stable matchings regarding the original coarse priorities. Therefore, this smart lottery mechanism can provide a clear Pareto-improvement in expectation for any cardinal utilities compared to the standard DA with lottery solution, without sacrificing the stability of the final outcome. We show that although the underlying computational problem is NP-hard, we can solve the problem by using advanced optimization techniques such as integer programming with column generation. We conduct computational experiments on generated and real instances. Our results show that the welfare gains by our mechanism are substantially larger than the expected gains by standard methods that realize efficiency improvements after ties have already been broken.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10679v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haris Aziz, P\'eter Bir\'o, Gergely Cs\'aji, Tom Demeulemeester</dc:creator>
    </item>
    <item>
      <title>The Computational Intractability of Not Worst Responding</title>
      <link>https://arxiv.org/abs/2602.10966</link>
      <description>arXiv:2602.10966v1 Announce Type: cross 
Abstract: Finding, counting, or determining the existence of Nash equilibria, where players must play optimally given each others' actions, are known to be computational intractable problems. We ask whether weakening optimality to the requirement that each player merely avoid worst responses -- arguably the weakest meaningful rationality criterion -- yields tractable solution concepts. We show that it does not: any solution concept with this minimal guarantee is ``as intractable'' as pure Nash equilibrium. In general games, determining the existence of no-worst-response action profiles is NP-complete, finding one is NP-hard, and counting them is #P-complete. In potential games, where existence is guaranteed, the search problem is PLS-complete. Computational intractability therefore stems not only from the requirement of optimality, but also from the requirement of a minimal rationality guarantee for each player. Moreover, relaxing the latter requirement gives rise to a tractability trade-off between the strength of individual rationality guarantees and the fraction of players satisfying them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10966v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mete \c{S}eref Ahunbay, Paul W. Goldberg, Edwin Lock, Panayotis Mertikopoulos, Bary S. R. Pradelski, Bassel Tarbush</dc:creator>
    </item>
    <item>
      <title>Improving Robust Decisions with Data</title>
      <link>https://arxiv.org/abs/2310.16281</link>
      <description>arXiv:2310.16281v5 Announce Type: replace 
Abstract: A decision-maker faces uncertainty governed by a data-generating process (DGP), which is only known to belong to a set of sequences of independent but possibly non-identical distributions. A robust decision maximizes the expected payoff against the worst possible DGP in this set. This paper characterizes when and how such robust decisions can be \emph{objectively} improved with data -- that is, yield higher expected payoffs under the true DGP regardless of which DGP is the truth. It further develops simple and novel inference procedures that achieve such improvement, while common methods (e.g., maximum likelihood) may fail to do so.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16281v5</guid>
      <category>econ.TH</category>
      <category>econ.EM</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyu Cheng</dc:creator>
    </item>
    <item>
      <title>Statistical Equilibrium of Optimistic Beliefs</title>
      <link>https://arxiv.org/abs/2502.09569</link>
      <description>arXiv:2502.09569v3 Announce Type: replace 
Abstract: We study finite normal-form games in which payoffs are subject to random perturbations and players face uncertainty about how these shocks co-move across actions, an ambiguity that naturally arises when only realized (not counterfactual) payoffs are observed. We introduce the Statistical Equilibrium of Optimistic Beliefs (SE-OB), inspired by discrete choice theory. We model players as \textit{optimistic better responders}: they face ambiguity about the dependence structure (copula) of payoff perturbations across actions and resolve this ambiguity by selecting, from a belief set, the joint distribution that maximizes the expected value of the best perturbed payoff. Given this optimistic belief, players choose actions according to the induced random-utility choice rule. We define SE-OB as a fixed point of this two-step response mapping.
  SE-OB generalizes the Nash equilibrium and the structural quantal response equilibrium. We establish existence under standard regularity conditions on belief sets. For the economically important class of marginal belief sets, that is, the set of all joint distributions with fixed action-wise marginals, optimistic belief selection reduces to an optimal coupling problem, and SE-OB admits a characterization via Nash equilibrium of a smooth regularized game, yielding tractability and enabling computation.
  We characterize the relationship between SE-OB and existing equilibrium notions and illustrate its empirical relevance in simulations, where it captures systematic violations of independence of irrelevant alternatives that standard logit-based models fail to explain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09569v3</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Gui, Bahar Ta\c{s}kesen</dc:creator>
    </item>
    <item>
      <title>Central Bank Digital Currency: Demand Shocks and Optimal Monetary Policy</title>
      <link>https://arxiv.org/abs/2507.15048</link>
      <description>arXiv:2507.15048v2 Announce Type: replace 
Abstract: We study the implications of a central bank digital currency (CBDC) for the transmission of household preference shocks and for welfare in a New Keynesian framework where the CBDC competes with bank deposits for household resources and banks have market power. We show that an increase in the perceived benefit of CBDC has a mildly expansionary effect, weakening bank market power and significantly reducing the deposit spread. As households economize on liquid asset holdings, they reduce both CBDC and deposit balances. However, the degree of bank disintermediation is low, as deposit outflows remain modest. We then examine the welfare implications of CBDC rate setting and find that, compared to a non-interest-bearing CBDC, the gains with standard coefficients for a CBDC interest rate Taylor rule are modest, but they become considerable when the coefficients are optimized. Welfare gains increase with the CBDC benefit, and the optimal policy responses vary with the banking market structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15048v2</guid>
      <category>econ.TH</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanfeng Chen, Maria Elena Filippin</dc:creator>
    </item>
    <item>
      <title>The value of conceptual knowledge</title>
      <link>https://arxiv.org/abs/2509.09170</link>
      <description>arXiv:2509.09170v2 Announce Type: replace 
Abstract: We study the instrumental value of conceptual knowledge when making statistical decisions. Such knowledge tells agents how unknown, payoff-relevant states relate. It is distinct from the statistical knowledge gained from observing signals of those states. We formalize this distinction in a tractable framework used by economists and statisticians. Conceptual knowledge is valuable because it empowers agents to design more informative signals. It is more valuable when states are more "reducible": when they can be explained with fewer common concepts. Its value is non-monotone in the number of signals and vanishes when agents have infinitely many signals. Agents who know more concepts can attain the same payoffs with fewer signals. This is especially true when states are highly reducible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09170v2</guid>
      <category>econ.TH</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Davies, Anirudh Sankar</dc:creator>
    </item>
    <item>
      <title>Games with Payments between Learning Agents</title>
      <link>https://arxiv.org/abs/2405.20880</link>
      <description>arXiv:2405.20880v3 Announce Type: replace-cross 
Abstract: In repeated games, such as auctions, players rely on autonomous learning agents to choose their actions. We study settings in which players have their agents make monetary transfers to other agents during play at their own expense, in order to influence learning dynamics in their favor. Our goal is to understand when players have incentives to use such payments, how payments between agents affect learning outcomes, and what the resulting implications are for welfare and its distribution. We propose a simple game-theoretic model to capture the incentive structure of such scenarios. We find that, quite generally, abstaining from payments is not robust to strategic deviations by users of learning agents: self-interested players benefit from having their agents make payments to other learners. In a broad class of games, such endogenous payments between learning agents lead to higher welfare for all players. In first- and second-price auctions, equilibria of the induced "payment-policy game" lead to highly collusive learning outcomes, with low or vanishing revenue for the auctioneer. These results highlight a fundamental challenge for mechanism design, as well as for regulatory policies, in environments where learning agents may interact in the digital ecosystem beyond a mechanism's boundaries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20880v3</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>econ.TH</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yoav Kolumbus, Joe Halpern, \'Eva Tardos</dc:creator>
    </item>
    <item>
      <title>Information Structures in Stablecoin Markets</title>
      <link>https://arxiv.org/abs/2408.07227</link>
      <description>arXiv:2408.07227v2 Announce Type: replace-cross 
Abstract: Stablecoins have historically depegged due from par to large sales, possibly of speculative nature, or poor reserve asset quality. Using a global game which addresses both concerns, we show that the selling pressure on stablecoin holders increases in the presence of a large sale. While precise public knowledge reduces (increases) the probability of a run when fundamentals are strong (weak), interestingly, more precise private signals increase (reduce) the probability of a run when fundamentals are strong (weak), potentially explaining the stability of opaque stablecoins. The total run probability can be decomposed into components representing risks from large sales and poor collateral. By analyzing how these risk components vary with respect to information uncertainty and fundamentals, we can split the fundamental space into regions based on the type of risk a stablecoin issuer is more prone to. We suggest testable implications and connect our model's implications to real-world applications, including depegging events and the no-questions-asked property of money.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07227v2</guid>
      <category>q-fin.TR</category>
      <category>econ.TH</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brian Zhu</dc:creator>
    </item>
    <item>
      <title>Intrinsic Geometry and the Stability of Minimum Differentiation</title>
      <link>https://arxiv.org/abs/2507.01985</link>
      <description>arXiv:2507.01985v2 Announce Type: replace-cross 
Abstract: We develop a framework for horizontal differentiation in which firms compete on a product manifold representing the feasible combinations of characteristics. This approach generalizes both the Hotelling line and Salop circle to any Riemannian space, allowing for a unified analysis of product space. We show that equilibrium existence and stability are governed by intrinsic geometric properties, specifically curvature, symmetry and dimension. We prove that negative curvature and high intrinsic dimension act as stabilizers of minimum differentiation equilibria, moving the analysis beyond the symmetry-induced instabilities found in simpler, fixed domains. By characterizing curvature as a measure of consumer heterogeneity, we demonstrate that intrinsic geometry is a fundamental determinant of competitive outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01985v2</guid>
      <category>q-fin.MF</category>
      <category>econ.TH</category>
      <category>math.DG</category>
      <category>math.PR</category>
      <category>q-fin.GN</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aldric Labarthe, Yann Kerzreho</dc:creator>
    </item>
    <item>
      <title>Explainable Information Design</title>
      <link>https://arxiv.org/abs/2508.14196</link>
      <description>arXiv:2508.14196v3 Announce Type: replace-cross 
Abstract: Optimal signaling schemes in information design (Bayesian persuasion) often involve randomization or disconnected partitions of state space, which might be too intricate to be audited or communicated. We propose explainable information design in the context of linear information design with a continuous state space. In the case of single-dimensional state, we restrict the information designer to use $K$-partitional signaling schemes defined by deterministic and monotone partitions of the state space, where a unique signal is sent for all states in each part. We prove that the price of explainability (PoE) -- the ratio between the performances of the optimal explainable signaling scheme and unrestricted signaling scheme -- is exactly $1/2$ in the worst case, meaning that partitional signaling schemes are never worse than arbitrary signaling schemes by a factor of $2$. For a uniform prior, this PoE can be improved to a tight $2/3$. We then extend the analysis to multi-dimensional state spaces by studying two natural explainability notions: convex-partitional policies and axis-aligned rectangular policies. For convex-partitional policies, we prove a tight PoE of $1/(m+1)$, while for rectangular policies we establish a PoE guarantee under uniform prior that is independent of $K$ but unavoidably exponential in $m$. On the computational side, we prove that the exact optimization of explainable policy is NP-hard in general, but provide efficient approximation methods, including an FPTAS for Lipschitz utility functions and a polynomial-time algorithm that achieves the worst-case $1/2$ benchmark for the broad class of discontinuous, piecewise Lipschitz, utility functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14196v3</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>econ.TH</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiling Chen, Tao Lin, Wei Tang, Jamie Tucker-Foltz</dc:creator>
    </item>
  </channel>
</rss>
