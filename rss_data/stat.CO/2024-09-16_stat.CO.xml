<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Sep 2024 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Reducing Shape-Graph Complexity with Application to Classification of Retinal Blood Vessels and Neurons</title>
      <link>https://arxiv.org/abs/2409.09168</link>
      <description>arXiv:2409.09168v1 Announce Type: new 
Abstract: Shape graphs are complex geometrical structures commonly found in biological and anatomical systems. A shape graph is a collection of nodes, some connected by curvilinear edges with arbitrary shapes. Their high complexity stems from the large number of nodes and edges and the complex shapes of edges. With an eye for statistical analysis, one seeks low-complexity representations that retain as much of the global structures of the original shape graphs as possible. This paper develops a framework for reducing graph complexity using hierarchical clustering procedures that replace groups of nodes and edges with their simpler representatives. It demonstrates this framework using graphs of retinal blood vessels in two dimensions and neurons in three dimensions. The paper also presents experiments on classifications of shape graphs using progressively reduced levels of graph complexity. The accuracy of disease detection in retinal blood vessels drops quickly when the complexity is reduced, with accuracy loss particularly associated with discarding terminal edges. Accuracy in identifying neural cell types remains stable with complexity reduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09168v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Beaudett, Anuj Srivastava</dc:creator>
    </item>
    <item>
      <title>Replicating The Log of Gravity</title>
      <link>https://arxiv.org/abs/2409.09066</link>
      <description>arXiv:2409.09066v1 Announce Type: cross 
Abstract: This document replicates the main results from Santos Silva and Tenreyro (2006 in R. The original results were obtained in TSP back in 2006. The idea here is to be explicit regarding the conceptual approach to regression in R. For most of the replication I used base R without external libraries except when it was absolutely necessary. The findings are consistent with the original article and reveal that the replication effort is minimal, without the need to contact the authors for clarifications or incur into data transformations or filtering not mentioned in the article.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09066v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mauricio Vargas Sep\'ulveda</dc:creator>
    </item>
    <item>
      <title>Identification of distributions for risks based on the first moment and c-statistic</title>
      <link>https://arxiv.org/abs/2409.09178</link>
      <description>arXiv:2409.09178v1 Announce Type: cross 
Abstract: We show that for any family of distributions with support on [0,1] with strictly monotonic cumulative distribution function (CDF) that has no jumps and is quantile-identifiable (i.e., any two distinct quantiles identify the distribution), knowing the first moment and c-statistic is enough to identify the distribution. The derivations motivate numerical algorithms for mapping a given pair of expected value and c-statistic to the parameters of specified two-parameter distributions for probabilities. We implemented these algorithms in R and in a simulation study evaluated their numerical accuracy for common families of distributions for risks (beta, logit-normal, and probit-normal). An area of application for these developments is in risk prediction modeling (e.g., sample size calculations and Value of Information analysis), where one might need to estimate the parameters of the distribution of predicted risks from the reported summary statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09178v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Sadatsafavi, Tae Yoon Lee, John Petkau</dc:creator>
    </item>
    <item>
      <title>Topological Tensor Eigenvalue Theorems in Data Fusion</title>
      <link>https://arxiv.org/abs/2409.09392</link>
      <description>arXiv:2409.09392v1 Announce Type: cross 
Abstract: This paper introduces a novel framework for tensor eigenvalue analysis in the context of multi-modal data fusion, leveraging topological invariants such as Betti numbers. While traditional approaches to tensor eigenvalues rely on algebraic extensions of matrix theory, this work provides a topological perspective that enriches the understanding of tensor structures. By establishing new theorems linking eigenvalues to topological features, the proposed framework offers deeper insights into the latent structure of data, enhancing both interpretability and robustness. Applications to data fusion illustrate the theoretical and practical significance of the approach, demonstrating its potential for broad impact across machine learning and data science domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09392v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronald Katende</dc:creator>
    </item>
    <item>
      <title>Foundations of Vision-Based Localization: A New Approach to Localizability Analysis Using Stochastic Geometry</title>
      <link>https://arxiv.org/abs/2409.09525</link>
      <description>arXiv:2409.09525v1 Announce Type: cross 
Abstract: Despite significant algorithmic advances in vision-based positioning, a comprehensive probabilistic framework to study its performance has remained unexplored. The main objective of this paper is to develop such a framework using ideas from stochastic geometry. Due to limitations in sensor resolution, the level of detail in prior information, and computational resources, we may not be able to differentiate between landmarks with similar appearances in the vision data, such as trees, lampposts, and bus stops. While one cannot accurately determine the absolute target position using a single indistinguishable landmark, obtaining an approximate position fix is possible if the target can see multiple landmarks whose geometric placement on the map is unique. Modeling the locations of these indistinguishable landmarks as a Poisson point process (PPP) $\Phi$ on $\mathbb{R}^2$, we develop a new approach to analyze the localizability in this setting. From the target location $\mathbb{x}$, the measurements are obtained from landmarks within the visibility region. These measurements, including ranges and angles to the landmarks, denoted as $f(\mathbb{x})$, can be treated as mappings from the target location. We are interested in understanding the probability that the measurements $f(\mathbb{x})$ are sufficiently distinct from the measurement $f(\mathbb{x}_0)$ at the given location, which we term localizability. Expressions of localizability probability are derived for specific vision-inspired measurements, such as ranges to landmarks and snapshots of their locations. Our analysis reveals that the localizability probability approaches one when the landmark intensity tends to infinity, which means that error-free localization is achievable in this limiting regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09525v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haozhou Hu, Harpreet S. Dhillon, R. Michael Buehrer</dc:creator>
    </item>
    <item>
      <title>HJ-sampler: A Bayesian sampler for inverse problems of a stochastic process by leveraging Hamilton-Jacobi PDEs and score-based generative models</title>
      <link>https://arxiv.org/abs/2409.09614</link>
      <description>arXiv:2409.09614v1 Announce Type: cross 
Abstract: The interplay between stochastic processes and optimal control has been extensively explored in the literature. With the recent surge in the use of diffusion models, stochastic processes have increasingly been applied to sample generation. This paper builds on the log transform, known as the Cole-Hopf transform in Brownian motion contexts, and extends it within a more abstract framework that includes a linear operator. Within this framework, we found that the well-known relationship between the Cole-Hopf transform and optimal transport is a particular instance where the linear operator acts as the infinitesimal generator of a stochastic process. We also introduce a novel scenario where the linear operator is the adjoint of the generator, linking to Bayesian inference under specific initial and terminal conditions. Leveraging this theoretical foundation, we develop a new algorithm, named the HJ-sampler, for Bayesian inference for the inverse problem of a stochastic differential equation with given terminal observations. The HJ-sampler involves two stages: (1) solving the viscous Hamilton-Jacobi partial differential equations, and (2) sampling from the associated stochastic optimal control problem. Our proposed algorithm naturally allows for flexibility in selecting the numerical solver for viscous HJ PDEs. We introduce two variants of the solver: the Riccati-HJ-sampler, based on the Riccati method, and the SGM-HJ-sampler, which utilizes diffusion models. We demonstrate the effectiveness and flexibility of the proposed methods by applying them to solve Bayesian inverse problems involving various stochastic processes and prior distributions, including applications that address model misspecifications and quantifying model uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09614v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tingwei Meng, Zongren Zou, J\'er\^ome Darbon, George Em Karniadakis</dc:creator>
    </item>
    <item>
      <title>RandALO: Out-of-sample risk estimation in no time flat</title>
      <link>https://arxiv.org/abs/2409.09781</link>
      <description>arXiv:2409.09781v1 Announce Type: cross 
Abstract: Estimating out-of-sample risk for models trained on large high-dimensional datasets is an expensive but essential part of the machine learning process, enabling practitioners to optimally tune hyperparameters. Cross-validation (CV) serves as the de facto standard for risk estimation but poorly trades off high bias ($K$-fold CV) for computational cost (leave-one-out CV). We propose a randomized approximate leave-one-out (RandALO) risk estimator that is not only a consistent estimator of risk in high dimensions but also less computationally expensive than $K$-fold CV. We support our claims with extensive simulations on synthetic and real data and provide a user-friendly Python package implementing RandALO available on PyPI as randalo and at https://github.com/cvxgrp/randalo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09781v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Parth T. Nobel, Daniel LeJeune, Emmanuel J. Cand\`es</dc:creator>
    </item>
    <item>
      <title>BEnDEM:A Boltzmann Sampler Based on Bootstrapped Denoising Energy Matching</title>
      <link>https://arxiv.org/abs/2409.09787</link>
      <description>arXiv:2409.09787v1 Announce Type: cross 
Abstract: Developing an efficient sampler capable of generating independent and identically distributed (IID) samples from a Boltzmann distribution is a crucial challenge in scientific research, e.g. molecular dynamics. In this work, we intend to learn neural samplers given energy functions instead of data sampled from the Boltzmann distribution. By learning the energies of the noised data, we propose a diffusion-based sampler, ENERGY-BASED DENOISING ENERGY MATCHING, which theoretically has lower variance and more complexity compared to related works. Furthermore, a novel bootstrapping technique is applied to EnDEM to balance between bias and variance. We evaluate EnDEM and BEnDEM on a 2-dimensional 40 Gaussian Mixture Model (GMM) and a 4-particle double-welling potential (DW-4). The experimental results demonstrate that BEnDEM can achieve state-of-the-art performance while being more robust.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09787v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>RuiKang OuYang, Bo Qiang, Jos\'e Miguel Hern\'andez-Lobato</dc:creator>
    </item>
    <item>
      <title>A General Equilibrium Study of Venture Capitalists' Effort on Entrepreneurship</title>
      <link>https://arxiv.org/abs/2409.09960</link>
      <description>arXiv:2409.09960v1 Announce Type: cross 
Abstract: In this paper, I propose a new general equilibrium model that explains stylized facts about venture capitalists' impact on their portfolio firms. Venture capitalists can help increase firms' productivity, yet they face increasing entry costs to enter. I characterize steady state effort choice, entry threshold, and mass of venture capitalists, and show how they are affected by change in upfront investment, interest rate, and entry costs. The key contribution is that public policy to stimulate startups by subsidizing upfront investments or reducing interest cost have limited success if not accompanied by an increasing supply of experts who can improve business ideas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09960v1</guid>
      <category>econ.TH</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liukun Wu</dc:creator>
    </item>
    <item>
      <title>bayesCureRateModel: Bayesian Cure Rate Modeling for Time to Event Data in R</title>
      <link>https://arxiv.org/abs/2409.10221</link>
      <description>arXiv:2409.10221v1 Announce Type: cross 
Abstract: The family of cure models provides a unique opportunity to simultaneously model both the proportion of cured subjects (those not facing the event of interest) and the distribution function of time-to-event for susceptibles (those facing the event). In practice, the application of cure models is mainly facilitated by the availability of various R packages. However, most of these packages primarily focus on the mixture or promotion time cure rate model. This article presents a fully Bayesian approach implemented in R to estimate a general family of cure rate models in the presence of covariates. It builds upon the work by Papastamoulis and Milienos (2024) by additionally considering various options for describing the promotion time, including the Weibull, exponential, Gompertz, log-logistic and finite mixtures of gamma distributions, among others. Moreover, the user can choose any proper distribution function for modeling the promotion time (provided that some specific conditions are met). Posterior inference is carried out by constructing a Metropolis-coupled Markov chain Monte Carlo (MCMC) sampler, which combines Gibbs sampling for the latent cure indicators and Metropolis-Hastings steps with Langevin diffusion dynamics for parameter updates. The main MCMC algorithm is embedded within a parallel tempering scheme by considering heated versions of the target posterior distribution. The package is illustrated on a real dataset analyzing the duration of the first marriage under the presence of various covariates such as the race, age and the presence of kids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10221v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Panagiotis Papastamoulis, Fotios Milienos</dc:creator>
    </item>
    <item>
      <title>Nonlinear Causality in Brain Networks: With Application to Motor Imagery vs Execution</title>
      <link>https://arxiv.org/abs/2409.10374</link>
      <description>arXiv:2409.10374v1 Announce Type: cross 
Abstract: One fundamental challenge of data-driven analysis in neuroscience is modeling causal interactions and exploring the connectivity of nodes in a brain network. Various statistical methods, relying on various perspectives and employing different data modalities, are being developed to examine and comprehend the underlying causal structures inherent to brain dynamics. This study introduces a novel statistical approach, TAR4C, to dissect causal interactions in multichannel EEG recordings. TAR4C uses the threshold autoregressive model to describe the causal interaction between nodes or clusters of nodes in a brain network. The perspective involves testing whether one node, which may represent a brain region, can control the dynamics of the other. The node that has such an impact on the other is called a threshold variable and can be classified as a causative because its functionality is the leading source operating as an instantaneous switching mechanism that regulates the time-varying autoregressive structure of the other. This statistical concept is commonly referred to as threshold non-linearity. Once threshold non-linearity has been verified between a pair of nodes, the subsequent essential facet of TAR modeling is to assess the predictive ability of the causal node for the current activity on the other and represent causal interactions in autoregressive terms. This predictive ability is what underlies Granger causality. The TAR4C approach can discover non-linear and time-dependent causal interactions without negating the G-causality perspective. The efficacy of the proposed approach is exemplified by analyzing the EEG signals recorded during the motor movement/imagery experiment. The similarities and differences between the causal interactions manifesting during the execution and the imagery of a given motor movement are demonstrated by analyzing EEG recordings from multiple subjects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10374v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sipan Aslan, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Kolmogorov-Arnold Networks in Low-Data Regimes: A Comparative Study with Multilayer Perceptrons</title>
      <link>https://arxiv.org/abs/2409.10463</link>
      <description>arXiv:2409.10463v1 Announce Type: cross 
Abstract: Multilayer Perceptrons (MLPs) have long been a cornerstone in deep learning, known for their capacity to model complex relationships. Recently, Kolmogorov-Arnold Networks (KANs) have emerged as a compelling alternative, utilizing highly flexible learnable activation functions directly on network edges, a departure from the neuron-centric approach of MLPs. However, KANs significantly increase the number of learnable parameters, raising concerns about their effectiveness in data-scarce environments. This paper presents a comprehensive comparative study of MLPs and KANs from both algorithmic and experimental perspectives, with a focus on low-data regimes. We introduce an effective technique for designing MLPs with unique, parameterized activation functions for each neuron, enabling a more balanced comparison with KANs. Using empirical evaluations on simulated data and two real-world data sets from medicine and engineering, we explore the trade-offs between model complexity and accuracy, with particular attention to the role of network depth. Our findings show that MLPs with individualized activation functions achieve significantly higher predictive accuracy with only a modest increase in parameters, especially when the sample size is limited to around one hundred. For example, in a three-class classification problem within additive manufacturing, MLPs achieve a median accuracy of 0.91, significantly outperforming KANs, which only reach a median accuracy of 0.53 with default hyperparameters. These results offer valuable insights into the impact of activation function selection in neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10463v1</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Farhad Pourkamali-Anaraki</dc:creator>
    </item>
    <item>
      <title>Core-Elements for Large-Scale Least Squares Estimation</title>
      <link>https://arxiv.org/abs/2206.10240</link>
      <description>arXiv:2206.10240v4 Announce Type: replace 
Abstract: The coresets approach, also called subsampling or subset selection, aims to select a subsample as a surrogate for the observed sample and has found extensive applications in large-scale data analysis. Existing coresets methods construct the subsample using a subset of rows from the predictor matrix. Such methods can be significantly inefficient when the predictor matrix is sparse or numerically sparse. To overcome this limitation, we develop a novel element-wise subset selection approach, called core-elements, for large-scale least squares estimation. We provide a deterministic algorithm to construct the core-elements estimator, only requiring an $O(\mathrm{nnz}(X)+rp^2)$ computational cost, where $X$ is an $n\times p$ predictor matrix, $r$ is the number of elements selected from each column of $X$, and $\mathrm{nnz}(\cdot)$ denotes the number of non-zero elements. Theoretically, we show that the proposed estimator is unbiased and approximately minimizes an upper bound of the estimation variance. We also provide an approximation guarantee by deriving a coresets-like finite sample bound for the proposed estimator. To handle potential outliers in the data, we further combine core-elements with the median-of-means procedure, resulting in an efficient and robust estimator with theoretical consistency guarantees. Numerical studies on various synthetic and real-world datasets demonstrate the proposed method's superior performance compared to mainstream competitors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.10240v4</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengyu Li, Jun Yu, Tao Li, Cheng Meng</dc:creator>
    </item>
    <item>
      <title>Normalizing Basis Functions: Approximate Stationary Models for Large Spatial Data</title>
      <link>https://arxiv.org/abs/2405.13821</link>
      <description>arXiv:2405.13821v3 Announce Type: replace 
Abstract: In geostatistics, traditional spatial models often rely on the Gaussian Process (GP) to fit stationary covariances to data. It is well known that this approach becomes computationally infeasible when dealing with large data volumes, necessitating the use of approximate methods. A powerful class of methods approximate the GP as a sum of basis functions with random coefficients. Although this technique offers computational efficiency, it does not inherently guarantee a stationary covariance. To mitigate this issue, the basis functions can be "normalized" to maintain a constant marginal variance, avoiding unwanted artifacts and edge effects. This allows for the fitting of nearly stationary models to large, potentially non-stationary datasets, providing a rigorous base to extend to more complex problems. Unfortunately, the process of normalizing these basis functions is computationally demanding. To address this, we introduce two fast and accurate algorithms to the normalization step, allowing for efficient prediction on fine grids. The practical value of these algorithms is showcased in the context of a spatial analysis on a large dataset, where significant computational speedups are achieved. While implementation and testing are done specifically within the LatticeKrig framework, these algorithms can be adapted to other basis function methods operating on regular grids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13821v3</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antony Sikorski, Daniel McKenzie, Douglas Nychka</dc:creator>
    </item>
    <item>
      <title>Mathematical modelling, selection and hierarchical inference to determine the minimal dose in IFN$\alpha$ therapy against Myeloproliferative Neoplasms</title>
      <link>https://arxiv.org/abs/2112.10688</link>
      <description>arXiv:2112.10688v2 Announce Type: replace-cross 
Abstract: Myeloproliferative Neoplasms (MPN) are blood cancers that appear after acquiring a driver mutation in a hematopoietic stem cell. These hematological malignancies result in the overproduction of mature blood cells and, if not treated, induce a risk of cardiovascular events and thrombosis. Pegylated IFN$\alpha$ is commonly used to treat MPN, but no clear guidelines exist concerning the dose prescribed to patients. We applied a model selection procedure and ran a hierarchical Bayesian inference method to decipher how dose variations impact the response to the therapy. We inferred that IFN$\alpha$ acts on mutated stem cells by inducing their differentiation into progenitor cells; the higher the dose, the higher the effect. We found that the treatment can induce long-term remission when a sufficient (patient-dependent) dose is reached. We determined this minimal dose for individuals in a cohort of patients and estimated the most suitable starting dose to give to a new patient to increase the chances of being cured.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.10688v2</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/imammb/dqae006</arxiv:DOI>
      <arxiv:journal_reference>Mathematical Medicine and Biology: A Journal of the IMA, Volume 41, Issue 2, June 2024, Pages 110-134</arxiv:journal_reference>
      <dc:creator>Gurvan Hermange, William Vainchenker, Isabelle Plo, Paul-Henry Courn\`ede</dc:creator>
    </item>
    <item>
      <title>Energy based diffusion generator for efficient sampling of Boltzmann distributions</title>
      <link>https://arxiv.org/abs/2401.02080</link>
      <description>arXiv:2401.02080v2 Announce Type: replace-cross 
Abstract: Sampling from Boltzmann distributions, particularly those tied to high-dimensional and complex energy functions, poses a significant challenge in many fields. In this work, we present the Energy-Based Diffusion Generator (EDG), a novel approach that integrates ideas from variational autoencoders and diffusion models. EDG leverages a decoder to transform latent variables from a simple distribution into samples approximating the target Boltzmann distribution, while the diffusion-based encoder provides an accurate estimate of the Kullback-Leibler divergence during training. Notably, EDG is simulation-free, eliminating the need to solve ordinary or stochastic differential equations during training. Furthermore, by removing constraints such as bijectivity in the decoder, EDG allows for flexible network design. Through empirical evaluation, we demonstrate the superior performance of EDG across a variety of complex distribution tasks, outperforming existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.02080v2</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Wang, Ling Guo, Hao Wu, Tao Zhou</dc:creator>
    </item>
  </channel>
</rss>
