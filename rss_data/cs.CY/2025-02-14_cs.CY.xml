<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Feb 2025 05:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Scientific Map of Artificial Intelligence in Communication (2004-2024)</title>
      <link>https://arxiv.org/abs/2502.08648</link>
      <description>arXiv:2502.08648v1 Announce Type: new 
Abstract: Introduction: Artificial Intelligence (AI) is having a significant impact in the field of communication, causing transcendental changes in the processing and consumption of information. The objective of this work was to analyze the most influential AI topic areas in the field of communication based on scientific literature. Methodology: 996 references indexed in Web of Science between 2004-2024 were selected, a bibliometric analysis of co-words was carried out and visualization techniques were applied to build scientific maps. Results: The most relevant thematic areas were datafication, the linking of AI with social media and digital journalism. The emerging area of generative AI was identified, linked to new AI models, such as ChatGPT, designed to generate content in the form of written text, audio, images or videos. Another emerging topic area was China's impact on the use of AI in communication. Discussions: Despite the impact of AI in communication, the field is still in the process of structuring, with few consolidated topics. Conclusions: This study made it possible to identify the thematic areas of the field studied, as well as the detection of emerging trends.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08648v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.31637/epsir-2024-947</arxiv:DOI>
      <arxiv:journal_reference>European Public &amp; Social Innovation Review, 9, 1-17, 2024</arxiv:journal_reference>
      <dc:creator>Carmen G\'alvez</dc:creator>
    </item>
    <item>
      <title>Who is Responsible? The Data, Models, Users or Regulations? Responsible Generative AI for a Sustainable Future</title>
      <link>https://arxiv.org/abs/2502.08650</link>
      <description>arXiv:2502.08650v1 Announce Type: new 
Abstract: Responsible Artificial Intelligence (RAI) has emerged as a crucial framework for addressing ethical concerns in the development and deployment of Artificial Intelligence (AI) systems. A significant body of literature exists, primarily focusing on either RAI guidelines and principles or the technical aspects of RAI, largely within the realm of traditional AI. However, a notable gap persists in bridging theoretical frameworks with practical implementations in real-world settings, as well as transitioning from RAI to Responsible Generative AI (Gen AI). To bridge this gap, we present this article, which examines the challenges and opportunities in implementing ethical, transparent, and accountable AI systems in the post-ChatGPT era, an era significantly shaped by Gen AI. Our analysis includes governance and technical frameworks, the exploration of explainable AI as the backbone to achieve RAI, key performance indicators in RAI, alignment of Gen AI benchmarks with governance frameworks, reviews of AI-ready test beds, and RAI applications across multiple sectors. Additionally, we discuss challenges in RAI implementation and provide a philosophical perspective on the future of RAI. This comprehensive article aims to offer an overview of RAI, providing valuable insights for researchers, policymakers, users, and industry practitioners to develop and deploy AI systems that benefit individuals and society while minimizing potential risks and societal impacts. A curated list of resources and datasets covered in this survey is available on GitHub {https://github.com/anas-zafar/Responsible-AI}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08650v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaina Raza, Rizwan Qureshi, Anam Zahid, Joseph Fioresi, Ferhat Sadak, Muhammad Saeed, Ranjan Sapkota, Aditya Jain, Anas Zafar, Muneeb Ul Hassan, Aizan Zafar, Hasan Maqbool, Jia Wu, Maged Shoman</dc:creator>
    </item>
    <item>
      <title>Democratizing AI Governance: Balancing Expertise and Public Participation</title>
      <link>https://arxiv.org/abs/2502.08651</link>
      <description>arXiv:2502.08651v1 Announce Type: new 
Abstract: The development and deployment of artificial intelligence (AI) systems, with their profound societal impacts, raise critical challenges for governance. Historically, technological innovations have been governed by concentrated expertise with limited public input. However, AI's pervasive influence across domains such as healthcare, employment, and justice necessitates inclusive governance approaches. This article explores the tension between expert-led oversight and democratic participation, analyzing models of participatory and deliberative democracy. Using case studies from France and Brazil, we highlight how inclusive frameworks can bridge the gap between technical complexity and public accountability. Recommendations are provided for integrating these approaches into a balanced governance model tailored to the European Union, emphasizing transparency, diversity, and adaptive regulation to ensure that AI governance reflects societal values while maintaining technical rigor. This analysis underscores the importance of hybrid frameworks that unite expertise and public voice in shaping the future of AI policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08651v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lucile Ter-Minassian</dc:creator>
    </item>
    <item>
      <title>LegalScore: Development of a Benchmark for Evaluating AI Models in Legal Career Exams in Brazil</title>
      <link>https://arxiv.org/abs/2502.08652</link>
      <description>arXiv:2502.08652v1 Announce Type: new 
Abstract: This research introduces LegalScore, a specialized index for assessing how generative artificial intelligence models perform in a selected range of career exams that require a legal background in Brazil. The index evaluates fourteen different types of artificial intelligence models' performance, from proprietary to open-source models, in answering objective questions applied to these exams. The research uncovers the response of the models when applying English-trained large language models to Brazilian legal contexts, leading us to reflect on the importance and the need for Brazil-specific training data in generative artificial intelligence models. Performance analysis shows that while proprietary and most known models achieved better results overall, local and smaller models indicated promising performances due to their Brazilian context alignment in training. By establishing an evaluation framework with metrics including accuracy, confidence intervals, and normalized scoring, LegalScore enables systematic assessment of artificial intelligence performance in legal examinations in Brazil. While the study demonstrates artificial intelligence's potential value for exam preparation and question development, it concludes that significant improvements are needed before AI can match human performance in advanced legal assessments. The benchmark creates a foundation for continued research, highlighting the importance of local adaptation in artificial intelligence development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08652v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberto Caparroz, Marcelo Roitman, Beatriz G. Chow, Caroline Giusti, Larissa Torhacs, Pedro A. Sola, Jo\~ao H. M. Diogo, Luiza Balby, Carolina D. L. Vasconcelos, Leonardo R. Caparroz, Albano P. Franco</dc:creator>
    </item>
    <item>
      <title>Beyond the Lens: Quantifying the Impact of Scientific Documentaries through Amazon Reviews</title>
      <link>https://arxiv.org/abs/2502.08705</link>
      <description>arXiv:2502.08705v1 Announce Type: new 
Abstract: Engaging the public with science is critical for a well-informed population. A popular method of scientific communication is documentaries. Once released, it can be difficult to assess the impact of such works on a large scale, due to the overhead required for in-depth audience feedback studies. In what follows, we overview our complementary approach to qualitative studies through quantitative impact and sentiment analysis of Amazon reviews for several scientific documentaries. In addition to developing a novel impact category taxonomy for this analysis, we release a dataset containing 1296 human-annotated sentences from 1043 Amazon reviews for six movies created in whole or part by a team of visualization designers who focus on cinematic presentations of scientific data. Using this data, we train and evaluate several machine learning and large language models, discussing their effectiveness and possible generalizability for documentaries beyond those focused on for this work. Themes are also extracted from our annotated dataset which, along with our large language model analysis, demonstrate a measure of the ability of scientific documentaries to engage with the public.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08705v1</guid>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>physics.ed-ph</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jill Naiman, Aria Pessianzadeh, Hanyu Zhao, AJ Christensen, Alistair Nunn, Shriya Srikanth, Anushka Gami, Emma Maxwell, Louisa Zhang, Sri Nithya Yeragorla, Rezvaneh Rezapour</dc:creator>
    </item>
    <item>
      <title>Unlocking Mental Health: Exploring College Students' Well-being through Smartphone Behaviors</title>
      <link>https://arxiv.org/abs/2502.08766</link>
      <description>arXiv:2502.08766v1 Announce Type: new 
Abstract: The global mental health crisis is a pressing concern, with college students particularly vulnerable to rising mental health disorders. The widespread use of smartphones among young adults, while offering numerous benefits, has also been linked to negative outcomes such as addiction and regret, significantly impacting well-being. Leveraging the longest longitudinal dataset collected over four college years through passive mobile sensing, this study is the first to examine the relationship between students' smartphone unlocking behaviors and their mental health at scale in real-world settings. We provide the first evidence demonstrating the predictability of phone unlocking behaviors for mental health outcomes based on a large dataset, highlighting the potential of these novel features for future predictive models. Our findings reveal important variations in smartphone usage across genders and locations, offering a deeper understanding of the interplay between digital behaviors and mental health. We highlight future research directions aimed at mitigating adverse effects and promoting digital well-being in this population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08766v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Xuan, Meghna Roy Chowdhury, Yi Ding, Yixue Zhao</dc:creator>
    </item>
    <item>
      <title>Europe's AI Imperative -- A Pragmatic Blueprint for Global Tech Leadership</title>
      <link>https://arxiv.org/abs/2502.08781</link>
      <description>arXiv:2502.08781v1 Announce Type: new 
Abstract: Europe is at a make-or-break moment in the global AI race, squeezed between the massive venture capital and tech giants in the US and China's scale-oriented, top-down drive. At this tipping point, where the convergence of AI with complementary and synergistic technologies, like quantum computing, biotech, VR/AR, 5G/6G, robotics, advanced materials, and high-performance computing, could upend geopolitical balances, Europe needs to rethink its AI-related strategy. On the heels of the AI Action Summit 2025 in Paris, we present a sharp, doable strategy that builds upon Europe's strengths and closes gaps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08781v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gjergji Kasneci, Urs Gasser, Thomas F. Hofmann, Gerhard Kramer, Gerhard M\"uller, Claudia Peus, Helmut Sch\"onenberger, Enkelejda Kasneci</dc:creator>
    </item>
    <item>
      <title>Dynamic Incentive Allocation for City-scale Deep Decarbonization</title>
      <link>https://arxiv.org/abs/2502.08877</link>
      <description>arXiv:2502.08877v1 Announce Type: new 
Abstract: Greenhouse gas emissions from the residential sector represent a significant fraction of global emissions. Governments and utilities have designed incentives to stimulate the adoption of decarbonization technologies such as rooftop PV and heat pumps. However, studies have shown that many of these incentives are inefficient since a substantial fraction of spending does not actually promote adoption, and incentives are not equitably distributed across socioeconomic groups. We present a novel data-driven approach that adopts a holistic, emissions-based and city-scale perspective on decarbonization. We propose an optimization model that dynamically allocates a total incentive budget to households to directly maximize city-wide carbon reduction. We leverage techniques for the multi-armed bandits problem to estimate human factors, such as a household's willingness to adopt new technologies given a certain incentive. We apply our proposed framework to a city in the Northeast U.S., using real household energy data, grid carbon intensity data, and future price scenarios. We show that our learning-based technique significantly outperforms an example status quo incentive scheme, achieving up to 32.23% higher carbon reductions. We show that our framework can accommodate equity-aware constraints to equitably allocate incentives across socioeconomic groups, achieving 78.84% of the carbon reductions of the optimal solution on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08877v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anupama Sitaraman, Adam Lechowicz, Noman Bashir, Xutong Liu, Mohammad Hajiesmaili, Prashant Shenoy</dc:creator>
    </item>
    <item>
      <title>From Occupations to Tasks: A New Perspective on Automatability Prediction Using BERT</title>
      <link>https://arxiv.org/abs/2502.09021</link>
      <description>arXiv:2502.09021v1 Announce Type: new 
Abstract: As automation technologies continue to advance at an unprecedented rate, concerns about job displacement and the future of work have become increasingly prevalent. While existing research has primarily focused on the potential impact of automation at the occupation level, there has been a lack of investigation into the automatability of individual tasks. This paper addresses this gap by proposing a BERT-based classifier to predict the automatability of tasks in the forthcoming decade at a granular level leveraging the context and semantics information of tasks. We leverage three public datasets: O*NET Task Statements, ESCO Skills, and Australian Labour Market Insights Tasks, and perform expert annotation. Our BERT-based classifier, fine-tuned on our task statement data, demonstrates superior performance over traditional machine learning models, neural network architectures, and other transformer models. Our findings also indicate that approximately 25.1% of occupations within the O*NET database are at substantial risk of automation, with a diverse spectrum of automation vulnerability across sectors. This research provides a robust tool for assessing the future impact of automation on the labor market, offering valuable insights for policymakers, workers, and industry leaders in the face of rapid technological advancement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09021v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dawei Xu, Haoran Yang, Marian-Andrei Rizoiu, Guandong Xu</dc:creator>
    </item>
    <item>
      <title>Implementation of a Fuzzy Relational Database. Case Study: Chilean Cardboard Industry in the Maule Region</title>
      <link>https://arxiv.org/abs/2502.09035</link>
      <description>arXiv:2502.09035v1 Announce Type: new 
Abstract: The international database community refers to the manipulation of data with inaccuracy and uncertainty using the term fuzzy, which has been translated into Spanish as "borroso" and into French as "flou". Semantically, this term conveys two main ideas: first, the natural concept of ambiguity or vagueness in human reasoning, and second, its connection to fuzzy set theory, fuzzy logic, and possibility theory, as developed by Zadeh between 1965 and 1977. This article explores two key aspects: the attributes of the fuzzy data model GEFRED (GENeralized model for Fuzzy RElational Database) and their implementation in a Relational Database (RDB). The modeling of these attributes was conducted in a Chilian cardboard manufacturing company located in the Maule Region, where the described phenomena involve imprecise and uncertain attributes and values. Specifically, our focus is on the knowledge related to the manufacturing process of coated cardboard, particularly the quality control process for finished products in the company's Conversion Department. The quality of these products, categorized as either stacks or rolls, is characterized using both classical and fuzzy attributes. Classical attributes are typically measured with physical instruments, whereas fuzzy attributes are assessed through human senses, primarily sight and touch, as perceived by the operators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09035v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.29375/issn.2539-2115</arxiv:DOI>
      <arxiv:journal_reference>Rev. Colomb. de Computaci\'on 6(2): 48-58 (2005)</arxiv:journal_reference>
      <dc:creator>Leoncio Jimenez, Ang\'elica Urrutia, Jos\'e Galindo, Pascale Zarat\'e</dc:creator>
    </item>
    <item>
      <title>WhatsApp as an improvisation of health information systems in Southern African public hospitals: A socio-technical perspective</title>
      <link>https://arxiv.org/abs/2502.09049</link>
      <description>arXiv:2502.09049v1 Announce Type: new 
Abstract: Digital health interventions, particularly electronic referrals (e-referrals) and health information systems, have revolutionised clinical workflows in public hospitals by automating processes. However, the utilization of e-referrals has yielded mixed outcomes, with varying levels of success in organisational processes.This paper explores improvisation of health information systems in Southern African public hospitals from a socio-technical perspective. In particular the paper explains the design-reality gaps giving rise to improvisations of mandated health information systems in order to understand their occurrence and impact on referral outcomes. We employed the design-reality framework and the Process framework for Healthcare Information System Workarounds and Impacts to explain the socio-technical issues related to the phenomenon of interest.We conducted semi-interviews with 31 respondents from health organisations as case studies.Respondents from two public hospitals in South Africa and two in Namibia were interviewed to examine how they devised improvisations to various health information systems in each setting.The findings showed that using WhatsApp or improvising existing health information systems (HIS) improved efficiency and productivity of healthcare practitioners (HCPs) referral activities. Additionally, HCPs reported positive outcomes related to continual professional development in the given settings.The findings further show a relationship between design-reality gaps and improvisations enacted by HCPs.The observed gaps are related to poor management systems and structures lack of HCPs' involvement in the roll-out of HIS and inadequacies of existing HIS to support referral tasks.These study findings can be insightful and useful to system developers and other stakeholders for devising measures to address the gaps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09049v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Meke Kapepoa, Jean-Paul Van Belle, Edda Weimann</dc:creator>
    </item>
    <item>
      <title>AI Safety for Everyone</title>
      <link>https://arxiv.org/abs/2502.09288</link>
      <description>arXiv:2502.09288v1 Announce Type: new 
Abstract: Recent discussions and research in AI safety have increasingly emphasized the deep connection between AI safety and existential risk from advanced AI systems, suggesting that work on AI safety necessarily entails serious consideration of potential existential threats. However, this framing has three potential drawbacks: it may exclude researchers and practitioners who are committed to AI safety but approach the field from different angles; it could lead the public to mistakenly view AI safety as focused solely on existential scenarios rather than addressing a wide spectrum of safety challenges; and it risks creating resistance to safety measures among those who disagree with predictions of existential AI risks. Through a systematic literature review of primarily peer-reviewed research, we find a vast array of concrete safety work that addresses immediate and practical concerns with current AI systems. This includes crucial areas like adversarial robustness and interpretability, highlighting how AI safety research naturally extends existing technological and systems safety concerns and practices. Our findings suggest the need for an epistemically inclusive and pluralistic conception of AI safety that can accommodate the full range of safety considerations, motivations, and perspectives that currently shape the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09288v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Balint Gyevnar, Atoosa Kasirzadeh</dc:creator>
    </item>
    <item>
      <title>Pitfalls of Evidence-Based AI Policy</title>
      <link>https://arxiv.org/abs/2502.09618</link>
      <description>arXiv:2502.09618v1 Announce Type: new 
Abstract: Nations across the world are working to govern AI. However, from a technical perspective, there is uncertainty and disagreement on the best way to do this. Meanwhile, recent debates over AI regulation have led to calls for "evidence-based AI policy" which emphasize holding regulatory action to a high evidentiary standard. Evidence is of irreplaceable value to policymaking. However, holding regulatory action to too high an evidentiary standard can lead to systematic neglect of certain risks. In historical policy debates (e.g., over tobacco ca. 1965 and fossil fuels ca. 1985) "evidence-based policy" rhetoric is also a well-precedented strategy to downplay the urgency of action, delay regulation, and protect industry interests. Here, we argue that if the goal is evidence-based AI policy, the first regulatory objective must be to actively facilitate the process of identifying, studying, and deliberating about AI risks. We discuss a set of 15 regulatory goals to facilitate this and show that Brazil, Canada, China, the EU, South Korea, the UK, and the USA all have substantial opportunities to adopt further evidence-seeking policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09618v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen Casper, David Krueger, Dylan Hadfield-Menell</dc:creator>
    </item>
    <item>
      <title>Principles for Open Data Curation: A Case Study with the New York City 311 Service Request Data</title>
      <link>https://arxiv.org/abs/2502.08649</link>
      <description>arXiv:2502.08649v1 Announce Type: cross 
Abstract: In the early 21st century, the open data movement began to transform societies and governments by promoting transparency, innovation, and public engagement. The City of New York (NYC) has been at the forefront of this movement since the enactment of the Open Data Law in 2012, creating the NYC Open Data portal. The portal currently hosts 2,700 datasets, serving as a crucial resource for research across various domains, including health, urban development, and transportation. However, the effective use of open data relies heavily on data quality and usability, challenges that remain insufficiently addressed in the literature. This paper examines these challenges via a case study of the NYC 311 Service Request dataset, identifying key issues in data validity, consistency, and curation efficiency. We propose a set of data curation principles, tailored for government-released open data, to address these challenges. Our findings highlight the importance of harmonized field definitions, streamlined storage, and automated quality checks, offering practical guidelines for improving the reliability and utility of open datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08649v1</guid>
      <category>cs.DB</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Hussey, Jun Yan</dc:creator>
    </item>
    <item>
      <title>Hallucination Detection: A Probabilistic Framework Using Embeddings Distance Analysis</title>
      <link>https://arxiv.org/abs/2502.08663</link>
      <description>arXiv:2502.08663v1 Announce Type: cross 
Abstract: Hallucinations are one of the major issues affecting LLMs, hindering their wide adoption in production systems. While current research solutions for detecting hallucinations are mainly based on heuristics, in this paper we introduce a mathematically sound methodology to reason about hallucination, and leverage it to build a tool to detect hallucinations. To the best of our knowledge, we are the first to show that hallucinated content has structural differences with respect to correct content. To prove this result, we resort to the Minkowski distances in the embedding space. Our findings demonstrate statistically significant differences in the embedding distance distributions, that are also scale free -- they qualitatively hold regardless of the distance norm used and the number of keywords, questions, or responses. We leverage these structural differences to develop a tool to detect hallucinated responses, achieving an accuracy of 66\% for a specific configuration of system parameters -- comparable with the best results in the field. In conclusion, the suggested methodology is promising and novel, possibly paving the way for further research in the domain, also along the directions highlighted in our future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08663v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emanuele Ricco, Lorenzo Cima, Roberto Di Pietro</dc:creator>
    </item>
    <item>
      <title>A Systematic Review on the Evaluation of Large Language Models in Theory of Mind Tasks</title>
      <link>https://arxiv.org/abs/2502.08796</link>
      <description>arXiv:2502.08796v1 Announce Type: cross 
Abstract: In recent years, evaluating the Theory of Mind (ToM) capabilities of large language models (LLMs) has received significant attention within the research community. As the field rapidly evolves, navigating the diverse approaches and methodologies has become increasingly complex. This systematic review synthesizes current efforts to assess LLMs' ability to perform ToM tasks, an essential aspect of human cognition involving the attribution of mental states to oneself and others. Despite notable advancements, the proficiency of LLMs in ToM remains a contentious issue. By categorizing benchmarks and tasks through a taxonomy rooted in cognitive science, this review critically examines evaluation techniques, prompting strategies, and the inherent limitations of LLMs in replicating human-like mental state reasoning. A recurring theme in the literature reveals that while LLMs demonstrate emerging competence in ToM tasks, significant gaps persist in their emulation of human cognitive abilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08796v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karahan Sar{\i}ta\c{s}, K{\i}van\c{c} Tez\"oren, Yavuz Durmazkeser</dc:creator>
    </item>
    <item>
      <title>Delayed takedown of illegal content on social media makes moderation ineffective</title>
      <link>https://arxiv.org/abs/2502.08841</link>
      <description>arXiv:2502.08841v1 Announce Type: cross 
Abstract: Social media platforms face legal and regulatory demands to swiftly remove illegal content, sometimes under strict takedown deadlines. However, the effects of moderation speed and the impact of takedown deadlines remain underexplored. This study models the relationship between the timeliness of illegal content removal and its prevalence, reach, and exposure on social media. By simulating illegal content diffusion using empirical data from the DSA Transparency Database, we demonstrate that rapid takedown (within hours) significantly reduces illegal content prevalence and exposure, while longer delays decrease the effectiveness of moderation efforts. While these findings support tight takedown deadlines for content removal, such deadlines cannot address the delay in identifying the illegal content and can adversely affect the quality of content moderation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08841v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bao Tran Truong, Sangyeon Kim, Gianluca Nogara, Enrico Verdolotti, Erfan Samieyan Sahneh, Florian Saurwein, Natascha Just, Luca Luceri, Silvia Giordano, Filippo Menczer</dc:creator>
    </item>
    <item>
      <title>Hope vs. Hate: Understanding User Interactions with LGBTQ+ News Content in Mainstream US News Media through the Lens of Hope Speech</title>
      <link>https://arxiv.org/abs/2502.09004</link>
      <description>arXiv:2502.09004v1 Announce Type: cross 
Abstract: This paper makes three contributions. First, via a substantial corpus of 1,419,047 comments posted on 3,161 YouTube news videos of major US cable news outlets, we analyze how users engage with LGBTQ+ news content. Our analyses focus both on positive and negative content. In particular, we construct a fine-grained hope speech classifier that detects positive (hope speech), negative, neutral, and irrelevant content. Second, in consultation with a public health expert specializing on LGBTQ+ health, we conduct an annotation study with a balanced and diverse political representation and release a dataset of 3,750 instances with fine-grained labels and detailed annotator demographic information. Finally, beyond providing a vital resource for the LGBTQ+ community, our annotation study and subsequent in-the-wild assessments reveal (1) strong association between rater political beliefs and how they rate content relevant to a marginalized community; (2) models trained on individual political beliefs exhibit considerable in-the-wild disagreement; and (3) zero-shot large language models (LLMs) align more with liberal raters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09004v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Pofcher, Christopher M. Homan, Randall Sell, Ashiqur R. KhudaBukhsh</dc:creator>
    </item>
    <item>
      <title>Anchor Sponsor Firms in Open Source Software Ecosystems</title>
      <link>https://arxiv.org/abs/2502.09060</link>
      <description>arXiv:2502.09060v1 Announce Type: cross 
Abstract: Firms are intensifying their involvement with open source software (OSS), going beyond contributing to individual projects and releasing their own core technologies as OSS. These technologies, from web frameworks to programming languages, are the foundations of large and growing ecosystems. Yet we know little about how these anchor sponsors shape the behavior of OSS contributors. We examine Mozilla Corporation's role as incubator and anchor sponsor in the Rust programming language ecosystem, leveraging data on nearly 30,000 developers and 40,000 OSS projects from 2015 to 2022. When Mozilla abruptly exited Rust in August 2020, event-study models estimate a negative impact on ecosystem activity: a 9\% immediate drop in weekly commits and a 0.6 percentage point decline in trend. We observe an asymmetry in the shock's effects: former Mozilla developers and close collaborators continued contributing relatively quickly, whereas more distant developers showed reduced or ceased activity even six months later. An agent-based model of an OSS ecosystem with an anchor sponsor replicates these patterns. We also find a marked slowdown in new developers and projects entering Rust post-shock. Our results suggest that Mozilla served as a critical signal of Rust's quality and stability. Once withdrawn, newcomers and less-embedded developers were the most discouraged, raising concerns about long-term ecosystem sustainability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09060v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brigitta N\'emeth, Johannes Wachs</dc:creator>
    </item>
    <item>
      <title>Citizen Science Games on the Timeline of Quantum Games</title>
      <link>https://arxiv.org/abs/2502.09169</link>
      <description>arXiv:2502.09169v1 Announce Type: cross 
Abstract: This article provides an overview of existing quantum physics-related games, referred to as \textit{quantum games}, that serve citizen science research in quantum physics. Additionally, we explore the connection between citizen science and \textit{quantum computer games}, games played on quantum computers. The information presented is derived from academic references and supplemented by diverse sources, including social media publications, conference presentations, and blog posts from research groups and developers associated with the presented games. We observe that the current landscape of quantum games is shaped by three distinct driving forces: the serious application of games, the evolution of quantum computers, and open game development events such as \textit{Quantum Game Jams}. Notably, citizen science plays an influential role in all three aspects. The article points to existing design guides for citizen science quantum games and views future prospects of citizen science projects and quantum games through collaborative endeavours, human-machine collaboration, and open access quantum computers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09169v1</guid>
      <category>quant-ph</category>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1140/epjp/s13360-024-05553-w</arxiv:DOI>
      <arxiv:journal_reference>European Physical Journal Plus 139 (2024) 753</arxiv:journal_reference>
      <dc:creator>Laura Piispanen</dc:creator>
    </item>
    <item>
      <title>EmoAssist: Emotional Assistant for Visual Impairment Community</title>
      <link>https://arxiv.org/abs/2502.09285</link>
      <description>arXiv:2502.09285v1 Announce Type: cross 
Abstract: The rapid advancement of large multi-modality models (LMMs) has significantly propelled the integration of artificial intelligence into practical applications. Visual Question Answering (VQA) systems, which can process multi-modal data including vision, text, and audio, hold great potential for assisting the Visual Impairment (VI) community in navigating complex and dynamic real-world environments. However, existing VI assistive LMMs overlook the emotional needs of VI individuals, and current benchmarks lack emotional evaluation of these LMMs. To address these gaps, this paper introduces the EmoAssist Benchmark, a comprehensive benchmark designed to evaluate the assistive performance of LMMs for the VI community. To the best of our knowledge, this is the first benchmark that incorporates emotional intelligence as a key consideration. Furthermore, we propose the EmoAssist Model, an Emotion-Assistive LMM specifically designed for the VI community. The EmoAssist Model utilizes Direct Preference Optimization (DPO) to align outputs with human emotional preferences. Experiment results demonstrate that the EmoAssist Model significantly enhances the recognition of implicit emotions and intentions of VI users, delivers empathetic responses, and provides actionable guidance. Specifically, it shows respective improvements of 147.8% and 89.7% in the Empathy and Suggestion metrics on the EmoAssist Benchmark, compared to the pre-tuning LMM, and even outperforms state-of-the-art LLMs such as GPT-4o.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09285v1</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingyu Qi, He Li, Linjie Li, Zhenyu Wu</dc:creator>
    </item>
    <item>
      <title>Language Agents as Digital Representatives in Collective Decision-Making</title>
      <link>https://arxiv.org/abs/2502.09369</link>
      <description>arXiv:2502.09369v1 Announce Type: cross 
Abstract: Consider the process of collective decision-making, in which a group of individuals interactively select a preferred outcome from among a universe of alternatives. In this context, "representation" is the activity of making an individual's preferences present in the process via participation by a proxy agent -- i.e. their "representative". To this end, learned models of human behavior have the potential to fill this role, with practical implications for multi-agent scenario studies and mechanism design. In this work, we investigate the possibility of training \textit{language agents} to behave in the capacity of representatives of human agents, appropriately expressing the preferences of those individuals whom they stand for. First, we formalize the setting of \textit{collective decision-making} -- as the episodic process of interaction between a group of agents and a decision mechanism. On this basis, we then formalize the problem of \textit{digital representation} -- as the simulation of an agent's behavior to yield equivalent outcomes from the mechanism. Finally, we conduct an empirical case study in the setting of \textit{consensus-finding} among diverse humans, and demonstrate the feasibility of fine-tuning large language models to act as digital representatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09369v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Jarrett, Miruna P\^islar, Michiel A. Bakker, Michael Henry Tessler, Raphael K\"oster, Jan Balaguer, Romuald Elie, Christopher Summerfield, Andrea Tacchetti</dc:creator>
    </item>
    <item>
      <title>Truth Knows No Language: Evaluating Truthfulness Beyond English</title>
      <link>https://arxiv.org/abs/2502.09387</link>
      <description>arXiv:2502.09387v1 Announce Type: cross 
Abstract: We introduce a professionally translated extension of the TruthfulQA benchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and Spanish. Truthfulness evaluations of large language models (LLMs) have primarily been conducted in English. However, the ability of LLMs to maintain truthfulness across languages remains under-explored. Our study evaluates 12 state-of-the-art open LLMs, comparing base and instruction-tuned models using human evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our findings reveal that, while LLMs perform best in English and worst in Basque (the lowest-resourced language), overall truthfulness discrepancies across languages are smaller than anticipated. Furthermore, we show that LLM-as-a-Judge correlates more closely with human judgments than multiple-choice metrics, and that informativeness plays a critical role in truthfulness assessment. Our results also indicate that machine translation provides a viable approach for extending truthfulness benchmarks to additional languages, offering a scalable alternative to professional translation. Finally, we observe that universal knowledge questions are better handled across languages than context- and time-dependent ones, highlighting the need for truthfulness evaluations that account for cultural and temporal variability. Dataset and code are publicly available under open licenses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09387v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Blanca Calvo Figueras, Eneko Sagarzazu, Julen Etxaniz, Jeremy Barnes, Pablo Gamallo, Iria De Dios Flores, Rodrigo Agerri</dc:creator>
    </item>
    <item>
      <title>Human-LLM Coevolution: Evidence from Academic Writing</title>
      <link>https://arxiv.org/abs/2502.09606</link>
      <description>arXiv:2502.09606v1 Announce Type: cross 
Abstract: With a statistical analysis of arXiv paper abstracts, we report a marked drop in the frequency of several words previously identified as overused by ChatGPT, such as "delve", starting soon after they were pointed out in early 2024. The frequency of certain other words favored by ChatGPT, such as "significant", has instead kept increasing. These phenomena suggest that some authors of academic papers have adapted their use of large language models (LLMs), for example, by selecting outputs or applying modifications to the LLM-generated content. Such coevolution and cooperation of humans and LLMs thus introduce additional challenges to the detection of machine-generated text in real-world scenarios. Estimating the impact of LLMs on academic writing by examining word frequency remains feasible, and more attention should be paid to words that were already frequently employed, including those that have decreased in frequency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09606v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.LG</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingmeng Geng, Roberto Trotta</dc:creator>
    </item>
    <item>
      <title>Copyright in Generative Deep Learning</title>
      <link>https://arxiv.org/abs/2105.09266</link>
      <description>arXiv:2105.09266v5 Announce Type: replace 
Abstract: Machine-generated artworks are now part of the contemporary art scene: they are attracting significant investments and they are presented in exhibitions together with those created by human artists. These artworks are mainly based on generative deep learning techniques, which have seen a formidable development and remarkable refinement in the very recent years. Given the inherent characteristics of these techniques, a series of novel legal problems arise. In this article, we consider a set of key questions in the area of generative deep learning for the arts, including the following: is it possible to use copyrighted works as training set for generative models? How do we legally store their copies in order to perform the training process? Who (if someone) will own the copyright on the generated data? We try to answer these questions considering the law in force in both the United States of America and the European Union, and potential future alternatives. We then extend our analysis to code generation, which is an emerging area of generative deep learning. Finally, we also formulate a set of practical guidelines for artists and developers working on deep learning generated art, as well as some policy suggestions for policymakers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.09266v5</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1017/dap.2022.10</arxiv:DOI>
      <arxiv:journal_reference>Data &amp; Policy. 2022;4:e17</arxiv:journal_reference>
      <dc:creator>Giorgio Franceschelli, Mirco Musolesi</dc:creator>
    </item>
    <item>
      <title>Differentiating Student Feedbacks for Knowledge Tracing</title>
      <link>https://arxiv.org/abs/2212.14695</link>
      <description>arXiv:2212.14695v2 Announce Type: replace 
Abstract: Knowledge tracing (KT) is a crucial task in computer-aided education and intelligent tutoring systems, predicting students' performance on new questions from their responses to prior ones. An accurate KT model can capture a student's mastery level of different knowledge topics, as reflected in their predicted performance on different questions. This helps improve the learning efficiency by suggesting appropriate new questions that complement students' knowledge states. However, current KT models have significant drawbacks that they neglect the imbalanced discrimination of historical responses. A significant proportion of question responses provide limited information for discerning students' knowledge mastery, such as those that demonstrate uniform performance across different students. Optimizing the prediction of these cases may increase overall KT accuracy, but also negatively impact the model's ability to trace personalized knowledge states, especially causing a deceptive surge of performance. Towards this end, we propose a framework to reweight the contribution of different responses based on their discrimination in training. Additionally, we introduce an adaptive predictive score fusion technique to maintain accuracy on less discriminative responses, achieving proper balance between student knowledge mastery and question difficulty. Experimental results demonstrate that our framework enhances the performance of three mainstream KT methods on three widely-used datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.14695v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiajun Cui, Hong Qian, Chanjin Zheng, Lu Wang, Mo Yu, Wei Zhang</dc:creator>
    </item>
    <item>
      <title>Toward A Dynamic Comfort Model for Human-Building Interaction in Grid-Interactive Efficient Buildings: Supported by Field Data</title>
      <link>https://arxiv.org/abs/2303.07206</link>
      <description>arXiv:2303.07206v2 Announce Type: replace 
Abstract: Controlling building electric loads could alleviate the increasing grid strain caused by the adoption of renewables and electrification. However, current approaches that automatically setback thermostats on the hottest day compromise their efficacy by neglecting human-building interaction (HBI). This study aims to define challenges and opportunities for developing engineering models of HBI to be used in the design of controls for grid-interactive efficient buildings (GEBs). Building system and measured and just-in-time surveyed psychophysiological data were collected from 41 participants in 20 homes from April-September. ASHRAE Standard 55 thermal comfort models for building design were evaluated with these data. Increased error bias was observed with increasing spatiotemporal temperature variations. Unsurprising, considering these models neglect such variance, but questioning their suitability for GEBs controlling thermostat setpoints, and given the observed 4{\deg}F intra-home spatial temperature variation. The results highlight opportunities for reducing these biases in GEBs through a paradigm shift to modeling discomfort instead of comfort, increasing use of low-cost sensors, and models that account for the observed dynamic occupant behavior: of the thermostat setpoint overrides made with 140-minutes of a previous setpoint change, 95% of small changes ( 2{\deg}F) were made with 120-minutes, while 95% of larger changes ( 10{\deg}F) were made within only 70-minutes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07206v2</guid>
      <category>cs.CY</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>SungKu Kang, Kunind Sharma, Maharshi Pathak, Emily Casavant, Katherine Bassett, Misha Pavel, David Fannon, Michael Kane</dc:creator>
    </item>
    <item>
      <title>The Dual Imperative: Innovation and Regulation in the AI Era</title>
      <link>https://arxiv.org/abs/2407.12690</link>
      <description>arXiv:2407.12690v2 Announce Type: replace 
Abstract: This article addresses the societal costs associated with the lack of regulation in Artificial Intelligence and proposes a framework combining innovation and regulation. Over fifty years of AI research, catalyzed by declining computing costs and the proliferation of data, have propelled AI into the mainstream, promising significant economic benefits. Yet, this rapid adoption underscores risks, from bias amplification and labor disruptions to existential threats posed by autonomous systems. The discourse is polarized between accelerationists, advocating for unfettered technological advancement, and doomers, calling for a slowdown to prevent dystopian outcomes. This piece advocates for a middle path that leverages technical innovation and smart regulation to maximize the benefits of AI while minimizing its risks, offering a pragmatic approach to the responsible progress of AI technology. Technical invention beyond the most capable foundation models is needed to contain catastrophic risks. Regulation is required to create incentives for this research while addressing current issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12690v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paulo Carv\~ao</dc:creator>
    </item>
    <item>
      <title>Measuring Human Contribution in AI-Assisted Content Generation</title>
      <link>https://arxiv.org/abs/2408.14792</link>
      <description>arXiv:2408.14792v2 Announce Type: replace 
Abstract: With the growing prevalence of generative artificial intelligence (AI), an increasing amount of content is no longer exclusively generated by humans but by generative AI models with human guidance. This shift presents notable challenges for the delineation of originality due to the varying degrees of human contribution in AI-assisted works. This study raises the research question of measuring human contribution in AI-assisted content generation and introduces a framework to address this question that is grounded in information theory. By calculating mutual information between human input and AI-assisted output relative to self-information of AI-assisted output, we quantify the proportional information contribution of humans in content generation. Our experimental results demonstrate that the proposed measure effectively discriminates between varying degrees of human contribution across multiple creative domains. We hope that this work lays a foundation for measuring human contributions in AI-assisted content generation in the era of generative AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14792v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yueqi Xie, Tao Qi, Jingwei Yi, Xiyuan Yang, Ryan Whalen, Junming Huang, Qian Ding, Yu Xie, Xing Xie, Fangzhao Wu</dc:creator>
    </item>
    <item>
      <title>A Law of One's Own: The Inefficacy of the DMCA for Non-Consensual Intimate Media</title>
      <link>https://arxiv.org/abs/2409.13575</link>
      <description>arXiv:2409.13575v2 Announce Type: replace 
Abstract: Non-consensual intimate media (NCIM) presents internet-scale harm to individuals who are depicted. One of the most powerful tools for requesting its removal is the Digital Millennium Copyright Act (DMCA). However, the DMCA was designed to protect copyright holders rather than to address the problem of NCIM. Using a dataset of more than 54,000 DMCA reports and over 85 million infringing URLs spanning over a decade, this paper evaluates the efficacy of the DMCA for NCIM takedown. Results show less than 50% of infringing URLs are removed from website hosts in 60 days, and Google Search takes a median of 11.7 days to deindex infringing content. Across web hosts, only 4% of URLs are removed within the first 48 hours. Additionally, the most frequently reported domains for non-commercial NCIM are smaller websites, not large platforms. We stress the need for new laws that ensure a shorter time to takedown that are enforceable across big and small platforms alike.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13575v2</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Li Qiwei, Shihui Zhang, Samantha Paige Pratt, Andrew Timothy Kasper, Eric Gilbert, Sarita Schoenebeck</dc:creator>
    </item>
    <item>
      <title>Exploring the Head Effect in Live Streaming Platforms: A Two-Sided Market and Welfare Analysis</title>
      <link>https://arxiv.org/abs/2410.13090</link>
      <description>arXiv:2410.13090v2 Announce Type: replace 
Abstract: We develop a comprehensive theoretical framework to analyze live streaming platforms as two-sided markets, focusing on the head effect where a small subset of elite streamers disproportionately attracts viewer attention. By constructing both static and dynamic models, we capture the interplay between network effects, content quality investments, and platform policies-such as commission structures and traffic allocation algorithms-that drive traffic concentration. Our welfare analysis demonstrates that although short-term consumer utility may benefit from concentrated viewership, long-term content diversity and overall social welfare are adversely impacted. Extensive simulations further validate our models and show that targeted policy interventions can rebalance viewer distribution and mitigate winner-takes-all dynamics. These findings offer actionable insights for platform designers and regulators in the digital economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13090v2</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yukun Zhang, Qi Dong</dc:creator>
    </item>
    <item>
      <title>The Value of Prediction in Identifying the Worst-Off</title>
      <link>https://arxiv.org/abs/2501.19334</link>
      <description>arXiv:2501.19334v2 Announce Type: replace 
Abstract: Machine learning is increasingly used in government programs to identify and support the most vulnerable individuals, prioritizing assistance for those at greatest risk over optimizing aggregate outcomes. This paper examines the welfare impacts of prediction in equity-driven contexts, and how they compare to other policy levers, such as expanding bureaucratic capacity. Through mathematical models and a real-world case study on long-term unemployment amongst German residents, we develop a comprehensive understanding of the relative effectiveness of prediction in surfacing the worst-off. Our findings provide clear analytical frameworks and practical, data-driven tools that empower policymakers to make principled decisions when designing these systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19334v2</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Unai Fischer-Abaigar, Christoph Kern, Juan Carlos Perdomo</dc:creator>
    </item>
    <item>
      <title>On the Creativity of Large Language Models</title>
      <link>https://arxiv.org/abs/2304.00008</link>
      <description>arXiv:2304.00008v5 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are revolutionizing several areas of Artificial Intelligence. One of the most remarkable applications is creative writing, e.g., poetry or storytelling: the generated outputs are often of astonishing quality. However, a natural question arises: can LLMs be really considered creative? In this article, we first analyze the development of LLMs under the lens of creativity theories, investigating the key open questions and challenges. In particular, we focus our discussion on the dimensions of value, novelty, and surprise as proposed by Margaret Boden in her work. Then, we consider different classic perspectives, namely product, process, press, and person. We discuss a set of ``easy'' and ``hard'' problems in machine creativity, presenting them in relation to LLMs. Finally, we examine the societal impact of these technologies with a particular focus on the creative industries, analyzing the opportunities offered, the challenges arising from them, and the potential associated risks, from both legal and ethical points of view.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.00008v5</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s00146-024-02127-3</arxiv:DOI>
      <arxiv:journal_reference>AI &amp; Soc (2024)</arxiv:journal_reference>
      <dc:creator>Giorgio Franceschelli, Mirco Musolesi</dc:creator>
    </item>
    <item>
      <title>Curating corpora with classifiers: A case study of clean energy sentiment online</title>
      <link>https://arxiv.org/abs/2305.03092</link>
      <description>arXiv:2305.03092v3 Announce Type: replace-cross 
Abstract: Well curated, large-scale corpora of social media posts containing broad public opinion offer an alternative data source to complement traditional surveys. While surveys are effective at collecting representative samples and are capable of achieving high accuracy, they can be both expensive to run and lag public opinion by days or weeks. Both of these drawbacks could be overcome with a real-time, high volume data stream and fast analysis pipeline. A central challenge in orchestrating such a data pipeline is devising an effective method for rapidly selecting the best corpus of relevant documents for analysis. Querying with keywords alone often includes irrelevant documents that are not easily disambiguated with bag-of-words natural language processing methods. Here, we explore methods of corpus curation to filter irrelevant tweets using pre-trained transformer-based models, fine-tuned for our binary classification task on hand-labeled tweets. We are able to achieve F1 scores of up to 0.95. The low cost and high performance of fine-tuning such a model suggests that our approach could be of broad benefit as a pre-processing step for social media datasets with uncertain corpus boundaries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.03092v3</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael V. Arnold, Peter Sheridan Dodds, Christopher M. Danforth</dc:creator>
    </item>
    <item>
      <title>AI Oversight and Human Mistakes: Evidence from Centre Court</title>
      <link>https://arxiv.org/abs/2401.16754</link>
      <description>arXiv:2401.16754v3 Announce Type: replace-cross 
Abstract: Powered by the increasing predictive capabilities of machine learning algorithms, artificial intelligence (AI) systems have the potential to overrule human mistakes in many settings. We provide the first field evidence that the use of AI oversight can impact human decision-making. We investigate one of the highest visibility settings where AI oversight has occurred: Hawk-Eye review of umpires in top tennis tournaments. We find that umpires lowered their overall mistake rate after the introduction of Hawk-Eye review, but also that umpires increased the rate at which they called balls in, producing a shift from making Type II errors (calling a ball out when in) to Type I errors (calling a ball in when out). We structurally estimate the psychological costs of being overruled by AI using a model of attention-constrained umpires, and our results suggest that because of these costs, umpires cared 37% more about Type II errors under AI oversight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16754v3</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Almog, Romain Gauriot, Lionel Page, Daniel Martin</dc:creator>
    </item>
    <item>
      <title>Long-Term Fairness Inquiries and Pursuits in Machine Learning: A Survey of Notions, Methods, and Challenges</title>
      <link>https://arxiv.org/abs/2406.06736</link>
      <description>arXiv:2406.06736v2 Announce Type: replace-cross 
Abstract: The widespread integration of Machine Learning systems in daily life, particularly in high-stakes domains, has raised concerns about the fairness implications. While prior works have investigated static fairness measures, recent studies reveal that automated decision-making has long-term implications and that off-the-shelf fairness approaches may not serve the purpose of achieving long-term fairness. Additionally, the existence of feedback loops and the interaction between models and the environment introduces additional complexities that may deviate from the initial fairness goals. In this survey, we review existing literature on long-term fairness from different perspectives and present a taxonomy for long-term fairness studies. We highlight key challenges and consider future research directions, analyzing both current issues and potential further explorations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06736v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Usman Gohar, Zeyu Tang, Jialu Wang, Kun Zhang, Peter L. Spirtes, Yang Liu, Lu Cheng</dc:creator>
    </item>
    <item>
      <title>Let's Influence Algorithms Together: How Millions of Fans Build Collective Understanding of Algorithms and Organize Coordinated Algorithmic Actions</title>
      <link>https://arxiv.org/abs/2409.10670</link>
      <description>arXiv:2409.10670v2 Announce Type: replace-cross 
Abstract: Previous research pays attention to how users strategically understand and consciously interact with algorithms but mainly focuses on an individual level, making it difficult to explore how users within communities could develop a collective understanding of algorithms and organize collective algorithmic actions. Through a two-year ethnography of online fan activities, this study investigates 43 core fans who always organize large-scale fans collective actions and their corresponding general fan groups. This study aims to reveal how these core fans mobilize millions of general fans through collective algorithmic actions. These core fans reported the rhetorical strategies used to persuade general fans, the steps taken to build a collective understanding of algorithms, and the collaborative processes that adapt collective actions across platforms and cultures. Our findings highlight the key factors that enable computer-supported collective algorithmic actions and extend collective action research into the large-scale domain targeting algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10670v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qing Xiao, Yuhang Zheng, Xianzhe Fan, Bingbing Zhang, Zhicong Lu</dc:creator>
    </item>
    <item>
      <title>"It Might be Technically Impressive, But It's Practically Useless to us": Motivations, Practices, Challenges, and Opportunities for Cross-Functional Collaboration around AI within the News Industry</title>
      <link>https://arxiv.org/abs/2409.12000</link>
      <description>arXiv:2409.12000v2 Announce Type: replace-cross 
Abstract: Recently, an increasing number of news organizations have integrated artificial intelligence (AI) into their workflows, leading to a further influx of AI technologists and data workers into the news industry. This has initiated cross-functional collaborations between these professionals and journalists. Although prior research has explored the impact of AI-related roles entering the news industry, there is a lack of studies on how internal cross-functional collaboration around AI unfolds between AI professionals and journalists within the news industry. Through interviews with 17 journalists, six AI technologists, and three AI workers with cross-functional experience from leading Chinese news organizations, we investigate the practices, challenges, and opportunities for internal cross-functional collaboration around AI in news industry. We first study how these journalists and AI professionals perceive existing internal cross-collaboration strategies. We explore the challenges of cross-functional collaboration and provide recommendations for enhancing future cross-functional collaboration around AI in the news industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12000v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qing Xiao, Xianzhe Fan, Felix M. Simon, Bingbing Zhang, Motahhare Eslami</dc:creator>
    </item>
    <item>
      <title>Revenue vs. Welfare: A Comprehensive Analysis of Strategic Trade-offs in Online Food Delivery Systems</title>
      <link>https://arxiv.org/abs/2410.16566</link>
      <description>arXiv:2410.16566v2 Announce Type: replace-cross 
Abstract: This paper investigates the trade-off between short-term revenue generation and long-term social welfare optimization in online food delivery platforms. We first develop a static model that captures the equilibrium interactions among restaurants, consumers, and delivery workers, using Gross Merchandise Value (GMV) as a proxy for immediate performance. Building on this, we extend our analysis to a dynamic model that integrates evolving state variables,such as platform reputation and participant retention-to capture long-term behavior. By applying dynamic programming techniques, we derive optimal strategies that balance GMV maximization with social welfare enhancement. Extensive multi-agent simulations validate our theoretical predictions, demonstrating that while a GMV-focused approach yields strong initial gains, it ultimately undermines long-term stability. In contrast, a social welfare-oriented strategy produces more sustainable and robust outcomes. Our findings provide actionable insights for platform operators and policymakers seeking to harmonize rapid growth with long-term</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16566v2</guid>
      <category>cs.GT</category>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yukun Zhang, Qi Dong</dc:creator>
    </item>
    <item>
      <title>Leveraging Large Language Models to Enhance Machine Learning Interpretability and Predictive Performance: A Case Study on Emergency Department Returns for Mental Health Patients</title>
      <link>https://arxiv.org/abs/2502.00025</link>
      <description>arXiv:2502.00025v2 Announce Type: replace-cross 
Abstract: Objective: To evaluate whether integrating large language models (LLMs) with traditional machine learning approaches improves both the predictive accuracy and clinical interpretability of ED mental health returns risk models. Methods: This retrospective cohort study analyzed 42,464 ED visits for 27,904 unique mental health patients at an Academic Medical Center in the deep South of the United States between January 2018 and December 2022. Main Outcomes and Measures: Two primary outcomes were evaluated: (1) 30 days ED return prediction accuracy and (2) model interpretability through a novel retrieval-augmented generation (RAG) framework integrating SHAP (SHapley Additive exPlanations) values with contextual clinical knowledge. Results: The proposed machine learning interpretability framework, leveraging LLM, achieved 99% accuracy in translating complex model predictions into clinically relevant explanations. Integration of LLM-extracted features enhanced predictive performance, improving the XGBoost model area under the curve (AUC) from 0.73 to 0.76. The LLM-based feature extraction using 10-shot learning significantly outperformed traditional approaches, achieving an accuracy of 0.882 and an F1 score of 0.86 for chief complaint classification (compared to conventional methods with an accuracy range of 0.59 to 0.63) and demonstrating accuracy values ranging from 0.65 to 0.93 across multiple SDoH categories, underscoring its robust performance in extracting features from clinical notes. Conclusions and Relevance: Integrating LLMs with traditional machine learning models yielded modest but consistent improvements in ED return prediction accuracy while substantially enhancing model interpretability through automated, clinically relevant explanations. This approach offers a framework for translating complex predictive analytics into actionable clinical insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00025v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 14 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abdulaziz Ahmed, Mohammad Saleem, Mohammed Alzeen, Badari Birur, Rachel E Fargason, Bradley G Burk, Hannah Rose Harkins, Ahmed Alhassan, Mohammed Ali Al-Garadi</dc:creator>
    </item>
  </channel>
</rss>
