<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Dec 2024 02:54:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Which Imputation Fits Which Feature Selection Method? A Survey-Based Simulation Study</title>
      <link>https://arxiv.org/abs/2412.13570</link>
      <description>arXiv:2412.13570v1 Announce Type: new 
Abstract: Tree-based learning methods such as Random Forest and XGBoost are still the gold-standard prediction methods for tabular data. Feature importance measures are usually considered for feature selection as well as to assess the effect of features on the outcome variables in the model. This also applies to survey data, which are frequently encountered in the social sciences and official statistics. These types of datasets often present the challenge of missing values. The typical solution is to impute the missing data before applying the learning method. However, given the large number of possible imputation methods available, the question arises as to which should be chosen to achieve the 'best' reflection of feature importance and feature selection in subsequent analyses. In the present paper, we investigate this question in a survey-based simulation study for eight state-of-the art imputation methods and three learners. The imputation methods comprise listwise deletion, three MICE options, four \texttt{missRanger} options as well as the recently proposed mixGBoost imputation approach. As learners, we consider the two most common tree-based methods, Random Forest and XGBoost, and an interpretable linear model with regularization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13570v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakob Schwerter (TU Dortmund University, Dortmund, Germany), Andr\'es Romero (TU Dortmund University, Dortmund, Germany), Florian Dumpert (Federal Statistical Office of Germany, Wiesbaden, Germany), Markus Pauly (TU Dortmund University, Dortmund, Germany, Research Center Trustworthy Data Science and Security, Dortmund, Germany)</dc:creator>
    </item>
    <item>
      <title>Breaching 1.5{\deg}C: Give me the odds</title>
      <link>https://arxiv.org/abs/2412.13855</link>
      <description>arXiv:2412.13855v1 Announce Type: new 
Abstract: Climate change communication is crucial to raising awareness and motivating action. In the context of breaching the limits set out by the Paris Agreement, we argue that climate scientists should move away from point estimates and towards reporting probabilities. Reporting probabilities will provide policymakers with a range of possible outcomes and will allow them to make informed timely decisions. To achieve this goal, we propose a method to calculate the probability of breaching the limits set out by the Paris Agreement. The method can be summarized as predicting future temperatures under different scenarios and calculating the number of possible outcomes that breach the limits as a proportion of the total number of outcomes. The probabilities can be computed for different time horizons and can be updated as new data become available. As an illustration, we performed a simulation study to investigate the probability of breaching the limits in a statistical model. Our results show that the probability of breaching the 1.5{\deg}C limit is already greater than zero for 2024. Moreover, the probability of breaching the limit is greater than 99% by 2042 if no action is taken to reduce greenhouse gas emissions. Our methodology is simple to implement and can easily be extended to more complex models of the climate system. We encourage climate model developers to include the probabilities of breaching the limits in their reports.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13855v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>J. Eduardo Vera-Vald\'es, Olivia Kvist</dc:creator>
    </item>
    <item>
      <title>Catalysts of Conversation: Examining Interaction Dynamics Between Topic Initiators and Commentors in Alzheimer's Disease Online Communities</title>
      <link>https://arxiv.org/abs/2412.13388</link>
      <description>arXiv:2412.13388v1 Announce Type: cross 
Abstract: Informal caregivers (e.g.,family members or friends) of people living with Alzheimers Disease and Related Dementias (ADRD) face substantial challenges and often seek informational or emotional support through online communities. Understanding the factors that drive engagement within these platforms is crucial, as it can enhance their long-term value for caregivers by ensuring that these communities effectively meet their needs. This study investigated the user interaction dynamics within two large, popular ADRD communities, TalkingPoint and ALZConnected, focusing on topic initiator engagement, initial post content, and the linguistic patterns of comments at the thread level. Using analytical methods such as propensity score matching, topic modeling, and predictive modeling, we found that active topic initiator engagement drives higher comment volumes, and reciprocal replies from topic initiators encourage further commentor engagement at the community level. Practical caregiving topics prompt more re-engagement of topic initiators, while emotional support topics attract more comments from other commentors. Additionally, the linguistic complexity and emotional tone of a comment influence its likelihood of receiving replies from topic initiators. These findings highlight the importance of fostering active and reciprocal engagement and providing effective strategies to enhance sustainability in ADRD caregiving and broader health-related online communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13388v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Congning Ni, Qingxia Chen, Lijun Song, Patricia Commiskey, Qingyuan Song, Bradley A. Malin, Zhijun Yin</dc:creator>
    </item>
    <item>
      <title>Revisiting Interactions of Multiple Driver States in Heterogenous Population and Cognitive Tasks</title>
      <link>https://arxiv.org/abs/2412.13574</link>
      <description>arXiv:2412.13574v2 Announce Type: cross 
Abstract: In real-world driving scenarios, multiple states occur simultaneously due to individual differences and environmental factors, complicating the analysis and estimation of driver states. Previous studies, limited by experimental design and analytical methods, may not be able to disentangle the relationships among multiple driver states and environmental factors. This paper introduces the Double Machine Learning (DML) analysis method to the field of driver state analysis to tackle this challenge. To train and test the DML model, a driving simulator experiment with 42 participants was conducted. All participants drove SAE level-3 vehicles and conducted three types of cognitive tasks in a 3-hour driving experiment. Drivers' subjective cognitive load and drowsiness levels were collected throughout the experiment. Then, we isolated individual and environmental factors affecting driver state variations and the factors affecting drivers' physiological and eye-tracking metrics when they are under specific states. The results show that our approach successfully decoupled and inferred the complex causal relationships between multiple types of drowsiness and cognitive load. Additionally, we identified key physiological and eye-tracking indicators in the presence of multiple driver states and under the influence of a single state, excluding the influence of other driver states, environmental factors, and individual characteristics. Our causal inference analytical framework can offer new insights for subsequent analysis of drivers' states. Further, the updated causal relation graph based on the DML analysis can provide theoretical bases for driver state monitoring based on physiological and eye-tracking measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13574v2</guid>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiyao Wang, Ange Wang, Song Yan, Dengbo He, Kaishun Wu</dc:creator>
    </item>
    <item>
      <title>Disease Progression Modelling and Stratification for detecting sub-trajectories in the natural history of pathologies: application to Parkinson's Disease trajectory modelling</title>
      <link>https://arxiv.org/abs/2412.13608</link>
      <description>arXiv:2412.13608v1 Announce Type: cross 
Abstract: Modelling the progression of Degenerative Diseases (DD) is essential for detection, prevention, and treatment, yet it remains challenging due to the heterogeneity in disease trajectories among individuals. Factors such as demographics, genetic conditions, and lifestyle contribute to diverse phenotypical manifestations, necessitating patient stratification based on these variations. Recent methods like Subtype and Stage Inference (SuStaIn) have advanced unsupervised stratification of disease trajectories, but they face potential limitations in robustness, interpretability, and temporal granularity. To address these challenges, we introduce Disease Progression Modelling and Stratification (DP-MoSt), a novel probabilistic method that optimises clusters of continuous trajectories over a long-term disease time-axis while estimating the confidence of trajectory sub-types for each biomarker. We validate DP-MoSt using both synthetic and real-world data from the Parkinson's Progression Markers Initiative (PPMI). Our results demonstrate that DP-MoSt effectively identifies both sub-trajectories and subpopulations, and is a promising alternative to current state-of-the-art models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13608v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alessandro Viani (CRISAM), Boris A Gutman (IIT), Emile d'Angremont (Amsterdam UMC), Marco Lorenzi (CRISAM)</dc:creator>
    </item>
    <item>
      <title>Practical privacy metrics for synthetic data</title>
      <link>https://arxiv.org/abs/2406.16826</link>
      <description>arXiv:2406.16826v2 Announce Type: replace 
Abstract: This paper explains how the synthpop package for R has been extended to include functions to calculate measures of identity and attribute disclosure risk for synthetic data that measure risks for the records used to create the synthetic data. The basic function, disclosure, calculates identity disclosure for a set of quasi-identifiers (keys) and attribute disclosure for one variable specified as a target from the same set of keys. The second function, disclosure.summary, is a wrapper for the first and presents summary results for a set of targets. This short paper explains the measures of disclosure risk and documents how they are calculated. We recommend two measures: $RepU$ (replicated uniques) for identity disclosure and $DiSCO$ (Disclosive in Synthetic Correct Original) for attribute disclosure. Both are expressed a \% of the original records and each can be compared to similar measures calculated from the original data. Experience with using the functions on real data found that some apparent disclosures could be identified as coming from relationships in the data that would be expected to be known to anyone familiar with its features. We flag cases when this seems to have occurred and provide means of excluding them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16826v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gillian M Raab, Beata Nowok, Chris Dibben</dc:creator>
    </item>
    <item>
      <title>Privacy risk from synthetic data: practical proposals</title>
      <link>https://arxiv.org/abs/2409.04257</link>
      <description>arXiv:2409.04257v2 Announce Type: replace 
Abstract: This paper proposes and compares measures of identity and attribute disclosure risk for synthetic data. Data custodians can use the methods proposed here to inform the decision as to whether to release synthetic versions of confidential data. Different measures are evaluated on two data sets. Insight into the measures is obtained by examining the details of the records identified as posing a disclosure risk. This leads to methods to identify, and possibly exclude, apparently risky records where the identification or attribution would be expected by someone with background knowledge of the data. The methods described are available as part of the \textbf{synthpop} package for \textbf{R}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04257v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gillian M Raab</dc:creator>
    </item>
  </channel>
</rss>
