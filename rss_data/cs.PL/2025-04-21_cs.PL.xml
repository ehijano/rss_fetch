<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Apr 2025 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Omelets Need Onions: E-graphs Modulo Theories via Bottom-up E-matching</title>
      <link>https://arxiv.org/abs/2504.14340</link>
      <description>arXiv:2504.14340v1 Announce Type: new 
Abstract: E-graphs are a data structure for equational reasoning and optimization over ground terms. One of the benefits of e-graph rewriting is that it can declaratively handle useful but difficult to orient identities like associativity and commutativity (AC) in a generic way. However, using these generic mechanisms is more computationally expensive than using bespoke routines on terms containing sets, multi-sets, linear expressions, polynomials, and binders. A natural question arises: How can one combine the generic capabilities of e-graph rewriting with these specialized theories. This paper discusses a pragmatic approach to this e-graphs modulo theories (EMT) question using two key ideas: bottom-up e-matching and semantic e-ids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14340v1</guid>
      <category>cs.PL</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philip Zucker</dc:creator>
    </item>
    <item>
      <title>Program Synthesis From Partial Traces</title>
      <link>https://arxiv.org/abs/2504.14480</link>
      <description>arXiv:2504.14480v1 Announce Type: new 
Abstract: We present the first technique to synthesize programs that compose side-effecting functions, pure functions, and control flow, from partial traces containing records of only the side-effecting functions. This technique can be applied to synthesize API composing scripts from logs of calls made to those APIs, or a script from traces of system calls made by a workload, for example. All of the provided traces are positive examples, meaning that they describe desired behavior. Our approach does not require negative examples. Instead, it generalizes over the examples and uses cost metrics to prevent over-generalization. Because the problem is too complex for traditional monolithic program synthesis techniques, we propose a new combination of optimizing rewrites and syntax-guided program synthesis. The resulting program is correct by construction, so its output will always be able to reproduce the input traces. We evaluate the quality of the programs synthesized when considering various optimization metrics and the synthesizer's efficiency on real-world benchmarks. The results show that our approach can generate useful real-world programs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14480v1</guid>
      <category>cs.PL</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3729316</arxiv:DOI>
      <dc:creator>Margarida Ferreira, Victor Nicolet, Joey Dodds, Daniel Kroening</dc:creator>
    </item>
    <item>
      <title>Dynamic Robustness Verification Against Weak Memory (Extended Version)</title>
      <link>https://arxiv.org/abs/2504.15036</link>
      <description>arXiv:2504.15036v1 Announce Type: new 
Abstract: Dynamic race detection is a highly effective runtime verification technique for identifying data races by instrumenting and monitoring concurrent program runs. However, standard dynamic race detection is incompatible with practical weak memory models; the added instrumentation introduces extra synchronization, which masks weakly consistent behaviors and inherently misses certain data races. In response, we propose to dynamically verify program robustness-a property ensuring that a program exhibits only strongly consistent behaviors. Building on an existing static decision procedures, we develop an algorithm for dynamic robustness verification under a C11-style memory model. The algorithm is based on "location clocks", a variant of vector clocks used in standard race detection. It allows effective and easy-to-apply defense against weak memory on a per-program basis, which can be combined with race detection that assumes strong consistency. We implement our algorithm in a tool, called RSAN, and evaluate it across various settings. To our knowledge, this work is the first to propose and develop dynamic verification of robustness against weak memory models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15036v1</guid>
      <category>cs.PL</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roy Margalit, Michalis Kokologiannakis, Shachar Itzhaky, Ori Lahav</dc:creator>
    </item>
    <item>
      <title>Flowco: Rethinking Data Analysis in the Age of LLMs</title>
      <link>https://arxiv.org/abs/2504.14038</link>
      <description>arXiv:2504.14038v1 Announce Type: cross 
Abstract: Conducting data analysis typically involves authoring code to transform, visualize, analyze, and interpret data. Large language models (LLMs) are now capable of generating such code for simple, routine analyses. LLMs promise to democratize data science by enabling those with limited programming expertise to conduct data analyses, including in scientific research, business, and policymaking. However, analysts in many real-world settings must often exercise fine-grained control over specific analysis steps, verify intermediate results explicitly, and iteratively refine their analytical approaches. Such tasks present barriers to building robust and reproducible analyses using LLMs alone or even in conjunction with existing authoring tools (e.g., computational notebooks). This paper introduces Flowco, a new mixed-initiative system to address these challenges. Flowco leverages a visual dataflow programming model and integrates LLMs into every phase of the authoring process. A user study suggests that Flowco supports analysts, particularly those with less programming experience, in quickly authoring, debugging, and refining data analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14038v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <category>stat.CO</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stephen N. Freund, Brooke Simon, Emery D. Berger, Eunice Jun</dc:creator>
    </item>
    <item>
      <title>Cyclic Proofs in Hoare Logic and its Reverse</title>
      <link>https://arxiv.org/abs/2504.14283</link>
      <description>arXiv:2504.14283v1 Announce Type: cross 
Abstract: We examine the relationships between axiomatic and cyclic proof systems for the partial and total versions of Hoare logic and those of its dual, known as reverse Hoare logic (or sometimes incorrectness logic). In the axiomatic proof systems for these logics, the proof rules for looping constructs involve an explicit loop invariant, which in the case of the total versions additionally require a well-founded termination measure. In the cyclic systems, these are replaced by rules that simply unroll the loops, together with a principle allowing the formation of cycles in the proof, subject to a global soundness condition that ensures the well-foundedness of the circular reasoning. Interestingly, the cyclic soundness conditions for partial Hoare logic and its reverse are similar and essentially coinductive in character, while those for the total versions are also similar and essentially inductive. We show that these cyclic systems are sound, by direct argument, and relatively complete, by translation from axiomatic to cyclic proofs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14283v1</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Brotherston, Quang Loc Le, Gauri Desai, Yukihiro Oda</dc:creator>
    </item>
    <item>
      <title>C2RUST-BENCH: A Minimized, Representative Dataset for C-to-Rust Transpilation Evaluation</title>
      <link>https://arxiv.org/abs/2504.15144</link>
      <description>arXiv:2504.15144v1 Announce Type: cross 
Abstract: Despite the effort in vulnerability detection over the last two decades, memory safety vulnerabilities continue to be a critical problem. Recent reports suggest that the key solution is to migrate to memory-safe languages. To this end, C-to-Rust transpilation becomes popular to resolve memory-safety issues in C programs. Recent works propose C-to-Rust transpilation frameworks; however, a comprehensive evaluation dataset is missing. Although one solution is to put together a large enough dataset, this increases the analysis time in automated frameworks as well as in manual efforts for some cases. In this work, we build a method to select functions from a large set to construct a minimized yet representative dataset to evaluate the C-to-Rust transpilation. We propose C2RUST-BENCH that contains 2,905 functions, which are representative of C-to-Rust transpilation, selected from 15,503 functions of real-world programs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15144v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Melih Sirlanci, Carter Yagemann, Zhiqiang Lin</dc:creator>
    </item>
    <item>
      <title>Spineless Traversal for Layout Invalidation</title>
      <link>https://arxiv.org/abs/2411.10659</link>
      <description>arXiv:2411.10659v5 Announce Type: replace 
Abstract: Latency is a major concern for web rendering engines like those in Chrome, Safari, and Firefox. These engines reduce latency by using an incremental layout algorithm to redraw the page when the user interacts with it. In such an algorithm, elements that change frame-to-frame are marked dirty; only the dirty elements need be processed to draw the next frame, dramatically reducing latency. However, the standard incremental layout algorithm must search the page for dirty elements, accessing a number of auxiliary elements in the process. These auxiliary elements add cache misses and stalled cycles, and are responsible for a sizable fraction of all layout latency. We introduce a new, faster incremental layout algorithm called Spineless Traversal. Spineless Traversal uses a more computationally demanding priority queue algorithm to avoid the need to access auxiliary nodes and thus reduces cache traffic and stalls. This leads to dramatic speedups on the most latency-critical interactions such as hovering, typing, or animations. Moreover, thanks to numerous low-level optimizations, we are able to make Spineless Traversal competitive across the whole spectrum of incremental layout workloads. As a result, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the benchmark, with a mean speedup of 3.23x concentrated in the most latency-critical interactions such as hovering, typing, and animations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10659v5</guid>
      <category>cs.PL</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marisa Kirisame, Tiezhi Wang, Pavel Panchekha</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Quantitative Cache Analysis</title>
      <link>https://arxiv.org/abs/2503.16588</link>
      <description>arXiv:2503.16588v3 Announce Type: replace 
Abstract: In this work we unify two existing lines of work towards cache analysis for non-LRU policies. To this end, we extend the notion of competitiveness to block competitiveness and systematically analyze the competitiveness and block competitiveness of FIFO and MRU relative to LRU for arbitrary associativities. We show how competitiveness and block competitiveness can be exploited in state-of-the-art WCET analysis based on the results of existing persistence analyses for LRU. Unlike prior work, our approach is applicable to microarchitectures that exhibit timing anomalies. We experimentally evaluate the precision and cost of our approach on benchmarks from TACLeBench. The experiments demonstrate that quantitative cache analysis for FIFO and MRU comes close to the precision of LRU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16588v3</guid>
      <category>cs.PL</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sophie Kahlen, Jan Reineke</dc:creator>
    </item>
    <item>
      <title>Intrinsic Verification of Parsers and Formal Grammar Theory in Dependent Lambek Calculus (Extended Version)</title>
      <link>https://arxiv.org/abs/2504.03995</link>
      <description>arXiv:2504.03995v2 Announce Type: replace 
Abstract: We present Dependent Lambek Calculus, a domain-specific dependent type theory for verified parsing and formal grammar theory. In Dependent Lambek Calculus, linear types are used as a syntax for formal grammars, and parsers can be written as linear terms. The linear typing restriction provides a form of intrinsic verification that a parser yields only valid parse trees for the input string. We demonstrate the expressivity of this system by showing that the combination of inductive linear types and dependency on non-linear data can be used to encode commonly used grammar formalisms such as regular and context-free grammars as well as traces of various types of automata. Using these encodings, we define parsers for regular expressions using deterministic automata, as well as examples of verified parsers of context-free grammars.
  We present a denotational semantics of our type theory that interprets the types as a mathematical notion of formal grammars. Based on this denotational semantics, we have made a prototype implementation of Dependent Lambek Calculus using a shallow embedding in the Agda proof assistant. All of our examples parsers have been implemented in this prototype implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03995v2</guid>
      <category>cs.PL</category>
      <category>cs.FL</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven Schaefer, Nathan Varner, Pedro H. Azevedo de Amorim, Max S. New</dc:creator>
    </item>
    <item>
      <title>Checkification: A Practical Approach for Testing Static Analysis Truths</title>
      <link>https://arxiv.org/abs/2501.12093</link>
      <description>arXiv:2501.12093v2 Announce Type: replace-cross 
Abstract: Static analysis is an essential component of many modern software development tools. Unfortunately, the ever-increasing complexity of static analyzers makes their coding error-prone. Even analysis tools based on rigorous mathematical techniques, such as abstract interpretation, are not immune to bugs. Ensuring the correctness and reliability of software analyzers is critical if they are to be inserted in production compilers and development environments. While compiler validation has seen notable success, formal validation of static analysis tools remains relatively unexplored. In this paper, we propose a method for testing abstract interpretation-based static analyzers. Broadly, it consists in checking, over a suite of benchmarks, that the properties inferred statically are satisfied dynamically. The main advantage of our approach lies in its simplicity, which stems directly from framing it within the Ciao assertion-based validation framework, and its blended static/dynamic assertion checking approach. We demonstrate that in this setting, the analysis can be tested with little effort by combining the following components already present in the framework: 1) the static analyzer, which outputs its results as the original program source with assertions interspersed; 2) the assertion run-time checking mechanism, which instruments a program to ensure that no assertion is violated at run time; 3) the random test case generator, which generates random test cases satisfying the properties present in assertion preconditions; and 4) the unit-test framework, which executes those test cases. We have applied our approach to the CiaoPP static analyzer, resulting in the identification of many bugs with reasonable overhead. Most of these bugs have been either fixed or confirmed, helping us detect a range of errors not only related to analysis soundness but also within other aspects of the framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12093v2</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniela Ferreiro, Ignacio Casso, Jose F. Morales, Pedro L\'opez-Garc\'ia, Manuel V. Hermenegildo</dc:creator>
    </item>
  </channel>
</rss>
