<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 14 Jan 2026 02:38:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Coalition Tactics: Bribery and Control in Parliamentary Elections</title>
      <link>https://arxiv.org/abs/2601.07279</link>
      <description>arXiv:2601.07279v1 Announce Type: new 
Abstract: Strategic manipulation of elections is typically studied in the context of promoting individual candidates.
  In parliamentary elections, however, the focus shifts: voters may care more about the overall governing coalition than the individual parties' seat counts.
  This paper studies this new problem: manipulating parliamentary elections with the goal of promoting the collective seat count of a coalition of parties.
  We focus on proportional representation elections, and consider two variants of the problem; one in which the sole goal is to maximize the total number of seats held by the desired coalition, and the other with a dual objective of both promoting the coalition and promoting the relative power of some favorite party within the coalition.
  We examine two types of strategic manipulations:
  \emph{bribery}, which allows modifying voters' preferences, and \emph{control}, which allows
  changing the sets of voters and parties.
  We consider multiple bribery types, presenting polynomial-time algorithms for some, while proving NP-hardness for others.
  For control, we provide polynomial-time algorithms for control by adding and deleting voters. In contrast, control by adding and deleting parties, we show, is either impossible (i.e., the problem is immune to control) or computationally hard, in particular, W[1]-hard when parameterized by the number of parties that can be added or deleted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07279v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hodaya Barr, Eden Hartman, Yonatan Aumann, Sarit Kraus</dc:creator>
    </item>
    <item>
      <title>Machine Learning Model Trading with Verification under Information Asymmetry</title>
      <link>https://arxiv.org/abs/2601.07510</link>
      <description>arXiv:2601.07510v1 Announce Type: new 
Abstract: Machine learning (ML) model trading, known for its role in protecting data privacy, faces a major challenge: information asymmetry. This issue can lead to model deception, a problem that current literature has not fully solved, where the seller misrepresents model performance to earn more. We propose a game-theoretic approach, adding a verification step in the ML model market that lets buyers check model quality before buying. However, this method can be expensive and offers imperfect information, making it harder for buyers to decide. Our analysis reveals that a seller might probabilistically conduct model deception considering the chance of model verification. This deception probability decreases with the verification accuracy and increases with the verification cost. To maximize seller payoff, we further design optimal pricing schemes accounting for heterogeneous buyers' strategic behaviors. Interestingly, we find that reducing information asymmetry benefits both the seller and buyer. Meanwhile, protecting buyer order information doesn't improve the payoff for the buyer or the seller. These findings highlight the importance of reducing information asymmetry in ML model trading and open new directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07510v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Li, Jianwei Huang, Kai Yang, Chenyou Fan</dc:creator>
    </item>
    <item>
      <title>Enforcing Priority in Schedule-based User Equilibrium Transit Assignment</title>
      <link>https://arxiv.org/abs/2601.07712</link>
      <description>arXiv:2601.07712v1 Announce Type: new 
Abstract: Denied boarding in congested transit systems induces queuing delays and departure-time shifts that can reshape passenger flows. Correctly modeling these responses in transit assignment hinges on the enforcement of two priority rules: continuance priority for onboard passengers and first-come-first-served (FCFS) boarding among waiting passengers. Existing schedule-based models typically enforce these rules through explicit dynamic loading and group-level expected costs, yet discrete vehicle runs can induce nontrivial within-group cost differences that undermine behavioral consistency. We revisit the implicit-priority framework of Nguyen et al. (2001), which, by encoding boarding priority through the notion of available capacity, characterizes route and departure choices based on realized personal (rather than group-averaged) travel experiences. However, the framework lacks an explicit mathematical formulation and exact computational methods for finding equilibria. Here, we derive an equivalent nonlinear complementarity problem (NCP) formulation and establish equilibrium existence under mild conditions. We also show that multiple equilibria may exist, including behaviorally questionable ones. To rule out these artifacts, we propose a refined arc-level NCP formulation that not only corresponds to a tighter, behaviorally consistent equilibrium concept but also is more computationally tractable. We reformulate the NCP as a continuously differentiable mathematical program with equilibrium constraints (MPEC) and propose two solution algorithms. Numerical studies on benchmark instances and a Hong Kong case study demonstrate that the model reproduces continuance priority and FCFS queuing and captures departure-time shifts driven by the competition for boarding priority.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07712v1</guid>
      <category>cs.GT</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liyang Feng, Hanlin Sun, Yu Marco Nie, Jun Xie, Jiayang Li</dc:creator>
    </item>
    <item>
      <title>Structural Approach to Guiding a Present-Biased Agent</title>
      <link>https://arxiv.org/abs/2601.07763</link>
      <description>arXiv:2601.07763v1 Announce Type: new 
Abstract: Time-inconsistent behavior, such as procrastination or abandonment of long-term goals, arises when agents evaluate immediate outcomes disproportionately higher than future ones. This leads to globally suboptimal behavior, where plans are frequently revised or abandoned entirely. In the influential model of Kleinberg and Oren (2014) such behavior is modeled by a present-biased agent navigating a task graph toward a goal, making locally optimal decisions at each step based on discounted future costs. As a result, the agent may repeatedly deviate from initial plans. Recent work by Belova et al. (2024) introduced a two-agent extension of this model, where a fully-aware principal attempts to guide the present-biased agent through a specific set of critical tasks without causing abandonment. This captures a rich class of principal-agent dynamics in behavioral settings.
  In this paper, we provide a comprehensive algorithmic characterization of this problem. We analyze its computational complexity through the framework of parameterized algorithms, focusing on graph parameters that naturally emerge in this setting, such as treewidth, vertex cover, and feedback vertex set. Our main result is a fixed-parameter tractable algorithm when parameterized by the treewidth of the task graph and the number of distinct (v,t)-path costs. Our algorithm encaptures several input settings, such as bounded edge costs and restricted task graph structure. We demonstrate that our main result yields efficient algorithms for a number of such configurations.
  We complement this with tight hardness results, that highlight the extreme difficulty of the problem even on simplest graphs with bounded number of nodes and constant parameter values, and motivate our choice of parameters. We delineate tractable and intractable regions of the problem landscape, which include answers to open questions of Belova et al. (2024).</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07763v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tatiana Belova, Yuriy Dementiev, Artur Ignatiev, Danil Sagunov</dc:creator>
    </item>
    <item>
      <title>The Complexity of Games with Randomised Control</title>
      <link>https://arxiv.org/abs/2601.07775</link>
      <description>arXiv:2601.07775v1 Announce Type: new 
Abstract: We study the complexity of solving two-player infinite duration games played on a fixed finite graph, where the control of a node is not predetermined but rather assigned randomly. In classic random-turn games, control of each node is assigned randomly every time the node is visited during a play. In this work, we study two natural variants of this where control of each node is assigned only once: (i) control is assigned randomly during a play when a node is visited for the first time and does not change for the rest of the play and (ii) control is assigned a priori before the game starts for every node by independent coin tosses and then the game is played. We investigate the complexity of computing the winning probability with three kinds of objectives-reachability, parity, and energy. We show that the qualitative questions on all variants and all objectives are NL-complete. For the quantitative questions, we show that deciding whether the maximiser can win with probability at least a given threshold for every objective is PSPACE-complete under the first mechanism, and that computing the exact winning probability for every objective is sharp-P-complete under the second. To complement our hardness results for the second mechanism, we propose randomised approximation schemes that efficiently estimate the winning probability for all three objectives, assuming a bounded number of parity colours and unary-encoded weights for energy objectives, and we empirically demonstrate their fast convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07775v1</guid>
      <category>cs.GT</category>
      <category>cs.LO</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarvin Bahmani, Rasmus Ibsen-Jensen, Soumyajit Paul, Sven Schewe, Friedrich Slivovsky, Qiyi Tang, Dominik Wojtczak, Shufang Zhu</dc:creator>
    </item>
    <item>
      <title>GroupSegment-SHAP: Shapley Value Explanations with Group-Segment Players for Multivariate Time Series</title>
      <link>https://arxiv.org/abs/2601.06114</link>
      <description>arXiv:2601.06114v1 Announce Type: cross 
Abstract: Multivariate time-series models achieve strong predictive performance in healthcare, industry, energy, and finance, but how they combine cross-variable interactions with temporal dynamics remains unclear. SHapley Additive exPlanations (SHAP) are widely used for interpretation. However, existing time-series variants typically treat the feature and time axes independently, fragmenting structural signals formed jointly by multiple variables over specific intervals. We propose GroupSegment SHAP (GS-SHAP), which constructs explanatory units as group-segment players based on cross-variable dependence and distribution shifts over time, and then quantifies each unit's contribution via Shapley attribution. We evaluate GS-SHAP across four real-world domains: human activity recognition, power-system forecasting, medical signal analysis, and financial time series, and compare it with KernelSHAP, TimeSHAP, SequenceSHAP, WindowSHAP, and TSHAP. GS-SHAP improves deletion-based faithfulness (DeltaAUC) by about 1.7x on average over time-series SHAP baselines, while reducing wall-clock runtime by about 40 percent on average under matched perturbation budgets. A financial case study shows that GS-SHAP identifies interpretable multivariate-temporal interactions among key market variables during high-volatility regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06114v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinwoong Kim, Sangjin Park</dc:creator>
    </item>
    <item>
      <title>Symmetry Breaking, Hysteresis, and Convergence to the Mean Voter in two-party Spatial Competition</title>
      <link>https://arxiv.org/abs/2601.07108</link>
      <description>arXiv:2601.07108v1 Announce Type: cross 
Abstract: Classical spatial models of two-party competition typically predict convergence to the median voter, yet real-world party systems often exhibit persistent and asymmetric polarization. We develop a spatial model of two-party competition in which voters evaluate parties through general satisfaction functions, and a width parameter $q$ captures how tolerant they are of ideological distance. This parameter governs the balance between centripetal and centrifugal incentives and acts as the bifurcation parameter governing equilibrium configurations. Under mild regularity assumptions, we characterize Nash equilibria through center-distance coordinates, which separate the endogenous political center from polarization. When the voter density is symmetric, the reduced equilibrium condition exhibits a generic supercritical pitchfork bifurcation at a critical value $q_{c}$. Above $q_{c}$, the unique stable equilibrium features convergence to the center, recovering the classical median voter result, whereas below it two symmetric polarized equilibria arise. Asymmetry in the voter distribution unfolds the pitchfork, producing drift in the endogenous center and asymmetric polarized equilibria. The resulting equilibrium diagram has an S-shaped geometry that generates hysteresis, allowing polarization to persist even after tolerance returns to levels that would support convergence in a symmetric environment. In the high-tolerance regime, we show that the unique non-polarized equilibrium converges to the mean of the voter distribution, while the median is recovered only under symmetry. Hence, unlike the Hotelling--Downs model, where convergence to the median is universal, the median voter appears here as an asymptotic benchmark rather than a robust predictor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07108v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.GT</category>
      <category>math.DS</category>
      <category>math.PR</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel Miranda Machado, Roberto Venegeroles</dc:creator>
    </item>
    <item>
      <title>Condorcet's Paradox as Non-Orientability</title>
      <link>https://arxiv.org/abs/2601.07283</link>
      <description>arXiv:2601.07283v1 Announce Type: cross 
Abstract: Preference cycles are prevalent in problems of decision-making, and are contradictory when preferences are assumed to be transitive. This contradiction underlies Condorcet's Paradox, a pioneering result of Social Choice Theory, wherein intuitive and seemingly desirable constraints on decision-making necessarily lead to contradictory preference cycles. Topological methods have since broadened Social Choice Theory and elucidated existing results. However, characterisations of preference cycles in Topological Social Choice Theory are lacking. In this paper, we address this gap by introducing a framework for topologically modelling preference cycles that generalises Baryshnikov's existing topological model of strict, ordinal preferences on 3 alternatives. In our framework, the contradiction underlying Condorcet's Paradox topologically corresponds to the non-orientability of a surface homeomorphic to either the Klein Bottle or Real Projective Plane, depending on how preference cycles are represented. These findings allow us to reduce Arrow's Impossibility Theorem to a statement about the orientability of a surface. Furthermore, these results contribute to existing wide-ranging interest in the relationship between non-orientability, impossibility phenomena in Economics, and logical paradoxes more broadly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07283v1</guid>
      <category>math.AT</category>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ori Livson, Siddharth Pritam, Mikhail Prokopenko</dc:creator>
    </item>
    <item>
      <title>Improved lower bounds for the maximum size of Condorcet domains</title>
      <link>https://arxiv.org/abs/2601.07336</link>
      <description>arXiv:2601.07336v1 Announce Type: cross 
Abstract: Condorcet domains are sets of linear orders with the property that, whenever voters' preferences are restricted to the domain, the pairwise majority relation (for an odd number of voters) is transitive and hence a linear order. Determining the maximum size of a Condorcet domain, sometimes under additional constraints, has been a longstanding problem in the mathematical theory of majority voting. The exact maximum is only known for $n\leq 8$ alternatives.
  In this paper we use a structural analysis of the largest domains for small $n$ to design a new inductive search method. Using an implementation of this method on a supercomputer, together with existing algorithms, we improve the size of the largest known domains for all $9 \leq n \leq 20$. These domains are then used in a separate construction to obtain the currently largest known domains for $21 \leq n \leq 25$, and to improve the best asymptotic lower bound for the maximum size of a Condorcet domain to $\Omega(2.198139^n)$. Finally, we discuss properties of the domains found and state several open problems and conjectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07336v1</guid>
      <category>cs.DM</category>
      <category>cs.GT</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Karpov, Klas Markstrom, Soren Riis, Bei Zhou</dc:creator>
    </item>
    <item>
      <title>The Secretary Problem with Predictions and a Chosen Order</title>
      <link>https://arxiv.org/abs/2601.07482</link>
      <description>arXiv:2601.07482v1 Announce Type: cross 
Abstract: We study a learning-augmented variant of the secretary problem, recently introduced by Fujii and Yoshida (2023), in which the decision-maker has access to machine-learned predictions of candidate values. The central challenge is to balance consistency and robustness: when predictions are accurate, the algorithm should select a near-optimal secretary, while under inaccurate predictions it should still guarantee a bounded competitive ratio.
  We consider both the classical Random Order Secretary Problem (ROSP), where candidates arrive in a uniformly random order, and a more natural learning-augmented model in which the decision-maker may choose the arrival order based on predicted values. We call this model the Chosen Order Secretary Problem (COSP), capturing scenarios such as interview schedules set in advance.
  We propose a new randomized algorithm applicable to both ROSP and COSP. Our method switches from fully trusting predictions to a threshold-based rule once a large prediction deviation is detected. Let $\epsilon \in [0,1]$ denote the maximum multiplicative prediction error. For ROSP, our algorithm achieves a competitive ratio of $\max\{0.221, (1-\epsilon)/(1+\epsilon)\}$, improving upon the prior bound of $\max\{0.215, (1-\epsilon)/(1+\epsilon)\}$. For COSP, we achieve $\max\{0.262, (1-\epsilon)/(1+\epsilon)\}$, surpassing the $0.25$ worst-case bound for prior approaches and moving closer to the classical secretary benchmark of $1/e \approx 0.368$. These results highlight the benefit of combining predictions with arrival-order control in online decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07482v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Helia Karisani, Mohammadreza Daneshvaramoli, Hedyeh Beyhaghi, Mohammad Hajiesmaili, Cameron Musco</dc:creator>
    </item>
    <item>
      <title>Active Evaluation of General Agents: Problem Definition and Comparison of Baseline Algorithms</title>
      <link>https://arxiv.org/abs/2601.07651</link>
      <description>arXiv:2601.07651v1 Announce Type: cross 
Abstract: As intelligent agents become more generally-capable, i.e. able to master a wide variety of tasks, the complexity and cost of properly evaluating them rises significantly. Tasks that assess specific capabilities of the agents can be correlated and stochastic, requiring many samples for accurate comparisons, leading to added costs. In this paper, we propose a formal definition and a conceptual framework for active evaluation of agents across multiple tasks, which assesses the performance of ranking algorithms as a function of number of evaluation data samples. Rather than curating, filtering, or compressing existing data sets as a preprocessing step, we propose an online framing: on every iteration, the ranking algorithm chooses the task and agents to sample scores from. Then, evaluation algorithms report a ranking of agents on each iteration and their performance is assessed with respect to the ground truth ranking over time. Several baselines are compared under different experimental contexts, with synthetic generated data and simulated online access to real evaluation data from Atari game-playing agents. We find that the classical Elo rating system -- while it suffers from well-known failure modes, in theory -- is a consistently reliable choice for efficient reduction of ranking error in practice. A recently-proposed method, Soft Condorcet Optimization, shows comparable performance to Elo on synthetic data and significantly outperforms Elo on real Atari agent evaluation. When task variation from the ground truth is high, selecting tasks based on proportional representation leads to higher rate of ranking error reduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07651v1</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marc Lanctot, Kate Larson, Ian Gemp, Michael Kaisers</dc:creator>
    </item>
    <item>
      <title>The value of random zero-sum games</title>
      <link>https://arxiv.org/abs/2601.07759</link>
      <description>arXiv:2601.07759v1 Announce Type: cross 
Abstract: We study the value of a two-player zero-sum game on a random matrix $M\in \mathbb{R}^{n\times m}$, defined by $v(M) = \min_{x\in\Delta_n}\max_{y\in \Delta_m}x^T M y$. In the setting where $n=m$ and $M$ has i.i.d. standard Gaussian entries, we prove that the standard deviation of $v(M)$ is of order $\frac{1}{n}$. This confirms an experimental conjecture dating back to the 1980s. We also investigate the case where $M$ is a rectangular Gaussian matrix with $m = n+\lambda\sqrt{n}$, showing that the expected value of the game is of order $\frac{\lambda}{n}$, as well as the case where $M$ is a random orthogonal matrix. Our techniques are based on probabilistic arguments and convex geometry. We argue that the study of random games could shed new light on various problems in theoretical computer science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07759v1</guid>
      <category>math.PR</category>
      <category>cs.GT</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Romain Cosson, Laurent Massouli\'e</dc:creator>
    </item>
    <item>
      <title>Settling the Score: Portioning with Cardinal Preferences</title>
      <link>https://arxiv.org/abs/2307.15586</link>
      <description>arXiv:2307.15586v5 Announce Type: replace 
Abstract: We study a portioning setting in which a public resource such as time or money is to be divided among a given set of candidates, and each agent proposes a division of the resource. We consider two families of aggregation rules for this setting -- those based on coordinate-wise aggregation and those that optimize some notion of welfare -- as well as the recently proposed independent markets rule. We provide a detailed analysis of these rules from an axiomatic perspective, both for classic axioms, such as strategyproofness and Pareto optimality, and for novel axioms, some of which aim to capture proportionality in this setting. Our results indicate that a simple rule that computes the average of the proposals satisfies many of our axioms and fares better than all other considered rules in terms of fairness properties. We complement these results by presenting two characterizations of the average rule.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15586v5</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edith Elkind, Matthias Greger, Patrick Lederer, Warut Suksompong, Nicholas Teh</dc:creator>
    </item>
    <item>
      <title>$\aleph$-IPOMDP: Mitigating Deception in a Cognitive Hierarchy with Off-Policy Counterfactual Anomaly Detection</title>
      <link>https://arxiv.org/abs/2405.01870</link>
      <description>arXiv:2405.01870v3 Announce Type: replace-cross 
Abstract: Social agents with finitely nested opponent models are vulnerable to manipulation by agents with deeper recursive capabilities. This imbalance, rooted in logic and the theory of recursive modelling frameworks, cannot be solved directly. We propose a computational framework called $\aleph$-IPOMDP, which augments the Bayesian inference of model-based RL agents with an anomaly detection algorithm and an out-of-belief policy. Our mechanism allows agents to realize that they are being deceived, even if they cannot understand how, and to deter opponents via a credible threat. We test this framework in both a mixed-motive and a zero-sum game. Our results demonstrate the $\aleph$-mechanism's effectiveness, leading to more equitable outcomes and less exploitation by more sophisticated agents. We discuss implications for AI safety, cybersecurity, cognitive science, and psychiatry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01870v3</guid>
      <category>cs.MA</category>
      <category>cs.GT</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nitay Alon, Joseph M. Barnby, Stefan Sarkadi, Lion Schulz, Jeffrey S. Rosenschein, Peter Dayan</dc:creator>
    </item>
    <item>
      <title>Two-Player Zero-Sum Games with Bandit Feedback</title>
      <link>https://arxiv.org/abs/2506.14518</link>
      <description>arXiv:2506.14518v3 Announce Type: replace-cross 
Abstract: We study a two-player zero-sum game in which the row player aims to maximize their payoff against an adversarial column player, under an unknown payoff matrix estimated through bandit feedback. We propose three algorithms based on the Explore-Then-Commit (ETC) framework. The first adapts it to zero-sum games, the second incorporates adaptive elimination that leverages the $\varepsilon$-Nash Equilibrium property to efficiently select the optimal action pair, and the third extends the elimination algorithm by employing non-uniform exploration. Our objective is to demonstrate the applicability of ETC in a zero-sum game setting by focusing on learning pure strategy Nash Equilibria. A key contribution of our work is a derivation of instance-dependent upper bounds on the expected regret of our proposed algorithms, which has received limited attention in the literature on zero-sum games. Particularly, after $T$ rounds, we achieve an instance-dependent regret upper bounds of $O(\Delta + \sqrt{T})$ for ETC in zero-sum game setting and $O(\log (T \Delta^2)/\Delta)$ for the adaptive elimination algorithm and its variant with non-uniform exploration, where $\Delta$ denotes the suboptimality gap. Therefore, our results indicate that ETC-based algorithms perform effectively in zero-sum game settings, achieving regret bounds comparable to existing methods while providing insight through instance-dependent analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14518v3</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elif Y{\i}lmaz, Christos Dimitrakakis</dc:creator>
    </item>
    <item>
      <title>A Graph-Theoretical Perspective on Law Design for Multiagent Systems</title>
      <link>https://arxiv.org/abs/2511.06361</link>
      <description>arXiv:2511.06361v2 Announce Type: replace-cross 
Abstract: A law in a multiagent system is a set of constraints imposed on agents' behaviours to avoid undesirable outcomes. The paper considers two types of laws: useful laws that, if followed, completely eliminate the undesirable outcomes and gap-free laws that guarantee that at least one agent can be held responsible each time an undesirable outcome occurs. In both cases, we study the problem of finding a law that achieves the desired result by imposing the minimum restrictions.
  We prove that, for both types of laws, the minimisation problem is NP-hard even in the simple case of one-shot concurrent interactions. We also show that the approximation algorithm for the vertex cover problem in hypergraphs could be used to efficiently approximate the minimum laws in both cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06361v2</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qi Shi, Pavel Naumov</dc:creator>
    </item>
    <item>
      <title>Learning the Value of Value Learning</title>
      <link>https://arxiv.org/abs/2511.17714</link>
      <description>arXiv:2511.17714v4 Announce Type: replace-cross 
Abstract: Standard decision frameworks address uncertainty about facts but assume fixed options and values. We extend the Jeffrey-Bolker framework to model refinements in values and prove a value-of-information theorem for axiological refinement. In multi-agent settings, we establish that mutual refinement will characteristically transform zero-sum games into positive-sum interactions and yield Pareto-improvements in Nash bargaining. These results show that a framework of rational choice can be extended to model value refinement. By unifying epistemic and axiological refinement under a single formalism, we broaden the conceptual foundations of rational choice and illuminate the normative status of ethical deliberation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17714v4</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex John London, Aydin Mohseni</dc:creator>
    </item>
  </channel>
</rss>
