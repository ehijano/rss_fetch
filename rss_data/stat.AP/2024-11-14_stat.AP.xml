<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Nov 2024 05:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Consistent model selection for estimating functional interactions among stochastic neurons with variable-length memory</title>
      <link>https://arxiv.org/abs/2411.08205</link>
      <description>arXiv:2411.08205v1 Announce Type: new 
Abstract: We address the problem of identifying functional interactions among stochastic neurons with variable-length memory from their spiking activity. The neuronal network is modeled by a stochastic system of interacting point processes with variable-length memory. Each chain describes the activity of a single neuron, indicating whether it spikes at a given time. One neuron's influence on another can be either excitatory or inhibitory. To identify the existence and nature of an interaction between a neuron and its postsynaptic counterpart, we propose a model selection procedure based on the observation of the spike activity of a finite set of neurons over a finite time. The proposed procedure is also based on the maximum likelihood estimator for the synaptic weight matrix of the network neuronal model. In this sense, we prove the consistency of the maximum likelihood estimator followed by a proof of the consistency of the neighborhood interaction estimation procedure. The effectiveness of the proposed model selection procedure is demonstrated using simulated data, which validates the underlying theory. The method is also applied to analyze spike train data recorded from hippocampal neurons in rats during a visual attention task, where a computational model reconstructs the spiking activity and the results reveal interesting and biologically relevant information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08205v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ricardo F. Ferreira, Matheus E. Pacola, Vitor G. Schiavone, Rodrigo F. O. Pena</dc:creator>
    </item>
    <item>
      <title>CMiNet: R package for learning the Consensus Microbiome Network</title>
      <link>https://arxiv.org/abs/2411.08309</link>
      <description>arXiv:2411.08309v1 Announce Type: new 
Abstract: Understanding complex interactions within microbiomes is essential for exploring their roles in health and disease. However, constructing reliable microbiome networks often poses a challenge due to variations in the output of different network inference algorithms. To address this issue, we present CMiNet, an R package designed to generate a consensus microbiome network by integrating results from multiple established network construction methods. CMiNet incorporates nine widely used algorithms, including Pearson, Spearman, Biweight Midcorrelation (Bicor), SparCC, SpiecEasi, SPRING, GCoDA, and CCLasso, along with a novel algorithm based on conditional mutual information (CMIMN). By combining the strengths of these algorithms, CMiNet generates a single, weighted consensus network that provides a more stable and comprehensive representation of microbial interactions. The package includes customizable functions for network construction, visualization, and analysis, allowing users to explore network structures at different threshold levels and assess connectivity and reliability. CMiNet is designed to handle both quantitative and compositional data, ensuring broad applicability for researchers aiming to understand the intricate relationships within microbiome communities. Availability: Source code is freely available at https://github.com/solislemuslab/CMiNet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08309v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rosa Aghdam, Claudia Solis-Lemus</dc:creator>
    </item>
    <item>
      <title>Equitable Length of Stay Prediction for Patients with Learning Disabilities and Multiple Long-term Conditions Using Machine Learning</title>
      <link>https://arxiv.org/abs/2411.08048</link>
      <description>arXiv:2411.08048v1 Announce Type: cross 
Abstract: People with learning disabilities have a higher mortality rate and premature deaths compared to the general public, as reported in published research in the UK and other countries. This study analyses hospitalisations of 9,618 patients identified with learning disabilities and long-term conditions for the population of Wales using electronic health record (EHR) data sources from the SAIL Databank. We describe the demographic characteristics, prevalence of long-term conditions, medication history, hospital visits, and lifestyle history for our study cohort, and apply machine learning models to predict the length of hospital stays for this cohort. The random forest (RF) model achieved an Area Under the Curve (AUC) of 0.759 (males) and 0.756 (females), a false negative rate of 0.224 (males) and 0.229 (females), and a balanced accuracy of 0.690 (males) and 0.689 (females). After examining model performance across ethnic groups, two bias mitigation algorithms (threshold optimization and the reductions algorithm using an exponentiated gradient) were applied to minimise performance discrepancies. The threshold optimizer algorithm outperformed the reductions algorithm, achieving lower ranges in false positive rate and balanced accuracy for the male cohort across the ethnic groups. This study demonstrates the potential of applying machine learning models with effective bias mitigation approaches on EHR data sources to enable equitable prediction of hospital stays by addressing data imbalances across groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08048v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emeka Abakasanga, Rania Kousovista, Georgina Cosma, Ashley Akbari, Francesco Zaccardi, Navjot Kaur, Danielle Fitt, Gyuchan Thomas Jun, Reza Kiani, Satheesh Gangadharan</dc:creator>
    </item>
    <item>
      <title>Mobility-based Traffic Forecasting in a Multimodal Transport System</title>
      <link>https://arxiv.org/abs/2411.08052</link>
      <description>arXiv:2411.08052v1 Announce Type: cross 
Abstract: We study the analysis of all the movements of the population on the basis of their mobility from one node to another, to observe, measure, and predict the impact of traffic according to this mobility. The frequency of congestion on roads directly or indirectly impacts our economic or social welfare. Our work focuses on exploring some machine learning methods to predict (with a certain probability) traffic in a multimodal transportation network from population mobility data. We analyze the observation of the influence of people's movements on the transportation network and make a likely prediction of congestion on the network based on this observation (historical basis).</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08052v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Henock M. Mboko, Mouhamadou A. M. T. Balde, Babacar M. Ndiaye</dc:creator>
    </item>
    <item>
      <title>We must re-evaluate assumptions about carbon trading for effective climate change mitigation</title>
      <link>https://arxiv.org/abs/2411.08053</link>
      <description>arXiv:2411.08053v1 Announce Type: cross 
Abstract: Effective climate action depends on dismantling the assumptions and oversimplifications that have become the basis of climate policy. The assumption that greenhouse gases (GHG) are fungible and the use of single-point values in normalizing GHG species to CO2-equivalents can propagate inaccuracies in carbon accounting and have already led to failures of carbon offset systems. Separate emission reduction targets and tracking by GHG species are recommended to achieve long-term climate stabilization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08053v1</guid>
      <category>econ.GN</category>
      <category>physics.ao-ph</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alyssa R. Pfadt-Trilling, Marie-Odile P. Fortier</dc:creator>
    </item>
    <item>
      <title>Targeted Maximum Likelihood Estimation for Integral Projection Models in Population Ecology</title>
      <link>https://arxiv.org/abs/2411.08150</link>
      <description>arXiv:2411.08150v1 Announce Type: cross 
Abstract: Integral projection models (IPMs) are widely used to study population growth and the dynamics of demographic structure (e.g. age and size distributions) within a population.These models use data on individuals' growth, survival, and reproduction to predict changes in the population from one time point to the next and use these in turn to ask about long-term growth rates, the sensitivity of that growth rate to environmental factors, and aspects of the long term population such as how much reproduction concentrates in a few individuals; these quantities are not directly measurable from data and must be inferred from the model. Building IPMs requires us to develop models for individual fates over the next time step -- Did they survive? How much did they grow or shrink? Did they Reproduce? -- conditional on their initial state as well as on environmental covariates in a manner that accounts for the unobservable quantities that are are ultimately interested in estimating.Targeted maximum likelihood estimation (TMLE) methods are particularly well-suited to a framework in which we are largely interested in the consequences of models. These build machine learning-based models that estimate the probability distribution of the data we observe and define a target of inference as a function of these. The initial estimate for the distribution is then modified by tilting in the direction of the efficient influence function to both de-bias the parameter estimate and provide more accurate inference. In this paper, we employ TMLE to develop robust and efficient estimators for properties derived from a fitted IPM. Mathematically, we derive the efficient influence function and formulate the paths for the least favorable sub-models. Empirically, we conduct extensive simulations using real data from both long term studies of Idaho steppe plant communities and experimental Rotifer populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08150v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunzhe Zhou, Giles Hooker</dc:creator>
    </item>
    <item>
      <title>Impact of Covid-19 on Taxi Industry and Travel Behavior: A Case Study on Chicago, IL</title>
      <link>https://arxiv.org/abs/2411.08168</link>
      <description>arXiv:2411.08168v1 Announce Type: cross 
Abstract: As the debate over the future of transportation continues in the midst of the COVID-19 pandemic as a deepening global crisis, taxi industry seems to be not spared by the quick and disrupting changes that may arise from the pandemic. The impact is relatively higher in major cities because of the high-density population and transportation congestion. In this study, we used spatial analysis and visualization to investigate the impact of the pandemic on the economics of the taxi industry and travel behavior using trip-by-trip data from the year of 2014 to 2020 in Chicago, IL. Results show that there is a drastic decline in the trips in the central city and airport areas. During the pandemic, people tended to travel longer distances, but travel times were considerably less because of the significant reduction in traffic volumes. Results also showed that the top twenty most popular pick-up and drop-off locations only included Chicago Downtown and OHare International Airport before the pandemic. However, during the pandemic, the top twenty most popular pick-up and drop-off locations is distributed between the Airport, the Downtown, as well as many other areas along Chicago Eastside.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08168v1</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naga Sireesha Chinthala, Jenell Lewis, Sravan Vuppalapati, Khiran Kumar Chidambaram Sivaraman, Chinmay Vivek Toley, Huthaifa Ashqar</dc:creator>
    </item>
    <item>
      <title>TowerDebias: A Novel Debiasing Method based on the Tower Property</title>
      <link>https://arxiv.org/abs/2411.08297</link>
      <description>arXiv:2411.08297v1 Announce Type: cross 
Abstract: Decision-making processes have increasingly come to rely on sophisticated machine learning tools, raising concerns about the fairness of their predictions with respect to any sensitive groups. The widespread use of commercial black-box machine learning models necessitates careful consideration of their legal and ethical implications on consumers. In situations where users have access to these "black-box" models, a key question emerges: how can we mitigate or eliminate the influence of sensitive attributes, such as race or gender? We propose towerDebias (tDB), a novel approach designed to reduce the influence of sensitive variables in predictions made by black-box models. Using the Tower Property from probability theory, tDB aims to improve prediction fairness during the post-processing stage in a manner amenable to the Fairness-Utility Tradeoff. This method is highly flexible, requiring no prior knowledge of the original model's internal structure, and can be extended to a range of different applications. We provide a formal improvement theorem for tDB and demonstrate its effectiveness in both regression and classification tasks, underscoring its impact on the fairness-utility tradeoff.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08297v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Norman Matloff, Aditya Mittal</dc:creator>
    </item>
    <item>
      <title>Confidence intervals for adaptive trial designs I: A methodological review</title>
      <link>https://arxiv.org/abs/2411.08495</link>
      <description>arXiv:2411.08495v1 Announce Type: cross 
Abstract: Regulatory guidance notes the need for caution in the interpretation of confidence intervals (CIs) constructed during and after an adaptive clinical trial. Conventional CIs of the treatment effects are prone to undercoverage (as well as other undesirable properties) in many adaptive designs, because they do not take into account the potential and realised trial adaptations. This paper is the first in a two-part series that explores CIs for adaptive trials. It provides a comprehensive review of the methods to construct CIs for adaptive designs, while the second paper illustrates how to implement these in practice and proposes a set of guidelines for trial statisticians. We describe several classes of techniques for constructing CIs for adaptive clinical trials, before providing a systematic literature review of available methods, classified by the type of adaptive design. As part of this, we assess, through a proposed traffic light system, which of several desirable features of CIs (such as achieving nominal coverage and consistency with the hypothesis test decision) each of these methods holds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08495v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David S. Robertson, Thomas Burnett, Babak Choodari-Oskooei, Munya Dimairo, Michael Grayling, Philip Pallmann, Thomas Jaki</dc:creator>
    </item>
    <item>
      <title>Gaussian Mixture Models Based Augmentation Enhances GNN Generalization</title>
      <link>https://arxiv.org/abs/2411.08638</link>
      <description>arXiv:2411.08638v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have shown great promise in tasks like node and graph classification, but they often struggle to generalize, particularly to unseen or out-of-distribution (OOD) data. These challenges are exacerbated when training data is limited in size or diversity. To address these issues, we introduce a theoretical framework using Rademacher complexity to compute a regret bound on the generalization error and then characterize the effect of data augmentation. This framework informs the design of GMM-GDA, an efficient graph data augmentation (GDA) algorithm leveraging the capability of Gaussian Mixture Models (GMMs) to approximate any distribution. Our approach not only outperforms existing augmentation techniques in terms of generalization but also offers improved time complexity, making it highly suitable for real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08638v1</guid>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yassine Abbahaddou, Fragkiskos D. Malliaros, Johannes F. Lutzeyer, Amine Mohamed Aboussalah, Michalis Vazirgiannis</dc:creator>
    </item>
    <item>
      <title>Mapping Methane -- The Impact of Dairy Farm Practices on Emissions Through Satellite Data and Machine Learning</title>
      <link>https://arxiv.org/abs/2411.08766</link>
      <description>arXiv:2411.08766v1 Announce Type: cross 
Abstract: This study investigates the correlation between dairy farm characteristics and methane concentrations as derived from satellite observations in Eastern Canada. Utilizing data from 11 dairy farms collected between January 2020 and December 2022, we integrated Sentinel-5P satellite methane data with critical farm-level attributes, including herd genetics, feeding practices, and management strategies. Initial analyses revealed significant correlations with methane concentrations, leading to the application of Variance Inflation Factor (VIF) and Principal Component Analysis (PCA) to address multicollinearity and enhance model stability. Subsequently, machine learning models - specifically Random Forest and Neural Networks - were employed to evaluate feature importance and predict methane emissions. Our findings indicate a strong negative correlation between the Estimated Breeding Value (EBV) for protein percentage and methane concentrations, suggesting that genetic selection for higher milk protein content could be an effective strategy for emissions reduction. The integration of atmospheric transport models with satellite data further refined our emission estimates, significantly enhancing accuracy and spatial resolution. This research underscores the potential of advanced satellite monitoring, machine learning techniques, and atmospheric modeling in improving methane emission assessments within the dairy sector. It emphasizes the critical role of farm-specific characteristics in developing effective mitigation strategies. Future investigations should focus on expanding the dataset and incorporating inversion modeling for more precise emission quantification. Balancing ecological impacts with economic viability will be essential for fostering sustainable dairy farming practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08766v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hanqing Bi, Suresh Neethirajan</dc:creator>
    </item>
    <item>
      <title>Confidence intervals for adaptive trial designs II: Case study and practical guidance</title>
      <link>https://arxiv.org/abs/2411.08771</link>
      <description>arXiv:2411.08771v1 Announce Type: cross 
Abstract: In adaptive clinical trials, the conventional confidence interval (CI) for a treatment effect is prone to undesirable properties such as undercoverage and potential inconsistency with the final hypothesis testing decision. Accordingly, as is stated in recent regulatory guidance on adaptive designs, there is the need for caution in the interpretation of CIs constructed during and after an adaptive clinical trial. However, it may be unclear which of the available CIs in the literature are preferable. This paper is the second in a two-part series that explores CIs for adaptive trials. Part I provided a methodological review of approaches to construct CIs for adaptive designs. In this paper (part II), we present an extended case study based around a two-stage group sequential trial, including a comprehensive simulation study of the proposed CIs for this setting. This facilitates an expanded description of considerations around what makes for an effective CI procedure following an adaptive trial. We show that the CIs can have notably different properties. Finally, we propose a set of guidelines for researchers around the choice of CIs and the reporting of CIs following an adaptive design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08771v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David S. Robertson, Thomas Burnett, Babak Choodari-Oskooei, Munya Dimairo, Michael Grayling, Philip Pallmann, Thomas Jaki</dc:creator>
    </item>
    <item>
      <title>Using Linked Micromaps for Evidence-Based Policy</title>
      <link>https://arxiv.org/abs/2411.04211</link>
      <description>arXiv:2411.04211v2 Announce Type: replace 
Abstract: Linked micromaps were originally developed to display geographically indexed statistics in an intuitive way by connecting them to a sequence of small maps. The approach integrates several visualization design principles, such as small multiples, discrete color indexing, and ordering. Linked micromaps allow for other types of data displays that are connected to and conditional on geographic areas. Initial applications of micromaps used data from the National Cancer Institute and the Environmental Protection Agency. In this paper, we will show how linked micromaps can be used to better understand and explore relationships and distributions of statistics linked to US states and Washington, DC. We will compare linked micromaps with other popular data displays of geographic data, such as bubble maps, choropleth maps, and bar charts. We will illustrate how linked micromaps can be used for evidence-based decision-making using data from the Bureau of Labor Statistics, the Census Bureau, and the Economic Research Service. The presentations, R scripts, and the data sets used in this article are available here: https://github.com/wlmcensus/Joint-Statistical-Meetings-Presentation-2024. The work discussed in this article was presented at the Joint Statistical Meetings (JSM) 2024 and the American Association for Public Opinion Research (AAPOR) 2024 Annual Conference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04211v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Randall Powers, John Eltinge, Wendy Martinez, Darcy Steeg Morris</dc:creator>
    </item>
    <item>
      <title>Forecasting macroeconomic data with Bayesian VARs: Sparse or dense? It depends!</title>
      <link>https://arxiv.org/abs/2206.04902</link>
      <description>arXiv:2206.04902v4 Announce Type: replace-cross 
Abstract: Vector autogressions (VARs) are widely applied when it comes to modeling and forecasting macroeconomic variables. In high dimensions, however, they are prone to overfitting. Bayesian methods, more concretely shrinkage priors, have shown to be successful in improving prediction performance. In the present paper, we introduce the semi-global framework, in which we replace the traditional global shrinkage parameter with group-specific shrinkage parameters. We show how this framework can be applied to various shrinkage priors, such as global-local priors and stochastic search variable selection priors. We demonstrate the virtues of the proposed framework in an extensive simulation study and in an empirical application forecasting data of the US economy. Further, we shed more light on the ongoing ``Illusion of Sparsity'' debate, finding that forecasting performances under sparse/dense priors vary across evaluated economic variables and across time frames. Dynamic model averaging, however, can combine the merits of both worlds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.04902v4</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luis Gruber, Gregor Kastner</dc:creator>
    </item>
    <item>
      <title>Neural Networks for Extreme Quantile Regression with an Application to Forecasting of Flood Risk</title>
      <link>https://arxiv.org/abs/2208.07590</link>
      <description>arXiv:2208.07590v4 Announce Type: replace-cross 
Abstract: Risk assessment for extreme events requires accurate estimation of high quantiles that go beyond the range of historical observations. When the risk depends on the values of observed predictors, regression techniques are used to interpolate in the predictor space. We propose the EQRN model that combines tools from neural networks and extreme value theory into a method capable of extrapolation in the presence of complex predictor dependence. Neural networks can naturally incorporate additional structure in the data. We develop a recurrent version of EQRN that is able to capture complex sequential dependence in time series. We apply this method to forecast flood risk in the Swiss Aare catchment. It exploits information from multiple covariates in space and time to provide one-day-ahead predictions of return levels and exceedance probabilities. This output complements the static return level from a traditional extreme value analysis, and the predictions are able to adapt to distributional shifts as experienced in a changing climate. Our model can help authorities to manage flooding more effectively and to minimize their disastrous impacts through early warning systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.07590v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/24-AOAS1907</arxiv:DOI>
      <arxiv:journal_reference>Annals of Applied Statistics 18(4), 2818-2839 (2024)</arxiv:journal_reference>
      <dc:creator>Olivier C. Pasche, Sebastian Engelke</dc:creator>
    </item>
    <item>
      <title>Improving randomized controlled trial analysis via data-adaptive borrowing</title>
      <link>https://arxiv.org/abs/2306.16642</link>
      <description>arXiv:2306.16642v2 Announce Type: replace-cross 
Abstract: In recent years, real-world external controls have grown in popularity as a tool to empower randomized placebo-controlled trials, particularly in rare diseases or cases where balanced randomization is unethical or impractical. However, as external controls are not always comparable to the trials, direct borrowing without scrutiny may heavily bias the treatment effect estimator. Our paper proposes a data-adaptive integrative framework capable of preventing unknown biases of the external controls. The adaptive nature is achieved by dynamically sorting out a comparable subset of the external controls via bias penalization. Our proposed method can simultaneously achieve (a) the semiparametric efficiency bound when the external controls are comparable and (b) selective borrowing that mitigates the impact of the existence of incomparable external controls. Furthermore, we establish statistical guarantees, including consistency, asymptotic distribution, and inference, providing type-I error control and good power. Extensive simulations and two real-data applications show that the proposed method leads to improved performance over the trial-only estimator across various bias-generating scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16642v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyin Gao, Shu Yang, Mingyang Shan, Wenyu Ye, Ilya Lipkovich, Douglas Faries</dc:creator>
    </item>
  </channel>
</rss>
