<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Sep 2025 01:18:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Navigating the Data Space Landscape: Concepts, Applications, and Future Directions</title>
      <link>https://arxiv.org/abs/2509.06983</link>
      <description>arXiv:2509.06983v1 Announce Type: new 
Abstract: This paper explores the evolving landscape of data spaces, focusing on key concepts, practical applications, and emerging future directions. It begins by introducing the foundational principles that underpin data space architectures, emphasizing their importance in facilitating secure and efficient data exchange. The core design principles and essential building blocks that form the backbone of data-space systems are then examined. Several real-world implementations are presented, showcasing how data spaces are applied across various industries to address challenges such as data sovereignty, interoperability, and trust. The paper concludes by discussing future directions, proposing that leveraging semantic data models can significantly enhance interoperability and data integration within data spaces. Furthermore, it suggests exploring the implementation of SPARQL as a sophisticated authorization layer to improve security and granular control over data access. This research provides a comprehensive understanding of the current state of data spaces and aims to guide future advancements in this rapidly evolving field by highlighting the potential of semantic data and SPARQL-based authorization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06983v1</guid>
      <category>cs.DB</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bojana Marojevikj, Riste Stojanov</dc:creator>
    </item>
    <item>
      <title>Private Queries with Sigma-Counting</title>
      <link>https://arxiv.org/abs/2509.07018</link>
      <description>arXiv:2509.07018v1 Announce Type: new 
Abstract: Many data applications involve counting queries, where a client specifies a feasible range of variables and a database returns the corresponding item counts. A program that produces the counts of different queries often risks leaking sensitive individual-level information. A popular approach to enhance data privacy is to return a noisy version of the actual count. It is typically achieved by adding independent noise to each query and then control the total privacy budget within a period. This approach may be limited in the number of queries and output accuracy in practice. Also, the returned counts do not maintain the total order for nested queries, an important feature in many applications. This work presents the design and analysis of a new method, sigma-counting, that addresses these challenges. Sigma-counting uses the notion of sigma-algebra to construct privacy-preserving counting queries. We show that the proposed concepts and methods can significantly improve output accuracy while maintaining a desired privacy level in the presence of massive queries to the same data. We also discuss how the technique can be applied to address large and time-varying datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07018v1</guid>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jun Gao, Jie Ding</dc:creator>
    </item>
    <item>
      <title>JOINT: Join Optimization and Inference via Network Traversal</title>
      <link>https://arxiv.org/abs/2509.07230</link>
      <description>arXiv:2509.07230v1 Announce Type: new 
Abstract: Traditional relational databases require users to manually specify join keys and assume exact matches between column names and values. In practice, this limits joinability across fragmented or inconsistently named tables. We propose a fuzzy join framework that automatically identifies joinable column pairs and traverses indirect (multi-hop) join paths across multiple databases. Our method combines column name similarity with row-level fuzzy value overlap, computes edge weights using negative log-transformed Jaccard scores, and performs join path discovery via graph traversal. Experiments on synthetic healthcare-style databases demonstrate the system's ability to recover valid joins despite fuzzified column names and partial value mismatches. This research has direct applications in data integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07230v1</guid>
      <category>cs.DB</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Szu-Yun Ko, Ethan Chen, Bo-Cian Chang, Alan Shu-Luen Chang</dc:creator>
    </item>
    <item>
      <title>Filtered Approximate Nearest Neighbor Search: A Unified Benchmark and Systematic Experimental Study [Experiment, Analysis &amp; Benchmark]</title>
      <link>https://arxiv.org/abs/2509.07789</link>
      <description>arXiv:2509.07789v1 Announce Type: new 
Abstract: For a given dataset $\mathcal{D}$ and structured label $f$, the goal of Filtered Approximate Nearest Neighbor Search (FANNS) algorithms is to find top-$k$ points closest to a query that satisfy label constraints, while ensuring both recall and QPS (Queries Per Second). In recent years, many FANNS algorithms have been proposed. However, the lack of a systematic investigation makes it difficult to understand their relative strengths and weaknesses. Additionally, we found that: (1) FANNS algorithms have coupled, dataset-dependent parameters, leading to biased comparisons. (2) Key impact factors are rarely analyzed systematically, leaving unclear when each algorithm performs well. (3) Disparate datasets, workloads, and biased experiment designs make cross-algorithm comparisons unreliable. Thus, a comprehensive survey and benchmark for FANNS is crucial to achieve the following goals: designing a fair evaluation and clarifying the classification of algorithms, conducting in-depth analysis of their performance, and establishing a unified benchmark. First, we propose a taxonomy (dividing methods into \textit{filter-then-search}, \textit{search-then-filter}, \textit{hybrid-search}) and a systematic evaluation framework, integrating unified parameter tuning and standardized filtering across algorithms to reduce implementation-induced performance variations and reflect core trade-offs. Then, we conduct a comprehensive empirical study to analyze how query difficulty and dataset properties impact performance, evaluating robustness under pressures like filter selectivity, Recall@k, and scalability to clarify each method's strengths. Finally, we establish a standardized benchmark with real-world datasets and open-source related resources to ensure reproducible future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07789v1</guid>
      <category>cs.DB</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jiayang Shi, Yuzheng Cai, Weiguo Zheng</dc:creator>
    </item>
    <item>
      <title>Proximity Graphs for Similarity Search: Fast Construction, Lower Bounds, and Euclidean Separation</title>
      <link>https://arxiv.org/abs/2509.07732</link>
      <description>arXiv:2509.07732v1 Announce Type: cross 
Abstract: Proximity graph-based methods have emerged as a leading paradigm for approximate nearest neighbor (ANN) search in the system community. This paper presents fresh insights into the theoretical foundation of these methods. We describe an algorithm to build a proximity graph for $(1+\epsilon)$-ANN search that has $O((1/\epsilon)^\lambda \cdot n \log \Delta)$ edges and guarantees $(1/\epsilon)^\lambda \cdot \text{polylog }\Delta$ query time. Here, $n$ and $\Delta$ are the size and aspect ratio of the data input, respectively, and $\lambda = O(1)$ is the doubling dimension of the underlying metric space. Our construction time is near-linear to $n$, improving the $\Omega(n^2)$ bounds of all previous constructions. We complement our algorithm with lower bounds revealing an inherent limitation of proximity graphs: the number of edges needs to be at least $\Omega((1/\epsilon)^\lambda \cdot n + n \log \Delta)$ in the worst case, up to a subpolynomial factor. The hard inputs used in our lower-bound arguments are non-geometric, thus prompting the question of whether improvement is possible in the Euclidean space (a key subclass of metric spaces). We provide an affirmative answer by using geometry to reduce the graph size to $O((1/\epsilon)^\lambda \cdot n)$ while preserving nearly the same query and construction time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07732v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shangqi Lu, Yufei Tao</dc:creator>
    </item>
    <item>
      <title>dciWebMapper2: Enhancing the dciWebMapper framework toward integrated, interactive visualization of linked multi-type maps, charts, and spatial statistics and analysis</title>
      <link>https://arxiv.org/abs/2509.07897</link>
      <description>arXiv:2509.07897v1 Announce Type: cross 
Abstract: As interactive web-based geovisualization becomes increasingly vital across disciplines, there is a growing need for open-source frameworks that support dynamic, multi-attribute spatial analysis and accessible design. This paper introduces dciWebMapper2, a significant expansion of the original dciWebMapper framework, designed to enable exploratory analysis across domains such as climate justice, food access, and social vulnerability. The enhanced framework integrates multiple map types, including choropleth, proportional symbol, small multiples, and heatmaps, with linked statistical charts (e.g., scatter plots, boxplots) and time sliders, all within a coordinated-view environment. Dropdown-based controls allow flexible, high-dimensional comparisons while maintaining visual clarity. Grounded in cartographic and information visualization principles, dciWebMapper2 is fully open-source, self-contained, and server-free, supporting modularity, reproducibility, and long-term sustainability. Three applied use cases demonstrate its adaptability and potential to democratize interactive web cartography. This work offers a versatile foundation for inclusive spatial storytelling and transparent geospatial analysis in research, education, and civic engagement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07897v1</guid>
      <category>cs.HC</category>
      <category>cs.DB</category>
      <category>cs.GR</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarigai Sarigai, Liping Yang, Katie Slack, Carolyn Fish, Michaela Buenemann, Qiusheng Wu, Yan Lin, Joseph A. Cook, David Jacobs</dc:creator>
    </item>
    <item>
      <title>DistJoin: A Decoupled Join Cardinality Estimator based on Adaptive Neural Predicate Modulation</title>
      <link>https://arxiv.org/abs/2503.08994</link>
      <description>arXiv:2503.08994v3 Announce Type: replace 
Abstract: Research on learned cardinality estimation has made significant progress in recent years. However, existing methods still face distinct challenges that hinder their practical deployment in production environments. We define these challenges as the ``Trilemma of Cardinality Estimation'', where learned cardinality estimation methods struggle to balance generality, accuracy, and updatability. To address these challenges, we introduce DistJoin, a join cardinality estimator based on efficient distribution prediction using multi-autoregressive models. Our contributions are threefold: (1) We propose a method to estimate join cardinality by leveraging the probability distributions of individual tables in a decoupled manner. (2) To meet the requirements of efficiency for DistJoin, we develop Adaptive Neural Predicate Modulation (ANPM), a high-throughput distribution estimation model. (3) We demonstrate that an existing similar approach suffers from variance accumulation issues by formal variance analysis. To mitigate this problem, DistJoin employs a selectivity-based approach to infer join cardinality, effectively reducing variance. In summary, DistJoin not only represents the first data-driven method to support both equi and non-equi joins simultaneously but also demonstrates superior accuracy while enabling fast and flexible updates. The experimental results demonstrate that DistJoin achieves the highest accuracy, robustness to data updates, generality, and comparable update and inference speed relative to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08994v3</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaixin Zhang, Hongzhi Wang, Ziqi Li, Yabin Lu, Yingze Li, Yu Yan, Yiming Guan</dc:creator>
    </item>
    <item>
      <title>MEBench: Benchmarking Large Language Models for Cross-Document Multi-Entity Question Answering</title>
      <link>https://arxiv.org/abs/2502.18993</link>
      <description>arXiv:2502.18993v2 Announce Type: replace-cross 
Abstract: Multi-entity question answering (MEQA) represents significant challenges for large language models (LLM) and retrieval-augmented generation (RAG) systems, which frequently struggle to consolidate scattered information across diverse documents. While existing methods excel at single-document comprehension, they often struggle with cross-document aggregation, particularly when resolving entity-dense questions like "What is the distribution of ACM Fellows among various fields of study?", which require integrating entity-centric insights from heterogeneous sources (e.g., Wikipedia pages). To address this gap, we introduce MEBench, a novel multi-document, multi-entity benchmark designed to systematically evaluate LLMs' capacity to retrieve, consolidate, and reason over fragmented information. Our benchmark comprises 4,780 questions which are systematically categorized into three primary categories, further divided into eight distinct types, ensuring broad coverage of real-world multi-entity reasoning scenarios. Our experiments on state-of-the-art LLMs (e.g., GPT-4, Llama-3) and RAG pipelines reveal critical limitations: even advanced models achieve only 59% accuracy on MEBench. Our benchmark emphasizes the importance of completeness and factual precision of information extraction in MEQA tasks, using Entity-Attributed F1 (EA-F1) metric for granular evaluation of entity-level correctness and attribution validity. MEBench not only highlights systemic weaknesses in current LLM frameworks but also provides a foundation for advancing robust, entity-aware QA architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18993v2</guid>
      <category>cs.CL</category>
      <category>cs.DB</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Teng Lin, Yuyu Luo, Nan Tang</dc:creator>
    </item>
    <item>
      <title>Smaller and More Flexible Cuckoo Filters</title>
      <link>https://arxiv.org/abs/2505.05847</link>
      <description>arXiv:2505.05847v3 Announce Type: replace-cross 
Abstract: Cuckoo filters are space-efficient approximate set membership data structures with a controllable false positive rate (FPR) and zero false negatives, similar to Bloom filters. In contrast to Bloom filters, Cuckoo filters store multi-bit fingerprints of keys in a hash table using variants of Cuckoo hashing, allowing each fingerprint to be stored at a small number of possible locations. Existing Cuckoo filters use fingerprints of $(k+3)$ bits per key and an additional space overhead factor of at least $1.05$ to achieve an FPR of $2^{-k}$. For $k=10$, this amounts to $1.365\, kn$ bits to store $n$ keys, which is better than $1.443\, kn$ bits for Bloom filters. The $+3$ for the fingerprint size is required to balance out the multiplied FPR caused by looking for the fingerprint at several locations. In the original Cuckoo filter, the number of hash table buckets is restricted to a power of 2, which may lead to much larger space overheads, up to $2.1\, (1+3/k)\, kn$ bits.
  We present two improvements of Cuckoo filters. First, we remove the restriction that the number of buckets must be a power of 2 by using a different placement strategy. Second, we reduce the space overhead factor of Cuckoo filters to $1.06 \, (1+2/k)$ by using overlapping windows instead of disjoint buckets to maintain the load threshold of the hash table, while reducing the number of alternative slots where any fingerprint may be found.
  A detailed evaluation demonstrates that the alternative memory layout based on overlapping windows decreases the size of Cuckoo filters not only in theory, but also in practice. A comparison with other state-of-the art filter types, Prefix filters and Vector Quotient filters (VQFs), shows that the reduced space overhead makes windowed Cuckoo filters the smallest filters supporting online insertions, with similarly fast queries, but longer insertion times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05847v3</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johanna Elena Schmitz, Jens Zentgraf, Sven Rahmann</dc:creator>
    </item>
  </channel>
</rss>
