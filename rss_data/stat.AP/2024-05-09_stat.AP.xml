<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 09 May 2024 04:01:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 09 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Clustering Retail Products Based on Customer Behaviour</title>
      <link>https://arxiv.org/abs/2405.05218</link>
      <description>arXiv:2405.05218v1 Announce Type: new 
Abstract: The categorization of retail products is essential for the business decision-making process. It is a common practice to classify products based on their quantitative and qualitative characteristics. In this paper we use a purely data-driven approach. Our clustering of products is based exclusively on the customer behaviour. We propose a method for clustering retail products using market basket data. Our model is formulated as an optimization problem which is solved by a genetic algorithm. It is demonstrated on simulated data how our method behaves in different settings. The application using real data from a Czech drugstore company shows that our method leads to similar results in comparison with the classification by experts. The number of clusters is a parameter of our algorithm. We demonstrate that if more clusters are allowed than the original number of categories is, the method yields additional information about the structure of the product categorization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05218v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.asoc.2017.02.004</arxiv:DOI>
      <arxiv:journal_reference>(2017) Applied Soft Computing, 60, 752-762</arxiv:journal_reference>
      <dc:creator>Vladim\'ir Hol\'y, Ond\v{r}ej Sokol, Michal \v{C}ern\'y</dc:creator>
    </item>
    <item>
      <title>Are Economically Advanced Countries More Efficient in Basic and Applied Research?</title>
      <link>https://arxiv.org/abs/2405.05227</link>
      <description>arXiv:2405.05227v1 Announce Type: new 
Abstract: Research and development (R&amp;D) of countries play a major role in a long-term development of the economy. We measure the R&amp;D efficiency of all 28 member countries of the European Union in the years 2008--2014. Super-efficient data envelopment analysis (DEA) based on robustness of classification into efficient and inefficient units is adopted. We use the number of citations as output of basic research, the number of patents as output of applied research and R&amp;D expenditures with manpower as inputs. To meet DEA assumptions and to capture R&amp;D characteristics, we analyze a homogeneous sample of countries, adjust prices using purchasing power parity and consider time lag between inputs and outputs. We find that the efficiency of general R&amp;D is higher for countries with higher GDP per capita. This relation also holds for specialized efficiencies of basic and applied research. However, it is much stronger for applied research suggesting its outputs are more easily distinguished and captured. Our findings are important in the evaluation of research and policy making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05227v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10100-018-0559-2</arxiv:DOI>
      <arxiv:journal_reference>(2018) Central European Journal of Operations Research, 26(4), 933-950</arxiv:journal_reference>
      <dc:creator>Vladim\'ir Hol\'y, Karel \v{S}afr</dc:creator>
    </item>
    <item>
      <title>Inference With Combining Rules From Multiple Differentially Private Synthetic Datasets</title>
      <link>https://arxiv.org/abs/2405.04769</link>
      <description>arXiv:2405.04769v1 Announce Type: cross 
Abstract: Differential privacy (DP) has been accepted as a rigorous criterion for measuring the privacy protection offered by random mechanisms used to obtain statistics or, as we will study here, synthetic datasets from confidential data. Methods to generate such datasets are increasingly numerous, using varied tools including Bayesian models, deep neural networks and copulas. However, little is still known about how to properly perform statistical inference with these differentially private synthetic (DIPS) datasets. The challenge is for the analyses to take into account the variability from the synthetic data generation in addition to the usual sampling variability. A similar challenge also occurs when missing data is imputed before analysis, and statisticians have developed appropriate inference procedures for this case, which we tend extended to the case of synthetic datasets for privacy. In this work, we study the applicability of these procedures, based on combining rules, to the analysis of DIPS datasets. Our empirical experiments show that the proposed combining rules may offer accurate inference in certain contexts, but not in all cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04769v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leila Nombo, Anne-Sophie Charest</dc:creator>
    </item>
    <item>
      <title>Testing the Fairness-Improvability of Algorithms</title>
      <link>https://arxiv.org/abs/2405.04816</link>
      <description>arXiv:2405.04816v1 Announce Type: cross 
Abstract: Many algorithms have a disparate impact in that their benefits or harms fall disproportionately on certain social groups. Addressing an algorithm's disparate impact can be challenging, however, because it is not always clear whether there exists an alternative more-fair algorithm that does not compromise on other key objectives such as accuracy or profit. Establishing the improvability of algorithms with respect to multiple criteria is of both conceptual and practical interest: in many settings, disparate impact that would otherwise be prohibited under US federal law is permissible if it is necessary to achieve a legitimate business interest. The question is how a policy maker can formally substantiate, or refute, this necessity defense. In this paper, we provide an econometric framework for testing the hypothesis that it is possible to improve on the fairness of an algorithm without compromising on other pre-specified objectives. Our proposed test is simple to implement and can incorporate any exogenous constraint on the algorithm space. We establish the large-sample validity and consistency of our test, and demonstrate its use empirically by evaluating a healthcare algorithm originally considered by Obermeyer et al. (2019). In this demonstration, we find strong statistically significant evidence that it is possible to reduce the algorithm's disparate impact without compromising on the accuracy of its predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04816v1</guid>
      <category>econ.EM</category>
      <category>cs.DS</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Auerbach, Annie Liang, Max Tabord-Meehan, Kyohei Okumura</dc:creator>
    </item>
    <item>
      <title>Dependence-based fuzzy clustering of functional time series</title>
      <link>https://arxiv.org/abs/2405.04904</link>
      <description>arXiv:2405.04904v1 Announce Type: cross 
Abstract: Time series clustering is an important data mining task with a wide variety of applications. While most methods focus on time series taking values on the real line, very few works consider functional time series. However, functional objects frequently arise in many fields, such as actuarial science, demography or finance. Functional time series are indexed collections of infinite-dimensional curves viewed as random elements taking values in a Hilbert space. In this paper, the problem of clustering functional time series is addressed. To this aim, a distance between functional time series is introduced and used to construct a clustering procedure. The metric relies on a measure of serial dependence which can be seen as a natural extension of the classical quantile autocorrelation function to the functional setting. Since the dynamics of the series may vary over time, we adopt a fuzzy approach, which enables the procedure to locate each series into several clusters with different membership degrees. The resulting algorithm can group series generated from similar stochastic processes, reaching accurate results with series coming from a broad variety of functional models and requiring minimum hyperparameter tuning. Several simulation experiments show that the method exhibits a high clustering accuracy besides being computationally efficient. Two interesting applications involving high-frequency financial time series and age-specific mortality improvement rates illustrate the potential of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04904v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Angel Lopez-Oriona, Ying Sun, Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>The Impact of TV Advertising on Website Traffic</title>
      <link>https://arxiv.org/abs/2112.08530</link>
      <description>arXiv:2112.08530v2 Announce Type: replace 
Abstract: We propose a modeling procedure for estimating immediate responses to TV ads and evaluating the factors influencing their size. First, we capture diurnal and seasonal patterns of website visits using the kernel smoothing method. Second, we estimate a gradual increase in website visits after an ad using the maximum likelihood method. Third, we analyze the non-linear dependence of the estimated increase in website visits on characteristics of the ads using the random forest method. The proposed methodology is applied to a dataset containing minute-by-minute organic website visits and detailed characteristics of TV ads for an e-commerce company in 2019. The results show that people are indeed willing to switch between screens and multitask. Moreover, the time of the day, the TV channel, and the advertising motive play a great role in the impact of the ads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.08530v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/asmb.2850</arxiv:DOI>
      <arxiv:journal_reference>(2024) Applied Stochastic Models in Business and Industry</arxiv:journal_reference>
      <dc:creator>Luk\'a\v{s} Veverka, Vladim\'ir Hol\'y</dc:creator>
    </item>
    <item>
      <title>Ranking-Based Second Stage in Data Envelopment Analysis: An Application to Research Efficiency in Higher Education</title>
      <link>https://arxiv.org/abs/2307.01869</link>
      <description>arXiv:2307.01869v2 Announce Type: replace 
Abstract: An alternative approach for the panel second stage of data envelopment analysis (DEA) is presented in this paper. Instead of efficiency scores, we propose to model rankings in the second stage using a dynamic ranking model in the score-driven framework. We argue that this approach is suitable to complement traditional panel regression as a robustness check. To demonstrate the proposed approach, we determine research efficiency of higher education systems at country level by examining scientific publications and analyze its relation to good governance. The proposed approach confirms positive relation to the Voice and Accountability indicator, as found by the standard panel linear regression, while suggesting caution regarding the Government Effectiveness indicator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.01869v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladim\'ir Hol\'y</dc:creator>
    </item>
    <item>
      <title>Non-locality and Spillover Effects of Residential Flood Damage on Community Recovery: Insights from High-resolution Flood Claim and Mobility Data</title>
      <link>https://arxiv.org/abs/2405.03874</link>
      <description>arXiv:2405.03874v2 Announce Type: replace 
Abstract: Examining the relationship between vulnerability of the built environment and community recovery is crucial for understanding disaster resilience. Yet, this relationship is rather neglected in the existing literature due to previous limitations in the availability of empirical datasets needed for such analysis. In this study, we combine fine-resolution flood damage claims data (composed of both insured and uninsured losses) and human mobility data (composed of millions of movement trajectories) during the 2017 Hurricane Harvey in Harris County, Texas, to specify the extent to which vulnerability of the built environment (i.e., flood property damage) affects community recovery (based on the speed of human mobility recovery) locally and regionally. We examine this relationship using a spatial lag, spatial reach, and spatial decay models to measure the extent of spillover effects of residential damage on community recovery. The findings show that: first, the severity of residential damage significantly affects the speed of community recovery. A greater extent of residential damage suppresses community recovery not only locally but also in the surrounding areas. Second, the spatial spillover effect of residential damage on community recovery speed decays with distance from the highly damaged areas. Third, spatial areas display heterogeneous spatial decay coefficients, which are associated with urban structure features such as the density of points-of-interest facilities and roads. These findings provide a novel data-driven characterization of the spatial diffusion of residential flood damage effects on community recovery and move us closer to a better understanding of complex spatial processes that shape community resilience to hazards. This study also provides valuable insights for emergency managers and public officials seeking to mitigate the non-local effects of residential damage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03874v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Junwei Ma, Russell Blessing, Samuel Brody, Ali Mostafavi</dc:creator>
    </item>
    <item>
      <title>Demand and Welfare Analysis in Discrete Choice Models with Social Interactions</title>
      <link>https://arxiv.org/abs/1905.04028</link>
      <description>arXiv:1905.04028v2 Announce Type: replace-cross 
Abstract: Many real-life settings of consumer-choice involve social interactions, causing targeted policies to have spillover-effects. This paper develops novel empirical tools for analyzing demand and welfare-effects of policy-interventions in binary choice settings with social interactions. Examples include subsidies for health-product adoption and vouchers for attending a high-achieving school. We establish the connection between econometrics of large games and Brock-Durlauf-type interaction models, under both I.I.D. and spatially correlated unobservables. We develop new convergence results for associated beliefs and estimates of preference-parameters under increasing-domain spatial asymptotics. Next, we show that even with fully parametric specifications and unique equilibrium, choice data, that are sufficient for counterfactual demand-prediction under interactions, are insufficient for welfare-calculations. This is because distinct underlying mechanisms producing the same interaction coefficient can imply different welfare-effects and deadweight-loss from a policy-intervention. Standard index-restrictions imply distribution-free bounds on welfare. We illustrate our results using experimental data on mosquito-net adoption in rural Kenya.</description>
      <guid isPermaLink="false">oai:arXiv.org:1905.04028v2</guid>
      <category>econ.EM</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Debopam Bhattacharya, Pascaline Dupas, Shin Kanaya</dc:creator>
    </item>
    <item>
      <title>Rethinking recidivism through a causal lens</title>
      <link>https://arxiv.org/abs/2011.11483</link>
      <description>arXiv:2011.11483v4 Announce Type: replace-cross 
Abstract: Predictive modeling of criminal recidivism, or whether people will re-offend in the future, has a long and contentious history. Modern causal inference methods allow us to move beyond prediction and target the "treatment effect" of a specific intervention on an outcome in an observational dataset. In this paper, we look specifically at the effect of incarceration (prison time) on recidivism, using a well-known dataset from North Carolina. Two popular causal methods for addressing confounding bias are explained and demonstrated: directed acyclic graph (DAG) adjustment and double machine learning (DML), including a sensitivity analysis for unobserved confounders. We find that incarceration has a detrimental effect on recidivism, i.e., longer prison sentences make it more likely that individuals will re-offend after release, although this conclusion should not be generalized beyond the scope of our data. We hope that this case study can inform future applications of causal inference to criminal justice analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2011.11483v4</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vik Shirvaikar, Choudur Lakshminarayan</dc:creator>
    </item>
    <item>
      <title>Time-Varying Identification of Monetary Policy Shocks</title>
      <link>https://arxiv.org/abs/2311.05883</link>
      <description>arXiv:2311.05883v4 Announce Type: replace-cross 
Abstract: We propose a new Bayesian heteroskedastic Markov-switching structural vector autoregression with data-driven time-varying identification. The model selects alternative exclusion restrictions over time and, as a condition for the search, allows to verify identification through heteroskedasticity within each regime. Based on four alternative monetary policy rules, we show that a monthly six-variable system supports time variation in US monetary policy shock identification. In the sample-dominating first regime, systematic monetary policy follows a Taylor rule extended by the term spread, effectively curbing inflation. In the second regime, occurring after 2000 and gaining more persistence after the global financial and COVID crises, it is characterized by a money-augmented Taylor rule. This regime's unconventional monetary policy provides economic stimulus, features the liquidity effect, and is complemented by a pure term spread shock. Absent the specific monetary policy of the second regime, inflation would be over one percentage point higher on average after 2008.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05883v4</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annika Camehl (Erasmus University Rotterdam), Tomasz Wo\'zniak (University of Melbourne)</dc:creator>
    </item>
    <item>
      <title>Statistical Response of ENSO Complexity to Initial Condition and Model Parameter Perturbations</title>
      <link>https://arxiv.org/abs/2401.03281</link>
      <description>arXiv:2401.03281v2 Announce Type: replace-cross 
Abstract: Studying the response of a climate system to perturbations has practical significance. Standard methods in computing the trajectory-wise deviation caused by perturbations may suffer from the chaotic nature that makes the model error dominate the true response after a short lead time. Statistical response, which computes the return described by the statistics, provides a systematic way of reaching robust outcomes with an appropriate quantification of the uncertainty and extreme events. In this paper, information theory is applied to compute the statistical response and find the most sensitive perturbation direction of different El Ni\~no-Southern Oscillation (ENSO) events to initial value and model parameter perturbations. Depending on the initial phase and the time horizon, different state variables contribute to the most sensitive perturbation direction. While initial perturbations in sea surface temperature (SST) and thermocline depth usually lead to the most significant response of SST at short- and long-range, respectively, initial adjustment of the zonal advection can be crucial to trigger strong statistical responses at medium-range around 5 to 7 months, especially at the transient phases between El Ni\~no and La Ni\~na. It is also shown that the response in the variance triggered by external random forcing perturbations, such as the wind bursts, often dominates the mean response, making the resulting most sensitive direction very different from the trajectory-wise methods. Finally, despite the strong non-Gaussian climatology distributions, using Gaussian approximations in the information theory is efficient and accurate for computing the statistical response, allowing the method to be applied to sophisticated operational systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03281v2</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marios Andreou, Nan Chen</dc:creator>
    </item>
  </channel>
</rss>
