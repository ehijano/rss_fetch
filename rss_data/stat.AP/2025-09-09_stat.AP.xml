<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Sep 2025 01:33:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Modeling Spatially Correlated Failure-time Data Under Two Distance Functions with an Application to Titan GPU Data</title>
      <link>https://arxiv.org/abs/2509.05444</link>
      <description>arXiv:2509.05444v1 Announce Type: new 
Abstract: One common approach to statistical analysis of spatially correlated data relies on defining a correlation structure based solely on unknown parameters and the physical distance between the locations of observed values. However, some data have a complex spatial structure that cannot be adequately described with the physical distance alone. In this work, the spatial failure-time data of focus contains information on GPUs that are connected through a network fabric topology that differs from their physical layout and that is expected to introduce additional correlations. The proposed lifetime regression model includes random effects capturing the dependency due to physical location as well as random effects explaining the dependency due to logical connections between GPUs. The analysis of this GPU dataset serves as an example of models with multiple spatial random effects and the ideas presented can be extended to other applications with complex spatial structures. A Bayesian modeling scheme is recommended for this class of analyses. The examples in this work use the software package, Stan, to produce Markov chain Monte Carlo draws for parameter estimation. This modeling effort is validated through simulation which demonstrates accuracy in statistical inference. We also apply the developed framework to the large-scale Titan GPU failure time data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05444v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jared M. Clark, Jie Min, Yueyao Wang, Yili Hong, George Ostrouchov</dc:creator>
    </item>
    <item>
      <title>Sparse Sensor Allocation for Inverse Problems of Detecting Sparse Leaking Emission Sources</title>
      <link>https://arxiv.org/abs/2509.05559</link>
      <description>arXiv:2509.05559v1 Announce Type: new 
Abstract: This paper investigates the sparse optimal allocation of sensors for detecting sparse leaking emission sources. Because of the non-negativity of emission rates, uncertainty associated with parameters in the forward model, and sparsity of leaking emission sources, the classical linear Gaussian Bayesian inversion setup is limited and no closed-form solutions are available. By incorporating the non-negativity constraints on emission rates, relaxing the Gaussian distributional assumption, and considering the parameter uncertainties associated with the forward model, this paper provides comprehensive investigations, technical details, in-depth discussions and implementation of the optimal sensor allocation problem leveraging a bilevel optimization framework. The upper-level problem determines the optimal sensor locations by minimizing the Integrated Mean Squared Error (IMSE) of the estimated emission rates over uncertain wind conditions, while the lower-level problem solves an inverse problem that estimates the emission rates. Two algorithms, including the repeated Sample Average Approximation (rSAA) and the Stochastic Gradient Descent based bilevel approximation (SBA), are thoroughly investigated. It is shown that the proposed approach can further reduce the IMSE of the estimated emission rates starting from various initial sensor deployment generated by existing approaches. Convergence analysis is performed to obtain the performance guarantee, and numerical investigations show that the proposed approach can allocate sensors according to the parameters and output of the forward model. Computationally efficient code with GPU acceleration is available on GitHub so that the approach readily applicable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05559v1</guid>
      <category>stat.AP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xinchao Liu, Youngdeok Hwang, Dzung Phan, Levente Klein, Xiao Liu, Kyongmin Yeo</dc:creator>
    </item>
    <item>
      <title>Robust Analysis for Resilient AI System</title>
      <link>https://arxiv.org/abs/2509.06172</link>
      <description>arXiv:2509.06172v1 Announce Type: new 
Abstract: Operational hazards in Manufacturing Industrial Internet (MII) systems generate severe data outliers that cripple traditional statistical analysis. This paper proposes a novel robust regression method, DPD-Lasso, which integrates Density Power Divergence with Lasso regularization to analyze contaminated data from AI resilience experiments. We develop an efficient iterative algorithm to overcome previous computational bottlenecks. Applied to an MII testbed for Aerosol Jet Printing, DPD-Lasso provides reliable, stable performance on both clean and outlier-contaminated data, accurately quantifying hazard impacts. This work establishes robust regression as an essential tool for developing and validating resilient industrial AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06172v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Wang, Ran Jin, Lulu Kang</dc:creator>
    </item>
    <item>
      <title>Intelligent Manufacturing Support: Specialized LLMs for Composite Material Processing and Equipment Operation</title>
      <link>https://arxiv.org/abs/2509.06734</link>
      <description>arXiv:2509.06734v1 Announce Type: new 
Abstract: Engineering educational curriculum and standards cover many material and manufacturing options. However, engineers and designers are often unfamiliar with certain composite materials or manufacturing techniques. Large language models (LLMs) could potentially bridge the gap. Their capacity to store and retrieve data from large databases provides them with a breadth of knowledge across disciplines. However, their generalized knowledge base can lack targeted, industry-specific knowledge. To this end, we present two LLM-based applications based on the GPT-4 architecture: (1) The Composites Guide: a system that provides expert knowledge on composites material and connects users with research and industry professionals who can provide additional support and (2) The Equipment Assistant: a system that provides guidance for manufacturing tool operation and material characterization. By combining the knowledge of general AI models with industry-specific knowledge, both applications are intended to provide more meaningful information for engineers. In this paper, we discuss the development of the applications and evaluate it through a benchmark and two informal user studies. The benchmark analysis uses the Rouge and Bertscore metrics to evaluate our model performance against GPT-4o. The results show that GPT-4o and the proposed models perform similarly or better on the ROUGE and BERTScore metrics. The two user studies supplement this quantitative evaluation by asking experts to provide qualitative and open-ended feedback about our model performance on a set of domain-specific questions. The results of both studies highlight a potential for more detailed and specific responses with the Composites Guide and the Equipment Assistant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06734v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gunnika Kapoor, Komal Chawla, Tirthankar Ghosal, Kris Villez, Dan Coughlin, Tyden Rucker, Vincent Paquit, Soydan Ozcan, Seokpum Kim</dc:creator>
    </item>
    <item>
      <title>Adversarial Obstacle Placement with Spatial Point Processes for Optimal Path Disruption</title>
      <link>https://arxiv.org/abs/2509.06837</link>
      <description>arXiv:2509.06837v1 Announce Type: new 
Abstract: We investigate the Optimal Obstacle Placement (OOP) problem under uncertainty, framed as the dual of the Optimal Traversal Path problem in the Stochastic Obstacle Scene paradigm. We consider both continuous domains, discretized for analysis, and already discrete spatial grids that form weighted geospatial networks using 8-adjacency lattices. Our unified framework integrates OOP with stochastic geometry, modeling obstacle placement via Strauss (regular) and Mat\'ern (clustered) processes, and evaluates traversal using the Reset Disambiguation algorithm. Through extensive Monte Carlo experiments, we show that traversal cost increases by up to 40% under strongly regular placements, while clustered configurations can decrease traversal costs by as much as 25% by leaving navigable corridors compared to uniform random layouts. In mixed (with both true and false obstacles) scenarios, increasing the proportion of true obstacles from 30% to 70% nearly doubles the traversal cost. These findings are further supported by statistical analysis and stochastic ordering, providing rigorous insights into how spatial patterns and obstacle compositions influence navigation under uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06837v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Li Zhou, Elvan Ceyhan, Polat Charyyev</dc:creator>
    </item>
    <item>
      <title>Sparse Seemingly Unrelated Regression (SSUR) Copula Mixed Models for Multivariate Loss Reserving</title>
      <link>https://arxiv.org/abs/2509.05426</link>
      <description>arXiv:2509.05426v1 Announce Type: cross 
Abstract: Insurance companies often operate across multiple interrelated lines of business (LOBs), and accounting for dependencies between them is essential for accurate reserve estimation and risk capital determination. In our previous work on the Extended Deep Triangle (EDT), we demonstrated that a more flexible model that uses multiple companies' data reduces reserve prediction error and increases diversification benefits. However, the EDT's limitation lies in its limited interpretability of the dependence structure, which is an important feature needed by insurers to guide strategic decisions. Motivated by the need for interpretability and flexibility, this paper proposes a Seemingly Unrelated Regression (SUR) copula mixed model to handle heterogeneous data across multiple companies. The model incorporates random effects to capture company-specific heterogeneity, uses flexible marginal distributions across LOBs, and treats development and accident year effects as fixed effects with shrinkage via LASSO to enhance robustness. We estimate the model using an iterative two-stage procedure and generate predictive reserve distributions via a modified bootstrap that accounts for systematic effects, dependence structure, and sparse fixed-effect coefficients. Through simulation studies and real data from the National Association of Insurance Commissioners, we show that the proposed model outperforms the SUR copula regression model in terms of reserve accuracy and generates larger risk capital gain. Overall, the SUR copula mixed model achieves better predictive performance, greater risk diversification, and retains interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05426v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pengfei Cai, Anas Abdallah, Pratheepa Jeganathan</dc:creator>
    </item>
    <item>
      <title>Multidimensional constructs and moderated linear and nonlinear factor analysis</title>
      <link>https://arxiv.org/abs/2509.05443</link>
      <description>arXiv:2509.05443v1 Announce Type: cross 
Abstract: Multidimensional factor models with moderations on all model parameters have so far been limited to single-factor and two-factor models. This does not align well with existing psychological measures, which are commonly intended to assess 3-5 dimensions of a latent construct. In this paper, we introduce a penalized maximum likelihood approach for multidimensional MNLFA that permits moderation of item intercepts, loadings, residual variances, factor means, variances, and correlations across three or more latent factors. Our approach incorporates ridge, lasso, and alignment penalties to stabilize estimation and detect partial measurement non-invariance while preserving model interpretability. We derive closed-form analytic gradients of the likelihood, eliminating the need for costly numerical or MCMC-based approximations, and demonstrate how this dramatically improves computational efficiency. Through simulation and application, we illustrate that penalized MNLFA recovers complex moderation patterns in multidimensional constructs and provides a scalable alternative to Bayesian implementations. We conclude by discussing the theoretical implications of penalization for measurement invariance, computational considerations, and future directions for extending the framework to categorical indicators, longitudinal data, and applied research contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05443v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R. Noah Padgett</dc:creator>
    </item>
    <item>
      <title>Optimal scheduling of interim analyses in group sequential trials</title>
      <link>https://arxiv.org/abs/2509.05537</link>
      <description>arXiv:2509.05537v1 Announce Type: cross 
Abstract: Group sequential designs (GSDs) are well established and the most commonly used adaptive design in confirmatory clinical trials with interim analyses. However, they remain underutilised, and their implementation involves unique theoretical and practical decisions that demand careful consideration to optimise efficiency. A common practice is to schedule interim analyses at equal intervals based on calendar time or accumulated data. While straightforward, this approach does not completely exploit the potential sample size savings achievable with GSDs. To address this challenge, we develop OptimInterim, an R-based tool that can determine the optimal scheduling of interim analyses to minimise the expected sample size under the alternative hypothesis while controlling overall type I and type II errors. Our method accommodates trials with continuous or binary endpoints, allows multiple interim analyses and supports a range of stopping boundaries. Through extensive simulations, we demonstrate that optimally spaced interim analyses can yield substantial savings in expected sample size compared to equally spaced interim analyses, without compromising the maximum sample size, across various endpoint types, effect sizes, error rates and stopping rules. We illustrate its practical utility with two landmark trials evaluating steroid use in septic shock. Notably, for given type I and type II error rates, the optimal scheduling is independent of endpoint types and effect sizes, ensuring broad applicability across a wide range of trial contexts. To facilitate implementation, we offer a ready-to-use reference table of optimal schedules for up to eight interim analyses under commonly used error rates and stopping rules. Access OptimInterim at https://github.com/zhangyi-he/GSD_OptimInterim.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05537v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhangyi He, Suzie Cro, Laurent Billot</dc:creator>
    </item>
    <item>
      <title>Interpretable dimension reduction for compositional data</title>
      <link>https://arxiv.org/abs/2509.05563</link>
      <description>arXiv:2509.05563v1 Announce Type: cross 
Abstract: High-dimensional compositional data, such as those from human microbiome studies, pose unique statistical challenges due to the simplex constraint and excess zeros. While dimension reduction is indispensable for analyzing such data, conventional approaches often rely on log-ratio transformations that compromise interpretability and distort the data through ad hoc zero replacements. We introduce a novel framework for interpretable dimension reduction of compositional data that avoids extra transformations and zero imputations. Our approach generalizes the concept of amalgamation by softening its operation, mapping high-dimensional compositions directly to a lower-dimensional simplex, which can be visualized in ternary plots. The framework further provides joint visualization of the reduction matrix, enabling intuitive, at-a-glance interpretation. To achieve optimal reduction within our framework, we incorporate sufficient dimension reduction, which defines a new identifiable objective: the central compositional subspace. For estimation, we propose a compositional kernel dimension reduction (CKDR) method. The estimator is provably consistent, exhibits sparsity that reveals underlying amalgamation structures, and comes with an intrinsic predictive model for downstream analyses. Applications to real microbiome datasets demonstrate that our approach provides a powerful graphical exploration tool for uncovering meaningful biological patterns, opening a new pathway for analyzing high-dimensional compositional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05563v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyoung Park, Cheolwoo Park, Jeongyoun Ahn</dc:creator>
    </item>
    <item>
      <title>Rethinking Beta: A Causal Take on CAPM</title>
      <link>https://arxiv.org/abs/2509.05760</link>
      <description>arXiv:2509.05760v1 Announce Type: cross 
Abstract: The CAPM regression is typically interpreted as if the market return contemporaneously \emph{causes} individual returns, motivating beta-neutral portfolios and factor attribution. For realized equity returns, however, this interpretation is inconsistent: a same-period arrow $R_{m,t} \to R_{i,t}$ conflicts with the fact that $R_m$ is itself a value-weighted aggregate of its constituents, unless $R_m$ is lagged or leave-one-out -- the ``aggregator contradiction.'' We formalize CAPM as a structural causal model and analyze the admissible three-node graphs linking an external driver $Z$, the market $R_m$, and an asset $R_i$. The empirically plausible baseline is a \emph{fork}, $Z \to \{R_m, R_i\}$, not $R_m \to R_i$. In this setting, OLS beta reflects not a causal transmission, but an attenuated proxy for how well $R_m$ captures the underlying driver $Z$. Consequently, ``beta-neutral'' portfolios can remain exposed to macro or sectoral shocks, and hedging on $R_m$ can import index-specific noise. Using stylized models and large-cap U.S.\ equity data, we show that contemporaneous betas act like proxies rather than mechanisms; any genuine market-to-stock channel, if at all, appears only at a lag and with modest economic significance. The practical message is clear: CAPM should be read as associational. Risk management and attribution should shift from fixed factor menus to explicitly declared causal paths, with ``alpha'' reserved for what remains invariant once those causal paths are explicitly blocked.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05760v1</guid>
      <category>econ.TH</category>
      <category>q-fin.PR</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naftali Cohen</dc:creator>
    </item>
    <item>
      <title>DETERring more than Deforestation: Environmental Enforcement Reduces Violence in the Amazon</title>
      <link>https://arxiv.org/abs/2509.06076</link>
      <description>arXiv:2509.06076v1 Announce Type: cross 
Abstract: We estimate the impact of environmental law enforcement on violence in the Brazilian Amazon. The introduction of the Real-Time Deforestation Detection System (DETER), which enabled the government to monitor deforestation in real time and issue fines for illegal clearing, significantly reduced homicides in the region. To identify causal effects, we exploit exogenous variation in satellite monitoring generated by cloud cover as an instrument for enforcement intensity. Our estimates imply that the expansion of state presence through DETER prevented approximately 1,477 homicides per year, a 15% reduction in homicides. These results show that curbing deforestation produces important social co-benefits, strengthening state presence and reducing violence in regions marked by institutional fragility and resource conflict.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06076v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rafael Araujo, Vitor Possebom, Gabriela Setti</dc:creator>
    </item>
    <item>
      <title>Noise-Robust Phase Connectivity Estimation via Bayesian Circular Functional Models</title>
      <link>https://arxiv.org/abs/2509.06418</link>
      <description>arXiv:2509.06418v1 Announce Type: cross 
Abstract: The phase locking value (PLV) is a widely used measure to detect phase connectivity. Main drawbacks of the standard PLV are it can be sensitive to noisy observations and does not provide uncertainty measures under finite samples. To overcome the difficulty, we propose a model-based PLV through nonparametric statistical modeling. Specifically, since the discrete time series of phase can be regarded as a functional observation taking values on circle, we employ a Bayesian model for circular-variate functional data, which gives denoising and inference on the resulting PLV values. The proposed model is defined through "wrapping" functional Gaussian models on real line, for which we develop an efficient posterior computation algorithm using Gibbs sampler. The usefulness of the proposed method is demonstrated through simulation experiments based on real EEG data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06418v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shonosuke Sugasawa, Takeru Matsuda, Tomoyuki Nagakawa</dc:creator>
    </item>
    <item>
      <title>The use of financial and sustainability ratios to map a sector. An approach using compositional data</title>
      <link>https://arxiv.org/abs/2509.06468</link>
      <description>arXiv:2509.06468v1 Announce Type: cross 
Abstract: Purpose: The article aims to visualise in a single graph fish and meat processing company groups in Spain with respect to long-term solvency, energy, waste and water intensity and gender employment gap.
  Design/methodology/approach: The selected financial, environmental and social indicators are ratios, which require specific statistical analysis methods to prevent severe skewness and outliers. We use the compositional data methodology and the principal-component analysis biplot.
  Findings: Fish-processing companies have more homogeneous financial, environmental and social performance than their meat-processing counterparts. Specific company groups in both sectors can be identified as poor performers in some of the indicators. Firms with higher solvency tend to be less efficient in energy and water use. Two clusters of company groups with similar performances are identified.
  Research limitations/implications: As of now, few firms publish reports according to the EU Corporate Sustainability Reporting Directive. In future research larger samples will be available.
  Social Implications: Firm groups can visually see their areas of improvement in their financial, environmental and social performance compared to their competitors in the sector.
  Originality/value: This is the first time in which visualization tools have combined financial, environmental and social indicators. All individual firms can be visually ordered along all indicators simultaneously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06468v1</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Elena Rond\'os-Casas (University of Girona), Germ\`a Coenders (University of Girona), Miquel Carreras-Sim\'o (University of Girona), N\'uria Arimany-Serrat (University of Vic-Central University of Catalonia)</dc:creator>
    </item>
    <item>
      <title>Neural ARFIMA model for forecasting BRIC exchange rates with long memory under oil shocks and policy uncertainties</title>
      <link>https://arxiv.org/abs/2509.06697</link>
      <description>arXiv:2509.06697v1 Announce Type: cross 
Abstract: Accurate forecasting of exchange rates remains a persistent challenge, particularly for emerging economies such as Brazil, Russia, India, and China (BRIC). These series exhibit long memory, nonlinearity, and non-stationarity properties that conventional time series models struggle to capture. Additionally, there exist several key drivers of exchange rate dynamics, including global economic policy uncertainty, US equity market volatility, US monetary policy uncertainty, oil price growth rates, and country-specific short-term interest rate differentials. These empirical complexities underscore the need for a flexible modeling framework that can jointly accommodate long memory, nonlinearity, and the influence of external drivers. To address these challenges, we propose a Neural AutoRegressive Fractionally Integrated Moving Average (NARFIMA) model that combines the long-memory representation of ARFIMA with the nonlinear learning capacity of neural networks, while flexibly incorporating exogenous causal variables. We establish theoretical properties of the model, including asymptotic stationarity of the NARFIMA process using Markov chains and nonlinear time series techniques. We quantify forecast uncertainty using conformal prediction intervals within the NARFIMA framework. Empirical results across six forecast horizons show that NARFIMA consistently outperforms various state-of-the-art statistical and machine learning models in forecasting BRIC exchange rates. These findings provide new insights for policymakers and market participants navigating volatile financial conditions. The \texttt{narfima} \textbf{R} package provides an implementation of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06697v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tanujit Chakraborty, Donia Besher, Madhurima Panja, Shovon Sengupta</dc:creator>
    </item>
    <item>
      <title>Combining Unsupervised Learning and Statistical Inference For Multimodal N-of-1 Trials</title>
      <link>https://arxiv.org/abs/2309.06455</link>
      <description>arXiv:2309.06455v2 Announce Type: replace 
Abstract: N-of-1 trials are within-person crossover trials allowing both personalized and population-level inference on the effect of health interventions. Using the full potential of modern technologies, multimodal N-of-1 trials can integrate multimedia data for measuring health outcomes. However, methodology required for automated applications in large multimodal trials is not available yet. Here, we present an unsupervised approach for modeling multimodal N-of-1 trials, bypassing the need for expensive outcome labeling by medical experts. First, an autoencoder is trained on the outcome medical images. Then, the dimensionality of embeddings is reduced by extracting the first principal component, which is finally tested for its association with the treatment. Results from imaging simulation studies show high power in detecting a treatment effect while controlling type I error rates. An application to imaging N-of-1 trials of acne severity identifies individual treatment effects and supports that our methodology can enable large clinical multimodal N-of-1 trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.06455v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juliana Schneider, Thomas G\"artner, Stefan Konigorski</dc:creator>
    </item>
    <item>
      <title>Causal Representation Learning with Generative Artificial Intelligence: Application to Texts as Treatments</title>
      <link>https://arxiv.org/abs/2410.00903</link>
      <description>arXiv:2410.00903v4 Announce Type: replace 
Abstract: In this paper, we demonstrate how to enhance the validity of causal inference with unstructured high-dimensional treatments like texts, by leveraging the power of generative Artificial Intelligence (GenAI). Specifically, we propose to use a deep generative model such as large language models (LLMs) to efficiently generate treatments and use their internal representation for subsequent causal effect estimation. We show that the knowledge of this true internal representation helps disentangle the treatment features of interest, such as specific sentiments and certain topics, from other possibly unknown confounding features. Unlike existing methods, the proposed GenAI-Powered Inference (GPI) methodology eliminates the need to learn causal representation from the data, and hence produces more accurate and efficient estimates. We formally establish the conditions required for the nonparametric identification of the average treatment effect, propose an estimation strategy that avoids the violation of the overlap assumption, and derive the asymptotic properties of the proposed estimator through the application of double machine learning. Finally, using an instrumental variables approach, we extend the proposed GPI methodology to the settings in which the treatment feature is based on human perception. The GPI is also applicable to text reuse where an LLM is used to regenerate existing texts. We conduct simulation and empirical studies, using the generated text data from an open-source LLM, Llama~3, to illustrate the advantages of our estimator over state-of-the-art causal representation learning algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00903v4</guid>
      <category>stat.AP</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kosuke Imai, Kentaro Nakamura</dc:creator>
    </item>
    <item>
      <title>Deriving Duration Time from Occupancy Data -- A case study in the length of stay in Intensive Care Units for COVID-19 patients</title>
      <link>https://arxiv.org/abs/2505.02587</link>
      <description>arXiv:2505.02587v2 Announce Type: replace 
Abstract: This paper focuses on drawing information on underlying processes, which are not directly observed in the data. In particular, we work with data in which only the total count of units in a system at a given time point is observed, but the underlying process of inflows, length of stay and outflows is not. The particular data example looked at in this paper is the occupancy of intensive care units (ICU) during the COVID-19 pandemic, where the aggregated numbers of occupied beds in ICUs on the district level (`Landkreis') are recorded, but not the number of incoming and outgoing patients. The Skellam distribution allows us to infer the number of incoming and outgoing patients from the occupancy in the ICUs. This paper goes a step beyond and approaches the question of whether we can also estimate the average length of stay of ICU patients. Hence, the task is to derive not only the number of incoming and outgoing units from a total net count but also to gain information on the duration time of patients on ICUs. We make use of a stochastic Expectation-Maximisation algorithm and additionally include exogenous information which are assumed to explain the intensity of inflow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02587v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martje Rave, G\"oran Kauermann</dc:creator>
    </item>
    <item>
      <title>Unsupervised cell segmentation by fast Gaussian Processes</title>
      <link>https://arxiv.org/abs/2505.18902</link>
      <description>arXiv:2505.18902v2 Announce Type: replace 
Abstract: Cell boundary information is crucial for analyzing cell behaviors from time-lapse microscopy videos. Existing supervised cell segmentation tools, such as ImageJ, require tuning various parameters and rely on restrictive assumptions about the shape of the objects. While recent supervised segmentation tools based on convolutional neural networks enhance accuracy, they depend on high-quality labeled images, making them unsuitable for segmenting new types of objects not in the database. We developed a novel unsupervised cell segmentation algorithm based on fast Gaussian processes for noisy microscopy images without the need for parameter tuning or restrictive assumptions about the shape of the object. We derived robust thresholding criteria adaptive for heterogeneous images containing distinct brightness at different parts to separate objects from the background, and employed watershed segmentation to distinguish touching cell objects. Both simulated studies and real-data analysis of large microscopy images demonstrate the scalability and accuracy of our approach compared with the alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18902v2</guid>
      <category>stat.AP</category>
      <category>cs.CV</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laura Baracaldo, Blythe King, Haoran Yan, Yizi Lin, Nina Miolane, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>Forecasting Extreme Day and Night Heat in Paris</title>
      <link>https://arxiv.org/abs/2508.12886</link>
      <description>arXiv:2508.12886v3 Announce Type: replace 
Abstract: As a demonstration of concept, quantile gradient boosting is used to forecast diurnal and nocturnal Q(.90) air temperatures for Paris, France during late the spring and summer months of 2020. The data are provided by the Paris-Montsouris weather station. Q(.90) values are estimated because the 90th percentile requires that the temperatures be relatively rare and extreme. Predictors include seven routinely collected indicators of weather conditions, lagged by 14 days; the temperature forecasts are produced two weeks in advance. Conformal prediction regions capture forecasting uncertainty with provably valid properties. For both diurnal and nocturnal temperatures, forecasting accuracy is promising, and sound measures of uncertainty are provided. Benefits for policy and practice follow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12886v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Richard Berk</dc:creator>
    </item>
    <item>
      <title>A Composite-Loss Graph Neural Network for the Multivariate Post-Processing of Ensemble Weather Forecasts</title>
      <link>https://arxiv.org/abs/2509.02784</link>
      <description>arXiv:2509.02784v2 Announce Type: replace 
Abstract: Ensemble forecasting systems have advanced meteorology by providing probabilistic estimates of future states. Nonetheless, systematic biases often persist, making statistical post-processing essential. Traditional parametric post-processing techniques and machine learning-based methods can produce calibrated predictive distributions at specific locations and lead times, yet often struggle to capture dependencies across forecast dimensions. To address this, multivariate post-processing methods-such as ensemble copula coupling and the Schaake shuffle-are widely applied in a second step to restore realistic inter-variable or spatio-temporal dependencies. The aim of this study is the multivariate post-processing of ensemble forecasts using a graph neural network (dualGNN) trained with a composite loss function that combines the energy score (ES) and the variogram score (VS). The method is evaluated on two datasets: WRF-based solar irradiance forecasts over northern Chile and ECMWF visibility forecasts for Central Europe. The dualGNN consistently outperforms all empirical copula-based post-processed forecasts and shows significant improvements compared to graph neural networks trained solely on either the continuous ranked probability score or the ES, according to the evaluated multivariate verification metrics. Furthermore, for the WRF forecasts, the rank-order structure of the dualGNN forecasts captures valuable dependency information, enabling a more effective restoration of spatial relationships than either the raw numerical weather prediction ensemble or historical observational rank structures. Notably, incorporating VS into the loss function improved the univariate performance for both target variables compared to training on ES alone. Moreover, for the visibility forecasts, the ES-VS combination even outperformed the strongest calibrated reference in terms of univariate performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02784v2</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M\'aria Lakatos</dc:creator>
    </item>
    <item>
      <title>Directional Gaussian hypergeometric beta distributions and their uses in contaminated binary sampling</title>
      <link>https://arxiv.org/abs/2503.11128</link>
      <description>arXiv:2503.11128v2 Announce Type: replace-cross 
Abstract: We examine the Gaussian hypergeometric beta distribution and look at the effect of having an additional term in the density kernel relative to the standard beta distribution. We reparameterise and classify this distribution into left and right directional variants using parameters that give a simple and symmetrical representation of the directional push/pull from this additional term in the density kernel. We examine the properties of the directional variants and their uses in contaminated binary sampling using Bayesian inference. We find that the Gaussian hypergeometric beta distribution arises as the appropriate posterior distribution for inference in certain kinds of contaminated binary models and that the directional parameterisation aids in representation of the resulting Bayesian models. We derive a broad range of properties and computational methods for the directional variants of the distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11128v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ben O'Neill</dc:creator>
    </item>
    <item>
      <title>An information metric for comparing and assessing informative interim decisions in sequential clinical trials</title>
      <link>https://arxiv.org/abs/2509.04904</link>
      <description>arXiv:2509.04904v2 Announce Type: replace-cross 
Abstract: Group sequential designs enable interim analyses and potential early stopping for efficacy or futility. While these adaptations improve trial efficiency and ethical considerations, they also introduce bias into the adapted analyses. We demonstrate how failing to account for informative interim decisions in the analysis can substantially affect posterior estimates of the treatment effect, often resulting in overly optimistic credible intervals aligned with the stopping decision. Drawing on information theory, we use the Kullback-Leibler divergence to quantify this distortion and highlight its use for post-hoc evaluation of informative interim decisions, with a focus on end-of-study inference. Unlike pointwise comparisons, this measure provides an integrated summary of this distortion on the whole parameter space. By comparing alternative decision boundaries and prior specifications, we illustrate how this measure can improve the understanding of trial results and inform the planning of future adaptive studies. We also introduce an expected version of this metric to support clinicians in choosing decision boundaries. This guidance complements traditional strategies based on type-I error rate control by offering insights into the distortion introduced to the treatment effect at each interim phase. The use of this pre-experimental measure is finally illustrated in a group sequential trial for evaluating a treatment for central nervous system disorders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04904v2</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>G. Caruso, W. F. Rosenberger, P. Mozgunov, N. Flournoy</dc:creator>
    </item>
    <item>
      <title>Beyond Linearity and Time-homogeneity: Relational Hyper Event Models with Time-Varying Non-Linear Effects</title>
      <link>https://arxiv.org/abs/2509.05289</link>
      <description>arXiv:2509.05289v2 Announce Type: replace-cross 
Abstract: Recent technological advances have made it easier to collect large and complex networks of time-stamped relational events connecting two or more entities. Relational hyper-event models (RHEMs) aim to explain the dynamics of these events by modeling the event rate as a function of statistics based on past history and external information.
  However, despite the complexity of the data, most current RHEM approaches still rely on a linearity assumption to model this relationship. In this work, we address this limitation by introducing a more flexible model that allows the effects of statistics to vary non-linearly and over time. While time-varying and non-linear effects have been used in relational event modeling, we take this further by modeling joint time-varying and non-linear effects using tensor product smooths.
  We validate our methodology on both synthetic and empirical data. In particular, we use RHEMs to study how patterns of scientific collaboration and impact evolve over time. Our approach provides deeper insights into the dynamic factors driving relational hyper-events, allowing us to evaluate potential non-monotonic patterns that cannot be identified using linear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05289v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martina Boschi, J\"urgen Lerner, Ernst C. Wit</dc:creator>
    </item>
  </channel>
</rss>
