<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.soc-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.soc-ph</link>
    <description>physics.soc-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.soc-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Jun 2025 01:39:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>How urban scaling and resource distribution shape social welfare and migration dynamics</title>
      <link>https://arxiv.org/abs/2506.03384</link>
      <description>arXiv:2506.03384v1 Announce Type: new 
Abstract: Many outputs of cities scale in universal ways, including infrastructure, crime, and economic activity. Through a mathematical model, this study investigates the interplay between such scaling laws in human organization and governmental allocations of resources, focusing on impacts to migration patterns and social welfare. We find that if superlinear scaling resources of cities -- such as economic and social activity -- are the primary drivers of city dwellers' utility, then cities tend to converge to similar sizes and social welfare through migration. In contrast, if sublinear scaling resources, such as infrastructure, primarily impact utility, then migration tends to lead to megacities and inequity between large and small cities. These findings have implications for policymakers, economists, and political scientists addressing the challenges of equitable and efficient resource allocation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03384v1</guid>
      <category>physics.soc-ph</category>
      <category>econ.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bryce Morsky</dc:creator>
    </item>
    <item>
      <title>Fluctuations in email size modeled using a gamma-like distribution</title>
      <link>https://arxiv.org/abs/2506.03500</link>
      <description>arXiv:2506.03500v1 Announce Type: new 
Abstract: A previously established frequency distribution model, which integrates a lognormal distribution with a logarithmic equation, effectively characterizes fluctuations in email size during sending requests. In addition, an email size generation model has been developed based on this log-normal-like framework. While the fitting of these models has been deemed satisfactory, they can be further enhanced in the range of small email sizes. This study advances these models by incorporating a gamma distribution alongside a logarithmic equation. The resulting gamma-like model demonstrates a significantly improved fit compared with the log-normal-like model. These results contribute to the knowledge on the statistical properties of sending mail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03500v1</guid>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoshitsugu Matsubara</dc:creator>
    </item>
    <item>
      <title>Reconstructing North Korea's Plutonium Production History with Bayesian Inference-Based Reprocessing Waste Analysis</title>
      <link>https://arxiv.org/abs/2506.03865</link>
      <description>arXiv:2506.03865v1 Announce Type: new 
Abstract: Although North Korea's nuclear program has been the subject of extensive scrutiny, estimates of its fissile material stockpiles remain fraught with uncertainty. In potential future disarmament agreements, inspectors may need to use nuclear archaeology methods to verify or gain confidence in a North Korean fissile material declaration. This study explores the potential utility of a Bayesian inference-based analysis of the isotopic composition of reprocessing waste to reconstruct the operating history of the 5 MWe reactor and estimate its plutonium production history. We simulate several scenarios that reflect different assumptions and varying levels of prior knowledge about the reactor. The results show that correct prior assumptions can be confirmed and incorrect prior information (or a false declaration) can be detected. Model comparison techniques can distinguish between scenarios with different numbers of core discharges, a capability that could provide important insights into the early stages of operation of the 5 MWe reactor. Using these techniques, a weighted plutonium estimate can be calculated, even in cases where the number of core discharges is not known with certainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03865v1</guid>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Benjamin Jung, Johannes Bosse, Malte G\"ottsche</dc:creator>
    </item>
    <item>
      <title>Forecasting Seasonal Influenza Epidemics with Physics-Informed Neural Networks</title>
      <link>https://arxiv.org/abs/2506.03897</link>
      <description>arXiv:2506.03897v1 Announce Type: new 
Abstract: Accurate epidemic forecasting is critical for informing public health decisions and timely interventions. While Physics-Informed Neural Networks (PINNs) have shown promise in various scientific domains, their application to real-time epidemic forecasting remains limited. The reasons are mainly due to the intrinsic difficulty of the task and the tendency to fully leveraging their learning and inference potential, which, however, often results in non-optimal forecasting frameworks. Here, we present SIR-INN, a hybrid forecasting framework that integrates the mechanistic structure of the classical Susceptible-Infectious-Recovered (SIR) model into a neural network architecture. Trained once on synthetic epidemic scenarios, the model is able to generalize across epidemic conditions without retraining. From limited and noisy observations, SIR-INN infers key transmission parameters via Markov chain Monte Carlo (MCMC) generating probabilistic short- and long-term forecasts. We validate SIR-INN using national influenza data from the Italian National Institute of Health, in the 2023-2024 and 2024-2025 seasons. The model performs competitively with current state-of-the-art approaches, particularly in terms of Mean Absolute Error (MAE) and Weighted Interval Score (WIS). It shows accurate predictive performance in nearly all phases of the outbreak, with improved accuracy observed for the 2024-2025 influenza season. Credible uncertainty intervals are consistently maintained, despite occasional shortcomings in coverage. SIR-INN offers a computationally efficient, interpretable, and generalizable solution for epidemic forecasting, appropriately leveraging the framework's hybrid design. Its ability to provide real-time predictions of epidemic dynamics, together with uncertainty quantification, makes it a promising tool for real-world epidemic forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03897v1</guid>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martina Rama, Gabriele Santin, Giulia Cencetti, Michele Tizzoni, Bruno Lepri</dc:creator>
    </item>
    <item>
      <title>Investigating the emergent invariant properties of Hungarian electric distribution networks</title>
      <link>https://arxiv.org/abs/2506.04009</link>
      <description>arXiv:2506.04009v1 Announce Type: new 
Abstract: Electric power distribution networks serve as the final and essential stage in power delivery, bridging transmission infrastructure and end users. The structural configuration of these networks plays a critical role in determining system reliability, fault tolerance, and operational efficiency. Although the design of distribution systems is influenced by various regional factors, such as geography, customer density, and planning standards, the extent to which consistent structural characteristics emerge across different networks remains an open question. In this study, we perform a detailed spatial and topological analysis of five MV distribution networks in Hungary. Despite notable differences in geographic layout and consumer distribution, we identify statistically consistent patterns across several key metrics, including degree, BC, and powerline length. These findings suggest the influence of common underlying design principles or optimization constraints, potentially indicating universal structural tendencies in MV network design. The results provide insight into the organization of real-world distribution systems and offer a basis for improved planning, risk mitigation, and system optimization in future grid developments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04009v1</guid>
      <category>physics.soc-ph</category>
      <category>math.GN</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Michelle T. Cirunay, B\'alint Hartmann, T\'imea Erdei, Tam\'as Soha</dc:creator>
    </item>
    <item>
      <title>OpenCarbon: A Contrastive Learning-based Cross-Modality Neural Approach for High-Resolution Carbon Emission Prediction Using Open Data</title>
      <link>https://arxiv.org/abs/2506.03224</link>
      <description>arXiv:2506.03224v1 Announce Type: cross 
Abstract: Accurately estimating high-resolution carbon emissions is crucial for effective emission governance and mitigation planning. While conventional methods for precise carbon accounting are hindered by substantial data collection efforts, the rise of open data and advanced learning techniques offers a promising solution. Once an open data-based prediction model is developed and trained, it can easily infer emissions for new areas based on available open data. To address this, we incorporate two modalities of open data, satellite images and point-of-interest (POI) data, to predict high-resolution urban carbon emissions, with satellite images providing macroscopic and static and POI data offering fine-grained and relatively dynamic functionality information. However, estimating high-resolution carbon emissions presents two significant challenges: the intertwined and implicit effects of various functionalities on carbon emissions, and the complex spatial contiguity correlations that give rise to the agglomeration effect. Our model, OpenCarbon, features two major designs that target the challenges: a cross-modality information extraction and fusion module to extract complementary functionality information from two modules and model their interactions, and a neighborhood-informed aggregation module to capture the spatial contiguity correlations. Extensive experiments demonstrate our model's superiority, with a significant performance gain of 26.6\% on R2. Further generalizability tests and case studies also show OpenCarbon's capacity to capture the intrinsic relation between urban functionalities and carbon emissions, validating its potential to empower efficient carbon governance and targeted carbon mitigation planning. Codes and data are available: https://github.com/JinweiZzz/OpenCarbon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03224v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinwei Zeng, Yu Liu, Guozhen Zhang, Jingtao Ding, Yuming Lin, Jian Yuan, Yong Li</dc:creator>
    </item>
    <item>
      <title>Genotype networks drive oscillating endemicity and epidemic trajectories in viral evolution</title>
      <link>https://arxiv.org/abs/2506.03279</link>
      <description>arXiv:2506.03279v1 Announce Type: cross 
Abstract: Rapidly evolving viruses use antigenic drift as a key mechanism to evade host immunity and persist in real populations. While traditional models of antigenic drift and epidemic spread rely on low-dimensional antigenic spaces, genomic surveillance data reveal that viral evolution produces complex antigenic genotype networks with hierarchical modular structures. In this study, we present an eco-evolutionary framework in which viral evolution and population immunity dynamics are shaped by the structure of antigenic genotype networks. Using synthetic networks, we demonstrate that network topology alone can drive transitions between stable endemic states and recurrent seasonal epidemics. Furthermore, our results show how the integration of the genotype network of the H3N2 influenza in our model allows for estimating the emergence times of various haplotypes resulting from its evolution. Our findings underscore the critical role of the topology of genotype networks in shaping epidemic behavior and, besides, provide a robust framework for integrating real-world genomic data into predictive epidemic models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03279v1</guid>
      <category>q-bio.PE</category>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Santiago Lamata-Ot\'in, Octavian C. Rotita-Ion, Alex Arenas, David Soriano-Pa\~nos, Jes\'us G\'omez-Garde\~nes</dc:creator>
    </item>
    <item>
      <title>Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations</title>
      <link>https://arxiv.org/abs/2506.03941</link>
      <description>arXiv:2506.03941v1 Announce Type: cross 
Abstract: During a conversation, there can come certain moments where its outcome hangs in the balance. In these pivotal moments, how one responds can put the conversation on substantially different trajectories leading to significantly different outcomes. Systems that can detect when such moments arise could assist conversationalists in domains with highly consequential outcomes, such as mental health crisis counseling.
  In this work, we introduce an unsupervised computational method for detecting such pivotal moments as they happen, in an online fashion. Our approach relies on the intuition that a moment is pivotal if our expectation of the outcome varies widely depending on what might be said next. By applying our method to crisis counseling conversations, we first validate it by showing that it aligns with human perception -- counselors take significantly longer to respond during moments detected by our method -- and with the eventual conversational trajectory -- which is more likely to change course at these times. We then use our framework to explore the relation of the counselor's response during pivotal moments with the eventual outcome of the session.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03941v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vivian Nguyen, Lillian Lee, Cristian Danescu-Niculescu-Mizil</dc:creator>
    </item>
    <item>
      <title>Risk and Reward of Transitioning from a National to a Zonal Electricity Market in Great Britain</title>
      <link>https://arxiv.org/abs/2506.04107</link>
      <description>arXiv:2506.04107v1 Announce Type: cross 
Abstract: More spatially granular electricity wholesale markets promise more efficient operation and better asset siting in highly renewable power systems. Great Britain is considering moving from its current single-price national wholesale market to a zonal design. Existing studies reach varying and difficult-to-reconcile conclusions about the desirability of a zonal market in GB, partly because they rely on models that vary in their transparency and assumptions about future power systems. Using a novel open-source electricity market model, calibrated to match observed network behaviour, this article quantifies consumer savings, unit-level producer surplus impacts, and broader socioeconomic benefits that would have arisen had a six-zone market operated in Great Britain during 2022-2024. In the absence of mitigating policies, it is estimated that during those three years GB consumers would save approximately {\pounds}9.4/MWh (equalling an average of more than {\pounds}2.3B per year), but generators in northern regions would experience revenue reductions of 30-40\%. Policy interventions can restore these units' national market revenues to up to 97\% while still preserving around {\pounds}3.1/MWh in consumer savings (about {\pounds}750M per year). It is further estimated that the current system could achieve approximately {\pounds}380-{\pounds}770 million in annual welfare gain during 2022-2024 through improved operational efficiency alone. The drivers behind these benefits, notably wind curtailment volumes, are expected to become more pronounced towards 2030, suggesting that purely operationally achieved annual benefits of around {\pounds}1-2 billion beyond 2029 are likely. It is found that the scale of these benefits would outweigh the potential downsides related to increases in the cost of capital that have been estimated elsewhere.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04107v1</guid>
      <category>econ.GN</category>
      <category>cs.CE</category>
      <category>physics.data-an</category>
      <category>physics.soc-ph</category>
      <category>q-fin.EC</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Franken, Andrew Lyden, Daniel Friedrich</dc:creator>
    </item>
    <item>
      <title>Quantifying edge relevance for epidemic spreading via the semi-metric topology of complex networks</title>
      <link>https://arxiv.org/abs/2311.14817</link>
      <description>arXiv:2311.14817v2 Announce Type: replace 
Abstract: Sparsification aims at extracting a reduced core of associations that best preserves both the dynamics and topology of networks while reducing the computational cost of simulations. We show that the semi-metric topology of complex networks yields a natural and algebraically-principled sparsification that outperforms existing methods on those goals. Weighted graphs whose edges represent distances between nodes are semi-metric when at least one edge breaks the triangle inequality (transitivity). We first confirm with new experiments that the metric backbone$\unicode{x2013}$a unique subgraph of all edges that obey the triangle inequality and thus preserve all shortest paths$\unicode{x2013}$recovers Susceptible-Infected dynamics over the original non-sparsified graph. This recovery is improved when we remove only those edges that break the triangle inequality significantly, i.e., edges with large semi-metric distortion. Based on these results, we propose the new semi-metric distortion sparsification method to progressively sparsify networks in decreasing order of semi-metric distortion. Our method recovers the macro- and micro-level dynamics of epidemic outbreaks better than other methods while also yielding sparser yet connected subgraphs that preserve all shortest paths. Overall, we show that semi-metric distortion overcomes the limitations of edge betweenness in ranking the dynamical relevance of edges not participating in any shortest path, as it quantifies the existence and strength of alternative transmission pathways.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.14817v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.SI</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Soriano Pa\~nos, Felipe Xavier Costa, Luis M. Rocha</dc:creator>
    </item>
    <item>
      <title>Ideological Fragmentation of the Social Media Ecosystem: From echo chambers to echo platforms</title>
      <link>https://arxiv.org/abs/2411.16826</link>
      <description>arXiv:2411.16826v2 Announce Type: replace-cross 
Abstract: The entertainment-driven nature of social media encourages users to engage with like-minded individuals and consume content aligned with their beliefs, limiting exposure to diverse perspectives. Simultaneously, users migrate between platforms, either due to moderation policies like de-platforming or in search of environments better suited to their preferences. These dynamics drive the specialization of the social media ecosystem, shifting from internal echo chambers to "echo platforms"--entire platforms functioning as ideologically homogeneous niches. To systematically analyze this phenomenon in political discussions, we propose a quantitative approach based on three key dimensions: platform centrality, news consumption, and user base composition. We analyze 117 million posts related to the 2020 US Presidential elections from nine social media platforms--Facebook, Reddit, Twitter, YouTube, BitChute, Gab, Parler, Scored, and Voat. Our findings reveal significant differences among platforms in their centrality within the ecosystem, the reliability of circulated news, and the ideological diversity of their users, highlighting a clear divide between mainstream and alt-tech platforms. The latter occupy a peripheral role, feature a higher prevalence of unreliable content, and exhibit greater ideological uniformity. These results highlight the key dimensions shaping the fragmentation and polarization of the social media landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16826v2</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edoardo Di Martino, Alessandro Galeazzi, Michele Starnini, Walter Quattrociocchi, Matteo Cinelli</dc:creator>
    </item>
    <item>
      <title>On the class of coding optimality of human languages and the origins of Zipf's law</title>
      <link>https://arxiv.org/abs/2505.20015</link>
      <description>arXiv:2505.20015v3 Announce Type: replace-cross 
Abstract: Here we present a new class of optimality for coding systems. Members of that class are displaced linearly from optimal coding and thus exhibit Zipf's law, namely a power-law distribution of frequency ranks. Within that class, Zipf's law, the size-rank law and the size-probability law form a group-like structure. We identify human languages that are members of the class. All languages showing sufficient agreement with Zipf's law are potential members of the class. In contrast, there are communication systems in other species that cannot be members of that class for exhibiting an exponential distribution instead but dolphins and humpback whales might. We provide a new insight into plots of frequency versus rank in double logarithmic scale. For any system, a straight line in that scale indicates that the lengths of optimal codes under non-singular coding and under uniquely decodable encoding are displaced by a linear function whose slope is the exponent of Zipf's law. For systems under compression and constrained to be uniquely decodable, such a straight line may indicate that the system is coding close to optimality. We provide support for the hypothesis that Zipf's law originates from compression and define testable conditions for the emergence of Zipf's law in compressing systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20015v3</guid>
      <category>cs.CL</category>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ramon Ferrer-i-Cancho</dc:creator>
    </item>
  </channel>
</rss>
