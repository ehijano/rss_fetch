<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 31 Jul 2024 01:46:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Voice: Lessons on Trustworthy Conversational Agents from "Dune"</title>
      <link>https://arxiv.org/abs/2407.18928</link>
      <description>arXiv:2407.18928v1 Announce Type: new 
Abstract: The potential for untrustworthy conversational agents presents a significant threat for covert social manipulation. Taking inspiration from Frank Herbert's "Dune", where the Bene Gesserit Sisterhood uses the Voice for influence, manipulation, and control of people, we explore how generative AI provides a way to implement individualized influence at industrial scales. Already, these models can manipulate communication across text, image, speech, and most recently video. They are rapidly becoming affordable enough for any organization of even moderate means to train and deploy. If employed by malicious actors, they risk becoming powerful tools for shaping public opinion, sowing discord, and undermining organizations from companies to governments. As researchers and developers, it is crucial to recognize the potential for such weaponization and to explore strategies for prevention, detection, and defense against these emerging forms of sociotechnical manipulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18928v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3640794.3665890</arxiv:DOI>
      <arxiv:journal_reference>ACM Conversational User Interfaces 2024 (CUI '24), July 8--10, 2024, Luxembourg, Luxembourg</arxiv:journal_reference>
      <dc:creator>Philip Feldman</dc:creator>
    </item>
    <item>
      <title>Be More Real: Travel Diary Generation Using LLM Agents and Individual Profiles</title>
      <link>https://arxiv.org/abs/2407.18932</link>
      <description>arXiv:2407.18932v1 Announce Type: new 
Abstract: Human mobility is inextricably linked to social issues such as traffic congestion, energy consumption, and public health; however, privacy concerns restrict access to mobility data. Recently, research have utilized Large Language Models (LLMs) for human mobility generation, in which the challenge is how LLMs can understand individuals' mobility behavioral differences to generate realistic trajectories conforming to real world contexts. This study handles this problem by presenting an LLM agent-based framework (MobAgent) composing two phases: understanding-based mobility pattern extraction and reasoning-based trajectory generation, which enables generate more real travel diaries at urban scale, considering different individual profiles. MobAgent extracts reasons behind specific mobility trendiness and attribute influences to provide reliable patterns; infers the relationships between contextual factors and underlying motivations of mobility; and based on the patterns and the recursive reasoning process, MobAgent finally generates more authentic and personalized mobilities that reflect both individual differences and real-world constraints. We validate our framework with 0.2 million travel survey data, demonstrating its effectiveness in producing personalized and accurate travel diaries. This study highlights the capacity of LLMs to provide detailed and sophisticated understanding of human mobility through the real-world mobility data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18932v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuchuan Li, Fei Huang, Jianrong Lv, Zhixiong Xiao, Guolong Li, Yang Yue</dc:creator>
    </item>
    <item>
      <title>Awareness and Adoption of AI Technologies in the Libraries of Karnataka</title>
      <link>https://arxiv.org/abs/2407.18933</link>
      <description>arXiv:2407.18933v1 Announce Type: new 
Abstract: This study aims to determine the awareness and adoption of Artificial Intelligence (AI) technologies in the respondent libraries of Karnataka based on demographic variables such as gender, age, academic status, and professional experience. This study employed a survey research method to evaluate the awareness and adoption of AI technologies among the respondent library professionals in Karnataka. The study employed a stratified random sampling method to select a sample of 120 respondents from a diverse population, encompassing library professionals across multiple institution types including engineering colleges, medical colleges, and degree colleges. The Chi-square test was used to analyze the data. The study revealed that there is a statistically significant difference in the awareness and adoption of AI technologies based on the factor of gender. Whereas there no significant relationship exists between the degree of awareness and adoption of AI technologies based on factors such as age, academic ranking, and professional experience. AI-powered plagiarism detection, grammar checking, and ChatGPT are the most popularly employed AI technologies among the respondents. The respondents are of the perception that AI will support Librarians and not replace them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18933v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felcy D'Souza</dc:creator>
    </item>
    <item>
      <title>Promoting AI Competencies for Medical Students: A Scoping Review on Frameworks, Programs, and Tools</title>
      <link>https://arxiv.org/abs/2407.18939</link>
      <description>arXiv:2407.18939v1 Announce Type: new 
Abstract: As more clinical workflows continue to be augmented by artificial intelligence (AI), AI literacy among physicians will become a critical requirement for ensuring safe and ethical AI-enabled patient care. Despite the evolving importance of AI in healthcare, the extent to which it has been adopted into traditional and often-overloaded medical curricula is currently unknown. In a scoping review of 1,699 articles published between January 2016 and June 2024, we identified 18 studies which propose guiding frameworks, and 11 studies documenting real-world instruction, centered around the integration of AI into medical education. We found that comprehensive guidelines will require greater clinical relevance and personalization to suit medical student interests and career trajectories. Current efforts highlight discrepancies in the teaching guidelines, emphasizing AI evaluation and ethics over technical topics such as data science and coding. Additionally, we identified several challenges associated with integrating AI training into the medical education program, including a lack of guidelines to define medical students AI literacy, a perceived lack of proven clinical value, and a scarcity of qualified instructors. With this knowledge, we propose an AI literacy framework to define competencies for medical students. To prioritize relevant and personalized AI education, we categorize literacy into four dimensions: Foundational, Practical, Experimental, and Ethical, with tailored learning objectives to the pre-clinical, clinical, and clinical research stages of medical education. This review provides a road map for developing practical and relevant education strategies for building an AI-competent healthcare workforce.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18939v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingbo Ma, Yukyeong Song, Jeremy A. Balch, Yuanfang Ren, Divya Vellanki, Zhenhong Hu, Meghan Brennan, Suraj Kolla, Ziyuan Guan, Brooke Armfield, Tezcan Ozrazgat-Baslanti, Parisa Rashidi, Tyler J. Loftus, Azra Bihorac, Benjamin Shickel</dc:creator>
    </item>
    <item>
      <title>Unpopular Opinion: Generative Artificial Intelligence Is Not Eroding Academic Integrity</title>
      <link>https://arxiv.org/abs/2407.19088</link>
      <description>arXiv:2407.19088v1 Announce Type: new 
Abstract: This paper examines the role of generative artificial intelligence (GAI) in promoting academic integrity within educational settings. It explores how AI can be ethically integrated into classrooms to enhance learning experiences, foster intrinsic motivation, and support voluntary behavior change among students. By analyzing established ethical frameworks and educational theories such as deontological ethics, consequentialism, constructivist learning, and Self-Determination Theory (SDT), the paper argues that GAI, when used responsibly, can enhance digital literacy, encourage genuine knowledge construction, and uphold ethical standards in education. This research highlights the potential of GAI to create enriching, personalized learning environments that prepare students to navigate the complexities of the modern world ethically and effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19088v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Myles Joshua Toledo Tan, Nicholle Mae Amor Tan Maravilla</dc:creator>
    </item>
    <item>
      <title>AI Companions Reduce Loneliness</title>
      <link>https://arxiv.org/abs/2407.19096</link>
      <description>arXiv:2407.19096v1 Announce Type: new 
Abstract: Chatbots are now able to engage in sophisticated conversations with consumers in the domain of relationships, providing a potential coping solution to widescale societal loneliness. Behavioral research provides little insight into whether these applications are effective at alleviating loneliness. We address this question by focusing on AI companions applications designed to provide consumers with synthetic interaction partners. Studies 1 and 2 find suggestive evidence that consumers use AI companions to alleviate loneliness, by employing a novel methodology for fine tuning large language models to detect loneliness in conversations and reviews. Study 3 finds that AI companions successfully alleviate loneliness on par only with interacting with another person, and more than other activities such watching YouTube videos. Moreover, consumers underestimate the degree to which AI companions improve their loneliness. Study 4 uses a longitudinal design and finds that an AI companion consistently reduces loneliness over the course of a week. Study 5 provides evidence that both the chatbots' performance and, especially, whether it makes users feel heard, explain reductions in loneliness. Study 6 provides an additional robustness check for the loneliness alleviating benefits of AI companions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19096v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julian De Freitas, Ahmet K Uguralp, Zeliha O Uguralp, Puntoni Stefano</dc:creator>
    </item>
    <item>
      <title>Sponsored is the New Organic: Implications of Sponsored Results on Quality of Search Results in the Amazon Marketplace</title>
      <link>https://arxiv.org/abs/2407.19099</link>
      <description>arXiv:2407.19099v1 Announce Type: new 
Abstract: Interleaving sponsored results (advertisements) amongst organic results on search engine result pages (SERP) has become a common practice across multiple digital platforms. Advertisements have catered to consumer satisfaction and fostered competition in digital public spaces; making them an appealing gateway for businesses to reach their consumers. However, especially in the context of digital marketplaces, due to the competitive nature of the sponsored results with the organic ones, multiple unwanted repercussions have surfaced affecting different stakeholders. From the consumers' perspective the sponsored ads/results may cause degradation of search quality and nudge consumers to potentially irrelevant and costlier products. The sponsored ads may also affect the level playing field of the competition in the marketplaces among sellers. To understand and unravel these potential concerns, we analyse the Amazon digital marketplace in four different countries by simulating 4,800 search operations. Our analyses over SERPs consisting 2M organic and 638K sponsored results show items with poor organic ranks (beyond 100th position) appear as sponsored results even before the top organic results on the first page of Amazon SERP. Moreover, we also observe that in majority of the cases, these top sponsored results are costlier and are of poorer quality than the top organic results. We believe these observations can motivate researchers for further deliberation to bring in more transparency and guard rails in the advertising practices followed in digital marketplaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19099v1</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <category>cs.IR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhisek Dash, Saptarshi Ghosh, Animesh Mukherjee, Abhijnan Chakraborty, Krishna P. Gummadi</dc:creator>
    </item>
    <item>
      <title>Towards the Terminator Economy: Assessing Job Exposure to AI through LLMs</title>
      <link>https://arxiv.org/abs/2407.19204</link>
      <description>arXiv:2407.19204v1 Announce Type: new 
Abstract: The spread and rapid development of AI-related technologies are influencing many aspects of our daily lives, from social to educational, including the labour market. Many researchers have been highlighting the key role AI and technologies play in reshaping jobs and their related tasks, either by automating or enhancing human capabilities in the workplace. Can we estimate if, and to what extent, jobs and related tasks are exposed to the risk of being automatized by state-of-the-art AI-related technologies? Our work tackles this question through a data-driven approach: (i) developing a reproducible framework that exploits a battery of open-source Large Language Models to assess current AI and robotics' capabilities in performing job-related tasks; (ii) formalising and computing an AI exposure measure by occupation, namely the teai (Task Exposure to AI) index. Our results show that about one-third of U.S. employment is highly exposed to AI, primarily in high-skill jobs (aka, white collars). This exposure correlates positively with employment and wage growth from 2019 to 2023, indicating a beneficial impact of AI on productivity. The source codes and results are publicly available, enabling the whole community to benchmark and track AI and technology capabilities over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19204v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emilio Colombo, Fabio Mercorio, Mario Mezzanzanica, Antonio Serino</dc:creator>
    </item>
    <item>
      <title>Danish DPA Banned the Use of Google Chromebooks and Google Workspace in Schools in Helsingor Municipality</title>
      <link>https://arxiv.org/abs/2407.19377</link>
      <description>arXiv:2407.19377v1 Announce Type: new 
Abstract: On July 14th, 2022, the Danish Data Protection Authority issued a reprimand against Helsingor Municipality. It imposed a general ban on using Google Chromebooks and Google Workspace for education in primary schools in the Municipality. The Danish DPA banned such processing and suspended any related data transfers to the United States (U.S.) until it is brought in line with the General Data Protection Regulation (GDPR). The suspension took effect immediately, and the Municipality had until August 3rd, 2022, to withdraw and terminate the processing, as well as delete data already transferred. Finally, in a new decision on August 18th, 2022, the Danish DPA has ratified the ban to the use of Google Chromebooks and Workspace. In the eyes of the Danish DPA, the Municipality failed for example to document that they have assessed and reduced the relevant risks to the rights and freedoms of the pupils. This article is structured as follows: section II provides the background concerning the unfolding events after the Schrems II ruling. Section III discusses the origins and facts of the Danish DPA case. Section IV examines the reasoning and critical findings of the Danish DPA decision. Finally, section V concludes with some general recommendations the Danish municipalities must follow based on the ensuing effects stemming from this case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19377v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcelo Corrales Compagnucci</dc:creator>
    </item>
    <item>
      <title>Moral and emotional influences on attitude stability towards COVID-19 vaccines on social media</title>
      <link>https://arxiv.org/abs/2407.19406</link>
      <description>arXiv:2407.19406v1 Announce Type: new 
Abstract: Effective public health messaging benefits from understanding antecedents to unstable attitudes that are more likely to be influenced. This work investigates the relationship between moral and emotional bases for attitudes towards COVID-19 vaccines and variance in stance. Evaluating nearly 1 million X users over a two month period, we find that emotional language in tweets about COVID-19 vaccines is largely associated with more variation in stance of the posting user, except anger and surprise. The strength of COVID-19 vaccine attitudes associated with moral values varies across foundations. Most notably, liberty is consistently used by users with no or less variation in stance, while fairness and sanctity are used by users with more variation. Our work has implications for designing constructive pro-vaccine messaging and identifying receptive audiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19406v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Samantha C. Phillips, Lynnette Hui Xian Ng, Wenqi Zhou, Kathleen M. Carley</dc:creator>
    </item>
    <item>
      <title>The influence of Automated Decision-Making systems in the context of street-level bureaucrats' practices</title>
      <link>https://arxiv.org/abs/2407.19427</link>
      <description>arXiv:2407.19427v1 Announce Type: new 
Abstract: In an era of digital governance, the use of automation for individual and cooperative work is increasing in public administrations (Tangi et al., 2022). Despite the promises of efficiency and cost reduction, automation could bring new challenges to the governance schemes. Regional, national, and local governments are taking measures to regulate and measure the impact of automated decision-making systems (ADMS). This research focuses on the use and adoption of ADMS in European public administrations to understand how these systems have been transforming the roles, tasks, and duties of street-level bureaucrats. We conducted a qualitative study in which we interviewed street-level bureaucrats from three administrations who had used an ADMS for several years, which was embedded in their daily work routines. The outcome of our research is an analysis of five dimensions of how collaborative work, the organizational settings, the capacities of bureaucrats and the implementation of the ADMS enable or limit the capacities for offering better services towards the citizens.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19427v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Manuel Portela, A. Paula Rodriguez M\"uller, Luca Tangi</dc:creator>
    </item>
    <item>
      <title>The Traveling Mailman: Topological Optimization Methods for User-Centric Redistricting</title>
      <link>https://arxiv.org/abs/2407.19535</link>
      <description>arXiv:2407.19535v1 Announce Type: new 
Abstract: This study introduces a new districting approach using the US Postal Service network to measure community connectivity. We combine Topological Data Analysis with Markov Chain Monte Carlo methods to assess district boundaries' impact on community integrity. Using Iowa as a case study, we generate and refine districting plans using KMeans clustering and stochastic rebalancing. Our method produces plans with fewer cut edges and more compact shapes than the official Iowa plan under relaxed conditions. The low likelihood of finding plans as disruptive as the official one suggests potential inefficiencies in existing boundaries. Gaussian Mixture Model analysis reveals three distinct distributions in the districting landscape. This framework offers a more accurate reflection of community interactions for fairer political representation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19535v1</guid>
      <category>cs.CY</category>
      <category>math.SG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nelson A. Col\'on Vargas</dc:creator>
    </item>
    <item>
      <title>Machine-arranged Interactions Improve Institutional Belonging and Cohesion</title>
      <link>https://arxiv.org/abs/2407.19565</link>
      <description>arXiv:2407.19565v1 Announce Type: new 
Abstract: We investigated how participation in machine-arranged meetings were associated with feelings of institutional belonging and perceptions of demographic groups. We collected data from 535 individuals who participated in a program to meet new friends. Data consisted of surveys measuring demography, belonging, and perceptions of various demographic groups at the start and end of the program. Participants were partitioned into a control group who received zero introductions, and an intervention group who received multiple introductions. For each participant, we computed twelve features describing participation status, demography and the amount of program-facilitated exposure to others who were similar to them and different from them. We used a linear model to study the association of our features with the participants' final belonging and perceptions while controlling for their initial belonging and perceptions. We found that those who participated in the machine-arranged meetings had 4.5% higher belonging, and 3.9% more positive perception of others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19565v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad M. Ghassemi, Tuka Alhanai</dc:creator>
    </item>
    <item>
      <title>SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning</title>
      <link>https://arxiv.org/abs/2407.19604</link>
      <description>arXiv:2407.19604v1 Announce Type: new 
Abstract: Prior studies have shown that the retention time of the non-volatile spin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's write energy and latency. However, since different applications may require different retention times, STT-RAM retention times must be critically explored to satisfy various applications' needs. This process can be challenging due to exploration overhead, and exacerbated by the fact that STT-RAM caches are emerging and are not readily available for design time exploration. This paper explores using known and easily obtainable statistics (e.g., SRAM statistics) to predict the appropriate STT-RAM retention times, in order to minimize exploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model, which utilizes machine learning to enable design time or runtime prediction of right-provisioned STT-RAM retention times for latency or energy optimization. Experimental results show that, on average, SCART can reduce the latency and energy by 20.34% and 29.12%, respectively, compared to a homogeneous retention time while reducing the exploration overheads by 52.58% compared to prior work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19604v1</guid>
      <category>cs.CY</category>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/IGSC48788.2019.8957182</arxiv:DOI>
      <arxiv:journal_reference>2019 Tenth International Green and Sustainable Computing Conference (IGSC), Alexandria, VA, USA, 2019, pp. 1-7,</arxiv:journal_reference>
      <dc:creator>Dhruv Gajaria, Kyle Kuan, Tosiron Adegbija</dc:creator>
    </item>
    <item>
      <title>ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient Multicore Processors</title>
      <link>https://arxiv.org/abs/2407.19612</link>
      <description>arXiv:2407.19612v1 Announce Type: new 
Abstract: Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been widely studied as a way to reduce STT-RAM's write energy and latency overheads. Given a relaxed retention time STT-RAM level one (L1) cache, we analyze the impacts of dynamic voltage and frequency scaling (DVFS) -- a common optimization in modern processors -- on STT-RAM L1 cache design. Our analysis reveals that, apart from the fact that different applications may require different retention times, the clock frequency, which is typically ignored in most STT-RAM studies, may also significantly impact applications' retention time needs. Based on our findings, we propose an asymmetric-retention core (ARC) design for multicore architectures. ARC features retention time heterogeneity to specialize STT-RAM retention times to applications' needs. We also propose a runtime prediction model to determine the best core on which to run an application, based on the applications' characteristics, their retention time requirements, and available DVFS settings. Results reveal that the proposed approach can reduce the average cache energy by 20.19% and overall processor energy by 7.66%, compared to a homogeneous STT-RAM cache design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19612v1</guid>
      <category>cs.CY</category>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3357526.3357553</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the international symposium on memory systems, pp. 439-450. 2019</arxiv:journal_reference>
      <dc:creator>Dhruv Gajaria, Tosiron Adegbija</dc:creator>
    </item>
    <item>
      <title>CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory Processing</title>
      <link>https://arxiv.org/abs/2407.19627</link>
      <description>arXiv:2407.19627v1 Announce Type: new 
Abstract: Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures, especially those utilizing bit-line computing, offer promising solutions to mitigate data movement bottlenecks within the memory hierarchy. While previous studies have explored the integration of compute units within individual memory levels, the complexity and potential overheads associated with these designs have often limited their capabilities. This paper introduces a novel PiC/PiM architecture, Concurrent Hierarchical In-Memory Processing (CHIME), which strategically incorporates heterogeneous compute units across multiple levels of the memory hierarchy. This design targets the efficient execution of diverse, domain-specific workloads by placing computations closest to the data where it optimizes performance, energy consumption, data movement costs, and area. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing, such as high density, low leakage, and better resiliency to data corruption from activating multiple word lines. We demonstrate that CHIME enhances concurrency and improves compute unit utilization at each level of the memory hierarchy. We present strategies for exploring the design space, grouping, and placing the compute units across the memory hierarchy. Experiments reveal that, compared to the state-of-the-art bit-line computing approaches, CHIME achieves significant speedup and energy savings of 57.95% and 78.23% for various domain-specific workloads, while reducing the overheads associated with single-level compute designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19627v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dhruv Gajaria, Tosiron Adegbija, Kevin Gomez</dc:creator>
    </item>
    <item>
      <title>STT-RAM-based Hierarchical In-Memory Computing</title>
      <link>https://arxiv.org/abs/2407.19637</link>
      <description>arXiv:2407.19637v1 Announce Type: new 
Abstract: In-memory computing promises to overcome the von Neumann bottleneck in computer systems by performing computations directly within the memory. Previous research has suggested using Spin-Transfer Torque RAM (STT-RAM) for in-memory computing due to its non-volatility, low leakage power, high density, endurance, and commercial viability. This paper explores hierarchical in-memory computing, where different levels of the memory hierarchy are augmented with processing elements to optimize workload execution. The paper investigates processing in memory (PiM) using non-volatile STT-RAM and processing in cache (PiC) using volatile STT-RAM with relaxed retention, which helps mitigate STT-RAM's write latency and energy overheads. We analyze tradeoffs and overheads associated with data movement for PiC versus write overheads for PiM using STT-RAMs for various workloads. We examine workload characteristics, such as computational intensity and CPU-dependent workloads with limited instruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using these workloads, we evaluate computing in STT-RAM versus SRAM at different cache hierarchy levels and explore the potential of heterogeneous STT-RAM cache architectures with various retention times for PiC and CPU-based computing. Our experiments reveal significant advantages of STT-RAM-based PiC over PiM for specific workloads. Finally, we describe open research problems in hierarchical in-memory computing architectures to further enhance this paradigm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19637v1</guid>
      <category>cs.CY</category>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TPDS.2024.3430853</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Parallel and Distributed Systems, vol. 35, no. 9, pp. 1615-1629, Sept. 2024</arxiv:journal_reference>
      <dc:creator>Dhruv Gajaria, Kevin Antony Gomez, Tosiron Adegbija</dc:creator>
    </item>
    <item>
      <title>Navigating the United States Legislative Landscape on Voice Privacy: Existing Laws, Proposed Bills, Protection for Children, and Synthetic Data for AI</title>
      <link>https://arxiv.org/abs/2407.19677</link>
      <description>arXiv:2407.19677v1 Announce Type: new 
Abstract: Privacy is a hot topic for policymakers across the globe, including the United States. Evolving advances in AI and emerging concerns about the misuse of personal data have pushed policymakers to draft legislation on trustworthy AI and privacy protection for its citizens. This paper presents the state of the privacy legislation at the U.S. Congress and outlines how voice data is considered as part of the legislation definition. This paper also reviews additional privacy protection for children. This paper presents a holistic review of enacted and proposed privacy laws, and consideration for voice data, including guidelines for processing children's data, in those laws across the fifty U.S. states. As a groundbreaking alternative to actual human data, ethically generated synthetic data allows much flexibility to keep AI innovation in progress. Given the consideration of synthetic data in AI legislation by policymakers to be relatively new, as compared to that of privacy laws, this paper reviews regulatory considerations for synthetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19677v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satwik Dutta, John H. L. Hansen</dc:creator>
    </item>
    <item>
      <title>To accept or not to accept? An IRT-TOE Framework to Understand Educators' Resistance to Generative AI in Higher Education</title>
      <link>https://arxiv.org/abs/2407.20130</link>
      <description>arXiv:2407.20130v1 Announce Type: new 
Abstract: Since the public release of Chat Generative Pre-Trained Transformer (ChatGPT), extensive discourse has emerged concerning the potential advantages and challenges of integrating Generative Artificial Intelligence (GenAI) into education. In the realm of information systems, research on technology adoption is crucial for understanding the diverse factors influencing the uptake of specific technologies. Theoretical frameworks, refined and validated over decades, serve as guiding tools to elucidate the individual and organizational dynamics, obstacles, and perceptions surrounding technology adoption. However, while several models have been proposed, they often prioritize elucidating the factors that facilitate acceptance over those that impede it, typically focusing on the student perspective and leaving a gap in empirical evidence regarding educators viewpoints. Given the pivotal role educators play in higher education, this study aims to develop a theoretical model to empirically predict the barriers preventing educators from adopting GenAI in their classrooms. Acknowledging the lack of theoretical models tailored to identifying such barriers, our approach is grounded in the Innovation Resistance Theory (IRT) framework and augmented with constructs from the Technology-Organization-Environment (TOE) framework. This model is transformed into a measurement instrument employing a quantitative approach, complemented by a qualitative approach to enrich the analysis and uncover concerns related to GenAI adoption in the higher education domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20130v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan-Erik Kalmus, Anastasija Nikiforova</dc:creator>
    </item>
    <item>
      <title>Monitoring Time-Varying Changes of Historic Structures Through Photogrammetry-Driven Digital Twinning</title>
      <link>https://arxiv.org/abs/2407.18925</link>
      <description>arXiv:2407.18925v1 Announce Type: cross 
Abstract: Historic structures are important for our society but could be prone to structural deterioration due to long service durations and natural impacts. Monitoring the deterioration of historic structures becomes essential for stakeholders to take appropriate interventions. Existing work in the literature primarily focuses on assessing the structural damage at a given moment instead of evaluating the development of deterioration over time. To address this gap, we proposed a novel five-component digital twin framework to monitor time-varying changes in historic structures. A testbed of a casemate in Fort Soledad on the island of Guam was selected to validate our framework. Using this testbed, key implementation steps in our digital twin framework were performed. The findings from this study confirm that our digital twin framework can effectively monitor deterioration over time, which is an urgent need in the cultural heritage preservation community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18925v1</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5194/isprs-archives-XLVIII-2-2024-181-2024</arxiv:DOI>
      <dc:creator>Xiangxiong Kong</dc:creator>
    </item>
    <item>
      <title>To which reference class do you belong? Measuring racial fairness of reference classes with normative modeling</title>
      <link>https://arxiv.org/abs/2407.19114</link>
      <description>arXiv:2407.19114v1 Announce Type: cross 
Abstract: Reference classes in healthcare establish healthy norms, such as pediatric growth charts of height and weight, and are used to chart deviations from these norms which represent potential clinical risk. How the demographics of the reference class influence clinical interpretation of deviations is unknown. Using normative modeling, a method for building reference classes, we evaluate the fairness (racial bias) in reference models of structural brain images that are widely used in psychiatry and neurology. We test whether including race in the model creates fairer models. We predict self-reported race using the deviation scores from three different reference class normative models, to better understand bias in an integrated, multivariate sense. Across all of these tasks, we uncover racial disparities that are not easily addressed with existing data or commonly used modeling techniques. Our work suggests that deviations from the norm could be due to demographic mismatch with the reference class, and assigning clinical meaning to these deviations should be done with caution. Our approach also suggests that acquiring more representative samples is an urgent research priority.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19114v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saige Rutherford, Thomas Wolfers, Charlotte Fraza, Nathaniel G. Harrnet, Christian F. Beckmann, Henricus G. Ruhe, Andre F. Marquand</dc:creator>
    </item>
    <item>
      <title>Interactive Learning in Computer Science Education Supported by a Discord Chatbot</title>
      <link>https://arxiv.org/abs/2407.19266</link>
      <description>arXiv:2407.19266v1 Announce Type: cross 
Abstract: Enhancing interaction and feedback collection in a first-semester computer science course poses a significant challenge due to students' diverse needs and engagement levels. To address this issue, we created and integrated a command-based chatbot on the course communication server on Discord. The DiscordBot enables students to provide feedback on course activities through short surveys, such as exercises, quizzes, and lectures, facilitating stress-free communication with instructors. It also supports attendance tracking and introduces lectures before they start.
  The research demonstrates the effectiveness of the DiscordBot as a communication tool. The ongoing feedback allowed course instructors to dynamically adjust and improve the difficulty level of upcoming activities and promote discussion in subsequent tutor sessions. The data collected reveal that students can accurately perceive the activities' difficulty and expected results, providing insights not possible through traditional end-of-semester surveys. Students reported that interaction with the DiscordBot was easy and expressed a desire to continue using it in future semesters. This responsive approach ensures the course meets the evolving needs of students, thereby enhancing their overall learning experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19266v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Santiago Berrezueta-Guzman, Ivan Parmacli, Stephan Krusche, Stefan Wagner</dc:creator>
    </item>
    <item>
      <title>Deep Learning Based Crime Prediction Models: Experiments and Analysis</title>
      <link>https://arxiv.org/abs/2407.19324</link>
      <description>arXiv:2407.19324v1 Announce Type: cross 
Abstract: Crime prediction is a widely studied research problem due to its importance in ensuring safety of city dwellers. Starting from statistical and classical machine learning based crime prediction methods, in recent years researchers have focused on exploiting deep learning based models for crime prediction. Deep learning based crime prediction models use complex architectures to capture the latent features in the crime data, and outperform the statistical and classical machine learning based crime prediction methods. However, there is a significant research gap in existing research on the applicability of different models in different real-life scenarios as no longitudinal study exists comparing all these approaches in a unified setting. In this paper, we conduct a comprehensive experimental evaluation of all major state-of-the-art deep learning based crime prediction models. Our evaluation provides several key insights on the pros and cons of these models, which enables us to select the most suitable models for different application scenarios. Based on the findings, we further recommend certain design practices that should be taken into account while building future deep learning based crime prediction models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19324v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rittik Basak Utsha, Muhtasim Noor Alif, Yeasir Rayhan, Tanzima Hashem, Mohammad Eunus Ali</dc:creator>
    </item>
    <item>
      <title>Enhancing Group Fairness in Federated Learning through Personalization</title>
      <link>https://arxiv.org/abs/2407.19331</link>
      <description>arXiv:2407.19331v1 Announce Type: cross 
Abstract: Personalized Federated Learning (FL) algorithms collaboratively train customized models for each client, enhancing the accuracy of the learned models on the client's local data (e.g., by clustering similar clients, or by fine-tuning models locally). In this paper, we investigate the impact of such personalization techniques on the group fairness of the learned models, and show that personalization can also lead to improved (local) fairness as an unintended benefit. We begin by illustrating these benefits of personalization through numerical experiments comparing two classes of personalized FL algorithms (clustering and fine-tuning) against a baseline FedAvg algorithm, elaborating on the reasons behind improved fairness using personalized FL, and then providing analytical support. Motivated by these, we further propose a new, Fairness-aware Federated Clustering Algorithm, Fair-FCA, in which clients can be clustered to obtain a (tuneable) fairness-accuracy tradeoff. Through numerical experiments, we demonstrate the ability of Fair-FCA to strike a balance between accuracy and fairness at the client level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19331v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Yang, Ali Payani, Parinaz Naghizadeh</dc:creator>
    </item>
    <item>
      <title>Business and Regulatory Responses to Artificial Intelligence: Dynamic Regulation, Innovation Ecosystems and the Strategic Management of Disruptive Technology</title>
      <link>https://arxiv.org/abs/2407.19439</link>
      <description>arXiv:2407.19439v1 Announce Type: cross 
Abstract: Identifying and then implementing an effective response to disruptive new AI technologies is enormously challenging for any business looking to integrate AI into their operations, as well as regulators looking to leverage AI-related innovation as a mechanism for achieving regional economic growth. These business and regulatory challenges are particularly significant given the broad reach of AI, as well as the multiple uncertainties surrounding such technologies and their future development and effects. This article identifies two promising strategies for meeting the AI challenge, focusing on the example of Fintech. First, dynamic regulation, in the form of regulatory sandboxes and other regulatory approaches that aim to provide a space for responsible AI-related innovation. An empirical study provides preliminary evidence to suggest that jurisdictions that adopt a more proactive approach to Fintech regulation can attract greater investment. The second strategy relates to so-called innovation ecosystems. It is argued that such ecosystems are most effective when they afford opportunities for creative partnerships between well-established corporations and AI-focused startups and that this aspect of a successful innovation ecosystem is often overlooked in the existing discussion. The article suggests that these two strategies are interconnected, in that greater investment is an important element in both fostering and signaling a well-functioning innovation ecosystem and that a well-functioning ecosystem will, in turn, attract more funding. The resulting synergies between these strategies can, therefore, provide a jurisdiction with a competitive edge in becoming a regional hub for AI-related activity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19439v1</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark Fenwick, Erik P. M. Vermeulen, Marcelo Corrales Compagnucci</dc:creator>
    </item>
    <item>
      <title>"A Good Bot Always Knows Its Limitations": Assessing Autonomous System Decision-making Competencies through Factorized Machine Self-confidence</title>
      <link>https://arxiv.org/abs/2407.19631</link>
      <description>arXiv:2407.19631v1 Announce Type: cross 
Abstract: How can intelligent machines assess their competencies in completing tasks? This question has come into focus for autonomous systems that algorithmically reason and make decisions under uncertainty. It is argued here that machine self-confidence -- a form of meta-reasoning based on self-assessments of an agent's knowledge about the state of the world and itself, as well as its ability to reason about and execute tasks -- leads to many eminently computable and useful competency indicators for such agents. This paper presents a culmination of work on this concept in the form of a computational framework called Factorized Machine Self-confidence (FaMSeC), which provides an engineering-focused holistic description of factors driving an algorithmic decision-making process, including outcome assessment, solver quality, model quality, alignment quality, and past experience. In FaMSeC, self-confidence indicators are derived from hierarchical `problem-solving statistics' embedded within broad classes of probabilistic decision-making algorithms such as Markov decision processes. The problem-solving statistics are obtained by evaluating and grading probabilistic exceedance margins with respect to given competency standards, which are specified for each decision-making competency factor by the informee (e.g. a non-expert user or an expert system designer). This approach allows `algorithmic goodness of fit' evaluations to be easily incorporated into the design of many kinds of autonomous agents via human-interpretable competency self-assessment reports. Detailed descriptions and running application examples for a Markov decision process agent show how two FaMSeC factors (outcome assessment and solver quality) can be practically computed and reported for a range of possible tasking contexts through novel use of meta-utility functions, behavior simulations, and surrogate prediction models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19631v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brett Israelsen, Nisar R. Ahmed, Matthew Aitken, Eric W. Frew, Dale A. Lawrence, Brian M. Argrow</dc:creator>
    </item>
    <item>
      <title>PersonalityScanner: Exploring the Validity of Personality Assessment Based on Multimodal Signals in Virtual Reality</title>
      <link>https://arxiv.org/abs/2407.19728</link>
      <description>arXiv:2407.19728v1 Announce Type: cross 
Abstract: Human cognition significantly influences expressed behavior and is intrinsically tied to authentic personality traits. Personality assessment plays a pivotal role in various fields, including psychology, education, social media, etc. However, traditional self-report questionnaires can only provide data based on what individuals are willing and able to disclose, thereby lacking objective. Moreover, automated measurements and peer assessments demand significant human effort and resources. In this paper, given the advantages of the Virtual Reality (VR) technique, we develop a VR simulator -- PersonalityScanner, to stimulate cognitive processes and simulate daily behaviors based on an immersive and interactive simulation environment, in which participants carry out a battery of engaging tasks that formulate a natural story of first-day at work. Through this simulator, we collect a synchronous multi-modal dataset with ten modalities, including first/third-person video, audio, text, eye tracking, facial microexpression, pose, depth data, log, and inertial measurement unit. By systematically examining the contributions of different modalities on revealing personality, we demonstrate the superior performance and effectiveness of PersonalityScanner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19728v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xintong Zhang, Di Lu, Huiqi Hu, Nan Jiang, Xianhao Yu, Jinan Xu, Yujia Peng, Qing Li, Wenjuan Han</dc:creator>
    </item>
    <item>
      <title>Legal Minds, Algorithmic Decisions: How LLMs Apply Constitutional Principles in Complex Scenarios</title>
      <link>https://arxiv.org/abs/2407.19760</link>
      <description>arXiv:2407.19760v1 Announce Type: cross 
Abstract: In this paper, we conduct an empirical analysis of how large language models (LLMs), specifically GPT-4, interpret constitutional principles in complex decision-making scenarios. We examine rulings from the Italian Constitutional Court on bioethics issues that involve trade-offs between competing values and compare model-generated legal arguments on these issues to those presented by the State, the Court, and the applicants. Our results indicate that GPT-4 consistently aligns more closely with progressive interpretations of the Constitution, often overlooking competing values and mirroring the applicants' views rather than the more conservative perspectives of the State or the Court's moderate positions. Our experiments reveal a distinct tendency of GPT-4 to favor progressive legal interpretations, underscoring the influence of underlying data biases. We thus underscore the importance of testing alignment in real-world scenarios and considering the implications of deploying LLMs in decision-making processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19760v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Camilla Bignotti, Carolina Camassa</dc:creator>
    </item>
    <item>
      <title>Fairness Through Controlled (Un)Awareness in Node Embeddings</title>
      <link>https://arxiv.org/abs/2407.20024</link>
      <description>arXiv:2407.20024v1 Announce Type: cross 
Abstract: Graph representation learning is central for the application of machine learning (ML) models to complex graphs, such as social networks. Ensuring `fair' representations is essential, due to the societal implications and the use of sensitive personal data. In this paper, we demonstrate how the parametrization of the \emph{CrossWalk} algorithm influences the ability to infer a sensitive attributes from node embeddings. By fine-tuning hyperparameters, we show that it is possible to either significantly enhance or obscure the detectability of these attributes. This functionality offers a valuable tool for improving the fairness of ML systems utilizing graph embeddings, making them adaptable to different fairness paradigms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20024v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dennis Vetter, Jasper Forth, Gemma Roig, Holger Dell</dc:creator>
    </item>
    <item>
      <title>Mapping Election Polarization and Competitiveness using Election Results</title>
      <link>https://arxiv.org/abs/2308.10862</link>
      <description>arXiv:2308.10862v2 Announce Type: replace 
Abstract: The simplified hypothesis that an election is polarized as an explanation of recent electoral outcomes worldwide is centered on perceptions of voting patterns rather than ideological data from the electorate. While the literature focuses on measuring polarization using ideological-like data from electoral studies-which are limited to economically advantageous countries and are representative mostly to national scales-we argue that, in fact, voting patterns can lead to mapping effective proxies of citizen divisions on election day. This paper perspectives two complementary concepts, Election Polarization (EP) and Election Competitiveness (EC), as a means to understand voting patterns on Election Day. We present an agnostic approach that relies solely on election data and validate it using synthetic and real-world election data across 13 countries in the Eurozone, North America, Latin America, and New Zealand. Overall, we find that we can label and distinguish expectations of polarized and competitive elections in these countries, and we report that EP positively correlates with a metric of political polarization in the U.S., unlocking opportunities for studies of polarization at the regional level and for lower/middle-income countries where electoral studies are available, but surveys are limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.10862v2</guid>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Navarrete, Mariana Macedo, Viktor Stojkoski, Marcela Parada-Contzen, Christopher A Mart\'inez</dc:creator>
    </item>
    <item>
      <title>Data Privacy Vocabulary (DPV) -- Version 2</title>
      <link>https://arxiv.org/abs/2404.13426</link>
      <description>arXiv:2404.13426v2 Announce Type: replace 
Abstract: The Data Privacy Vocabulary (DPV), developed by the W3C Data Privacy Vocabularies and Controls Community Group (DPVCG), enables the creation of machine-readable, interoperable, and standards-based representations for describing the processing of personal data. The group has also published extensions to the DPV to describe specific applications to support legislative requirements such as the EU's GDPR. The DPV fills a crucial niche in the state of the art by providing a vocabulary that can be embedded and used alongside other existing standards such as W3C ODRL, and which can be customised and extended for adapting to specifics of use-cases or domains. This article describes the version 2 iteration of the DPV in terms of its contents, methodology, current adoptions and uses, and future potential. It also describes the relevance and role of DPV in acting as a common vocabulary to support various regulatory (e.g. EU's DGA and AI Act) and community initiatives (e.g. Solid) emerging across the globe.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13426v2</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harshvardhan J. Pandit, Beatriz Esteves, Georg P. Krog, Paul Ryan, Delaram Golpayegani, Julian Flake</dc:creator>
    </item>
    <item>
      <title>Strategies for Increasing Corporate Responsible AI Prioritization</title>
      <link>https://arxiv.org/abs/2405.03855</link>
      <description>arXiv:2405.03855v2 Announce Type: replace 
Abstract: Responsible artificial intelligence (RAI) is increasingly recognized as a critical concern. However, the level of corporate RAI prioritization has not kept pace. In this work, we conduct 16 semi-structured interviews with practitioners to investigate what has historically motivated companies to increase the prioritization of RAI. What emerges is a complex story of conflicting and varied factors, but we bring structure to the narrative by highlighting the different strategies available to employ, and point to the actors with access to each. While there are no guaranteed steps for increasing RAI prioritization, we paint the current landscape of motivators so that practitioners can learn from each other, and put forth our own selection of promising directions forward.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03855v2</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Angelina Wang, Teresa Datta, John P. Dickerson</dc:creator>
    </item>
    <item>
      <title>Unpacking Approaches to Learning and Teaching Machine Learning in K-12 Education: Transparency, Ethics, and Design Activities</title>
      <link>https://arxiv.org/abs/2406.03480</link>
      <description>arXiv:2406.03480v2 Announce Type: replace 
Abstract: In this conceptual paper, we review existing literature on artificial intelligence/machine learning (AI/ML) education to identify three approaches to how learning and teaching ML could be conceptualized. One of them, a data-driven approach, emphasizes providing young people with opportunities to create data sets, train, and test models. A second approach, learning algorithm-driven, prioritizes learning about how the learning algorithms or engines behind how ML models work. In addition, we identify efforts within a third approach that integrates the previous two. In our review, we focus on how the approaches: (1) glassbox and blackbox different aspects of ML, (2) build on learner interests and provide opportunities for designing applications, (3) integrate ethics and justice. In the discussion, we address the challenges and opportunities of current approaches and suggest future directions for the design of learning activities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03480v2</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>The 19th WiPSCE Conference on Primary and Secondary Computing Education Research 2024</arxiv:journal_reference>
      <dc:creator>Luis Morales-Navarro, Yasmin B. Kafai</dc:creator>
    </item>
    <item>
      <title>Cutting through the noise to motivate people: A comprehensive analysis of COVID-19 social media posts de/motivating vaccination</title>
      <link>https://arxiv.org/abs/2407.03190</link>
      <description>arXiv:2407.03190v2 Announce Type: replace 
Abstract: The COVID-19 pandemic exposed significant weaknesses in the healthcare information system. The overwhelming volume of misinformation on social media and other socioeconomic factors created extraordinary challenges to motivate people to take proper precautions and get vaccinated. In this context, our work explored a novel direction by analyzing an extensive dataset collected over two years, identifying the topics de/motivating the public about COVID-19 vaccination. We analyzed these topics based on time, geographic location, and political orientation. We noticed that while the motivating topics remain the same over time and geographic location, the demotivating topics change rapidly. We also identified that intrinsic motivation, rather than external mandate, is more advantageous to inspire the public. This study addresses scientific communication and public motivation in social media. It can help public health officials, policymakers, and social media platforms develop more effective messaging strategies to cut through the noise of misinformation and educate the public about scientific findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03190v2</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.nlp.2024.100085</arxiv:DOI>
      <arxiv:journal_reference>Natural Language Processing Journal, Volume 8, 2024, 100085, ISSN 2949-7191</arxiv:journal_reference>
      <dc:creator>Ashiqur Rahman, Ehsan Mohammadi, Hamed Alhoori</dc:creator>
    </item>
    <item>
      <title>Evaluation of LLMs Biases Towards Elite Universities: A Persona-Based Exploration</title>
      <link>https://arxiv.org/abs/2407.12801</link>
      <description>arXiv:2407.12801v3 Announce Type: replace 
Abstract: This study investigates whether popular LLMs exhibit bias towards elite universities when generating personas for technology industry professionals. We employed a novel persona-based approach to compare the educational background predictions of GPT-3.5, Gemini, and Claude 3 Sonnet with actual data from LinkedIn. The study focused on various roles at Microsoft, Meta, and Google, including VP Product, Director of Engineering, and Software Engineer. We generated 432 personas across the three LLMs and analyzed the frequency of elite universities (Stanford, MIT, UC Berkeley, and Harvard) in these personas compared to LinkedIn data. Results showed that LLMs significantly overrepresented elite universities, featuring these universities 72.45% of the time, compared to only 8.56% in the actual LinkedIn data. ChatGPT 3.5 exhibited the highest bias, followed by Claude Sonnet 3, while Gemini performed best. This research highlights the need to address educational bias in LLMs and suggests strategies for mitigating such biases in AI-driven recruitment processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12801v3</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shailja Gupta, Rajesh Ranjan</dc:creator>
    </item>
    <item>
      <title>AI Emergency Preparedness: Examining the federal government's ability to detect and respond to AI-related national security threats</title>
      <link>https://arxiv.org/abs/2407.17347</link>
      <description>arXiv:2407.17347v2 Announce Type: replace 
Abstract: We examine how the federal government can enhance its AI emergency preparedness: the ability to detect and prepare for time-sensitive national security threats relating to AI. Emergency preparedness can improve the government's ability to monitor and predict AI progress, identify national security threats, and prepare effective response plans for plausible threats and worst-case scenarios. Our approach draws from fields in which experts prepare for threats despite uncertainty about their exact nature or timing (e.g., counterterrorism, cybersecurity, pandemic preparedness). We focus on three plausible risk scenarios: (1) loss of control (threats from a powerful AI system that becomes capable of escaping human control), (2) cybersecurity threats from malicious actors (threats from a foreign actor that steals the model weights of a powerful AI system), and (3) biological weapons proliferation (threats from users identifying a way to circumvent the safeguards of a publicly-released model in order to develop biological weapons.) We evaluate the federal government's ability to detect, prevent, and respond to these threats. Then, we highlight potential gaps and offer recommendations to improve emergency preparedness. We conclude by describing how future work on AI emergency preparedness can be applied to improve policymakers' understanding of risk scenarios, identify gaps in detection capabilities, and form preparedness plans to improve the effectiveness of federal responses to AI-related national security threats.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17347v2</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akash Wasil, Everett Smith, Corin Katzke, Justin Bullock</dc:creator>
    </item>
    <item>
      <title>How Does Fine-Tuning Impact Out-of-Distribution Detection for Vision-Language Models?</title>
      <link>https://arxiv.org/abs/2306.06048</link>
      <description>arXiv:2306.06048v3 Announce Type: replace-cross 
Abstract: Recent large vision-language models such as CLIP have shown remarkable out-of-distribution (OOD) detection and generalization performance. However, their zero-shot in-distribution (ID) accuracy is often limited for downstream datasets. Recent CLIP-based fine-tuning methods such as prompt learning have demonstrated significant improvements in ID classification and OOD generalization where OOD labels are available. Nonetheless, it remains unclear whether the model is reliable to semantic shifts without OOD labels. In this paper, we aim to bridge the gap and present a comprehensive study to understand how fine-tuning impact OOD detection for few-shot downstream tasks. By framing OOD detection as multi-modal concept matching, we establish a connection between fine-tuning methods and various OOD scores. Our results suggest that a proper choice of OOD scores is essential for CLIP-based fine-tuning. In particular, the maximum concept matching (MCM) score provides a promising solution consistently. We also show that prompt learning demonstrates the state-of-the-art OOD detection performance over the zero-shot counterpart.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.06048v3</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11263-023-01895-7</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Computer Vision 2023</arxiv:journal_reference>
      <dc:creator>Yifei Ming, Yixuan Li</dc:creator>
    </item>
    <item>
      <title>Scaling Laws Do Not Scale</title>
      <link>https://arxiv.org/abs/2307.03201</link>
      <description>arXiv:2307.03201v2 Announce Type: replace-cross 
Abstract: Recent work has advocated for training AI models on ever-larger datasets, arguing that as the size of a dataset increases, the performance of a model trained on that dataset will correspondingly increase (referred to as "scaling laws"). In this paper, we draw on literature from the social sciences and machine learning to critically interrogate these claims. We argue that this scaling law relationship depends on metrics used to measure performance that may not correspond with how different groups of people perceive the quality of models' output. As the size of datasets used to train large AI models grows and AI systems impact ever larger groups of people, the number of distinct communities represented in training or evaluation datasets grows. It is thus even more likely that communities represented in datasets may have values or preferences not reflected in (or at odds with) the metrics used to evaluate model performance in scaling laws. Different communities may also have values in tension with each other, leading to difficult, potentially irreconcilable choices about metrics used for model evaluations -- threatening the validity of claims that model performance is improving at scale. We end the paper with implications for AI development: that the motivation for scraping ever-larger datasets may be based on fundamentally flawed assumptions about model performance. That is, models may not, in fact, continue to improve as the datasets get larger -- at least not for all people or communities impacted by those models. We suggest opportunities for the field to rethink norms and values in AI development, resisting claims for universality of large models, fostering more local, small-scale designs, and other ways to resist the impetus towards scale in AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.03201v2</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fernando Diaz, Michael Madaio</dc:creator>
    </item>
    <item>
      <title>Uncertainty-Aware Probabilistic Graph Neural Networks for Road-Level Traffic Accident Prediction</title>
      <link>https://arxiv.org/abs/2309.05072</link>
      <description>arXiv:2309.05072v4 Announce Type: replace-cross 
Abstract: Traffic accidents present substantial challenges to human safety and socio-economic development in urban areas. Developing a reliable and responsible traffic accident prediction model is crucial to addressing growing public safety concerns and enhancing the safety of urban mobility systems. Traditional methods face limitations at fine spatiotemporal scales due to the sporadic nature of highrisk accidents and the predominance of non-accident characteristics. Furthermore, while most current models show promising occurrence prediction, they overlook the uncertainties arising from the inherent nature of accidents, and then fail to adequately map the hierarchical ranking of accident risk values for more precise insights. To address these issues, we introduce the Spatiotemporal Zero-Inflated Tweedie Graph Neural Network STZITDGNN -- the first uncertainty-aware probabilistic graph deep learning model in roadlevel traffic accident prediction for multisteps. This model integrates the interpretability of the statistical Tweedie family model and the expressive power of graph neural networks. Its decoder innovatively employs a compound Tweedie model,a Poisson distribution to model the frequency of accident occurrences and a Gamma distribution to assess injury severity, supplemented by a zeroinflated component to effectively identify exessive nonincident instances. Empirical tests using realworld traffic data from London, UK, demonstrate that the STZITDGNN surpasses other baseline models across multiple benchmarks and metrics, including accident risk value prediction, uncertainty minimisation, non-accident road identification and accident occurrence accuracy. Our study demonstrates that STZTIDGNN can effectively inform targeted road monitoring, thereby improving urban road safety strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.05072v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaowei Gao, Xinke Jiang, Dingyi Zhuang, Huanfa Chen, Shenhao Wang, Stephen Law, James Haworth</dc:creator>
    </item>
    <item>
      <title>LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins</title>
      <link>https://arxiv.org/abs/2309.10254</link>
      <description>arXiv:2309.10254v2 Announce Type: replace-cross 
Abstract: Large language model (LLM) platforms, such as ChatGPT, have recently begun offering an app ecosystem to interface with third-party services on the internet. While these apps extend the capabilities of LLM platforms, they are developed by arbitrary third parties and thus cannot be implicitly trusted. Apps also interface with LLM platforms and users using natural language, which can have imprecise interpretations. In this paper, we propose a framework that lays a foundation for LLM platform designers to analyze and improve the security, privacy, and safety of current and future third-party integrated LLM platforms. Our framework is a formulation of an attack taxonomy that is developed by iteratively exploring how LLM platform stakeholders could leverage their capabilities and responsibilities to mount attacks against each other. As part of our iterative process, we apply our framework in the context of OpenAI's plugin (apps) ecosystem. We uncover plugins that concretely demonstrate the potential for the types of issues that we outline in our attack taxonomy. We conclude by discussing novel challenges and by providing recommendations to improve the security, privacy, and safety of present and future LLM-based computing platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10254v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Umar Iqbal, Tadayoshi Kohno, Franziska Roesner</dc:creator>
    </item>
    <item>
      <title>Analyzing User Characteristics of Hate Speech Spreaders on Social Media</title>
      <link>https://arxiv.org/abs/2310.15772</link>
      <description>arXiv:2310.15772v2 Announce Type: replace-cross 
Abstract: Hate speech on social media threatens the mental and physical well-being of individuals and contributes to real-world violence. Resharing is an important driver behind the spread of hate speech on social media. Yet, little is known about who reshares hate speech and what their characteristics are. In this paper, we analyze the role of user characteristics in hate speech resharing across different types of hate speech (e.g., political hate). For this, we proceed as follows: First, we cluster hate speech posts using large language models to identify different types of hate speech. Then we model the effects of user attributes on users' probability to reshare hate speech using an explainable machine learning model. To do so, we apply debiasing to control for selection bias in our observational social media data and further control for the latent vulnerability of users to hate speech. We find that, all else equal, users with fewer followers, fewer friends, fewer posts, and older accounts share more hate speech. This shows that users with little social influence tend to share more hate speech. Further, we find substantial heterogeneity across different types of hate speech. For example, racist and misogynistic hate is spread mostly by users with little social influence. In contrast, political anti-Trump and anti-right-wing hate is reshared by users with larger social influence. Overall, understanding the factors that drive users to share hate speech is crucial for detecting individuals at risk of engaging in harmful behavior and for designing effective mitigation strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.15772v2</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominique Geissler, Abdurahman Maarouf, Stefan Feuerriegel</dc:creator>
    </item>
    <item>
      <title>Complexity-Theoretic Implications of Multicalibration</title>
      <link>https://arxiv.org/abs/2312.17223</link>
      <description>arXiv:2312.17223v2 Announce Type: replace-cross 
Abstract: We present connections between the recent literature on multigroup fairness for prediction algorithms and classical results in computational complexity. Multiaccurate predictors are correct in expectation on each member of an arbitrary collection of pre-specified sets. Multicalibrated predictors satisfy a stronger condition: they are calibrated on each set in the collection. Multiaccuracy is equivalent to a regularity notion for functions defined by Trevisan, Tulsiani, and Vadhan (2009). They showed that, given a class $F$ of (possibly simple) functions, an arbitrarily complex function $g$ can be approximated by a low-complexity function $h$ that makes a small number of oracle calls to members of $F$, where the notion of approximation requires that $h$ cannot be distinguished from $g$ by members of $F$. This complexity-theoretic Regularity Lemma is known to have implications in different areas, including in complexity theory, additive number theory, information theory, graph theory, and cryptography. Starting from the stronger notion of multicalibration, we obtain stronger and more general versions of a number of applications of the Regularity Lemma, including the Hardcore Lemma, the Dense Model Theorem, and the equivalence of conditional pseudo-min-entropy and unpredictability. For example, we show that every boolean function (regardless of its hardness) has a small collection of disjoint hardcore sets, where the sizes of those hardcore sets are related to how balanced the function is on corresponding pieces of an efficient partition of the domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17223v2</guid>
      <category>cs.CC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S\'ilvia Casacuberta, Cynthia Dwork, Salil Vadhan</dc:creator>
    </item>
    <item>
      <title>LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning</title>
      <link>https://arxiv.org/abs/2404.00027</link>
      <description>arXiv:2404.00027v4 Announce Type: replace-cross 
Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00027v4</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Azmine Toushik Wasi, Mst Rafia Islam, Raima Islam</dc:creator>
    </item>
    <item>
      <title>An Exploratory Case Study on Data Breach Journalism</title>
      <link>https://arxiv.org/abs/2405.01446</link>
      <description>arXiv:2405.01446v2 Announce Type: replace-cross 
Abstract: This paper explores the novel topic of data breach journalism and data breach news through the case of databreaches.net, a news outlet dedicated to data breaches and related cyber crime. Motivated by the issues in traditional crime news and crime journalism, the case is explored by the means of text mining. According to the results, the outlet has kept a steady publishing pace, mainly focusing on plain and short reporting but with generally high-quality source material for the news articles. Despite these characteristics, the news articles exhibit fairly strong sentiments, which is partially expected due to the presence of emotionally laden crime and the long history of sensationalism in crime news. The news site has also covered the full scope of data breaches, although many of these are fairly traditional, exposing personal identifiers and financial details of the victims. Also hospitals and the healthcare sector stand out. With these results, the paper advances the study of data breaches by considering these from the perspective of media and journalism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01446v2</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3664476.3670456</arxiv:DOI>
      <dc:creator>Jukka Ruohonen, Kalle Hjerppe, Maximilian von Zastrow</dc:creator>
    </item>
    <item>
      <title>CogErgLLM: Exploring Large Language Model Systems Design Perspective Using Cognitive Ergonomics</title>
      <link>https://arxiv.org/abs/2407.02885</link>
      <description>arXiv:2407.02885v2 Announce Type: replace-cross 
Abstract: Integrating cognitive ergonomics with LLMs is essential for enhancing safety, reliability, and user satisfaction in human-AI interactions. Current LLM design often lacks this integration, leading to systems that may not fully align with human cognitive capabilities and limitations. Insufficient focus on incorporating cognitive science methods exacerbates biases in LLM outputs, while inconsistent application of user-centered design principles results in sub-optimal user experiences. To address these challenges, our position paper explores the critical integration of cognitive ergonomics principles into LLM design, aiming to provide a comprehensive framework and practical guidelines for ethical LLM development. Through our contributions, we seek to advance understanding and practice in integrating cognitive ergonomics into LLM systems, fostering safer, more reliable, and ethically sound human-AI interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02885v2</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Azmine Toushik Wasi</dc:creator>
    </item>
  </channel>
</rss>
