<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Mar 2024 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 26 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Approximation Algorithms for Network Design in Non-Uniform Fault Models</title>
      <link>https://arxiv.org/abs/2403.15547</link>
      <description>arXiv:2403.15547v1 Announce Type: new 
Abstract: The Survivable Network Design problem (SNDP) is a well-studied problem, motivated by the design of networks that are robust to faults under the assumption that any subset of edges up to a specific number can fail. We consider non-uniform fault models where the subset of edges that fail can be specified in different ways. Our primary interest is in the flexible graph connectivity model, in which the edge set is partitioned into safe and unsafe edges. The goal is to design a network that has desired connectivity properties under the assumption that only unsafe edges up to a specific number can fail. We also discuss the bulk-robust model and the relative survivable network design model. While SNDP admits a 2-approximation, the approximability of problems in these more complex models is much less understood even in special cases. We make two contributions.
  Our first set of results are in the flexible graph connectivity model. Motivated by a conjecture that a constant factor approximation is feasible when the robustness parameters are fixed constants, we consider two important special cases, namely the single pair case, and the global connectivity case. For both these, we obtain constant factor approximations in several parameter ranges of interest. These are based on an augmentation framework and via decomposing the families of cuts that need to be covered into a small number of uncrossable families.
  Our second set of results are poly-logarithmic approximations for the bulk-robust model when the "width" of the given instance (the maximum number of edges that can fail in any particular scenario) is fixed. Via this, we derive corresponding approximations for the flexible graph connectivity model and the relative survivable network design model. The results are obtained via two algorithmic approaches and they have different tradeoffs in terms of the approximation ratio and generality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15547v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chandra Chekuri, Rhea Jain</dc:creator>
    </item>
    <item>
      <title>Approximation Algorithms for School Assignment: Group Fairness and Multi-criteria Optimization</title>
      <link>https://arxiv.org/abs/2403.15623</link>
      <description>arXiv:2403.15623v1 Announce Type: new 
Abstract: We consider the problem of assigning students to schools, when students have different utilities for schools and schools have capacity. There are additional group fairness considerations over students that can be captured either by concave objectives, or additional constraints on the groups. We present approximation algorithms for this problem via convex program rounding that achieve various trade-offs between utility violation, capacity violation, and running time. We also show that our techniques easily extend to the setting where there are arbitrary covering constraints on the feasible assignment, capturing multi-criteria and ranking optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15623v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Santhini K. A., Kamesh Munagala, Meghana Nasre, Govind S. Sankar</dc:creator>
    </item>
    <item>
      <title>Distance Adjustment of a Graph Drawing Stress Model</title>
      <link>https://arxiv.org/abs/2403.15811</link>
      <description>arXiv:2403.15811v1 Announce Type: new 
Abstract: Stress models are a promising approach for graph drawing. They minimize the weighted sum of the squared errors of the Euclidean and desired distances for each node pair. The desired distance typically uses the graph-theoretic distances obtained from the all-node pair shortest path problem. In a minimized stress function, the obtained coordinates are affected by the non-Euclidean property and the high-dimensionality of the graph-theoretic distance matrix. Therefore, the graph-theoretic distances used in stress models may not necessarily be the best metric for determining the node coordinates. In this study, we propose two different methods of adjusting the graph-theoretical distance matrix to a distance matrix suitable for graph drawing while preserving its structure. The first method is the application of eigenvalue decomposition to the inner product matrix obtained from the distance matrix and the obtainment of a new distance matrix by setting some eigenvalues with small absolute values to zero. The second approach is the usage of a stress model modified by adding a term that minimizes the Frobenius norm between the adjusted and original distance matrices. We perform computational experiments using several benchmark graphs to demonstrate that the proposed method improves some quality metrics, including the node resolution and the Gabriel graph property, when compared to conventional stress models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15811v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yosuke Onoue</dc:creator>
    </item>
    <item>
      <title>On the complexity and approximability of Bounded access Lempel Ziv coding</title>
      <link>https://arxiv.org/abs/2403.15871</link>
      <description>arXiv:2403.15871v1 Announce Type: new 
Abstract: We study the complexity of constructing an optimal parsing $\varphi$ of a string ${\bf s} = s_1 \dots s_n$ under the constraint that given a position $p$ in the original text, and the LZ76-like (Lempel Ziv 76) encoding of $T$ based on $\varphi$, it is possible to identify/decompress the character $s_p$ by performing at most $c$ accesses to the LZ encoding, for a given integer $c.$ We refer to such a parsing $\varphi$ as a $c$-bounded access LZ parsing or $c$-BLZ parsing of ${\bf s}.$ We show that for any constant $c$ the problem of computing the optimal $c$-BLZ parsing of a string, i.e., the one with the minimum number of phrases, is NP-hard and also APX hard, i.e., no PTAS can exist under the standard complexity assumption $P \neq NP.$ We also study the ratio between the sizes of an optimal $c$-BLZ parsing of a string ${\bf s}$ and an optimal LZ76 parsing of ${\bf s}$ (which can be greedily computed in polynomial time).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15871v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ferdinando Cicalese, Francesca Ugazio</dc:creator>
    </item>
    <item>
      <title>Convolution and Knapsack in Higher Dimensions</title>
      <link>https://arxiv.org/abs/2403.16117</link>
      <description>arXiv:2403.16117v1 Announce Type: new 
Abstract: In the Knapsack problem, one is given the task of packing a knapsack of a given size with items in order to gain a packing with a high profit value. In recent years, a connection to the $(\max,+)$-convolution problem has been established, where knapsack solutions can be combined by building the convolution of two sequences. This observation has been used to give conditional lower bounds but also parameterized algorithms.
  In this paper we want to carry these results into higher dimension. We consider Knapsack where items are characterized by multiple properties - given through a vector - and a knapsack that has a capacity vector. The packing must now not exceed any of the given capacity constraints. In order to show a similar sub-quadratic lower bound we introduce a multi-dimensional version of convolution as well. Instead of combining sequences, we will generalize this problem and combine higher dimensional matrices. We will establish a few variants of these problems and prove that they are all equivalent in terms of algorithms that allow for a running time sub-quadratic in the number of entries of the matrix.
  We further develop a parameterized algorithm to solve higher dimensional Knapsack. The techniques we apply are inspired by an algorithm introduced by Axiotis and Tzamos. In general, we manage not only to extend their result to higher dimension. We will show that even for higher dimensional Knapsack, we can reduce the problem to convolution on one-dimensional sequences, leading to an $\mathcal{O}(d(n + D \cdot \max\{\Pi_{i=1}^d{t_i}, t_{\max}\log t_{\max}\} ))$ algorithm, where $D$ is the number of different weight vectors, $t$ the capacity vector and $d$ is the dimension of the problem. Finally we also modify this algorithm to handle items with negative weights to cross the bridge from solving not only Knapsack but also Integer Linear Programs (ILPs) in general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16117v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kilian Grage, Klaus Jansen</dc:creator>
    </item>
    <item>
      <title>Hashing geographical point data using the space-filling H-curve</title>
      <link>https://arxiv.org/abs/2403.16216</link>
      <description>arXiv:2403.16216v1 Announce Type: new 
Abstract: We construct geohashing procedure based on using of space-filling H-curve. This curve provides a way to construct geohash with less computations than the construction based on usage of Hilbert curve. At the same time, H-curve has better clustering properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16216v1</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Igor V. Netay</dc:creator>
    </item>
    <item>
      <title>A Novel exact algorithm for economic lot-sizing with piecewise linear production costs</title>
      <link>https://arxiv.org/abs/2403.16314</link>
      <description>arXiv:2403.16314v1 Announce Type: new 
Abstract: In this paper, we study the single-item economic lot-sizing problem with production cost functions that are piecewise linear. The lot-sizing problem stands as a foundational cornerstone within the domain of lot-sizing problems. It is also applicable to a variety of important production planning problems which are special cases to it according to \cite{ou}. The problem becomes intractable when $m$, the number of different breakpoints of the production-cost function is variable as the problem was proven NP-hard by \cite{Florian1980}. For a fixed $m$ an $O(T^{2m+3})$ time algorithm was given by \cite{Koca2014} which was subsequently improved to $O(T^{m+2}\log(T))$ time by \cite{ou} where $T$ is the number of periods in the planning horizon.\newline We introduce a more efficient $O(T^{m+2})$ time algorithm for this problem which improves upon the previous state-of-the-art algorithm by Ou and which is derived using several novel algorithmic techniques that may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16314v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kleitos Papadopoulos</dc:creator>
    </item>
    <item>
      <title>Adaptive Frequency Bin Interval in FFT via Dense Sampling Factor $\alpha$</title>
      <link>https://arxiv.org/abs/2403.16665</link>
      <description>arXiv:2403.16665v1 Announce Type: new 
Abstract: The Fast Fourier Transform (FFT) is a fundamental tool for signal analysis, widely used across various fields. However, traditional FFT methods encounter challenges in adjusting the frequency bin interval, which may impede accurate spectral analysis. In this study, we propose a method for adjusting the frequency bin interval in FFT by introducing a parameter $\alpha$. We elucidate the underlying principles of the proposed method and discuss its potential applications across various contexts. Our findings suggest that the proposed method offers a promising approach to overcome the limitations of traditional FFT methods and enhance spectral analysis accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16665v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>eess.SP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haichao Xu</dc:creator>
    </item>
    <item>
      <title>Minimum-cost paths for electric cars</title>
      <link>https://arxiv.org/abs/2403.16936</link>
      <description>arXiv:2403.16936v1 Announce Type: new 
Abstract: An electric car equipped with a battery of a finite capacity travels on a road network with an infrastructure of charging stations. Each charging station has a possibly different cost per unit of energy. Traversing a given road segment requires a specified amount of energy that may be positive, zero or negative. The car can only traverse a road segment if it has enough charge to do so (the charge cannot drop below zero), and it cannot charge its battery beyond its capacity.
  To travel from one point to another the car needs to choose a \emph{travel plan} consisting of a path in the network and a recharging schedule that specifies how much energy to charge at each charging station on the path, making sure of having enough energy to reach the next charging station or the destination. The cost of the plan is the total charging cost along the chosen path. We reduce the problem of computing plans between every two junctions of the network to two problems: Finding optimal energetic paths when no charging is allowed and finding standard shortest paths. When there are no negative cycles in the network, we obtain an $O(n^3)$-time algorithm for computing all-pairs travel plans, where~$n$ is the number of junctions in the network. We obtain slightly faster algorithms under some further assumptions. We also consider the case in which a bound is placed on the number of rechargings allowed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16936v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dani Dorfman, Haim Kaplan, Robert E. Tarjan, Mikkel Thorup, Uri Zwick</dc:creator>
    </item>
    <item>
      <title>SAT Encoding of Partial Ordering Models for Graph Coloring Problems</title>
      <link>https://arxiv.org/abs/2403.15961</link>
      <description>arXiv:2403.15961v1 Announce Type: cross 
Abstract: In this paper, we suggest new SAT encodings of the partial-ordering based ILP model for the graph coloring problem (GCP) and the bandwidth coloring problem (BCP). The GCP asks for the minimum number of colors that can be assigned to the vertices of a given graph such that each two adjacent vertices get different colors. The BCP is a generalization, where each edge has a weight that enforces a minimal "distance" between the assigned colors, and the goal is to minimize the "largest" color used. For the widely studied GCP, we experimentally compare our new SAT encoding to the state-of-the-art approaches on the DIMACS benchmark set. Our evaluation confirms that this SAT encoding is effective for sparse graphs and even outperforms the state-of-the-art on some DIMACS instances. For the BCP, our theoretical analysis shows that the partial-ordering based SAT and ILP formulations have an asymptotically smaller size than that of the classical assignment-based model. Our practical evaluation confirms not only a dominance compared to the assignment-based encodings but also to the state-of-the-art approaches on a set of benchmark instances. Up to our knowledge, we have solved several open instances of the BCP from the literature for the first time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15961v1</guid>
      <category>cs.AI</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Faber, Adalat Jabrayilov, Petra Mutzel</dc:creator>
    </item>
    <item>
      <title>Maximum Polygon Packing: The CG:SHOP Challenge 2024</title>
      <link>https://arxiv.org/abs/2403.16203</link>
      <description>arXiv:2403.16203v1 Announce Type: cross 
Abstract: We give an overview of the 2024 Computational Geometry Challenge targeting the problem \textsc{Maximum Polygon Packing}: Given a convex region $P$ in the plane, and a collection of simple polygons $Q_1, \ldots, Q_n$, each $Q_i$ with a respective value $c_i$, find a subset $S \subseteq \{1, \ldots,n\}$ and a feasible packing within $P$ of the polygons $Q_i$ (without rotation) for $i \in S$, maximizing $\sum_{i \in S} c_i$. Geometric packing problems, such as this, present significant computational challenges and are of substantial practical importance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16203v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S\'andor P. Fekete, Phillip Keldenich, Dominik Krupke, Stefan Schirra</dc:creator>
    </item>
    <item>
      <title>Optimization on a Finer Scale: Bounded Local Subgradient Variation Perspective</title>
      <link>https://arxiv.org/abs/2403.16317</link>
      <description>arXiv:2403.16317v1 Announce Type: cross 
Abstract: We initiate the study of nonsmooth optimization problems under bounded local subgradient variation, which postulates bounded difference between (sub)gradients in small local regions around points, in either average or maximum sense. The resulting class of objective functions encapsulates the classes of objective functions traditionally studied in optimization, which are defined based on either Lipschitz continuity of the objective or H\"{o}lder/Lipschitz continuity of its gradient. Further, the defined class contains functions that are neither Lipschitz continuous nor have a H\"{o}lder continuous gradient. When restricted to the traditional classes of optimization problems, the parameters defining the studied classes lead to more fine-grained complexity bounds, recovering traditional oracle complexity bounds in the worst case but generally leading to lower oracle complexity for functions that are not ``worst case.'' Some highlights of our results are that: (i) it is possible to obtain complexity results for both convex and nonconvex problems with the (local or global) Lipschitz constant being replaced by a constant of local subgradient variation and (ii) mean width of the subdifferential set around the optima plays a role in the complexity of nonsmooth optimization, particularly in parallel settings. A consequence of (ii) is that for any error parameter $\epsilon &gt; 0$, parallel oracle complexity of nonsmooth Lipschitz convex optimization is lower than its sequential oracle complexity by a factor $\tilde{\Omega}\big(\frac{1}{\epsilon}\big)$ whenever the objective function is piecewise linear with polynomially many pieces in the input size. This is particularly surprising as existing parallel complexity lower bounds are based on such classes of functions. The seeming contradiction is resolved by considering the region in which the algorithm is allowed to query the objective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16317v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jelena Diakonikolas, Crist\'obal Guzm\'an</dc:creator>
    </item>
    <item>
      <title>Algorithms and data structures for numerical computations with automatic precision estimation</title>
      <link>https://arxiv.org/abs/2403.16660</link>
      <description>arXiv:2403.16660v1 Announce Type: cross 
Abstract: We introduce data structures and algorithms to count numerical inaccuracies arising from usage of floating numbers described in IEEE 754. Here we describe how to estimate precision for some collection of functions most commonly used for array manipulations and training of neural networks. For highly optimized functions like matrix multiplication, we provide a fast estimation of precision and some hint how the estimation can be strengthened.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16660v1</guid>
      <category>math.NA</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Igor V. Netay</dc:creator>
    </item>
    <item>
      <title>High-Temperature Gibbs States are Unentangled and Efficiently Preparable</title>
      <link>https://arxiv.org/abs/2403.16850</link>
      <description>arXiv:2403.16850v1 Announce Type: cross 
Abstract: We show that thermal states of local Hamiltonians are separable above a constant temperature. Specifically, for a local Hamiltonian $H$ on a graph with degree $\mathfrak{d}$, its Gibbs state at inverse temperature $\beta$, denoted by $\rho =e^{-\beta H}/ \textrm{tr}(e^{-\beta H})$, is a classical distribution over product states for all $\beta &lt; 1/(c\mathfrak{d})$, where $c$ is a constant. This sudden death of thermal entanglement upends conventional wisdom about the presence of short-range quantum correlations in Gibbs states.
  Moreover, we show that we can efficiently sample from the distribution over product states. In particular, for any $\beta &lt; 1/( c \mathfrak{d}^3)$, we can prepare a state $\epsilon$-close to $\rho$ in trace distance with a depth-one quantum circuit and $\textrm{poly}(n) \log(1/\epsilon)$ classical overhead. A priori the task of preparing a Gibbs state is a natural candidate for achieving super-polynomial quantum speedups, but our results rule out this possibility above a fixed constant temperature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16850v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ainesh Bakshi, Allen Liu, Ankur Moitra, Ewin Tang</dc:creator>
    </item>
    <item>
      <title>Vehicle Routing with Time-Dependent Travel Times: Theory, Practice, and Benchmarks</title>
      <link>https://arxiv.org/abs/2205.00889</link>
      <description>arXiv:2205.00889v2 Announce Type: replace 
Abstract: We develop theoretical foundations and practical algorithms for vehicle routing with time-dependent travel times. We also provide new benchmark instances and experimental results. First, we study basic operations on piecewise linear arrival time functions. In particular, we devise a faster algorithm to compute the pointwise minimum of a set of piecewise linear functions and a monotonicity-preserving variant of the Imai-Iri algorithm to approximate an arrival time function with fewer breakpoints.
  Next, we show how to evaluate insertion and deletion operations in tours efficiently and update the underlying data structure faster than previously known when a tour changes. Evaluating a tour also requires a scheduling step which is non-trivial in the presence of time windows and time-dependent travel times. We show how to perform this in linear time.
  Based on these results, we develop a local search heuristic to solve real-world vehicle routing problems with various constraints efficiently and report experimental results on classical benchmarks. Since most of these do not have time-dependent travel times, we generate and publish new benchmark instances that are based on real-world data. This data also demonstrates the importance of considering time-dependent travel times in instances with tight time windows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.00889v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jannis Blauth, Stephan Held, Dirk M\"uller, Niklas Schlomberg, Vera Traub, Thorben Tr\"obst, Jens Vygen</dc:creator>
    </item>
    <item>
      <title>The landscape of compressibility measures for two-dimensional data</title>
      <link>https://arxiv.org/abs/2307.02629</link>
      <description>arXiv:2307.02629v2 Announce Type: replace 
Abstract: In this paper we extend to two-dimensional data two recently introduced one-dimensional compressibility measures: the $\gamma$ measure defined in terms of the smallest string attractor, and the $\delta$ measure defined in terms of the number of distinct substrings of the input string. Concretely, we introduce the two-dimensional measures $\gamma_{2D}$ and $\delta_{2D}$ as natural generalizations of $\gamma$ and $\delta$ and study some of their properties. Among other things, we prove that $\delta_{2D}$ is monotone and can be computed in linear time, and we show that, although it is still true that $\delta_{2D} \leq \gamma_{2D}$, the gap between the two measures can be $\Omega(\sqrt{n})$ for families of $n\times n$ matrices and therefore asymptotically larger than the gap in one-dimension. To complete the scenario of two-dimensional compressibility measures, we introduce also the measure $b_{2D}$ which generalizes to two dimensions the notion of optimal parsing. We prove that, somewhat surprisingly, the relationship between $b_{2D}$ and $\gamma_{2D}$ is significantly different than in the one-dimensional case. As an application of the measures $\gamma_{2D}$ and $\delta_{2D}$ we provide the first analysis of the space usage of the two-dimensional block tree introduced in [Brisaboa et al., Two-dimensional block trees, The computer Journal, 2023]. Finally, we present a linear time algorithm for constructing the two-dimensional block tree for arbitrary matrices, that is asymptotically faster than the (probabilistic) known solution which can only be used for binary matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.02629v2</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lorenzo Carfagna, Giovanni Manzini</dc:creator>
    </item>
    <item>
      <title>Multi-Agent Online Graph Exploration on Cycles and Tadpole Graphs</title>
      <link>https://arxiv.org/abs/2402.13845</link>
      <description>arXiv:2402.13845v2 Announce Type: replace 
Abstract: We study the problem of multi-agent online graph exploration, in which a team of k agents has to explore a given graph, starting and ending on the same node. The graph is initially unknown. Whenever a node is visited by an agent, its neighborhood and adjacent edges are revealed. The agents share a global view of the explored parts of the graph. The cost of the exploration has to be minimized, where cost either describes the time needed for the entire exploration (time model), or the length of the longest path traversed by any agent (energy model). We investigate graph exploration on cycles and tadpole graphs for 2-4 agents, providing optimal results on the competitive ratio in the energy model (1-competitive with two agents on cycles and three agents on tadpole graphs), and for tadpole graphs in the time model (1.5-competitive with four agents). We also show competitive upper bounds of 2 for the exploration of tadpole graphs with three agents, and 2.5 for the exploration of tadpole graphs with two agents in the time model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13845v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erik van den Akker, Kevin Buchin, Klaus-Tycho Foerster</dc:creator>
    </item>
    <item>
      <title>Flip-width: Cops and Robber on dense graphs</title>
      <link>https://arxiv.org/abs/2302.00352</link>
      <description>arXiv:2302.00352v3 Announce Type: replace-cross 
Abstract: We define new graph parameters, called flip-width, that generalize treewidth, degeneracy, and generalized coloring numbers for sparse graphs, and clique-width and twin-width for dense graphs. The flip-width parameters are defined using variants of the Cops and Robber game, in which the robber has speed bounded by a fixed constant $r\in\mathbb N\cup\{\infty\}$, and the cops perform flips (or perturbations) of the considered graph. We then propose a new notion of tameness of a graph class, called bounded flip-width, which is a dense counterpart of classes of bounded expansion of Ne\v{s}etril and Ossona de Mendez, and includes classes of bounded twin-width of Bonnet, Kim, Thomass{\'e}, and Watrigant. This unifies Sparsity Theory and Twin-width Theory, providing a common language for studying the central notions of the two theories, such as weak coloring numbers and twin-width -- corresponding to winning strategies of one player -- or dense shallow minors, rich divisions, or well-linked sets, corresponding to winning strategies of the other player. We prove that boundedness of flip-width is preserved by first-order interpretations, or transductions, generalizing previous results concerning classes of bounded expansion and bounded twin-width. We provide an algorithm approximating the flip-width of a given graph, which runs in slicewise polynomial time (XP) in the size of the graph. Finally, we propose a more general notion of tameness, called almost bounded flip-width, which is a dense counterpart of nowhere dense classes. We conjecture, and provide evidence, that classes with almost bounded flip-width coincide with monadically dependent (or monadically NIP) classes, introduced by Shelah in model theory. We also provide evidence that classes of almost bounded flip-width characterise the hereditary graph classes for which the model-checking problem is fixed-parameter tractable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.00352v3</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Szymon Toru\'nczyk</dc:creator>
    </item>
    <item>
      <title>Implementing any Linear Combination of Unitaries on Intermediate-term Quantum Computers</title>
      <link>https://arxiv.org/abs/2302.13555</link>
      <description>arXiv:2302.13555v3 Announce Type: replace-cross 
Abstract: We develop three new methods to implement any Linear Combination of Unitaries (LCU), a powerful quantum algorithmic tool with diverse applications. While the standard LCU procedure requires several ancilla qubits and sophisticated multi-qubit controlled operations, our methods consume significantly fewer quantum resources. The first method (Single-Ancilla LCU) estimates expectation values of observables with respect to any quantum state prepared by an LCU procedure while requiring only a single ancilla qubit, and no multi-qubit controlled operations. The second approach (Analog LCU) is a simple, physically motivated, continuous-time analogue of LCU, tailored to hybrid qubit-qumode systems. The third method (Ancilla-free LCU) requires no ancilla qubit at all and is useful when we are interested in the projection of a quantum state (prepared by the LCU procedure) in some subspace of interest. We apply the first two techniques to develop new quantum algorithms for a wide range of practical problems, ranging from Hamiltonian simulation, ground state preparation and property estimation, and quantum linear systems. Remarkably, despite consuming fewer quantum resources they retain a provable quantum advantage. The third technique allows us to connect discrete and continuous-time quantum walks with their classical counterparts. It also unifies the recently developed optimal quantum spatial search algorithms in both these frameworks, and leads to the development of new ones that require fewer ancilla qubits. Overall, our results are quite generic and can be readily applied to other problems, even beyond those considered here.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.13555v3</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shantanav Chakraborty</dc:creator>
    </item>
    <item>
      <title>Gradient-less Federated Gradient Boosting Trees with Learnable Learning Rates</title>
      <link>https://arxiv.org/abs/2304.07537</link>
      <description>arXiv:2304.07537v3 Announce Type: replace-cross 
Abstract: The privacy-sensitive nature of decentralized datasets and the robustness of eXtreme Gradient Boosting (XGBoost) on tabular data raise the needs to train XGBoost in the context of federated learning (FL). Existing works on federated XGBoost in the horizontal setting rely on the sharing of gradients, which induce per-node level communication frequency and serious privacy concerns. To alleviate these problems, we develop an innovative framework for horizontal federated XGBoost which does not depend on the sharing of gradients and simultaneously boosts privacy and communication efficiency by making the learning rates of the aggregated tree ensembles learnable. We conduct extensive evaluations on various classification and regression datasets, showing our approach achieves performance comparable to the state-of-the-art method and effectively improves communication efficiency by lowering both communication rounds and communication overhead by factors ranging from 25x to 700x. Project Page: https://flower.ai/blog/2023-04-19-xgboost-with-flower/</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.07537v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3578356.3592579</arxiv:DOI>
      <dc:creator>Chenyang Ma, Xinchi Qiu, Daniel J. Beutel, Nicholas D. Lane</dc:creator>
    </item>
    <item>
      <title>Spectral clustering in the Gaussian mixture block model</title>
      <link>https://arxiv.org/abs/2305.00979</link>
      <description>arXiv:2305.00979v2 Announce Type: replace-cross 
Abstract: Gaussian mixture block models are distributions over graphs that strive to model modern networks: to generate a graph from such a model, we associate each vertex $i$ with a latent feature vector $u_i \in \mathbb{R}^d$ sampled from a mixture of Gaussians, and we add edge $(i,j)$ if and only if the feature vectors are sufficiently similar, in that $\langle u_i,u_j \rangle \ge \tau$ for a pre-specified threshold $\tau$. The different components of the Gaussian mixture represent the fact that there may be different types of nodes with different distributions over features -- for example, in a social network each component represents the different attributes of a distinct community. Natural algorithmic tasks associated with these networks are embedding (recovering the latent feature vectors) and clustering (grouping nodes by their mixture component).
  In this paper we initiate the study of clustering and embedding graphs sampled from high-dimensional Gaussian mixture block models, where the dimension of the latent feature vectors $d\to \infty$ as the size of the network $n \to \infty$. This high-dimensional setting is most appropriate in the context of modern networks, in which we think of the latent feature space as being high-dimensional. We analyze the performance of canonical spectral clustering and embedding algorithms for such graphs in the case of 2-component spherical Gaussian mixtures, and begin to sketch out the information-computation landscape for clustering and embedding in these models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.00979v2</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.SI</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuangping Li, Tselil Schramm</dc:creator>
    </item>
    <item>
      <title>Best of Both Worlds Guarantees for Smoothed Online Quadratic Optimization</title>
      <link>https://arxiv.org/abs/2311.00181</link>
      <description>arXiv:2311.00181v2 Announce Type: replace-cross 
Abstract: We study the smoothed online quadratic optimization (SOQO) problem where, at each round $t$, a player plays an action $x_t$ in response to a quadratic hitting cost and an additional squared $\ell_2$-norm cost for switching actions. This problem class has strong connections to a wide range of application domains including smart grid management, adaptive control, and data center management, where switching-efficient algorithms are highly sought after. We study the SOQO problem in both adversarial and stochastic settings, and in this process, perform the first stochastic analysis of this class of problems. We provide the online optimal algorithm when the minimizers of the hitting cost function evolve as a general stochastic process, which, for the case of martingale process, takes the form of a distribution-agnostic dynamic interpolation algorithm (LAI). Next, we present the stochastic-adversarial trade-off by proving an $\Omega(T)$ expected regret for the adversarial optimal algorithm in the literature (ROBD) with respect to LAI and, a sub-optimal competitive ratio for LAI in the adversarial setting. Finally, we present a best-of-both-worlds algorithm that obtains a robust adversarial performance while simultaneously achieving a near-optimal stochastic performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00181v2</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neelkamal Bhuyan, Debankur Mukherjee, Adam Wierman</dc:creator>
    </item>
    <item>
      <title>Enhancing Grover's Search Algorithm: A Modified Approach to Increase the Probability of Good States</title>
      <link>https://arxiv.org/abs/2402.00082</link>
      <description>arXiv:2402.00082v4 Announce Type: replace-cross 
Abstract: This article introduces an enhancement to the Grover search algorithm to speed up computing the probability of finding good states. It suggests incorporating a rotation phase angle determined mathematically from the derivative of the model during the initial iteration. At each iteration, a new phase angle is computed and used in a rotation gate around y+z axis in the diffusion operator. The computed phase angles are optimized through an adaptive adjustment based on the estimated increasing ratio of the consecutive amplitudes. The findings indicate an average decrease of 28% in the required number of iterations resulting in a faster overall process and fewer number of quantum gates. For large search space, this improvement rises to 29.58%. Given the computational capabilities of the computer utilized for the simulation, the approach is applied to instances with up to 12 qubits or 4096 possible combination of search entries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00082v4</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ismael Abdulrahman</dc:creator>
    </item>
  </channel>
</rss>
