<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Jun 2024 04:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>How to Rent GPUs on a Budget</title>
      <link>https://arxiv.org/abs/2406.15560</link>
      <description>arXiv:2406.15560v1 Announce Type: new 
Abstract: The explosion in Machine Learning (ML) over the past ten years has led to a dramatic increase in demand for GPUs to train ML models. Because it is prohibitively expensive for most users to build and maintain a large GPU cluster, large cloud providers (Microsoft Azure, Amazon AWS, Google Cloud) have seen explosive growth in demand for renting cloud-based GPUs. In this cloud-computing paradigm, a user must specify their demand for GPUs at every moment in time, and will pay for every GPU-hour they use. ML training jobs are known to be parallelizable to different degrees. Given a stream of ML training jobs, a user typically wants to minimize the mean response time across all jobs. Here, the response time of a job denotes the time from when a job arrives until it is complete. Additionally, the user is constrained by some operating budget. Specifically, in this paper the user is constrained to use no more than $b$ GPUs per hour, over a long-run time average. The question is how to minimize mean response time while meeting the budget constraint. Because training jobs receive a diminishing marginal benefit from running on additional GPUs, allocating too many GPUs to a single training job can dramatically increase the overall cost paid by the user. Hence, an optimal rental policy must balance a tradeoff between training cost and mean response time. This paper derives the optimal rental policy for a stream of training jobs where the jobs have different levels of parallelizability (specified by a speedup function) and different job sizes (amounts of inherent work). We make almost no assumptions about the arrival process and about the job size distribution. Our optimal policy specifies how many GPUs to rent at every moment in time and how to allocate these GPUs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15560v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhouzi Li, Benjamin Berg, Arpan Mukhopadhyay, Mor Harchol-Balter</dc:creator>
    </item>
    <item>
      <title>Humas: A Heterogeneity- and Upgrade-aware Microservice Auto-scaling Framework in Large-scale Data Centers</title>
      <link>https://arxiv.org/abs/2406.15769</link>
      <description>arXiv:2406.15769v1 Announce Type: new 
Abstract: An effective auto-scaling framework is essential for microservices to ensure performance stability and resource efficiency under dynamic workloads. As revealed by many prior studies, the key to efficient auto-scaling lies in accurately learning performance patterns, i.e., the relationship between performance metrics and workloads in data-driven schemes. However, we notice that there are two significant challenges in characterizing performance patterns for large-scale microservices. Firstly, diverse microservices demonstrate varying sensitivities to heterogeneous machines, causing difficulty in quantifying the performance difference in a fixed manner. Secondly, frequent version upgrades of microservices result in uncertain changes in performance patterns, known as pattern drifts, leading to imprecise resource capacity estimation issues. To address these challenges, we propose Humas, a heterogeneity- and upgrade-aware auto-scaling framework for large-scale microservices. Firstly, Humas quantifies the difference in resource efficiency among heterogeneous machines for various microservices online and normalizes their resources in standard units. Additionally, Humas develops a least squares density-difference (LSDD) based algorithm to identify pattern drifts caused by upgrades. Lastly, Humas generates capacity adjustment plans for microservices based on the latest performance patterns and predicted workloads. The experiment results conducted on 50 real microservices with over 11,000 containers demonstrate that Humas improves resource efficiency and performance stability by approximately 30.4% and 48.0%, respectively, compared to state-of-the-art approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15769v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qin Hua, Dingyu Yang, Shiyou Qian, Jian Cao, Guangtao Xue, Minglu Li</dc:creator>
    </item>
    <item>
      <title>Split Federated Learning Empowered Vehicular Edge Intelligence: Adaptive Parellel Design and Future Directions</title>
      <link>https://arxiv.org/abs/2406.15804</link>
      <description>arXiv:2406.15804v1 Announce Type: new 
Abstract: To realize ubiquitous intelligence of future vehicular networks, artificial intelligence (AI) is critical since it can mine knowledge from vehicular data to improve the quality of many AI driven vehicular services. By combining AI techniques with vehicular networks, Vehicular Edge Intelligence (VEI) can utilize the computing, storage, and communication resources of vehicles to train the AI models. Nevertheless, when executing the model training, the traditional centralized learning paradigm requires vehicles to upload their raw data to a central server, which results in significant communication overheads and the risk of privacy leakage. In this article, we first overview the system architectures, performance metrics and challenges ahead of VEI design. Then we propose to utilize distribute machine learning scheme, namely split federated learning (SFL), to boost the development of VEI. We present a novel adaptive and parellel SFL scheme and conduct corresponding analysis on its performance. Future research directions are highlighted to shed light on the efficient design of SFL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15804v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xianke Qiang, Zheng Chang, Chaoxiong Ye, Timo Hamalainen</dc:creator>
    </item>
    <item>
      <title>A Tale of Two Paths: Toward a Hybrid Data Plane for Efficient Far-Memory Applications</title>
      <link>https://arxiv.org/abs/2406.16005</link>
      <description>arXiv:2406.16005v1 Announce Type: new 
Abstract: With rapid advances in network hardware, far memory has gained a great deal of traction due to its ability to break the memory capacity wall. Existing far memory systems fall into one of two data paths: one that uses the kernel's paging system to transparently access far memory at the page granularity, and a second that bypasses the kernel, fetching data at the object granularity. While it is generally believed that object fetching outperforms paging due to its fine-grained access, it requires significantly more compute resources to run object-level LRU and eviction.
  We built Atlas, a hybrid data plane enabled by a runtime-kernel co-design that simultaneously enables accesses via these two data paths to provide high efficiency for real-world applications. Atlas uses always-on profiling to continuously measure page locality. For workloads already with good locality, paging is used to fetch data, whereas for those without, object fetching is employed. Object fetching moves objects that are accessed close in time to contiguous local space, dynamically improving locality and making the execution increasingly amenable to paging, which is much more resource-efficient. Our evaluation shows that Atlas improves the throughput (e.g., by 1.5x and 3.2x) and reduces the tail latency (e.g., by one and two orders of magnitude) when using remote memory, compared with AIFM and Fastswap, the state-of-the-art techniques respectively in the two categories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16005v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lei Chen, Shi Liu, Chenxi Wang, Haoran Ma, Yifan Qiao, Zhe Wang, Chenggang Wu, Youyou Lu, Xiaobing Feng, Huimin Cui, Shan Lu, Harry Xu</dc:creator>
    </item>
    <item>
      <title>Towards Real-Time Neural Volumetric Rendering on Mobile Devices: A Measurement Study</title>
      <link>https://arxiv.org/abs/2406.16068</link>
      <description>arXiv:2406.16068v1 Announce Type: new 
Abstract: Neural Radiance Fields (NeRF) is an emerging technique to synthesize 3D objects from 2D images with a wide range of potential applications. However, rendering existing NeRF models is extremely computation intensive, making it challenging to support real-time interaction on mobile devices. In this paper, we take the first initiative to examine the state-of-the-art real-time NeRF rendering technique from a system perspective. We first define the entire working pipeline of the NeRF serving system. We then identify possible control knobs that are critical to the system from the communication, computation, and visual performance perspective. Furthermore, an extensive measurement study is conducted to reveal the effects of these control knobs on system performance. Our measurement results reveal that different control knobs contribute differently towards improving the system performance, with the mesh granularity being the most effective knob and the quantization being the least effective knob. In addition, diverse hardware device settings and network conditions have to be considered to fully unleash the benefit of operating under the appropriate knobs</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16068v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <category>cs.MM</category>
      <category>cs.PF</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhe Wang, Yifei Zhu</dc:creator>
    </item>
    <item>
      <title>Efficient GPU Implementation of Particle Interactions with Cutoff Radius and Few Particles per Cell</title>
      <link>https://arxiv.org/abs/2406.16091</link>
      <description>arXiv:2406.16091v1 Announce Type: new 
Abstract: This paper presents novel approaches to parallelizing particle interactions on a GPU when there are few particles per cell and the interactions are limited by a cutoff distance. The paper surveys classical algorithms and then introduces two alternatives that aim to utilize shared memory. The first approach copies the particles of a sub-box, while the second approach loads particles in a pencil along the X-axis. The different implementations are compared on three GPU models using Cuda and Hip. The results show that the X-pencil approach can provide a significant speedup but only in very specific cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16091v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Algis, Berenger Bramas, Emmanuelle Darles, Lilian Aveneau</dc:creator>
    </item>
    <item>
      <title>Privacy-Preserving and Trustworthy Localization in an IoT Environment</title>
      <link>https://arxiv.org/abs/2406.16182</link>
      <description>arXiv:2406.16182v1 Announce Type: new 
Abstract: The Internet of Things (IoT) is increasingly prevalent in various applications, such as healthcare and logistics. One significant service of IoT technologies that is essential for these applications is localization. The goal of this service is to determine the precise position of a specific target. The localization data often needs to be private, accessible only to specific entities, and must maintain authenticity and integrity to ensure trustworthiness. IoT technology has evolved significantly, with Ultra-Wide Band (UWB) technology enhancing localization speed and precision. However, IoT device security remains a concern, as devices can be compromised or act maliciously. Furthermore, localization data is typically stored centrally, which can also be a point of vulnerability. Our approach leverages the features of a permissioned blockchain, specifically Hyperledger Fabric, to address these challenges. Hyperledger Fabric's collection feature ensures data privacy, and its smart contracts (chaincode) enhance trustworthiness. We tested our solution using a network of devices known as CLOVES, demonstrating robust performance characteristics with UWB technology. Additionally, we evaluated our approach through an indoor localization use case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16182v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Guglielmo Zocca, Omar Hasan</dc:creator>
    </item>
    <item>
      <title>Evaluating Serverless Machine Learning Performance on Google Cloud Run</title>
      <link>https://arxiv.org/abs/2406.16250</link>
      <description>arXiv:2406.16250v1 Announce Type: new 
Abstract: End-users can get functions-as-a-service from serverless platforms, which promise lower hosting costs, high availability, fault tolerance, and dynamic flexibility for hosting individual functions known as microservices. Machine learning tools are seen to be reliably useful, and the services created using these tools are in increasing demand on a large scale. The serverless platforms are uniquely suited for hosting these machine learning services to be used for large-scale applications. These platforms are well known for their cost efficiency, fault tolerance, resource scaling, robust APIs for communication, and global reach. However, machine learning services are different from the web-services in that these serverless platforms were originally designed to host web services. We aimed to understand how these serverless platforms handle machine learning workloads with our study. We examine machine learning performance on one of the serverless platforms - Google Cloud Run, which is a GPU-less infrastructure that is not designed for machine learning application deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16250v1</guid>
      <category>cs.DC</category>
      <category>cs.OS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prerana Khatiwada, Pranjal Dhakal</dc:creator>
    </item>
    <item>
      <title>Placing Timely Refreshing Services at the Network Edge</title>
      <link>https://arxiv.org/abs/2406.16280</link>
      <description>arXiv:2406.16280v1 Announce Type: new 
Abstract: Accommodating services at the network edge is favorable for time-sensitive applications. However, maintaining service usability is resource-consuming in terms of pulling service images to the edge, synchronizing databases of service containers, and hot updates of service modules. Accordingly, it is critical to determine which service to place based on the received user requests and service refreshing (maintaining) cost, which is usually neglected in existing studies. In this work, we study how to cooperatively place timely refreshing services and offload user requests among edge servers to minimize the backhaul transmission costs. We formulate an integer non-linear programming problem and prove its NP-hardness. This problem is highly non-tractable due to the complex spatial-and-temporal coupling effect among service placement, offloading, and refreshing costs. We first decouple the problem in the temporal domain by transforming it into a Markov shortest-path problem. We then propose a light-weighted Discounted Value Approximation (DVA) method, which further decouples the problem in the spatial domain by estimating the offloading costs among edge servers. The worst performance of DVA is proved to be bounded. 5G service placement testbed experiments and real-trace simulations show that DVA reduces the total transmission cost by up to 59.1% compared with the state-of-the-art baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16280v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/JIOT.2023.3268308</arxiv:DOI>
      <dc:creator>Xishuo Li, Shan Zhang, Hongbin Luo, Xiao Ma, Junyi He</dc:creator>
    </item>
    <item>
      <title>Towards Communication-Efficient Peer-to-Peer Networks</title>
      <link>https://arxiv.org/abs/2406.16661</link>
      <description>arXiv:2406.16661v1 Announce Type: new 
Abstract: We focus on designing Peer-to-Peer (P2P) networks that enable efficient communication. Over the last two decades, there has been substantial algorithmic research on distributed protocols for building P2P networks with various desirable properties such as high expansion, low diameter, and robustness to a large number of deletions. A key underlying theme in all of these works is to distributively build a \emph{random graph} topology that guarantees the above properties. Moreover, the random connectivity topology is widely deployed in many P2P systems today, including those that implement blockchains and cryptocurrencies. However, a major drawback of using a random graph topology for a P2P network is that the random topology does not respect the \emph{underlying} (Internet) communication topology. This creates a large \emph{propagation delay}, which is a major communication bottleneck in modern P2P networks.
  In this paper, we work towards designing P2P networks that are communication-efficient (having small propagation delay) with provable guarantees. Our main contribution is an efficient, decentralized protocol, $\textsc{Close-Weaver}$, that transforms a random graph topology embedded in an underlying Euclidean space into a topology that also respects the underlying metric. We then present efficient point-to-point routing and broadcast protocols that achieve essentially optimal performance with respect to the underlying space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16661v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khalid Hourani, William K. Moses Jr., Gopal Pandurangan</dc:creator>
    </item>
    <item>
      <title>A Flexible Cryptographic Infrastructure for High-security SDR-based Systems</title>
      <link>https://arxiv.org/abs/2406.15489</link>
      <description>arXiv:2406.15489v1 Announce Type: cross 
Abstract: Military software defined radio (SDR) systems are a major factor in future network-centric operations due to their flexibility and support for more capable radio communications systems. The inherent nature of software-based systems requires a more complex auxiliary infrastructure and multiple independent levels of security compared with typical systems: Secure booting of the SDR device, cryptographically signed software, real time operating platform software as well as radio applications. This technology raises new challenges with respect to the management. The largest impact on SDR deployments is due to the auxiliary cryptographic infrastructure for the security of the software life cycle and the cyclic update of the keys. Compared to conventional radio devices, the SDR system with the cryptographic infrastructure described in this paper reaches a higher security level and is more flexible. The advantage is the possibility to deploy trunked radio system and further waveforms, such as coalition wideband, which will be standardized in the future. Also it is possible to update cryptographic mechanisms. In this work, we analyze the requirements for a high secure SDR deployment and model the life cycle of the components of a deployed SDR node based on the Joint Program Executive Office (JPEO) Software Communication Architecture (SCA).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15489v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/MilCIS.2013.6694489</arxiv:DOI>
      <arxiv:journal_reference>Military Communications and Information Systems Conference (MilCIS 2013)</arxiv:journal_reference>
      <dc:creator>Peter Hillmann, Bj\"orn Stelte</dc:creator>
    </item>
    <item>
      <title>EDGE-LLM: Enabling Efficient Large Language Model Adaptation on Edge Devices via Layerwise Unified Compression and Adaptive Layer Tuning and Voting</title>
      <link>https://arxiv.org/abs/2406.15758</link>
      <description>arXiv:2406.15758v1 Announce Type: cross 
Abstract: Efficient adaption of large language models (LLMs) on edge devices is essential for applications requiring continuous and privacy-preserving adaptation and inference. However, existing tuning techniques fall short because of the high computation and memory overheads. To this end, we introduce a computation- and memory-efficient LLM tuning framework, called Edge-LLM, to facilitate affordable and effective LLM adaptation on edge devices. Specifically, Edge-LLM features three core components: (1) a layer-wise unified compression (LUC) technique to reduce the computation overhead by generating layer-wise pruning sparsity and quantization bit-width policies, (2) an adaptive layer tuning and voting scheme to reduce the memory overhead by reducing the backpropagation depth, and (3) a complementary hardware scheduling strategy to handle the irregular computation patterns introduced by LUC and adaptive layer tuning, thereby achieving efficient computation and data movements. Extensive experiments demonstrate that Edge-LLM achieves a 2.92x speed up and a 4x memory overhead reduction as compared to vanilla tuning methods with comparable task accuracy. Our code is available at https://github.com/GATECH-EIC/Edge-LLM</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15758v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zhongzhi Yu, Zheng Wang, Yuhan Li, Haoran You, Ruijie Gao, Xiaoya Zhou, Sreenidhi Reedy Bommu, Yang Katie Zhao, Yingyan Celine Lin</dc:creator>
    </item>
    <item>
      <title>Wireless MapReduce Arrays for Coded Distributed Computing</title>
      <link>https://arxiv.org/abs/2406.15791</link>
      <description>arXiv:2406.15791v1 Announce Type: cross 
Abstract: We consider a wireless distributed computing system based on the MapReduce framework, which consists of three phases: \textit{Map}, \textit{Shuffle}, and \textit{Reduce}. The system consists of a set of distributed nodes assigned to compute arbitrary output functions depending on a file library. The computation of the output functions is decomposed into Map and Reduce functions, and the Shuffle phase, which involves the data exchange, links the two. In our model, the Shuffle phase communication happens over a full-duplex wireless interference channel. For this setting, a coded wireless MapReduce distributed computing scheme exists in the literature, achieving optimal performance under one-shot linear schemes. However, the scheme requires the number of input files to be very large, growing exponentially with the number of nodes. We present schemes that require the number of files to be in the order of the number of nodes and achieve the same performance as the existing scheme. The schemes are obtained by designing a structure called wireless MapReduce array that succinctly represents all three phases in a single array. The wireless MapReduce arrays can also be obtained from the extended placement delivery arrays known for multi-antenna coded caching schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15791v1</guid>
      <category>cs.IT</category>
      <category>cs.DC</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elizabath Peter, K. K. Krishnan Namboodiri, B. Sundar Rajan</dc:creator>
    </item>
    <item>
      <title>Bisimulation for Impure Simplicial Complexes</title>
      <link>https://arxiv.org/abs/2406.16785</link>
      <description>arXiv:2406.16785v1 Announce Type: cross 
Abstract: As an alternative to Kripke models, simplicial complexes are a versatile semantic primitive on which to interpret epistemic logic. Given a set of vertices, a simplicial complex is a downward closed set of subsets, called simplexes, of the vertex set. A maximal simplex is called a facet. Impure simplicial complexes represent that some agents (processes) are dead. It is known that impure simplicial complexes categorically correspond to so-called partial epistemic (Kripke) models. In this contribution, we define a notion of bisimulation to compare impure simplicial complexes and show that it has the Hennessy-Milner property. These results are for a logical language including atoms that express whether agents are alive or dead. Without these atoms no reasonable standard notion of bisimulation exists, as we amply justify by counterexamples, because such a restricted language is insufficiently expressive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16785v1</guid>
      <category>cs.LO</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marta B\'ilkov\'a, Hans van Ditmarsch, Roman Kuznets, Rojo Randrianomentsoa</dc:creator>
    </item>
    <item>
      <title>A Multi-Party, Multi-Blockchain Atomic Swap Protocol with Universal Adaptor Secret</title>
      <link>https://arxiv.org/abs/2406.16822</link>
      <description>arXiv:2406.16822v1 Announce Type: cross 
Abstract: The increasing complexity of digital asset transactions across multiple blockchains necessitates a robust atomic swap protocol that can securely handle more than two participants. Traditional atomic swap protocols, including those based on adaptor signatures, are vulnerable to malicious dropout attacks, which break atomicity and compromise the security of the transaction. This paper presents a novel multi-party atomic swap protocol that operates almost entirely off-chain, requiring only a single on-chain transaction for finalization. Our protocol leverages Schnorr-like signature verification and a universal adaptor secret to ensure atomicity and scalability across any number of participants and blockchains without the need for smart contracts or trusted third parties. By addressing key challenges such as collusion attacks and malicious dropouts, our protocol significantly enhances the security and efficiency of multi-party atomic swaps. Our contributions include the first scalable, fully off-chain protocol for atomic swaps involving any number of participants, adding zero overhead to native blockchains, and providing a practical and cost-effective solution for decentralized asset exchanges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16822v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengewei You, Aditya Joshi, Andrey Kuehlkamp, Jarek Nabrzyski</dc:creator>
    </item>
    <item>
      <title>Holistic generational offsets: Fostering a primitive online abstraction for human vs. machine cognition</title>
      <link>https://arxiv.org/abs/1810.03955</link>
      <description>arXiv:1810.03955v3 Announce Type: replace 
Abstract: We propose a unified architecture for next generation cognitive, low cost, mobile internet. The end user platform is able to scale as per the application and network requirements. It takes computing out of the data center and into end user platform. Internet enables open standards, accessible computing and applications programmability on a commodity platform. The architecture is a super-set to present day infrastructure web computing. The Java virtual machine (JVM) derives from the stack architecture. Applications can be developed and deployed on a multitude of host platforms. O(1) &lt;-&gt; O(N). Computing and the internet today are more accessible and available to the larger community. Machine learning has made extensive advances with the availability of modern computing. It is used widely in NLP, Computer Vision, Deep learning and AI. A prototype device for mobile could contain N compute and N MB of memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:1810.03955v3</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.7302/23386</arxiv:DOI>
      <dc:creator>Shaun D'Souza, Trevor Mudge</dc:creator>
    </item>
    <item>
      <title>Morlet wavelet transform using attenuated sliding Fourier transform and kernel integral for graphic processing unit</title>
      <link>https://arxiv.org/abs/2110.11866</link>
      <description>arXiv:2110.11866v3 Announce Type: replace 
Abstract: Morlet or Gabor wavelet transforms as well as Gaussian smoothing, are widely used in signal processing and image processing. However, the computational complexity of their direct calculations is proportional not only to the number of data points in a signal but also to the smoothing size, which is the standard deviation in the Gaussian function in their transform functions. Thus, when the standard deviation is large, its considerable computation time diminishes the advantages of aforementioned transforms. Therefore, it is important to formulate an algorithm to reduce the calculation time of the transformations. In this paper, we first review calculation methods of Gaussian smoothing by using the sliding Fourier transform (SFT) and our proposed attenuated SFT (ASFT) \cite{YamashitaICPR2020}. Based on these methods, we propose two types of calculation methods for Morlet wavelet transforms. We also propose an algorithm to calculate SFT using the kernel integral on graphic processing unit (GPU). When the number of calculation cores in GPU is not less than the number of data points, the order of its calculation time is the logarithm of the smoothing size and does not depend on the number of data points. Using experiments, we compare the two methods for calculating the Morlet wavelet transform and evaluate the calculation time of the proposed algorithm using a kernel integral on GPU. For example, when the number of data points and the standard deviation are 102400 and 8192.0, respectively, the calculation time of the Morlet wavelet transform by the proposed method is 0.545 ms, which 413.6 times faster than a conventional method. (In this version, mistakes in fitures are corrected.)</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.11866v3</guid>
      <category>cs.DC</category>
      <category>eess.SP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yukihiko Yamashita, Toru Wakahara</dc:creator>
    </item>
    <item>
      <title>GVE-Louvain: Fast Louvain Algorithm for Community Detection in Shared Memory Setting</title>
      <link>https://arxiv.org/abs/2312.04876</link>
      <description>arXiv:2312.04876v5 Announce Type: replace 
Abstract: Community detection is the problem of identifying natural divisions in networks. Efficient parallel algorithms for identifying such divisions is critical in a number of applications, where the size of datasets have reached significant scales. This technical report presents one of the most efficient multicore implementations of the Louvain algorithm, a high quality community detection method. On a server equipped with dual 16-core Intel Xeon Gold 6226R processors, our Louvain, which we term as GVE-Louvain, outperforms Vite, Grappolo, NetworKit Louvain, and cuGraph Louvain (running on NVIDIA A100 GPU) by 50x, 22x, 20x, and 5.8x faster respectively - achieving a processing rate of 560M edges/s on a 3.8B edge graph. In addition, GVE-Louvain improves performance at an average rate of 1.6x for every doubling of threads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04876v5</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Subhajit Sahu</dc:creator>
    </item>
    <item>
      <title>The Economic Limits of Permissionless Consensus</title>
      <link>https://arxiv.org/abs/2405.09173</link>
      <description>arXiv:2405.09173v2 Announce Type: replace 
Abstract: The purpose of a consensus protocol is to keep a distributed network of nodes "in sync," even in the presence of an unpredictable communication network and adversarial behavior by some of the participating nodes. In the permissionless setting, these nodes may be operated by unknown players, with each player free to use multiple identifiers and to start or stop running the protocol at any time. Establishing that a permissionless consensus protocol is "secure" thus requires both a distributed computing argument (that the protocol guarantees consistency and liveness unless the fraction of adversarial participation is sufficiently large) and an economic argument (that carrying out an attack would be prohibitively expensive for an attacker). There is a mature toolbox for assembling arguments of the former type; the goal of this paper is to lay the foundations for arguments of the latter type.
  An ideal permissionless consensus protocol would, in addition to satisfying standard consistency and liveness guarantees, render consistency violations prohibitively expensive for the attacker without collateral damage to honest participants. We make this idea precise with our notion of the EAAC (expensive to attack in the absence of collapse) property, and prove the following results:
  1. In the synchronous and dynamically available setting, with an adversary that controls at least one-half of the overall resources, no protocol can be EAAC.
  2. In the partially synchronous and quasi-permissionless setting, with an adversary that controls at least one-third of the overall resources, no protocol can be EAAC.
  3. In the synchronous and quasi-permissionless setting, there is a proof-of-stake protocol that, provided the adversary controls less than two-thirds of the overall stake, satisfies the EAAC property.
  All three results are optimal with respect to the size of the adversary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09173v2</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Budish, Andrew Lewis-Pye, Tim Roughgarden</dc:creator>
    </item>
    <item>
      <title>OMPGPT: A Generative Pre-trained Transformer Model for OpenMP</title>
      <link>https://arxiv.org/abs/2401.16445</link>
      <description>arXiv:2401.16445v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs)such as ChatGPT have significantly advanced the field of Natural Language Processing (NLP). This trend led to the development of code-based large language models such as StarCoder, WizardCoder, and CodeLlama, which are trained extensively on vast repositories of code and programming languages. While the generic abilities of these code LLMs are useful for many programmers in tasks like code generation, the area of high-performance computing (HPC) has a narrower set of requirements that make a smaller and more domain-specific model a smarter choice. This paper presents OMPGPT, a novel domain-specific model meticulously designed to harness the inherent strengths of language models for OpenMP pragma generation. Furthermore, we leverage prompt engineering techniques from the NLP domain to create Chain-of-OMP, an innovative strategy designed to enhance OMPGPT's effectiveness. Our extensive evaluations demonstrate that OMPGPT outperforms existing large language models specialized in OpenMP tasks and maintains a notably smaller size, aligning it more closely with the typical hardware constraints of HPC environments. We consider our contribution as a pivotal bridge, connecting the advantage of language models with the specific demands of HPC tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16445v3</guid>
      <category>cs.SE</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Le Chen, Arijit Bhattacharjee, Nesreen Ahmed, Niranjan Hasabnis, Gal Oren, Vy Vo, Ali Jannesari</dc:creator>
    </item>
    <item>
      <title>Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization</title>
      <link>https://arxiv.org/abs/2405.15861</link>
      <description>arXiv:2405.15861v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) offers a promising framework for collaborative and privacy-preserving machine learning across distributed data sources. However, the substantial communication costs associated with FL pose a significant challenge to its efficiency. Specifically, in each communication round, the communication costs scale linearly with the model's dimension, which presents a formidable obstacle, especially in large model scenarios. Despite various communication efficient strategies, the intrinsic dimension-dependent communication cost remains a major bottleneck for current FL implementations. In this paper, we introduce a novel dimension-free communication strategy for FL, leveraging zero-order optimization techniques. We propose a new algorithm, FedDisco, which facilitates the transmission of only a constant number of scalar values between clients and the server in each communication round, thereby reducing the communication cost from $\mathscr{O}(d)$ to $\mathscr{O}(1)$, where $d$ is the dimension of the model parameters. Theoretically, in non-convex functions, we prove that our algorithm achieves state-of-the-art rates, which show a linear speedup of the number of clients and local steps under standard assumptions and dimension-free rate for low effective rank scenarios. Empirical evaluations through classic deep learning training and large language model fine-tuning substantiate significant reductions in communication overhead compared to traditional FL approaches. Our code is available at https://github.com/ZidongLiu/FedDisco.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15861v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhe Li, Bicheng Ying, Zidong Liu, Haibo Yang</dc:creator>
    </item>
    <item>
      <title>Beyond Efficiency: Scaling AI Sustainably</title>
      <link>https://arxiv.org/abs/2406.05303</link>
      <description>arXiv:2406.05303v2 Announce Type: replace-cross 
Abstract: Barroso's seminal contributions in energy-proportional warehouse-scale computing launched an era where modern datacenters have become more energy efficient and cost effective than ever before. At the same time, modern AI applications have driven ever-increasing demands in computing, highlighting the importance of optimizing efficiency across the entire deep learning model development cycle. This paper characterizes the carbon impact of AI, including both operational carbon emissions from training and inference as well as embodied carbon emissions from datacenter construction and hardware manufacturing. We highlight key efficiency optimization opportunities for cutting-edge AI technologies, from deep learning recommendation models to multi-modal generative AI tasks. To scale AI sustainably, we must also go beyond efficiency and optimize across the life cycle of computing infrastructures, from hardware manufacturing to datacenter operations and end-of-life processing for the hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05303v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Carole-Jean Wu, Bilge Acun, Ramya Raghavendra, Kim Hazelwood</dc:creator>
    </item>
  </channel>
</rss>
