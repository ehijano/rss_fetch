<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Oct 2024 02:10:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Modeling Time-Varying Effects of Mobile Health Interventions Using Longitudinal Functional Data from HeartSteps Micro-Randomized Trial</title>
      <link>https://arxiv.org/abs/2410.15049</link>
      <description>arXiv:2410.15049v1 Announce Type: new 
Abstract: To optimize mobile health interventions and advance domain knowledge on intervention design, it is critical to understand how the intervention effect varies over time and with contextual information. This study aims to assess how a push notification suggesting physical activity influences individuals' step counts using data from the HeartSteps micro-randomized trial (MRT). The statistical challenges include the time-varying treatments and longitudinal functional step count measurements. We propose the first semiparametric causal excursion effect model with varying coefficients to model the time-varying effects within a decision point and across decision points in an MRT. The proposed model incorporates double time indices to accommodate the longitudinal functional outcome, enabling the assessment of time-varying effect moderation by contextual variables. We propose a two-stage causal effect estimator that is robust against a misspecified high-dimensional outcome regression nuisance model. We establish asymptotic theory and conduct simulation studies to validate the proposed estimator. Our analysis provides new insights into individuals' change in response profiles (such as how soon a response occurs) due to the activity suggestions, how such changes differ by the type of suggestions received, and how such changes depend on other contextual information such as being recently sedentary and the day being a weekday.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15049v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaxin Yu, Tianchen Qian</dc:creator>
    </item>
    <item>
      <title>Robust evaluation of vaccine effects based on estimation of vaccine efficacy curve</title>
      <link>https://arxiv.org/abs/2410.15719</link>
      <description>arXiv:2410.15719v1 Announce Type: new 
Abstract: Background: The Cox model and its extensions assuming proportional hazards is widely used to estimate vaccine efficacy (VE). In the typical situation that VE wanes over time, the VE estimates are not only sensitive to study duration and timing of vaccine delivery in relation to disease seasonality but also biased in the presence of sample attrition. Furthermore, estimates of vaccine impact such as number of cases averted (NCA) are sensitive to background disease incidence and timing of vaccine delivery. Comparison of the estimates between trials with different features can be misleading. Methods: We propose estimation of VE as a function of time in the Cox model framework, using the area under the VE curve as a summary measure of VE, and extension of the method to estimate vaccine impact. We use simulations and re-analysis of a RTS,S/AS01 malaria vaccine trial dataset to demonstrate their properties and applications. Results: Simulation under scenarios with different trial duration, magnitude of sample attrition and timing of vaccine delivery, all assuming vaccine protection wanes over time, demonstrated the problems of conventional methods assuming proportional hazard, robustness and unbiasedness of the proposed methods, and comparability of the proposed estimates of vaccine efficacy and impact across trials with different features. Furthermore, the proposed NCA estimators are informative in determining the optimal vaccine delivery strategy in regions with highly seasonal disease transmission. Conclusions: The proposed method based on estimation of vaccine efficacy trajectory provides a robust, unbiased, and flexible approach to evaluate vaccine effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15719v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ziwei Zhao, Xiangmei Ma, Paul Milligan, Yin Bun Cheung</dc:creator>
    </item>
    <item>
      <title>Towards more realistic climate model outputs: A multivariate bias correction based on zero-inflated vine copulas</title>
      <link>https://arxiv.org/abs/2410.15931</link>
      <description>arXiv:2410.15931v1 Announce Type: new 
Abstract: Climate model large ensembles are an essential research tool for analysing and quantifying natural climate variability and providing robust information for rare extreme events. The models simulated representations of reality are susceptible to bias due to incomplete understanding of physical processes. This paper aims to correct the bias of five climate variables from the CRCM5 Large Ensemble over Central Europe at a 3-hourly temporal resolution. At this high temporal resolution, two variables, precipitation and radiation, exhibit a high share of zero inflation. We propose a novel bias-correction method, VBC (Vine copula bias correction), that models and transfers multivariate dependence structures for zero-inflated margins in the data from its error-prone model domain to a reference domain. VBC estimates the model and reference distribution using vine copulas and corrects the model distribution via (inverse) Rosenblatt transformation. To deal with the variables' zero-inflated nature, we develop a new vine density decomposition that accommodates such variables and employs an adequately randomized version of the Rosenblatt transform. This novel approach allows for more accurate modelling of multivariate zero-inflated climate data. Compared with state-of-the-art correction methods, VBC is generally the best-performing correction and the most accurate method for correcting zero-inflated events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15931v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henri Funk, Ralf Ludwig, Helmut Kuechenhoff, Thomas Nagler</dc:creator>
    </item>
    <item>
      <title>Interpretable Prediction Rule Ensembles in the Presence of Missing Data</title>
      <link>https://arxiv.org/abs/2410.16187</link>
      <description>arXiv:2410.16187v1 Announce Type: new 
Abstract: Prediction Rule Ensembles (PREs) are robust and interpretable statistical learning techniques with potential for predictive analytics, yet their efficacy in the presence of missing data is untested. This study uses multiple imputation to fill in missing values, but uses a data stacking approach instead of a traditional model pooling approach to combine the results.
  We perform a simulation study to compare imputation methods under realistic conditions, focusing on sample sizes of $N=200$ and $N=400$ across 1,000 replications. Evaluated techniques include multiple imputation by chained equations with predictive mean matching (MICE PMM), MICE with Random Forest (MICE RF), Random Forest imputation with the ranger algorithm (missRanger), and imputation using extreme gradient boosting (MIXGBoost), with results compared to listwise deletion. Because stacking multiple imputed datasets can overly complicate models, we additionally explore different coarsening levels to simplify and enhance the interpretability and performance of PRE models.
  Our findings highlight a trade-off between predictive performance and model complexity in selecting imputation methods. While MIXGBoost and MICE PMM yield high rule recovery rates, they also increase false positives in rule selection. In contrast, MICE RF and missRanger promote rule sparsity. MIXGBoost achieved the greatest MSE reduction, followed by MICE PMM, MICE RF, and missRanger. Avoiding too-course rounding of variables helps to reduce model size with marginal loss in performance. Listwise deletion has an adverse impact on model validity. Our results emphasize the importance of choosing suitable imputation techniques based on research goals and of advancing methods for handling missing data in statistical learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16187v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Schroeder, Jakob Schwerter, Marjolein Fokkema, Philipp Doebler</dc:creator>
    </item>
    <item>
      <title>The impact of the spatial resolution of wind data on multi-decadal wind power forecasts in Germany</title>
      <link>https://arxiv.org/abs/2410.14681</link>
      <description>arXiv:2410.14681v1 Announce Type: cross 
Abstract: Accurate multi-decadal wind power predictions are crucial for sustainable energy transitions but are challenged by the coarse spatial resolution of global climate models (GCMs). This study examines the impact of spatial resolution on wind power forecasts by analyzing historical wind speed outputs from ten CMIP6 GCMs in Germany, using ERA5 reanalysis as a reference. Results show that the choice of GCM is the primary influence on wind speed output, with higher resolution models partly, but not consistently, improving predictions. While high-resolution models better capture extreme wind speeds, they do not systematically improve the prediction of the whole wind speed distribution. The data set MPI-ESM1-2-HR (MPI-HR) was found to represent the wind speed distribution particularly faithfully, while the MIROC6 (JAP) data set showed substantial underestimation for the German region compared to ERA5. These findings underscore the complexity of wind speed modeling for power predictions and emphasize the need for careful GCM selection and appropriate downscaling and bias correction methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14681v1</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sofia Morelli</dc:creator>
    </item>
    <item>
      <title>Stochastic Loss Reserving: Dependence and Estimation</title>
      <link>https://arxiv.org/abs/2410.14985</link>
      <description>arXiv:2410.14985v1 Announce Type: cross 
Abstract: Nowadays insurers have to account for potentially complex dependence between risks. In the field of loss reserving, there are many parametric and non-parametric models attempting to capture dependence between business lines. One common approach has been to use additive background risk models (ABRMs) which provide rich and interpretable dependence structures via a common shock model. Unfortunately, ABRMs are often restrictive. Models that capture necessary features may have impractical to estimate parameters. For example models without a closed-form likelihood function for lack of a probability density function (e.g. some Tweedie, Stable Distributions, etc).
  We apply a modification of the continuous generalised method of moments (CGMM) of [Carrasco and Florens, 2000] which delivers comparable estimators to the MLE to loss reserving. We examine models such as the one proposed by [Avanzi et al., 2016] and a related but novel one derived from the stable family of distributions. Our CGMM method of estimation provides conventional non-Bayesian estimates in the case where MLEs are impractical.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14985v1</guid>
      <category>stat.ME</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andrew Fleck, Edward Furman, Yang Shen</dc:creator>
    </item>
    <item>
      <title>Latency correction in sparse neuronal spike trains with overlapping global events</title>
      <link>https://arxiv.org/abs/2410.15018</link>
      <description>arXiv:2410.15018v1 Announce Type: cross 
Abstract: Background: In Kreuz et al., J Neurosci Methods 381, 109703 (2022) two methods were proposed that perform latency correction, i.e., optimize the spike time alignment of sparse neuronal spike trains with well defined global spiking events. The first one based on direct shifts is fast but uses only partial latency information, while the other one makes use of the full information but relies on the computationally costly simulated annealing. Both methods reach their limits and can become unreliable when successive global events are not sufficiently separated or even overlap.
  New Method: Here we propose an iterative scheme that combines the advantages of the two original methods by using in each step as much of the latency information as possible and by employing a very fast extrapolation direct shift method instead of the much slower simulated annealing.
  Results: We illustrate the effectiveness and the improved performance, measured in terms of the relative shift error, of the new iterative scheme not only on simulated data with known ground truths but also on single-unit recordings from two medial superior olive neurons of a gerbil.
  Comparison with Existing Method(s): The iterative scheme outperforms the existing approaches on both the simulated and the experimental data. Due to its low computational demands, and in contrast to simulated annealing, it can also be applied to very large datasets.
  Conclusions: The new method generalizes and improves on the original method both in terms of accuracy and speed. Importantly, it is the only method that allows to disentangle global events with overlap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15018v1</guid>
      <category>q-bio.NC</category>
      <category>physics.bio-ph</category>
      <category>physics.data-an</category>
      <category>physics.med-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arturo Mariani, Federico Senocrate, Jason Mikiel-Hunter, David McAlpine, Barbara Beiderbeck, Michael Pecka, Kevin Lin, Thomas Kreuz</dc:creator>
    </item>
    <item>
      <title>Randomization Inference for Before-and-After Studies with Multiple Units: An Application to a Criminal Procedure Reform in Uruguay</title>
      <link>https://arxiv.org/abs/2410.15477</link>
      <description>arXiv:2410.15477v1 Announce Type: cross 
Abstract: We study the immediate impact of a new code of criminal procedure on crime. In November 2017, Uruguay switched from an inquisitorial system (where a single judge leads the investigation and decides the appropriate punishment for a particular crime) to an adversarial system (where the investigation is now led by prosecutors and the judge plays an overseeing role). To analyze the short-term effects of this reform, we develop a randomization-based approach for before-and-after studies with multiple units. Our framework avoids parametric time series assumptions and eliminates extrapolation by basing statistical inferences on finite-sample methods that rely only on the time periods closest to the time of the policy intervention. A key identification assumption underlying our method is that there would have been no time trends in the absence of the intervention, which is most plausible in a small window around the time of the reform. We also discuss several falsification methods to assess the plausibility of this assumption. Using our proposed inferential approach, we find statistically significant short-term causal effects of the crime reform. Our unbiased estimate shows an average increase of approximately 25 police reports per day in the week following the implementation of the new adversarial system in Montevideo, representing an 8 percent increase compared to the previous week under the old system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15477v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Carlos Diaz, Rocio Titiunik</dc:creator>
    </item>
    <item>
      <title>changepointGA: An R package for Fast Changepoint Detection via Genetic Algorithm</title>
      <link>https://arxiv.org/abs/2410.15571</link>
      <description>arXiv:2410.15571v1 Announce Type: cross 
Abstract: Genetic algorithm (GA) are stochastic search techniques designed to address combinatorial optimization problems by mimicking the principles of natural selection and evolution. GAs have proven effective in both single and multiple changepoint analyses within time series data, where each chromosome encodes the hyperparameters, number, and locations of changepoints, along with the associated model parameters. Starting with a population of potential changepoint configurations, GAs utilize genetic operators -- selection, crossover, and mutation -- to evolve toward solutions with enhanced fitness. This paper presents the R package changepointGA, which encodes changepoint chromosomes in an integer format. Furthermore, changepointGA facilitates the dynamic and simultaneous estimation of changepoint models hyperparameters, changepoint configurations, and model parameters, leading to more robust and accurate analyses. Several simulation studies and real-world applications are discussed to illustrate the package capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15571v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mo Li</dc:creator>
    </item>
    <item>
      <title>Massimo: Public Queue Monitoring and Management using Mass-Spring Model</title>
      <link>https://arxiv.org/abs/2410.16012</link>
      <description>arXiv:2410.16012v1 Announce Type: cross 
Abstract: An efficient system of a queue control and regulation in public spaces is very important in order to avoid the traffic jams and to improve the customer satisfaction. This article offers a detailed road map based on a merger of intelligent systems and creating an efficient systems of queues in public places. Through the utilization of different technologies i.e. computer vision, machine learning algorithms, deep learning our system provide accurate information about the place is crowded or not and the necessary efforts to be taken.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16012v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhijeet Kumar, Unnati Singh, Rajdeep Chatterjee, Tathagata Bandyopadhyay</dc:creator>
    </item>
    <item>
      <title>Multiple merger coalescent inference of effective population size</title>
      <link>https://arxiv.org/abs/2407.14976</link>
      <description>arXiv:2407.14976v3 Announce Type: replace 
Abstract: Variation in a sample of molecular sequence data informs about the past evolutionary history of the sample's population. Traditionally, Bayesian modeling coupled with the standard coalescent, is used to infer the sample's bifurcating genealogy and demographic and evolutionary parameters such as effective population size, and mutation rates. However, there are many situations where binary coalescent models do not accurately reflect the true underlying ancestral processes. Here, we propose a Bayesian nonparametric method for inferring effective population size trajectories from a multifurcating genealogy under the $\Lambda-$coalescent. In particular, we jointly estimate the effective population size and model parameters for the Beta-coalescent model, a special type of $\Lambda-$coalescent. Finally, we test our methods on simulations and apply them to study various viral dynamics as well as Japanese sardine population size changes over time. The code and vignettes can be found in the phylodyn package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14976v3</guid>
      <category>stat.AP</category>
      <category>q-bio.PE</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julie Zhang, Julia A. Palacios</dc:creator>
    </item>
    <item>
      <title>Boosting with copula-based components</title>
      <link>https://arxiv.org/abs/2208.04669</link>
      <description>arXiv:2208.04669v2 Announce Type: replace-cross 
Abstract: The authors propose new additive models for binary outcomes, where the components are copula-based regression models (Noh et al, 2013), and designed such that the model may capture potentially complex interaction effects. The models do not require discretisation of continuous covariates, and are therefore suitable for problems with many such covariates. A fitting algorithm, and efficient procedures for model selection and evaluation of the components are described. Software is provided in the R-package copulaboost. Simulations and illustrations on data sets indicate that the method's predictive performance is either better than or comparable to the other methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.04669v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Boge Brant, Ingrid Hob{\ae}k Haff</dc:creator>
    </item>
    <item>
      <title>A Bayesian Nonparametric Method to Adjust for Unmeasured Confounding with Negative Controls</title>
      <link>https://arxiv.org/abs/2309.02631</link>
      <description>arXiv:2309.02631v2 Announce Type: replace-cross 
Abstract: Unmeasured confounding bias threatens the validity of observational studies. While sensitivity analyses and study designs have been proposed to address this issue, they often overlook the growing availability of auxiliary data. Using negative controls from these data is a promising new approach to reduce unmeasured confounding bias. In this article, we develop a Bayesian nonparametric method to estimate a causal exposure-response function (CERF) leveraging information from negative controls to adjust for unmeasured confounding. We model the CERF as a mixture of linear models. This strategy captures the potential nonlinear shape of CERFs while maintaining computational efficiency, and it leverages closed-form results that hold under the linear model assumption. We assess the performance of our method through simulation studies. We found that the proposed method can recover the true shape of the CERF in the presence of unmeasured confounding under assumptions. To show the practical utility of our approach, we apply it to adjust for a possible unmeasured confounder when evaluating the relationship between long-term exposure to ambient $PM_{2.5}$ and cardiovascular hospitalization rates among the elderly in the continental US. We implement our estimation procedure in open-source software and have made the code publicly available to ensure reproducibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.02631v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jie Kate Hu, Dafne Zorzetto, Francesca Dominici</dc:creator>
    </item>
    <item>
      <title>Maximum Likelihood Estimation under the Emax Model: Existence, Geometry and Efficiency</title>
      <link>https://arxiv.org/abs/2401.00354</link>
      <description>arXiv:2401.00354v2 Announce Type: replace-cross 
Abstract: This study focuses on the estimation of the Emax dose-response model, a widely utilized framework in clinical trials, agriculture, and environmental experiments. Existing challenges in obtaining maximum likelihood estimates (MLE) for model parameters are often ascribed to computational issues but, in reality, stem from the absence of a MLE. Our contribution provides a new understanding and control of all the experimental situations that practitioners might face, guiding them in the estimation process. We derive the exact MLE for a three-point experimental design and we identify the two scenarios where the MLE fails. To address these challenges, we propose utilizing Firth's modified score, providing its analytical expression as a function of the experimental design. Through a simulation study, we demonstrate that, in one of the problematic cases, the Firth modification yields a finite estimate. For the remaining case, we introduce a design-augmentation strategy akin to a hypothesis test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00354v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giacomo Aletti, Nancy Flournoy, Caterina May, Chiara Tommasi</dc:creator>
    </item>
    <item>
      <title>Data-driven modeling and prediction of microglial cell dynamics in the ischemic penumbra</title>
      <link>https://arxiv.org/abs/2404.10915</link>
      <description>arXiv:2404.10915v2 Announce Type: replace-cross 
Abstract: Neuroinflammation immediately follows the onset of ischemic stroke. During this process, microglial cells are activated in and recruited to the tissue surrounding the irreversibly injured infarct core, referred to as the penumbra. Microglial cells can be activated into two distinct phenotypes; however, the dynamics between the detrimental M1 phenotype and beneficial M2 phenotype are not fully understood. Using phenotype-specific cell count data obtained from experimental studies on middle cerebral artery occlusion-induced stroke in mice, we employ sparsity-promoting system identification techniques combined with Bayesian statistical methods for uncertainty quantification to generate continuous and discrete-time predictive models of the M1 and M2 microglial cell dynamics. The resulting data-driven models include constant and linear terms but do not include nonlinear interactions between the cells. Results emphasize an initial M2 dominance followed by a takeover of M1 cells, capture potential long-term dynamics of microglial cells, and suggest a persistent inflammatory response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10915v2</guid>
      <category>q-bio.CB</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sara Amato, Andrea Arnold</dc:creator>
    </item>
    <item>
      <title>Valid standard errors for Bayesian quantile regression with clustered and independent data</title>
      <link>https://arxiv.org/abs/2407.09772</link>
      <description>arXiv:2407.09772v2 Announce Type: replace-cross 
Abstract: In Bayesian quantile regression, the most commonly used likelihood is the asymmetric Laplace (AL) likelihood. The reason for this choice is not that it is a plausible data-generating model but that the corresponding maximum likelihood estimator is identical to the classical estimator by Koenker and Bassett (1978), and in that sense, the AL likelihood can be thought of as a working likelihood. AL-based quantile regression has been shown to produce good finite-sample Bayesian point estimates and to be consistent. However, if the AL distribution does not correspond to the data-generating distribution, credible intervals based on posterior standard deviations can have poor coverage. Yang, Wang, and He (2016) proposed an adjustment to the posterior covariance matrix that produces asymptotically valid intervals. However, we show that this adjustment is sensitive to the choice of scale parameter for the AL likelihood and can lead to poor coverage when the sample size is small to moderate. We therefore propose using Infinitesimal Jackknife (IJ) standard errors (Giordano &amp; Broderick, 2023). These standard errors do not require resampling but can be obtained from a single MCMC run. We also propose a version of IJ standard errors for clustered data. Simulations and applications to real data show that the IJ standard errors have good frequentist properties, both for independent and clustered data. We provide an R-package, IJSE, that computes IJ standard errors for clustered or independent data after estimation with the brms wrapper in R for Stan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09772v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feng Ji, JoonHo Lee, Sophia Rabe-Hesketh</dc:creator>
    </item>
    <item>
      <title>Neural Networks with LSTM and GRU in Modeling Active Fires in the Amazon</title>
      <link>https://arxiv.org/abs/2409.02681</link>
      <description>arXiv:2409.02681v5 Announce Type: replace-cross 
Abstract: This study presents a comprehensive methodology for modeling and forecasting the historical time series of active fire spots detected by the AQUA\_M-T satellite in the Amazon, Brazil. The approach employs a mixed Recurrent Neural Network (RNN) model, combining Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) architectures to predict the monthly accumulations of daily detected active fire spots. Data analysis revealed a consistent seasonality over time, with annual maximum and minimum values tending to repeat at the same periods each year. The primary objective is to verify whether the forecasts capture this inherent seasonality through machine learning techniques. The methodology involved careful data preparation, model configuration, and training using cross-validation with two seeds, ensuring that the data generalizes well to both the test and validation sets for both seeds. The results indicate that the combined LSTM and GRU model delivers excellent forecasting performance, demonstrating its effectiveness in capturing complex temporal patterns and modeling the observed time series. This research significantly contributes to the application of deep learning techniques in environmental monitoring, specifically in forecasting active fire spots. The proposed approach highlights the potential for adaptation to other time series forecasting challenges, opening new opportunities for research and development in machine learning and prediction of natural phenomena.
  Keywords: Time Series Forecasting; Recurrent Neural Networks; Deep Learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02681v5</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ramon Tavares, Ricardo Olinda</dc:creator>
    </item>
    <item>
      <title>Functional Singular Value Decomposition</title>
      <link>https://arxiv.org/abs/2410.03619</link>
      <description>arXiv:2410.03619v2 Announce Type: replace-cross 
Abstract: Heterogeneous functional data are commonly seen in time series and longitudinal data analysis. To capture the statistical structures of such data, we propose the framework of Functional Singular Value Decomposition (FSVD), a unified framework with structure-adaptive interpretability for the analysis of heterogeneous functional data. We establish the mathematical foundation of FSVD by proving its existence and providing its fundamental properties using operator theory. We then develop an implementation approach for noisy and irregularly observed functional data based on a novel joint kernel ridge regression scheme and provide theoretical guarantees for its convergence and estimation accuracy. The framework of FSVD also introduces the concepts of intrinsic basis functions and intrinsic basis vectors, which represent two fundamental statistical structures for random functions and connect FSVD to various tasks including functional principal component analysis, factor models, functional clustering, and functional completion. We compare the performance of FSVD with existing methods in several tasks through extensive simulation studies. To demonstrate the value of FSVD in real-world datasets, we apply it to extract temporal patterns from a COVID-19 case count dataset and perform data completion on an electronic health record dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03619v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jianbin Tan, Pixu Shi, Anru R. Zhang</dc:creator>
    </item>
  </channel>
</rss>
