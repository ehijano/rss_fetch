<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Jun 2025 04:01:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>When Tukey meets Chauvenet: a new boxplot criterion for outlier detection</title>
      <link>https://arxiv.org/abs/2506.06491</link>
      <description>arXiv:2506.06491v1 Announce Type: new 
Abstract: The box-and-whisker plot, introduced by Tukey (1977), is one of the most popular graphical methods in descriptive statistics. On the other hand, however, Tukey's boxplot is free of sample size, yielding the so-called "one-size-fits-all" fences for outlier detection. Although improvements on the sample size adjusted boxplots do exist in the literature, most of them are either not easy to implement or lack justification. As another common rule for outlier detection, Chauvenet's criterion uses the sample mean and standard derivation to perform the test, but it is often sensitive to the included outliers and hence is not robust. In this paper, by combining Tukey's boxplot and Chauvenet's criterion, we introduce a new boxplot, namely the Chauvenet-type boxplot, with the fence coefficient determined by an exact control of the outside rate per observation. Our new outlier criterion not only maintains the simplicity of the boxplot from a practical perspective, but also serves as a robust Chauvenet's criterion. Simulation study and a real data analysis on the civil service pay adjustment in Hong Kong demonstrate that the Chauvenet-type boxplot performs extremely well regardless of the sample size, and can therefore be highly recommended for practical use to replace both Tukey's boxplot and Chauvenet's criterion. Lastly, to increase the visibility of the work, a user-friendly R package named `ChauBoxplot' has also been officially released on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06491v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Journal of Computational and Graphical Statistics, 2025</arxiv:journal_reference>
      <dc:creator>Hongmei Lin, Riquan Zhang, Tiejun Tong</dc:creator>
    </item>
    <item>
      <title>Inward and Outward Spillover Effects of One Unit's Treatment on Network Neighbors under Partial Interference</title>
      <link>https://arxiv.org/abs/2506.06615</link>
      <description>arXiv:2506.06615v1 Announce Type: new 
Abstract: In settings where interference is present, direct effects are commonly defined as the average effect of a unit's treatment on their own outcome while fixing the treatment status or probability among interfering units, and spillover effects measure the average effect of a change in the latter while the individual's treatment status is kept fixed. Here, we define the average causal effect of a unit's treatment status on the outcome of their network neighbors, while fixing the treatment probability in the remaining interference set. We propose two different weighting schemes defining two causal effects: i) the outward spillover effect, which represents the average effect of a unit's treatment on their neighbors' potential outcomes, and ii) the inward spillover effect, which represents the impact of each neighbor's treatment on an individual's own potential outcome. We prove that outward and inward spillover effects generally differ, even in an undirected network. However, under specific conditions these two causal estimands become equivalent. We provide numerous examples illustrating the conditions for equivalence or discrepancy of the two spillover effects. We then compare their Horvitz-Thompson estimators, examining their relative variance under various graph structures and structural assumptions on potential outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06615v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fei Fang, Edoardo M Airoldi, Laura Forastiere</dc:creator>
    </item>
    <item>
      <title>Tensor Stochastic Regression for High-dimensional Time Series via CP Decomposition</title>
      <link>https://arxiv.org/abs/2506.06919</link>
      <description>arXiv:2506.06919v1 Announce Type: new 
Abstract: As tensor-valued data become increasingly common in time series analysis, there is a growing need for flexible and interpretable models that can handle high-dimensional predictors and responses across multiple modes. We propose a unified framework for high-dimensional tensor stochastic regression based on CANDECOMP/PARAFAC (CP) decomposition, which encompasses vector, matrix, and tensor responses and predictors as special cases. Tensor autoregression naturally arises as a special case within this framework. By leveraging CP decomposition, the proposed models interpret the interactive roles of any two distinct tensor modes, enabling dynamic modeling of input-output mechanisms. We develop both CP low-rank and sparse CP low-rank estimators, establish their non-asymptotic error bounds, and propose an efficient alternating minimization algorithm for estimation. Simulation studies confirm the theoretical properties and demonstrate the computational advantage. Applications to mixed-frequency macroeconomic data and spatio-temporal air pollution data reveal interpretable low-dimensional structures and meaningful dynamic dependencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06919v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shibo Li, Yao Zheng</dc:creator>
    </item>
    <item>
      <title>Efficient and Robust Block Designs for Order-of-Addition Experiments</title>
      <link>https://arxiv.org/abs/2506.07096</link>
      <description>arXiv:2506.07096v1 Announce Type: new 
Abstract: Designs for Order-of-Addition (OofA) experiments have received growing attention due to their impact on responses based on the sequence of component addition. In certain cases, these experiments involve heterogeneous groups of units, which necessitates the use of blocking to manage variation effects. Despite this, the exploration of block OofA designs remains limited in the literature. As experiments become increasingly complex, addressing this gap is essential to ensure that the designs accurately reflect the effects of the addition sequence and effectively handle the associated variability. Motivated by this, this paper seeks to address the gap by expanding the indicator function framework for block OofA designs. We propose the use of the word length pattern as a criterion for selecting robust block OofA designs. To improve search efficiency and reduce computational demands, we develop algorithms that employ orthogonal Latin squares for design construction and selection, minimizing the need for exhaustive searches. Our analysis, supported by correlation plots, reveals that the algorithms effectively manage confounding and aliasing between effects. Additionally, simulation studies indicate that designs based on our proposed criterion and algorithms achieve power and type I error rates comparable to those of full block OofA designs. This approach offers a practical and efficient method for constructing block OofA designs and may provide valuable insights for future research and applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07096v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chang-Yun Lin</dc:creator>
    </item>
    <item>
      <title>Spectral Clustering with Likelihood Refinement is Optimal for Latent Class Recovery</title>
      <link>https://arxiv.org/abs/2506.07167</link>
      <description>arXiv:2506.07167v1 Announce Type: new 
Abstract: Latent class models are widely used for identifying unobserved subgroups from multivariate categorical data in social sciences, with binary data as a particularly popular example. However, accurately recovering individual latent class memberships and determining the number of classes remains challenging, especially when handling large-scale datasets with many items. This paper proposes a novel two-stage algorithm for latent class models with high-dimensional binary responses. Our method first initializes latent class assignments by an easy-to-implement spectral clustering algorithm, and then refines these assignments with a one-step likelihood-based update. This approach combines the computational efficiency of spectral clustering with the improved statistical accuracy of likelihood-based estimation. We establish theoretical guarantees showing that this method leads to optimal latent class recovery and exact clustering of subjects under mild conditions. Additionally, we propose a simple consistent estimator for the number of latent classes. Extensive experiments on both simulated data and real data validate our theoretical results and demonstrate our method's superior performance over alternative methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07167v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongyuan Lyu, Yuqi Gu</dc:creator>
    </item>
    <item>
      <title>Change-Points Detection and Support Recovery for Spatially Indexed Functional Data</title>
      <link>https://arxiv.org/abs/2506.07206</link>
      <description>arXiv:2506.07206v1 Announce Type: new 
Abstract: Large volumes of spatiotemporal data, characterized by high spatial and temporal variability, may experience structural changes over time. Unlike traditional change-point problems, each sequence in this context consists of function-valued curves observed at multiple spatial locations, with typically only a small subset of locations affected. This paper addresses two key issues: detecting the global change-point and identifying the spatial support set, within a unified framework tailored to spatially indexed functional data. By leveraging a weakly separable cross-covariance structure -- an extension beyond the restrictive assumption of space-time separability -- we incorporate functional principal component analysis into the change-detection methodology, while preserving common temporal features across locations. A kernel-based test statistic is further developed to integrate spatial clustering pattern into the detection process, and its local variant, combined with the estimated change-point, is employed to identify the subset of locations contributing to the mean shifts. To control the false discovery rate in multiple testing, we introduce a functional symmetrized data aggregation approach that does not rely on pointwise p-values and effectively pools spatial information. We establish the asymptotic validity of the proposed change detection and support recovery method under mild regularity conditions. The efficacy of our approach is demonstrated through simulations, with its practical usefulness illustrated in an application to China's precipitation data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07206v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fengyi Song, Decai Liang, Changliang Zou</dc:creator>
    </item>
    <item>
      <title>Strongly Consistent Community Detection in Popularity Adjusted Block Models</title>
      <link>https://arxiv.org/abs/2506.07224</link>
      <description>arXiv:2506.07224v1 Announce Type: new 
Abstract: The Popularity Adjusted Block Model (PABM) provides a flexible framework for community detection in network data by allowing heterogeneous node popularity across communities. However, this flexibility increases model complexity and raises key unresolved challenges, particularly in effectively adapting spectral clustering techniques and efficiently achieving strong consistency in label recovery. To address these challenges, we first propose the Thresholded Cosine Spectral Clustering (TCSC) algorithm and establish its weak consistency under the PABM. We then introduce the one-step Refined TCSC algorithm and prove that it achieves strong consistency under the PABM, correctly recovering all community labels with high probability. We further show that the two-step Refined TCSC accelerates clustering error convergence, especially with small sample sizes. Additionally, we propose a data-driven approach for selecting the number of communities, which outperforms existing methods under the PABM. The effectiveness and robustness of our methods are validated through extensive simulations and real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07224v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Quan Yuan, Binghui Liu, Danning Li, Lingzhou Xue</dc:creator>
    </item>
    <item>
      <title>Impact of Label Noise from Large Language Models Generated Annotations on Evaluation of Diagnostic Model Performance</title>
      <link>https://arxiv.org/abs/2506.07273</link>
      <description>arXiv:2506.07273v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used to generate labels from radiology reports to enable large-scale AI evaluation. However, label noise from LLMs can introduce bias into performance estimates, especially under varying disease prevalence and model quality. This study quantifies how LLM labeling errors impact downstream diagnostic model evaluation. We developed a simulation framework to assess how LLM label errors affect observed model performance. A synthetic dataset of 10,000 cases was generated across different prevalence levels. LLM sensitivity and specificity were varied independently between 90% and 100%. We simulated diagnostic models with true sensitivity and specificity ranging from 90% to 100%. Observed performance was computed using LLM-generated labels as the reference. We derived analytical performance bounds and ran 5,000 Monte Carlo trials per condition to estimate empirical uncertainty. Observed performance was highly sensitive to LLM label quality, with bias strongly influenced by disease prevalence. In low-prevalence settings, small reductions in LLM specificity led to substantial underestimation of sensitivity. For example, at 10% prevalence, an LLM with 95% specificity yielded an observed sensitivity of ~53% despite a perfect model. In high-prevalence scenarios, reduced LLM sensitivity caused underestimation of model specificity. Monte Carlo simulations consistently revealed downward bias, with observed performance often falling below true values even when within theoretical bounds. LLM-generated labels can introduce systematic, prevalence-dependent bias into model evaluation. Specificity is more critical in low-prevalence tasks, while sensitivity dominates in high-prevalence settings. These findings highlight the importance of prevalence-aware prompt design and error characterization when using LLMs for post-deployment model assessment in clinical AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07273v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammadreza Chavoshi, Hari Trivedi, Janice Newsome, Aawez Mansuri, Chiratidzo Rudado Sanyika, Rohan Satya Isaac, Frank Li, Theo Dapamede, Judy Gichoya</dc:creator>
    </item>
    <item>
      <title>Integrating tumor burden with survival outcome for treatment effect evaluation in oncology trials</title>
      <link>https://arxiv.org/abs/2506.07387</link>
      <description>arXiv:2506.07387v1 Announce Type: new 
Abstract: In early-phase cancer clinical trials, the limited availability of data presents significant challenges in developing a framework to efficiently quantify treatment effectiveness. To address this, we propose a novel utility-based Bayesian approach for assessing treatment effects in these trials, where data scarcity is a major concern. Our approach synthesizes tumor burden, a key biomarker for evaluating patient response to oncology treatments, and survival outcome, a widely used endpoint for assessing clinical benefits, by jointly modeling longitudinal and survival data. The proposed method, along with its novel estimand, aims to efficiently capture signals of treatment efficacy in early-phase studies and holds potential for development as an endpoint in Phase 3 confirmatory studies. We conduct simulations to investigate the frequentist characteristics of the proposed estimand in a simple setting, which demonstrate relatively controlled Type I error rates when testing the treatment effect on outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07387v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saurabh Bhandari, Michael J. Daniels, Chenguang Wang</dc:creator>
    </item>
    <item>
      <title>One-dimensional quantile-stratified sampling and its application in statistical simulations</title>
      <link>https://arxiv.org/abs/2506.07437</link>
      <description>arXiv:2506.07437v1 Announce Type: new 
Abstract: In this paper we examine quantile-stratified samples from a known univariate probability distribution, with stratification occurring over a partition of the quantile regions in the distribution. We examine some general properties of this sampling method and we contrast it with standard IID sampling to highlight its similarities and differences. We examine the applications of this sampling method to various statistical simulations including importance sampling. We conduct simulation analysis to compare the performance of standard importance sampling against the quantile-stratified importance sampling to see how they each perform on a range of functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07437v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ben O'Neill</dc:creator>
    </item>
    <item>
      <title>Individual Treatment Effect: Prediction Intervals and Sharp Bounds</title>
      <link>https://arxiv.org/abs/2506.07469</link>
      <description>arXiv:2506.07469v1 Announce Type: new 
Abstract: Individual treatment effect (ITE) is often regarded as the ideal target of inference in causal analyses and has been the focus of several recent studies. In this paper, we describe the intrinsic limits regarding what can be learned concerning ITEs given data from large randomized experiments. We consider when a valid prediction interval for the ITE is informative and when it can be bounded away from zero. The joint distribution over potential outcomes is only partially identified from a randomized trial. Consequently, to be valid, an ITE prediction interval must be valid for all joint distribution consistent with the observed data and hence will in general be wider than that resulting from knowledge of this joint distribution. We characterize prediction intervals in the binary treatment and outcome setting, and extend these insights to models with continuous and ordinal outcomes. We derive sharp bounds on the probability mass function (pmf) of the individual treatment effect (ITE). Finally, we contrast prediction intervals for the ITE and confidence intervals for the average treatment effect (ATE). This also leads to the consideration of Fisher versus Neyman null hypotheses. While confidence intervals for the ATE shrink with increasing sample size due to its status as a population parameter, prediction intervals for the ITE generally do not vanish, leading to scenarios where one may reject the Neyman null yet still find evidence consistent with the Fisher null, highlighting the challenges of individualized decision-making under partial identification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07469v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhehao Zhang, Thomas S. Richardson</dc:creator>
    </item>
    <item>
      <title>Scalable Spatiotemporal Modeling for Bicycle Count Prediction</title>
      <link>https://arxiv.org/abs/2506.07582</link>
      <description>arXiv:2506.07582v1 Announce Type: new 
Abstract: We propose a novel sparse spatiotemporal dynamic generalized linear model for efficient inference and prediction of bicycle count data. Assuming Poisson distributed counts with spacetime-varying rates, we model the log-rate using spatiotemporal intercepts, dynamic temporal covariates, and site-specific effects additively. Spatiotemporal dependence is modeled using a spacetime-varying intercept that evolves smoothly over time with spatially correlated errors, and coefficients of some temporal covariates including seasonal harmonics also evolve dynamically over time. Inference is performed following the Bayesian paradigm, and uncertainty quantification is naturally accounted for when predicting bicycle counts for unobserved locations and future times of interest. To address the challenges of high-dimensional inference of spatiotemporal data in a Bayesian setting, we develop a customized hybrid Markov Chain Monte Carlo (MCMC) algorithm. To address the computational burden of dense covariance matrices, we extend our framework to high-dimensional spatial settings using the sparse SPDE approach of Lindgren et al. (2011), demonstrating its accuracy and scalability on both synthetic data and Montreal Island bicycle datasets. The proposed approach naturally provides missing value imputations, kriging, future forecasting, spatiotemporal predictions, and inference of model components. Moreover, it provides ways to predict average annual daily bicycles (AADB), a key metric often sought when designing bicycle networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07582v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rishikesh Yadav, Alexandra M. Schmidt, Aurelie Labbe, Pratheepa Jeganathan, Luis F. Miranda-Moreno</dc:creator>
    </item>
    <item>
      <title>Heavy Lasso: sparse penalized regression under heavy-tailed noise via data-augmented soft-thresholding</title>
      <link>https://arxiv.org/abs/2506.07790</link>
      <description>arXiv:2506.07790v1 Announce Type: new 
Abstract: High-dimensional linear regression is a fundamental tool in modern statistics, particularly when the number of predictors exceeds the sample size. The classical Lasso, which relies on the squared loss, performs well under Gaussian noise assumptions but often deteriorates in the presence of heavy-tailed errors or outliers commonly encountered in real data applications such as genomics, finance, and signal processing. To address these challenges, we propose a novel robust regression method, termed Heavy Lasso, which incorporates a loss function inspired by the Student's t-distribution within a Lasso penalization framework. This loss retains the desirable quadratic behavior for small residuals while adaptively downweighting large deviations, thus enhancing robustness to heavy-tailed noise and outliers. Heavy Lasso enjoys computationally efficient by leveraging a data augmentation scheme and a soft-thresholding algorithm, which integrate seamlessly with classical Lasso solvers. Theoretically, we establish non-asymptotic bounds under both $\ell_1$ and $\ell_2 $ norms, by employing the framework of localized convexity, showing that the Heavy Lasso estimator achieves rates comparable to those of the Huber loss. Extensive numerical studies demonstrate Heavy Lasso's superior performance over classical Lasso and other robust variants, highlighting its effectiveness in challenging noisy settings. Our method is implemented in the R package heavylasso available on Github.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07790v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>The Tien Mai</dc:creator>
    </item>
    <item>
      <title>Identifiability in epidemic models with prior immunity and under-reporting</title>
      <link>https://arxiv.org/abs/2506.07825</link>
      <description>arXiv:2506.07825v1 Announce Type: new 
Abstract: Identifiability is the property in mathematical modelling that determines if model parameters can be uniquely estimated from data. For infectious disease models, failure to ensure identifiability can lead to misleading parameter estimates and unreliable policy recommendations. We examine the identifiability of a modified SIR model that accounts for under-reporting and pre-existing immunity in the population. We provide a mathematical proof of the unidentifiability of jointly estimating three parameters: the fraction under-reporting, the proportion of the population with prior immunity, and the community transmission rate, when only reported case data are available. We then show, analytically and with a simulation study, that the identifiability of all three parameters is achieved if the reported incidence is complemented with sample survey data of prior immunity or prevalence during the outbreak. Our results show the limitations of parameter inference in partially observed epidemics and the importance of identifiability analysis when developing and applying models for public health decision making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07825v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fanny Bergstr\"om, Martina Favero, Tom Britton</dc:creator>
    </item>
    <item>
      <title>Conditional Local Independence Testing with Application to Dynamic Causal Discovery</title>
      <link>https://arxiv.org/abs/2506.07844</link>
      <description>arXiv:2506.07844v1 Announce Type: new 
Abstract: In this note, we extend the conditional local independence testing theory developed in Christgau et al. (2024) to Ito processes. The result can be applied to causal discovery in dynamic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07844v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingzhou Liu, Xinwei Sun, Yizhou Wang</dc:creator>
    </item>
    <item>
      <title>A structural nested rate model for estimating the effects of time-varying exposure on recurrent event outcomes in the presence of death</title>
      <link>https://arxiv.org/abs/2506.07910</link>
      <description>arXiv:2506.07910v1 Announce Type: new 
Abstract: Assessing the causal effect of time-varying exposures on recurrent event processes is challenging in the presence of a terminating event. Our objective is to estimate both the short-term and delayed marginal causal effects of exposures on recurrent events while addressing the bias of a potentially correlated terminal event. Existing estimators based on marginal structural models and proportional rate models are unsuitable for estimating delayed marginal causal effects for many reasons, and furthermore, they do not account for competing risks associated with a terminating event. To address these limitations, we propose a class of semiparametric structural nested recurrent event models and two estimators of short-term and delayed marginal causal effects of exposures. We establish the asymptotic linearity of these two estimators under regularity conditions through the novel use of modern empirical process and semiparametric efficiency theory. We examine the performance of these estimators via simulation and provide an R package sncure to apply our methods in real data scenarios. Finally, we present the utility of our methods in the context of a large epidemiological study of 299,661 Medicare beneficiaries, where we estimate the effects of fine particulate matter air pollution on recurrent hospitalizations for cardiovascular disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07910v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Mork, Robert L. Strawderman, Michelle Audirac, Francesca Dominici, Ashkan Ertefaie</dc:creator>
    </item>
    <item>
      <title>Graph-theoretic Inference for Random Effects in High-dimensional Studies</title>
      <link>https://arxiv.org/abs/2506.07946</link>
      <description>arXiv:2506.07946v1 Announce Type: new 
Abstract: We study the problem of testing for the presence of random effects in mixed models with high-dimensional fixed effects. To this end, we propose a rank-based graph-theoretic approach to test whether a collection of random effects is zero. Our approach is non-parametric and model-free in the sense that we not require correct specification of the mixed model nor estimation of unknown parameters. Instead, the test statistic evaluates whether incorporating group-level correlation meaningfully improves the ability of a potentially high-dimensional covariate vector $X$ to predict a response variable $Y$. We establish the consistency of the proposed test and derive its asymptotic null distribution. Through simulation studies and a real data application, we demonstrate the practical effectiveness of the proposed test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07946v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lynna Chu, Yichuan Bai</dc:creator>
    </item>
    <item>
      <title>Mediation Analysis for Sparse and Irregularly Spaced Longitudinal Outcomes with Application to the MrOS Sleep Study</title>
      <link>https://arxiv.org/abs/2506.07953</link>
      <description>arXiv:2506.07953v1 Announce Type: new 
Abstract: Mediation analysis has become a widely used method for identifying the pathways through which an independent variable influences a dependent variable via intermediate mediators. However, limited research addresses the case where mediators are high-dimensional and the outcome is represented by sparse, irregularly spaced longitudinal data. To address these challenges, we propose a mediation analysis approach for scalar exposures, high-dimensional mediators, and sparse longitudinal outcomes. This approach effectively identifies significant mediators by addressing two key issues: (i) the underlying correlation structure within the sparse and irregular cognitive measurements, and (ii) adjusting mediation effects to handle the high-dimensional set of candidate mediators. In the MrOS Sleep study, our primary objective is to explore lipid pathways that may mediate the relationship between rest-activity rhythms and longitudinal cognitive decline in older men. Our findings suggest a potential mechanism involving rest-activity rhythms, lipid metabolites, and cognitive decline, and highlight significant mediators identified through multiple testing procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07953v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Ren, Haoyi Yang, Qian Xiao, Lingzhou Xue, Yuan Huang</dc:creator>
    </item>
    <item>
      <title>Optimal Fluctuations for Nonlinear Chemical Reaction Systems with General Rate Law</title>
      <link>https://arxiv.org/abs/2506.06974</link>
      <description>arXiv:2506.06974v1 Announce Type: cross 
Abstract: This paper investigates optimal fluctuations for chemical reaction systems with N species, M reactions, and general rate law. In the limit of large volume, large fluctuations for such models occur with overwhelming probability in the vicinity of the so-called optimal path, which is a basic consequence of the Freidlin-Wentzell theory, and is vital in biochemistry as it unveils the almost deterministic mechanism concealed behind rare noisy phenomena such as escapes from the attractive domain of a stable state and transitions between different metastable states. In this study, an alternative description for optimal fluctuations is proposed in both non-stationary and stationary settings by means of a quantity called prehistory probability in the same setting, respectively. The evolution law of each of them is derived, showing their relationship with the time reversal of a specified family of probability distributions respectively. The law of large numbers and the central limit theorem for the reversed processes are then proved. In doing so, the prehistorical approach to optimal fluctuations for Langevin dynamics is naturally generalized to the present case, thereby suggesting a strong connection between optimal fluctuations and the time reversal of the chemical reaction model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06974v1</guid>
      <category>math.PR</category>
      <category>physics.chem-ph</category>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feng Zhao, Jinjie Zhu, Yang Li, Xianbin Liu, Dongping Jin</dc:creator>
    </item>
    <item>
      <title>Uncovering the topology of an infinite-server queueing network from population data</title>
      <link>https://arxiv.org/abs/2506.07057</link>
      <description>arXiv:2506.07057v1 Announce Type: cross 
Abstract: This paper studies statistical inference in a network of infinite-server queues, with the aim of estimating the underlying parameters (routing matrix, arrival rates, parameters pertaining to the service times) using observations of the network population vector at Poisson time points. We propose a method-of-moments estimator and establish its consistency. The method relies on deriving the covariance structure of different nodes at different sampling epochs. Numerical experiments demonstrate that the method yields accurate estimates, even in settings with a large number of parameters. Two model variants are considered: one that assumes a known parametric form for the service-time distributions, and a model-free version that does not require such assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07057v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hritika Gupta, Michel Mandjes, Liron Ravner, Jiesen Wang</dc:creator>
    </item>
    <item>
      <title>GEEPERs: Principal Stratification using Principal Scores and Stacked Estimating Equations</title>
      <link>https://arxiv.org/abs/2212.10406</link>
      <description>arXiv:2212.10406v2 Announce Type: replace 
Abstract: Principal stratification is a framework for making sense of causal effects conditioned on variables that themselves may have been affected by treatment. For instance, one component of an educational computer application is the availability of ``bottom-out'' hints that provide the answer. In evaluating a recent experimental evaluation against alternative programs without bottom-out hints, researchers may be interested in estimating separate average treatment effects for students who, if given the opportunity, would request bottom-out hints frequently, and for students who would not. Most principal stratification estimators rely on strong structural or modeling assumptions, and many require advanced statistical training to fit and check. In this paper, we introduce a new M-estimation principal effect estimator for one-way noncompliance based on a binary indicator. Estimates may be computed using conventional regressions (though the standard errors require a specialized sandwich formula) and do not rely on distributional assumptions. We present a simulation study that demonstrates the novel method's greater robustness compared to popular alternatives and illustrate the method through two real-data analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.10406v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam C. Sales, Kirk P. Vanacore, Erin R. Ottmar</dc:creator>
    </item>
    <item>
      <title>Cox reduction and confidence sets of models: a theoretical elucidation</title>
      <link>https://arxiv.org/abs/2302.12627</link>
      <description>arXiv:2302.12627v3 Announce Type: replace 
Abstract: For sparse high-dimensional regression problems, Cox and Battey [1, 9] emphasised the need for confidence sets of models: an enumeration of those small sets of variables that fit the data equivalently well in a suitable statistical sense. This is to be contrasted with the single model returned by penalised regression procedures, effective for prediction but potentially misleading for subject-matter understanding. The proposed construction of such sets relied on preliminary reduction of the full set of variables, and while various possibilities could be considered for this, [9] proposed a succession of regression fits based on incomplete block designs. The purpose of the present paper is to provide insight on both aspects of that work. For an unspecified reduction strategy, we begin by characterising models that are likely to be retained in the model confidence set, emphasising geometric aspects. We then evaluate possible reduction schemes based on penalised regression or marginal screening, before theoretically elucidating the reduction of [9]. We identify features of the covariate matrix that may reduce its efficacy, and indicate improvements to the original proposal. An advantage of the approach is its ability to reveal its own stability or fragility for the data at hand.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.12627v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/24-STS934</arxiv:DOI>
      <arxiv:journal_reference>Statistical Science 40(2): 313-328 (2025)</arxiv:journal_reference>
      <dc:creator>R. M. Lewis, H. S. Battey</dc:creator>
    </item>
    <item>
      <title>Joint Coverage Regions: Simultaneous Confidence and Prediction Sets</title>
      <link>https://arxiv.org/abs/2303.00203</link>
      <description>arXiv:2303.00203v3 Announce Type: replace 
Abstract: We introduce Joint Coverage Regions (JCRs), which unify confidence intervals and prediction regions in frequentist statistics. Specifically, joint coverage regions aim to cover a pair formed by an unknown fixed parameter (such as the mean of a distribution), and an unobserved random datapoint (such as the outcomes associated to a new test datapoint). The first corresponds to a confidence component, while the second corresponds to a prediction part. In particular, our notion unifies classical statistical methods such as the Wald confidence interval with distribution-free prediction methods such as conformal prediction. We show how to construct finite-sample valid JCRs when a conditional pivot is available; under the same conditions where exact finite-sample confidence and prediction sets are known to exist. We further develop efficient JCR algorithms, including split-data versions by introducing adequate sets to reduce the cost of repeated computation. We illustrate the use of JCRs in statistical problems such as constructing efficient prediction sets when the parameter space is structured.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.00203v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edgar Dobriban, Zhanran Lin</dc:creator>
    </item>
    <item>
      <title>Debiased regression adjustment in completely randomized experiments with moderately high-dimensional covariates</title>
      <link>https://arxiv.org/abs/2309.02073</link>
      <description>arXiv:2309.02073v3 Announce Type: replace 
Abstract: Completely randomized experiment is the gold standard for causal inference. When the covariate information for each experimental candidate is available, one typical way is to include them in covariate adjustments for more accurate treatment effect estimation. In this paper, we investigate this problem under the randomization-based framework, i.e., that the covariates and potential outcomes of all experimental candidates are assumed as deterministic quantities and the randomness comes solely from the treatment assignment mechanism. Under this framework, to achieve asymptotically valid inference, existing estimators usually require either (i) that the dimension of covariates $p$ is much smaller than the sample size $n$; or (ii) certain sparsity constraints on the linear representations of potential outcomes constructed via possibly high-dimensional covariates. In this paper, we consider the moderately high-dimensional regime where $p$ is allowed to be in the same order of magnitude as $n$. We develop a novel debiased estimator with a corresponding inference procedure and establish its asymptotic normality under mild assumptions. Our estimator is model-free and does not require any sparsity constraint on potential outcome's linear representations. We also discuss its asymptotic efficiency improvements over the unadjusted treatment effect estimator under different dimensionality constraints. Numerical analysis confirms that compared to other regression adjustment based treatment effect estimators, our debiased estimator performs well in moderately high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.02073v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Lu, Fan Yang, Yuhao Wang</dc:creator>
    </item>
    <item>
      <title>Targeting relative risk heterogeneity with causal forests</title>
      <link>https://arxiv.org/abs/2309.15793</link>
      <description>arXiv:2309.15793v3 Announce Type: replace 
Abstract: The identification of heterogeneous treatment effects (HTE) across subgroups is of significant interest in clinical trial analysis. Several state-of-the-art HTE estimation methods, including causal forests, apply recursive partitioning for non-parametric identification of relevant covariates and interactions. However, the partitioning criterion is typically based on differences in absolute risk. This can dilute statistical power by masking variation in the relative risk, which is often a more appropriate quantity of clinical interest. In this work, we propose and implement a methodology for modifying causal forests to target relative risk, using a novel node-splitting procedure based on exhaustive generalized linear model comparison. We present results from simulated data that suggest relative risk causal forests can capture otherwise undetected sources of heterogeneity. We implement our method on real-world trial data to explore HTEs for liraglutide in patients with type 2 diabetes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15793v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vik Shirvaikar, Andrea Stor{\aa}s, Xi Lin, Chris Holmes</dc:creator>
    </item>
    <item>
      <title>An alternative measure for quantifying the heterogeneity in meta-analysis</title>
      <link>https://arxiv.org/abs/2403.16706</link>
      <description>arXiv:2403.16706v2 Announce Type: replace 
Abstract: Quantifying the heterogeneity is an important issue in meta-analysis, and among the existing measures, the $I^2$ statistic is most commonly used. In this paper, we first illustrate with a simple example that the $I^2$ statistic is heavily dependent on the study sample sizes, mainly because it is used to quantify the heterogeneity between the observed effect sizes. To reduce the influence of sample sizes, we introduce an alternative measure that aims to directly measure the heterogeneity between the study populations involved in the meta-analysis. We further propose a new estimator, namely the $I_A^2$ statistic, to estimate the newly defined measure of heterogeneity. For practical implementation, the exact formulas of the $I_A^2$ statistic are also derived under two common scenarios with the effect size as the mean difference (MD) or the standardized mean difference (SMD). Simulations and real data analysis demonstrate that the $I_A^2$ statistic provides an asymptotically unbiased estimator for the absolute heterogeneity between the study populations, and it is also independent of the study sample sizes as expected. To conclude, our newly defined $I_A^2$ statistic can be used as a supplemental measure of heterogeneity to monitor the situations where the study effect sizes are indeed similar with little biological difference. In such scenario, the fixed-effect model can be appropriate; nevertheless, when the sample sizes are sufficiently large, the $I^2$ statistic may still increase to 1 and subsequently suggest the random-effects model for meta-analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16706v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/sim.70089</arxiv:DOI>
      <arxiv:journal_reference>Statistics in Medicine, 44:e70089 (2025)</arxiv:journal_reference>
      <dc:creator>Ke Yang, Enxuan Lin, Wangli Xu, Liping Zhu, Tiejun Tong</dc:creator>
    </item>
    <item>
      <title>The inverse Kalman filter</title>
      <link>https://arxiv.org/abs/2407.10089</link>
      <description>arXiv:2407.10089v4 Announce Type: replace 
Abstract: We introduce the inverse Kalman filter, which enables exact matrix-vector multiplication between a covariance matrix from a dynamic linear model and any real-valued vector with linear computational cost. We integrate the inverse Kalman filter with the conjugate gradient algorithm, which substantially accelerates the computation of matrix inversion for a general form of covariance matrix, where other approximation approaches may not be directly applicable. We demonstrate the scalability and efficiency of the proposed approach through applications in nonparametric estimation of particle interaction functions, using both simulations and cell trajectories from microscopy data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10089v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Fang, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>Quasi-Bayes empirical Bayes: a sequential approach to the Poisson compound decision problem</title>
      <link>https://arxiv.org/abs/2411.07651</link>
      <description>arXiv:2411.07651v2 Announce Type: replace 
Abstract: The Poisson compound decision problem is a long-standing problem in statistics, where empirical Bayes methodologies are commonly used to estimate Poisson's means in static or batch domains. In this paper, we study the Poisson compound decision problem in a streaming or online domain. Adopting a quasi-Bayesian approach, referred to as Newton's algorithm, we obtain a sequential estimate that is easy to evaluate, computationally efficient, and maintain a constant per-observation computational cost as data accumulate. Asymptotic frequentist guarantees of this estimate are established, showing consistency and asymptotic optimality, where the latter is understood as vanishing excess Bayes risk or regret. We demonstrate the effectiveness of our methodology through empirical analysis on synthetic and real data, with comparisons to existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07651v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefano Favaro, Sandra Fortini</dc:creator>
    </item>
    <item>
      <title>Automatic Doubly Robust Forests</title>
      <link>https://arxiv.org/abs/2412.07184</link>
      <description>arXiv:2412.07184v2 Announce Type: replace 
Abstract: This paper proposes the automatic Doubly Robust Random Forest (DRRF) algorithm for estimating the conditional expectation of a moment functional in the presence of high-dimensional nuisance functions. DRRF extends the automatic debiasing framework based on the Riesz representer to the conditional setting and enables nonparametric, forest-based estimation (Athey et al., 2019; Oprescu et al., 2019). In contrast to existing methods, DRRF does not require prior knowledge of the form of the debiasing term or impose restrictive parametric or semi-parametric assumptions on the target quantity. Additionally, it is computationally efficient in making predictions at multiple query points. We establish consistency and asymptotic normality results for the DRRF estimator under general assumptions, allowing for the construction of valid confidence intervals. Through extensive simulations in heterogeneous treatment effect (HTE) estimation, we demonstrate the superior performance of DRRF over benchmark approaches in terms of estimation accuracy, robustness, and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07184v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaomeng Chen, Junting Duan, Victor Chernozhukov, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>A Randomization-Based Method for Evaluating Time-Varying Treatment Effects</title>
      <link>https://arxiv.org/abs/2412.09697</link>
      <description>arXiv:2412.09697v3 Announce Type: replace 
Abstract: Tests for paired censored outcomes have been extensively studied, with some justified in the context of randomization-based inference. These tests are primarily designed to detect an overall treatment effect across the entire follow-up period, providing limited insight into when the effect manifests and how it changes over time. In this article, we introduce new randomization-based tests for paired censored outcomes that enable both time-specific and long-term analysis of a treatment effect. The tests utilize time-specific scores, quantifying each individual's impact on sample survival at a fixed time, obtained via pseudo-observations. Moreover, we develop corresponding sensitivity analysis methods to address potential unmeasured confounding in observational studies where randomization often lacks support. To illustrate how our methods can provide a fuller analysis of a time-varying treatment effect, we apply them to a matched cohort study using data from the Korean Longitudinal Study of Aging (KLoSA), focusing on the effect of social engagement on survival.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09697v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sangjin Lee, Kwonsang Lee</dc:creator>
    </item>
    <item>
      <title>Forests for Differences: Robust Causal Inference Beyond Parametric DiD</title>
      <link>https://arxiv.org/abs/2505.09706</link>
      <description>arXiv:2505.09706v2 Announce Type: replace 
Abstract: This paper introduces the Difference-in-Differences Bayesian Causal Forest (DiD-BCF), a novel non-parametric model addressing key challenges in DiD estimation, such as staggered adoption and heterogeneous treatment effects. DiD-BCF provides a unified framework for estimating Average (ATE), Group-Average (GATE), and Conditional Average Treatment Effects (CATE). A core innovation, its Parallel Trends Assumption (PTA)-based reparameterization, enhances estimation accuracy and stability in complex panel data settings. Extensive simulations demonstrate DiD-BCF's superior performance over established benchmarks, particularly under non-linearity, selection biases, and effect heterogeneity. Applied to U.S. minimum wage policy, the model uncovers significant conditional treatment effect heterogeneity related to county population, insights obscured by traditional methods. DiD-BCF offers a robust and versatile tool for more nuanced causal inference in modern DiD applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09706v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hugo Gobato Souto, Francisco Louzada Neto</dc:creator>
    </item>
    <item>
      <title>The Fundamental Limits of Structure-Agnostic Functional Estimation</title>
      <link>https://arxiv.org/abs/2305.04116</link>
      <description>arXiv:2305.04116v2 Announce Type: replace-cross 
Abstract: Many recent developments in causal inference, and functional estimation problems more generally, have been motivated by the fact that classical one-step (first-order) debiasing methods, or their more recent sample-split double machine-learning avatars, can outperform plugin estimators under surprisingly weak conditions. These first-order corrections improve on plugin estimators in a black-box fashion, and consequently are often used in conjunction with powerful off-the-shelf estimation methods. These first-order methods are however provably suboptimal in a minimax sense for functional estimation when the nuisance functions live in Holder-type function spaces. This suboptimality of first-order debiasing has motivated the development of "higher-order" debiasing methods. The resulting estimators are, in some cases, provably optimal over Holder-type spaces, but both the estimators which are minimax-optimal and their analyses are crucially tied to properties of the underlying function space.
  In this paper we investigate the fundamental limits of structure-agnostic functional estimation, where relatively weak conditions are placed on the underlying nuisance functions. We show that there is a strong sense in which existing first-order methods are optimal. We achieve this goal by providing a formalization of the problem of functional estimation with black-box nuisance function estimates, and deriving minimax lower bounds for this problem. Our results highlight some clear tradeoffs in functional estimation -- if we wish to remain agnostic to the underlying nuisance function spaces, impose only high-level rate conditions, and maintain compatibility with black-box nuisance estimators then first-order methods are optimal. When we have an understanding of the structure of the underlying nuisance functions then carefully constructed higher-order estimators can outperform first-order estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04116v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sivaraman Balakrishnan, Edward H. Kennedy, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation</title>
      <link>https://arxiv.org/abs/2402.14264</link>
      <description>arXiv:2402.14264v4 Announce Type: replace-cross 
Abstract: Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines. While many estimation strategies have been proposed in the literature, the statistical optimality of these methods has still remained an open area of investigation, especially in regimes where these methods do not achieve parametric rates. In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to black-box estimators that achieve some statistical estimation rate. This framework is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as black-box sub-processes. Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATT), as well as weighted variants of the former, which arise in policy evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14264v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jikai Jin, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>Scalable Inference for Bayesian Multinomial Logistic-Normal Dynamic Linear Models</title>
      <link>https://arxiv.org/abs/2410.05548</link>
      <description>arXiv:2410.05548v2 Announce Type: replace-cross 
Abstract: Many scientific fields collect longitudinal count compositional data. Each observation is a multivariate count vector, where the total counts are arbitrary, and the information lies in the relative frequency of the counts. Multiple authors have proposed Bayesian Multinomial Logistic-Normal Dynamic Linear Models (MLN-DLMs) as a flexible approach to modeling these data. However, adoption of these methods has been limited by computational challenges. This article develops an efficient and accurate approach to posterior state estimation, called $\textit{Fenrir}$. Our approach relies on a novel algorithm for MAP estimation and an accurate approximation to a key posterior marginal of the model. As there are no equivalent methods against which we can compare, we also develop an optimized Stan implementation of MLN-DLMs. Our experiments suggest that Fenrir can be three orders of magnitude more efficient than Stan and can even be incorporated into larger sampling schemes for joint inference of model hyperparameters. Our methods are made available to the community as a user-friendly software library written in C++ with an R interface.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05548v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manan Saxena, Tinghua Chen, Justin D. Silverman</dc:creator>
    </item>
    <item>
      <title>Prediction-Powered Adaptive Shrinkage Estimation</title>
      <link>https://arxiv.org/abs/2502.14166</link>
      <description>arXiv:2502.14166v2 Announce Type: replace-cross 
Abstract: Prediction-Powered Inference (PPI) is a powerful framework for enhancing statistical estimates by combining limited gold-standard data with machine learning (ML) predictions. While prior work has demonstrated PPI's benefits for individual statistical problems, modern applications require answering numerous parallel statistical questions. We introduce Prediction-Powered Adaptive Shrinkage (PAS), a method that bridges PPI with empirical Bayes shrinkage to improve the estimation of multiple means. PAS debiases noisy ML predictions within each task and then borrows strength across tasks by using those same predictions as a reference point for shrinkage. The amount of shrinkage is determined by minimizing an unbiased estimate of risk, and we prove that this tuning strategy is asymptotically optimal. Experiments on both synthetic and real-world datasets show that PAS adapts to the reliability of the ML predictions and outperforms traditional and modern baselines in large-scale applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14166v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sida Li, Nikolaos Ignatiadis</dc:creator>
    </item>
    <item>
      <title>Hierarchical models for small area estimation using zero-inflated forest inventory variables: comparison and implementation</title>
      <link>https://arxiv.org/abs/2503.22103</link>
      <description>arXiv:2503.22103v2 Announce Type: replace-cross 
Abstract: National Forest Inventory (NFI) data are typically limited to sparse networks of sample locations due to cost constraints. While traditional design-based estimators provide reliable forest parameter estimates for large areas, there is increasing interest in model-based small area estimation (SAE) methods to improve precision for smaller spatial, temporal, or biophysical domains. SAE methods can be broadly categorized into area- and unit-level models, with unit-level models offering greater flexibility -- making them the focus of this study. Ensuring valid inference requires satisfying model distributional assumptions, which is particularly challenging for NFI variables that exhibit positive support and zero inflation, such as forest biomass, carbon, and volume. Here, we evaluate a class of two-stage unit-level hierarchical Bayesian models for estimating forest biomass at the county-level in Washington and Nevada, United States. We compare these models to simpler Bayesian single-stage and two-stage frequentist approaches. To assess estimator performance, we employ simulated populations and cross-validation techniques. Results indicate that small area estimators that incorporate a two-stage approach to account for zero inflation, county-specific random intercepts and residual variances, and spatial random effects provide the most reliable county-level estimates. We illustrate the usefulness of simulated populations and cross-validation for assessing qualities of the various estimators considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22103v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grayson W. White, Andrew O. Finley, Josh K. Yamamoto, Jennifer L. Green, Tracey S. Frescino, David. W. MacFarlane, Hans-Erik Andersen</dc:creator>
    </item>
    <item>
      <title>A Framework of decision-relevant observability: Reinforcement Learning converges under relative ignorability</title>
      <link>https://arxiv.org/abs/2504.07722</link>
      <description>arXiv:2504.07722v4 Announce Type: replace-cross 
Abstract: From clinical dosing algorithms to autonomous robots, sequential decision-making systems routinely operate with missing or incomplete data. Classical reinforcement learning theory, which is commonly used to solve sequential decision problems, assumes Markovian observability, which may not hold under partial observability. Causal inference paradigms formalise ignorability of missingness. We show these views can be unified and generalized in order to guarantee Q-learning convergence even when the Markov property fails. To do so, we introduce the concept of \emph{relative ignorability}. Relative ignorability is a graphical-causal criterion which refines the requirements for accurate decision-making based on incomplete data. Theoretical results and simulations both reveal that non-markovian stochastic processes whose missingness is relatively ignorable with respect to causal estimands can still be optimized using standard Reinforcement Learning algorithms. These results expand the theoretical foundations of safe, data-efficient AI to real-world environments where complete information is unattainable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07722v4</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>MaryLena Bleile</dc:creator>
    </item>
    <item>
      <title>Learning from Double Positive and Unlabeled Data for Potential-Customer Identification</title>
      <link>https://arxiv.org/abs/2506.00436</link>
      <description>arXiv:2506.00436v2 Announce Type: replace-cross 
Abstract: In this study, we propose a method for identifying potential customers in targeted marketing by applying learning from positive and unlabeled data (PU learning). We consider a scenario in which a company sells a product and can observe only the customers who purchased it. Decision-makers seek to market products effectively based on whether people have loyalty to the company. Individuals with loyalty are those who are likely to remain interested in the company even without additional advertising. Consequently, those loyal customers would likely purchase from the company if they are interested in the product. In contrast, people with lower loyalty may overlook the product or buy similar products from other companies unless they receive marketing attention. Therefore, by focusing marketing efforts on individuals who are interested in the product but do not have strong loyalty, we can achieve more efficient marketing. To achieve this goal, we consider how to learn, from limited data, a classifier that identifies potential customers who (i) have interest in the product and (ii) do not have loyalty to the company. Although our algorithm comprises a single-stage optimization, its objective function implicitly contains two losses derived from standard PU learning settings. For this reason, we refer to our approach as double PU learning. We verify the validity of the proposed algorithm through numerical experiments, confirming that it functions appropriately for the problem at hand.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00436v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato, Yuki Ikeda, Kentaro Baba, Takashi Imai, Ryo Inokuchi</dc:creator>
    </item>
  </channel>
</rss>
