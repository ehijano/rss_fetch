<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 23 Oct 2025 01:42:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Hey Pentti, We Did It!: A Fully Vector-Symbolic Lisp</title>
      <link>https://arxiv.org/abs/2510.17889</link>
      <description>arXiv:2510.17889v1 Announce Type: new 
Abstract: Kanerva (2014) suggested that it would be possible to construct a complete Lisp out of a vector-symbolic architecture. We present the general form of a vector-symbolic representation of the five Lisp elementary functions, lambda expressions, and other auxiliary functions, found in the Lisp 1.5 specification McCarthy (1960), which is near minimal and sufficient for Turing-completeness. Our specific implementation uses holographic reduced representations Plate (1995), with a lookup table cleanup memory. Lisp, as all Turing-complete languages, is a Cartesian closed category, unusual in its proximity to the mathematical abstraction. We discuss the mathematics, the purpose, and the significance of demonstrating vector-symbolic architectures' Cartesian-closure, as well as the importance of explicitly including cleanup memories in the specification of the architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17889v1</guid>
      <category>cs.PL</category>
      <category>cs.AI</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Tomkins Flanagan, E., &amp; Kelly, M. A. (2024, July). Hey Pentti, We Did It!: A Fully Vector-Symbolic Lisp. Abstract published at MathPsych / ICCM 2024. Via mathpsych.org/presentation/1541</arxiv:journal_reference>
      <dc:creator>Eilene Tomkins-Flanagan (Department of Cognitive Science, Carleton University), Mary A. Kelly (Department of Cognitive Science, Carleton University)</dc:creator>
    </item>
    <item>
      <title>ZipLex: Verified Invertible Lexing with Memoized Derivatives and Zippers</title>
      <link>https://arxiv.org/abs/2510.18479</link>
      <description>arXiv:2510.18479v1 Announce Type: new 
Abstract: We present ZipLex, a verified framework for invertible lexical analysis. Unlike past verified lexers that focus only on satisfying the semantics of regular expressions and the maximal munch property, ZipLex also guarantees that lexing and printing are mutual inverses. Our design relies on two sets of ideas: (1) a new abstraction of token sequences that captures the separability of tokens in a sequence while supporting their efficient manipulation, and (2) a combination of verified data structures and optimizations, including Huet's zippers and memoized derivatives, to achieve practical performance. We implemented ZipLex in Scala and verified its correctness, including invertibility, using the Stainless verifier. Our evaluation demonstrates that ZipLex supports realistic applications such as JSON processing and lexers of programming languages. In comparison to other verified lexers (which do not enforce invertibility), ZipLex is 4x slower than Coqlex and two orders of magnitude faster than Verbatim++, showing that verified invertibility can be achieved without prohibitive cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18479v1</guid>
      <category>cs.PL</category>
      <category>cs.FL</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Samuel Chassot, Viktor Kun\v{c}ak</dc:creator>
    </item>
    <item>
      <title>CPSLint: A Domain-Specific Language Providing Data Validation and Sanitisation for Industrial Cyber-Physical Systems</title>
      <link>https://arxiv.org/abs/2510.18651</link>
      <description>arXiv:2510.18651v1 Announce Type: new 
Abstract: Raw datasets are often too large and unstructured to work with directly, and require a data preparation process. The domain of industrial Cyber-Physical Systems (CPS) is no exception, as raw data typically consists of large amounts of time-series data logging the system's status in regular time intervals. Such data has to be sanity checked and preprocessed to be consumable by data-centric workflows. We introduce CPSLint, a Domain-Specific Language designed to provide data preparation for industrial CPS. We build up on the fact that many raw data collections in the CPS domain require similar actions to render them suitable for Machine-Learning (ML) solutions, e.g., Fault Detection and Identification (FDI) workflows, yet still vary enough to hope for one universally applicable solution.
  CPSLint's main features include type checking and enforcing constraints through validation and remediation for data columns, such as imputing missing data from surrounding rows. More advanced features cover inference of extra CPS-specific data structures, both column-wise and row-wise. For instance, as row-wise structures, descriptive execution phases are an effective method of data compartmentalisation are extracted and prepared for ML-assisted FDI workflows. We demonstrate CPSLint's features through a proof of concept implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18651v1</guid>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Uraz Odyurt, \"Omer Sayilir, Mari\"elle Stoelinga, Vadim Zaytsev</dc:creator>
    </item>
    <item>
      <title>A Lazy, Concurrent Convertibility Checker</title>
      <link>https://arxiv.org/abs/2510.18418</link>
      <description>arXiv:2510.18418v1 Announce Type: cross 
Abstract: Convertibility checking - determining whether two lambda-terms are equal up to reductions - is a crucial component of proof assistants and dependently-typed languages. Practical implementations often use heuristics to quickly conclude that two terms are or are not convertible without reducing them to normal form. However, these heuristics can backfire, triggering huge amounts of unnecessary computation. This paper presents a novel convertibility-checking algorithm that relies crucially on laziness and concurrency} Laziness is used to share computations, while concurrency is used to explore multiple convertibility subproblems in parallel or via fair interleaving. Unlike heuristics-based approaches, our algorithm always finds an easy solution to the convertibility problem, if one exists. The paper presents the algorithm in process calculus style and discusses its mechanized proof of partial correctness, its complexity, and its lightweight experimental evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18418v1</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathana\"elle Courant (CAMBIUM), Xavier Leroy (CAMBIUM)</dc:creator>
    </item>
    <item>
      <title>LatticeHashForest: An Efficient Data Structure for Repetitive Data and Operations</title>
      <link>https://arxiv.org/abs/2510.18496</link>
      <description>arXiv:2510.18496v1 Announce Type: cross 
Abstract: Analysis of entire programs as a single unit, or whole-program analysis, involves propagation of large amounts of information through the control flow of the program. This is especially true for pointer analysis, where, unless significant compromises are made in the precision of the analysis, there is a combinatorial blowup of information. One of the key problems we observed in our own efforts is that a lot of duplicate data was being propagated, and many low-level data structure operations were repeated a large number of times.
  We present what we consider to be a novel and generic data structure, LatticeHashForest (LHF), to store and operate on such information in a manner that eliminates a majority of redundant computations and duplicate data in scenarios similar to those encountered in compilers and program optimization. LHF differs from similar work in this vein, such as hash-consing, ZDDs, and BDDs, by not only providing a way to efficiently operate on large, aggregate structures, but also modifying the elements of such structures in a manner that they can be deduplicated immediately. LHF also provides a way to perform a nested construction of elements such that they can be deduplicated at multiple levels, cutting down the need for additional, nested computations.
  We provide a detailed structural description, along with an abstract model of this data structure. An entire C++ implementation of LHF is provided as an artifact along with evaluations of LHF using examples and benchmark programs. We also supply API documentation and a user manual for users to make independent applications of LHF. Our main use case in the realm of pointer analysis shows memory usage reduction to an almost negligible fraction, and speedups beyond 4x for input sizes approaching 10 million when compared to other implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18496v1</guid>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>cs.OS</category>
      <category>cs.PL</category>
      <category>math.IT</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anamitra Ghorui, Uday P. Khedker</dc:creator>
    </item>
    <item>
      <title>Big-Stop Semantics: Small-Step Semantics in a Big-Step Judgment</title>
      <link>https://arxiv.org/abs/2508.15157</link>
      <description>arXiv:2508.15157v3 Announce Type: replace 
Abstract: As is evident in the programming language literature, many practitioners favor specifying dynamic program behavior using big-step over small-step semantics. Unlike small-step semantics, which must dwell on every intermediate program state, big-step semantics conveniently jumps directly to the ever-important result of the computation. Big-step semantics also typically involves fewer inference rules than their small-step counterparts. However, in exchange for ergonomics, big-step semantics gives up power: Small-step semantics describes program behaviors that are outside the grasp of big-step semantics, notably divergence.
  This work presents a little-known extension of big-step semantics with inductive definitions that captures diverging computations without introducing error states. This big-stop semantics is illustrated for typed, untyped, and effectful variants of PCF, as well as a while-loop-based imperative language. Big-stop semantics extends the standard big-step inference rules with a few additional rules to define an evaluation judgment that is equivalent to the reflexive-transitive closure of small-step transitions. This simple extension contrasts with other solutions in the literature that sacrifice ergonomics by introducing many additional inference rules, global state, and/or less-commonly-understood reasoning principles like coinduction. The ergonomics of big-stop semantics is exemplified via concise Agda proofs for some key results and compilation theorems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15157v3</guid>
      <category>cs.PL</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David M Kahn, Jan Hoffmann, Runming Li</dc:creator>
    </item>
    <item>
      <title>Monoidal closure of Grothendieck constructions via $\Sigma$-tractable monoidal structures and Dialectica formulas</title>
      <link>https://arxiv.org/abs/2405.07724</link>
      <description>arXiv:2405.07724v3 Announce Type: replace-cross 
Abstract: We examine the categorical structure of the Grothendieck construction $\Sigma_{\mathsf{C}}\mathsf{L}$ of an indexed category $\mathsf{L} \colon \mathsf{C}^{op} \to \mathsf{CAT}$. Our analysis begins with characterisations of fibred limits, colimits, and monoidal (closed) structures. The study of fibred colimits leads naturally to a generalisation of the notion of extensive indexed category introduced in CHAD for Expressive Total Languages, and gives rise to the concept of Left Kan extensivity, which provides a uniform framework for computing colimits in Grothendieck constructions.
  We then establish sufficient conditions for the (non-fibred) monoidal closure of the total category $\Sigma_{\mathsf{C}}\mathsf{L}$. This extends G\"odel's Dialectica interpretation and rests upon a new notion of $\Sigma$-tractable monoidal structure. Under this notion, $\Sigma$-tractable coproducts unify and extend cocartesian coclosed structures, biproducts, and extensive coproducts. Finally, we consider when the induced closed structure is fibred, showing that this need not hold in general, even in the presence of a fibred monoidal structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07724v3</guid>
      <category>math.CT</category>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fernando Lucatelli Nunes, Matthijs V\'ak\'ar</dc:creator>
    </item>
    <item>
      <title>Evaluating Program Semantics Reasoning with Type Inference in System F</title>
      <link>https://arxiv.org/abs/2509.23686</link>
      <description>arXiv:2509.23686v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly integrated into the software engineering ecosystem. Their test-time compute (TTC) reasoning capabilities show significant potential for understanding program logic and semantics beyond mere token recognition. However, current benchmarks for code reasoning lack a formal, program-centric deductive framework to ensure sound evaluation, and are incapable of assessing whether models genuinely reason about program semantics or merely exploit superficial associations between natural language and code tokens. To bridge this gap, we introduce TF-Bench, a benchmark designed to evaluate LLM reasoning based on type inference in System F, a task we refer to as program semantics reasoning. By employing verified transformations to remove semantically irrelevant natural language, we construct TF-Bench_pure, a purely semantics-driven variant of TF-Bench. Our analysis reveals substantial limitations in state-of-the-art LLMs, with the best-performing LLM (Claude-3.7-sonnet) achieving only 55.85% accuracy on TF-Bench_pure. Additionally, we propose two novel metrics to assess robustness and the effectiveness of test-time reasoning, underscoring critical limitations in current LLM capabilities and highlighting essential directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23686v2</guid>
      <category>cs.CL</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifeng He, Luning Yang, Christopher Castro Gaw Gonzalo, Hao Chen</dc:creator>
    </item>
  </channel>
</rss>
