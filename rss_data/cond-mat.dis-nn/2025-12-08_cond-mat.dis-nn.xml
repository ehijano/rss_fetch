<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 08 Dec 2025 05:05:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Training Multi-Layer Binary Neural Networks With Local Binary Error Signals</title>
      <link>https://arxiv.org/abs/2412.00119</link>
      <description>arXiv:2412.00119v4 Announce Type: cross 
Abstract: Binary Neural Networks (BNNs) significantly reduce computational complexity and memory usage in machine and deep learning by representing weights and activations with just one bit. However, most existing training algorithms for BNNs rely on quantization-aware floating-point Stochastic Gradient Descent (SGD), limiting the full exploitation of binary operations to the inference phase only. In this work, we propose, for the first time, a fully binary and gradient-free training algorithm for multi-layer BNNs, eliminating the need for back-propagated floating-point gradients. Specifically, the proposed algorithm relies on local binary error signals and binary weight updates, employing integer-valued hidden weights that serve as a synaptic metaplasticity mechanism, thereby enhancing its neurobiological plausibility. Our proposed solution enables the training of binary multi-layer perceptrons by using exclusively XNOR, Popcount, and increment/decrement operations. Experimental results on multi-class classification benchmarks show test accuracy improvements of up to +35.47% over the only existing fully binary single-layer state-of-the-art solution. Compared to full-precision SGD, our solution improves test accuracy by up to +35.30% under the same total memory demand, while also reducing computational cost by two to three orders of magnitude in terms of the total number of Boolean gates. The proposed algorithm is made available to the scientific community as a public repository.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00119v4</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.CV</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1088/2632-2153/adf0c1</arxiv:DOI>
      <arxiv:journal_reference>Machine Learning: Science and Technology, 2025, 6(3): 035015</arxiv:journal_reference>
      <dc:creator>Luca Colombo, Fabrizio Pittorino, Manuel Roveri</dc:creator>
    </item>
    <item>
      <title>BEP: A Binary Error Propagation Algorithm for Binary Neural Networks Training</title>
      <link>https://arxiv.org/abs/2512.04189</link>
      <description>arXiv:2512.04189v1 Announce Type: cross 
Abstract: Binary Neural Networks (BNNs), which constrain both weights and activations to binary values, offer substantial reductions in computational complexity, memory footprint, and energy consumption. These advantages make them particularly well suited for deployment on resource-constrained devices. However, training BNNs via gradient-based optimization remains challenging due to the discrete nature of their variables. The dominant approach, quantization-aware training, circumvents this issue by employing surrogate gradients. Yet, this method requires maintaining latent full-precision parameters and performing the backward pass with floating-point arithmetic, thereby forfeiting the efficiency of binary operations during training. While alternative approaches based on local learning rules exist, they are unsuitable for global credit assignment and for back-propagating errors in multi-layer architectures. This paper introduces Binary Error Propagation (BEP), the first learning algorithm to establish a principled, discrete analog of the backpropagation chain rule. This mechanism enables error signals, represented as binary vectors, to be propagated backward through multiple layers of a neural network. BEP operates entirely on binary variables, with all forward and backward computations performed using only bitwise operations. Crucially, this makes BEP the first solution to enable end-to-end binary training for recurrent neural network architectures. We validate the effectiveness of BEP on both multi-layer perceptrons and recurrent neural networks, demonstrating gains of up to +6.89% and +10.57% in test accuracy, respectively. The proposed algorithm is released as an open-source repository.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04189v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Colombo, Fabrizio Pittorino, Daniele Zambon, Carlo Baldassi, Manuel Roveri, Cesare Alippi</dc:creator>
    </item>
    <item>
      <title>Competition, stability, and functionality in excitatory-inhibitory neural circuits</title>
      <link>https://arxiv.org/abs/2512.05252</link>
      <description>arXiv:2512.05252v1 Announce Type: cross 
Abstract: Energy-based models have become a central paradigm for understanding computation and stability in both theoretical neuroscience and machine learning. However, the energetic framework typically relies on symmetry in synaptic or weight matrices - a constraint that excludes biologically realistic systems such as excitatory-inhibitory (E-I) networks. When symmetry is relaxed, the classical notion of a global energy landscape fails, leaving the dynamics of asymmetric neural systems conceptually unanchored. In this work, we extend the energetic framework to asymmetric firing rate networks, revealing an underlying game-theoretic structure for the neural dynamics in which each neuron is an agent that seeks to minimize its own energy. In addition, we exploit rigorous stability principles from network theory to study regulation and balancing of neural activity in E-I networks. We combine the novel game-energetic interpretation and the stability results to revisit standard frameworks in theoretical neuroscience, such as the Wilson-Cowan and lateral inhibition models. These insights allow us to study cortical columns of lateral inhibition microcircuits as contrast enhancer - with the ability to selectively sharpen subtle differences in the environment through hierarchical excitation-inhibition interplay. Our results bridge energetic and game-theoretic views of neural computation, offering a pathway toward the systematic engineering of biologically grounded, dynamically stable neural architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05252v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>math.OC</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simone Betteti, William Retnaraj, Alexander Davydov, Jorge Cort\'es, Francesco Bullo</dc:creator>
    </item>
    <item>
      <title>Equation of state for the hard sphere fluids</title>
      <link>https://arxiv.org/abs/2512.05488</link>
      <description>arXiv:2512.05488v1 Announce Type: cross 
Abstract: Based on the survey of the literatures on the new improvements on the equation of state (EOS) for the hard sphere fluids, we here compare lots of different EOSs and present a very accurate equation of state for this kind of fluids. The new equation is built up on the basis of (1) the best estimated virial coefficients B5-B11 by Tian et al. [ Phys. Chem. Chem. Phys., 2019, 21, 13070] and (2) the newest numerical simulation data of the compressibility factor versus the density by Pieprzyk et al. [Phys. Chem. Chem. Phys., 2019, 21, 6886]. Our results show that this equation is accurate in not only the stable density range but also the metastable density range with the proper closest packing fraction pole, and well derives the predictive values of the high order virial coefficients B13-B16.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05488v1</guid>
      <category>cond-mat.soft</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Can Cui, Jianxiang Tian</dc:creator>
    </item>
    <item>
      <title>Yielding and memory in a driven mean-field model of glasses</title>
      <link>https://arxiv.org/abs/2505.19900</link>
      <description>arXiv:2505.19900v2 Announce Type: replace 
Abstract: Glassy systems reveal a wide variety of generic behaviors, which lack a unified theoretical description. Here, we study a mean-field model, recently shown to reproduce the universal non-phononic vibrational spectra of glasses, under oscillatory driving forces. The driven mean-field model, featuring a disordered Hamiltonian structure, naturally predicts the salient dynamical phenomena in cyclically deformed glasses. Specifically, it features an oscillatory yielding transition, characterized by an absorbing-to-diffusive transition in the system's microscopic trajectories and large-scale hysteresis. The model also reveals dynamic slowing-down from both sides of the transition, as well as mechanical and thermal annealing effects that mirror their glass counterparts. Finally, we demonstrate a non-equilibrium ensemble equivalence between the driven post-yielding dynamics at fixed quenched disorder and quenched disorder averages of the non-driven system, along with memory formation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19900v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cond-mat.soft</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Makoto Suda, Edan Lerner, Eran Bouchbinder</dc:creator>
    </item>
    <item>
      <title>Competing structures in a minimal double-well potential model of condensed matter</title>
      <link>https://arxiv.org/abs/2401.17901</link>
      <description>arXiv:2401.17901v4 Announce Type: replace-cross 
Abstract: The microscopic structure of several amorphous substances often reveals complex patterns such as medium- or long-range order, spatial heterogeneity, and even local polycrystallinity. To capture all these features, models usually incorporate a refined description of the particle interaction that includes an ad hoc design of the inside of the system constituents, and use temperature as a control parameter. We show that all these features can emerge from a minimal athermal two-dimensional model where particles interact isotropically by a double-well potential, which includes an excluded volume and a maximum coordination number. The rich variety of structural patterns shown by this simple geometrical model apply to a wide range of real systems including water, silicon, and different amorphous materials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17901v4</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1063/5.0286950</arxiv:DOI>
      <arxiv:journal_reference>Chaos 35, 123117 (2025)</arxiv:journal_reference>
      <dc:creator>Julyan H. E. Cartwright, Bruno Escribano, S\'andalo Rold\'an-Vargas, C. Ignacio Sainz-D\'iaz</dc:creator>
    </item>
    <item>
      <title>Why is topology hard to learn?</title>
      <link>https://arxiv.org/abs/2509.26261</link>
      <description>arXiv:2509.26261v2 Announce Type: replace-cross 
Abstract: Much attention has been devoted to the use of machine learning to approximate physical concepts. Yet, due to challenges in interpretability of machine learning techniques, the question of what physics machine learning models are able to learn remains open. Here we bridge the concept a physical quantity and its machine learning approximation in the context of the original application of neural networks in physics: topological phase classification. We construct a hybrid tensor-neural network object that exactly expresses real space topological invariant and rigorously assess its trainability and generalization. Specifically, we benchmark the accuracy and trainability of a tensor-neural network to multiple types of neural networks, thus exemplifying the differences in trainability and representational power. Our work highlights the challenges in learning topological invariants and constitutes a stepping stone towards more accurate and better generalizable machine learning representations in condensed matter physics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26261v2</guid>
      <category>cond-mat.mes-hall</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>D. O. Oriekhov, Stan Bergkamp, Guliuxin Jin, Juan Daniel Torres Luna, Badr Zouggari, Sibren van der Meer, Naoual El Yazidi, Eliska Greplova</dc:creator>
    </item>
  </channel>
</rss>
