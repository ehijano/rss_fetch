<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Sep 2024 04:00:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Agent modelling, statistical control, and the strength of middle knowledge</title>
      <link>https://arxiv.org/abs/2409.17196</link>
      <description>arXiv:2409.17196v1 Announce Type: new 
Abstract: This methods article concerns analysing data generated from running experiments on agent based models to study industries and organisations. It demonstrates that when researchers study virtual ecologies they can and should discard statistical controls in favour of experiment controls. In the first of two illustrations we show that we can detect an effect with a fraction of the data needed for a traditional analysis, which is valuable given the computational complexity of many models. In the second we show that agent based models can provide control without introducing the biases associated with certain causal structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17196v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Chesney, Tim Gruchman, Robert Pasley, Altricia Dawson, Stefan Gold</dc:creator>
    </item>
    <item>
      <title>Identifying Time Patterns of Highland and Lowland Air Temperature Trends in Italy and UK across monthly and annual scales</title>
      <link>https://arxiv.org/abs/2409.17732</link>
      <description>arXiv:2409.17732v1 Announce Type: new 
Abstract: This paper presents a statistical analysis of air temperature data from 32 stations in Italy and the UK up to 2000 m above sea level, from 2002 to 2021. The data came from both highland and lowland areas, in order to evaluate both the differences due to location, and elevation. The analysis focused on detecting trends at annual and monthly time scales, employing both ordinary least squares, robust S-estimator regression, and Mann-Kendall and Sen's slope methods. Then hierarchical clustering using Dynamic Time Warping (DTW) was applied to the monthly data to analyze the intra-annual pattern similarity of trends within and across the groups. Two different regions of Europe were chosen because of the different climate and temperature trends, namely the Northern UK (smaller trends) and the North-West Italian Alps (larger trends). The main novelty of the work is to show that stations having similar locations and altitudes have similar monthly slopes by quantifying them using DTW and clustering. These results reveal the nonrandomness of different trends along the year and among different parts of Europe, with a modest influence of altitude in wintertime. The findings revealed that group average trends were close to the NOAA values for the areas in Italy and the UK, confirming the validity of analyzing a small number of stations. More interestingly, intra-annual patterns were detected commonly at the stations of each of the groups, and clearly different between them Hierarchical clustering in combination with DTW showed consistent similarity between monthly patterns of means and trends within the group of stations and inconsistent similarity between patterns across groups. Distance correlation matrices also contributes to what is the main result of the paper, which is to clearly show the different temporal patterns in relation to location and (in some months) altitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17732v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chalachew Muluken Liyew, Elvira Di Nardo, Rosa Meo, Stefano Ferraris</dc:creator>
    </item>
    <item>
      <title>Confounder-adjusted Covariances of System Outputs and Applications to Structural Health Monitoring</title>
      <link>https://arxiv.org/abs/2409.17735</link>
      <description>arXiv:2409.17735v1 Announce Type: new 
Abstract: Automated damage detection is an integral component of each structural health monitoring (SHM) system. Typically, measurements from various sensors are collected and reduced to damage-sensitive features, and diagnostic values are generated by statistically evaluating the features. Since changes in data do not only result from damage, it is necessary to determine the confounding factors (environmental or operational variables) and to remove their effects from the measurements or features. Many existing methods for correcting confounding effects are based on different types of mean regression. This neglects potential changes in higher-order statistical moments, but in particular, the output covariances are essential for generating reliable diagnostics for damage detection. This article presents an approach to explicitly quantify the changes in the covariance, using conditional covariance matrices based on a non-parametric, kernel-based estimator. The method is applied to the Munich Test Bridge and the KW51 Railway Bridge in Leuven, covering both raw sensor measurements (acceleration, strain, inclination) and extracted damage-sensitive features (natural frequencies). The results show that covariances between different vibration or inclination sensors can significantly change due to temperature changes, and the same is true for natural frequencies. To highlight the advantages, it is explained how conditional covariances can be combined with standard approaches for damage detection, such as the Mahalanobis distance and principal component analysis. As a result, more reliable diagnostic values can be generated with fewer false alarms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17735v1</guid>
      <category>stat.AP</category>
      <category>eess.SP</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lizzie Neumann, Philipp Wittenberg, Alexander Mendler, Jan Gertheiss</dc:creator>
    </item>
    <item>
      <title>Entropy-based feature selection for capturing impacts in Earth system models with extreme forcing</title>
      <link>https://arxiv.org/abs/2409.18011</link>
      <description>arXiv:2409.18011v1 Announce Type: new 
Abstract: This paper presents the development of a new entropy-based feature selection method for identifying and quantifying impacts. Here, impacts are defined as statistically significant differences in spatio-temporal fields when comparing datasets with and without an external forcing in Earth system models. Temporal feature selection is performed by first computing the cross-fuzzy entropy to quantify similarity of patterns between two datasets and then applying changepoint detection to identify regions of statistically constant entropy. The method is used to capture temperate north surface cooling from a 9-member simulation ensemble of the Mt. Pinatubo volcanic eruption, which injected 10 Tg of SO2 into the stratosphere. The results estimate a mean difference decrease in near surface air temperature of -0.560 K with a 99% confidence interval between -0.864 K and -0.257 K between April and November of 1992, one year following the eruption. A sensitivity analysis with decreasing SO2 injection revealed that the impact is statistically significant at 5 Tg but not at 3 Tg. Using identified features, a dependency graph model composed of 68 nodes and 229 edges directly connecting initial aerosol optical depth changes in the tropics to solar flux and temperature changes before the temperate north surface cooling is presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18011v1</guid>
      <category>stat.AP</category>
      <category>physics.geo-ph</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jerry Watkins, Luca Bertagna, Graham Harper, Andrew Steyer, Irina Tezaur, Diana Bull</dc:creator>
    </item>
    <item>
      <title>Uncertainty quantification for seismic response using dimensionality reduction-based stochastic simulator</title>
      <link>https://arxiv.org/abs/2409.17159</link>
      <description>arXiv:2409.17159v1 Announce Type: cross 
Abstract: This paper introduces a stochastic simulator for seismic uncertainty quantification, which is crucial for performance-based earthquake engineering. The proposed simulator extends the recently developed dimensionality reduction-based surrogate modeling method (DR-SM) to address high-dimensional ground motion uncertainties and the high computational demands associated with nonlinear response history analyses. By integrating physics-based dimensionality reduction with multivariate conditional distribution models, the proposed simulator efficiently propagates seismic input into multivariate response quantities of interest. The simulator can incorporate both aleatory and epistemic uncertainties and does not assume distribution models for the seismic responses. The method is demonstrated through three finite element building models subjected to synthetic and recorded ground motions. The proposed method effectively predicts multivariate seismic responses and quantifies uncertainties, including correlations among responses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17159v1</guid>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jungho Kim, Ziqi Wang</dc:creator>
    </item>
    <item>
      <title>Sparsity, Regularization and Causality in Agricultural Yield: The Case of Paddy Rice in Peru</title>
      <link>https://arxiv.org/abs/2409.17298</link>
      <description>arXiv:2409.17298v1 Announce Type: cross 
Abstract: This study introduces a novel approach that integrates agricultural census data with remotely sensed time series to develop precise predictive models for paddy rice yield across various regions of Peru. By utilizing sparse regression and Elastic-Net regularization techniques, the study identifies causal relationships between key remotely sensed variables-such as NDVI, precipitation, and temperature-and agricultural yield. To further enhance prediction accuracy, the first- and second-order dynamic transformations (velocity and acceleration) of these variables are applied, capturing non-linear patterns and delayed effects on yield. The findings highlight the improved predictive performance when combining regularization techniques with climatic and geospatial variables, enabling more precise forecasts of yield variability. The results confirm the existence of causal relationships in the Granger sense, emphasizing the value of this methodology for strategic agricultural management. This contributes to more efficient and sustainable production in paddy rice cultivation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17298v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rita Rocio Guzman-Lopez, Luis Huamanchumo, Kevin Fernandez, Oscar Cutipa-Luque, Yhon Tiahuallpa, Helder Rojas</dc:creator>
    </item>
    <item>
      <title>Granger Causality for Mixed Time Series Generalized Linear Models: A Case Study on Multimodal Brain Connectivity</title>
      <link>https://arxiv.org/abs/2409.17751</link>
      <description>arXiv:2409.17751v1 Announce Type: cross 
Abstract: This paper is motivated by studies in neuroscience experiments to understand interactions between nodes in a brain network using different types of data modalities that capture different distinct facets of brain activity. To assess Granger-causality, we introduce a flexible framework through a general class of models that accommodates mixed types of data (binary, count, continuous, and positive components) formulated in a generalized linear model (GLM) fashion. Statistical inference for causality is performed based on both frequentist and Bayesian approaches, with a focus on the latter. Here, we develop a procedure for conducting inference through the proposed Bayesian mixed time series model. By introducing spike and slab priors for some parameters in the model, our inferential approach guides causality order selection and provides proper uncertainty quantification. The proposed methods are then utilized to study the rat spike train and local field potentials (LFP) data recorded during the olfaction working memory task. The proposed methodology provides critical insights into the causal relationship between the rat spiking activity and LFP spectral power. Specifically, power in the LFP beta band is predictive of spiking activity 300 milliseconds later, providing a novel analytical tool for this area of emerging interest in neuroscience and demonstrating its usefulness and flexibility in the study of causality in general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17751v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luiza S. C. Piancastelli, Wagner Barreto-Souza, Norbert J. Fortin, Keiland W. Cooper, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Incorporating sparse labels into biologging studies using hidden Markov models with weighted likelihoods</title>
      <link>https://arxiv.org/abs/2409.18091</link>
      <description>arXiv:2409.18091v1 Announce Type: cross 
Abstract: Ecologists often use a hidden Markov model to decode a latent process, such as a sequence of an animal's behaviours, from an observed biologging time series. Modern technological devices such as video recorders and drones now allow researchers to directly observe an animal's behaviour. Using these observations as labels of the latent process can improve a hidden Markov model's accuracy when decoding the latent process. However, many wild animals are observed infrequently. Including such rare labels often has a negligible influence on parameter estimates, which in turn does not meaningfully improve the accuracy of the decoded latent process. We introduce a weighted likelihood approach that increases the relative influence of labelled observations. We use this approach to develop two hidden Markov models to decode the foraging behaviour of killer whales (Orcinus orca) off the coast of British Columbia, Canada. Using cross-validated evaluation metrics, we show that our weighted likelihood approach produces more accurate and understandable decoded latent processes compared to existing methods. Thus, our method effectively leverages sparse labels to enhance researchers' ability to accurately decode hidden processes across various fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18091v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evan Sidrow, Nancy Heckman, Tess M. McRae, Beth L. Volpov, Andrew W. Trites, Sarah M. E. Fortune, Marie Auger-M\'eth\'e</dc:creator>
    </item>
    <item>
      <title>Effect of electric vehicles, heat pumps, and solar panels on low-voltage feeders: Evidence from smart meter profiles</title>
      <link>https://arxiv.org/abs/2409.18105</link>
      <description>arXiv:2409.18105v1 Announce Type: cross 
Abstract: Electric vehicles (EVs), heat pumps (HPs) and solar panels are low-carbon technologies (LCTs) that are being connected to the low-voltage grid (LVG) at a rapid pace. One of the main hurdles to understand their impact on the LVG is the lack of recent, large electricity consumption datasets, measured in real-world conditions. We investigated the contribution of LCTs to the size and timing of peaks on LV feeders by using a large dataset of 42,089 smart meter profiles of residential LVG customers. These profiles were measured in 2022 by Fluvius, the distribution system operator (DSO) of Flanders, Belgium. The dataset contains customers that proactively requested higher-resolution smart metering data, and hence is biased towards energy-interested people. LV feeders of different sizes were statistically modelled with a profile sampling approach. For feeders with 40 connections, we found a contribution to the feeder peak of 1.2 kW for a HP, 1.4 kW for an EV and 2.0 kW for an EV charging faster than 6.5 kW. A visual analysis of the feeder-level loads shows that the classical duck curve is replaced by a night-camel curve for feeders with only HPs and a night-dromedary curve for feeders with only EVs charging faster than 6.5 kW. Consumption patterns will continue to change as the energy transition is carried out, because of e.g. dynamic electricity tariffs or increased battery capacities. Our introduced methods are simple to implement, making it a useful tool for DSOs that have access to smart meter data to monitor changing consumption patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18105v1</guid>
      <category>eess.SY</category>
      <category>cs.CY</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>T. Becker, R. Smet, B. Macharis, K. Vanthournout</dc:creator>
    </item>
    <item>
      <title>Formulating the Proxy Pattern-Mixture Model as a Selection Model to Assist with Sensitivity Analysis</title>
      <link>https://arxiv.org/abs/2409.18117</link>
      <description>arXiv:2409.18117v1 Announce Type: cross 
Abstract: Proxy pattern-mixture models (PPMM) have previously been proposed as a model-based framework for assessing the potential for nonignorable nonresponse in sample surveys and nonignorable selection in nonprobability samples. One defining feature of the PPMM is the single sensitivity parameter, $\phi$, that ranges from 0 to 1 and governs the degree of departure from ignorability. While this sensitivity parameter is attractive in its simplicity, it may also be of interest to describe departures from ignorability in terms of how the odds of response (or selection) depend on the outcome being measured. In this paper, we re-express the PPMM as a selection model, using the known relationship between pattern-mixture models and selection models, in order to better understand the underlying assumptions of the PPMM and the implied effect of the outcome on nonresponse. The selection model that corresponds to the PPMM is a quadratic function of the survey outcome and proxy variable, and the magnitude of the effect depends on the value of the sensitivity parameter, $\phi$ (missingness/selection mechanism), the differences in the proxy means and standard deviations for the respondent and nonrespondent populations, and the strength of the proxy, $\rho^{(1)}$. Large values of $\phi$ (beyond $0.5$) often result in unrealistic selection mechanisms, and the corresponding selection model can be used to establish more realistic bounds on $\phi$. We illustrate the results using data from the U.S. Census Household Pulse Survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18117v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seth Adarkwah Yiadom, Rebecca Andridge</dc:creator>
    </item>
    <item>
      <title>A Diagnostic Tool for Functional Causal Discovery</title>
      <link>https://arxiv.org/abs/2406.07787</link>
      <description>arXiv:2406.07787v2 Announce Type: replace-cross 
Abstract: Causal discovery methods aim to determine the causal direction between variables using observational data. Functional causal discovery methods, such as those based on the Linear Non-Gaussian Acyclic Model (LiNGAM), rely on structural and distributional assumptions to infer the causal direction. However, approaches for assessing causal discovery methods' performance as a function of sample size or the impact of assumption violations, inevitable in real-world scenarios, are lacking. To address this need, we propose Causal Direction Detection Rate (CDDR) diagnostic that evaluates whether and to what extent the interaction between assumption violations and sample size affects the ability to identify the hypothesized causal direction. Given a bivariate dataset of size N on a pair of variables, X and Y, CDDR diagnostic is the plotted comparison of the probability of each causal discovery outcome (e.g. X causes Y, Y causes X, or inconclusive) as a function of sample size less than N. We fully develop CDDR diagnostic in a bivariate case and demonstrate its use for two methods, LiNGAM and our new test-based causal discovery approach. We find CDDR diagnostic for the test-based approach to be more informative since it uses a richer set of causal discovery outcomes. Under certain assumptions, we prove that the probability estimates of detecting each possible causal discovery outcome are consistent and asymptotically normal. Through simulations, we study CDDR diagnostic's behavior when linearity and non-Gaussianity assumptions are violated. Additionally, we illustrate CDDR diagnostic on four real datasets, including three for which the causal direction is known.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07787v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shreya Prakash, Fan Xia, Elena Erosheva</dc:creator>
    </item>
    <item>
      <title>Optimal Visual Search with Highly Heuristic Decision Rules</title>
      <link>https://arxiv.org/abs/2409.12124</link>
      <description>arXiv:2409.12124v2 Announce Type: replace-cross 
Abstract: Visual search is a fundamental natural task for humans and other animals. We investigated the decision processes humans use when searching briefly presented displays having well-separated potential target-object locations. Performance was compared with the Bayesian-optimal decision process under the assumption that the information from the different potential target locations is statistically independent. Surprisingly, humans performed slightly better than optimal, despite humans' substantial loss of sensitivity in the fovea, and the implausibility of the human brain replicating the optimal computations. We show that three factors can quantitatively explain these seemingly paradoxical results. Most importantly, simple and fixed heuristic decision rules reach near optimal search performance. Secondly, foveal neglect primarily affects only the central potential target location. Finally, spatially correlated neural noise causes search performance to exceed that predicted for independent noise. These findings have far-reaching implications for understanding visual search tasks and other identification tasks in humans and other animals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12124v2</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anqi Zhang, Wilson S. Geisler</dc:creator>
    </item>
  </channel>
</rss>
