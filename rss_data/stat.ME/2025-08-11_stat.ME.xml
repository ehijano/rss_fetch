<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 12 Aug 2025 02:41:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Explainable AI in Healthcare: to Explain, to Predict, or to Describe?</title>
      <link>https://arxiv.org/abs/2508.05753</link>
      <description>arXiv:2508.05753v1 Announce Type: new 
Abstract: Explainable Artificial Intelligence (AI) methods are designed to provide information about how AI-based models make predictions. In healthcare, there is a widespread expectation that these methods will provide relevant and accurate information about a model's inner-workings to different stakeholders (ranging from patients and healthcare providers to AI and medical guideline developers). This is a challenging endeavour since what qualifies as relevant information may differ greatly depending on the stakeholder. For many stakeholders, relevant explanations are causal in nature, yet, explainable AI methods are often not able to deliver this information. Using the Describe-Predict-Explain framework, we argue that Explainable AI methods are good descriptive tools, as they may help to describe how a model works but are limited in their ability to explain why a model works in terms of true underlying biological mechanisms and cause-and-effect relations. This limits the suitability of explainable AI methods to provide actionable advice to patients or to judge the face validity of AI-based models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05753v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alex Carriero, Anne de Hond, Bram Cappers, Fernando Paulovich, Sanne Abeln, Karel GM Moons, Maarten van Smeden</dc:creator>
    </item>
    <item>
      <title>Doubly robust integration of nonprobability and probability survey data</title>
      <link>https://arxiv.org/abs/2508.05859</link>
      <description>arXiv:2508.05859v1 Announce Type: new 
Abstract: Doubly robust estimators combine an inverse probability weighting estimator and a mass imputation estimator. Several doubly robust estimators for estimating the population mean (or prevalence) of an outcome have been proposed for integrating outcome and covariate data from a nonprobability survey with covariate data from an auxiliary probability survey. However, the question of how to combine a doubly robust estimate with a corresponding estimate based on outcome data from the auxiliary probability survey alone has only received limited attention. In this paper, we (i) review previously proposed doubly robust estimators, (ii) provide formulae for the variance of doubly robust estimators and the covariance between doubly robust and probability survey estimates, (iii) propose a framework for how to combine efficiently a doubly robust estimate from a nonprobability sample with an estimate based on the auxiliary probability sample alone, and (iv) provide formulae for the variance of such combined estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05859v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaun R Seaman, Tommy Nyberg, Anne M Presanis</dc:creator>
    </item>
    <item>
      <title>Identifiability and Inference for Generalized Latent Factor Models</title>
      <link>https://arxiv.org/abs/2508.05866</link>
      <description>arXiv:2508.05866v1 Announce Type: new 
Abstract: Generalized latent factor analysis not only provides a useful latent embedding approach in statistics and machine learning, but also serves as a widely used tool across various scientific fields, such as psychometrics, econometrics, and social sciences. Ensuring the identifiability of latent factors and the loading matrix is essential for the model's estimability and interpretability, and various identifiability conditions have been employed by practitioners. However, fundamental statistical inference issues for latent factors and factor loadings under commonly used identifiability conditions remain largely unaddressed, especially for correlated factors and/or non-orthogonal loading matrix. In this work, we focus on the maximum likelihood estimation for generalized factor models and establish statistical inference properties under popularly used identifiability conditions. The developed theory is further illustrated through numerical simulations and an application to a personality assessment dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05866v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengyu Cui, Gongjun Xu</dc:creator>
    </item>
    <item>
      <title>Bayesian weighted discrete-time dynamic models for association football prediction</title>
      <link>https://arxiv.org/abs/2508.05891</link>
      <description>arXiv:2508.05891v1 Announce Type: new 
Abstract: In recent years, great emphasis has been placed on the prediction of association football. Due to this, several studies have proposed different types of statistical models to predict the outcome of a football match. However, most existing approaches usually assume that the offensive and defensive abilities of teams remain static over time. We introduce a Bayesian dynamic approach for football goal based models that uses period-specific commensurate priors to flexibly weight the evolution of attacking and defensive abilities. Our approach assigns separate, time varying precisions for each ability and period, controlled via spike and slab hyperpriors. This adaptive shrinkage borrows information about teams' strength when past and current performance aligns and allows rapid adjustments when teams experience substantial changes (e.g., transfer windows or coaching changes). We integrate this framework into six standard goal based models evaluating predictive performance using data from the last five seasons of the German Bundesliga, English Premier League, and Spanish La Liga. Compared with the other discrete time dynamic models, our adaptive approach yields better predictive performance. The proposed methodology has also been implemented in the free and open source R package footBayes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05891v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Macr\`i-Demartino, Leonardo Egidi, Nicola Torelli</dc:creator>
    </item>
    <item>
      <title>Structural Equation Modeling with Latent Variables and Composites</title>
      <link>https://arxiv.org/abs/2508.06112</link>
      <description>arXiv:2508.06112v1 Announce Type: new 
Abstract: Structural equation modeling (SEM) is a prevalent approach for studying constructs. Traditionally, these constructs are modeled as reflectively measured latent variables - common factors that account for the variance-covariance structure of their associated indicators. Over the past two decades, there has been growing interest in an alternative way of modeling constructs: the composite, i.e., a linear combination of indicators. However, existing approaches to estimating composite models either limit researchers from fully leveraging SEM's capabilities, such as handling missing data, evaluating overall model fit, and testing group differences, or significantly increase complexity of the model specification by introducing additional variables. Against this background, this paper presents SEM with latent variables and composites. Our presented model specification, along with its model-implied variance-covariance matrix, enables researchers to: (i) utilize well-established SEM estimators, including maximum likelihood and generalized least squares estimators, and (ii) fully exploit SEM's capabilities in model specification, assessment, and missing data handling. This advancement aims to enhance the flexibility and applicability of SEM in analyzing constructs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06112v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tamara Schamberger, Florian Schuberth, J\"org Henseler, Yves Rosseel</dc:creator>
    </item>
    <item>
      <title>IOCC: Aligning Semantic and Cluster Centers for Few-shot Short Text Clustering</title>
      <link>https://arxiv.org/abs/2508.06126</link>
      <description>arXiv:2508.06126v1 Announce Type: new 
Abstract: In clustering tasks, it is essential to structure the feature space into clear, well-separated distributions. However, because short text representations have limited expressiveness, conventional methods struggle to identify cluster centers that truly capture each category's underlying semantics, causing the representations to be optimized in suboptimal directions. To address this issue, we propose IOCC, a novel few-shot contrastive learning method that achieves alignment between the cluster centers and the semantic centers. IOCC consists of two key modules: Interaction-enhanced Optimal Transport (IEOT) and Center-aware Contrastive Learning (CACL). Specifically, IEOT incorporates semantic interactions between individual samples into the conventional optimal transport problem, and generate pseudo-labels. Based on these pseudo-labels, we aggregate high-confidence samples to construct pseudo-centers that approximate the semantic centers. Next, CACL optimizes text representations toward their corresponding pseudo-centers. As training progresses, the collaboration between the two modules gradually reduces the gap between cluster centers and semantic centers. Therefore, the model will learn a high-quality distribution, improving clustering performance. Extensive experiments on eight benchmark datasets show that IOCC outperforms previous methods, achieving up to 7.34\% improvement on challenging Biomedical dataset and also excelling in clustering stability and efficiency. The code is available at: https://anonymous.4open.science/r/IOCC-C438.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06126v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jixuan Yin, Zhihao Yao, Wenshuai Huo, Xinmiao Yu, Xiaocheng Feng, Bo Li</dc:creator>
    </item>
    <item>
      <title>Variable selection via knockoffs in missing data settings with categorical predictors</title>
      <link>https://arxiv.org/abs/2508.06138</link>
      <description>arXiv:2508.06138v1 Announce Type: new 
Abstract: Large-scale assessment data typically include numerous categorical variables, often affected by missing values. Motivated by the challenges arising in this framework, we extend the knockoffs method for selecting predictors to settings with missing values. Our proposal relies on a preliminary phase consisting of multiple imputations of missing values. Each imputed dataset is then processed using a suitable knockoff filter. We evaluate the performance of the proposed method through a simulation study, showing satisfactory results consistent with a recently advocated cutting-edge method. We apply the method to large-scale assessment data collected by INVALSI about test scores of Italian students in grade 5 with many background variables. This case study is challenging, as most predictors have unordered categories, a setting not taken into account by traditional knockoffs methods. In addition, some of the key predictors are affected by missing values. The model includes random effects to account for the multilevel structure of students nested into schools. Our proposal to implement the knockoffs method within a multiple imputation framework proves to be feasible, flexible and effective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06138v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Silvia Bacci, Emanuela Dreassi, Leonardo Grilli, Carla Rampichini</dc:creator>
    </item>
    <item>
      <title>Bayesian online collective anomaly and change point detection in fine-grained time series</title>
      <link>https://arxiv.org/abs/2508.06385</link>
      <description>arXiv:2508.06385v1 Announce Type: new 
Abstract: Fine-grained time series data are crucial for accurate and timely online change detection. While both collective anomalies and change points can coexist in such data, their joint online detection has received limited attention. In this research, we develop a Bayesian framework capturing time series with collective anomalies and change points, and introduce a recursive online inference algorithm to detect the most recent collective anomaly and change point jointly. For scaling, we further propose an algorithm enhanced with collective anomaly removal that effectively reduces the time and space complexity to linear. We demonstrate the effectiveness of our approach via extensive experiments on simulated data and two real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06385v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xian Chen, Weichi Wu</dc:creator>
    </item>
    <item>
      <title>Coverage correlation: detecting singular dependencies between random variables</title>
      <link>https://arxiv.org/abs/2508.06402</link>
      <description>arXiv:2508.06402v1 Announce Type: new 
Abstract: We introduce the coverage correlation coefficient, a novel nonparametric measure of statistical association designed to quantifies the extent to which two random variables have a joint distribution concentrated on a singular subset with respect to the product of the marginals. Our correlation statistic consistently estimates an $f$-divergence between the joint distribution and the product of the marginals, which is 0 if and only if the variables are independent and 1 if and only if the copula is singular. Using Monge--Kantorovich ranks, the coverage correlation naturally extends to measure association between random vectors. It is distribution-free, admits an analytically tractable asymptotic null distribution, and can be computed efficiently, making it well-suited for detecting complex, potentially nonlinear associations in large-scale pairwise testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06402v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuzhi Yang, Mona Azadkia, Tengyao Wang</dc:creator>
    </item>
    <item>
      <title>Diagrams-to-Dynamics (D2D): Exploring Causal Loop Diagram Leverage Points under Uncertainty</title>
      <link>https://arxiv.org/abs/2508.05659</link>
      <description>arXiv:2508.05659v2 Announce Type: cross 
Abstract: Causal loop diagrams (CLDs) are widely used in health and environmental research to represent hypothesized causal structures underlying complex problems. However, as qualitative and static representations, CLDs are limited in their ability to support dynamic analysis and inform intervention strategies. Additionally, quantitative CLD analysis methods like network centrality analysis often lead to false inference. We propose Diagrams-to-Dynamics (D2D), a method for converting CLDs into exploratory system dynamics models (SDMs) in the absence of empirical data. With minimal user input - following a protocol to label variables as stocks, flows or auxiliaries, and constants - D2D leverages the structural information already encoded in CLDs, namely, link existence and polarity, to simulate hypothetical interventions and explore potential leverage points under uncertainty. Results suggest that D2D helps distinguish between high- and low-ranked leverage points. We compare D2D to a data-driven SDM constructed from the same CLD and variable labels. D2D showed greater consistency with the data-driven model than network centrality analysis, while providing uncertainty estimates and guidance for future data collection. The method is implemented in an open-source Python package and a web-based application to support further testing and lower the barrier to dynamic modeling for researchers working with CLDs. We expect additional validation will further establish the approach's utility across a broad range of cases and domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05659v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jeroen F. Uleman, Loes Crielaard, Leonie K. Elsenburg, Guido A. Veldhuis, Karien Stronks, Naja Hulvej Rod, Rick Quax, V\'itor V. Vasconcelos</dc:creator>
    </item>
    <item>
      <title>Latent confounding in high-dimensional nonlinear models</title>
      <link>https://arxiv.org/abs/2508.06274</link>
      <description>arXiv:2508.06274v1 Announce Type: cross 
Abstract: We consider the the problem of identifying causal effects given a high-dimensional treatment vector in the presence of low-dimensional latent confounders. We assume a parametric structural causal model in which the outcome is permitted to depend on a sparse linear combination of the treatment vector and confounders nonlinearly. We consider a generalisation of the LAVA estimator of Chernozhukov et al. [2017] for estimating the treatment effects and show that under the so-called `dense confounding' assumption that each confounder can affect a wide range of observed treatment variables, one can estimate the causal parameters at the same rate as possible without confounding. Notably, the results permit a form of weak confounding in that the minimum non-zero singular value of the loading matrix of the confounders can grow more slowly than the $\sqrt{p}$, where $p$ is the dimension of the treatment vector. We further use our generalised LAVA procedure within a generalised covariance measure-based test for edges in a causal DAG in the presence of latent confounding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06274v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhao Wang, Rajen Shah</dc:creator>
    </item>
    <item>
      <title>Decorrelated feature importance from local sample weighting</title>
      <link>https://arxiv.org/abs/2508.06337</link>
      <description>arXiv:2508.06337v1 Announce Type: cross 
Abstract: Feature importance (FI) statistics provide a prominent and valuable method of insight into the decision process of machine learning (ML) models, but their effectiveness has well-known limitations when correlation is present among the features in the training data. In this case, the FI often tends to be distributed among all features which are in correlation with the response-generating signal features. Even worse, if multiple signal features are in strong correlation with a noise feature, while being only modestly correlated with one another, this can result in a noise feature having a distinctly larger FI score than any signal feature. Here we propose local sample weighting (losaw) which can flexibly be integrated into many ML algorithms to improve FI scores in the presence of feature correlation in the training data. Our approach is motivated from inverse probability weighting in causal inference and locally, within the ML model, uses a sample weighting scheme to decorrelate a target feature from the remaining features. This reduces model bias locally, whenever the effect of a potential signal feature is evaluated and compared to others. Moreover, losaw comes with a natural tuning parameter, the minimum effective sample size of the weighted population, which corresponds to an interpretation-prediction-tradeoff, analog to a bias-variance-tradeoff as for classical ML tuning parameters. We demonstrate how losaw can be integrated within decision tree-based ML methods and within mini-batch training of neural networks. We investigate losaw for random forest and convolutional neural networks in a simulation study on settings showing diverse correlation patterns. We found that losaw improves FI consistently. Moreover, it often improves prediction accuracy for out-of-distribution, while maintaining a similar accuracy for in-distribution test data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06337v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benedikt Fr\"ohlich, Alison Durst, Merle Behr</dc:creator>
    </item>
    <item>
      <title>A causal fused lasso for interpretable heterogeneous treatment effects estimation</title>
      <link>https://arxiv.org/abs/2110.00901</link>
      <description>arXiv:2110.00901v4 Announce Type: replace 
Abstract: We propose a novel method for estimating heterogeneous treatment effects based on the fused lasso. By first ordering samples based on the propensity or prognostic score, we match units from the treatment and control groups. We then run the fused lasso to obtain piecewise constant treatment effects with respect to the ordering defined by the score. Similar to the existing methods based on discretizing the score, our methods yield interpretable subgroup effects. However, existing methods fixed the subgroup a priori, but our causal fused lasso forms data-adaptive subgroups. We show that the estimator consistently estimates the treatment effects conditional on the score under very general conditions on the covariates and treatment. We demonstrate the performance of our procedure using extensive experiments that show that it can be interpretable and competitive with state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.00901v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oscar Hernan Madrid Padilla, Yanzhen Chen, Carlos Misael Madrid Padilla, Gabriel Ruiz</dc:creator>
    </item>
    <item>
      <title>Mode-based estimation of the center of symmetry</title>
      <link>https://arxiv.org/abs/2406.08241</link>
      <description>arXiv:2406.08241v2 Announce Type: replace 
Abstract: In the mean-median-mode triad of univariate centrality measures, the mode has been overlooked for estimating the center of symmetry in continuous and unimodal settings. This paper expands on the connection between kernel mode estimators and M-estimators for location, bridging the gap between the nonparametrics and robust statistics communities. The variance of modal estimators is studied in terms of a bandwidth parameter, establishing conditions for an optimal solution that outperforms the household sample mean. A purely nonparametric approach is adopted, modeling heavy-tailedness through regular variation. The results lead to an estimator proposal that includes a novel one-parameter family of kernels with compact support, offering extra robustness and efficiency. The effectiveness and versatility of the new method are demonstrated in a real-world case study and a thorough simulation study, comparing favorably to traditional and more competitive alternatives. Several myths about the mode are clarified along the way, reopening the quest for flexible and efficient nonparametric estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08241v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10463-025-00942-z</arxiv:DOI>
      <arxiv:journal_reference>Annals of the Institute of Statistical Mathematics (2025)</arxiv:journal_reference>
      <dc:creator>Jos\'e E. Chac\'on, Javier Fern\'andez Serrano</dc:creator>
    </item>
    <item>
      <title>A Debiased Estimator for the Mediation Functional in Ultra-High-Dimensional Setting in the Presence of Interaction Effects</title>
      <link>https://arxiv.org/abs/2412.08827</link>
      <description>arXiv:2412.08827v3 Announce Type: replace 
Abstract: Mediation analysis is a crucial tool for uncovering the mechanisms through which a treatment affects the outcome, providing deeper causal insights and guiding effective interventions. Despite advances in analyzing the mediation effect with fixed/low-dimensional mediators and covariates, our understanding of estimation and inference of mediation functional in the presence of (ultra)-high-dimensional mediators and covariates is still limited. In this paper, we present an estimator for mediation functional in a high-dimensional setting that accommodates the interaction between covariates and treatment in generating mediators, as well as interactions between both covariates and treatment and mediators and treatment in generating the response. We demonstrate that our estimator is $\sqrt{n}$-consistent and asymptotically normal, thus enabling reliable inference on direct and indirect treatment effects with asymptotically valid confidence intervals. A key technical contribution of our work is to develop a multi-step debiasing technique, which may also be valuable in other statistical settings with similar structural complexities where accurate estimation depends on debiasing. We evaluate our proposed methodology through extensive simulation studies and apply it to the TCGA lung cancer dataset to estimate the effect of smoking, mediated by DNA methylation, on the survival time of lung cancer patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08827v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shi Bo, AmirEmad Ghassami, Debarghya Mukherjee</dc:creator>
    </item>
    <item>
      <title>KenCoh: A Ranked-Based Canonical Coherence</title>
      <link>https://arxiv.org/abs/2412.10521</link>
      <description>arXiv:2412.10521v2 Announce Type: replace 
Abstract: This work is inspired by the problem of characterizing a dependence measure between two cortical regions of the brain where each region contains multiple signal recordings from several neurons or channels (e.g., inhibitory and excitatory neurons). The goal is to identify differences in the structure of brain functional connectivity between known brain states. An exploratory tool for studying the dependence between two random vectors is via canonical correlation analysis. However, these are limited to only capturing linear associations and are sensitive to outlier observations. Mitigating these limitations is crucial because brain functional connectivity is likely to be more complex than linear, and brain signals may exhibit heavy-tailed properties. To overcome these limitations, we develop a robust method, Kendall's tau-based canonical coherence (KenCoh), to learn connectivity structure among neuronal signals filtered at given frequency bands. Our simulation study demonstrates that KenCoh is competitive with the moment-based estimator and outperforms the latter when the underlying distributions are heavy-tailed. We apply our method to EEG recordings from a virtual-reality driving experiment and to calcium imaging recordings in inhibitory and excitatory neurons of the auditory cortex in mice subjected to sound stimuli. Our findings reveal distinct regional dependencies across frequency bands and brain states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10521v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mara Sherlin D. Talento, Sarbojit Roy, Tania Reyes Vallejo, Leena A Ibrahim, Hernando C. Ombao</dc:creator>
    </item>
    <item>
      <title>On Bayes factor functions</title>
      <link>https://arxiv.org/abs/2506.16674</link>
      <description>arXiv:2506.16674v2 Announce Type: replace 
Abstract: We describe Bayes factors functions based on the sampling distributions of \emph{z}, \emph{t}, $\chi^2$, and \emph{F} statistics, using a class of inverse-moment prior distributions to define alternative hypotheses. These non-local alternative prior distributions are centered on standardized effects, which serve as indices for the Bayes factor function. We compare the conclusions drawn from resulting Bayes factor functions to those drawn from Bayes factors defined using local alternative prior specifications and examine their frequentist operating characteristics. Finally, an application of Bayes factor functions to replicated experimental designs in psychology is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16674v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saptati Datta, Riana Guha, Rachael Shudde, Valen E. Johnson</dc:creator>
    </item>
    <item>
      <title>Propensity score weighting across counterfactual worlds: longitudinal effects under positivity violations</title>
      <link>https://arxiv.org/abs/2507.10774</link>
      <description>arXiv:2507.10774v2 Announce Type: replace 
Abstract: When examining a contrast between two interventions, longitudinal causal inference studies frequently encounter positivity violations when one or both regimes are impossible to observe for some subjects. Existing weighting methods either assume positivity holds or produce effects that conflate interventions' impacts on ultimate outcomes with their effects on intermediate treatments and covariates. We propose a novel class of estimands -- cumulative cross-world weighted effects -- that weights potential outcome differences using propensity scores adapting to positivity violations cumulatively across timepoints and simultaneously across both counterfactual treatment histories. This new estimand isolates mechanistic differences between treatment regimes, is identifiable without positivity assumptions, and circumvents the limitations of existing longitudinal methods. Further, our analysis reveals two fundamental insights about longitudinal causal inference under positivity violations. First, while mechanistically meaningful, these effects correspond to non-implementable interventions, exposing a core interpretability-implementability tradeoff. Second, the identified effects faithfully capture mechanistic differences only under a partial common support assumption; violations cause the identified functional to collapse to zero, even when the causal effect is non-zero. We develop doubly robust-style estimators that achieve asymptotic normality and parametric convergence under nonparametric assumptions on the nuisance estimators. To this end, we reformulate challenging density ratio estimation as regression function estimation, which is achievable with standard machine learning methods. We illustrate our methods through analysis of union membership's effect on earnings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10774v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alec McClean, Iv\'an D\'iaz</dc:creator>
    </item>
    <item>
      <title>Marginal and conditional summary measures: transportability and compatibility across studies</title>
      <link>https://arxiv.org/abs/2507.21925</link>
      <description>arXiv:2507.21925v2 Announce Type: replace 
Abstract: Marginal and conditional summary measures do not generally coincide, have different interpretations and correspond to different decision questions. While these aspects have primarily been recognized for non-collapsible summary measures, they are equally problematic for some collapsible measures in the presence of effect modification. We clarify the interpretation and properties of several marginal and conditional summary measures, considering different types of outcomes and hypothetical outcome-generating mechanisms. We describe implications of the choice of summary measure for transportability, highlighting that covariates not conventionally described as effect modifiers can modify population-level treatment effects. Finally, we illustrate existing summary measure incompatibility issues in the context of evidence synthesis, using the case of covariate adjustment methods for indirect treatment comparisons. Because marginal and conditional summary measures do not generally coincide, their na\"ive pooling in evidence synthesis can produce bias. Almost invariably, care is needed to ensure that evidence synthesis methods are combining compatible summary measures, and this may be easier to accomplish with full access to individual patient data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21925v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonio Remiro-Az\'ocar, David M. Phillippo, Nicky J. Welton, Sofia Dias, A. E. Ades, Anna Heath, Gianluca Baio</dc:creator>
    </item>
    <item>
      <title>Proximal optimal transport divergences</title>
      <link>https://arxiv.org/abs/2505.12097</link>
      <description>arXiv:2505.12097v2 Announce Type: replace-cross 
Abstract: We introduce the proximal optimal transport divergence, a novel discrepancy measure that interpolates between information divergences and optimal transport distances via an infimal convolution formulation. This divergence provides a principled foundation for optimal transport proximals and proximal optimization methods frequently used in generative modeling. We explore its mathematical properties, including smoothness, boundedness, and computational tractability, and establish connections to primal-dual formulations and adversarial learning. The proximal operator associated with the proximal optimal transport divergence can be interpreted as a transport map that pushes a reference distribution toward the optimal generative distribution, which approximates the target distribution that is only accessible through data samples. Building on the Benamou-Brenier dynamic formulation of classical optimal transport, we also establish a dynamic formulation for proximal OT divergences. The resulting dynamic formulation is a first order mean-field game whose optimality conditions are governed by a pair of nonlinear partial differential equations: a backward Hamilton-Jacobi equation and a forward continuity equation. Our framework generalizes existing approaches while offering new insights and computational tools for generative modeling, distributionally robust optimization, and gradient-based learning in probability spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12097v2</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ricardo Baptista, Panagiota Birmpa, Markos A. Katsoulakis, Luc Rey-Bellet, Benjamin J. Zhang</dc:creator>
    </item>
  </channel>
</rss>
