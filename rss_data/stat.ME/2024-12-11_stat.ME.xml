<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Dec 2024 02:42:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Application of Random Matrix Theory in High-Dimensional Statistics</title>
      <link>https://arxiv.org/abs/2412.06848</link>
      <description>arXiv:2412.06848v1 Announce Type: new 
Abstract: This review article provides an overview of random matrix theory (RMT) with a focus on its growing impact on the formulation and inference of statistical models and methodologies. Emphasizing applications within high-dimensional statistics, we explore key theoretical results from RMT and their role in addressing challenges associated with high-dimensional data. The discussion highlights how advances in RMT have significantly influenced the development of statistical methods, particularly in areas such as covariance matrix inference, principal component analysis (PCA), signal processing, and changepoint detection, demonstrating the close interplay between theory and practice in modern high-dimensional statistical inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06848v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swapnaneel Bhattacharyya, Srijan Chattopadhyay, Sevantee Basu</dc:creator>
    </item>
    <item>
      <title>Variable Selection for Comparing High-dimensional Time-Series Data</title>
      <link>https://arxiv.org/abs/2412.06870</link>
      <description>arXiv:2412.06870v1 Announce Type: new 
Abstract: Given a pair of multivariate time-series data of the same length and dimensions, an approach is proposed to select variables and time intervals where the two series are significantly different. In applications where one time series is an output from a computationally expensive simulator, the approach may be used for validating the simulator against real data, for comparing the outputs of two simulators, and for validating a machine learning-based emulator against the simulator. With the proposed approach, the entire time interval is split into multiple subintervals, and on each subinterval, the two sample sets are compared to select variables that distinguish their distributions and a two-sample test is performed. The validity and limitations of the proposed approach are investigated in synthetic data experiments. Its usefulness is demonstrated in an application with a particle-based fluid simulator, where a deep neural network model is compared against the simulator, and in an application with a microscopic traffic simulator, where the effects of changing the simulator's parameters on traffic flows are analysed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06870v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kensuke Mitsuzawa, Margherita Grossi, Stefano Bortoli, Motonobu Kanagawa</dc:creator>
    </item>
    <item>
      <title>On the Consistency of Bayesian Adaptive Testing under the Rasch Model</title>
      <link>https://arxiv.org/abs/2412.07170</link>
      <description>arXiv:2412.07170v1 Announce Type: new 
Abstract: This study establishes the consistency of Bayesian adaptive testing methods under the Rasch model, addressing a gap in the literature on their large-sample guarantees. Although Bayesian approaches are recognized for their finite-sample performance and capability to circumvent issues such as the cold-start problem; however, rigorous proofs of their asymptotic properties, particularly in non-i.i.d. structures, remain lacking. We derive conditions under which the posterior distributions of latent traits converge to the true values for a sequence of given items, and demonstrate that Bayesian estimators remain robust under the mis-specification of the prior. Our analysis then extends to adaptive item selection methods in which items are chosen endogenously during the test. Additionally, we develop a Bayesian decision-theoretical framework for the item selection problem and propose a novel selection that aligns the test process with optimal estimator performance. These theoretical results provide a foundation for Bayesian methods in adaptive testing, complementing prior evidence of their finite-sample advantages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07170v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hau-Hung Yang, Chia-Min Wei, Yu-Chang Chen</dc:creator>
    </item>
    <item>
      <title>Automatic Doubly Robust Forests</title>
      <link>https://arxiv.org/abs/2412.07184</link>
      <description>arXiv:2412.07184v1 Announce Type: new 
Abstract: This paper proposes the automatic Doubly Robust Random Forest (DRRF) algorithm for estimating the conditional expectation of a moment functional in the presence of high-dimensional nuisance functions. DRRF combines the automatic debiasing framework using the Riesz representer (Chernozhukov et al., 2022) with non-parametric, forest-based estimation methods for the conditional moment (Athey et al., 2019; Oprescu et al., 2019). In contrast to existing methods, DRRF does not require prior knowledge of the form of the debiasing term nor impose restrictive parametric or semi-parametric assumptions on the target quantity. Additionally, it is computationally efficient for making predictions at multiple query points and significantly reduces runtime compared to methods such as Orthogonal Random Forest (Oprescu et al., 2019). We establish the consistency and asymptotic normality results of DRRF estimator under general assumptions, allowing for the construction of valid confidence intervals. Through extensive simulations in heterogeneous treatment effect (HTE) estimation, we demonstrate the superior performance of DRRF over benchmark approaches in terms of estimation accuracy, robustness, and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07184v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaomeng Chen, Junting Duan, Victor Chernozhukov, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>Median Based Unit Weibull Distribution (MBUW): Does the Higher Order Probability Weighted Moments (PWM) Add More Information over the Lower Order PWM in Parameter Estimation</title>
      <link>https://arxiv.org/abs/2412.07404</link>
      <description>arXiv:2412.07404v1 Announce Type: new 
Abstract: In the present paper, Probability weighted moments (PWMs) method for parameter estimation of the median based unit weibull (MBUW) distribution is discussed. The most widely used first order PWMs is compared with the higher order PWMs for parameter estimation of (MBUW) distribution. Asymptotic distribution of this PWM estimator is derived. This comparison is illustrated using real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07404v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Mohammed Attia</dc:creator>
    </item>
    <item>
      <title>A comparison of Kaplan--Meier-based inverse probability of censoring weighted regression methods</title>
      <link>https://arxiv.org/abs/2412.07495</link>
      <description>arXiv:2412.07495v1 Announce Type: new 
Abstract: Weighting with the inverse probability of censoring is an approach to deal with censoring in regression analyses where the outcome may be missing due to right-censoring. In this paper, three separate approaches involving this idea in a setting where the Kaplan--Meier estimator is used for estimating the censoring probability are compared. In more detail, the three approaches involve weighted regression, regression with a weighted outcome, and regression of a jack-knife pseudo-observation based on a weighted estimator. Expressions of the asymptotic variances are given in each case and the expressions are compared to each other and to the uncensored case. In terms of low asymptotic variance, a clear winner cannot be found. Which approach will have the lowest asymptotic variance depends on the censoring distribution. Expressions of the limit of the standard sandwich variance estimator in the three cases are also provided, revealing an overestimation under the implied assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07495v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Morten Overgaard</dc:creator>
    </item>
    <item>
      <title>Gaussian Process with dissolution spline kernel</title>
      <link>https://arxiv.org/abs/2412.07524</link>
      <description>arXiv:2412.07524v1 Announce Type: new 
Abstract: In-vitro dissolution testing is a critical component in the quality control of manufactured drug products. The $\mathrm{f}_2$ statistic is the standard for assessing similarity between two dissolution profiles. However, the $\mathrm{f}_2$ statistic has known limitations: it lacks an uncertainty estimate, is a discrete-time metric, and is a biased measure, calculating the differences between profiles at discrete time points. To address these limitations, we propose a Gaussian Process (GP) with a dissolution spline kernel for dissolution profile comparison. The dissolution spline kernel is a new spline kernel using logistic functions as its basis functions, enabling the GP to capture the expected monotonic increase in dissolution curves. This results in better predictions of dissolution curves. This new GP model reduces bias in the $\mathrm{f}_2$ calculation by allowing predictions to be interpolated in time between observed values, and provides uncertainty quantification. We assess the model's performance through simulations and real datasets, demonstrating its improvement over a previous GP-based model introduced for dissolution testing. We also show that the new model can be adapted to include dissolution-specific covariates. Applying the model to real ibuprofen dissolution data under various conditions, we demonstrate its ability to extrapolate curve shapes across different experimental settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07524v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fiona Murphy, Marina Navas Bachiller, Deirdre M. D'Arcy, Alessio Benavoli</dc:creator>
    </item>
    <item>
      <title>Nested exemplar latent space models for dimension reduction in dynamic networks</title>
      <link>https://arxiv.org/abs/2412.07604</link>
      <description>arXiv:2412.07604v1 Announce Type: new 
Abstract: Dynamic latent space models are widely used for characterizing changes in networks and relational data over time. These models assign to each node latent attributes that characterize connectivity with other nodes, with these latent attributes dynamically changing over time. Node attributes can be organized as a three-way tensor with modes corresponding to nodes, latent space dimension, and time. Unfortunately, as the number of nodes and time points increases, the number of elements of this tensor becomes enormous, leading to computational and statistical challenges, particularly when data are sparse. We propose a new approach for massively reducing dimensionality by expressing the latent node attribute tensor as low rank. This leads to an interesting new {\em nested exemplar} latent space model, which characterizes the node attribute tensor as dependent on low-dimensional exemplar traits for each node, weights for each latent space dimension, and exemplar curves characterizing time variation. We study properties of this framework, including expressivity, and develop efficient Bayesian inference algorithms. The approach leads to substantial advantages in simulations and applications to ecological networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07604v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jennifer Noelle Kampe, Luca Alessandro Silva, David Brian Dunson, Tomas Roslin</dc:creator>
    </item>
    <item>
      <title>Deep Partially Linear Transformation Model for Right-Censored Survival Data</title>
      <link>https://arxiv.org/abs/2412.07611</link>
      <description>arXiv:2412.07611v1 Announce Type: new 
Abstract: Although the Cox proportional hazards model is well established and extensively used in the analysis of survival data, the proportional hazards (PH) assumption may not always hold in practical scenarios. The semiparametric transformation model extends the conventional Cox model and also includes many other survival models as special cases. This paper introduces a deep partially linear transformation model (DPLTM) as a general and flexible framework for estimation, inference and prediction. The proposed method is capable of avoiding the curse of dimensionality while still retaining the interpretability of some covariates of interest. We derive the overall convergence rate of the maximum likelihood estimators, the minimax lower bound of the nonparametric deep neural network (DNN) estimator, the asymptotic normality and the semiparametric efficiency of the parametric estimator. Comprehensive simulation studies demonstrate the impressive performance of the proposed estimation procedure in terms of both estimation accuracy and prediction power, which is further validated by an application to a real-world dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07611v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junkai Yin, Yue Zhang, Zhangsheng Yu</dc:creator>
    </item>
    <item>
      <title>Emperical Study on Various Symmetric Distributions for Modeling Time Series</title>
      <link>https://arxiv.org/abs/2412.07194</link>
      <description>arXiv:2412.07194v1 Announce Type: cross 
Abstract: This study evaluated probability distributions for modeling time series with abrupt structural changes. The Pearson type VII distribution, with an adjustable shape parameter $b$, proved versatile. The generalized Laplace distribution performed similarly to the Pearson model, occasionally surpassing it in terms of likelihood and AIC. Mixture models, including the mixture of $\delta$-function and Gaussian distribution, showed potential but were less stable. Pearson type VII and extended Laplace models were deemed more reliable for general cases. Model selection depends on data characteristics and goals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07194v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Genshiro Kitagawa (Tokyo University of Marine Science,Technology,The Institute of Statistical Mathmatics)</dc:creator>
    </item>
    <item>
      <title>Probabilistic Modelling of Multiple Long-Term Condition Onset Times</title>
      <link>https://arxiv.org/abs/2412.07657</link>
      <description>arXiv:2412.07657v1 Announce Type: cross 
Abstract: The co-occurrence of multiple long-term conditions (MLTC), or multimorbidity, in an individual can reduce their lifespan and severely impact their quality of life. Exploring the longitudinal patterns, e.g. clusters, of disease accrual can help better understand the genetic and environmental drivers of multimorbidity, and potentially identify individuals who may benefit from early targeted intervention. We introduce $\textit{probabilistic modelling of onset times}$, or $\texttt{ProMOTe}$, for clustering and forecasting MLTC trajectories. $\texttt{ProMOTe}$ seamlessly learns from incomplete and unreliable disease trajectories that is commonplace in Electronic Health Records but often ignored in existing longitudinal clustering methods. We analyse data from 150,000 individuals in the UK Biobank and identify 50 clusters showing patterns of disease accrual that have also been reported by some recent studies. We further discuss the forecasting capabilities of the model given the history of disease accrual.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07657v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kieran Richards, Kelly Fleetwood, Regina Prigge, Paolo Missier, Michael Barnes, Nick J. Reynolds, Bruce Guthrie, Sohan Seth</dc:creator>
    </item>
    <item>
      <title>Nearly Optimal Learning using Sparse Deep ReLU Networks in Regularized Empirical Risk Minimization with Lipschitz Loss</title>
      <link>https://arxiv.org/abs/2108.05990</link>
      <description>arXiv:2108.05990v5 Announce Type: replace 
Abstract: We propose a sparse deep ReLU network (SDRN) estimator of the regression function obtained from regularized empirical risk minimization with a Lipschitz loss function. Our framework can be applied to a variety of regression and classification problems. We establish novel non-asymptotic excess risk bounds for our SDRN estimator when the regression function belongs to a Sobolev space with mixed derivatives. We obtain a new nearly optimal risk rate in the sense that the SDRN estimator can achieve nearly the same optimal minimax convergence rate as one-dimensional nonparametric regression with the dimension only involved in a logarithm term when the feature dimension is fixed. The estimator has a slightly slower rate when the dimension grows with the sample size. We show that the depth of the SDRN estimator grows with the sample size in logarithmic order, and the total number of nodes and weights grows in polynomial order of the sample size to have the nearly optimal risk rate. The proposed SDRN can go deeper with fewer parameters to well estimate the regression and overcome the overfitting problem encountered by conventional feed-forward neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.05990v5</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ke Huang, Mingming Liu, Shujie Ma</dc:creator>
    </item>
    <item>
      <title>A novel decomposition to explain heterogeneity in observational and randomized studies of causality</title>
      <link>https://arxiv.org/abs/2208.05543</link>
      <description>arXiv:2208.05543v3 Announce Type: replace 
Abstract: This paper introduces a novel decomposition framework to explain heterogeneity in causal effects observed across different studies, considering both observational and randomized settings. We present a formal decomposition of between-study heterogeneity, identifying sources of variability in treatment effects across studies. The proposed methodology allows for robust estimation of causal parameters under various assumptions, addressing differences in pre-treatment covariate distributions, mediating variables, and the outcome mechanism. Our approach is validated through a simulation study and applied to data from the Moving to Opportunity (MTO) study, demonstrating its practical relevance. This work contributes to the broader understanding of causal inference in multi-study environments, with potential applications in evidence synthesis and policy-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.05543v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian Gilbert, Ivan D{\i}az, Kara E. Rudolph, Tat-Thang Vo</dc:creator>
    </item>
    <item>
      <title>A Bayesian Nonparametric Stochastic Block Model for Directed Acyclic Graphs</title>
      <link>https://arxiv.org/abs/2301.07513</link>
      <description>arXiv:2301.07513v2 Announce Type: replace 
Abstract: Random graphs have been widely used in statistics, for example in network and social interaction analysis. In some applications, data may contain an inherent hierarchical ordering among its vertices, which prevents any directed edge between pairs of vertices that do not respect this order. For example, in bibliometrics, older papers cannot cite newer ones. In such situations, the resulting graph forms a Directed Acyclic Graph. In this article, we propose an extension of the popular Stochastic Block Model (SBM) to account for the presence of a latent hierarchical ordering in the data. The proposed approach includes a topological ordering in the likelihood of the model, which allows a directed edge to have positive probability only if the corresponding pair of vertices respect the order. This latent ordering is treated as an unknown parameter and endowed with a prior distribution. We describe how to formalize the model and perform posterior inference for a Bayesian nonparametric version of the SBM in which both the hierarchical ordering and the number of latent blocks are learnt from the data. Finally, an illustration with a real-world dataset from bibliometrics is presented. Additional supplementary materials are available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.07513v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Clement Lee, Marco Battiston</dc:creator>
    </item>
    <item>
      <title>Efficient Inference on High-Dimensional Linear Models with Missing Outcomes</title>
      <link>https://arxiv.org/abs/2309.06429</link>
      <description>arXiv:2309.06429v3 Announce Type: replace 
Abstract: This paper is concerned with inference on the regression function of a high-dimensional linear model when outcomes are missing at random. We propose an estimator which combines a Lasso pilot estimate of the regression function with a bias correction term based on the weighted residuals of the Lasso regression. The weights depend on estimates of the missingness probabilities (propensity scores) and solve a convex optimization program that trades off bias and variance optimally. Provided that the propensity scores can be pointwise consistently estimated at in-sample data points, our proposed estimator for the regression function is asymptotically normal and semi-parametrically efficient among all asymptotically linear estimators. Furthermore, the proposed estimator keeps its asymptotic properties even if the propensity scores are estimated by modern machine learning techniques. We validate the finite-sample performance of the proposed estimator through comparative simulation studies and the real-world problem of inferring the stellar masses of galaxies in the Sloan Digital Sky Survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.06429v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yikun Zhang, Alexander Giessing, Yen-Chi Chen</dc:creator>
    </item>
    <item>
      <title>Weighted Q-learning for optimal dynamic treatment regimes with nonignorable missing covariates</title>
      <link>https://arxiv.org/abs/2312.01735</link>
      <description>arXiv:2312.01735v5 Announce Type: replace 
Abstract: Dynamic treatment regimes (DTRs) formalize medical decision-making as a sequence of rules for different stages, mapping patient-level information to recommended treatments. In practice, estimating an optimal DTR using observational data from electronic medical record (EMR) databases can be complicated by nonignorable missing covariates resulting from informative monitoring of patients. Since complete case analysis can provide consistent estimation of outcome model parameters under the assumption of outcome-independent missingness, Q-learning is a natural approach to accommodating nonignorable missing covariates. However, the backward induction algorithm used in Q-learning can introduce challenges, as nonignorable missing covariates at later stages can result in nonignorable missing pseudo-outcomes at earlier stages, leading to suboptimal DTRs, even if the longitudinal outcome variables are fully observed. To address this unique missing data problem in DTR settings, we propose two weighted Q-learning approaches where inverse probability weights for missingness of the pseudo-outcomes are obtained through estimating equations with valid nonresponse instrumental variables or sensitivity analysis. The asymptotic properties of the weighted Q-learning estimators are derived, and the finite-sample performance of the proposed methods is evaluated and compared with alternative methods through extensive simulation studies. Using EMR data from the Medical Information Mart for Intensive Care database, we apply the proposed methods to investigate the optimal fluid strategy for sepsis patients in intensive care units.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01735v5</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jian Sun, Bo Fu, Li Su</dc:creator>
    </item>
    <item>
      <title>Principal Component Copulas for Capital Modelling and Systemic Risk</title>
      <link>https://arxiv.org/abs/2312.13195</link>
      <description>arXiv:2312.13195v2 Announce Type: replace 
Abstract: We introduce a class of copulas that we call Principal Component Copulas (PCCs). This class combines the strong points of copula-based techniques with principal component-based models, which results in flexibility when modelling tail dependence along the most important directions in high-dimensional data. We obtain theoretical results for PCCs that are important for practical applications. In particular, we derive tractable expressions for the high-dimensional copula density, which can be represented in terms of characteristic functions. We also develop algorithms to perform Maximum Likelihood and Generalized Method of Moment estimation in high-dimensions and show very good performance in simulation experiments. Finally, we apply the copula to the international stock market in order to study systemic risk. We find that PCCs lead to excellent performance on measures of systemic risk due to their ability to distinguish between parallel market movements, which increase systemic risk, and orthogonal movements, which reduce systemic risk. As a result, we consider the PCC promising for internal capital models, which financial institutions use to protect themselves against systemic risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13195v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>q-fin.RM</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>K. B. Gubbels, J. Y. Ypma, C. W. Oosterlee</dc:creator>
    </item>
    <item>
      <title>A consensus-constrained parsimonious Gaussian mixture model for clustering hyperspectral images</title>
      <link>https://arxiv.org/abs/2403.03349</link>
      <description>arXiv:2403.03349v2 Announce Type: replace 
Abstract: The use of hyperspectral imaging to investigate food samples has grown due to the improved performance and lower cost of instrumentation. Food engineers use hyperspectral images to classify the type and quality of a food sample, typically using classification methods. In order to train these methods, every pixel in each training image needs to be labelled. Typically, computationally cheap threshold-based approaches are used to label the pixels, and classification methods are trained based on those labels. However, threshold-based approaches are subjective and cannot be generalized across hyperspectral images taken in different conditions and of different foods. Here a consensus-constrained parsimonious Gaussian mixture model (ccPGMM) is proposed to label pixels in hyperspectral images using a model-based clustering approach. The ccPGMM utilizes information that is available on some pixels and specifies constraints on those pixels belonging to the same or different clusters while clustering the rest of the pixels in the image. A latent variable model is used to represent the high-dimensional data in terms of a small number of underlying latent factors. To ensure computational feasibility, a consensus clustering approach is employed, where the data are divided into multiple randomly selected subsets of variables and constrained clustering is applied to each data subset; the clustering results are then consolidated across all data subsets to provide a consensus clustering solution. The ccPGMM approach is applied to simulated datasets and real hyperspectral images of three types of puffed cereal, corn, rice, and wheat. Improved clustering performance and computational efficiency are demonstrated when compared to other current state-of-the-art approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03349v2</guid>
      <category>stat.ME</category>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ganesh Babu, Aoife Gowen, Michael Fop, Isobel Claire Gormley</dc:creator>
    </item>
    <item>
      <title>Identification and estimation of mediational effects of longitudinal modified treatment policies</title>
      <link>https://arxiv.org/abs/2403.09928</link>
      <description>arXiv:2403.09928v3 Announce Type: replace 
Abstract: We demonstrate a comprehensive semiparametric approach to causal mediation analysis, addressing the complexities inherent in settings with longitudinal and continuous treatments, confounders, and mediators. Our methodology utilizes a nonparametric structural equation model and a cross-fitted sequential regression technique based on doubly robust pseudo-outcomes, yielding an efficient, asymptotically normal estimator without relying on restrictive parametric modeling assumptions. We are motivated by a recent scientific controversy regarding the effects of invasive mechanical ventilation (IMV) on the survival of COVID-19 patients, considering acute kidney injury (AKI) as a mediating factor. We highlight the possibility of "inconsistent mediation," in which the direct and indirect effects of the exposure operate in opposite directions. We discuss the significance of mediation analysis for scientific understanding and its potential utility in treatment decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09928v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian Gilbert, Katherine L. Hoffman, Nicholas Williams, Kara E. Rudolph, Edward J. Schenck, Iv\'an D\'iaz</dc:creator>
    </item>
    <item>
      <title>Data-Adaptive Identification of Effect Modifiers through Stochastic Shift Interventions and Cross-Validated Targeted Learning</title>
      <link>https://arxiv.org/abs/2406.10792</link>
      <description>arXiv:2406.10792v2 Announce Type: replace 
Abstract: In epidemiology, identifying subpopulations that are particularly vulnerable to exposures and those who may benefit differently from exposure-reducing interventions is essential. Factors such as age, gender-specific vulnerabilities, and physiological states such as pregnancy are critical for policymakers when setting regulatory guidelines. However, current semi-parametric methods for estimating heterogeneous treatment effects are often limited to binary exposures and can function as black boxes, lacking clear, interpretable rules for subpopulation-specific policy interventions. This study introduces a novel method that uses cross-validated targeted minimum loss-based estimation (TMLE) paired with a data-adaptive target parameter strategy to identify subpopulations with the most significant differential impact of simulated policy interventions that reduce exposure. Our approach is assumption-lean, allowing for the integration of machine learning while still yielding valid confidence intervals. We demonstrate the robustness of our methodology through simulations and application to data from the National Health and Nutrition Examination Survey. Our analysis of NHANES data on persistent organic pollutants (POPs) and leukocyte telomere length (LTL) identified age as a significant effect modifier. Specifically, we found that exposure to 3,3',4,4',5-pentachlorobiphenyl (PCNB) consistently had a differential impact on LTL, with a one-standard deviation reduction in exposure leading to a more pronounced increase in LTL among younger populations compared to older ones. We offer our method as an open-source software package, EffectXshift, enabling researchers to investigate the effect modification of continuous exposures. The EffectXshift package provides clear and interpretable results, informing targeted public health interventions and policy decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10792v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David McCoy, Wenxin Zhang, Alan Hubbard, Mark van der Laan, Alejandro Schuler</dc:creator>
    </item>
    <item>
      <title>Improve Sensitivity Analysis Synthesizing Randomized Clinical Trials With Limited Overlap</title>
      <link>https://arxiv.org/abs/2409.07391</link>
      <description>arXiv:2409.07391v3 Announce Type: replace 
Abstract: Randomized clinical trials are the gold standard when estimating the average treatment effect. However, they are usually not a random sample from the real-world population because of the inclusion/exclusion rules. Meanwhile, observational studies typically consist of representative samples from the real-world population. However, due to unmeasured confounding, sensitivity analysis is often used to estimate bounds for the average treatment effect without relying on stringent assumptions of other existing methods. This article introduces a synthesis estimator that improves sensitivity analysis in observational studies by incorporating randomized clinical trial data, even when overlap in covariate distribution is limited due to inclusion/exclusion criteria. We show that the proposed estimator will give a tighter bound when a "separability" condition holds for the sensitivity parameter. Theoretical proofs and simulations show that this method provides a tighter bound than the sensitivity analysis using only observational study. We apply this method to combine an observational study on drug effectiveness with a partially overlapping RCT dataset, yielding improved average treatment effect bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07391v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kuan Jiang, Wenjie Hu, Shu Yang, Xinxing Lai, Xiaohua Zhou</dc:creator>
    </item>
    <item>
      <title>Optimal heteroskedasticity testing in nonparametric regression</title>
      <link>https://arxiv.org/abs/2310.12424</link>
      <description>arXiv:2310.12424v3 Announce Type: replace-cross 
Abstract: Heteroskedasticity testing in nonparametric regression is a classic statistical problem with important practical applications, yet fundamental limits are unknown. Adopting a minimax perspective, this article considers the testing problem in the context of an $\alpha$-H\"{o}lder mean and a $\beta$-H\"{o}lder variance function. For $\alpha &gt; 0$ and $\beta \in (0, 1/2)$, the sharp minimax separation rate $n^{-4\alpha} + n^{-4\beta/(4\beta+1)} + n^{-2\beta}$ is established. To achieve the minimax separation rate, a kernel-based statistic using first-order squared differences is developed. Notably, the statistic estimates a proxy rather than a natural quadratic functional (the squared distance between the variance function and its best $L^2$ approximation by a constant) suggested in previous work.
  The setting where no smoothness is assumed on the variance function is also studied; the variance profile across the design points can be arbitrary. Despite the lack of structure, consistent testing turns out to still be possible by using the Gaussian character of the noise, and the minimax rate is shown to be $n^{-4\alpha} + n^{-1/2}$. Exploiting noise information happens to be a fundamental necessity as consistent testing is impossible if nothing more than zero mean and unit variance is known about the noise distribution. Furthermore, in the setting where the variance function is $\beta$-H\"{o}lder but heteroskedasticity is measured only with respect to the design points, the minimax separation rate is shown to be $n^{-4\alpha} + n^{-\left((1/2) \vee (4\beta/(4\beta+1))\right)}$ when the noise is Gaussian and $n^{-4\alpha} + n^{-4\beta/(4\beta+1)} + n^{-2\beta}$ when the noise distribution is unknown.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12424v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subhodh Kotekal, Soumyabrata Kundu</dc:creator>
    </item>
    <item>
      <title>Simulating Relational Event Histories: Why and How</title>
      <link>https://arxiv.org/abs/2403.19329</link>
      <description>arXiv:2403.19329v3 Announce Type: replace-cross 
Abstract: Many important social phenomena are characterized by repeated interactions among individuals over time such as email exchanges in an organization or face-to-face interactions in a classroom. To understand the underlying mechanisms of social interaction dynamics, statistical simulation techniques of longitudinal network data on a fine temporal granularity are crucially important. This paper makes two contributions to the field. First, we present statistical frameworks to simulate relational event networks under dyadic and actor-oriented relational event models which are implemented in a new R package 'remulate'. Second, we explain how the simulation framework can be used to address challenging problems in temporal social network analysis, such as model fit assessment, theory building, network intervention planning, making predictions, understanding the impact of network structures, to name a few. This is shown in three extensive case studies. In the first study, it is elaborated why simulation-based techniques are crucial for relational event model assessment which is illustrated for a network of criminal gangs. In the second study, it is shown how simulation techniques are important when building and extending theories about social phenomena which is illustrated via optimal distinctiveness theory. In the third study, we demonstrate how simulation techniques contribute to a better understanding of the longevity and the potential effect sizes of network interventions. Through these case studies and software, researchers will be able to better understand social interaction dynamics using relational event data from real-life networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19329v3</guid>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rumana Lakdawala, Joris Mulder, Roger Leenders</dc:creator>
    </item>
  </channel>
</rss>
