<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 May 2024 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 10 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A note on the minimax risk of sparse linear regression</title>
      <link>https://arxiv.org/abs/2405.05344</link>
      <description>arXiv:2405.05344v1 Announce Type: new 
Abstract: Sparse linear regression is one of the classical and extensively studied problems in high-dimensional statistics and compressed sensing. Despite the substantial body of literature dedicated to this problem, the precise determination of its minimax risk remains elusive. This paper aims to fill this gap by deriving asymptotically constant-sharp characterization for the minimax risk of sparse linear regression. More specifically, the paper focuses on scenarios where the sparsity level, denoted as k, satisfies the condition $(k \log p)/n {\to} 0$, with p and n representing the number of features and observations respectively. We establish that the minimax risk under isotropic Gaussian random design is asymptotically equal to $2{\sigma}^2k/n log(p/k)$, where ${\sigma}$ denotes the standard deviation of the noise. In addition to this result, we will summarize the existing results in the literature, and mention some of the fundamental problems that have still remained open.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05344v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yilin Guo, Shubhangi Ghosh, Haolei Weng, Arian Maleki</dc:creator>
    </item>
    <item>
      <title>Decompounding Under General Mixing Distributions</title>
      <link>https://arxiv.org/abs/2405.05419</link>
      <description>arXiv:2405.05419v1 Announce Type: new 
Abstract: This study focuses on statistical inference for compound models of the form $X=\xi_1+\ldots+\xi_N$, where $N$ is a random variable denoting the count of summands, which are independent and identically distributed (i.i.d.) random variables $\xi_1, \xi_2, \ldots$. The paper addresses the problem of reconstructing the distribution of $\xi$ from observed samples of $X$'s distribution, a process referred to as decompounding, with the assumption that $N$'s distribution is known. This work diverges from the conventional scope by not limiting $N$'s distribution to the Poisson type, thus embracing a broader context. We propose a nonparametric estimate for the density of $\xi$, derive its rates of convergence and prove that these rates are minimax optimal for suitable classes of distributions for $\xi$ and $N$. Finally, we illustrate the numerical performance of the algorithm on simulated examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05419v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Denis Belomestny, Ekaterina Morozova, Vladimir Panov</dc:creator>
    </item>
    <item>
      <title>The empirical copula process in high dimensions: Stute's representation and applications</title>
      <link>https://arxiv.org/abs/2405.05597</link>
      <description>arXiv:2405.05597v1 Announce Type: new 
Abstract: The empirical copula process, a fundamental tool for copula inference, is studied in the high dimensional regime where the dimension is allowed to grow to infinity exponentially in the sample size. Under natural, weak smoothness assumptions on the underlying copula, it is shown that Stute's representation is valid in the following sense: all low-dimensional margins of fixed dimension of the empirical copula process can be approximated by a functional of the low-dimensional margins of the standard empirical process, with the almost sure error term being uniform in the margins. The result has numerous potential applications, and is exemplary applied to the problem of testing pairwise stochastic independence in high dimensions, leading to various extensions of recent results in the literature: for certain test statistics based on pairwise association measures, type-I error control is obtained for models beyond mutual independence. Moreover, bootstrap-based critical values are shown to yield strong control of the familywise error rate for a large class of data generating processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05597v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Axel B\"ucher, Cambyse Pakzad</dc:creator>
    </item>
    <item>
      <title>Estimation of ill-conditioned models using penalized sums of squares of the residuals</title>
      <link>https://arxiv.org/abs/2405.05644</link>
      <description>arXiv:2405.05644v1 Announce Type: new 
Abstract: This paper analyzes the estimation of econometric models by penalizing the sum of squares of the residuals with a factor that makes the model estimates approximate those that would be obtained when considering the possible simple regressions between the dependent variable of the econometric model and each of its independent variables. It is shown that the ridge estimator is a particular case of the penalized estimator obtained, which, upon analysis of its main characteristics, presents better properties than the ridge especially in reference to the individual boostrap inference of the coefficients of the model and the numerical stability of the estimates obtained. This improvement is due to the fact that instead of shrinking the estimator towards zero, the estimator shrinks towards the estimates of the coefficients of the simple regressions discussed above.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05644v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rom\'an Salmer\'on G\'omez, Catalina B. Garc\'ia Garc\'ia</dc:creator>
    </item>
    <item>
      <title>Consistent Empirical Bayes estimation of the mean of a mixing distribution without identifiability assumption. With applications to treatment of non-response</title>
      <link>https://arxiv.org/abs/2405.05656</link>
      <description>arXiv:2405.05656v1 Announce Type: new 
Abstract: {\bf Abstract}
  Consider a Non-Parametric Empirical Bayes (NPEB) setup. We observe $Y_i, \sim f(y|\theta_i)$, $\theta_i \in \Theta$ independent, where $\theta_i \sim G$ are independent $i=1,...,n$. The mixing distribution $G$ is unknown $G \in \{G\}$ with no parametric assumptions about the class $\{G \}$. The common NPEB task is to estimate $\theta_i, \; i=1,...,n$. Conditions that imply 'optimality' of such NPEB estimators typically require identifiability of $G$ based on $Y_1,...,Y_n$. We consider the task of estimating $E_G \theta$. We show that `often' consistent estimation of $E_G \theta$ is implied without identifiability.
  We motivate the later task, especially in setups with non-response and missing data. We demonstrate consistency in simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05656v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eitan Greenshtein</dc:creator>
    </item>
    <item>
      <title>Non-asymptotic estimates for accelerated high order Langevin Monte Carlo algorithms</title>
      <link>https://arxiv.org/abs/2405.05679</link>
      <description>arXiv:2405.05679v1 Announce Type: new 
Abstract: In this paper, we propose two new algorithms, namely aHOLA and aHOLLA, to sample from high-dimensional target distributions with possibly super-linearly growing potentials. We establish non-asymptotic convergence bounds for aHOLA in Wasserstein-1 and Wasserstein-2 distances with rates of convergence equal to $1+q/2$ and $1/2+q/4$, respectively, under a local H\"{o}lder condition with exponent $q\in(0,1]$ and a convexity at infinity condition on the potential of the target distribution. Similar results are obtained for aHOLLA under certain global continuity conditions and a dissipativity condition. Crucially, we achieve state-of-the-art rates of convergence of the proposed algorithms in the non-convex setting which are higher than those of the existing algorithms. Numerical experiments are conducted to sample from several distributions and the results support our main findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05679v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariel Neufeld, Ying Zhang</dc:creator>
    </item>
    <item>
      <title>A fast and accurate inferential method for complex parametric models: the implicit bootstrap</title>
      <link>https://arxiv.org/abs/2405.05403</link>
      <description>arXiv:2405.05403v1 Announce Type: cross 
Abstract: Performing inference such a computing confidence intervals is traditionally done, in the parametric case, by first fitting a model and then using the estimates to compute quantities derived at the asymptotic level or by means of simulations such as the ones from the family of bootstrap methods. These methods require the derivation and computation of a consistent estimator that can be very challenging to obtain when the models are complex as is the case for example when the data exhibit some types of features such as censoring, missclassification errors or contain outliers. In this paper, we propose a simulation based inferential method, the implicit bootstrap, that bypasses the need to compute a consistent estimator and can therefore be easily implemented. While being transformation respecting, we show that under similar conditions as for the studentized bootstrap, without the need of a consistent estimator, the implicit bootstrap is first and second order accurate. Using simulation studies, we also show the coverage accuracy of the method with data settings for which traditional methods are computationally very involving and also lead to poor coverage, especially when the sample size is relatively small. Based on these empirical results, we also explore theoretically the case of exact inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05403v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuel Orso, Mucyo Karemera, Maria-Pia Victoria-Feser, St\'ephane Guerrier</dc:creator>
    </item>
    <item>
      <title>Estimation and Inference for Change Points in Functional Regression Time Series</title>
      <link>https://arxiv.org/abs/2405.05459</link>
      <description>arXiv:2405.05459v1 Announce Type: cross 
Abstract: In this paper, we study the estimation and inference of change points under a functional linear regression model with changes in the slope function. We present a novel Functional Regression Binary Segmentation (FRBS) algorithm which is computationally efficient as well as achieving consistency in multiple change point detection. This algorithm utilizes the predictive power of piece-wise constant functional linear regression models in the reproducing kernel Hilbert space framework. We further propose a refinement step that improves the localization rate of the initial estimator output by FRBS, and derive asymptotic distributions of the refined estimators for two different regimes determined by the magnitude of a change. To facilitate the construction of confidence intervals for underlying change points based on the limiting distribution, we propose a consistent block-type long-run variance estimator. Our theoretical justifications for the proposed approach accommodate temporal dependence and heavy-tailedness in both the functional covariates and the measurement errors. Empirical effectiveness of our methodology is demonstrated through extensive simulation studies and an application to the Standard and Poor's 500 index dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05459v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shivam Kumar, Haotian Xu, Haeran Cho, Daren Wang</dc:creator>
    </item>
    <item>
      <title>Characteristic Learning for Provable One Step Generation</title>
      <link>https://arxiv.org/abs/2405.05512</link>
      <description>arXiv:2405.05512v1 Announce Type: cross 
Abstract: We propose the characteristic generator, a novel one-step generative model that combines the efficiency of sampling in Generative Adversarial Networks (GANs) with the stable performance of flow-based models. Our model is driven by characteristics, along which the probability density transport can be described by ordinary differential equations (ODEs). Specifically, We estimate the velocity field through nonparametric regression and utilize Euler method to solve the probability flow ODE, generating a series of discrete approximations to the characteristics. We then use a deep neural network to fit these characteristics, ensuring a one-step mapping that effectively pushes the prior distribution towards the target distribution. In the theoretical aspect, we analyze the errors in velocity matching, Euler discretization, and characteristic fitting to establish a non-asymptotic convergence rate for the characteristic generator in 2-Wasserstein distance. To the best of our knowledge, this is the first thorough analysis for simulation-free one step generative models. Additionally, our analysis refines the error analysis of flow-based generative models in prior works. We apply our method on both synthetic and real datasets, and the results demonstrate that the characteristic generator achieves high generation quality with just a single evaluation of neural network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05512v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhao Ding, Chenguang Duan, Yuling Jiao, Ruoxuan Li, Jerry Zhijian Yang, Pingwen Zhang</dc:creator>
    </item>
    <item>
      <title>On the Computational Complexity of Metropolis-Adjusted Langevin Algorithms for Bayesian Posterior Sampling</title>
      <link>https://arxiv.org/abs/2206.06491</link>
      <description>arXiv:2206.06491v2 Announce Type: replace 
Abstract: In this paper, we examine the computational complexity of sampling from a Bayesian posterior (or pseudo-posterior) using the Metropolis-adjusted Langevin algorithm (MALA). MALA first employs a discrete-time Langevin SDE to propose a new state, and then adjusts the proposed state using Metropolis-Hastings rejection. Most existing theoretical analyses of MALA rely on the smoothness and strong log-concavity properties of the target distribution, which are often lacking in practical Bayesian problems. Our analysis hinges on statistical large sample theory, which constrains the deviation of the Bayesian posterior from being smooth and log-concave in a very specific way. In particular, we introduce a new technique for bounding the mixing time of a Markov chain with a continuous state space via the $s$-conductance profile, offering improvements over existing techniques in several aspects. By employing this new technique, we establish the optimal parameter dimension dependence of $d^{1/3}$ and condition number dependence of $\kappa$ in the non-asymptotic mixing time upper bound for MALA after the burn-in period, under a standard Bayesian setting where the target posterior distribution is close to a $d$-dimensional Gaussian distribution with a covariance matrix having a condition number $\kappa$. We also prove a matching mixing time lower bound for sampling from a multivariate Gaussian via MALA to complement the upper bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.06491v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rong Tang, Yun Yang</dc:creator>
    </item>
    <item>
      <title>Multiple testing under negative dependence</title>
      <link>https://arxiv.org/abs/2212.09706</link>
      <description>arXiv:2212.09706v4 Announce Type: replace 
Abstract: The multiple testing literature has primarily dealt with three types of dependence assumptions between p-values: independence, positive regression dependence, and arbitrary dependence. In this paper, we provide what we believe are the first theoretical results under various notions of negative dependence (negative Gaussian dependence, negative regression dependence, negative association, negative orthant dependence and weak negative dependence). These include the Simes global null test and the Benjamini-Hochberg procedure, which are known experimentally to be anti-conservative under negative dependence. The anti-conservativeness of these procedures is bounded by factors smaller than that under arbitrary dependence (in particular, by factors independent of the number of hypotheses). We also provide new results about negatively dependent e-values, and provide several examples as to when negative dependence may arise. Our proofs are elementary and short, thus amenable to extensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.09706v4</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyu Chi, Aaditya Ramdas, Ruodu Wang</dc:creator>
    </item>
    <item>
      <title>Codivergences and information matrices</title>
      <link>https://arxiv.org/abs/2303.08122</link>
      <description>arXiv:2303.08122v3 Announce Type: replace 
Abstract: We propose a new concept of codivergence, which quantifies the similarity between two probability measures $P_1, P_2$ relative to a reference probability measure $P_0$. In the neighborhood of the reference measure $P_0$, a codivergence behaves like an inner product between the measures $P_1 - P_0$ and $P_2 - P_0$. Codivergences of covariance-type and correlation-type are introduced and studied with a focus on two specific correlation-type codivergences, the $\chi^2$-codivergence and the Hellinger codivergence. We derive explicit expressions for several common parametric families of probability distributions. For a codivergence, we introduce moreover the divergence matrix as an analogue of the Gram matrix. It is shown that the $\chi^2$-divergence matrix satisfies a data-processing inequality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.08122v3</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexis Derumigny, Johannes Schmidt-Hieber</dc:creator>
    </item>
    <item>
      <title>Jittering and Clustering: Strategies for the Construction of Robust Designs</title>
      <link>https://arxiv.org/abs/2309.08538</link>
      <description>arXiv:2309.08538v5 Announce Type: replace 
Abstract: We discuss, and give examples of, methods for randomly implementing some minimax robust designs from the literature. These have the advantage, over their deterministic counterparts, of having bounded maximum loss in large and very rich neighbourhoods of the, almost certainly inexact, response model fitted by the experimenter. Their maximum loss rivals that of the theoretically best possible, but not implementable, minimax designs. The procedures are then extended to more general robust designs. For two-dimensional designs we sample from contractions of Voronoi tessellations, generated by selected basis points, which partition the design space. These ideas are then extended to $k$-dimensional designs for general k.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08538v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Douglas Wiens</dc:creator>
    </item>
    <item>
      <title>Multivariate generalized Pareto distributions along extreme directions</title>
      <link>https://arxiv.org/abs/2311.04618</link>
      <description>arXiv:2311.04618v2 Announce Type: replace 
Abstract: When modeling a vector of risk variables, extreme scenarios are often of special interest. The peaks-over-thresholds method hinges on the notion that, asymptotically, the excesses over a vector of high thresholds follow a multivariate generalized Pareto distribution. However, existing literature has primarily concentrated on the setting when all risk variables are always large simultaneously. In reality, this assumption is often not met, especially in high dimensions.
  In response to this limitation, we study scenarios where distinct groups of risk variables may exhibit joint extremes while others do not. These discernible groups are derived from the angular measure inherent in the corresponding max-stable distribution, whence the term extreme direction. We explore such extreme directions within the framework of multivariate generalized Pareto distributions, with a focus on their probability density functions in relation to an appropriate dominating measure.
  Furthermore, we provide a stochastic construction that allows any prespecified set of risk groups to constitute the distribution's extreme directions. This construction takes the form of a smoothed max-linear model and accommodates the full spectrum of conceivable max-stable dependence structures. Additionally, we introduce a generic simulation algorithm tailored for multivariate generalized Pareto distributions, offering specific implementations for extensions of the logistic and H\"usler-Reiss families capable of carrying arbitrary extreme directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04618v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anas Mourahib, Anna Kiriliouk, Johan Segers</dc:creator>
    </item>
    <item>
      <title>Regularized Stein Variational Gradient Flow</title>
      <link>https://arxiv.org/abs/2211.07861</link>
      <description>arXiv:2211.07861v2 Announce Type: replace-cross 
Abstract: The Stein Variational Gradient Descent (SVGD) algorithm is a deterministic particle method for sampling. However, a mean-field analysis reveals that the gradient flow corresponding to the SVGD algorithm (i.e., the Stein Variational Gradient Flow) only provides a constant-order approximation to the Wasserstein Gradient Flow corresponding to the KL-divergence minimization. In this work, we propose the Regularized Stein Variational Gradient Flow, which interpolates between the Stein Variational Gradient Flow and the Wasserstein Gradient Flow. We establish various theoretical properties of the Regularized Stein Variational Gradient Flow (and its time-discretization) including convergence to equilibrium, existence and uniqueness of weak solutions, and stability of the solutions. We provide preliminary numerical evidence of the improved performance offered by the regularization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.07861v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye He, Krishnakumar Balasubramanian, Bharath K. Sriperumbudur, Jianfeng Lu</dc:creator>
    </item>
    <item>
      <title>Stability and Statistical Inversion of Travel time Tomography</title>
      <link>https://arxiv.org/abs/2309.12544</link>
      <description>arXiv:2309.12544v3 Announce Type: replace-cross 
Abstract: In this paper, we consider the travel time tomography problem for conformal metrics on a bounded domain, which seeks to determine the conformal factor of the metric from the lengths of geodesics joining boundary points. We establish forward and inverse stability estimates for simple conformal metrics under some a priori conditions. We then apply the stability estimates to show the consistency of a Bayesian statistical inversion technique for travel time tomography with discrete, noisy measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.12544v3</guid>
      <category>math.DG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashwin Tarikere, Hanming Zhou</dc:creator>
    </item>
  </channel>
</rss>
