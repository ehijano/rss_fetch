<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Oct 2025 01:43:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Centered MA Dirichlet ARMA for Financial Compositions: Theory &amp; Empirical Evidence</title>
      <link>https://arxiv.org/abs/2510.18903</link>
      <description>arXiv:2510.18903v1 Announce Type: new 
Abstract: Observation-driven Dirichlet models for compositional time series often use the additive log-ratio (ALR) link and include a moving-average (MA) term built from ALR residuals. In the standard B--DARMA recursion, the usual MA regressor $\alr(\mathbf{Y}_t)-\boldsymbol{\eta}_t$ has nonzero conditional mean under the Dirichlet likelihood, which biases the mean path and blurs the interpretation of MA coefficients. We propose a minimal change: replace the raw regressor with a \emph{centered} innovation $\boldsymbol{\epsilon}_t^{\circ}=\alr(\mathbf{Y}_t)-\mathbb{E}\{\alr(\mathbf{Y}_t)\mid \boldsymbol{\eta}_t,\phi_t\}$, computable in closed form via digamma functions. Centering restores mean-zero innovations for the MA block without altering either the likelihood or the ALR link. We provide simple identities for the conditional mean and the forecast recursion, show first-order equivalence to a digamma-link DARMA while retaining a closed-form inverse to $\boldsymbol{\mu}_t$, and give ready-to-use code. A weekly application to the Federal Reserve H.8 bank-asset composition compares the original (raw-MA) and centered specifications under a fixed holdout and rolling one-step origins. The centered formulation improves log predictive scores with essentially identical point error and markedly cleaner Hamiltonian Monte Carlo diagnostics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18903v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison Katz</dc:creator>
    </item>
    <item>
      <title>Estimation of causal dose-response functions under data fusion</title>
      <link>https://arxiv.org/abs/2510.19094</link>
      <description>arXiv:2510.19094v1 Announce Type: new 
Abstract: Estimating the causal dose-response function is challenging, particularly when data from a single source are insufficient to estimate responses precisely across all exposure levels. To overcome this limitation, we propose a data fusion framework that leverages multiple data sources that are partially aligned with the target distribution. Specifically, we derive a Neyman-orthogonal loss function tailored for estimating the dose-response function within data fusion settings. To improve computational efficiency, we propose a stochastic approximation that retains orthogonality. We apply kernel ridge regression with this approximation, which provides closed-form estimators. Our theoretical analysis demonstrates that incorporating additional data sources yields tighter finite-sample regret bounds and improved worst-case performance, as confirmed via minimax lower bound comparison. Simulation studies validate the practical advantages of our approach, showing improved estimation accuracy when employing data fusion. This study highlights the potential of data fusion for estimating non-smooth parameters such as causal dose-response functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19094v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaewon Lim, Alex Luedtke</dc:creator>
    </item>
    <item>
      <title>Spatially Regularized Gaussian Mixtures for Clustering Spatial Transcriptomic Data</title>
      <link>https://arxiv.org/abs/2510.19108</link>
      <description>arXiv:2510.19108v1 Announce Type: new 
Abstract: Spatial transcriptomics measures the expression of thousands of genes in a tissue sample while preserving its spatial structure. This class of technologies has enabled the investigation of the spatial variation of gene expressions and their impact on specific biological processes. Identifying genes with similar expression profiles is of utmost importance, thus motivating the development of flexible methods leveraging spatial data structure to cluster genes. Here, we propose a modeling framework for clustering observations measured over numerous spatial locations via Gaussian processes. Rather than specifying their covariance kernels as a function of the spatial structure, we use it to inform a generalized Cholesky decomposition of their precision matrices. This approach prevents issues with kernel misspecification and facilitates the estimation of a non-stationarity spatial covariance structure. Applied to spatial transcriptomic data, our model identifies gene clusters with distinctive spatial correlation patterns across tissue areas comprising different cell types, like tumoral and stromal areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19108v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrea Sottosanti, Davide Risso, Francesco Denti</dc:creator>
    </item>
    <item>
      <title>Efficient scenario analysis in real-time Bayesian election forecasting via sequential meta-posterior sampling</title>
      <link>https://arxiv.org/abs/2510.19133</link>
      <description>arXiv:2510.19133v1 Announce Type: new 
Abstract: Bayesian aggregation lets election forecasters combine diverse sources of information, such as state polls and economic and political indicators: as in our collaboration with The Economist magazine. However, the demands of real-time posterior updating, model checking, and communication introduce practical methodological challenges. In particular, sensitivity and scenario analysis help trace forecast shifts to model assumptions and understand model behavior. Yet, under standard Markov chain Monte Carlo, even small tweaks to the model (e.g., in priors, data, hyperparameters) require full refitting, making such real-time analysis computationally expensive. To overcome the bottleneck, we introduce a meta-modeling strategy paired with a sequential sampling scheme; by traversing posterior meta-models, we enable real-time inference and structured scenario and sensitivity analysis without repeated refitting. In a back-test of the model, we show substantial computational gains and uncover non-trivial sensitivity patterns. For example, forecasts remain responsive to prior confidence in fundamentals-based forecasts, but less so to random walk scale; these help clarify the relative influence of polling data versus structural assumptions. Code is available at https://github.com/geonhee619/SMC-Sense.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19133v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geonhee Han, Andrew Gelman, Aki Vehtari</dc:creator>
    </item>
    <item>
      <title>Inverse-intensity weighted generalized estimating equations with irregularly measured longitudinal data and informative dropout</title>
      <link>https://arxiv.org/abs/2510.19154</link>
      <description>arXiv:2510.19154v1 Announce Type: new 
Abstract: Longitudinal data are commonly encountered in biomedical research, including randomized trials and retrospective cohort studies. Subjects are typically followed over a period of time and may be scheduled for follow-up at pre-determined time points. However, subjects may miss their appointments or return at non-specified times, leading to irregularity in the visit process. IIW-GEEs have been developed as one method to account for this irregularity, whereby estimates from a visit intensity model are used as weights in a GEE model with an independent correlation structure. We show that currently available methods can be biased for situations in which the health outcome of interest may influence a subject's dropout from the study. We have extended the IIW-GEE framework to adjust for informative dropout and have demonstrated via simulation studies that this bias can be significantly reduced. We have illustrated this method using the STAR*D clinical trial data, and observed that the disease trajectory was generally overestimated when informative dropout was not accounted for.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19154v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>George Stefan, Eleanor Pullenayegum</dc:creator>
    </item>
    <item>
      <title>No Intelligence Without Statistics: The Invisible Backbone of Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2510.19212</link>
      <description>arXiv:2510.19212v1 Announce Type: new 
Abstract: The rapid ascent of artificial intelligence (AI) is often portrayed as a revolution born from computer science and engineering. This narrative, however, obscures a fundamental truth: the theoretical and methodological core of AI is, and has always been, statistical. This paper systematically argues that the field of statistics provides the indispensable foundation for machine learning and modern AI. We deconstruct AI into nine foundational pillars-Inference, Density Estimation, Sequential Learning, Generalization, Representation Learning, Interpretability, Causality, Optimization, and Unification-demonstrating that each is built upon century-old statistical principles. From the inferential frameworks of hypothesis testing and estimation that underpin model evaluation, to the density estimation roots of clustering and generative AI; from the time-series analysis inspiring recurrent networks to the causal models that promise true understanding, we trace an unbroken statistical lineage. While celebrating the computational engines that power modern AI, we contend that statistics provides the brain-the theoretical frameworks, uncertainty quantification, and inferential goals-while computer science provides the brawn-the scalable algorithms and hardware. Recognizing this statistical backbone is not merely an academic exercise, but a necessary step for developing more robust, interpretable, and trustworthy intelligent systems. We issue a call to action for education, research, and practice to re-embrace this statistical foundation. Ignoring these roots risks building a fragile future; embracing them is the path to truly intelligent machines. There is no machine learning without statistical learning; no artificial intelligence without statistical thought.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19212v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ernest Fokou\'e</dc:creator>
    </item>
    <item>
      <title>A New Targeted-Federated Learning Framework for Estimating Heterogeneity of Treatment Effects: A Robust Framework with Applications in Aging Cohorts</title>
      <link>https://arxiv.org/abs/2510.19243</link>
      <description>arXiv:2510.19243v1 Announce Type: new 
Abstract: Analyzing data from multiple sources offers valuable opportunities to improve the estimation efficiency of causal estimands. However, this analysis also poses many challenges due to population heterogeneity and data privacy constraints. While several advanced methods for causal inference in federated settings have been developed in recent years, many focus on difference-based averaged causal effects and are not designed to study effect modification. In this study, we introduce a novel targeted-federated learning framework to study the heterogeneity of treatment effects (HTEs) for a targeted population by proposing a projection-based estimand. This HTE framework integrates information from multiple data sources without sharing raw data, while accounting for covariate distribution shifts among sources. Our proposed approach is shown to be doubly robust, conveniently supporting both difference-based estimands for continuous outcomes and odds ratio-based estimands for binary outcomes. Furthermore, we develop a communication-efficient bootstrap-based selection procedure to detect non-transportable data sources, thereby enhancing robust information aggregation without introducing bias. The superior performance of the proposed estimator over existing methods is demonstrated through extensive simulation studies, and the utility of our approach has been shown in a real-world data application using nationwide Medicare-linked data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19243v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rong Zhao, Jason Falvey, Xu Shi, Vernon M. Chinchilli, Chixiang Chen</dc:creator>
    </item>
    <item>
      <title>Hierarchical Overlapping Group Lasso for GMANOVA Model</title>
      <link>https://arxiv.org/abs/2510.19311</link>
      <description>arXiv:2510.19311v1 Announce Type: new 
Abstract: This paper deals with the GMANOVA model with a matrix of polynomial basis functions as a within-individual design matrix. The model involves two model selection problems: the selection of explanatory variables and the selection of the degrees of the polynomials. The two problems can be uniformly addressed by hierarchically incorporating zeros into the vectors of regression coefficients. Based on this idea, we propose hierarchical overlapping group Lasso (HOGL) to perform the variable and degree selections simultaneously. Importantly, when using a polynomial basis, fitting a highdegree polynomial often causes problems in model selection. In the approach proposed here, these problems are handled by using a matrix of orthonormal basis functions obtained by transforming the matrix of polynomial basis functions. Algorithms are developed with optimality and convergence to optimize the method. The performance of the proposed method is evaluated using numerical simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19311v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. Ohishi, I. Nagai, R. Oda, H. Yanagihara</dc:creator>
    </item>
    <item>
      <title>Living Synthetic Benchmarks: A Neutral and Cumulative Framework for Simulation Studies</title>
      <link>https://arxiv.org/abs/2510.19489</link>
      <description>arXiv:2510.19489v1 Announce Type: new 
Abstract: Simulation studies are widely used to evaluate statistical methods. However, new methods are often introduced and evaluated using data-generating mechanisms (DGMs) devised by the same authors. This coupling creates misaligned incentives, e.g., the need to demonstrate the superiority of new methods, potentially compromising the neutrality of simulation studies. Furthermore, results of simulation studies are often difficult to compare due to differences in DGMs, competing methods, and performance measures. This fragmentation can lead to conflicting conclusions, hinder methodological progress, and delay the adoption of effective methods. To address these challenges, we introduce the concept of living synthetic benchmarks. The key idea is to disentangle method and simulation study development and continuously update the benchmark whenever a new DGM, method, or performance measure becomes available. This separation benefits the neutrality of method evaluation, emphasizes the development of both methods and DGMs, and enables systematic comparisons. In this paper, we outline a blueprint for building and maintaining such benchmarks, discuss the technical and organizational challenges of implementation, and demonstrate feasibility with a prototype benchmark for publication bias adjustment methods. We conclude that living synthetic benchmarks have the potential to foster neutral, reproducible, and cumulative evaluation of methods, benefiting both method developers and users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19489v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Franti\v{s}ek Barto\v{s}, Samuel Pawel, Bj\"orn S. Siepe</dc:creator>
    </item>
    <item>
      <title>Robust Rank Estimation for Noisy Matrices</title>
      <link>https://arxiv.org/abs/2510.19583</link>
      <description>arXiv:2510.19583v1 Announce Type: new 
Abstract: Estimating the true rank of a noisy data matrix is a fundamental problem underlying techniques such as principal component analysis, matrix completion, etc. Existing rank estimation criteria, including information-based and cross-validation methods, are either highly sensitive to outliers or computationally demanding when combined with robust estimators. This paper proposes a new criterion, the Divergence Information Criterion for Matrix Rank (DICMR), that achieves both robustness and computational simplicity. Derived from the density power divergence framework, DICMR inherits the robustness properties while being computationally very simple. We provide asymptotic bounds on its overestimation and underestimation probabilities, and demonstrate first-order B-robustness of the criteria. Extensive simulations show that DICMR delivers accuracy comparable to the robustified cross-validation methods, but with far lower computational cost. We also showcase a real-data application to microarray imputation to further demonstrate its practical utility, outperforming several state-of-the-art algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19583v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhrajyoty Roy, Abhik Ghosh, Ayanendranath Basu</dc:creator>
    </item>
    <item>
      <title>A Class of Markovian Self-Reinforcing Processes with Power-Law Distributions</title>
      <link>https://arxiv.org/abs/2510.19034</link>
      <description>arXiv:2510.19034v1 Announce Type: cross 
Abstract: Solar flares, email exchanges, and many natural or social systems exhibit bursty dynamics, with periods of intense activity separated by long inactivity. These patterns often follow power- law distributions in inter-event intervals or event rates. Existing models typically capture only one of these features and rely on non-local memory, which complicates analysis and mechanistic interpretation. We introduce a novel self-reinforcing point process whose event rates are governed by local, Markovian nonlinear dynamics and post-event resets. The model generates power-law tails for both inter-event intervals and event rates over a broad range of exponents observed empirically across natural and human phenomena. Compared to non-local models such as Hawkes processes, our approach is mechanistically simpler, highly analytically tractable, and also easier to simulate. We provide methods for model fitting and validation, establishing this framework as a versatile foundation for the study of bursty phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19034v1</guid>
      <category>physics.soc-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pavlo Bulanchuk, Sue Ann Koay, Sandro Romani</dc:creator>
    </item>
    <item>
      <title>Simulation-Guided Planning of a Target Trial Emulated Cluster Randomized Trial for Mass Small-Quantity Lipid Nutrient Supplementation Combined with Expanded Program on Immunization in Rural Niger</title>
      <link>https://arxiv.org/abs/2510.19077</link>
      <description>arXiv:2510.19077v1 Announce Type: cross 
Abstract: While target trial emulation (TTE) is increasingly used to improve the analysis of non-randomized studies by applying trial design principles, TTE applications to emulate cluster randomized trials (RCTs) have been limited. We performed simulations to prospectively plan data collection of a non-randomized study intended to emulate a village-level cluster RCT when cluster-randomization was infeasible. The planned study will assess the impact of mass distribution of nutritional supplements embedded within an existing immunization program to improve pentavalent vaccination rates among children 12-24 months old in Niger. The design included covariate-constrained random selection of villages for outcome ascertainment at follow-up. Simulations used baseline census data on pentavalent vaccination rates and cluster-level covariates to compare the type I error rate and power of four statistical methods: beta-regression; quasi-binomial regression; inverse probability of treatment weighting (IPTW); and na\"ive Wald test. Of these methods, only IPTW and beta-regression controlled the type I error rate at 0.05, but IPTW yielded poor statistical power. Beta-regression, which showed adequate statistical power, was chosen as our primary analysis. Adopting simulation-guided design principles within TTE can enable robust planning of a group-level non-randomized study emulating a cluster RCT. Lessons from this study also apply to TTE planning of individually-RCTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19077v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rebecca K. Metcalfe, Nathaniel Dyrkton, Yichen Yan, Shomoita Alam, Susan Shepherd, Ibrahim Sana, Kevin Phelan, Jay JH Park</dc:creator>
    </item>
    <item>
      <title>Knowledge Distillation of Uncertainty using Deep Latent Factor Model</title>
      <link>https://arxiv.org/abs/2510.19290</link>
      <description>arXiv:2510.19290v1 Announce Type: cross 
Abstract: Deep ensembles deliver state-of-the-art, reliable uncertainty quantification, but their heavy computational and memory requirements hinder their practical deployments to real applications such as on-device AI. Knowledge distillation compresses an ensemble into small student models, but existing techniques struggle to preserve uncertainty partly because reducing the size of DNNs typically results in variation reduction. To resolve this limitation, we introduce a new method of distribution distillation (i.e. compressing a teacher ensemble into a student distribution instead of a student ensemble) called Gaussian distillation, which estimates the distribution of a teacher ensemble through a special Gaussian process called the deep latent factor model (DLF) by treating each member of the teacher ensemble as a realization of a certain stochastic process. The mean and covariance functions in the DLF model are estimated stably by using the expectation-maximization (EM) algorithm. By using multiple benchmark datasets, we demonstrate that the proposed Gaussian distillation outperforms existing baselines. In addition, we illustrate that Gaussian distillation works well for fine-tuning of language models and distribution shift problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19290v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sehyun Park, Jongjin Lee, Yunseop Shin, Ilsang Ohn, Yongdai Kim</dc:creator>
    </item>
    <item>
      <title>Network Contagion Dynamics in European Banking: A Navier-Stokes Framework for Systemic Risk Assessment</title>
      <link>https://arxiv.org/abs/2510.19630</link>
      <description>arXiv:2510.19630v2 Announce Type: cross 
Abstract: This paper develops a continuous functional framework for analyzing contagion dynamics in financial networks, extending the Navier-Stokes-based approach to network-structured spatial processes. We model financial distress propagation as a diffusion process on weighted networks, deriving a network diffusion equation from first principles that predicts contagion decay depends on the network's algebraic connectivity through the relation $\kappa = \sqrt{\lambda_2/D}$, where $\lambda_2$ is the second-smallest eigenvalue of the graph Laplacian and $D$ is the diffusion coefficient. Applying this framework to European banking data from the EBA stress tests (2018, 2021, 2023), we estimate interbank exposure networks using maximum entropy methods and track the evolution of systemic risk through the COVID-19 crisis. Our key finding is that network connectivity declined by 45\% from 2018 to 2023, implying a 26\% reduction in the contagion decay parameter. Difference-in-differences analysis reveals this structural change was driven by regulatory-induced deleveraging of systemically important banks, which experienced differential asset reductions of 17\% relative to smaller institutions. The networks exhibit lognormal rather than scale-free degree distributions, suggesting greater resilience than previously assumed in the literature. Extensive robustness checks across parametric and non-parametric estimation methods confirm declining systemic risk, with cross-method correlations exceeding 0.95. These findings demonstrate that post-COVID-19 regulatory reforms effectively reduced network interconnectedness and systemic vulnerability in the European banking system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19630v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Semi-Implicit Approaches for Large-Scale Bayesian Spatial Interpolation</title>
      <link>https://arxiv.org/abs/2510.19722</link>
      <description>arXiv:2510.19722v1 Announce Type: cross 
Abstract: Spatial statistics often rely on Gaussian processes (GPs) to capture dependencies across locations. However, their computational cost increases rapidly with the number of locations, potentially needing multiple hours even for moderate sample sizes. To address this, we propose using Semi-Implicit Variational Inference (SIVI), a highly flexible Bayesian approximation method, for scalable Bayesian spatial interpolation. We evaluated SIVI with a GP prior and a Nearest-Neighbour Gaussian Process (NNGP) prior compared to Automatic Differentiation Variational Inference (ADVI), Pathfinder, and Hamiltonian Monte Carlo (HMC), the reference method in spatial statistics. Methods were compared based on their predictive ability measured by the CRPS, the interval score, and the negative log-predictive density across 50 replicates for both Gaussian and Poisson outcomes. SIVI-based methods achieved similar results to HMC, while being drastically faster. On average, for the Poisson scenario with 500 training locations, SIVI reduced the computational time from roughly 6 hours for HMC to 130 seconds. Furthermore, SIVI-NNGP analyzed a simulated land surface temperature dataset of 150,000 locations while estimating all unknown model parameters in under two minutes. These results highlight the potential of SIVI as a flexible and scalable inference technique in spatial statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19722v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S\'ebastien Garneau (Department of Epidemiology, Biostatistics and Occupational Health, McGill University), Carlos T. P. Zanini (Department of Statistical Methods, Federal University of Rio de Janeiro), Alexandra M. Schmidt (Department of Epidemiology, Biostatistics and Occupational Health, McGill University)</dc:creator>
    </item>
    <item>
      <title>Stratification in Randomised Clinical Trials for Rare Diseases and Analysis of Covariance: Some Simple Theory and Recommendations</title>
      <link>https://arxiv.org/abs/2408.06760</link>
      <description>arXiv:2408.06760v5 Announce Type: replace 
Abstract: A simple device for balancing for a continuous covariate in clinical trials is to stratify by whether the covariate is above or below some target value, typically the predicted median. This raises an issue as to which model should be used for modelling the effect of treatment on the outcome variable, $Y$. Should one fit, the stratum indicator, $S$, the continuous covariate, $X$, both or neither? When a covariate is added to a linear model there are three consequences for inference: 1) the mean square error effect, 2) the variance inflation factor and 3) second order precision. We consider that it is valuable to consider these three factors separately, even if, ultimately, it is their joint effect that matters. We present some simple theory, concentrating in particular on the variance inflation factor, that may be used to guide trialists in their choice of model. We also consider the case where the precise form of the relationship between the outcome and the covariate is not known. We conclude by recommending that the continuous covariate should always be in the model but that, depending on circumstances, there may be some justification in fitting the stratum indicator also.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06760v5</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Stephen Senn, Franz K\"onig, Martin Posch</dc:creator>
    </item>
    <item>
      <title>Leveraging external data in the analysis of randomized controlled trials: a comparative analysis</title>
      <link>https://arxiv.org/abs/2408.13409</link>
      <description>arXiv:2408.13409v2 Announce Type: replace 
Abstract: The use of patient-level information from previous studies, registries, and other external datasets can support the analysis of single-arm and randomized controlled trials to evaluate and test experimental treatments. However, the integration of external data in the analysis of clinical trials can also compromise the scientific validity of the results due to selection bias, study-to-study differences, unmeasured confounding, and other distortion mechanisms. Therefore, leveraging external data in the analysis of a clinical trial requires the use of appropriate methods that can detect, prevent or mitigate the risks of bias and potential distortion mechanisms. We review several methods that allow investigators to leverage external datasets, such as propensity score procedures and random effects modeling. Different methods present distinct trade-offs between risks and efficiencies. We conduct a comparative analysis of statistical methods to leverage external data and analyze randomized controlled trials. Multiple operating characteristics are discussed, such as the control of false positive results, power, and the bias of the treatment effect estimates, across candidate statistical methods. We compare the statistical methods through a broad set of simulation scenarios. We then compare the methods using a collection of datasets with individual patient-level information from several glioblastoma studies in order to provide recommendations for future glioblastoma trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13409v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gopal Kotecha, Daniel E. Schwartz, Steffen Ventz, Lorenzo Trippa</dc:creator>
    </item>
    <item>
      <title>Bayes factor functions for testing partial correlation coefficients</title>
      <link>https://arxiv.org/abs/2503.10787</link>
      <description>arXiv:2503.10787v3 Announce Type: replace 
Abstract: Partial correlation coefficients are widely applied in the social sciences to evaluate the relationship between two variables after accounting for the influence of others. In this article, we present Bayes Factor Functions (BFFs) for assessing the presence of partial correlation. BFFs represent Bayes factors derived from test statistics and are expressed as functions of a standardized effect size. While traditional frequentist methods based on $p$-values have been criticized for their inability to provide cumulative evidence in favor of the true hypothesis, Bayesian approaches are often challenged due to their computational demands and sensitivity to prior distributions. BFFs overcome these limitations and offer summaries of hypothesis tests as alternative hypotheses are varied over a range of prior distributions on standardized effects. They also enable the integration of evidence across multiple studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10787v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saptati Datta</dc:creator>
    </item>
    <item>
      <title>Adaptive Designs in Fast-Track Registration Processes for Digital Health Applications</title>
      <link>https://arxiv.org/abs/2507.04092</link>
      <description>arXiv:2507.04092v3 Announce Type: replace 
Abstract: Fast-track procedures play an important role in the context of conditional registration of medical devices, such as listing processes for digital health applications. They offer the potential for earlier patient access to innovative products and involve two registration steps. The applicants can apply first for conditional registration. A successful conditional registration provides a limited funding or approval period and time to prepare the application for permanent registration (the second registration step). For conditional registration, products have to fulfill only a part of the requirements necessary for permanent registration. There is interest in valid and efficient study designs for fast-track procedures. This will be addressed in this paper. A motivating example is the German fast-track registration process of digital health applications (DiGA) for reimbursement by statutory health insurances. The main focus of the paper is the systematic statistical investigation of the utility of adaptive designs in the context of fast-track registration processes like the DiGA fast-track. We demonstrate that, in most cases, such designs are much more efficient than the current standard of two separate studies. A careful statistical discussion of the registration requirements and their consequences is also included. The results are based on numerical calculations supported by mathematical arguments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04092v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Liane Kluge, Werner Brannath</dc:creator>
    </item>
    <item>
      <title>Loss Functions for Detecting Outliers in Panel Data</title>
      <link>https://arxiv.org/abs/2509.07014</link>
      <description>arXiv:2509.07014v2 Announce Type: replace 
Abstract: The detection of outliers is of critical importance in the assurance of data quality. Outliers may exist in observed data or in data derived from these observed data, such as estimates and forecasts. An outlier may indicate a problem with its data generation process or may simply be a true, but unusual, statement about the world. Without making any distributional assumptions, we proposes the use of loss functions to detect these outliers in panel data.
  Part I covers nonnegative data. We axiomatically derive an unsigned loss function. We then develop a signed loss function ito account for positive and negative outliers separately. In the case of nominal time we obtain an exact parametrization of the loss function. A time-invariant loss function permits the comparison of data at multiple times on the same basis. We provide several examples, including an example in which the outliers are classified by another variable.
  Part II covers data of mixed sign. Similar to Part I, we axiomatically develop unsigned and signed loss functions. We search for optimal values of the loss function parameter using graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07014v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Charles D. Coleman, Thomas Bryan</dc:creator>
    </item>
    <item>
      <title>DR-VIDAL -- Doubly Robust Variational Information-theoretic Deep Adversarial Learning for Counterfactual Prediction and Treatment Effect Estimation on Real World Data</title>
      <link>https://arxiv.org/abs/2303.04201</link>
      <description>arXiv:2303.04201v4 Announce Type: replace-cross 
Abstract: Determining causal effects of interventions onto outcomes from real-world, observational (non-randomized) data, e.g., treatment repurposing using electronic health records, is challenging due to underlying bias. Causal deep learning has improved over traditional techniques for estimating individualized treatment effects (ITE). We present the Doubly Robust Variational Information-theoretic Deep Adversarial Learning (DR-VIDAL), a novel generative framework that combines two joint models of treatment and outcome, ensuring an unbiased ITE estimation even when one of the two is misspecified. DR-VIDAL integrates: (i) a variational autoencoder (VAE) to factorize confounders into latent variables according to causal assumptions; (ii) an information-theoretic generative adversarial network (Info-GAN) to generate counterfactuals; (iii) a doubly robust block incorporating treatment propensities for outcome predictions. On synthetic and real-world datasets (Infant Health and Development Program, Twin Birth Registry, and National Supported Work Program), DR-VIDAL achieves better performance than other non-generative and generative methods. In conclusion, DR-VIDAL uniquely fuses causal assumptions, VAE, Info-GAN, and doubly robustness into a comprehensive, performant framework. Code is available at: https://github.com/Shantanu48114860/DR-VIDAL-AMIA-22 under MIT license.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.04201v4</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Journal="AMIA Annu Symp Proc", Year="2022", Volume="2022",Pages="485--494"</arxiv:journal_reference>
      <dc:creator>Shantanu Ghosh, Zheng Feng, Jiang Bian, Kevin Butler, Mattia Prosperi</dc:creator>
    </item>
    <item>
      <title>Survey Data Integration for Distribution Function Estimation</title>
      <link>https://arxiv.org/abs/2409.14284</link>
      <description>arXiv:2409.14284v4 Announce Type: replace-cross 
Abstract: We propose a novel CDF estimator that integrates data from probability samples with data from, potentially big, nonprobability samples. Assuming that a set of shared covariates are observed in both, while the response variable is observed only in the latter, the proposed estimator uses a survey-weighted empirical CDF of regression residuals trained on the convenience sample to estimate the CDF of the response variable. Under some assumptions, we derive the asymptotic bias and variance of our CDF estimator and show that it is asymptotically unbiased for the finite population CDF if ignorability holds. Empirical results demonstrate that the estimator performs well under model misspecification when ignorability holds, and under nonignorable sampling when the outcome model is correctly specified. Even when both assumptions fail, the residual-based estimator continues to outperform its plug-in and na\"ive counterparts, albeit with noted decreases in efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14284v4</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 23 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy Flood, Sayed Mostafa</dc:creator>
    </item>
  </channel>
</rss>
