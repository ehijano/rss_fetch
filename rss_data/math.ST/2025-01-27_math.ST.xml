<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Jan 2025 03:45:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A new class of tests for convex-ordered families based on expected order statistics</title>
      <link>https://arxiv.org/abs/2501.14075</link>
      <description>arXiv:2501.14075v1 Announce Type: new 
Abstract: Consider a pair of cumulative distribution functions $F$ and $G$, where $F$ is unknown and $G$ is a known reference distribution. Given a sample from $F$, we propose tests to detect the convexity or the concavity of $G^{-1}\circ F$ versus equality in distribution (up to location and scale transformations). This framework encompasses well-known cases, including increasing hazard rate distributions, as well as some other relevant families that have garnered attention more recently, for which no tests are currently available. We introduce test statistics based on the estimated probability that the random variable of interest does not exceed a given expected order statistic, which, in turn, is estimated via L-estimation. The tests are unbiased, consistent, and exhibit monotone power with respect to the convex transform order. To ensure consistency, we extend the strong law of large numbers for L-estimators to random variables without finite means, making the tests suitable for heavy-tailed distributions. Unlike other approaches, these tests are broadly applicable, regardless of the choice of $G$ and without support restrictions. The performance of the method under various conditions is demonstrated via simulations, and its applicability is illustrated through a concrete example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14075v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tommaso Lando, Mohammed Es-Salih Benjrada</dc:creator>
    </item>
    <item>
      <title>Statistical Verification of Linear Classifiers</title>
      <link>https://arxiv.org/abs/2501.14430</link>
      <description>arXiv:2501.14430v1 Announce Type: cross 
Abstract: We propose a homogeneity test closely related to the concept of linear separability between two samples. Using the test one can answer the question whether a linear classifier is merely ``random'' or effectively captures differences between two classes. We focus on establishing upper bounds for the test's \emph{p}-value when applied to two-dimensional samples. Specifically, for normally distributed samples we experimentally demonstrate that the upper bound is highly accurate. Using this bound, we evaluate classifiers designed to detect ER-positive breast cancer recurrence based on gene pair expression. Our findings confirm significance of IGFBP6 and ELOVL5 genes in this process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14430v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anton Zhiyanov, Alexander Shklyaev, Alexey Galatenko, Vladimir Galatenko, Alexander Tonevitsky</dc:creator>
    </item>
    <item>
      <title>Models Parametric Analysis via Adaptive Kernel Learning</title>
      <link>https://arxiv.org/abs/2501.14485</link>
      <description>arXiv:2501.14485v1 Announce Type: cross 
Abstract: Any applied mathematical model contains parameters. The paper proposes to use kernel learning for the parametric analysis of the model. The approach consists in setting a distribution on the parameter space, obtaining a finite training sample from this distribution, solving the problem for each parameter value from this sample, and constructing a kernel approximation of the parametric dependence on the entire set of parameter values. The kernel approximation is obtained by minimizing the approximation error on the training sample and adjusting kernel parameters (width) on the same or another independent sample of parameters. This approach to learning complex dependencies is called kernel learning (or kernel SVM). Traditionally, kernel learning is considered in the so-called Reproducing Kernel Hilbert Space (RKHS) with a fixed kernel. The novelty of our approach is that we consider the kernel learning in a broad subspace of square-integrable functions with a corresponding L_2-norm regularization. This subspace contains linear combinations of kernel functions with kernels of different shapes at different data points. The approach essentially uses a derived analytical representation of the L2-norm for kernel functions. Thus the approach substantially extends the flexibility of the traditional kernel SVM for account of the number of adjusted parameters and minimization of the training error not only over weights of kernels but also over their shapes. The important issue of selecting the optimal regularization parameter is resolved by minimizing the test error over this parameter. Numerical illustrations are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14485v1</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vladimir Norkin, Alois Pichler</dc:creator>
    </item>
    <item>
      <title>On the multidimensional elephant random walk with stops</title>
      <link>https://arxiv.org/abs/2501.14594</link>
      <description>arXiv:2501.14594v1 Announce Type: cross 
Abstract: The goal of this paper is to investigate the asymptotic behavior of the multidimensional elephant random walk with stops (MERWS). In contrast with the standard elephant random walk, the elephant is allowed to stay on his own position. We prove that the Gram matrix associated with the MERWS, properly normalized, converges almost surely to the product of a deterministic matrix, related to the axes on which the MERWS moves uniformly, and a Mittag-Leffler distribution. It allows us to extend all the results previously established for the one-dimensional elephant random walk with stops. More precisely, in the diffusive and critical regimes, we prove the almost sure convergence of the MERWS. In the superdiffusive regime, we establish the almost sure convergence of the MERWS, properly normalized, to a nondegenerate random vector. We also study the self-normalized asymptotic normality of the MERWS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14594v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bernard Bercu</dc:creator>
    </item>
    <item>
      <title>Hidden Markov Models and the Bayes Filter in Categorical Probability</title>
      <link>https://arxiv.org/abs/2401.14669</link>
      <description>arXiv:2401.14669v3 Announce Type: replace 
Abstract: We use Markov categories to generalize the basic theory of Markov chains and hidden Markov models to an abstract setting. This comprises characterizations of hidden Markov models in terms of conditional independences and algorithms for Bayesian filtering and smoothing applicable in all Markov categories with conditionals. When instantiated in appropriate Markov categories, these algorithms specialize to existing ones such as the Kalman filter, forward-backward algorithm, and the Rauch-Tung-Striebel smoother. We also prove that the sequence of outputs of our abstract Bayes filter is itself a Markov chain with a concrete formula for its transition maps.
  There are two main features of this categorical framework. The first is its abstract generality, as manifested in our unified account of hidden Markov models and algorithms for filtering and smoothing in discrete probability, Gaussian probability, measure-theoretic probability, possibilistic nondeterminism and others at the same time. The second feature is the intuitive visual representation of information flow in terms of string diagrams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14669v3</guid>
      <category>math.ST</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.CT</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tobias Fritz, Andreas Klingler, Drew McNeely, Areeb Shah-Mohammed, Yuwen Wang</dc:creator>
    </item>
    <item>
      <title>Concentration of discrepancy-based approximate Bayesian computation via Rademacher complexity</title>
      <link>https://arxiv.org/abs/2206.06991</link>
      <description>arXiv:2206.06991v5 Announce Type: replace-cross 
Abstract: There has been increasing interest on summary-free solutions for approximate Bayesian computation (ABC) which replace distances among summaries with discrepancies between the empirical distributions of the observed data and the synthetic samples generated under the proposed parameter values. The success of these strategies has motivated theoretical studies on the limiting properties of the induced posteriors. However, there is still the lack of a theoretical framework for summary-free ABC that (i) is unified, instead of discrepancy-specific, (ii) does not require to constrain the analysis to data generating processes and statistical models meeting specific regularity conditions, but rather facilitates the derivation of limiting properties that hold uniformly, and (iii) relies on verifiable assumptions that provide explicit concentration bounds clarifying which factors govern the limiting behavior of the ABC posterior. We address this gap via a novel theoretical framework that introduces the concept of Rademacher complexity in the analysis of the limiting properties for discrepancy-based ABC posteriors, including in non-i.i.d. and misspecified settings. This yields a unified theory that relies on constructive arguments and provides more informative asymptotic results and uniform concentration bounds, even in settings not covered by current studies. These advancements are obtained by relating the asymptotic properties of summary-free ABC posteriors to the behavior of the Rademacher complexity associated with the chosen discrepancy in the family of integral probability semimetrics (IPS). The IPS class extends summary-based distances, and includes the Wasserstein distance and maximum mean discrepancy, among others. As clarified in specialized theoretical analyses of popular IPS discrepancies and via illustrative simulations, this perspective improves the understanding of summary-free ABC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.06991v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sirio Legramanti, Daniele Durante, Pierre Alquier</dc:creator>
    </item>
    <item>
      <title>On the Sample Complexity of One Hidden Layer Networks with Equivariance, Locality and Weight Sharing</title>
      <link>https://arxiv.org/abs/2411.14288</link>
      <description>arXiv:2411.14288v2 Announce Type: replace-cross 
Abstract: Weight sharing, equivariance, and local filters, as in convolutional neural networks, are believed to contribute to the sample efficiency of neural networks. However, it is not clear how each one of these design choices contributes to the generalization error. Through the lens of statistical learning theory, we aim to provide insight into this question by characterizing the relative impact of each choice on the sample complexity. We obtain lower and upper sample complexity bounds for a class of single hidden layer networks. For a large class of activation functions, the bounds depend merely on the norm of filters and are dimension-independent. We also provide bounds for max-pooling and an extension to multi-layer networks, both with mild dimension dependence. We provide a few takeaways from the theoretical results. It can be shown that depending on the weight-sharing mechanism, the non-equivariant weight-sharing can yield a similar generalization bound as the equivariant one. We show that locality has generalization benefits, however the uncertainty principle implies a trade-off between locality and expressivity. We conduct extensive experiments and highlight some consistent trends for these models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14288v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arash Behboodi, Gabriele Cesa</dc:creator>
    </item>
  </channel>
</rss>
