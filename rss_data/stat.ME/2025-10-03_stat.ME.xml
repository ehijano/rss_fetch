<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 Oct 2025 04:01:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>DiffKnock: Diffusion-based Knockoff Statistics for Neural Networks Inference</title>
      <link>https://arxiv.org/abs/2510.01418</link>
      <description>arXiv:2510.01418v1 Announce Type: new 
Abstract: We introduce DiffKnock, a diffusion-based knockoff framework for high-dimensional feature selection with finite-sample false discovery rate (FDR) control. DiffKnock addresses two key limitations of existing knockoff methods: preserving complex feature dependencies and detecting non-linear associations. Our approach trains diffusion models to generate valid knockoffs and uses neural network--based gradient and filter statistics to construct antisymmetric feature importance measures. Through simulations, we showed that DiffKnock achieved higher power than autoencoder-based knockoffs while maintaining target FDR, indicating its superior performance in scenarios involving complex non-linear architectures. Applied to murine single-cell RNA-seq data of LPS-stimulated macrophages, DiffKnock identifies canonical NF-$\kappa$B target genes (Ccl3, Hmox1) and regulators (Fosb, Pdgfb). These results highlight that, by combining the flexibility of deep generative models with rigorous statistical guarantees, DiffKnock is a powerful and reliable tool for analyzing single-cell RNA-seq data, as well as high-dimensional and structured data in other domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01418v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heng Ge, Qing Lu</dc:creator>
    </item>
    <item>
      <title>Repro Samples Method for Model-Free Inference in High-Dimensional Binary Classification</title>
      <link>https://arxiv.org/abs/2510.01468</link>
      <description>arXiv:2510.01468v1 Announce Type: new 
Abstract: This paper presents a novel method for statistical inference in high-dimensional binary models with unspecified structure, where we leverage a (potentially misspecified) sparsity-constrained working generalized linear model (GLM) to facilitate the inference process. Our method is based on the repro samples framework, which generates artificial samples that mimic the actual data-generating process. Our inference targets include the model support, case probabilities, and the oracle regression coefficients defined in the working GLM. The proposed method has three major advantages. First, this approach is model-free, that is, it does not rely on specific model assumptions such as logistic or probit regression, nor does it require sparsity assumptions on the underlying model. Second, for model support, we construct a model candidate set for the most influential covariates that achieves guaranteed coverage under a weak signal strength assumption. Third, for oracle regression coefficients, we establish confidence sets for any group of linear combinations of regression coefficients. Simulation results demonstrate that the proposed method produces valid and small model candidate sets. It also achieves better coverage for regression coefficients than the state-of-the-art debiasing methods when the working model is the actual model that generates the sample data. Additionally, we analyze single-cell RNA-seq data on the immune response. Besides identifying genes previously proven as relevant in the literature, our method also discovers a significant gene that has not been studied before, revealing a potential new direction in understanding cellular immune response mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01468v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaotian Hou, Peng Wang, Minge Xie, Linjun Zhang</dc:creator>
    </item>
    <item>
      <title>SLOPE and Designing Robust Studies for Generalization</title>
      <link>https://arxiv.org/abs/2510.01577</link>
      <description>arXiv:2510.01577v1 Announce Type: new 
Abstract: A popular task in generalization is to learn about a new, target population based on data from an existing, source population. This task relies on conditional exchangeability, which asserts that differences between the source and target populations are fully captured by observable characteristics of the two populations. Unfortunately, this assumption is often untenable in practice due to unobservable differences between the source and target populations. Worse, the assumption cannot be verified with data, warranting the need for robust data collection processes and study designs that are inherently less sensitive to violation of the assumption. In this paper, we propose SLOPE (Sensitivity of LOcal Perturbations from Exchangeability), a simple, intuitive, and novel measure that quantifies the sensitivity to local violation of conditional exchangeability. SLOPE combines ideas from sensitivity analysis in causal inference and derivative-based measure of robustness from Hampel (1974). Among other properties, SLOPE can help investigators to choose (a) a robust source or target population or (b) a robust estimand. Also, we show an analytic relationship between SLOPE and influence functions (IFs), which investigators can use to derive SLOPE given an IF. We conclude with a re-analysis of a multi-national randomized experiment and illustrate the role of SLOPE in informing robust study designs for generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01577v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinran Miao, Jiwei Zhao, Hyunseung Kang</dc:creator>
    </item>
    <item>
      <title>Forecasting intraday particle number size distribution: A functional time series approach</title>
      <link>https://arxiv.org/abs/2510.01692</link>
      <description>arXiv:2510.01692v1 Announce Type: new 
Abstract: Particulate matter data now include various particle sizes, which often manifest as a collection of curves observed sequentially over time. When considering 51 distinct particle sizes, these curves form a high-dimensional functional time series observed over equally spaced and densely sampled grids. While high dimensionality poses statistical challenges due to the curse of dimensionality, it also offers a rich source of information that enables detailed analysis of temporal variation across short time intervals for all particle sizes. To model this complexity, we propose a multilevel functional time series framework incorporating a functional factor model to facilitate one-day-ahead forecasting. To quantify forecast uncertainty, we develop a calibration approach and a split conformal prediction approach to construct prediction intervals. Both approaches are designed to minimise the absolute difference between empirical and nominal coverage probabilities using a validation dataset. Furthermore, to improve forecast accuracy as new intraday data become available, we implement dynamic updating techniques for point and interval forecasts. The proposed methods are validated through an empirical application to hourly measurements of particulate matter in 51 size categories in London.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01692v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Han Lin Shang, Israel Martinez Hernandez</dc:creator>
    </item>
    <item>
      <title>Stabilizing Thompson Sampling with Point Null Bayesian Response-Adaptive Randomization</title>
      <link>https://arxiv.org/abs/2510.01734</link>
      <description>arXiv:2510.01734v1 Announce Type: new 
Abstract: Response-adaptive randomization (RAR) methods use accumulated data to adapt randomization probabilities, aiming to increase the probability of allocating patients to effective treatments. A popular RAR method is Thompson sampling, which randomizes patients proportionally to the Bayesian posterior probability that each treatment is the most effective. However, its high variability early in a trial can also increase the risk of assigning patients to inferior treatments. We propose a principled method based on Bayesian hypothesis testing to mitigate this issue. Specifically, we introduce a point null hypothesis that postulates equal effectiveness of treatments. This induces shrinkage toward equal randomization probabilities, with the degree of shrinkage controlled by the prior probability of the null hypothesis. Equal randomization and Thompson sampling arise as special cases when the prior probability is set to one or zero, respectively. Simulated and real-world examples illustrate that the proposed method balances highly variable Thompson sampling with static equal randomization. A simulation study demonstrates that the method can mitigate issues with ordinary Thompson sampling and has comparable statistical properties to Thompson sampling with common ad hoc modifications such as power transformation and probability capping. We implement the method in the open-source R package brar, enabling experimenters to easily perform point null Bayesian RAR and support more effective randomization of patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01734v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Pawel, Leonhard Held</dc:creator>
    </item>
    <item>
      <title>Scalable Asynchronous Federated Modeling for Spatial Data</title>
      <link>https://arxiv.org/abs/2510.01771</link>
      <description>arXiv:2510.01771v1 Announce Type: new 
Abstract: Spatial data are central to applications such as environmental monitoring and urban planning, but are often distributed across devices where privacy and communication constraints limit direct sharing. Federated modeling offers a practical solution that preserves data privacy while enabling global modeling across distributed data sources. For instance, environmental sensor networks are privacy- and bandwidth-constrained, motivating federated spatial modeling that shares only privacy-preserving summaries to produce timely, high-resolution pollution maps without centralizing raw data. However, existing federated modeling approaches either ignore spatial dependence or rely on synchronous updates that suffer from stragglers in heterogeneous environments. This work proposes an asynchronous federated modeling framework for spatial data based on low-rank Gaussian process approximations. The method employs block-wise optimization and introduces strategies for gradient correction, adaptive aggregation, and stabilized updates. We establish linear convergence with explicit dependence on staleness, a result of standalone theoretical significance. Moreover, numerical experiments demonstrate that the asynchronous algorithm achieves synchronous performance under balanced resource allocation and significantly outperforms it in heterogeneous settings, showcasing superior robustness and scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01771v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianwei Shi, Sameh Abdulah, Ying Sun, Marc G. Genton</dc:creator>
    </item>
    <item>
      <title>Optimal smoothing parameter in Eilers-Wittaker smoother</title>
      <link>https://arxiv.org/abs/2510.01798</link>
      <description>arXiv:2510.01798v1 Announce Type: new 
Abstract: The Eilers-Whittaker method for data smoothing effectiveness depends on the choice of the regularisation parameter, and automatic selection is a necessity for large datasets. Common methods, such as leave-one-out cross-validation, can perform poorly when serially correlated noise is present. We propose a novel procedure for selecting the control parameter based on the spectral entropy of the residuals. We define an S-curve from the Euclidean distance between points in a plot of the spectral entropy of the residuals versus that of the smoothed signal. The regularisation parameter corresponding to the absolute maximum of this S-curve is chosen as the optimal parameter. Using simulated data, we benchmarked our method against cross-validation and the V-curve. Validation was also performed on diverse experimental data. This robust and straightforward procedure can be a valuable addition to the available selection methods for the Eilers smoother.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01798v1</guid>
      <category>stat.ME</category>
      <category>physics.app-ph</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Bernal-Arencibia, Karel Garcia Medina, Ernesto Estevez-Rams, Beatriz Aragon-Fernandez</dc:creator>
    </item>
    <item>
      <title>Compressed Bayesian Tensor Regression</title>
      <link>https://arxiv.org/abs/2510.01861</link>
      <description>arXiv:2510.01861v1 Announce Type: new 
Abstract: To address the common problem of high dimensionality in tensor regressions, we introduce a generalized tensor random projection method that embeds high-dimensional tensor-valued covariates into low-dimensional subspaces with minimal loss of information about the responses. The method is flexible, allowing for tensor-wise, mode-wise, or combined random projections as special cases. A Bayesian inference framework is provided featuring the use of a hierarchical prior distribution and a low-rank representation of the parameter. Strong theoretical support is provided for the concentration properties of the random projection and posterior consistency of the Bayesian inference. An efficient Gibbs sampler is developed to perform inference on the compressed data. To mitigate the sensitivity introduced by random projections, Bayesian model averaging is employed, with normalising constants estimated using reverse logistic regression. An extensive simulation study is conducted to examine the effects of different tuning parameters. Simulations indicate, and the real data application confirms, that compressed Bayesian tensor regression can achieve better out-of-sample prediction while significantly reducing computational cost compared to standard Bayesian tensor regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01861v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Casarin, Radu Craiu, Qing Wang</dc:creator>
    </item>
    <item>
      <title>Predictively Oriented Posteriors</title>
      <link>https://arxiv.org/abs/2510.01915</link>
      <description>arXiv:2510.01915v1 Announce Type: new 
Abstract: We advocate for a new statistical principle that combines the most desirable aspects of both parameter inference and density estimation. This leads us to the predictively oriented (PrO) posterior, which expresses uncertainty as a consequence of predictive ability. Doing so leads to inferences which predictively dominate both classical and generalised Bayes posterior predictive distributions: up to logarithmic factors, PrO posteriors converge to the predictively optimal model average at rate $n^{-1/2}$. Whereas classical and generalised Bayes posteriors only achieve this rate if the model can recover the data-generating process, PrO posteriors adapt to the level of model misspecification. This means that they concentrate around the true model at rate $n^{1/2}$ in the same way as Bayes and Gibbs posteriors if the model can recover the data-generating distribution, but do \textit{not} concentrate in the presence of non-trivial forms of model misspecification. Instead, they stabilise towards a predictively optimal posterior whose degree of irreducible uncertainty admits an interpretation as the degree of model misspecification -- a sharp contrast to how Bayesian uncertainty and its existing extensions behave. Lastly, we show that PrO posteriors can be sampled from by evolving particles based on mean field Langevin dynamics, and verify the practical significance of our theoretical developments on a number of numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01915v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yann McLatchie, Badr-Eddine Cherief-Abdellatif, David T. Frazier, Jeremias Knoblauch</dc:creator>
    </item>
    <item>
      <title>Identifying Subgroup and Context Effects in Conjoint Experiments</title>
      <link>https://arxiv.org/abs/2510.02123</link>
      <description>arXiv:2510.02123v1 Announce Type: new 
Abstract: Conjoint experiments have become central to survey research in political science and related fields because they allow researchers to study preferences across multiple attributes simultaneously. Beyond estimating main effects, scholars increasingly analyze heterogeneity through subgroup analysis and contextual variables, raising methodological challenges in detecting and interpreting interaction effects. Statistical power constraints, common in survey experiments, further complicate this task. This paper addresses the question: how can both main and interaction effects be reliably inferred in conjoint studies? We contribute in two ways. First, we conduct a systematic evaluation of leading approaches, including post-hoc corrections, sparse regression methods, and Bayesian models, across simulation regimes that vary sparsity, noise, and data availability. Second, we propose a novel black-box inference framework that leverages machine learning to recover main and interaction effects in conjoint experiments. Our approach balances computational efficiency with accuracy, providing a practical tool for researchers studying heterogeneous effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02123v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven Wang, Isys Johnson, Jessica Grogan, Lalit Jain, Atri Rudra, Kyle Hunt, Kenneth Joseph</dc:creator>
    </item>
    <item>
      <title>Improving Survival Models in Healthcare by Balancing Imbalanced Cohorts: A Novel Approach</title>
      <link>https://arxiv.org/abs/2510.02137</link>
      <description>arXiv:2510.02137v1 Announce Type: new 
Abstract: We explore whether survival model performance in underrepresented high- and low-risk subgroups - regions of the prognostic spectrum where clinical decisions are most consequential - can be improved through targeted restructuring of the training dataset. Rather than modifying model architecture, we propose a novel risk-stratified sampling method that addresses imbalances in prognostic subgroup density to support more reliable learning in underrepresented tail strata. We introduce a novel methodology that partitions patients by baseline prognostic risk and applies matching within each stratum to equalize representation across the risk distribution. We implement this framework on a cohort of 1,799 patients with resected colorectal liver metastases (CRLM), including 1,197 who received adjuvant chemotherapy and 602 who did not. All models used in this study are Cox proportional hazards models trained on the same set of selected variables. Model performance is assessed via Harrell's C index, time-dependent AUC, and Integrated Calibration Index (ICI), with internal validation using Efron's bias-corrected bootstrapping. External validation is conducted on two independent CRLM datasets. Cox models trained on risk-balanced cohorts showed consistent improvements in internal validation compared to models trained on the full dataset while noticeably enhancing stratified C-index values in underrepresented high- and low-risk strata of the external cohorts. Our findings suggest that survival model performance in observational oncology cohorts can be meaningfully improved through targeted rebalancing of the training data across prognostic risk strata. This approach offers a practical and model-agnostic complement to existing methods, especially in applications where predictive reliability across the full risk continuum is critical to downstream clinical decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02137v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Catherine Ning, Dimitris Bertsimas, Johan Gagni\`ere, Stefan Buettner, Per Eystein Loenning, Hideo Baba, Itaru Endo, Georgios Stasinos, Richard Burkhart, Federico N. Auecio, Felix Balzer, Cornelis Verhoef, Martin E. Kreis, Georgios Antonios Margonis</dc:creator>
    </item>
    <item>
      <title>Multivariate distributional modeling of low, moderate, and large intensities without threshold selection steps</title>
      <link>https://arxiv.org/abs/2510.02152</link>
      <description>arXiv:2510.02152v1 Announce Type: new 
Abstract: In fields such as hydrology and climatology, modelling the entire distribution of positive data is essential, as stakeholders require insights into the full range of values, from low to extreme. Traditional approaches often segment the distribution into separate regions, which introduces subjectivity and limits coherence. This is especially true when dealing with multivariate data.
  In line with multivariate extreme value theory, this paper presents a unified, threshold-free framework for modelling marginal behaviours and dependence structures based on an extended generalized Pareto distribution (EGPD). We propose decomposing multivariate data into radial and angular components. The radial component is modelled using a semi-parametric EGPD and the angular distribution is permitted to vary conditionally. This approach allows for sufficiently flexible dependence modelling.
  The hierarchical structure of the model facilitates the inference process. First, we combine classical maximum likelihood estimation (MLE) methods with semi-parametric approaches based on Bernstein polynomials to estimate the distribution of the radial component. Then, we use multivariate regression techniques to estimate the angular component's parameters.
  The model is evaluated through synthetic simulations and applied to hydrological datasets to exemplify its capacity to capture heavy-tailed marginals and complex multivariate dependencies without threshold specification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02152v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Carlo Gaetan, Philippe Naveau</dc:creator>
    </item>
    <item>
      <title>Dependent stochastic block models for age-indexed sequences of directed causes-of-death networks</title>
      <link>https://arxiv.org/abs/2510.01806</link>
      <description>arXiv:2510.01806v1 Announce Type: cross 
Abstract: Death events commonly arise from complex interactions among interrelated causes, formally classified in reporting practices as underlying and contributing. Leveraging information from death certificates, these interactions can be naturally represented through a sequence of directed networks encoding co-occurrence strengths between pairs of underlying and contributing causes across ages. Although this perspective opens the avenues to learn informative age-specific block interactions among endogenous groups of underlying and contributing causes displaying similar co-occurrence patterns, there has been limited research along this direction in mortality modeling. This is mainly due to the lack of suitable stochastic block models for age-indexed sequences of directed networks. We cover this gap through a novel Bayesian formulation which crucially learns two separate group structures for underlying and contributing causes, while allowing both structures to change smoothly across ages via dependent random partition priors. As illustrated in simulations, this formulation outperforms state-of-the-art solutions that could be adapted to our motivating application. Moreover, when applied to USA mortality data, it unveils structures in the composition, evolution, and modular interactions among causes-of-death groups that were hidden to previous studies. Such findings could have relevant policy implications and contribute to an improved understanding of the recent "death of despair" phenomena in USA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01806v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giovanni Roman\`o, Cristian Castiglione, Daniele Durante</dc:creator>
    </item>
    <item>
      <title>Knots and variance ordering of sequential Monte Carlo algorithms</title>
      <link>https://arxiv.org/abs/2510.01901</link>
      <description>arXiv:2510.01901v1 Announce Type: cross 
Abstract: Sequential Monte Carlo algorithms, or particle filters, are widely used for approximating intractable integrals, particularly those arising in Bayesian inference and state-space models. We introduce a new variance reduction technique, the knot operator, which improves the efficiency of particle filters by incorporating potential function information into part, or all, of a transition kernel. The knot operator induces a partial ordering of Feynman-Kac models that implies an order on the asymptotic variance of particle filters, offering a new approach to algorithm design. We discuss connections to existing strategies for designing efficient particle filters, including model marginalisation. Our theory generalises such techniques and provides quantitative asymptotic variance ordering results. We revisit the fully-adapted (auxiliary) particle filter using our theory of knots to show how a small modification guarantees an asymptotic variance ordering for all relevant test functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01901v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua J Bon, Anthony Lee</dc:creator>
    </item>
    <item>
      <title>DAG DECORation: Continuous Optimization for Structure Learning under Hidden Confounding</title>
      <link>https://arxiv.org/abs/2510.02117</link>
      <description>arXiv:2510.02117v1 Announce Type: cross 
Abstract: We study structure learning for linear Gaussian SEMs in the presence of latent confounding. Existing continuous methods excel when errors are independent, while deconfounding-first pipelines rely on pervasive factor structure or nonlinearity. We propose \textsc{DECOR}, a single likelihood-based and fully differentiable estimator that jointly learns a DAG and a correlated noise model. Our theory gives simple sufficient conditions for global parameter identifiability: if the mixed graph is bow free and the noise covariance has a uniform eigenvalue margin, then the map from $(\B,\OmegaMat)$ to the observational covariance is injective, so both the directed structure and the noise are uniquely determined. The estimator alternates a smooth-acyclic graph update with a convex noise update and can include a light bow complementarity penalty or a post hoc reconciliation step. On synthetic benchmarks that vary confounding density, graph density, latent rank, and dimension with $n&lt;p$, \textsc{DECOR} matches or outperforms strong baselines and is especially robust when confounding is non-pervasive, while remaining competitive under pervasiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02117v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Samhita Pal, James O'quinn, Kaveh Aryan, Heather Pua, James P. Long, Amir Asiaee</dc:creator>
    </item>
    <item>
      <title>A fast and effective kernel two-sample test for large-scale data</title>
      <link>https://arxiv.org/abs/2110.03118</link>
      <description>arXiv:2110.03118v2 Announce Type: replace 
Abstract: Kernel two-sample tests have been widely used, and the development of efficient methods for high-dimensional, large-scale data is receiving increasing attention in the big data era. However, existing methods, such as the maximum mean discrepancy (MMD) and recently proposed kernel-based tests for large-scale data, are computationally intensive and/or ineffective for some common alternatives in high-dimensional data. In this paper, we propose a new test that exhibits high power across a wide range of alternatives. Furthermore, the new test is more robust to high dimensions than existing methods and does not require optimization procedures for choosing kernel bandwidth and other parameters through data splitting. Numerical studies demonstrate that the new approach performs well on both synthetic and real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.03118v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hoseung Song, Hao Chen</dc:creator>
    </item>
    <item>
      <title>Quantifying Individual Risk for Binary Outcome</title>
      <link>https://arxiv.org/abs/2402.10537</link>
      <description>arXiv:2402.10537v3 Announce Type: replace 
Abstract: Understanding treatment effect heterogeneity is crucial for reliable decision-making in treatment evaluation and selection. The conditional average treatment effect (CATE) is widely used to capture treatment effect heterogeneity induced by observed covariates and to design individualized treatment policies. However, it is an average metric within subpopulations, which prevents it from revealing individual-level risks, potentially leading to misleading results. This article fills this gap by examining individual risk for binary outcomes, specifically focusing on the fraction negatively affected (FNA), a metric that quantifies the percentage of individuals experiencing worse outcomes under treatment compared with control. Even under the strong ignorability assumption, FNA is still unidentifiable, and the existing Frechet--Hoeffding bounds are usually too wide and attainable only under extreme data-generating processes. By invoking mild conditions on the value range of the Pearson correlation coefficient between potential outcomes, we obtain improved bounds compared with the Frechet--Hoeffding bounds. We show that paradoxically, even with a positive CATE, the lower bound on FNA can be positive, i.e., in the best-case scenario, many individuals will be harmed if they receive treatment. Additionally, we establish a nonparametric sensitivity analysis framework for FNA using the Pearson correlation coefficient as the sensitivity parameter. Furthermore, we propose nonparametric estimators for the refined FNA bounds and prove their consistency and asymptotic normality. We conduct extensive simulation to evaluate the performance of the proposed estimators and apply the method to right heart catheterization data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10537v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Wu, Peng Ding, Zhi Geng, Yue Liu</dc:creator>
    </item>
    <item>
      <title>A cautionary tale of model misspecification and identifiability</title>
      <link>https://arxiv.org/abs/2507.04894</link>
      <description>arXiv:2507.04894v2 Announce Type: replace 
Abstract: Mathematical models are routinely applied to interpret biological data, with common goals that include both prediction and parameter estimation. A challenge in mathematical biology, in particular, is that models are often complex and non-identifiable, while data are limited. Rectifying identifiability through simplification can seemingly yield more precise parameter estimates, albeit, as we explore in this perspective, at the potentially catastrophic cost of introducing model misspecification and poor accuracy. We demonstrate how uncertainty in model structure can be propagated through to uncertainty in parameter estimates using a semi-parametric Gaussian process approach that delineates parameters of interest from uncertainty in model terms. Specifically, we study generalised logistic growth with an unknown crowding function, and a spatially resolved process described by a partial differential equation with a time-dependent diffusivity parameter. Allowing for structural model uncertainty yields more robust and accurate parameter estimates, and a better quantification of remaining uncertainty. We conclude our perspective by discussing the connections between identifiability and model misspecification, and alternative approaches to dealing with model misspecification in mathematical biology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04894v2</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander P Browning, Jennifer A Flegg, Ryan J Murphy</dc:creator>
    </item>
    <item>
      <title>Synthetic Blips: Generalizing Synthetic Controls for Dynamic Treatment Effects</title>
      <link>https://arxiv.org/abs/2210.11003</link>
      <description>arXiv:2210.11003v2 Announce Type: replace-cross 
Abstract: We propose a generalization of the synthetic control and interventions methods to the setting with dynamic treatment effects. We consider the estimation of unit-specific treatment effects from panel data collected under a general treatment sequence. Here, each unit receives multiple treatments sequentially, according to an adaptive policy that depends on a latent, endogenously time-varying confounding state. Under a low-rank latent factor model assumption, we develop an identification strategy for any unit-specific mean outcome under any sequence of interventions. The latent factor model we propose admits linear time-varying and time-invariant dynamical systems as special cases. Our approach can be viewed as an identification strategy for structural nested mean models -- a widely used framework for dynamic treatment effects -- under a low-rank latent factor assumption on the blip effects. Unlike these models, however, it is more permissive in observational settings, thereby broadening its applicability. Our method, which we term synthetic blip effects, is a backwards induction process in which the blip effect of a treatment at each period and for a target unit is recursively expressed as a linear combination of the blip effects of a group of other units that received the designated treatment. This strategy avoids the combinatorial explosion in the number of units that would otherwise be required by a naive application of prior synthetic control and intervention methods in dynamic treatment settings. We provide estimation algorithms that are easy to implement in practice and yield estimators with desirable properties. Using unique Korean firm-level panel data, we demonstrate how the proposed framework can be used to estimate individualized dynamic treatment effects and to derive optimal treatment allocation rules in the context of financial support for exporting firms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.11003v2</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anish Agarwal, Sukjin Han, Dwaipayan Saha, Vasilis Syrgkanis, Haeyeon Yoon</dc:creator>
    </item>
    <item>
      <title>Optimal and Provable Calibration in High-Dimensional Binary Classification: Angular Calibration and Platt Scaling</title>
      <link>https://arxiv.org/abs/2502.15131</link>
      <description>arXiv:2502.15131v3 Announce Type: replace-cross 
Abstract: We study the fundamental problem of calibrating a linear binary classifier of the form $\sigma(\hat{w}^\top x)$, where the feature vector $x$ is Gaussian, $\sigma$ is a link function, and $\hat{w}$ is an estimator of the true linear weight $w^\star$. By interpolating with a noninformative $\textit{chance classifier}$, we construct a well-calibrated predictor whose interpolation weight depends on the angle $\angle(\hat{w}, w_\star)$ between the estimator $\hat{w}$ and the true linear weight $w_\star$. We establish that this angular calibration approach is provably well-calibrated in a high-dimensional regime where the number of samples and features both diverge, at a comparable rate. The angle $\angle(\hat{w}, w_\star)$ can be consistently estimated. Furthermore, the resulting predictor is uniquely $\textit{Bregman-optimal}$, minimizing the Bregman divergence to the true label distribution within a suitable class of calibrated predictors. Our work is the first to provide a calibration strategy that satisfies both calibration and optimality properties provably in high dimensions. Additionally, we identify conditions under which a classical Platt-scaling predictor converges to our Bregman-optimal calibrated solution. Thus, Platt-scaling also inherits these desirable properties provably in high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15131v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yufan Li, Pragya Sur</dc:creator>
    </item>
    <item>
      <title>How Much Is Too Much? Adaptive, Context-Aware Risk Detection in Naturalistic Driving</title>
      <link>https://arxiv.org/abs/2508.00888</link>
      <description>arXiv:2508.00888v3 Announce Type: replace-cross 
Abstract: Reliable risk identification based on driver behavior data underpins real-time safety feedback, fleet risk management, and evaluation of driver-assist systems. While naturalistic driving studies have become foundational for providing real-world driver behavior data, the existing frameworks for identifying risk based on such data have two fundamental limitations: (i) they rely on predefined time windows and fixed thresholds to disentangle risky and normal driving behavior, and (ii) they assume behavior is stationary across drivers and time, ignoring heterogeneity and temporal drift. In practice, these limitations can lead to timing errors and miscalibration in alerts, weak generalization to new drivers/routes/conditions, and higher false-alarm and miss rates, undermining driver trust and reducing safety intervention effectiveness. To address this gap, we propose a unified, context-aware framework that adapts labels and models over time and across drivers via rolling windows, joint optimization, dynamic calibration, and model fusion, tailored for time-stamped kinematic data. The framework is tested using two safety indicators, speed-weighted headway and harsh driving events, and three models: Random Forest, XGBoost, and Deep Neural Network (DNN). Speed-weighted headway yielded more stable and context-sensitive classifications than harsh-event counts. XGBoost maintained consistent performance under changing thresholds, whereas DNN achieved higher recall at lower thresholds but with greater variability across trials. The ensemble aggregated signals from multiple models into a single risk decision, balancing responsiveness to risky behavior with control of false alerts. Overall, the framework shows promise for adaptive, context-aware risk detection that can enhance real-time safety feedback and support driver-focused interventions in intelligent transportation systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00888v3</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 03 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amir Hossein Kalantari, Eleonora Papadimitriou, Arkady Zgonnikov, Amir Pooyan Afghari</dc:creator>
    </item>
  </channel>
</rss>
