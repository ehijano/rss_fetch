<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 Aug 2024 04:03:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Discrete Imprecise Copulas</title>
      <link>https://arxiv.org/abs/2408.15352</link>
      <description>arXiv:2408.15352v1 Announce Type: new 
Abstract: In this paper, we study discrete quasi-copulas associated with imprecise copulas. We focus on discrete imprecise copulas that are in correspondence with the Alternating Sign Matrices and provide some construction techniques of dual pairs. Additionally, we show how to obtain (full-domain) self-dual imprecise copulas through patchwork techniques. We further investigate the properties of the constructed dual pairs and prove that they are coherent and avoiding sure loss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15352v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toma\v{z} Ko\v{s}ir, Elisa Perrone</dc:creator>
    </item>
    <item>
      <title>Learning latent tree models with small query complexity</title>
      <link>https://arxiv.org/abs/2408.15624</link>
      <description>arXiv:2408.15624v1 Announce Type: new 
Abstract: We consider the problem of structure recovery in a graphical model of a tree where some variables are latent. Specifically, we focus on the Gaussian case, which can be reformulated as a well-studied problem: recovering a semi-labeled tree from a distance metric. We introduce randomized procedures that achieve query complexity of optimal order. Additionally, we provide statistical analysis for scenarios where the tree distances are noisy. The Gaussian setting can be extended to other situations, including the binary case and non-paranormal distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15624v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luc Devroye, Gabor Lugosi, Piotr Zwiernik</dc:creator>
    </item>
    <item>
      <title>Fast convergence rates for estimating the stationary density in SDEs driven by a fractional Brownian motion with semi-contractive drift</title>
      <link>https://arxiv.org/abs/2408.15904</link>
      <description>arXiv:2408.15904v1 Announce Type: new 
Abstract: We consider the solution of an additive fractional stochastic differential equation (SDE) and, leveraging continuous observations of the process, introduce a methodology for estimating its stationary density $\pi$. Initially, employing a tailored martingale decomposition specifically designed for the statistical challenge at hand, we establish convergence rates surpassing those found in existing literature. Subsequently, we refine the attained rate for the case where $H &lt; \frac{1}{2}$ by incorporating bounds on the density of the semi-group. This enhancement outperforms previous rates. Finally, our results weaken the usual convexity assumptions on the drift component, allowing to consider settings where strong convexity only holds outside a compact set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15904v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Amorino, Eulalia Nualart, Fabien Panloup, Julian Sieber</dc:creator>
    </item>
    <item>
      <title>Filtering SPDEs with Spatio-Temporal Point Process Observations</title>
      <link>https://arxiv.org/abs/2408.15920</link>
      <description>arXiv:2408.15920v1 Announce Type: new 
Abstract: In this paper, we develop the mathematical framework for filtering problems arising from biophysical applications where data is collected from confocal laser scanning microscopy recordings of the space-time evolution of intracellular wave dynamics of biophysical quantities. In these applications, signals are described by stochastic partial differential equations (SPDEs) and observations can be modelled as functionals of marked point processes whose intensities depend on the underlying signal. We derive both the unnormalized and normalized filtering equations for these systems, demonstrate the asymptotic consistency and approximations of finite dimensional observation schemes respectively partial observations. Our theoretical results are validated through extensive simulations using synthetic and real data. These findings contribute to a deeper understanding of filtering with point process observations and provide a robust framework for future research in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15920v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jan Szalankiewicz, Cristina Martinez-Torres, Wilhelm Stannat</dc:creator>
    </item>
    <item>
      <title>Optimal level set estimation for non-parametric tournament and crowdsourcing problems</title>
      <link>https://arxiv.org/abs/2408.15356</link>
      <description>arXiv:2408.15356v1 Announce Type: cross 
Abstract: Motivated by crowdsourcing, we consider a problem where we partially observe the correctness of the answers of $n$ experts on $d$ questions. In this paper, we assume that both the experts and the questions can be ordered, namely that the matrix $M$ containing the probability that expert $i$ answers correctly to question $j$ is bi-isotonic up to a permutation of it rows and columns. When $n=d$, this also encompasses the strongly stochastic transitive (SST) model from the tournament literature. Here, we focus on the relevant problem of deciphering small entries of $M$ from large entries of $M$, which is key in crowdsourcing for efficient allocation of workers to questions. More precisely, we aim at recovering a (or several) level set $p$ of the matrix up to a precision $h$, namely recovering resp. the sets of positions $(i,j)$ in $M$ such that $M_{ij}&gt;p+h$ and $M_{i,j}&lt;p-h$. We consider, as a loss measure, the number of misclassified entries. As our main result, we construct an efficient polynomial-time algorithm that turns out to be minimax optimal for this classification problem. This heavily contrasts with existing literature in the SST model where, for the stronger reconstruction loss, statistical-computational gaps have been conjectured. More generally, this shades light on the nature of statistical-computational gaps for permutations models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15356v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maximilian Graf, Alexandra Carpentier, Nicolas Verzelen</dc:creator>
    </item>
    <item>
      <title>Implicit Regularization Paths of Weighted Neural Representations</title>
      <link>https://arxiv.org/abs/2408.15784</link>
      <description>arXiv:2408.15784v1 Announce Type: cross 
Abstract: We study the implicit regularization effects induced by (observation) weighting of pretrained features. For weight and feature matrices of bounded operator norms that are infinitesimally free with respect to (normalized) trace functionals, we derive equivalence paths connecting different weighting matrices and ridge regularization levels. Specifically, we show that ridge estimators trained on weighted features along the same path are asymptotically equivalent when evaluated against test vectors of bounded norms. These paths can be interpreted as matching the effective degrees of freedom of ridge estimators fitted with weighted features. For the special case of subsampling without replacement, our results apply to independently sampled random features and kernel features and confirm recent conjectures (Conjectures 7 and 8) of the authors on the existence of such paths in Patil et al. We also present an additive risk decomposition for ensembles of weighted estimators and show that the risks are equivalent along the paths when the ensemble size goes to infinity. As a practical consequence of the path equivalences, we develop an efficient cross-validation method for tuning and apply it to subsampled pretrained representations across several models (e.g., ResNet-50) and datasets (e.g., CIFAR-100).</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15784v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin-Hong Du, Pratik Patil</dc:creator>
    </item>
    <item>
      <title>Cartan-Schouten metrics for information geometry and machine learning</title>
      <link>https://arxiv.org/abs/2408.15854</link>
      <description>arXiv:2408.15854v1 Announce Type: cross 
Abstract: We study Cartan-Schouten metrics, explore invariant dual connections, and propose them as models for Information Geometry.
  Based on the underlying Riemannian barycenter and the biinvariant mean of Lie groups, we subsequently propose a new parametric mean for data science and machine learning which comes with several advantages compared to traditional tools such as the arithmetic mean, median, mode, expectation, least square method, maximum likelihood, linear regression. We call a metric on a Lie group, a Cartan-Schouten metric, if its Levi-Civita connection is biinvariant, so every 1-parameter subgroup through the unit is a geodesic.
  Except for not being left or right invariant in general, Cartan-Schouten metrics enjoy the same geometry as biinvariant metrics, since they share the same Levi-Civita connection. To bypass the non-invariance apparent drawback, we show that Cartan-Schouten metrics are completely determined by their value at the unit. We give an explicit formula for recovering them from their value at the unit, thus making them much less computationally demanding, compared to general metrics on manifolds. Furthermore, Lie groups with Cartan-Schouten metrics are complete Riemannian or pseudo-Riemannian manifolds. We give a complete characterization of Lie groups with Riemannian or Lorentzian Cartan-Schouten metrics. Cartan-Schouten metrics are in abundance on 2-nilpotent Lie groups. Namely, on every 2-nilpotent Lie group, there is a 1-1 correspondence between the set of left invariant metrics and that of Cartan-Schouten metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15854v1</guid>
      <category>math.DG</category>
      <category>cs.IT</category>
      <category>math-ph</category>
      <category>math.IT</category>
      <category>math.MP</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andre Diatta, Bakary Manga, Fatimata Sy</dc:creator>
    </item>
    <item>
      <title>Optimal and exact recovery on general non-uniform Hypergraph Stochastic Block Model</title>
      <link>https://arxiv.org/abs/2304.13139</link>
      <description>arXiv:2304.13139v3 Announce Type: replace 
Abstract: Consider the community detection problem in random hypergraphs under the non-uniform hypergraph stochastic block model (HSBM), where each hyperedge appears independently with some given probability depending only on the labels of its vertices. We establish, for the first time in the literature, a sharp threshold for exact recovery under this non-uniform case, subject to minor constraints; in particular, we consider the model with multiple communities. One crucial point here is that by aggregating information from all the uniform layers, we may obtain exact recovery even in cases when this may appear impossible if each layer were considered alone. Besides that, we prove a wide-ranging, information-theoretic lower bound on the number of misclassified vertices \emph{for any algorithm}, depending on a \emph{generalized Chernoff-Hellinger} divergence involving model parameters. We provide two efficient algorithms which successfully achieve exact recovery when above the threshold, and attain the lowest possible mismatch ratio when the exact recovery is impossible, proved to be optimal. The theoretical analysis of our algorithms relies on the concentration and regularization of the adjacency matrix for non-uniform random hypergraphs, which could be of independent interest. We also address some open problems regarding parameter knowledge and estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.13139v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ioana Dumitriu, Haixiao Wang</dc:creator>
    </item>
    <item>
      <title>Uniform error bound for PCA matrix denoising</title>
      <link>https://arxiv.org/abs/2306.12690</link>
      <description>arXiv:2306.12690v3 Announce Type: replace 
Abstract: Principal component analysis (PCA) is a simple and popular tool for processing high-dimensional data. We investigate its effectiveness for matrix denoising.
  We consider the clean data are generated from a low-dimensional subspace, but masked by independent high-dimensional sub-Gaussian noises with standard deviation $\sigma$. Under the low-rank assumption on the clean data with a mild spectral gap assumption, we prove that the distance between each pair of PCA-denoised data point and the clean data point is uniformly bounded by $O(\sigma \log n)$. To illustrate the spectral gap assumption, we show it can be satisfied when the clean data are independently generated with a non-degenerate covariance matrix. We then provide a general lower bound for the error of the denoised data matrix, which indicates PCA denoising gives a uniform error bound that is rate-optimal. Furthermore, we examine how the error bound impacts downstream applications such as clustering and manifold learning. Numerical results validate our theoretical findings and reveal the importance of the uniform error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.12690v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin T. Tong, Wanjie Wang, Yuguan Wang</dc:creator>
    </item>
    <item>
      <title>Mixing properties of nonstationary multivariate count processes</title>
      <link>https://arxiv.org/abs/2311.10692</link>
      <description>arXiv:2311.10692v2 Announce Type: replace 
Abstract: We prove absolute regularity ($\beta$-mixing) for nonstationary and multivariate versions of two popular classes of integer-valued processes. We show how this result can be used to prove asymptotic normality of a least squares estimator of an involved model parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.10692v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zinsou Max Debaly, Michael H. Neumann, Lionel Truquet</dc:creator>
    </item>
    <item>
      <title>A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules</title>
      <link>https://arxiv.org/abs/2404.01245</link>
      <description>arXiv:2404.01245v2 Announce Type: replace 
Abstract: Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of incorrectly classifying LLM-generated text as human-written). Our framework further reduces the problem of determining the optimal detection rule to solving a minimax optimization program. We apply this framework to two representative watermarks -- one of which has been internally implemented at OpenAI -- and obtain several findings that can be instrumental in guiding the practice of implementing watermarks. In particular, we derive optimal detection rules for these watermarks under our framework. These theoretically derived detection rules are demonstrated to be competitive and sometimes enjoy a higher power than existing detection approaches through numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01245v2</guid>
      <category>math.ST</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su</dc:creator>
    </item>
    <item>
      <title>Compressed Empirical Measures (in finite dimensions)</title>
      <link>https://arxiv.org/abs/2204.08847</link>
      <description>arXiv:2204.08847v3 Announce Type: replace-cross 
Abstract: We study approaches for compressing the empirical measure in the context of finite dimensional reproducing kernel Hilbert spaces (RKHSs). In this context, the empirical measure is contained within a natural convex set and can be approximated using convex optimization methods. Such an approximation gives rise to a coreset of data points. A key quantity that controls how large such a coreset has to be is the size of the largest ball around the empirical measure that is contained within the empirical convex set. The bulk of our work is concerned with deriving high probability lower bounds on the size of such a ball under various conditions and in various settings: we show how conditions on the density of the data and the kernel function can be used to infer such lower bounds; we further develop an approach that uses a lower bound on the smallest eigenvalue of a covariance operator to provide lower bounds on the size of such a ball; we extend the approach to approximate covariance operators and we show how it can be used in the context of kernel ridge regression. We also derive compression guarantees when standard algorithms like the conditional gradient method are used and we discuss variations of such algorithms to improve the runtime of these standard algorithms. We conclude with a construction of an infinite dimensional RKHS for which the compression is poor, highlighting some of the difficulties one faces when trying to move to infinite dimensional RKHSs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.08847v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steffen Gr\"unew\"alder</dc:creator>
    </item>
    <item>
      <title>Input estimation from discrete workload observations in a L\'evy-driven storage system</title>
      <link>https://arxiv.org/abs/2205.09980</link>
      <description>arXiv:2205.09980v3 Announce Type: replace-cross 
Abstract: Our goal is to estimate the characteristic exponent of the input to a L\'evy-driven storage system from a sample of equispaced workload observations. The estimator relies on an approximate moment equation associated with the Laplace-Stieltjes transform of the workload at exponentially distributed sampling times. The estimator is pointwise consistent for any observation grid. Moreover, a high frequency sampling scheme yields asymptotically normal estimation errors for a class of input processes. A resampling scheme that uses the available information in a more efficient manner is suggested and assessed via simulation experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.09980v3</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.spl.2024.110250</arxiv:DOI>
      <dc:creator>Dennis Nieman, Michel Mandjes, Liron Ravner</dc:creator>
    </item>
    <item>
      <title>Unveiling the Statistical Foundations of Chain-of-Thought Prompting Methods</title>
      <link>https://arxiv.org/abs/2408.14511</link>
      <description>arXiv:2408.14511v2 Announce Type: replace-cross 
Abstract: Chain-of-Thought (CoT) prompting and its variants have gained popularity as effective methods for solving multi-step reasoning problems using pretrained large language models (LLMs). In this work, we analyze CoT prompting from a statistical estimation perspective, providing a comprehensive characterization of its sample complexity. To this end, we introduce a multi-step latent variable model that encapsulates the reasoning process, where the latent variable encodes the task information. Under this framework, we demonstrate that when the pretraining dataset is sufficiently large, the estimator formed by CoT prompting is equivalent to a Bayesian estimator. This estimator effectively solves the multi-step reasoning problem by aggregating a posterior distribution inferred from the demonstration examples in the prompt. Moreover, we prove that the statistical error of the CoT estimator can be decomposed into two main components: (i) a prompting error, which arises from inferring the true task using CoT prompts, and (ii) the statistical error of the pretrained LLM. We establish that, under appropriate assumptions, the prompting error decays exponentially to zero as the number of demonstrations increases. Additionally, we explicitly characterize the approximation and generalization errors of the pretrained LLM. Notably, we construct a transformer model that approximates the target distribution of the multi-step reasoning problem with an error that decreases exponentially in the number of transformer blocks. Our analysis extends to other variants of CoT, including Self-Consistent CoT, Tree-of-Thought, and Selection-Inference, offering a broad perspective on the efficacy of these methods. We also provide numerical experiments to validate the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14511v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyang Hu, Fengzhuo Zhang, Siyu Chen, Zhuoran Yang</dc:creator>
    </item>
  </channel>
</rss>
