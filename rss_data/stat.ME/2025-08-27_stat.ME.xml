<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 27 Aug 2025 04:01:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bayesian Covariance Uncertainty for Adaptive Pilot-Sampling Termination in Multi-fidelity Uncertainty Quantification</title>
      <link>https://arxiv.org/abs/2508.18490</link>
      <description>arXiv:2508.18490v1 Announce Type: new 
Abstract: Monte Carlo integration becomes prohibitively expensive when each sample requires a high-fidelity model evaluation. Multi-fidelity uncertainty quantification methods mitigate this by combining estimators from high- and low-fidelity models, preserving unbiasedness while reducing variance under a fixed budget. Constructing such estimators optimally requires the model-output covariance matrix, typically estimated from pilot samples. Too few pilot samples lead to inaccurate covariance estimates and suboptimal estimators, while too many consume budget that could be used for final estimation. We propose a Bayesian framework to quantify covariance uncertainty from pilot samples, incorporating prior knowledge and enabling probabilistic assessments of estimator performance. A central component is a flexible $\gamma$-Gaussian prior that ensures computational tractability and supports efficient posterior projection under additional pilot samples. These tools enable adaptive pilot-sampling termination via an interpretable loss criterion that decomposes variance inefficiency into accuracy and cost components. While demonstrated here in the context of approximate control variates (ACV), the framework generalizes to other multi-fidelity estimators. We validate the approach on a monomial benchmark and a PDE-based Darcy flow problem. Across these tests, our adaptive method demonstrates its value for multi-fidelity estimation under limited pilot budgets and expensive models, achieving variance reduction comparable to baseline estimators with oracle covariance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18490v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas E. Coons, Aniket Jivani, Xun Huan</dc:creator>
    </item>
    <item>
      <title>Mapping beyond diseases: Controlled variable selection for secondary phenotypes using tilted knockoffs</title>
      <link>https://arxiv.org/abs/2508.18548</link>
      <description>arXiv:2508.18548v1 Announce Type: new 
Abstract: Researchers in biomedical studies often work with samples that are not selected uniformly at random from the population of interest, a major example being a case-control study. While these designs are motivated by specific scientific questions, it is often of interest to use the data collected to pursue secondary lines of investigations. In these cases, ignoring the fact that observations are not sampled uniformly at random can lead to spurious results. For example, in a case-control study, one might identify a spurious association between an exposure and a secondary phenotype when both affect the case-control status. This phenomenon is known as collider bias in the causal inference literature. While tests of independence under biased sampling are available, these methods typically do not apply when the number of variables is large.
  Here, we are interested in using the biased sample to select important exposures among a multitude of possible variables with replicability guarantees. While the model-X knockoff framework has been developed to test conditional independence hypotheses with False Discovery Rate (FDR) control, we show that its naive application fails to control FDR in the presence of biased sampling. We show how tilting the population distribution with the selection probability and constructing knockoff variables according to this tilted distribution instead leads to selection with FDR control. We study the FDR and power of the tilted knockoff method using simulated examples, and apply it to identify genetic underpinning of endophenotypes in a case-control study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18548v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qian Zhao, Susan Service, Carrie E. Bearden, Carlos Lopez-Jaramillo, Freimer Nelson, Chiara Sabatti</dc:creator>
    </item>
    <item>
      <title>Unified theory of testing relevant hypothesis in functional time series</title>
      <link>https://arxiv.org/abs/2508.18624</link>
      <description>arXiv:2508.18624v1 Announce Type: new 
Abstract: In this paper, we present a general framework for testing relevant hypotheses in functional time series. Our unified approach covers one-sample, two-sample, and change point problems under contaminated observations with arbitrary sampling schemes. By employing B-spline estimators and the self-normalization technique, we propose nuisance-parameter-free testing procedures, obviating the need for additional procedures such as estimating long-run covariance or measurement-error variance functions. A key challenge arises from related nonparametric statistics may not be tight, complicating the joint weak convergence for the test statistics and self-normalizers, particularly in sparse scenarios. To address this, we leverage a Gaussian approximation in a diverging-dimension regime to derive a pivotal approximate distribution. Then, we develop consistent decision rules, provide sufficient conditions ensuring non-degeneracy, and establish phase transition boundaries from sparse to dense. We also examine the multiple change point scenario and extend the theory when one obtains consistent estimates of the change points. The choice of self-normalizers is further discussed, including the recently developed range-adjusted self-normalizer. Extensive numerical experiments support the proposed theory, and we illustrate our methodologies using the AU.SHF implied volatility and traffic volume datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18624v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leheng Cai, Qirui Hu</dc:creator>
    </item>
    <item>
      <title>Blinded sample size recalculation in randomized controlled trials with analysis of covariance</title>
      <link>https://arxiv.org/abs/2508.18815</link>
      <description>arXiv:2508.18815v1 Announce Type: new 
Abstract: In randomized controlled trials, covariate adjustment can improve statistical power and reduce the required sample size compared with unadjusted estimators. Several regulatory agencies have released guidance on covariate adjustment, which has recently attracted attention in biopharmaceutical research. Analysis of covariance (ANCOVA) is often used to adjust for baseline covariates when outcomes are continuous. To design a sample size based on ANCOVA, it is necessary to prespecify the association between the outcome and baseline covariates, as well as among the baseline covariates themselves. However, determining these parameters at the design stage is challenging. Although it may be possible to adaptively assess these during the trial and recalculate the required sample size, existing sample size recalculation methods assume that the joint distribution of the outcome and baseline covariates is multivariate normal, which is not always the case in practice. In this study, we propose a blinded sample size recalculation method for the ANCOVA estimator based on the asymptotic relative efficiency under minimal distributional assumptions, thus accommodating arbitrary model misspecification. The proposed method is able to achieve the nominal power and reduce the required sample size without inflating the type I error rate in the final analysis. We conducted simulations to evaluate the performance of the proposed method under various scenarios and applied it to data from an HIV clinical trial. We provide R and SAS macro codes to implement the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18815v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takumi Kanata, Yasuhiro Hagiwara, Koji Oba</dc:creator>
    </item>
    <item>
      <title>Think before you fit: parameter identifiability, sensitivity and uncertainty in systems biology models</title>
      <link>https://arxiv.org/abs/2508.18853</link>
      <description>arXiv:2508.18853v1 Announce Type: new 
Abstract: Reliable predictions from systems biology models require knowing whether parameters can be estimated from available data, and with what certainty. Identifiability analysis reveals whether parameters are learnable in principle (structural identifiability) and in practice (practical identifiability). We introduce the core ideas using linear models, highlighting how experimental design and output sensitivity shape identifiability. In nonlinear models, identifiability can vary with parameter values, motivating global and simulation-based approaches. We summarise computational methods for assessing identifiability noting that weakly identifiable parameters can undermine predictions beyond the calibration dataset. Strategies to improve identifiability include measuring different outputs, refining model structure, and adding prior knowledge. Far from a technical afterthought, identifiability determines the limits of inference and prediction. Recognising and addressing identifiability is essential for building models that are not only well-fitted to data, but also capable of delivering predictions with robust, quantifiable uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18853v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon P. Preston, Richard D. Wilkinson, Richard H. Clayton, Mike J. Chappell, Gary R. Mirams</dc:creator>
    </item>
    <item>
      <title>Two-stage indirect determinantal sampling designs</title>
      <link>https://arxiv.org/abs/2508.18858</link>
      <description>arXiv:2508.18858v1 Announce Type: new 
Abstract: A key feature of determinantal sampling designs is their capacity to provide known and parametrisable inclusion probabilities at any order. This paper aims to demonstrate how to effectively leverage this characteristic, highlighting its implications by addressing a practical challenge that arises when managing a network of face-to-face surveyors. This challenge is formulated as an optimization problem within the framework of two-stage indirect sampling, utilizing the Generalized Weight Share Method (GWSM). A general closed-form expression for the optimal weight matrix defined by the GWSM is derived, and based on a reasonable hypothesis, a formula for the optimal inclusion probabilities used in the second stage is provided. The implementation of the global optimization process is illustrated with real data, assuming that the intermediate and the second stage sampling designs are determinantal. Additionally, given these designs, closed-form expressions for the target first-order and joint inclusion probabilities are presented, thus paving the way for an alternative application of the Horvitz-Thompson estimator for evaluating any total within the target population. In short, determinantal sampling designs prove to be a versatile and useful tool for addressing practical challenges involving high-order inclusion probabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18858v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Loons</dc:creator>
    </item>
    <item>
      <title>Rejoinder to the discussion of "Mode-based estimation of the center of symmetry"</title>
      <link>https://arxiv.org/abs/2508.18909</link>
      <description>arXiv:2508.18909v1 Announce Type: new 
Abstract: Rejoinder to the discussion by Hino (2025) and Pardo-Fern\'andez (2025) of Chac\'on and Fern\'andez Serrano (2025), a special paper (with discussion) published in Annals of the Institute of Statistical Mathematics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18909v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10463-025-00945-w</arxiv:DOI>
      <arxiv:journal_reference>Annals of the Institute of Statistical Mathematics (2025)</arxiv:journal_reference>
      <dc:creator>Jos\'e E. Chac\'on, Javier Fern\'andez Serrano</dc:creator>
    </item>
    <item>
      <title>Power new generalized class of Kavya-Manoharan distributions with an application to exponential distribution</title>
      <link>https://arxiv.org/abs/2508.18930</link>
      <description>arXiv:2508.18930v1 Announce Type: new 
Abstract: Recently, Verma et al. (2025) introduced a novel generalized class of Kavya-Manoharan distributions, which have demonstrated significant utility in reliability analysis and the modeling of lifetime data. This paper proposes an extension of this class by applying the power generalization technique, thereby enhancing more flexibility and applicability. We take the exponential distribution as the baseline distribution to introduce a new model capable of accommodating both monotonic and non-monotonic hazard rate functions. Our model includes eleven submodels. We present several statistical properties of the introduced model, including moments, generating and characteristic functions, mean deviations, quantile function, mean residual life function, R\'enyi entropy, order statistics, and reliability. To estimate the unknown model parameters, we use the maximum likelihood approach. A simulation study is conducted to assess the validity of the maximum likelihood estimator. The superiority of the new distribution is demonstrated through the use of a real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18930v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lazhar Benkhelifa</dc:creator>
    </item>
    <item>
      <title>Bayesian Joint Modeling of Zero-Inflated Longitudinal Data and Survival with a Cure Fraction: Application to AIDS Data</title>
      <link>https://arxiv.org/abs/2508.19001</link>
      <description>arXiv:2508.19001v1 Announce Type: new 
Abstract: We propose a comprehensive Bayesian joint modeling framework for zero-inflated longitudinal count data and time-to-event outcomes, explicitly incorporating a cure fraction to account for subjects who never experience the event. The longitudinal sub-model employs a flexible mixed-effects Hurdle model, with distributional options including zero-inflated Poisson and zero-inflated negative binomial, accommodating excess zeros and overdispersion common in count data. The survival component is modeled using a Cox proportional hazards model combined with a mixture cure model to distinguish cured from susceptible individuals. To link the longitudinal and survival processes, we include a linear combination of current longitudinal values as predictors in the survival model. Inference is performed via Hamiltonian Monte Carlo, enabling efficient and robust parameter estimation. The joint model supports dynamic predictions, facilitating real-time risk assessment and personalized medicine. Model performance and estimation accuracy are validated through simulation studies. Finally, we illustrate the methodology using a real-world HIV cohort dataset, demonstrating its practical utility in predicting patient survival outcomes and supporting personalized treatment decisions. Our results highlight the benefits of integrating complex longitudinal count data with survival information in clinical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19001v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taban Baghfalaki, Mojtaba Ganjali</dc:creator>
    </item>
    <item>
      <title>Sparse minimum Redundancy Maximum Relevance for feature selection</title>
      <link>https://arxiv.org/abs/2508.18901</link>
      <description>arXiv:2508.18901v1 Announce Type: cross 
Abstract: We propose a feature screening method that integrates both feature-feature and feature-target relationships. Inactive features are identified via a penalized minimum Redundancy Maximum Relevance (mRMR) procedure, which is the continuous version of the classic mRMR penalized by a non-convex regularizer, and where the parameters estimated as zero coefficients represent the set of inactive features. We establish the conditions under which zero coefficients are correctly identified to guarantee accurate recovery of inactive features. We introduce a multi-stage procedure based on the knockoff filter enabling the penalized mRMR to discard inactive features while controlling the false discovery rate (FDR). Our method performs comparably to HSIC-LASSO but is more conservative in the number of selected features. It only requires setting an FDR threshold, rather than specifying the number of features to retain. The effectiveness of the method is illustrated through simulations and real-world datasets. The code to reproduce this work is available on the following GitHub: https://github.com/PeterJackNaylor/SmRMR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18901v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Naylor, Benjamin Poignard, H\'ector Climente-Gonz\'alez, Makoto Yamada</dc:creator>
    </item>
    <item>
      <title>Dimension Reduction for Large-Scale Federated Data: Statistical Rate and Asymptotic Inference</title>
      <link>https://arxiv.org/abs/2306.06857</link>
      <description>arXiv:2306.06857v2 Announce Type: replace 
Abstract: In light of the rapidly growing large-scale data in federated ecosystems, the traditional principal component analysis (PCA) is often not applicable due to privacy protection considerations and large computational burden. Algorithms were proposed to lower the computational cost, but few can handle both high dimensionality and massive sample size under distributed settings. In this paper, we propose the FAst DIstributed (FADI) PCA method for federated data when both the dimension $d$ and the sample size $n$ are ultra-large, by simultaneously performing parallel computing along $d$ and distributed computing along $n$. Specifically, we utilize $L$ parallel copies of $p$-dimensional fast sketches to divide the computing burden along $d$ and aggregate the results distributively along the split samples. We present a general framework applicable to multiple statistical problems, and establish comprehensive theoretical results under the general framework. We show that FADI accelerates the computation while enjoying the same non-asymptotic error rate as the traditional PCA when $Lp \ge d$. We also derive inferential results that characterize the asymptotic distribution of FADI, and show a phase-transition phenomenon as $Lp$ increases. We perform extensive simulations to empirically validate our theoretical findings, and apply FADI to the 1000 Genomes data to study the population structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.06857v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuting Shen, Junwei Lu, Xihong Lin</dc:creator>
    </item>
    <item>
      <title>Network inference via approximate Bayesian computation. Illustration on a stochastic multi-population neural mass model</title>
      <link>https://arxiv.org/abs/2306.15787</link>
      <description>arXiv:2306.15787v4 Announce Type: replace 
Abstract: In this article, we propose an adapted sequential Monte Carlo approximate Bayesian computation (SMC-ABC) algorithm for network inference in coupled stochastic differential equations (SDEs) used for multivariate time series modeling. Our approach is motivated by neuroscience, specifically the challenge of estimating brain connectivity before and during epileptic seizures. To this end, we make four key contributions. First, we introduce a 6N-dimensional SDE to model the activity of N coupled neuronal populations, extending the (single-population) stochastic Jansen and Rit neural mass model used to describe human electroencephalography (EEG) rhythms, particularly epileptic activity. Second, we construct a reliable and efficient numerical splitting scheme for the model simulation. Third, we apply the proposed adapted SMC-ABC algorithm to the neural mass model and validate it on different types of simulated data. Compared to standard SMC-ABC, our approach significantly reduces computational cost by requiring fewer model simulations to reach the desired posterior region, thanks to the inclusion of binary parameters describing the presence or absence of coupling directions. Finally, we apply our method to real multi-channel EEG data, uncovering potential similarities in patients' brain activities across different epileptic seizures, as well as differences between pre-seizure and seizure periods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15787v4</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Susanne Ditlevsen, Massimiliano Tamborrino, Irene Tubikanec</dc:creator>
    </item>
    <item>
      <title>Powerful Large-scale Inference in High Dimensional Mediation Analysis</title>
      <link>https://arxiv.org/abs/2402.13933</link>
      <description>arXiv:2402.13933v3 Announce Type: replace 
Abstract: In genome-wide epigenetic studies, exposures (e.g., Single Nucleotide Polymorphisms) affect outcomes (e.g., gene expression) through intermediate variables such as DNA methylation. Mediation analysis offers a way to study these intermediate variables and identify the presence or absence of causal mediation effects. Testing for mediation effects lead to a composite null hypothesis. Existing methods like the Sobel's test or the Max-P test are often underpowered because 1) statistical inference is often conducted based on distributions determined under a subset of the null and 2) they are not designed to shoulder the multiple testing burden. To tackle these issues, we introduce a technique called MLFDR (Mediation Analysis using Local False Discovery Rates) for high dimensional mediation analysis, which uses the local False Discovery Rates based on the coefficients of the structural equation model specifying the mediation relationship to construct a rejection region. We have shown theoretically as well as through simulation studies that in the high-dimensional setting, the new method of identifying the mediating variables controls the FDR asymptotically and performs better with respect to power than several existing methods such as DACT (Liu et al.)and JS-mixture (Dai et al).</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13933v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Asmita Roy, Xianyang Zhang</dc:creator>
    </item>
    <item>
      <title>E-Values for Exponential Families: the General Case</title>
      <link>https://arxiv.org/abs/2409.11134</link>
      <description>arXiv:2409.11134v2 Announce Type: replace 
Abstract: We analyze common types of e-variables and e-processes for composite exponential family nulls: the optimal e-variable based on the reverse information projection (RIPr), the conditional (COND) e-variable, and the universal inference (UI) and sequen\-tialized RIPr e-processes. We characterize the RIPr prior for simple and Bayes-mixture based alternatives, either precisely (for Gaussian nulls and alternatives) or in an approximate sense (general exponential families). We provide conditions under which the RIPr e-variable is (again exactly vs. approximately) equal to the COND e-variable. Based on these and other interrelations which we establish, we determine the e-power of the four e-statistics as a function of sample size, exactly for Gaussian and up to $o(1)$ in general. For $d$-dimensional null and alternative, the e-power of UI tends to be smaller by a term of $(d/2) \log n + O(1)$ than that of the COND e-variable, which is the clear winner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11134v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunda Hao, Peter Gr\"unwald</dc:creator>
    </item>
    <item>
      <title>On MCMC mixing for predictive inference under unidentified transformation models</title>
      <link>https://arxiv.org/abs/2411.01382</link>
      <description>arXiv:2411.01382v3 Announce Type: replace 
Abstract: Reliable Bayesian predictive inference has long been an open problem under unidentified transformation models, since the Markov Chain Monte Carlo (MCMC) chains of posterior predictive distribution (PPD) values are generally poorly mixed. We address the poorly mixed PPD value chains under unidentified transformation models through an adaptive scheme for prior adjustment. Specifically, we originate a conception of sufficient informativeness, which explicitly quantifies the information level provided by nonparametric priors, and assesses MCMC mixing by comparison with the within-chain MCMC variance. We formulate the prior information level by a set of hyperparameters induced from the nonparametric prior elicitation with an analytic expression, which is guaranteed by asymptotic theory for the posterior variance under unidentified transformation models. The analytic prior information level consequently drives a hyperparameter tuning procedure to achieve MCMC mixing. The proposed method is general enough to cover various data domains through a multiplicative error working model. Comprehensive simulations and real-world data analysis demonstrate that our method successfully achieves MCMC mixing and outperforms state-of-the-art competitors in predictive capability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01382v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chong Zhong, Jin Yang, Junshan Shen, Zhaohai Li, Catherine C. Liu</dc:creator>
    </item>
    <item>
      <title>SID: A Novel Class of Nonparametric Tests of Independence for Censored Outcomes</title>
      <link>https://arxiv.org/abs/2412.06311</link>
      <description>arXiv:2412.06311v2 Announce Type: replace 
Abstract: We propose a new class of metrics, called the survival independence divergence (SID), to test dependence between a right-censored outcome and covariates. A key technique for deriving the SIDs is to use a counting process strategy, which equivalently transforms the intractable independence test due to the presence of censoring into a test problem for complete observations. The SIDs are equal to zero if and only if the right-censored response and covariates are independent, and they are capable of detecting various types of nonlinear dependence. We propose empirical estimates of the SIDs and establish their asymptotic properties. We further develop a wild bootstrap method to estimate the critical values and show the consistency of the bootstrap tests. The numerical studies demonstrate that our SID-based tests are highly competitive with existing methods in a wide range of settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06311v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinhong Li, Jicai Liu, Jinhong You, Riquan Zhang</dc:creator>
    </item>
    <item>
      <title>Change Point Detection for Random Objects with Periodic Behavior</title>
      <link>https://arxiv.org/abs/2501.01657</link>
      <description>arXiv:2501.01657v2 Announce Type: replace 
Abstract: Time-varying random objects have been increasingly encountered in modern data analysis. Moreover, in a substantial number of these applications, periodic behaviour of the random objects has been observed. We develop a novel procedure to identify and localize abrupt changes in the distribution of non-Euclidean random objects with periodic behaviour. The proposed procedure is flexible and broadly applicable, accommodating a variety of suitable change point detectors for random objects. We further construct a specific detector used in the proposed procedure which is nonparametric and effectively captures the entire distribution of these random objects. The theoretical results cover the limiting distribution of the detector under the null hypothesis of no change point, the power of the test in the presence of change points under local alternatives and the consistency in estimating the number and locations of change points, whether dealing with a single change point or multiple ones. We demonstrate that the most competitive method currently in the literature for change point detection in random objects is degraded by periodic behaviour, as periodicity leads to blurring of the changes that this procedure aims to discover. Through comprehensive simulation studies, we demonstrate the superior power and accuracy of our approach in both detecting change points and pinpointing their locations. Our main application is to weighted networks, represented through graph Laplacians. The proposed method delivers highly interpretable results, as evidenced by the identification of meaningful change points in the New York City Citi Bike sharing system that align with significant historical events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01657v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiazhen Xu, Andrew T. A. Wood, Tao Zou</dc:creator>
    </item>
    <item>
      <title>Aligning NLP Models with Target Population Perspectives using PAIR: Population-Aligned Instance Replication</title>
      <link>https://arxiv.org/abs/2501.06826</link>
      <description>arXiv:2501.06826v3 Announce Type: replace 
Abstract: Models trained on crowdsourced annotations may not reflect population views, if those who work as annotators do not represent the broader population. In this paper, we propose PAIR: Population-Aligned Instance Replication, a post-processing method that adjusts training data to better reflect target population characteristics without collecting additional annotations. Using simulation studies on offensive language and hate speech detection with varying annotator compositions, we show that non-representative pools degrade model calibration while leaving accuracy largely unchanged. PAIR corrects these calibration problems by replicating annotations from underrepresented annotator groups to match population proportions. We conclude with recommendations for improving the representativity of training data and model performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06826v3</guid>
      <category>stat.ME</category>
      <category>cs.CL</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stephanie Eckman, Bolei Ma, Christoph Kern, Rob Chew, Barbara Plank, Frauke Kreuter</dc:creator>
    </item>
    <item>
      <title>Tipping Point Sensitivity Analysis for Missing Data in Time-to-Event Endpoints: Model-Based and Model-Free Approaches</title>
      <link>https://arxiv.org/abs/2506.19988</link>
      <description>arXiv:2506.19988v2 Announce Type: replace 
Abstract: Missing data frequently occurs in clinical trials with time-to-event endpoints, often due to administrative censoring. Other reasons, such as loss-to-follow up and patient withdrawal of consent, can violate the censoring-at-random assumption hence lead to biased estimates of the treatment effect under treatment policy estimand. Numerous methods have been proposed to conduct sensitivity analyses in these situations, one of which is the tipping point analysis. It aims to evaluate the robustness of trial conclusions by varying certain data and/or model aspects while imputing missing data. We provide an overview of the missing data considerations. The main contribution of this paper lies in categorizing and contrasting tipping point methods as two groups, namely model-based and model-free approaches, where the latter is under-emphasized in the literature. We highlight their important differences in terms of assumptions, behaviors and interpretations. Through two case studies and simulations under various scenarios, we provide insight on how different tipping point methods impact the interpretation of trial outcomes. Through these comparisons, we aim to provide a practical guide to conduct tipping point analyses from the choice of methods to ways of conducting clinical plausibility assessment, and ultimately contributing to more robust and reliable interpretation of clinical trial results in the presence of missing data for time-to-event endpoints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19988v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ajmal Oodally, Craig Wang, Zheng Li, Arunava Chakravartty</dc:creator>
    </item>
    <item>
      <title>Estimating the average treatment effect in cluster-randomized trials with misclassified outcomes and non-random validation subsets</title>
      <link>https://arxiv.org/abs/2508.18137</link>
      <description>arXiv:2508.18137v2 Announce Type: replace 
Abstract: Randomized trials are viewed as the benchmark for assessing causal effects of treatments on outcomes of interest. Nonetheless, challenges such as measurement error can undermine the standard causal assumptions for randomized trials. In ASPIRE, a cluster-randomized trial, pediatric primary care clinics were assigned to one of two treatments aimed at promoting clinician delivery of a secure firearm program to parents during well-child visits. A key outcome of interest is thus parent receipt of the program at each visit. Clinicians documented program delivery in patients' electronic health records for all visits, but their reporting is a proxy measure for the parent receipt outcome. Parents were also surveyed to report directly on program receipt after their child's visit; however, only a small subset of them completed the survey. Here, we develop a causal inference framework for a binary outcome that is subject to misclassification through silver-standard measures (clinician reports), but gold-standard measures (parent reports) are only available for a non-random internal validation subset. We propose a method for identifying the average treatment effect (ATE) that addresses the risk of bias due to misclassification and non-random validation selection, even when the outcome (parent receipt) may directly impact selection propensity (survey responsiveness). We show that ATE estimation relies on specifying the relationship between the gold- and silver-standard outcome measures in the validation subset, which may depend on treatment and covariates. Additionally, the clustered design is reflected in our causal assumptions and in our cluster-robust approach to estimation of the ATE. Simulation studies demonstrate acceptable finite-sample operating characteristics of our ATE estimator, supporting its application to ASPIRE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18137v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dane Isenberg, Nandita Mitra, Steven C. Marcus, Rinad S. Beidas, Kristin A. Linn</dc:creator>
    </item>
    <item>
      <title>Solving Fredholm Integral Equations of the Second Kind via Wasserstein Gradient Flows</title>
      <link>https://arxiv.org/abs/2409.19642</link>
      <description>arXiv:2409.19642v2 Announce Type: replace-cross 
Abstract: Motivated by a recent method for approximate solution of Fredholm equations of the first kind, we develop a corresponding method for a class of Fredholm equations of the \emph{second kind}. In particular, we consider the class of equations for which the solution is a probability measure. The approach centres around specifying a functional whose gradient flow admits a minimizer corresponding to a regularized version of the solution of the underlying equation and using a mean-field particle system to approximately simulate that flow. Theoretical support for the method is presented, along with some illustrative numerical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19642v2</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesca R. Crucinio, Adam M. Johansen</dc:creator>
    </item>
  </channel>
</rss>
