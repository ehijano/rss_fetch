<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Nov 2025 02:54:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Physics-Informed Neural Network-based Reliability Analysis of Buried Pipelines</title>
      <link>https://arxiv.org/abs/2511.11613</link>
      <description>arXiv:2511.11613v1 Announce Type: cross 
Abstract: Buried pipelines transporting oil and gas across geohazard-prone regions are exposed to potential ground movement, leading to the risk of significant strain demand and structural failure. Reliability analysis, which determines the probability of failure after accounting for pertinent uncertainties, is essential for ensuring the safety of pipeline systems. However, traditional reliability analysis methods involving computationally intensive numerical models, such as finite element simulations of pipeline subjected to ground movement, have limited applications; this is partly because stochastic sampling approaches require repeated simulations over a large number of samples for the uncertain variables when estimating low probabilities. This study introduces Physics-Informed Neural Network for Reliability Analysis (PINN-RA) for buried pipelines subjected to ground movement, which integrates PINN-based surrogate model with Monte Carlo Simulation (MCS) to achieve efficient reliability assessment. To enable its application under uncertain variables associated with soil properties and ground movement, the PINN-based surrogate model is extended to solve a parametric differential equation system, namely the governing equation of pipelines embedded in soil with different properties. The findings demonstrate that PINN-RA significantly reduces the computational effort required and thus accelerates reliability analysis. By eliminating the need for repetitive numerical evaluations of pipeline subjected to permanent ground movement, the proposed approach provides an efficient and scalable tool for pipeline reliability assessment, enabling rapid decision-making in geohazard-prone regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11613v1</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pouya Taraghi, Yong Li, Samer Adeeb</dc:creator>
    </item>
    <item>
      <title>Universalities in the Avalanche Dynamics of Novelties and Non-Novelties</title>
      <link>https://arxiv.org/abs/2511.12637</link>
      <description>arXiv:2511.12637v1 Announce Type: cross 
Abstract: Unprecedented events intertwine with the repetition of the past in natural phenomena and human activities. Key statistical patterns, such as Heaps' and Taylor's laws and Zipf's law, have been identified as characterizing the dynamical processes that govern the emergence of novelties and the abundance of repeated elements. Observing these statistical regularities has been pivotal in motivating the search for modeling schemes that can explain them and clarify key mechanisms underlying the appearance of new elements and their subsequent recurrence. In this study, we analyze sequences of novel and non-novel elements, referred to as avalanches, in real-world systems. We show that avalanche statistics provide a complementary characterization of innovation dynamics, extending beyond the three fundamental laws mentioned above. Although arising from collective dynamics, some systems behave as a single instance of a stochastic process. Others, such as natural language, exhibit features that we can only explain by a superposition of different dynamics. This distinction is not apparent when considering Heaps' law alone, while it clearly emerges in the avalanche statistics. By interpreting these empirical observations, we also advance the theoretical understanding of urn-based models that successfully reproduce the observed behaviors associated with Heaps', Zipf's, and Taylor's laws. We derive analytical expressions that accurately describe the probability distributions of avalanches and the Heaps law beyond its asymptotic regime. Building on these results, we derive a scaling relation that we show also holds in real-world systems, indicating a form of universality in the dynamics of novelty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12637v1</guid>
      <category>physics.soc-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filippo Santoro, Alberto Petri, Francesca Tria</dc:creator>
    </item>
    <item>
      <title>NuBench: An Open Benchmark for Deep Learning-Based Event Reconstruction in Neutrino Telescopes</title>
      <link>https://arxiv.org/abs/2511.13111</link>
      <description>arXiv:2511.13111v1 Announce Type: cross 
Abstract: Neutrino telescopes are large-scale detectors designed to observe Cherenkov radiation produced from neutrino interactions in water or ice. They exist to identify extraterrestrial neutrino sources and to probe fundamental questions pertaining to the elusive neutrino itself. A central challenge common across neutrino telescopes is to solve a series of inverse problems known as event reconstruction, which seeks to resolve properties of the incident neutrino, based on the detected Cherenkov light. In recent times, significant efforts have been made in adapting advances from deep learning research to event reconstruction, as such techniques provide several benefits over traditional methods. While a large degree of similarity in reconstruction needs and low-level data exists, cross-experimental collaboration has been hindered by a lack of diverse open-source datasets for comparing methods.
  We present NuBench, an open benchmark for deep learning-based event reconstruction in neutrino telescopes. NuBench comprises seven large-scale simulated datasets containing nearly 130 million charged- and neutral-current muon-neutrino interactions spanning 10 GeV to 100 TeV, generated across six detector geometries inspired by existing and proposed experiments. These datasets provide pulse- and event-level information suitable for developing and comparing machine-learning reconstruction methods in both water and ice environments. Using NuBench, we evaluate four reconstruction algorithms - ParticleNeT and DynEdge, both actively used within the KM3NeT and IceCube collaborations, respectively, along with GRIT and DeepIce - on up to five core tasks: energy and direction reconstruction, topology classification, interaction vertex prediction, and inelasticity estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13111v1</guid>
      <category>hep-ex</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>physics.ins-det</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rasmus F. Orsoe, Stephan Meighen-Berger, Jeffrey Lazar, Jorge Prado, Ivan Mozun-Mateo, Aske Rosted, Philip Weigel, Arturo Llorente Anaya</dc:creator>
    </item>
    <item>
      <title>The correlated matching decoder for the 4.8.8 color code</title>
      <link>https://arxiv.org/abs/2511.13192</link>
      <description>arXiv:2511.13192v1 Announce Type: cross 
Abstract: Color codes present distinct advantages for fault-tolerant quantum computing, such as high encoding rates and the transversal implementation of Clifford gates. However, existing matching-based decoders for the color codes such as the restricted decoder (Kubica and Delfosse, 2023), suffer from limited decoding performance. Inspired by the global decoding insight of the unified decoder (Benhemou et al., 2023), this paper introduces a correlated decoder for the 4.8.8 color code, which improves upon the conventional restricted decoder by leveraging correlations between restricted lattices, and is derived by mapping the correlated matching decoder for the surface code onto the color code lattice. Analytical and numerical results show that the correlated decoder achieves higher thresholds than the restricted and unified decoders, while matching the performance of the unified decoder at very low physical error rates. Under the code capacity and phenomenological noise models, the estimated thresholds for the color code against bit-flip error are 10.38% and 3.13%, respectively. Furthermore, by applying the surface-color code mapping, the thresholds of 16.62% and 3.52% are obtained for the surface code against depolarizing noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13192v1</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yantong Liu, Junjie Wu, Lingling Lao</dc:creator>
    </item>
    <item>
      <title>Stationary Distributions of the Mode-switching Chiarella Model</title>
      <link>https://arxiv.org/abs/2511.13277</link>
      <description>arXiv:2511.13277v1 Announce Type: cross 
Abstract: We derive the stationary distribution in various regimes of the extended Chiarella model of financial markets. This model is a stochastic nonlinear dynamical system that encompasses dynamical competition between a (saturating) trending and a mean-reverting component. We find the so-called mispricing distribution and the trend distribution to be unimodal Gaussians in the small noise, small feedback limit. Slow trends yield Gaussian-cosh mispricing distributions that allow for a P-bifurcation: unimodality occurs when mean-reversion is fast, bimodality when it is slow. The critical point of this bifurcation is established and refutes previous ad-hoc reports and differs from the bifurcation condition of the dynamical system itself. For fast, weakly coupled trends, deploying the Furutsu-Novikov theorem reveals that the result is again unimodal Gaussian. For the same case with higher coupling we disprove another claim from the literature: bimodal trend distributions do not generally imply bimodal mispricing distributions. The latter becomes bimodal only for stronger trend feedback. The exact solution in this last regime remains unfortunately beyond our proficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13277v1</guid>
      <category>q-fin.TR</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jutta G. Kurth, Jean-Philippe Bouchaud</dc:creator>
    </item>
    <item>
      <title>Scientific Data Compression and Super-Resolution Sampling</title>
      <link>https://arxiv.org/abs/2511.13675</link>
      <description>arXiv:2511.13675v1 Announce Type: cross 
Abstract: Modern scientific simulations, observations, and large-scale experiments generate data at volumes that often exceed the limits of storage, processing, and analysis. This challenge drives the development of data reduction methods that efficiently manage massive datasets while preserving essential physical features and quantities of interest. In many scientific workflows, it is also crucial to enable data recovery from compressed representations - a task known as super-resolution - with guarantees on the preservation of key physical characteristics. A notable example is checkpointing and restarting, which is essential for long-running simulations to recover from failures, resume after interruptions, or examine intermediate results. In this work, we introduce a novel framework for scientific data compression and super-resolution, grounded in recent advances in learning exponential families. Our method preserves and quantifies uncertainty in physical quantities of interest and supports flexible trade-offs between compression ratio and reconstruction fidelity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13675v1</guid>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minh Vu, Andrey Lokhov</dc:creator>
    </item>
    <item>
      <title>Particle Identification with MLPs and PINNs Using HADES Data</title>
      <link>https://arxiv.org/abs/2509.17685</link>
      <description>arXiv:2509.17685v2 Announce Type: replace 
Abstract: In experimental nuclear and particle physics, the extraction of high-purity samples of rare events critically depends on the efficiency and accuracy of particle identification (PID). In this work, we present a PID method applied to HADES data at the level of fully reconstructed particle track candidates. The results demonstrate a significant improvement in PID performance compared to conventional techniques, highlighting the potential of physics-informed neural networks as a powerful tool for future data analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17685v2</guid>
      <category>physics.data-an</category>
      <category>nucl-ex</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marvin Kohls</dc:creator>
    </item>
    <item>
      <title>Exploring the BSM parameter space with Neural Network aided Simulation-Based Inference</title>
      <link>https://arxiv.org/abs/2502.11928</link>
      <description>arXiv:2502.11928v2 Announce Type: replace-cross 
Abstract: Some of the issues that make sampling parameter spaces of various beyond the Standard Model (BSM) scenarios computationally expensive are the high dimensionality of the input parameter space, complex likelihoods, and stringent experimental constraints. In this work, we explore likelihood-free approaches, leveraging neural network-aided Simulation-Based Inference (SBI) to alleviate this issue. We focus on three amortized SBI methods: Neural Posterior Estimation (NPE), Neural Likelihood Estimation (NLE), and Neural Ratio Estimation (NRE) and perform a comparative analysis through the validation test known as the \textit{ Test of Accuracy with Random Points} (TARP), as well as through posterior sample efficiency and computational time. As an example, we focus on the scalar sector of the phenomenological minimal supersymmetric SM (pMSSM) and observe that the NPE method outperforms the others and generates correct posterior distributions of the parameters with a minimal number of samples. The efficacy of this framework is tested on 5 parameter pMSSM with Higgs and flavor physics data and its performance is compared with the MCMC method. We further add dark matter (DM) observables to make the task more challenging and consider a 9 parameter pMSSM. We observe that even though the efficiency factor drops, the amortized SBI method still produces faithful posterior distributions. SBI predicted points satisfying DM constraints are mostly bino-dominated upto $\sim$ 1.5 TeV, and are mostly wino-dominated within the 1.5 - 2 TeV range.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11928v2</guid>
      <category>hep-ph</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Atrideb Chatterjee, Arghya Choudhury, Sourav Mitra, Arpita Mondal, Subhadeep Mondal</dc:creator>
    </item>
    <item>
      <title>On the emergence of numerical instabilities in Next Generation Reservoir Computing</title>
      <link>https://arxiv.org/abs/2505.00846</link>
      <description>arXiv:2505.00846v2 Announce Type: replace-cross 
Abstract: Next Generation Reservoir Computing (NGRC) is a low-cost machine learning method for forecasting chaotic time series from data. Computational efficiency is crucial for scalable reservoir computing, requiring better strategies to reduce training cost. In this work, we uncover a connection between the numerical conditioning of the NGRC feature matrix -- formed by polynomial evaluations on time-delay coordinates -- and the long-term NGRC dynamics. We show that NGRC can be trained without regularization, reducing computational time. Our contributions are twofold. First, merging tools from numerical linear algebra and ergodic theory of dynamical systems, we systematically study how the feature matrix conditioning varies across hyperparameters. We demonstrate that the NGRC feature matrix tends to be ill-conditioned for short time lags, high-degree polynomials, and short length of training data. Second, we evaluate the impact of different numerical algorithms (Cholesky, singular value decomposition (SVD), and lower-upper (LU) decomposition) for solving the regularized least-squares problem. Our results reveal that SVD-based training achieves accurate forecasts without regularization, being preferable when compared against the other algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00846v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Edmilson Roque dos Santos, Erik Bollt</dc:creator>
    </item>
  </channel>
</rss>
