<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Mar 2025 01:58:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A primer on optimal transport for causal inference with observational data</title>
      <link>https://arxiv.org/abs/2503.07811</link>
      <description>arXiv:2503.07811v1 Announce Type: new 
Abstract: The theory of optimal transportation has developed into a powerful and elegant framework for comparing probability distributions, with wide-ranging applications in all areas of science. The fundamental idea of analyzing probabilities by comparing their underlying state space naturally aligns with the core idea of causal inference, where understanding and quantifying counterfactual states is paramount. Despite this intuitive connection, explicit research at the intersection of optimal transport and causal inference is only beginning to develop. Yet, many foundational models in causal inference have implicitly relied on optimal transport principles for decades, without recognizing the underlying connection. Therefore, the goal of this review is to offer an introduction to the surprisingly deep existing connections between optimal transport and the identification of causal effects with observational data -- where optimal transport is not just a set of potential tools, but actually builds the foundation of model assumptions. As a result, this review is intended to unify the language and notation between different areas of statistics, mathematics, and econometrics, by pointing out these existing connections, and to explore novel problems and directions for future work in both areas derived from this realization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07811v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian F Gunsilius</dc:creator>
    </item>
    <item>
      <title>Health Prognostics in Multi-sensor Systems Based on Multivariate Functional Data Analysis</title>
      <link>https://arxiv.org/abs/2503.07854</link>
      <description>arXiv:2503.07854v1 Announce Type: new 
Abstract: Recent developments in big data analysis, machine learning, Industry 4.0, and IoT applications have enabled the monitoring and processing of multi-sensor data collected from systems, allowing for the prediction of the "Remaining Useful Life" (RUL) of system components. Particularly in the aviation industry, Prognostic Health Management (PHM) has become one of the most important practices for ensuring reliability and safety. Not only is the accuracy of RUL prediction important, but the implementability of techniques, domain adaptability, and interpretability of system degradation behaviors have also become essential. In this paper, the data collected from the multi-sensor environment of complex systems are processed using a Functional Data Analysis (FDA) approach to predict when the systems will fail and to understand and interpret the systems' life cycles. The approach is applied to the C-MAPSS datasets shared by National Aeronautics and Space Administration, and the behaviors of the sensors in aircraft engine failures are adaptively modeled with Multivariate Functional Principal Component Analysis (MFPCA). While the results indicate that the proposed method predicts the RUL competitively compared to other methods in the literature, it also demonstrates how multivariate Functional Data Analysis is useful for interpretability in prognostic studies within multi-sensor environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07854v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cevahir Yildirim, Alba M. Franco-Pereira, Rosa E. Lillo</dc:creator>
    </item>
    <item>
      <title>On "confirmatory" methodological research in statistics and related fields</title>
      <link>https://arxiv.org/abs/2503.08124</link>
      <description>arXiv:2503.08124v1 Announce Type: new 
Abstract: Empirical substantive research, such as in the life or social sciences, is commonly categorized into the two modes exploratory and confirmatory, both of which are essential to scientific progress. The former is also referred to as hypothesis-generating or data-contingent research, the latter is also called hypothesis-testing research. In the context of empirical methodological research in statistics, however, the exploratory-confirmatory distinction has received very little attention so far. Our paper aims to fill this gap. First, we revisit the concept of empirical methodological research through the lens of the exploratory-confirmatory distinction. Secondly, we examine current practice with respect to this distinction through a literature survey including 115 articles from the field of biostatistics. Thirdly, we provide practical recommendations towards more appropriate design, interpretation, and reporting of empirical methodological research in light of this distinction. In particular, we argue that both modes of research are crucial to methodological progress, but that most published studies - even if sometimes disguised as confirmatory - are essentially of exploratory nature. We emphasize that it may be adequate to consider empirical methodological research as a continuum between "pure" exploration and "strict" confirmation, recommend to transparently report the mode of conducted research within the spectrum between exploratory and confirmatory, and stress the importance of study protocols written before conducting the study, especially in confirmatory methodological research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08124v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>F. J. D. Lange, Juliane Wilcke, Sabine Hoffmann, Moritz Herrmann, Anne-Laure Boulesteix</dc:creator>
    </item>
    <item>
      <title>Causal Networks of Infodemiological Data: Modelling Dermatitis</title>
      <link>https://arxiv.org/abs/2503.08252</link>
      <description>arXiv:2503.08252v1 Announce Type: new 
Abstract: Environmental and mental conditions are known risk factors for dermatitis and symptoms of skin inflammation, but their interplay is difficult to quantify; epidemiological studies rarely include both, along with possible confounding factors. Infodemiology leverages large online data sets to address this issue, but fusing them produces strong patterns of spatial and temporal correlation, missingness, and heterogeneity.
  In this paper, we design a causal network that correctly models these complex structures in large-scale infodemiological data from Google, EPA, NOAA and US Census (434 US counties, 134 weeks). Our model successfully captures known causal relationships between weather patterns, pollutants, mental conditions, and dermatitis. Key findings reveal that anxiety accounts for 57.4% of explained variance in dermatitis, followed by NO2 (33.9%), while environmental factors show significant mediation effects through mental conditions. The model predicts that reducing PM2.5 emissions by 25% could decrease dermatitis prevalence by 18%. Through statistical validation and causal inference, we provide unprecedented insights into the complex interplay between environmental and mental health factors affecting dermatitis, offering valuable guidance for public health policies and environmental regulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08252v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Scutari, Samir Salah, Delphine Kerob, Jean Krutmann</dc:creator>
    </item>
    <item>
      <title>Continuously updated estimation of conditional hazard functions</title>
      <link>https://arxiv.org/abs/2503.08356</link>
      <description>arXiv:2503.08356v1 Announce Type: new 
Abstract: Motivated by the need to analyze continuously updated data sets in the context of time-to-event modeling, we propose a novel nonparametric approach to estimate the conditional hazard function given a set of continuous and discrete predictors. The method is based on a representation of the conditional hazard as a ratio between a joint density and a conditional expectation determined by the distribution of the observed variables. It is shown that such ratio representations are available for uni- and bivariate time-to-events, in the presence of common types of random censoring, truncation, and with possibly cured individuals, as well as for competing risks. This opens the door to nonparametric approaches in many time-to-event predictive models. To estimate joint densities and conditional expectations we propose the recursive kernel smoothing, which is well suited for online estimation. Asymptotic results for such estimators are derived and it is shown that they achieve optimal convergence rates. Simulation experiments show the good finite sample performance of our recursive estimator with right censoring. The method is applied to a real dataset of primary breast cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08356v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daphn\'e Aurouet, Valentin Patilea</dc:creator>
    </item>
    <item>
      <title>Clustered Flexible Calibration Plots For Binary Outcomes Using Random Effects Modeling</title>
      <link>https://arxiv.org/abs/2503.08389</link>
      <description>arXiv:2503.08389v1 Announce Type: new 
Abstract: Evaluation of clinical prediction models across multiple clusters, whether centers or datasets, is becoming increasingly common. A comprehensive evaluation includes an assessment of the agreement between the estimated risks and the observed outcomes, also known as calibration. Calibration is of utmost importance for clinical decision making with prediction models and it may vary between clusters. We present three approaches to take clustering into account when evaluating calibration. (1) Clustered group calibration (CG-C), (2) two stage meta-analysis calibration (2MA-C) and (3) mixed model calibration (MIX-C) can obtain flexible calibration plots with random effects modelling and providing confidence and prediction intervals. As a case example, we externally validate a model to estimate the risk that an ovarian tumor is malignant in multiple centers (N = 2489). We also conduct a simulation study and synthetic data study generated from a true clustered dataset to evaluate the methods. In the simulation and the synthetic data analysis MIX-C gave estimated curves closest to the true overall and center specific curves. Prediction interval was best for 2MA-C with splines. Standard flexible calibration worked likewise in terms of calibration error when sample size is limited. We recommend using 2MA-C (splines) to estimate the curve with the average effect and the 95% PI and MIX-C for the cluster specific curves, specially when sample size per cluster is limited. We provide ready-to-use code to construct summary flexible calibration curves with confidence and prediction intervals to assess heterogeneity in calibration across datasets or centers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08389v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lasai Barre\~nada, Bavo D. C. Campo, Laure Wynants, Ben Van Calster</dc:creator>
    </item>
    <item>
      <title>Information Criterion for the Gaussian and/or Laplace Distribution Models</title>
      <link>https://arxiv.org/abs/2503.08458</link>
      <description>arXiv:2503.08458v1 Announce Type: new 
Abstract: The information criterion AIC has been used successfully in many areas of statistical modeling, and since it is derived based on the Taylor expansion of the log-likelihood function and the asymptotic distribution of the maximum likelihood estimator, it is not directly justified for likelihood functions that include non-differentiable points such as the Laplace distribution. In fact, it is known to work effectively in many such cases. In this paper, we attempt to evaluate the bias correction directly for the case where the true model or the model to be estimated is a simple Laplace distribution model. As a result, an approximate expression for the bias correction term was obtained. Numerical results show that the AIC approximations are relatively good except when the Gauss distribution model is fitted to data following the Laplace distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08458v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Genshiro Kitagawa (Tokyo University of Marine Science,Technology,The Institute of Statistical Mathmatics)</dc:creator>
    </item>
    <item>
      <title>A Multi-Omics Framework for Survival Mediation Analysis of High-Dimensional Proteogenomic Data</title>
      <link>https://arxiv.org/abs/2503.08606</link>
      <description>arXiv:2503.08606v1 Announce Type: new 
Abstract: Survival analysis plays a crucial role in understanding time-to-event (survival) outcomes such as disease progression. Despite recent advancements in causal mediation frameworks for survival analysis, existing methods are typically based on Cox regression and primarily focus on a single exposure or individual omics layers, often overlooking multi-omics interplay. This limitation hinders the full potential of integrated biological insights. In this paper, we propose SMAHP, a novel method for survival mediation analysis that simultaneously handles high-dimensional exposures and mediators, integrates multi-omics data, and offers a robust statistical framework for identifying causal pathways on survival outcomes. This is one of the first attempts to introduce the accelerated failure time (AFT) model within a multi-omics causal mediation framework for survival outcomes. Through simulations across multiple scenarios, we demonstrate that SMAHP achieves high statistical power, while effectively controlling false discovery rate (FDR), compared with two other approaches. We further apply SMAHP to the largest head-and-neck carcinoma proteogenomic data, detecting a gene mediated by a protein that influences survival time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08606v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seungjun Ahn, Weijia Fu, Maaike van Gerwen, Lei Liu, Zhigang Li</dc:creator>
    </item>
    <item>
      <title>On a new robust method of inference for general time series models</title>
      <link>https://arxiv.org/abs/2503.08655</link>
      <description>arXiv:2503.08655v1 Announce Type: new 
Abstract: In this article, we propose a novel logistic quasi-maximum likelihood estimation (LQMLE) for general parametric time series models. Compared to the classical Gaussian QMLE and existing robust estimations, it enjoys many distinctive advantages, such as robustness in respect of distributional misspecification and heavy-tailedness of the innovation, more resiliency to outliers, smoothness and strict concavity of the log logistic quasi-likelihood function, and boundedness of the influence function among others. Under some mild conditions, we establish the strong consistency and asymptotic normality of the LQMLE. Moreover, we propose a new and vital parameter identifiability condition to ensure desirable asymptotics of the LQMLE. Further, based on the LQMLE, we consider the Wald test and the Lagrange multiplier test for the unknown parameters, and derive the limiting distributions of the corresponding test statistics. The applicability of our methodology is demonstrated by several time series models, including DAR, GARCH, ARMA-GARCH, DTARMACH, and EXPAR. Numerical simulation studies are carried out to assess the finite-sample performance of our methodology, and an empirical example is analyzed to illustrate its usefulness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08655v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihan Wang, Xinghao Qiao, Dong Li, Howell Tong</dc:creator>
    </item>
    <item>
      <title>How do the professional players select their shot locations? An analysis of Field Goal Attempts via Bayesian Additive Regression Trees</title>
      <link>https://arxiv.org/abs/2503.07789</link>
      <description>arXiv:2503.07789v1 Announce Type: cross 
Abstract: Basketball analytics has significantly advanced our understanding of the game, with shot selection emerging as a critical factor in both individual and team performance. With the advent of player tracking technologies, a wealth of granular data on shot attempts has become available, enabling a deeper analysis of shooting behavior. However, modeling shot selection presents unique challenges due to the spatial and contextual complexities influencing shooting decisions. This paper introduces a novel approach to the analysis of basketball shot data, focusing on the spatial distribution of shot attempts, also known as intensity surfaces. We model these intensity surfaces using a Functional Bayesian Additive Regression Trees (FBART) framework, which allows for flexible, nonparametric regression, and uncertainty quantification while addressing the nonlinearity and nonstationarity inherent in shot selection patterns to provide a more accurate representation of the factors driving player performance; we further propose the Adaptive Functional Bayesian Additive Regression Trees (AFBART) model, which builds on FBART by introducing adaptive basis functions for improved computational efficiency and model fit. AFBART is particularly well suited for the analysis of two-dimensional shot intensity surfaces and provides a robust tool for uncovering latent patterns in shooting behavior. Through simulation studies and real-world applications to NBA player data, we demonstrate the effectiveness of the model in quantifying shooting tendencies, improving performance predictions, and informing strategic decisions for coaches, players, and team managers. This work represents a significant step forward in the statistical modeling of basketball shot selection and its applications in optimizing game strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07789v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiahao Cao, Hou-Cheng Yang, Guanyu Hu</dc:creator>
    </item>
    <item>
      <title>Counterfactual Explanations for Model Ensembles Using Entropic Risk Measures</title>
      <link>https://arxiv.org/abs/2503.07934</link>
      <description>arXiv:2503.07934v1 Announce Type: cross 
Abstract: Counterfactual explanations indicate the smallest change in input that can translate to a different outcome for a machine learning model. Counterfactuals have generated immense interest in high-stakes applications such as finance, education, hiring, etc. In several use-cases, the decision-making process often relies on an ensemble of models rather than just one. Despite significant research on counterfactuals for one model, the problem of generating a single counterfactual explanation for an ensemble of models has received limited interest. Each individual model might lead to a different counterfactual, whereas trying to find a counterfactual accepted by all models might significantly increase cost (effort). We propose a novel strategy to find the counterfactual for an ensemble of models using the perspective of entropic risk measure. Entropic risk is a convex risk measure that satisfies several desirable properties. We incorporate our proposed risk measure into a novel constrained optimization to generate counterfactuals for ensembles that stay valid for several models. The main significance of our measure is that it provides a knob that allows for the generation of counterfactuals that stay valid under an adjustable fraction of the models. We also show that a limiting case of our entropic-risk-based strategy yields a counterfactual valid for all models in the ensemble (worst-case min-max approach). We study the trade-off between the cost (effort) for the counterfactual and its validity for an ensemble by varying degrees of risk aversion, as determined by our risk parameter knob. We validate our performance on real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07934v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erfaun Noorani, Pasan Dissanayake, Faisal Hamman, Sanghamitra Dutta</dc:creator>
    </item>
    <item>
      <title>CAD-VAE: Leveraging Correlation-Aware Latents for Comprehensive Fair Disentanglement</title>
      <link>https://arxiv.org/abs/2503.07938</link>
      <description>arXiv:2503.07938v1 Announce Type: cross 
Abstract: While deep generative models have significantly advanced representation learning, they may inherit or amplify biases and fairness issues by encoding sensitive attributes alongside predictive features. Enforcing strict independence in disentanglement is often unrealistic when target and sensitive factors are naturally correlated. To address this challenge, we propose CAD-VAE (Correlation-Aware Disentangled VAE), which introduces a correlated latent code to capture the shared information between target and sensitive attributes. Given this correlated latent, our method effectively separates overlapping factors without extra domain knowledge by directly minimizing the conditional mutual information between target and sensitive codes. A relevance-driven optimization strategy refines the correlated code by efficiently capturing essential correlated features and eliminating redundancy. Extensive experiments on benchmark datasets demonstrate that CAD-VAE produces fairer representations, realistic counterfactuals, and improved fairness-aware image editing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07938v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenrui Ma, Rongchang Zhao, Xi Xiao, Hongyang Xie, Tianyang Wang, Xiao Wang, Hao Zhang, Yanning Shen</dc:creator>
    </item>
    <item>
      <title>Empirical Error Estimates for Graph Sparsification</title>
      <link>https://arxiv.org/abs/2503.08031</link>
      <description>arXiv:2503.08031v1 Announce Type: cross 
Abstract: Graph sparsification is a well-established technique for accelerating graph-based learning algorithms, which uses edge sampling to approximate dense graphs with sparse ones. Because the sparsification error is random and unknown, users must contend with uncertainty about the reliability of downstream computations. Although it is possible for users to obtain conceptual guidance from theoretical error bounds in the literature, such results are typically impractical at a numerical level. Taking an alternative approach, we propose to address these issues from a data-driven perspective by computing empirical error estimates. The proposed error estimates are highly versatile, and we demonstrate this in four use cases: Laplacian matrix approximation, graph cut queries, graph-structured regression, and spectral clustering. Moreover, we provide two theoretical guarantees for the error estimates, and explain why the cost of computing them is manageable in comparison to the overall cost of a typical graph sparsification workflow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08031v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siyao Wang, Miles E. Lopes</dc:creator>
    </item>
    <item>
      <title>Locally Private Nonparametric Contextual Multi-armed Bandits</title>
      <link>https://arxiv.org/abs/2503.08098</link>
      <description>arXiv:2503.08098v1 Announce Type: cross 
Abstract: Motivated by privacy concerns in sequential decision-making on sensitive data, we address the challenge of nonparametric contextual multi-armed bandits (MAB) under local differential privacy (LDP). We develop a uniform-confidence-bound-type estimator, showing its minimax optimality supported by a matching minimax lower bound. We further consider the case where auxiliary datasets are available, subject also to (possibly heterogeneous) LDP constraints. Under the widely-used covariate shift framework, we propose a jump-start scheme to effectively utilize the auxiliary data, the minimax optimality of which is further established by a matching lower bound. Comprehensive experiments on both synthetic and real-world datasets validate our theoretical results and underscore the effectiveness of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08098v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuheng Ma, Feiyu Jiang, Zifeng Zhao, Hanfang Yang, Yi Yu</dc:creator>
    </item>
    <item>
      <title>Sparsity-Induced Global Matrix Autoregressive Model with Auxiliary Network Data</title>
      <link>https://arxiv.org/abs/2503.08579</link>
      <description>arXiv:2503.08579v1 Announce Type: cross 
Abstract: Jointly modeling and forecasting economic and financial variables across a large set of countries has long been a significant challenge. Two primary approaches have been utilized to address this issue: the vector autoregressive model with exogenous variables (VARX) and the matrix autoregression (MAR). The VARX model captures domestic dependencies, but treats variables exogenous to represent global factors driven by international trade. In contrast, the MAR model simultaneously considers variables from multiple countries but ignores the trade network. In this paper, we propose an extension of the MAR model that achieves these two aims at once, i.e., studying both international dependencies and the impact of the trade network on the global economy. Additionally, we introduce a sparse component to the model to differentiate between systematic and idiosyncratic cross-predictability. To estimate the model parameters, we propose both a likelihood estimation method and a bias-corrected alternating minimization version. We provide theoretical and empirical analyses of the model's properties, alongside presenting intriguing economic insights derived from our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08579v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanyou Wu, Dan Yang, Yan Xu, Long Feng</dc:creator>
    </item>
    <item>
      <title>Sparse, Low-bias, and Scalable Estimation of High Dimensional Vector Autoregressive Models via Union of Intersections</title>
      <link>https://arxiv.org/abs/1908.11464</link>
      <description>arXiv:1908.11464v2 Announce Type: replace 
Abstract: Vector autoregressive (VAR) models are widely used for causal discovery and forecasting in multivariate time series analyses in fields as diverse as neuroscience, environmental science, and econometrics. In the high-dimensional setting, model parameters are typically estimated by L1-regularized maximum likelihood; yet, when applied to VAR models, this technique produces a sizable trade-off between sparsity and bias with the choice of the regularization hyperparameter, and thus between causal discovery and prediction. That is, low-bias estimation entails dense parameter selection, and sparse selection entails increased bias; the former is useful in forecasting but less likely to yield scientific insight leading to discovery of causal influences, and conversely for the latter. This paper presents a scalable algorithm for simultaneous low-bias and low-variance estimation (hence good prediction) with sparse selection for high-dimensional VAR models. The method leverages the recently developed Union of Intersections (UoI) algorithmic framework for flexible, modular, and scalable feature selection and estimation that allows control of false discovery and false omission in feature selection while maintaining low bias and low variance. This paper demonstrates the superior performance of the UoI-VAR algorithm compared with other methods in simulation studies, exhibits its application in data analysis, and illustrates its good algorithmic scalability in multi-node distributed memory implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:1908.11464v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Trevor Ruiz, Mahesh Balasubramanian, Kristofer E. Bouchard, Sharmodeep Bhattacharyya</dc:creator>
    </item>
    <item>
      <title>Semiparametric logistic regression for inference on relative vaccine efficacy in case-only studies with informative missingness</title>
      <link>https://arxiv.org/abs/2303.11462</link>
      <description>arXiv:2303.11462v2 Announce Type: replace 
Abstract: We develop semiparametric methods for estimating subgroup-specific relative vaccine efficacy against multiple viral strains in a partially vaccinated population. Focusing on observational case-only studies, we address informative missingness in strain type due to vaccination status, pre-vaccination characteristics, and post-infection factors such as viral load. We establish general conditions for the nonparametric identification of relative conditional vaccine efficacy between strains using covariate-adjusted conditional odds ratio parameters. Assuming a log-linear parametric form for strain-specific conditional vaccine efficacy, we propose targeted maximum likelihood estimators based on partially linear logistic regression, leveraging machine learning for flexible confounding adjustment. Finally, we apply our methods to estimate relative strain-specific conditional vaccine efficacy in the ENSEMBLE COVID-19 vaccine trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.11462v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars van der Laan, Peter B. Gilbert</dc:creator>
    </item>
    <item>
      <title>Multiple Testing of Linear Forms for Noisy Matrix Completion</title>
      <link>https://arxiv.org/abs/2312.00305</link>
      <description>arXiv:2312.00305v2 Announce Type: replace 
Abstract: Many important tasks of large-scale recommender systems can be naturally cast as testing multiple linear forms for noisy matrix completion. These problems, however, present unique challenges because of the subtle bias-and-variance tradeoff of and an intricate dependence among the estimated entries induced by the low-rank structure. In this paper, we develop a general approach to overcome these difficulties by introducing new statistics for individual tests with sharp asymptotics both marginally and jointly, and utilizing them to control the false discovery rate (FDR) via a data splitting and symmetric aggregation scheme. We show that valid FDR control can be achieved with guaranteed power under nearly optimal sample size requirements using the proposed methodology. Extensive numerical simulations and real data examples are also presented to further illustrate its practical merits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.00305v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wanteng Ma, Lilun Du, Dong Xia, Ming Yuan</dc:creator>
    </item>
    <item>
      <title>Selecting the Number of Communities for Weighted Degree-Corrected Stochastic Block Models</title>
      <link>https://arxiv.org/abs/2406.05340</link>
      <description>arXiv:2406.05340v3 Announce Type: replace 
Abstract: We investigate how to select the number of communities for weighted networks without a full likelihood modeling. First, we propose a novel weighted degree-corrected stochastic block model (DCSBM), where the mean adjacency matrix is modeled in the same way as in the standard DCSBM, while the variance profile matrix is assumed to be related to the mean adjacency matrix through a given variance function. Our method of selecting the number of communities is based on a sequential testing framework. In each step, the weighted DCSBM is fitted via some spectral clustering method. A key component of our method is matrix scaling on the estimated variance profile matrix. The resulting scaling factors can be used to normalize the adjacency matrix, from which the test statistic is then obtained. Under mild conditions on the weighted DCSBM, our proposed procedure is shown to be consistent in estimating the true number of communities. Numerical experiments on both simulated and real-world network data demonstrate the desirable empirical properties of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05340v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yucheng Liu, Xiaodong Li</dc:creator>
    </item>
    <item>
      <title>Robust Bayesian approach for reliability prognosis of nondestructive one-shot devices under cumulative risk model</title>
      <link>https://arxiv.org/abs/2406.08867</link>
      <description>arXiv:2406.08867v3 Announce Type: replace 
Abstract: The present study aims to determine the lifetime prognosis of highly durable nondestructive one-shot devices (NOSD) units under a step-stress accelerated life testing (SSALT) experiment applying a cumulative risk model (CRM). In an SSALT experiment, CRM retains the continuity of hazard function by allowing the lag period before the effects of stress change emerge. When prior information about the model parameters is available, Bayesian inference is crucial. In a Bayesian analysis of such lifetime data, conventional likelihood-based Bayesian estimation frequently fails in the presence of outliers in the dataset. This work incorporates a robust Bayesian approach utilizing a robustified posterior based on the density power divergence measure. The order restriction on shape parameters has been incorporated as a prior assumption to reflect the decreasing expected lifetime with increasing stress levels. In testing of hypothesis, a Bayes factor is implemented based on the robustified posterior. In Bayesian estimation, we exploit Hamiltonian Monte Carlo, which has certain advantages over the conventional Metropolis-Hastings algorithms. Further, the influence functions are examined to evaluate the robust behaviour of the estimators and the Bayes factor. Finally, the analytical development is validated through a simulation study and a real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08867v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shanya Baghel, Shuvashree Mondal</dc:creator>
    </item>
    <item>
      <title>Generalized implementation of invariant coordinate selection with positive semi-definite scatter matrices</title>
      <link>https://arxiv.org/abs/2409.02258</link>
      <description>arXiv:2409.02258v2 Announce Type: replace 
Abstract: Invariant coordinate selection is an unsupervised multivariate data transformation useful in many contexts such as outlier detection or clustering. It is based on the simultaneous diagonalization of two affine equivariant and positive definite scatter matrices. Its classical implementation relies on a non-symmetric eigenvalue problem by diagonalizing one scatter relatively to the other. In case of collinearity, at least one of the scatter matrices is singular, making the problem unsolvable. To address this limitation, three approaches are proposed using: a Moore-Penrose pseudo inverse, a dimension reduction, and a generalized singular value decomposition. Their properties are investigated both theoretically and through various empirical applications. Overall, the extension based on the generalized singular value decomposition seems the most promising, even though it restricts the choice of scatter matrices to those that can be expressed as cross-products. In practice, some of the approaches also appear suitable in the context of data in high-dimension low-sample-size data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02258v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aurore Archimbaud</dc:creator>
    </item>
    <item>
      <title>Modeling Neural Switching via Drift-Diffusion Models</title>
      <link>https://arxiv.org/abs/2410.00781</link>
      <description>arXiv:2410.00781v3 Announce Type: replace 
Abstract: Neural encoding is a field in neuroscience that focuses on characterizing how information from stimuli is encoded in the spiking activity of neurons. When more than one stimulus is present, a theory known as multiplexing posits that neurons temporally switch between encoding various stimuli, creating a fluctuating firing pattern. Here, we propose a new statistical framework to analyze rate fluctuations and discern whether neurons employ multiplexing as a means of encoding multiple stimuli. We adopt a mechanistic approach to modeling multiplexing by constructing a non-Markovian endogenous state-space model. Specifically, we posit that multiplexing arises from competition between the stimuli, which are modeled as latent drift-diffusion processes. We propose a new MCMC algorithm for conducting posterior inference on similar types of state-space models, where typical state-space MCMC methods fail due to strong dependence between the parameters. In addition, we develop alternative models that represent a wide class of alternative encoding theories and perform model comparison using WAIC to determine whether the data suggest the occurrence multiplexing over alternative theories of neural encoding. Using the proposed framework, we provide evidence of multiplexing within the inferior colliculus and novel insight into the switching dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00781v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nicholas Marco, Jennifer M. Groh, Surya T. Tokdar</dc:creator>
    </item>
    <item>
      <title>Two-stage Design for Failure Probability Estimation with Gaussian Process Surrogates</title>
      <link>https://arxiv.org/abs/2410.04496</link>
      <description>arXiv:2410.04496v2 Announce Type: replace 
Abstract: We tackle the problem of quantifying failure probabilities for expensive deterministic computer experiments with stochastic inputs. The computational cost of the computer simulation prohibits direct Monte Carlo (MC) and necessitates a surrogate model, turning the problem into a two-stage enterprise (surrogate training followed by probability estimation). Limited budgets create a design problem: how should expensive evaluations be allocated between and within the training and estimation stages? One may use the entire evaluation budget to sequentially train the surrogate through contour location (CL), with failure probabilities then estimated solely from the surrogate (we call it "surrogate MC"). But extended CL offers diminishing returns, and surrogate MC relies too stringently on surrogate accuracy. Alternatively, a partially trained surrogate may inform importance sampling, but this can provide erroneous results when budgets are limited. Instead we propose a two-stage design: starting with sequential CL, halting CL once learning has plateaued, then greedily allocating the remaining budget to MC samples with high classification entropy. Ultimately, we employ a "hybrid MC" estimator which leverages the trained surrogate in conjunction with the true responses observed in this second stage. Our unique two-stage design strikes an appropriate balance between exploring and exploiting, and outperforms alternatives, including both of the aforementioned approaches, on a variety of benchmark exercises. With these tools, we are able to effectively estimate small failure probabilities with only hundreds of simulator evaluations, showcasing functionality with both shallow and deep Gaussian process surrogates, and deploying our method on a simulation of fluid flow around an airfoil.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04496v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annie S. Booth, S. Ashwin Renganathan</dc:creator>
    </item>
    <item>
      <title>Bayesian Nonparametric Sensitivity Analysis of Multiple Test Procedures Under Dependence</title>
      <link>https://arxiv.org/abs/2410.08080</link>
      <description>arXiv:2410.08080v2 Announce Type: replace 
Abstract: This article introduces a sensitivity analysis method for Multiple Testing Procedures (MTPs) using marginal $p$-values. The method is based on the Dirichlet process (DP) prior distribution, specified to support the entire space of MTPs, where each MTP controls either the family-wise error rate (FWER) or the false discovery rate (FDR) under arbitrary dependence between $p$-values. The DP MTP sensitivity analysis method accounts for uncertainty in the selection of such MTPs and their respective cut-off points and decisions regarding which subset of $p$-values are significant from a given set of hypothesis tested, while measuring each $p$-value's probability of significance over the DP prior predictive distribution of this space of all MTPs, and reducing the possible conservativeness of using one such MTP for multiple testing. The DP MTP sensitivity analysis method is illustrated through the analysis of twenty-eight thousand $p$-values arising from hypothesis tests performed on a 2022 dataset of a representative sample of three million U.S. high school students observed on 239 variables. They include tests that relate variables about the disruption caused by school closures during the COVID-19 pandemic, with variables on mathematical cognition and academic achievement, and with student background variables. R software code for the DP MTP sensitivity analysis method is provided in the Appendix and in Supplementary Information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08080v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Karabatsos</dc:creator>
    </item>
    <item>
      <title>Bin-Conditional Conformal Prediction of Fatalities from Armed Conflict</title>
      <link>https://arxiv.org/abs/2410.14507</link>
      <description>arXiv:2410.14507v2 Announce Type: replace 
Abstract: Forecasting armed conflicts is a critical area of research with the potential to save lives and mitigate suffering. While existing forecasting models offer valuable point predictions, they often lack individual-level uncertainty estimates, limiting their usefulness for decision-making. Several approaches exist to estimate uncertainty, such as parametric and Bayesian prediction intervals, bootstrapping, quantile regression, but these methods often rely on restrictive assumptions, struggle to provide well-calibrated intervals across the full range of outcomes, or are computationally intensive. Conformal prediction offers a model-agnostic alternative that guarantees a user-specified level of coverage but typically provides only marginal coverage, potentially resulting in non-uniform coverage across different regions of the outcome space. In this paper, we introduce a novel extension called bin-conditional conformal prediction (BCCP), which enhances standard conformal prediction by ensuring consistent coverage rates across user-defined subsets (bins) of the outcome variable. We apply BCCP to simulated data as well as the forecasting of fatalities from armed conflicts, and demonstrate that it provides well-calibrated uncertainty estimates across various ranges of the outcome. Compared to standard conformal prediction, BCCP offers improved local coverage, though this comes at the cost of slightly wider prediction intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14507v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Randahl, Jonathan P. Williams, H{\aa}vard Hegre</dc:creator>
    </item>
    <item>
      <title>Stabilized Inverse Probability Weighting via Isotonic Calibration</title>
      <link>https://arxiv.org/abs/2411.06342</link>
      <description>arXiv:2411.06342v2 Announce Type: replace 
Abstract: Inverse weighting with an estimated propensity score is widely used by estimation methods in causal inference to adjust for confounding bias. However, directly inverting propensity score estimates can lead to instability, bias, and excessive variability due to large inverse weights, especially when treatment overlap is limited. In this work, we propose a post-hoc calibration algorithm for inverse propensity weights that generates well-calibrated, stabilized weights from user-supplied, cross-fitted propensity score estimates. Our approach employs a variant of isotonic regression with a loss function specifically tailored to the inverse propensity weights. Through theoretical analysis and empirical studies, we demonstrate that isotonic calibration improves the performance of doubly robust estimators of the average treatment effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06342v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars van der Laan, Ziming Lin, Marco Carone, Alex Luedtke</dc:creator>
    </item>
    <item>
      <title>Scalable piecewise smoothing with BART</title>
      <link>https://arxiv.org/abs/2411.07984</link>
      <description>arXiv:2411.07984v3 Announce Type: replace 
Abstract: Although it is an extremely effective, easy-to-use, and increasingly popular tool for nonparametric regression, the Bayesian Additive Regression Trees (BART) model is limited by the fact that it can only produce discontinuous output. Initial attempts to overcome this limitation were based on regression trees that output Gaussian Processes instead of constants. Unfortunately, implementations of these extensions cannot scale to large datasets. We propose ridgeBART, an extension of BART built with trees that output linear combinations of ridge functions (i.e., a composition of an affine transformation of the inputs and non-linearity); that is, we build a Bayesian ensemble of localized neural networks with a single hidden layer. We develop a new MCMC sampler that updates trees in linear time and establish posterior contraction rates for estimating piecewise anisotropic H\"{o}lder functions and nearly minimax-optimal rates for estimating isotropic H\"{o}lder functions. We demonstrate ridgeBART's effectiveness on synthetic data and use it to estimate the probability that a professional basketball player makes a shot from any location on the court in a spatially smooth fashion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07984v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Yee, Soham Ghosh, Sameer K. Deshpande</dc:creator>
    </item>
    <item>
      <title>A Bayesian prevalence-incidence mixture model for screening outcomes with misclassification</title>
      <link>https://arxiv.org/abs/2412.16065</link>
      <description>arXiv:2412.16065v2 Announce Type: replace 
Abstract: We present BayesPIM, a Bayesian prevalence-incidence mixture model for estimating time- and covariate-dependent disease incidence from screening and surveillance data. The method is particularly suited to settings where some individuals may have the disease at baseline, baseline tests may be missing or incomplete, and the screening test has imperfect test sensitivity. This setting was present in data from high-risk colorectal cancer (CRC) surveillance through colonoscopy, where adenomas, precursors of CRC, were already present at baseline and remained undetected due to imperfect test sensitivity. By including covariates, the model can quantify heterogeneity in disease risk, thereby informing personalized screening strategies. Internally, BayesPIM uses a Metropolis-within-Gibbs sampler with data augmentation and weakly informative priors on the incidence and prevalence model parameters. In simulations based on the real-world CRC surveillance data, we show that BayesPIM estimates model parameters without bias while handling latent prevalence and imperfect test sensitivity. However, informative priors on the test sensitivity are needed to stabilize estimation and mitigate non-convergence issues. We also show how conditioning incidence and prevalence estimates on covariates explains heterogeneity in adenoma risk and how model fit is assessed using information criteria and a non-parametric estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16065v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Klausch, Birgit I. Lissenberg-Witte, Veerle M. Coup\'e</dc:creator>
    </item>
    <item>
      <title>Ledoit-Wolf linear shrinkage with unknown mean</title>
      <link>https://arxiv.org/abs/2304.07045</link>
      <description>arXiv:2304.07045v2 Announce Type: replace-cross 
Abstract: This work addresses large dimensional covariance matrix estimation with unknown mean. The empirical covariance estimator fails when dimension and number of samples are proportional and tend to infinity, settings known as Kolmogorov asymptotics. When the mean is known, Ledoit and Wolf (2004) proposed a linear shrinkage estimator and proved its convergence under those asymptotics. To the best of our knowledge, no formal proof has been proposed when the mean is unknown. To address this issue, we propose to extend the linear shrinkage and its convergence properties to translation-invariant estimators. We expose four estimators respecting those conditions, proving their properties. Finally, we show empirically that a new estimator we propose outperforms other standard estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.07045v2</guid>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jmva.2025.105429</arxiv:DOI>
      <arxiv:journal_reference>Journal of Multivariate Analysis, Volume 208, 2025, 105429, ISSN 0047-259X</arxiv:journal_reference>
      <dc:creator>Benoit Oriol, Alexandre Miot</dc:creator>
    </item>
    <item>
      <title>Computing high-dimensional optimal transport by flow neural networks</title>
      <link>https://arxiv.org/abs/2305.11857</link>
      <description>arXiv:2305.11857v5 Announce Type: replace-cross 
Abstract: Computing optimal transport (OT) for general high-dimensional data has been a long-standing challenge. Despite much progress, most of the efforts including neural network methods have been focused on the static formulation of the OT problem. The current work proposes to compute the dynamic OT between two arbitrary distributions $P$ and $Q$ by optimizing a flow model, where both distributions are only accessible via finite samples. Our method learns the dynamic OT by finding an invertible flow that minimizes the transport cost. The trained optimal transport flow subsequently allows for performing many downstream tasks, including infinitesimal density ratio estimation (DRE) and domain adaptation by interpolating distributions in the latent space. The effectiveness of the proposed model on high-dimensional data is demonstrated by strong empirical performance on OT baselines, image-to-image translation, and high-dimensional DRE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.11857v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chen Xu, Xiuyuan Cheng, Yao Xie</dc:creator>
    </item>
    <item>
      <title>Out-of-distribution robustness for multivariate analysis via causal regularisation</title>
      <link>https://arxiv.org/abs/2403.01865</link>
      <description>arXiv:2403.01865v3 Announce Type: replace-cross 
Abstract: We propose a regularisation strategy of classical machine learning algorithms rooted in causality that ensures robustness against distribution shifts. Building upon the anchor regression framework, we demonstrate how incorporating a straightforward regularisation term into the loss function of classical multivariate analysis algorithms, such as (orthonormalized) partial least squares, reduced-rank regression, and multiple linear regression, enables out-of-distribution generalisation. Our framework allows users to efficiently verify the compatibility of a loss function with the regularisation strategy. Estimators for selected algorithms are provided, showcasing consistency and efficacy in synthetic and real-world climate science problems. The empirical validation highlights the versatility of anchor regularisation, emphasizing its compatibility with multivariate analysis approaches and its role in enhancing replicability while guarding against distribution shifts. The extended anchor framework advances causal inference methodologies, addressing the need for reliable out-of-distribution generalisation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01865v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Homer Durand, Gherardo Varando, Nathan Mankovich, Gustau Camps-Valls</dc:creator>
    </item>
    <item>
      <title>HACSurv: A Hierarchical Copula-Based Approach for Survival Analysis with Dependent Competing Risks</title>
      <link>https://arxiv.org/abs/2410.15180</link>
      <description>arXiv:2410.15180v2 Announce Type: replace-cross 
Abstract: In survival analysis, subjects often face competing risks; for example, individuals with cancer may also suffer from heart disease or other illnesses, which can jointly influence the prognosis of risks and censoring. Traditional survival analysis methods often treat competing risks as independent and fail to accommodate the dependencies between different conditions. In this paper, we introduce HACSurv, a survival analysis method that learns Hierarchical Archimedean Copulas structures and cause-specific survival functions from data with competing risks. HACSurv employs a flexible dependency structure using hierarchical Archimedean copulas to represent the relationships between competing risks and censoring. By capturing the dependencies between risks and censoring, HACSurv improves the accuracy of survival predictions and offers insights into risk interactions. Experiments on synthetic dataset demonstrate that our method can accurately identify the complex dependency structure and precisely predict survival distributions, whereas the compared methods exhibit significant deviations between their predictions and the true distributions. Experiments on multiple real-world datasets also demonstrate that our method achieves better survival prediction compared to previous state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15180v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Liu, Weijia Zhang, Min-Ling Zhang</dc:creator>
    </item>
    <item>
      <title>Sampling from Bayesian Neural Network Posteriors with Symmetric Minibatch Splitting Langevin Dynamics</title>
      <link>https://arxiv.org/abs/2410.19780</link>
      <description>arXiv:2410.19780v2 Announce Type: replace-cross 
Abstract: We propose a scalable kinetic Langevin dynamics algorithm for sampling parameter spaces of big data and AI applications. Our scheme combines a symmetric forward/backward sweep over minibatches with a symmetric discretization of Langevin dynamics. For a particular Langevin splitting method (UBU), we show that the resulting Symmetric Minibatch Splitting-UBU (SMS-UBU) integrator has bias $O(h^2 d^{1/2})$ in dimension $d&gt;0$ with stepsize $h&gt;0$, despite only using one minibatch per iteration, thus providing excellent control of the sampling bias as a function of the stepsize. We apply the algorithm to explore local modes of the posterior distribution of Bayesian neural networks (BNNs) and evaluate the calibration performance of the posterior predictive probabilities for neural networks with convolutional neural network architectures for classification problems on three different datasets (Fashion-MNIST, Celeb-A and chest X-ray). Our results indicate that BNNs sampled with SMS-UBU can offer significantly better calibration performance compared to standard methods of training and stochastic weight averaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19780v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Paulin, Peter A. Whalley, Neil K. Chada, Benedict Leimkuhler</dc:creator>
    </item>
  </channel>
</rss>
