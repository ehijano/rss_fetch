<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Jun 2024 01:55:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Towards Universal Unfolding of Detector Effects in High-Energy Physics using Denoising Diffusion Probabilistic Models</title>
      <link>https://arxiv.org/abs/2406.01507</link>
      <description>arXiv:2406.01507v1 Announce Type: new 
Abstract: The unfolding of detector effects in experimental data is critical for enabling precision measurements in high-energy physics. However, traditional unfolding methods face challenges in scalability, flexibility, and dependence on simulations. We introduce a novel unfolding approach using conditional Denoising Diffusion Probabilistic Models (cDDPM). Our method utilizes the cDDPM for a non-iterative, flexible posterior sampling approach, which exhibits a strong inductive bias that allows it to generalize to unseen physics processes without explicitly assuming the underlying distribution. We test our approach by training a single cDDPM to perform multidimensional particle-wise unfolding for a variety of physics processes, including those not seen during training. Our results highlight the potential of this method as a step towards a "universal" unfolding tool that reduces dependence on truth-level assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01507v1</guid>
      <category>physics.data-an</category>
      <category>hep-ex</category>
      <category>hep-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Camila Pazos, Shuchin Aeron, Pierre-Hugues Beauchemin, Vincent Croft, Martin Klassen, Taritree Wongjirad</dc:creator>
    </item>
    <item>
      <title>Predicting the fatigue life of asphalt concrete using neural networks</title>
      <link>https://arxiv.org/abs/2406.01523</link>
      <description>arXiv:2406.01523v1 Announce Type: new 
Abstract: Asphalt concrete's (AC) durability and maintenance demands are strongly influenced by its fatigue life. Traditional methods for determining this characteristic are both resource-intensive and time-consuming. This study employs artificial neural networks (ANNs) to predict AC fatigue life, focusing on the impact of strain level, binder content, and air-void content. Leveraging a substantial dataset, we tailored our models to effectively handle the wide range of fatigue life data, typically represented on a logarithmic scale. The mean square logarithmic error was utilized as the loss function to enhance prediction accuracy across all levels of fatigue life. Through comparative analysis of various hyperparameters, we developed a machine-learning model that captures the complex relationships within the data. Our findings demonstrate that higher binder content significantly enhances fatigue life, while the influence of air-void content is more variable, depending on binder levels. Most importantly, this study provides insights into the intricacies of using ANNs for modeling, showcasing their potential utility with larger datasets. The codes developed and the data used in this study are provided as open source on a GitHub repository, with a link included in the paper for full access.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01523v1</guid>
      <category>physics.data-an</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakub Houl\'ik, Jan Valentin, V\'aclav Ne\v{z}erka</dc:creator>
    </item>
    <item>
      <title>Surface roughness-informed fatigue life prediction of L-PBF Hastelloy X at elevated temperature</title>
      <link>https://arxiv.org/abs/2406.00186</link>
      <description>arXiv:2406.00186v1 Announce Type: cross 
Abstract: Additive manufacturing, especially laser powder bed fusion (L-PBF), is widely used for fabricating metal parts with intricate geometries. However, parts produced via L-PBF suffer from varied surface roughness which affects the dynamic or fatigue properties. Accurate prediction of fatigue properties as a function of surface roughness is a critical requirement for qualifying L-PBF parts. In this work, an analytical methodology is put forth to predict the fatigue life of L-PBF components having heterogeneous surface roughness. Thirty-six Hastelloy X specimens are printed using L-PBF followed by industry-standard heat treatment procedures. Half of these specimens are built with as-printed gauge sections and the other half is printed as cylinders from which fatigue specimens are extracted via machining. Specimens are printed in a vertical orientation and an orientation 30 degree from the vertical axis. The surface roughness of the specimens is measured using computed tomography and parameters such as the maximum valley depth are used to build an extreme value distribution. Fatigue testing is conducted at an isothermal condition of 500-degree F. It is observed that the rough specimens fail much earlier compared to the machined specimens due to the deep valleys present on the surfaces of the former ones. The valleys act as notches leading to high strain localization. Following this observation, a functional relationship is formulated analytically that considers surface valleys as notches and correlates the strain localization around those notches with fatigue life, using the Coffin-Manson-Basquin and Ramberg-Osgood equation. In conclusion, the proposed analytical model successfully predicts the fatigue life of L-PBF specimens at an elevated temperature undergoing different strain loadings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00186v1</guid>
      <category>physics.app-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ritam Pal, Brandon Kemerling, Daniel Ryan, Sudhakar Bollapragada, Amrita Basak</dc:creator>
    </item>
    <item>
      <title>Non-destructive Degradation Pattern Decoupling for Ultra-early Battery Prototype Verification Using Physics-informed Machine Learning</title>
      <link>https://arxiv.org/abs/2406.00276</link>
      <description>arXiv:2406.00276v1 Announce Type: cross 
Abstract: Manufacturing complexities and uncertainties have impeded the transition from material prototypes to commercial batteries, making prototype verification critical to quality assessment. A fundamental challenge involves deciphering intertwined chemical processes to characterize degradation patterns and their quantitative relationship with battery performance. Here we show that a physics-informed machine learning approach can quantify and visualize temporally resolved losses concerning thermodynamics and kinetics only using electric signals. Our method enables non-destructive degradation pattern characterization, expediting temperature-adaptable predictions of entire lifetime trajectories, rather than end-of-life points. The verification speed is 25 times faster yet maintaining 95.1% accuracy across temperatures. Such advances facilitate more sustainable management of defective prototypes before massive production, establishing a 19.76 billion USD scrap material recycling market by 2060 in China. By incorporating stepwise charge acceptance as a measure of the initial manufacturing variability of normally identical batteries, we can immediately identify long-term degradation variations. We attribute the predictive power to interpreting machine learning insights using material-agnostic featurization taxonomy for degradation pattern decoupling. Our findings offer new possibilities for dynamic system analysis, such as battery prototype degradation, demonstrating that complex pattern evolutions can be accurately predicted in a non-destructive and data-driven fashion by integrating physics-informed machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00276v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengyu Tao, Mengtian Zhang, Zixi Zhao, Haoyang Li, Ruifei Ma, Yunhong Che, Xin Sun, Lin Su, Xiangyu Chen, Zihao Zhou, Heng Chang, Tingwei Cao, Xiao Xiao, Yaojun Liu, Wenjun Yu, Zhongling Xu, Yang Li, Han Hao, Xuan Zhang, Xiaosong Hu, Guangmin ZHou</dc:creator>
    </item>
    <item>
      <title>Error evaluation of partial scattering functions obtained from contrast variation small-angle neutron scattering</title>
      <link>https://arxiv.org/abs/2406.00311</link>
      <description>arXiv:2406.00311v1 Announce Type: cross 
Abstract: Contrast variation small-angle neutron scattering (CV-SANS) is a powerful tool to evaluate the structure of multi-component systems by decomposing scattering intensities $I$ measured with different scattering contrasts into partial scattering functions $S$ of self- and cross-correlations between components. The measured $I$ contains a measurement error, $\Delta I$, and $\Delta I$ results in an uncertainty of partial scattering functions, $\Delta S$. However, the error propagation from $\Delta I$ to $\Delta S$ has not been quantitatively clarified. In this work, we have established deterministic and statistical approaches to determine $\Delta S$ from $\Delta I$. We have applied the two methods to experimental SANS data of polyrotaxane solutions with different contrasts, and have successfully estimated the errors of $S$. The quantitative error estimation of $S$ offers us a strategy to optimize the combination of scattering contrasts to minimize error propagation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00311v1</guid>
      <category>cond-mat.soft</category>
      <category>physics.chem-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Koichi Mayumi, Shinya Miyajima, Ippei Obayashi, Kazuaki Tanaka</dc:creator>
    </item>
    <item>
      <title>Kolmogorov-Arnold Network for Satellite Image Classification in Remote Sensing</title>
      <link>https://arxiv.org/abs/2406.00600</link>
      <description>arXiv:2406.00600v1 Announce Type: cross 
Abstract: In this research, we propose the first approach for integrating the Kolmogorov-Arnold Network (KAN) with various pre-trained Convolutional Neural Network (CNN) models for remote sensing (RS) scene classification tasks using the EuroSAT dataset. Our novel methodology, named KCN, aims to replace traditional Multi-Layer Perceptrons (MLPs) with KAN to enhance classification performance. We employed multiple CNN-based models, including VGG16, MobileNetV2, EfficientNet, ConvNeXt, ResNet101, and Vision Transformer (ViT), and evaluated their performance when paired with KAN. Our experiments demonstrated that KAN achieved high accuracy with fewer training epochs and parameters. Specifically, ConvNeXt paired with KAN showed the best performance, achieving 94% accuracy in the first epoch, which increased to 96% and remained consistent across subsequent epochs. The results indicated that KAN and MLP both achieved similar accuracy, with KAN performing slightly better in later epochs. By utilizing the EuroSAT dataset, we provided a robust testbed to investigate whether KAN is suitable for remote sensing classification tasks. Given that KAN is a novel algorithm, there is substantial capacity for further development and optimization, suggesting that KCN offers a promising alternative for efficient image analysis in the RS field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00600v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minjong Cheon</dc:creator>
    </item>
    <item>
      <title>A Point-cloud Clustering &amp; Tracking Algorithm for Radar Interferometry</title>
      <link>https://arxiv.org/abs/2406.00962</link>
      <description>arXiv:2406.00962v1 Announce Type: cross 
Abstract: In data mining, density-based clustering, which entails classifying datapoints according to their distributions in some space, is an essential method to extract information from large datasets. With the advent of software-based radio, ionospheric radars are capable of producing unprecedentedly large datasets of plasma turbulence backscatter observations, and new automatic techniques are needed to sift through them. We present an algorithm to automatically identify and track clusters of radar echoes through time, using dbscan, a celebrated density-based clustering method for noisy point-clouds. We demonstrate its efficiency by tracking turbulent structures in the E-region ionosphere, the so-called radar aurora. Through conjugate auroral imagery, as well as in-situ satellite observations, we demonstrate that the observed turbulent structures generally track the motion of the aurora. We discuss instances when this prediction is brought to fruition and when it ostensibly is not. Through case studies, we highlight the important instances when the radar echo bulk motions vary considerably around discrete auroral arcs, an effect we argue is produced by strong electric field modulations caused by energetic particle precipitation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00962v1</guid>
      <category>physics.space-ph</category>
      <category>physics.data-an</category>
      <category>physics.geo-ph</category>
      <category>physics.ins-det</category>
      <category>physics.plasm-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Magnus F Ivarsen, Jean-Pierre St-Maurice, Glenn C Hussey, Devin R Huyghebaert, Megan D Gillies</dc:creator>
    </item>
    <item>
      <title>Uncovering dynamical equations of stochastic decision models using data-driven SINDy algorithm</title>
      <link>https://arxiv.org/abs/2406.01370</link>
      <description>arXiv:2406.01370v1 Announce Type: cross 
Abstract: Decision formation in perceptual decision-making involves sensory evidence accumulation instantiated by the temporal integration of an internal decision variable towards some decision criterion or threshold, as described by sequential sampling theoretical models. The decision variable can be represented in the form of experimentally observable neural activities. Hence, elucidating the appropriate theoretical model becomes crucial to understanding the mechanisms underlying perceptual decision formation. Existing computational methods are limited to either fitting of choice behavioural data or linear model estimation from neural activity data. In this work, we made use of sparse identification of nonlinear dynamics (SINDy), a data-driven approach, to elucidate the deterministic linear and nonlinear components of often-used stochastic decision models within reaction time task paradigms. Based on the simulated decision variable activities of the models, SINDy, enhanced with a trial-averaging approach, could readily uncover the dynamical equations of the models while predicting the models' choice accuracy and decision time across a range of signal-to-noise ratio values. In particular, SINDy performed relatively better for decision models which have an accelerating dynamical component during decision formation, as expressed by a metastable linear competing accumulator model and a nonlinear bistable model. Taken together, our work suggests that SINDy can be a useful tool for uncovering the dynamics in perceptual decision-making, and more generally, for first-passage time problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01370v1</guid>
      <category>q-bio.NC</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brendan Lenfesty, Saugat Bhattacharyya, KongFatt Wong-Lin</dc:creator>
    </item>
    <item>
      <title>Inferring interaction potentials from stochastic particle trajectories</title>
      <link>https://arxiv.org/abs/2406.01522</link>
      <description>arXiv:2406.01522v1 Announce Type: cross 
Abstract: Accurate interaction potentials between microscopic components such as colloidal particles or cells are crucial to understanding a range of processes, including colloidal crystallization, bacterial colony formation, and cancer metastasis. Even in systems where the precise interaction mechanisms are unknown, effective interactions can be measured to inform simulation and design. However, these measurements are difficult and time-intensive, and often require conditions that are drastically different from in situ conditions of the system of interest. Moreover, existing methods of measuring interparticle potentials rely on constraining a small number of particles at equilibrium, placing limits on which interactions can be measured. We introduce a method for inferring interaction potentials directly from trajectory data of interacting particles. We explicitly solve the equations of motion to find a form of the potential that maximizes the probability of observing a known trajectory. Our method is valid for systems both in and out of equilibrium, is well-suited to large numbers of particles interacting in typical system conditions, and does not assume a functional form of the interaction potential. We apply our method to infer the interactions of colloidal spheres from experimental data, successfully extracting the range and strength of a depletion interaction from the motion of the particles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01522v1</guid>
      <category>cond-mat.soft</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ella M. King, Megan C. Engel, Caroline Martin, Alp M. Sunol, Qian-Ze Zhu, Sam S. Schoenholz, Vinothan N. Manoharan, Michael P. Brenner</dc:creator>
    </item>
    <item>
      <title>Accelerator system parameter estimation using variational autoencoded latent regression</title>
      <link>https://arxiv.org/abs/2406.01532</link>
      <description>arXiv:2406.01532v1 Announce Type: cross 
Abstract: Particle accelerators are time-varying systems whose components are perturbed by external disturbances. Tuning accelerators can be a time-consuming process involving manual adjustment of multiple components, such as RF cavities, to minimize beam loss due to time-varying drifts. The high dimensionality of the system ($\sim$100 amplitude and phase RF settings in the LANSCE accelerator) makes it difficult to achieve optimal operation. The time-varying drifts and the dimensionality make system parameter estimation a challenging optimization problem. In this work, we propose a Variational Autoencoded Latent Regression (VALeR) model for robust estimation of system parameters using 2D unique projections of a charged particle beam's 6D phase space. In VALeR, VAE projects the phase space projections into a lower-dimensional latent space, and a dense neural network maps the latent space onto the space of system parameters. The trained network can predict system parameters for unseen phase space projections. Furthermore, VALeR can generate new projections by randomly sampling the latent space of VAE and also estimate the corresponding system parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01532v1</guid>
      <category>physics.acc-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahindra Rautela, Alan Williams, Alexander Scheinker</dc:creator>
    </item>
    <item>
      <title>Towards latent space evolution of spatiotemporal dynamics of six-dimensional phase space of charged particle beams</title>
      <link>https://arxiv.org/abs/2406.01535</link>
      <description>arXiv:2406.01535v1 Announce Type: cross 
Abstract: Addressing the charged particle beam diagnostics in accelerators poses a formidable challenge, demanding high-fidelity simulations in limited computational time. Machine learning (ML) based surrogate models have emerged as a promising tool for non-invasive charged particle beam diagnostics. Trained ML models can make predictions much faster than computationally expensive physics simulations. In this work, we have proposed a temporally structured variational autoencoder model to autoregressively forecast the spatiotemporal dynamics of the 15 unique 2D projections of 6D phase space of charged particle beam as it travels through the LANSCE linear accelerator. In the model, VAE embeds the phase space projections into a lower dimensional latent space. A long-short-term memory network then learns the temporal correlations in the latent space. The trained network can evolve the phase space projections across further modules provided the first few modules as inputs. The model predicts all the projections across different modules with low mean squared error and high structural similarity index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01535v1</guid>
      <category>physics.acc-ph</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahindra Rautela, Alan Williams, Alexander Scheinker</dc:creator>
    </item>
    <item>
      <title>Statistics of drops generated from ensembles of randomly corrugated ligaments</title>
      <link>https://arxiv.org/abs/2106.16192</link>
      <description>arXiv:2106.16192v3 Announce Type: replace-cross 
Abstract: The size of drops generated by the capillary-driven disintegration of liquid ligaments plays a fundamental role in several important natural phenomena, ranging from heat and mass transfer at the ocean-atmosphere interface to pathogen transmission. The inherent non-linearity of the equations governing the ligament destabilization leads to significant differences in the resulting drop sizes, owing to small fluctuations in the myriad initial conditions. Previous experiments and simulations reveal a variety of drop size distributions, corresponding to competing underlying physical interpretations. Here, we perform numerical simulations of individual ligaments, the deterministic breakup of which is triggered by random initial surface corrugations. The simulations are grouped in a large ensemble, each corresponding to a random initial configuration. The resulting probability distributions reveal three stable drop sizes, generated via a sequence of two distinct stages of breakup. Four different distributions are tested, volume-based Poisson, Gaussian, Gamma and Log-Normal. Depending on the time, range of droplet sizes and criteria for success, each distribution has successes and failures. However the Log-Normal distribution roughly describes the data when fitting both the primary peak and the tail of the distribution while the number of droplets generated is the highest, while the Gamma and Log-Normal distributions perform equally well when fitting the tail. The study demonstrates a precisely controllable and reproducible framework, which can be employed to investigate the mechanisms responsible for the polydispersity of drop sizes found in complex fluid fragmentation scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.16192v3</guid>
      <category>physics.flu-dyn</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sagar Pal, Cesar Pairetti, Marco Crialesi-Esposito, Daniel Fuster, St\'ephane Zaleski</dc:creator>
    </item>
    <item>
      <title>Quantifying the Benefit of Artificial Intelligence for Scientific Research</title>
      <link>https://arxiv.org/abs/2304.10578</link>
      <description>arXiv:2304.10578v2 Announce Type: replace-cross 
Abstract: The ongoing artificial intelligence (AI) revolution has the potential to change almost every line of work. As AI capabilities continue to improve in accuracy, robustness, and reach, AI may outperform and even replace human experts across many valuable tasks. Despite enormous effort devoted to understanding the impact of AI on labor and the economy and AI's recent successes in accelerating scientific discovery and progress, we lack a systematic understanding of how AI advances may benefit scientific research across disciplines and fields. Here, drawing from the literature on the future of work and the science of science, we develop a measurement framework to estimate both the direct use of AI and the potential benefit of AI in scientific research, applying natural language processing techniques to 74.6 million publications and 7.1 million patents. We find that the use of AI in research is widespread throughout the sciences, growing especially rapidly since 2015, and papers that use AI exhibit a citation premium, more likely to be highly cited both within and outside their disciplines. Moreover, our analysis reveals considerable potential for AI to benefit numerous scientific fields, yet a notable disconnect exists between AI education and its research applications, highlighting a mismatch between the supply of AI expertise and its demand in research. Lastly, we examine demographic disparities in AI's benefits across scientific disciplines and find that disciplines with a higher proportion of women or Black scientists tend to be associated with less benefit, suggesting that AI's growing impact on research may further exacerbate existing inequalities in science. As the connection between AI and scientific research deepens, our findings may become increasingly important, with implications for the equity and sustainability of the research enterprise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.10578v2</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>physics.data-an</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jian Gao, Dashun Wang</dc:creator>
    </item>
    <item>
      <title>mdendro: An R package for extended agglomerative hierarchical clustering</title>
      <link>https://arxiv.org/abs/2309.13333</link>
      <description>arXiv:2309.13333v2 Announce Type: replace-cross 
Abstract: "mdendro" is an R package that provides a comprehensive collection of linkage methods for agglomerative hierarchical clustering on a matrix of proximity data (distances or similarities), returning a multifurcated dendrogram or multidendrogram. Multidendrograms can group more than two clusters at the same time, solving the nonuniqueness problem that arises when there are ties in the data. This problem causes that different binary dendrograms are possible depending both on the order of the input data and on the criterion used to break ties. Weighted and unweighted versions of the most common linkage methods are included in the package, which also implements two parametric linkage methods. In addition, package "mdendro" provides five descriptive measures to analyze the resulting dendrograms: cophenetic correlation coefficient, space distortion ratio, agglomeration coefficient, chaining coefficient and tree balance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13333v2</guid>
      <category>cs.IR</category>
      <category>physics.data-an</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alberto Fern\'andez, Sergio G\'omez</dc:creator>
    </item>
    <item>
      <title>Fair coins tend to land on the same side they started: Evidence from 350,757 flips</title>
      <link>https://arxiv.org/abs/2310.04153</link>
      <description>arXiv:2310.04153v3 Announce Type: replace-cross 
Abstract: Many people have flipped coins but few have stopped to ponder the statistical and physical intricacies of the process. In a preregistered study we collected $350{,}757$ coin flips to test the counterintuitive prediction from a physics model of human coin tossing developed by Diaconis, Holmes, and Montgomery (DHM; 2007). The model asserts that when people flip an ordinary coin, it tends to land on the same side it started -- DHM estimated the probability of a same-side outcome to be about 51%. Our data lend strong support to this precise prediction: the coins landed on the same side more often than not, $\text{Pr}(\text{same side}) = 0.508$, 95% credible interval (CI) [$0.506$, $0.509$], $\text{BF}_{\text{same-side bias}} = 2359$. Furthermore, the data revealed considerable between-people variation in the degree of this same-side bias. Our data also confirmed the generic prediction that when people flip an ordinary coin -- with the initial side-up randomly determined -- it is equally likely to land heads or tails: $\text{Pr}(\text{heads}) = 0.500$, 95% CI [$0.498$, $0.502$], $\text{BF}_{\text{heads-tails bias}} = 0.182$. Furthermore, this lack of heads-tails bias does not appear to vary across coins. Additional exploratory analyses revealed that the within-people same-side bias decreased as more coins were flipped, an effect that is consistent with the possibility that practice makes people flip coins in a less wobbly fashion. Our data therefore provide strong evidence that when some (but not all) people flip a fair coin, it tends to land on the same side it started. Our data provide compelling statistical support for the DHM physics model of coin tossing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.04153v3</guid>
      <category>math.HO</category>
      <category>physics.data-an</category>
      <category>stat.OT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Franti\v{s}ek Barto\v{s}, Alexandra Sarafoglou, Henrik R. Godmann, Amir Sahrani, David Klein Leunk, Pierre Y. Gui, David Voss, Kaleem Ullah, Malte J. Zoubek, Franziska Nippold, Frederik Aust, Felipe F. Vieira, Chris-Gabriel Islam, Anton J. Zoubek, Sara Shabani, Jonas Petter, Ingeborg B. Roos, Adam Finnemann, Aaron B. Lob, Madlen F. Hoffstadt, Jason Nak, Jill de Ron, Koen Derks, Karoline Huth, Sjoerd Terpstra, Thomas Bastelica, Magda Matetovici, Vincent L. Ott, Andreea S. Zetea, Katharina Karnbach, Michelle C. Donzallaz, Arne John, Roy M. Moore, Franziska Assion, Riet van Bork, Theresa E. Leidinger, Xiaochang Zhao, Adrian Karami Motaghi, Ting Pan, Hannah Armstrong, Tianqi Peng, Mara Bialas, Joyce Y. -C. Pang, Bohan Fu, Shujun Yang, Xiaoyi Lin, Dana Sleiffer, Miklos Bognar, Balazs Aczel, Eric-Jan Wagenmakers</dc:creator>
    </item>
    <item>
      <title>Characteristic Guidance: Non-linear Correction for Diffusion Model at Large Guidance Scale</title>
      <link>https://arxiv.org/abs/2312.07586</link>
      <description>arXiv:2312.07586v5 Announce Type: replace-cross 
Abstract: Popular guidance for denoising diffusion probabilistic model (DDPM) linearly combines distinct conditional models together to provide enhanced control over samples. However, this approach overlooks nonlinear effects that become significant when guidance scale is large. To address this issue, we propose characteristic guidance, a guidance method that provides first-principle non-linear correction for classifier-free guidance. Such correction forces the guided DDPMs to respect the Fokker-Planck (FP) equation of diffusion process, in a way that is training-free and compatible with existing sampling methods. Experiments show that characteristic guidance enhances semantic characteristics of prompts and mitigate irregularities in image generation, proving effective in diverse applications ranging from simulating magnet phase transitions to latent space sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07586v5</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Candi Zheng, Yuan Lan</dc:creator>
    </item>
    <item>
      <title>Is machine learning good or bad for the natural sciences?</title>
      <link>https://arxiv.org/abs/2405.18095</link>
      <description>arXiv:2405.18095v2 Announce Type: replace-cross 
Abstract: Machine learning (ML) methods are having a huge impact across all of the sciences. However, ML has a strong ontology - in which only the data exist - and a strong epistemology - in which a model is considered good if it performs well on held-out training data. These philosophies are in strong conflict with both standard practices and key philosophies in the natural sciences. Here we identify some locations for ML in the natural sciences at which the ontology and epistemology are valuable. For example, when an expressive machine learning model is used in a causal inference to represent the effects of confounders, such as foregrounds, backgrounds, or instrument calibration parameters, the model capacity and loose philosophy of ML can make the results more trustworthy. We also show that there are contexts in which the introduction of ML introduces strong, unwanted statistical biases. For one, when ML models are used to emulate physical (or first-principles) simulations, they amplify confirmation biases. For another, when expressive regressions are used to label datasets, those labels cannot be used in downstream joint or ensemble analyses without taking on uncontrolled biases. The question in the title is being asked of all of the natural sciences; that is, we are calling on the scientific communities to take a step back and consider the role and value of ML in their fields; the (partial) answers we give here come from the particular perspective of physics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18095v2</guid>
      <category>stat.ML</category>
      <category>astro-ph.IM</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David W. Hogg (NYU, MPIA, Flatiron), Soledad Villar (JHU, Flatiron)</dc:creator>
    </item>
    <item>
      <title>Convolutional L2LFlows: Generating Accurate Showers in Highly Granular Calorimeters Using Convolutional Normalizing Flows</title>
      <link>https://arxiv.org/abs/2405.20407</link>
      <description>arXiv:2405.20407v2 Announce Type: replace-cross 
Abstract: In the quest to build generative surrogate models as computationally efficient alternatives to rule-based simulations, the quality of the generated samples remains a crucial frontier. So far, normalizing flows have been among the models with the best fidelity. However, as the latent space in such models is required to have the same dimensionality as the data space, scaling up normalizing flows to high dimensional datasets is not straightforward. The prior L2LFlows approach successfully used a series of separate normalizing flows and sequence of conditioning steps to circumvent this problem. In this work, we extend L2LFlows to simulate showers with a 9-times larger profile in the lateral direction. To achieve this, we introduce convolutional layers and U-Net-type connections, move from masked autoregressive flows to coupling layers, and demonstrate the successful modelling of showers in the ILD Electromagnetic Calorimeter as well as Dataset 3 from the public CaloChallenge dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20407v2</guid>
      <category>physics.ins-det</category>
      <category>cs.LG</category>
      <category>hep-ex</category>
      <category>hep-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thorsten Buss, Frank Gaede, Gregor Kasieczka, Claudius Krause, David Shih</dc:creator>
    </item>
  </channel>
</rss>
