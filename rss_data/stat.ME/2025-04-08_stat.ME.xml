<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Apr 2025 01:53:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bayesian Modal Regression for Forecast Combinations</title>
      <link>https://arxiv.org/abs/2504.03859</link>
      <description>arXiv:2504.03859v1 Announce Type: new 
Abstract: Forecast combination methods have traditionally emphasized symmetric loss functions, particularly squared error loss, with equally weighted combinations often justified as a robust approach under such criteria. However, these justifications do not extend to asymmetric loss functions, where optimally weighted combinations may provide superior predictive performance. This study introduces a novel contribution by incorporating modal regression into forecast combinations, offering a Bayesian hierarchical framework that models the conditional mode of the response through combinations of time-varying parameters and exponential discounting. The proposed approach utilizes error distributions characterized by asymmetry and heavy tails, specifically the asymmetric Laplace, asymmetric normal, and reverse Gumbel distributions. Simulated data validate the parameter estimation for the modal regression models, confirming the robustness of the proposed methodology. Application of these methodologies to a real-world analyst forecast dataset shows that modal regression with asymmetric Laplace errors outperforms mean regression based on two key performance metrics: the hit rate, which measures the accuracy of classifying the sign of revenue surprises, and the win rate, which assesses the proportion of forecasts surpassing the equally weighted consensus. These results underscore the presence of skewness and fat-tailed behavior in forecast combination errors for revenue forecasting, highlighting the advantages of modal regression in financial applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03859v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Henry D. van Eijk, Sujit K. Ghosh</dc:creator>
    </item>
    <item>
      <title>MaxTDA: Robust Statistical Inference for Maximal Persistence in Topological Data Analysis</title>
      <link>https://arxiv.org/abs/2504.03897</link>
      <description>arXiv:2504.03897v1 Announce Type: new 
Abstract: Persistent homology is an area within topological data analysis (TDA) that can uncover different dimensional holes (connected components, loops, voids, etc.) in data. The holes are characterized, in part, by how long they persist across different scales. Noisy data can result in many additional holes that are not true topological signal. Various robust TDA techniques have been proposed to reduce the number of noisy holes, however, these robust methods have a tendency to also reduce the topological signal. This work introduces Maximal TDA (MaxTDA), a statistical framework addressing a limitation in TDA wherein robust inference techniques systematically underestimate the persistence of significant homological features. MaxTDA combines kernel density estimation with level-set thresholding via rejection sampling to generate consistent estimators for the maximal persistence features that minimizes bias while maintaining robustness to noise and outliers. We establish the consistency of the sampling procedure and the stability of the maximal persistence estimator. The framework also enables statistical inference on topological features through rejection bands, constructed from quantiles that bound the estimator's deviation probability. MaxTDA is particularly valuable in applications where precise quantification of statistically significant topological features is essential for revealing underlying structural properties in complex datasets. Numerical simulations across varied datasets, including an example from exoplanet astronomy, highlight the effectiveness of MaxTDA in recovering true topological signals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03897v1</guid>
      <category>stat.ME</category>
      <category>math.AT</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sixtus Dakurah, Jessi Cisewski-Kehe</dc:creator>
    </item>
    <item>
      <title>Confirmatory Biomarker Identification via Derandomized Knockoffs for Cox Regression with k-FWER Control</title>
      <link>https://arxiv.org/abs/2504.03907</link>
      <description>arXiv:2504.03907v1 Announce Type: new 
Abstract: Selecting important features in high-dimensional survival analysis is critical for identifying confirmatory biomarkers while maintaining rigorous error control. In this paper, we propose a derandomized knockoffs procedure for Cox regression that enhances stability in feature selection while maintaining rigorous control over the k-familywise error rate (k-FWER). By aggregating across multiple randomized knockoff realizations, our approach mitigates the instability commonly observed with conventional knockoffs. Through extensive simulations, we demonstrate that our method consistently outperforms standard knockoffs in both selection power and error control. Moreover, we apply our procedure to a clinical dataset on primary biliary cirrhosis (PBC) to identify key prognostic biomarkers associated with patient survival. The results confirm the superior stability of the derandomized knockoffs method, allowing for a more reliable identification of important clinical variables. Additionally, our approach is applicable to datasets containing both continuous and categorical covariates, broadening its utility in real-world biomedical studies. This framework provides a robust and interpretable solution for high-dimensional survival analysis, making it particularly suitable for applications requiring precise and stable variable selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03907v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rui Liu, Nan Sun</dc:creator>
    </item>
    <item>
      <title>Leveraging Shared Factor Structures for Enhanced Matrix Completion with Nonconvex Penalty Regularization</title>
      <link>https://arxiv.org/abs/2504.04020</link>
      <description>arXiv:2504.04020v1 Announce Type: new 
Abstract: This article investigates the problem of noisy low-rank matrix completion with a shared factor structure, leveraging the auxiliary information from the missing indicator matrix to enhance prediction accuracy. Despite decades of development in matrix completion, the potential relationship between observed data and missing indicators has largely been overlooked. To address this gap, we propose a joint modeling framework for the observed data and missing indicators within the context of a generalized factor model and derive the asymptotic limit distribution of the estimators. Furthermore, to tackle the rank estimation problem for model specification, we employ matrix nonconvex penalty regularization and establish nonasymptotic probability guarantees for the Oracle property. The theoretical results are validated through extensive simulation studies and real-world data analysis, demonstrating the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04020v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanhong A, Xinyan Fan, Bingyi Jing, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>nonprobsvy -- An R package for modern methods for non-probability surveys</title>
      <link>https://arxiv.org/abs/2504.04255</link>
      <description>arXiv:2504.04255v1 Announce Type: new 
Abstract: The following paper presents {nonprobsvy} -- an {R} package for inference based on non-probability samples. The package implements various approaches that can be categorized into three groups: prediction-based approach, inverse probability weighting and doubly robust approach. In the package, we assume the existence of either population-level data or probability-based population information and leverage the \pkg{survey} package for inference. The package implements both analytical and bootstrap variance estimation for the proposed estimators. In the paper we present the theory behind the package, its functionalities and case study that showcases the usage of the package. The package is aimed at scientists and researchers who would like to use non-probability samples (e.g.big data, opt-in web panels, social media) to accurately estimate population characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04255v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>{\L}ukasz Chrostowski, Piotr Chlebicki, Maciej Ber\k{e}sewicz</dc:creator>
    </item>
    <item>
      <title>A statistical framework for analyzing activity pattern from GPS data</title>
      <link>https://arxiv.org/abs/2504.04316</link>
      <description>arXiv:2504.04316v1 Announce Type: new 
Abstract: We introduce a novel statistical framework for analyzing the GPS data of a single individual. Our approach models daily GPS observations as noisy measurements of an underlying random trajectory, enabling the definition of meaningful concepts such as the average GPS density function. We propose estimators for this density function and establish their asymptotic properties. To study human activity patterns using GPS data, we develop a simple movement model based on mixture models for generating random trajectories. Building on this framework, we introduce several analytical tools to explore activity spaces and mobility patterns. We demonstrate the effectiveness of our approach through applications to both simulated and real-world GPS data, uncovering insightful mobility trends.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04316v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyang Wu, Yen-Chi Chen, Adrian Dobra</dc:creator>
    </item>
    <item>
      <title>Forecasting a time series of Lorenz curves: One-way functional analysis of variance</title>
      <link>https://arxiv.org/abs/2504.04437</link>
      <description>arXiv:2504.04437v1 Announce Type: new 
Abstract: The Lorenz curve is a fundamental tool for analysing income and wealth distribution and inequality at national and regional levels. We utilise a one-way functional analysis of variance to decompose a time series of Lorenz curves and develop a method for producing one-step-ahead point and interval forecasts. The one-way functional analysis of variance is easily interpretable by decomposing an array into a functional grand effect, a functional row effect and residual functions. We evaluate and compare the forecast accuracy between the functional analysis of variance and three non-functional methods using the Italian household income and wealth data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04437v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>On the bias of the Gini coefficient estimator for zero-truncated Poisson distributions</title>
      <link>https://arxiv.org/abs/2504.04518</link>
      <description>arXiv:2504.04518v1 Announce Type: new 
Abstract: This paper analyzes the Gini coefficient estimator for zero-truncated Poisson populations, revealing the presence of bias, and provides a mathematical expression for the bias, along with a bias-corrected estimator, which is evaluated using Monte Carlo simulation methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04518v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vila, Helton Saulo</dc:creator>
    </item>
    <item>
      <title>Sequential Hierarchical Regression Imputation with Variable Selection Routines</title>
      <link>https://arxiv.org/abs/2504.04539</link>
      <description>arXiv:2504.04539v1 Announce Type: new 
Abstract: We aim to incorporate variable selection routines into variable-by-variable (or sequential) imputation in clustered data to achieve computational improvement in applications with large-scale health data. Specifically, we utilize variable selection routines using spike-and-slab priors within the Bayesian variable selection routine. The choice of these priors allows us to ``force'' variables of importance (e.g., design variables or variables known to play a role in the missingness mechanism) into the imputation models based on a class of mixed-effects models. Our ultimate goal is to improve computational speed by removing unnecessary variables. We employ Markov chain Monte Carlo techniques to sample from the implied posterior distributions for model unknowns as well as missing data. We assess the performance of our proposed methodology via simulation studies. Our results show that our proposed algorithms lead to satisfactory estimates and, in some instances, outperform some of the existing methods that are available to practitioners. We illustrate our methods using a national survey of children's health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04539v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiushuang Li, Recai Yucel</dc:creator>
    </item>
    <item>
      <title>Variational Bayesian Multiple Imputation in High-Dimensional Regression Models With Missing Responses</title>
      <link>https://arxiv.org/abs/2504.04547</link>
      <description>arXiv:2504.04547v1 Announce Type: new 
Abstract: Multiple imputation has become one of the standard methods in drawing inferences in many incomplete data applications. Applications of multiple imputation in relatively more complex settings, such as high-dimensional clustered data, require specialized methods to overcome the computational burden. Using linear mixed-effects models, we develop such methods that can be applied to continuous, binary, or categorical incomplete data by employing variational Bayesian inference to sample the posterior predictive distribution of the missing data. These methods specifically target high-dimensional data and work with the spike-and-slab prior, which automatically selects the variables of importance to be in the imputation model. The individual regression computation is then incorporated into a variable-by-variable imputation algorithm. Finally, we use a calibration-based algorithm to adopt these methods to multiple imputations of categorical variables. We present a simulation study and illustrate on National Survey of Children's Health data to assess the performance of these methods in a repetitive sampling framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04547v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiushuang Li, Recai Yucel</dc:creator>
    </item>
    <item>
      <title>A small-area ecological approach for estimating vote changes and their determinants</title>
      <link>https://arxiv.org/abs/2504.04568</link>
      <description>arXiv:2504.04568v1 Announce Type: new 
Abstract: Empirical analyses on the factors driving vote switching are rare, usually conducted at the national level without considering the parties of origin and destination, and often unreliable due to the severe inaccuracy of recall survey data. To overcome the problem of lack of adequate data and to incorporate the increasingly relevant role of local factors, we propose an ecological inference methodology to estimate the number of vote transitions within small homogeneous areas and to assess the relationships between these counts and local characteristics through multinomial logistic models. This approach allows for a disaggregate analysis of contextual factors behind vote switching, distinguishing between their different origins and destinations. We apply this methodology to the Italian region of Umbria, divided into 19 small areas. To explain the number of transitions toward the right-wing nationalist party that won the elections and towards increasing abstentionism, we focused on measures of geographical, economic, and cultural disadvantages of local communities. Among the main findings, the economic disadvantages mainly pushed previous abstainers and far-right Lega voters to change their choices in favor of the rising right-wing party, while transitions from the opposite political camp were mostly influenced by cultural factors such as a lack of social capital, negative attitude towards the EU, and political tradition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04568v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bruno Bracalente, Antonio Forcina, Nicola Falocci</dc:creator>
    </item>
    <item>
      <title>Quantifying uncertainty of individualized treatment effects in right-censored survival data: A comparison of Bayesian additive regression trees and causal survival forest</title>
      <link>https://arxiv.org/abs/2504.04571</link>
      <description>arXiv:2504.04571v1 Announce Type: new 
Abstract: Estimation of individualized treatment effects (ITE), also known as conditional average treatment effects (CATE), is an active area of methodology development. However, much less attention has been paid to the quantification of uncertainty of ITE/CATE estimates in right-censored survival data. Here we undertake an extensive simulation study to examine the coverage of interval estimates from two popular estimation algorithms, Bayesian additive regression trees (BART) and causal survival forest (CSF). We conducted simulation designs from 3 different settings: first, in a setting where BART was developed for an accelerated failure time model; second, where CSF was developed; and finally, a ``neutral'' simulation taken from a setting where neither BART nor CSF was developed. BART outperformed CSF in all three simulation settings. Both the BART and CSF algorithms involve multiple hyperparameters, and BART credible intervals had better coverage than the CSF confidence intervals under the default values, as well as under optimized values, of these hyperparameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04571v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daijiro Kabata, Nicholas C. Henderson, Ravi Varadhan</dc:creator>
    </item>
    <item>
      <title>How Untested Modeling Assumptions Influence the U.S. EPA's Estimates of Population-Level Ozone Exposure Risk</title>
      <link>https://arxiv.org/abs/2504.04591</link>
      <description>arXiv:2504.04591v1 Announce Type: new 
Abstract: In recent reviews of the National Ambient Air Quality Standards (NAAQS) for ozone, the U.S. Environmental Protection Agency (U.S. EPA) has presented estimates of the health risks associated with ozone exposure. One way in which the U.S. EPA calculates population-level ozone risk estimates is through a simulation model that calculates ozone exposures and the resulting lung function decrements for a simulated population. This simulation model includes several random error terms to capture inter- and intra-individual variability in responsiveness to ozone exposure. In this manuscript we undertake a sensitivity analysis examining the influence of untested assumptions about these error terms. We show that ad hoc bounds imposed on the error terms and the frequency of redrawing the intra-individual error terms have a strong influence on the population-level ozone exposure risk reported by the U.S. EPA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04591v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Garrett Glasgow, Anne E. Smith</dc:creator>
    </item>
    <item>
      <title>Statistical Prediction of Peaks Over a Threshold</title>
      <link>https://arxiv.org/abs/2504.04602</link>
      <description>arXiv:2504.04602v1 Announce Type: new 
Abstract: In many applied fields, the prediction of more severe events than those already recorded is crucial for safeguarding against potential future calamities. What-if analyses, which evaluate hypothetical scenarios up to the worst-case event, play a key role in assessing the potential impacts of extreme events and guiding the development of effective safety policies. This problem can be analyzed using extreme value theory. We employ the well-established peaks-over-threshold method and describe a comprehensive toolkit to address forecasting needs. We examine an \lq\lq out-of-sample" variable and focus on its conditional probability of exceeding a high threshold, representing the predictive distribution of future extreme peaks. We demonstrate that the generalized Pareto approximation of the corresponding predictive density can be remarkably accurate. We then introduce frequentist methods and a Bayesian approach for estimating this predictive density, enabling the derivation of informative predictive intervals. By leveraging threshold stability, we illustrate how predictions can be reliably extended deep into the tail of the unknown data distribution. We establish the asymptotic accuracy of the proposed estimators and, more importantly, prove that the resulting predictive inference is asymptotically valid. Forecasters satisfying the tail-equivalence property allow to recover widely used risk measures for risk assessment through point forecasts. This insight lays the groundwork for a new perspective that integrates risk assessment into the statistical predictive toolbox. Finally, we extend the prediction framework to the case of linear time series. We apply the proposed predictive tools to two real-world datasets: summer peak temperatures recorded in Milan, Italy, over the past 30 years, and daily negative log-returns of the Dow Jones Industrial Average observed over 30 years.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04602v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simone A. Padoan, Stefano Rizzelli</dc:creator>
    </item>
    <item>
      <title>Regularization and Selection in A Directed Network Model with Nodal Homophily and Nodal Effects</title>
      <link>https://arxiv.org/abs/2504.04622</link>
      <description>arXiv:2504.04622v1 Announce Type: new 
Abstract: This article introduces a regularization and selection methods for directed networks with nodal homophily and nodal effects. The proposed approach not only preserves the statistical efficiency of the resulting estimator, but also ensures that the selection of nodal homophily and nodal effects is scalable with large-scale network data and multiple nodal features. In particular, we propose a directed random network model with nodal homophily and nodal effects, which includes the nodal features in the probability density of random networks. Subsequently, we propose a regularized maximum likelihood estimator with an adaptive LASSO-type regularizer. We demonstrate that the regularized estimator exhibits the consistency and possesses the oracle properties. In addition, we propose a network Bayesian information criterion which ensures the selection consistency while tuning the model. Simulation experiments are conducted to demonstrate the excellent numerical performance. An online friendship network among musicians with nodal musical preference is used to illustrate the usefulness of the proposed new network model in network-related empirical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04622v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaoyu Xing, Y. X. Rachel Wang, Andrew T. A. Wood, Tao Zou</dc:creator>
    </item>
    <item>
      <title>Multimodal Distributions for Circular Axial Data</title>
      <link>https://arxiv.org/abs/2504.04681</link>
      <description>arXiv:2504.04681v1 Announce Type: new 
Abstract: The family of circular distributions based on non-negative trigonometric sums (NNTS), developed by Fern\'andez-Dur\'an (2004), is highly flexible for modeling datasets exhibiting multimodality and/or skewness. In this article, we extend the NNTS family to axial data by identifying conditions under which the original NNTS family is suitable for modeling undirected vectors. Since the estimation is performed using maximum likelihood, likelihood ratio tests are developed for characteristics of the density function such as uniformity and symmetry. The proposed methodology is applied to real datasets involving orientations of rocks, animals, and plants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04681v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator> Fern\'andez-Dur\'an, J. J.,  Gregorio-Dom\'inguez, M. M</dc:creator>
    </item>
    <item>
      <title>Robustifying Approximate Bayesian Computation</title>
      <link>https://arxiv.org/abs/2504.04733</link>
      <description>arXiv:2504.04733v1 Announce Type: new 
Abstract: Approximate Bayesian computation (ABC) is one of the most popular "likelihood-free" methods. These methods have been applied in a wide range of fields by providing solutions to intractable likelihood problems in which exact Bayesian approaches are either infeasible or computationally costly. However, the performance of ABC can be unreliable when dealing with model misspecification. To circumvent the poor behavior of ABC in these settings, we propose a novel ABC approach that is robust to model misspecification. This new method can deliver more accurate statistical inference under model misspecification than alternatives and also enables the detection of summary statistics that are incompatible with the assumed data-generating process. We demonstrate the effectiveness of our approach through several simulated examples, where it delivers more accurate point estimates and uncertainty quantification over standard ABC approaches when the model is misspecified. Additionally, we apply our approach to an empirical example, further showcasing its advantages over alternative methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04733v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chaya Weerasinghe, David T. Frazier, Ruben Loaiza-Maya, Christopher Drovandi</dc:creator>
    </item>
    <item>
      <title>Statistical parametric simulation studies based on real data</title>
      <link>https://arxiv.org/abs/2504.04864</link>
      <description>arXiv:2504.04864v1 Announce Type: new 
Abstract: Simulation studies are indispensable for evaluating and comparing statistical methods. The most common simulation approach is parametric simulation, where the data-generating mechanism (DGM) corresponds to a predefined parametric model from which observations are drawn. Many statistical simulation studies aim to provide practical recommendations on a method's suitability for a given application; however, parametric simulations in particular are frequently criticized for being too simplistic and not reflecting reality. To overcome this drawback, it is generally considered a sensible approach to employ real data for constructing the parametric DGMs. However, while the concept of real-data-based parametric DGMs is widely recognized, the specific ways in which DGM components are inferred from real data vary, and their implications may not always be well understood. Additionally, researchers often rely on a limited selection of real datasets, with the rationale for their selection often unclear. This paper addresses these issues by formally discussing how components of parametric DGMs can be inferred from real data and how dataset selection can be performed more systematically. By doing so, we aim to support researchers in conducting simulation studies with a lower risk of overgeneralization and misinterpretation. We illustrate the construction of parametric DGMs based on a systematically selected set of real datasets using two examples: one on ordinal outcomes in randomized controlled trials and one on differential gene expression analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04864v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christina Sauer, F. Julian D. Lange, Maria Thurow, Ina Dormuth, Anne-Laure Boulesteix</dc:creator>
    </item>
    <item>
      <title>Optimal Network-Guided Covariate Selection for High-Dimensional Data Integration</title>
      <link>https://arxiv.org/abs/2504.04866</link>
      <description>arXiv:2504.04866v1 Announce Type: new 
Abstract: When integrating datasets from different studies, it is common that they have components of different formats. How to combine them organically for improved estimation is important and challenging. This paper investigates this problem in a two-study scenario, where covariates are observed for all subjects, but network data is available in only one study, and response variables are available only in the other.
  To leverage the partially observed network information, we propose the Network-Guided Covariate Selection (NGCS) algorithm. It integrates the spectral information from network adjacency matrices with the Higher Criticism Thresholding approach for informative covariates identification. Theoretically, we prove that NGCS achieves the optimal rate in covariate selection, which is the same rate in the supervised learning setting. Furthermore, this optimality is robust to network models and tuning parameters.
  This framework extends naturally to clustering and regression tasks, with two proposed algorithms: NG-clu and NG-reg. For clustering, NG-clu accurately clusters data points despite incomplete network information. For regression, NG-reg enhances predictive performance by incorporating latent covariate structures inferred from network data. Empirical studies on synthetic and real-world datasets demonstrate the robustness and superior performance of our algorithms, underscoring their effectiveness in handling heterogeneous data formats.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04866v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tao Shen, Wanjie Wang</dc:creator>
    </item>
    <item>
      <title>Nonparametric modal regression with missing response observations</title>
      <link>https://arxiv.org/abs/2504.04914</link>
      <description>arXiv:2504.04914v1 Announce Type: new 
Abstract: Modal regression has emerged as a flexible alternative to classical regression models when the conditional mean or median are unable to adequately capture the underlying relation between a response and a predictor variable. This approach is particularly useful when the conditional distribution of the response given the covariate presents several modes, so the suitable regression function is a multifunction. In recent years, some proposals have addressed modal (smooth) regression estimation using kernel methods. In addition, some remarkable extensions to deal with censored, dependent or circular data have been also introduced. However, the case of incomplete samples due to missingness has not been studied in the literature. This paper adapts the nonparametric modal regression tools to handle missing observations in the response, investigating several imputation approaches through an extensive simulation study. The performance in practice of our proposals are also illustrated with two real--data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04914v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ana P\'erez-Gonz\'alez, Tom\'as R. Cotos-Y\'a\~nez, Rosa M. Crujeiras</dc:creator>
    </item>
    <item>
      <title>Dominating Hyperplane Regularization for Variable Selection in Multivariate Count Regression</title>
      <link>https://arxiv.org/abs/2504.05034</link>
      <description>arXiv:2504.05034v1 Announce Type: new 
Abstract: Identifying relevant factors that influence the multinomial counts in compositional data is difficult in high dimensional settings due to the complex associations and overdispersion. Multivariate count models such as the Dirichlet-multinomial (DM), negative multinomial, and generalized DM accommodate overdispersion but are difficult to optimize due to their non-concave likelihood functions. Further, for the class of regression models that associate covariates to the multivariate count outcomes, variable selection becomes necessary as the number of potentially relevant factors becomes large. The sparse group lasso (SGL) is a natural choice for regularizing these models. Motivated by understanding the associations between water quality and benthic macroinvertebrate compositions in Canada's Athabasca oil sands region, we develop dominating hyperplane regularization (DHR), a novel method for optimizing regularized regression models with the SGL penalty. Under the majorization-minimization framework, we show that applying DHR to a SGL penalty gives rise to a surrogate function that can be expressed as a weighted ridge penalty. Consequently, we prove that for multivariate count regression models with the SGL penalty, the optimization leads to an iteratively reweighted Poisson ridge regression. We demonstrate stable optimization and high performance of our algorithm through simulation and real world application to benthic macroinvertebrate compositions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05034v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alysha Cooper (Department of Mathematics,Statistics, University of Guelph), Zeny Feng (Department of Mathematics,Statistics, University of Guelph), Ayesha Ali (Department of Mathematics,Statistics, University of Guelph), Tim Arciszewski (Alberta Environment,Protected Areas), Lorna Deeth (Department of Mathematics,Statistics, University of Guelph)</dc:creator>
    </item>
    <item>
      <title>Detecting relevant dependencies under measurement error with applications to the analysis of planetary system evolution</title>
      <link>https://arxiv.org/abs/2504.05055</link>
      <description>arXiv:2504.05055v1 Announce Type: new 
Abstract: Exoplanets play an important role in understanding the mechanics of planetary system formation and orbital evolution. In this context the correlations of different parameters of the planets and their host star are useful guides in the search for explanatory mechanisms. Based on a reanalysis of the data set from \cite{figueria14} we study the as of now still poorly understood correlation between planetary surface gravity and stellar activity of Hot Jupiters. Unfortunately, data collection often suffers from measurement errors due to complicated and indirect measurement setups, rendering standard inference techniques unreliable.
  We present new methods to estimate and test for correlations in a deconvolution framework and thereby improve the state of the art analysis of the data in two directions. First, we are now able to account for additive measurement errors which facilitates reliable inference. Second we test for relevant changes, i.e. we are testing for correlations exceeding a certain threshold $\Delta$. This reflects the fact that small nonzero correlations are to be expected for real life data almost always and that standard statistical tests will therefore always reject the null of no correlation given sufficient data. Our theory focuses on quantities that can be estimated by U-Statistics which contain a variety of correlation measures. We propose a bootstrap test and establish its theoretical validity. As a by product we also obtain confidence intervals. Applying our methods to the Hot Jupiter data set from \cite{figueria14}, we observe that taking into account the measurement errors yields smaller point estimates and the null of no relevant correlation is rejected only for very small $\Delta$. This demonstrates the importance of considering the impact of measurement errors to avoid misleading conclusions from the resulting statistical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05055v1</guid>
      <category>stat.ME</category>
      <category>astro-ph.EP</category>
      <category>astro-ph.IM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Bastian, Nicolai Bissantz</dc:creator>
    </item>
    <item>
      <title>$Q_B$-Optimal Two-Level Designs</title>
      <link>https://arxiv.org/abs/2504.05072</link>
      <description>arXiv:2504.05072v1 Announce Type: new 
Abstract: Two-level designs are widely used for screening experiments where the goal is to identify a few active factors which have major effects. Orthogonal two-level designs in which all factors are level-balance and each of the four level combinations of any pair of factors appears equally often are commonly used. In this paper, we apply the model-robust $Q_B$ criterion introduced by Tsai, Gilmour and Mead (2007) to the selection of optimal two-level screening designs without the requirements of level-balance and pairwise orthogonality. The criterion incorporates experimenter's prior belief on how likely a factor is to be active and recommends different designs under different priors, and without the requirement of level-balance and pairwise orthogonality, a wider range of designs is possible. A coordinate exchange algorithm is developed for the construction of $Q_B$-optimal designs for given priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05072v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pi-Wen Tsai, Steven G. Gilmour</dc:creator>
    </item>
    <item>
      <title>Flexible Estimation of the Heterogeneous Non-Parametric Component in a Relative Survival Cure Model</title>
      <link>https://arxiv.org/abs/2504.05093</link>
      <description>arXiv:2504.05093v1 Announce Type: new 
Abstract: Estimating the cure fraction in a diseased population, especially in the presence of competing mortality causes, is crucial for both patients and clinicians. It offers a valuable measure for monitoring and interpreting trends in disease outcomes. When information on the cause of death is unavailable or unreliable, the Relative Survival (RS) framework is the preferred approach for estimating Net Survival, which represents survival in a hypothetical scenario where the disease of interest is the only possible cause of death. In the context of cancer, RS often reaches a plateau, indicating that a portion of diagnosed patients is cured, as they have the same risk of dying as a comparable group of healthy individuals with similar demographic characteristics. Classical RS cure models use logistic regression to estimate the fraction of cured patients. However, this functional form is somewhat arbitrary, and misspecifying it can severely distort the resulting cure indicators. Consequently, evaluations of the efficacy of cancer treatments at the population level could be inaccurate, leading to biased decision-making regarding patient care. In this paper, we address this issue by relaxing the parametric assumption and considering flexible functions of the covariates within the framework of \textit{Generalized Models} and \textit{Neural Networks}. We design an EM algorithm for these RS cure models and conduct a simulation study to compare our proposals with the classical approach. We apply our methodology to a real-world dataset from a historical Italian cancer registry. The results demonstrate that our proposed models outperform the classical approach and provide valuable insights into the survival outcomes of Italian colon cancer patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05093v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fabrizio Di Mari, Roberto Rocci, Silvia Rossi, Giovanna Tagliabue, Roberta De Angelis</dc:creator>
    </item>
    <item>
      <title>Bayesian estimation of causal effects from observational categorical data</title>
      <link>https://arxiv.org/abs/2504.05198</link>
      <description>arXiv:2504.05198v1 Announce Type: new 
Abstract: We present a Bayesian procedure for estimation of pairwise intervention effects in a high-dimensional system of categorical variables. We assume that we have observational data generated from an unknown causal Bayesian network for which there are no latent confounders. Most of the existing methods developed for this setting assume that the underlying model is linear Gaussian, including the Bayesian IDA (BIDA) method that we build upon in this work. By combining a Bayesian backdoor estimator with model averaging, we obtain a posterior over the intervention distributions of a cause-effect pair that can be expressed as a mixture over stochastic linear combinations of Dirichlet distributions. Although there is no closed-form expression for the posterior density, it is straightforward to produce Monte Carlo approximations of target quantities through direct sampling, and we also derive closed-form expressions for a few selected moments. To scale up the proposed procedure, we employ Markov Chain Monte Carlo (MCMC), which also enables us to use more efficient adjustment sets compared to the current exact BIDA. Finally, we use Jensen-Shannon divergence to define a novel causal effect based on a set of intervention distributions in the general categorical setting. We compare our method to the original IDA method and existing Bayesian approaches in numerical simulations and show that categorical BIDA performs favorably against the existing alternative methods in terms of producing point estimates and discovering strong effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05198v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Vera Kvisgaard, Johan Pensar</dc:creator>
    </item>
    <item>
      <title>Eigenvalue-Based Randomness Test for Residual Diagnostics in Panel Data Models</title>
      <link>https://arxiv.org/abs/2504.05297</link>
      <description>arXiv:2504.05297v1 Announce Type: new 
Abstract: This paper introduces the Eigenvalue-Based Randomness (EBR) test - a novel approach rooted in the Tracy-Widom law from random matrix theory - and applies it to the context of residual analysis in panel data models. Unlike traditional methods, which target specific issues like cross-sectional dependence or autocorrelation, the EBR test simultaneously examines multiple assumptions by analyzing the largest eigenvalue of a symmetrized residual matrix. Monte Carlo simulations demonstrate that the EBR test is particularly robust in detecting not only standard violations such as autocorrelation and linear cross-sectional dependence (CSD) but also more intricate non-linear and non-monotonic dependencies, making it a comprehensive and highly flexible tool for enhancing the reliability of panel data analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05297v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcell T. Kurbucz, Betsab\'e P\'erez Garrido, Antal Jakov\'ac</dc:creator>
    </item>
    <item>
      <title>MMCE: A Framework for Deep Monotonic Modeling of Multiple Causal Effects</title>
      <link>https://arxiv.org/abs/2504.03753</link>
      <description>arXiv:2504.03753v1 Announce Type: cross 
Abstract: When we plan to use money as an incentive to change the behavior of a person (such as making riders to deliver more orders or making consumers to buy more items), the common approach of this problem is to adopt a two-stage framework in order to maximize ROI under cost constraints. In the first stage, the individual price response curve is obtained. In the second stage, business goals and resource constraints are formally expressed and modeled as an optimization problem. The first stage is very critical. It can answer a very important question. This question is how much incremental results can incentives bring, which is the basis of the second stage. Usually, the causal modeling is used to obtain the curve. In the case of only observational data, causal modeling and evaluation are very challenging. In some business scenarios, multiple causal effects need to be obtained at the same time. This paper proposes a new observational data modeling and evaluation framework, which can simultaneously model multiple causal effects and greatly improve the modeling accuracy under some abnormal distributions. In the absence of RCT data, evaluation seems impossible. This paper summarizes three priors to illustrate the necessity and feasibility of qualitative evaluation of cognitive testing. At the same time, this paper innovatively proposes the conditions under which observational data can be considered as an evaluation dataset. Our approach is very groundbreaking. It is the first to propose a modeling framework that simultaneously obtains multiple causal effects. The offline analysis and online experimental results show the effectiveness of the results and significantly improve the effectiveness of the allocation strategies generated in real world marketing activities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03753v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juhua Chen, Karson shi, Jialing He, North Chen, Kele Jiang</dc:creator>
    </item>
    <item>
      <title>Regression Discontinuity Design with Distribution-Valued Outcomes</title>
      <link>https://arxiv.org/abs/2504.03992</link>
      <description>arXiv:2504.03992v1 Announce Type: cross 
Abstract: This article introduces Regression Discontinuity Design (RDD) with Distribution-Valued Outcomes (R3D), extending the standard RDD framework to settings where the outcome is a distribution rather than a scalar. Such settings arise when treatment is assigned at a higher level of aggregation than the outcome-for example, when a subsidy is allocated based on a firm-level revenue cutoff while the outcome of interest is the distribution of employee wages within the firm. Since standard RDD methods cannot accommodate such two-level randomness, I propose a novel approach based on random distributions. The target estimand is a "local average quantile treatment effect", which averages across random quantiles. To estimate this target, I introduce two related approaches: one that extends local polynomial regression to random quantiles and another based on local Fr\'echet regression, a form of functional regression. For both estimators, I establish asymptotic normality and develop uniform, debiased confidence bands together with a data-driven bandwidth selection procedure. Simulations validate these theoretical properties and show existing methods to be biased and inconsistent in this setting. I then apply the proposed methods to study the effects of gubernatorial party control on within-state income distributions in the US, using a close-election design. The results suggest a classic equality-efficiency tradeoff under Democratic governorship, driven by reductions in income at the top of the distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03992v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Van Dijcke</dc:creator>
    </item>
    <item>
      <title>Discriminating BCC Subtypes Using Entropy and Mutual Information from Dermoscopic Features</title>
      <link>https://arxiv.org/abs/2504.04212</link>
      <description>arXiv:2504.04212v1 Announce Type: cross 
Abstract: Objective: To analyze the frequency and co-occurrence of dermoscopic patterns in BCC lesions and their relationship with histopathologic subtypes, using statistical analysis and Information Theory tools such as entropy, conditional entropy, mutual information, and Hamming weight.
  Methods: A total of 223 dermoscopic images (256x256 pixels) of histologically confirmed BCC lesions from Hospital Universitario Virgen Macarena (Seville, Spain) were analyzed. Each image was multilabel-annotated for the presence of nine dermoscopic patterns and categorized into one of four BCC subtypes: superficial, nodular, infiltrative, or micronodular. Statistical and information-theoretic methods were applied, including co-occurrence matrices, Bayesian conditional probabilities, and entropy-based metrics. Mutual information quantified the predictive value of individual and paired patterns, and decision trees were built based on diagnostic informativeness.
  Results: Nodular BCC was highly associated with most dermoscopic patterns, particularly arborizing telangiectasia and blue-gray ovoid nests. Superficial BCC showed stronger associations with maple leaf-like structures and shiny white-red areas. Some patterns, like spoke-wheel areas and white streaks, showed low discriminative power. Mutual information and conditional probabilities identified meaningful pattern-pair dependencies for each subtype. Decision trees revealed that subsets of patterns could enhance subtype classification by accumulating diagnostic information.
  Conclusions: Information Theory enables a quantitative understanding of dermoscopic patterns and their relationship with BCC subtypes. This framework highlights key diagnostic features, aids in differentiating complex cases, and supports the development of automated, pattern-based diagnostic tools. Further research with larger, more balanced datasets is encouraged.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04212v1</guid>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iv\'an Matas, Bego\~na Acha, Francisca Silva-Claver\'ia, Amalia Serrano, Tom\'as Toledo-Pastrana, Carmen Serrano</dc:creator>
    </item>
    <item>
      <title>Perils of Label Indeterminacy: A Case Study on Prediction of Neurological Recovery After Cardiac Arrest</title>
      <link>https://arxiv.org/abs/2504.04243</link>
      <description>arXiv:2504.04243v1 Announce Type: cross 
Abstract: The design of AI systems to assist human decision-making typically requires the availability of labels to train and evaluate supervised models. Frequently, however, these labels are unknown, and different ways of estimating them involve unverifiable assumptions or arbitrary choices. In this work, we introduce the concept of label indeterminacy and derive important implications in high-stakes AI-assisted decision-making. We present an empirical study in a healthcare context, focusing specifically on predicting the recovery of comatose patients after resuscitation from cardiac arrest. Our study shows that label indeterminacy can result in models that perform similarly when evaluated on patients with known labels, but vary drastically in their predictions for patients where labels are unknown. After demonstrating crucial ethical implications of label indeterminacy in this high-stakes context, we discuss takeaways for evaluation, reporting, and design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04243v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jakob Schoeffer, Maria De-Arteaga, Jonathan Elmer</dc:creator>
    </item>
    <item>
      <title>Causal Inference Isn't Special: Why It's Just Another Prediction Problem</title>
      <link>https://arxiv.org/abs/2504.04320</link>
      <description>arXiv:2504.04320v1 Announce Type: cross 
Abstract: Causal inference is often portrayed as fundamentally distinct from predictive modeling, with its own terminology, goals, and intellectual challenges. But at its core, causal inference is simply a structured instance of prediction under distribution shift. In both cases, we begin with labeled data from a source domain and seek to generalize to a target domain where outcomes are not observed. The key difference is that in causal inference, the labels -- potential outcomes -- are selectively observed based on treatment assignment, introducing bias that must be addressed through assumptions. This perspective reframes causal estimation as a familiar generalization problem and highlights how techniques from predictive modeling, such as reweighting and domain adaptation, apply directly to causal tasks. It also clarifies that causal assumptions are not uniquely strong -- they are simply more explicit. By viewing causal inference through the lens of prediction, we demystify its logic, connect it to familiar tools, and make it more accessible to practitioners and educators alike.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04320v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Fern\'andez-Lor\'ia</dc:creator>
    </item>
    <item>
      <title>Constructing the Truth: Text Mining and Linguistic Networks in Public Hearings of Case 03 of the Special Jurisdiction for Peace (JEP)</title>
      <link>https://arxiv.org/abs/2504.04325</link>
      <description>arXiv:2504.04325v2 Announce Type: cross 
Abstract: Case 03 of the Special Jurisdiction for Peace (JEP), focused on the so-called false positives in Colombia, represents one of the most harrowing episodes of the Colombian armed conflict. This article proposes an innovative methodology based on natural language analysis and semantic co-occurrence models to explore, systematize, and visualize narrative patterns present in the public hearings of victims and appearing parties. By constructing skipgram networks and analyzing their modularity, the study identifies thematic clusters that reveal regional and procedural status differences, providing empirical evidence on dynamics of victimization, responsibility, and acknowledgment in this case. This computational approach contributes to the collective construction of both judicial and extrajudicial truth, offering replicable tools for other transitional justice cases. The work is grounded in the pillars of truth, justice, reparation, and non-repetition, proposing a critical and in-depth reading of contested memories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04325v2</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Sosa, Alejandro Urrego-L\'opez, Cesar Prieto, Emma J. Camargo-D\'iaz</dc:creator>
    </item>
    <item>
      <title>A Consequentialist Critique of Binary Classification Evaluation Practices</title>
      <link>https://arxiv.org/abs/2504.04528</link>
      <description>arXiv:2504.04528v1 Announce Type: cross 
Abstract: ML-supported decisions, such as ordering tests or determining preventive custody, often involve binary classification based on probabilistic forecasts. Evaluation frameworks for such forecasts typically consider whether to prioritize independent-decision metrics (e.g., Accuracy) or top-K metrics (e.g., Precision@K), and whether to focus on fixed thresholds or threshold-agnostic measures like AUC-ROC. We highlight that a consequentialist perspective, long advocated by decision theorists, should naturally favor evaluations that support independent decisions using a mixture of thresholds given their prevalence, such as Brier scores and Log loss. However, our empirical analysis reveals a strong preference for top-K metrics or fixed thresholds in evaluations at major conferences like ICML, FAccT, and CHIL. To address this gap, we use this decision-theoretic framework to map evaluation metrics to their optimal use cases, along with a Python package, briertools, to promote the broader adoption of Brier scores. In doing so, we also uncover new theoretical connections, including a reconciliation between the Brier Score and Decision Curve Analysis, which clarifies and responds to a longstanding critique by (Assel, et al. 2017) regarding the clinical utility of proper scoring rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04528v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gerardo Flores, Abigail Schiff, Alyssa H. Smith, Julia A Fukuyama, Ashia C. Wilson</dc:creator>
    </item>
    <item>
      <title>On multipolar magnetic anomaly detection: multipolar signal subspaces, an analytical orthonormal basis, multipolar truncature and detection performance</title>
      <link>https://arxiv.org/abs/2504.05212</link>
      <description>arXiv:2504.05212v1 Announce Type: cross 
Abstract: In this paper, we consider the magnetic anomaly detection problem which aims to find hidden ferromagnetic masses by estimating the weak perturbation they induce on local Earth's magnetic field. We consider classical detection schemes that rely on signals recorded on a moving sensor, and modeling of the source as a function of unknown parameters. As the usual spherical harmonic decomposition of the anomaly has to be truncated in practice, we study the signal vector subspaces induced by each multipole of the decomposition, proving they are not in direct sum, and discussing the impact it has on the choice of the truncation order. Further, to ease the detection strategy based on generalized likelihood ratio test, we rely on orthogonal polynomials theory to derive an analytical set of orthonormal functions (multipolar orthonormal basis functions) that spans the space of the noise-free measured signal. Finally, based on the subspace structure of the multipole vector spaces, we study the impact of the truncation order on the detection performance, beyond the issue of potential surparametrization, and the behaviour of the information criteria used to choose this order.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05212v1</guid>
      <category>eess.SP</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cl\'ement Chenevas-Paule, Steeve Zozor, Laure-Line Rouve, Olivier J. J. Michel, Olivier Pinaud, Romain Kukla</dc:creator>
    </item>
    <item>
      <title>Bayesian local clustering of age-period mortality surfaces across multiple countries</title>
      <link>https://arxiv.org/abs/2504.05240</link>
      <description>arXiv:2504.05240v1 Announce Type: cross 
Abstract: Although traditional literature on mortality modeling has focused on single countries in isolation, recent contributions have progressively moved toward joint models for multiple countries. Besides favoring borrowing of information to improve age-period forecasts, this perspective has also potentials to infer local similarities among countries' mortality patterns in specific age classes and periods that could unveil unexplored demographic trends, while guiding the design of targeted policies. Advancements along this latter relevant direction are currently undermined by the lack of a multi-country model capable of incorporating the core structures of age-period mortality surfaces together with clustering patterns among countries that are not global, but rather vary locally across different combinations of ages and periods. We cover this gap by developing a novel Bayesian model for log-mortality rates that characterizes the age structure of mortality through a B-spline expansion whose country-specific dynamic coefficients encode both changes of this age structure across periods and also local clustering patterns among countries under a time-dependent random partition prior for these country-specific dynamic coefficients. While flexible, this formulation admits tractable posterior inference leveraging a suitably-designed Gibbs-sampler. The application to mortality data from 14 countries unveils local similarities highlighting both previously-recognized demographic phenomena and also yet-unexplored trends.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05240v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giovanni Roman\`o, Emanuele Aliverti, Daniele Durante</dc:creator>
    </item>
    <item>
      <title>Interpretable sensitivity analysis for the Baron-Kenny approach to mediation with unmeasured confounding</title>
      <link>https://arxiv.org/abs/2205.08030</link>
      <description>arXiv:2205.08030v4 Announce Type: replace 
Abstract: Mediation analysis assesses the extent to which the exposure affects the outcome indirectly through a mediator and the extent to which it operates directly through other pathways. The popular Baron-Kenny approach estimates the indirect and direct effects of the exposure on the outcome based on linear regressions. However, when the exposure and the mediator are not randomized, the estimates may be biased due to unmeasured confounding. We first derive general omitted-variable bias formulas in linear regressions with vector responses and regressors. We then use the formulas to develop a sensitivity analysis method for the Baron-Kenny approach in the presence of unmeasured confounding. To ensure interpretability, we express the sensitivity parameters to correspond to the natural factorization of the joint distribution of the direct acyclic graph. They measure the partial correlation between the unmeasured confounder and the exposure, mediator, and outcome, respectively. We further propose a novel measure called the "robustness value for mediation" or simply the "robustness value", to assess the robustness of results based on the Baron-Kenny approach with respect to unmeasured confounding. Intuitively, the robustness value measures the minimum value of the maximum proportion of variability explained by the unmeasured confounding, for the exposure, mediator, and outcome, to overturn the results of the direct and indirect effect estimates. Importantly, we prove that all our sensitivity bounds are attainable and thus sharp.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.08030v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingrui Zhang, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Knowledge Distillation Decision Tree for Unravelling Black-box Machine Learning Models</title>
      <link>https://arxiv.org/abs/2206.04661</link>
      <description>arXiv:2206.04661v4 Announce Type: replace 
Abstract: Machine learning models, particularly the black-box models, are widely favored for their outstanding predictive capabilities. However, they often face scrutiny and criticism due to the lack of interpretability. Paradoxically, their strong predictive capabilities may indicate a deep understanding of the underlying data, implying significant potential for interpretation. Leveraging the emerging concept of knowledge distillation, we introduce the method of knowledge distillation decision tree (KDDT). This method enables the distillation of knowledge about the data from a black-box model into a decision tree, thereby facilitating the interpretation of the black-box model. Essential attributes for a good interpretable model include simplicity, stability, and predictivity. The primary challenge of constructing interpretable tree lies in ensuring structural stability under the randomness of the training data. KDDT is developed with the theoretical foundations demonstrating that structure stability can be achieved under mild assumptions. Furthermore, we propose the hybrid KDDT to achieve both simplicity and predictivity. An efficient algorithm is provided for constructing the hybrid KDDT. Simulation studies and a real-data analysis validate the hybrid KDDT's capability to deliver accurate and reliable interpretations. KDDT is an excellent interpretable model with great potential for practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.04661v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuetao Lu, J. Jack Lee</dc:creator>
    </item>
    <item>
      <title>Generalized Bayesian Multidimensional Scaling and Model Comparison</title>
      <link>https://arxiv.org/abs/2306.15908</link>
      <description>arXiv:2306.15908v2 Announce Type: replace 
Abstract: Multidimensional scaling (MDS) is widely used to reconstruct a low-dimensional representation of high-dimensional data while preserving pairwise distances. However, Bayesian MDS approaches based on Markov chain Monte Carlo (MCMC) face challenges in model generalization and comparison. To address these limitations, we propose a generalized Bayesian multidimensional scaling (GBMDS) framework that accommodates non-Gaussian errors and diverse dissimilarity metrics for improved robustness. We develop an adaptive annealed Sequential Monte Carlo (ASMC) algorithm for Bayesian inference, leveraging an annealing schedule to enhance posterior exploration and computational efficiency. The ASMC algorithm also provides a nearly unbiased marginal likelihood estimator, enabling principled Bayesian model comparison across different error distributions, dissimilarity metrics, and dimensional choices. Using synthetic and real data, we demonstrate the effectiveness of the proposed approach. Our results show that ASMC-based GBMDS achieves superior computational efficiency and robustness compared to MCMC-based methods under the same computational budget. The implementation of our proposed method and applications are available at https://github.com/SFU-Stat-ML/GBMDS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15908v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiarui Zhang, Jiguo Cao, Liangliang Wang</dc:creator>
    </item>
    <item>
      <title>Sparse Fr\'echet Sufficient Dimension Reduction with Graphical Structure Among Predictors</title>
      <link>https://arxiv.org/abs/2310.19114</link>
      <description>arXiv:2310.19114v2 Announce Type: replace 
Abstract: Fr\'echet regression has received considerable attention to model metric-space valued responses that are complex and non-Euclidean data, such as probability distributions and vectors on the unit sphere. However, existing Fr\'echet regression literature focuses on the classical setting where the predictor dimension is fixed, and the sample size goes to infinity. This paper proposes sparse Fr\'echet sufficient dimension reduction with graphical structure among high-dimensional Euclidean predictors. In particular, we propose a convex optimization problem that leverages the graphical information among predictors and avoids inverting the high-dimensional covariance matrix. We also provide the Alternating Direction Method of Multipliers (ADMM) algorithm to solve the optimization problem. Theoretically, the proposed method achieves subspace estimation and variable selection consistency under suitable conditions. Extensive simulations and a real data analysis are carried out to illustrate the finite-sample performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.19114v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaying Weng, Kai Tan, Cheng Wang, Zhou Yu</dc:creator>
    </item>
    <item>
      <title>Distributed Tensor Principal Component Analysis with Data Heterogeneity</title>
      <link>https://arxiv.org/abs/2405.11681</link>
      <description>arXiv:2405.11681v3 Announce Type: replace 
Abstract: As tensors become widespread in modern data analysis, Tucker low-rank Principal Component Analysis (PCA) has become essential for dimensionality reduction and structural discovery in tensor datasets. Motivated by the common scenario where large-scale tensors are distributed across diverse geographic locations, this paper investigates tensor PCA within a distributed framework where direct data pooling is impractical.
  We offer a comprehensive analysis of three specific scenarios in distributed Tensor PCA: a homogeneous setting in which tensors at various locations are generated from a single noise-affected model; a heterogeneous setting where tensors at different locations come from distinct models but share some principal components, aiming to improve estimation across all locations; and a targeted heterogeneous setting, designed to boost estimation accuracy at a specific location with limited samples by utilizing transferred knowledge from other sites with ample data.
  We introduce novel estimation methods tailored to each scenario, establish statistical guarantees, and develop distributed inference techniques to construct confidence regions. Our theoretical findings demonstrate that these distributed methods achieve sharp rates of accuracy by efficiently aggregating shared information across different tensors, while maintaining reasonable communication costs. Empirical validation through simulations and real-world data applications highlights the advantages of our approaches, particularly in managing heterogeneous tensor data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11681v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Elynn Chen, Xi Chen, Wenbo Jing, Yichen Zhang</dc:creator>
    </item>
    <item>
      <title>Analysis of Stepped-Wedge Cluster Randomized Trials when treatment effects vary by exposure time or calendar time</title>
      <link>https://arxiv.org/abs/2409.14706</link>
      <description>arXiv:2409.14706v2 Announce Type: replace 
Abstract: Stepped-wedge cluster randomized trials (SW-CRTs) are traditionally analyzed with models that assume an immediate and sustained treatment effect. Previous work has shown that making such an assumption in the analysis of SW-CRTs when the true underlying treatment effect varies by exposure time can produce severely misleading estimates. Alternatively, the true underlying treatment effect might vary by calendar time. Comparatively less work has examined treatment effect structure misspecification in this setting. Here, we evaluate the behavior of the mixed effects model-based immediate treatment effect, exposure time-averaged treatment effect, and calendar time-averaged treatment effect estimators in different scenarios where they are misspecified for the true underlying treatment effect structure. We show that the immediate treatment effect estimator is relatively robust to bias when estimating a true underlying calendar time-averaged treatment effect estimand. However, when there is a true underlying calendar (exposure) time-varying treatment effect, misspecifying an analysis with an exposure (calendar) time-averaged treatment effect estimator can yield severely misleading estimates and even converge to a value of the opposite sign of the true calendar (exposure) time-averaged treatment effect estimand. In this article, we highlight the two different time scales on which treatment effects can vary in SW-CRTs and clarify potential vulnerabilities that may arise when considering different types of time-varying treatment effects in a SW design. Accordingly, we emphasize the need for researchers to carefully consider whether the treatment effect may vary as a function of exposure time and/or calendar time in the analysis of SW-CRTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14706v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenneth M. Lee, Elizabeth L. Turner, Avi Kenny</dc:creator>
    </item>
    <item>
      <title>Negative Control Outcome Adjustment in Early-Phase Randomized Trials: Estimating Vaccine Effects on Immune Responses in HIV Exposed Uninfected Infants</title>
      <link>https://arxiv.org/abs/2410.08078</link>
      <description>arXiv:2410.08078v2 Announce Type: replace 
Abstract: Adjustment for prognostic baseline variables can reduce bias due to covariate imbalance and increase efficiency in randomized trials. While the use of covariate adjustment in late-phase trials is justified by favorable large-sample properties, it is seldom used in small, early-phase studies, due to uncertainty in which variables are prognostic and the potential for precision loss, type I error rate inflation, and undercoverage of confidence intervals. To address this problem, we consider adjustment for a valid negative control outcome (NCO), or an auxiliary post-randomization outcome believed completely unaffected by treatment but more highly correlated with the primary outcome than baseline covariates. We articulate the assumptions that permit adjustment for NCOs without producing post-randomization selection bias, and describe plausible data generating models where NCO adjustment can improve upon adjustment for baseline covariates alone. In numerical experiments, we illustrate performance and provide practical recommendations regarding model selection and finite-sample variance corrections. We apply our methods to the reanalysis of two early-phase vaccine trials in HIV exposed uninfected (HEU) infants, where we demonstrate that adjustment for auxiliary post-baseline immunological parameters can enhance precision of vaccine effect estimates relative to standard approaches that avoid adjustment or adjust for baseline covariates alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08078v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ethan Ashby, Bo Zhang, Genevieve G Fouda, Youyi Fong, Holly Janes</dc:creator>
    </item>
    <item>
      <title>Sparse Bayesian Factor Models with Mass-Nonlocal Factor Scores</title>
      <link>https://arxiv.org/abs/2412.00304</link>
      <description>arXiv:2412.00304v2 Announce Type: replace 
Abstract: Bayesian factor models are widely used for dimensionality reduction and pattern discovery in high-dimensional datasets across diverse fields. These models typically focus on imposing priors on factor loading to induce sparsity and improve interpretability. However, factor scores, which play a critical role in individual-level associations with factors, have received less attention and are assumed to follow a standard normal distribution. This assumption oversimplifies the heterogeneity often observed in real-world applications. We propose the sparse Bayesian Factor model with MAss-Nonlocal factor scores (BFMAN), a novel framework that addresses these limitations by introducing a mass-nonlocal prior on factor scores. This prior allows for both exact zeros and flexible, nonlocal behavior, capturing individual-level sparsity and heterogeneity. The sparsity in the score matrix enables a robust and novel approach to determine the optimal number of factors. Model parameters are estimated via a fast and efficient Gibbs sampler. Extensive simulations demonstrate that BFMAN outperforms standard Bayesian factor models in factor recovery, sparsity detection, score estimation, and selection of the optimal number of factors. We apply BFMAN to the Hispanic Community Health Study/Study of Latinos, identifying meaningful dietary patterns and their associations with cardiovascular disease, showcasing the model's ability to uncover insights into complex nutritional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00304v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingjie Huang, Dafne Zorzetto, Roberta De Vito</dc:creator>
    </item>
    <item>
      <title>CMHSU: An R Statistical Software Package to Detect Mental Health Status, Substance Use Status, and their Concurrent Status in the North American Healthcare Administrative Databases</title>
      <link>https://arxiv.org/abs/2501.06435</link>
      <description>arXiv:2501.06435v4 Announce Type: replace 
Abstract: The concept of concurrent mental health and substance use (MHSU) and its detection in patients has garnered growing interest among psychiatrists and healthcare policymakers over the past four decades. Researchers have proposed various diagnostic methods, including the Data-Driven Diagnostic Method (DDDM), for the identification of MHSU. However, the absence of a standalone statistical software package to facilitate DDDM for large healthcare administrative databases has remained a significant gap. This paper introduces the R statistical software package CMHSU, available on the Comprehensive R Archive Network (CRAN), for the diagnosis of mental health (MH), substance use (SU), and their concurrent status (MHSU). The package implements DDDM using hospital and medical service physician visit counts along with maximum time span parameters for MH, SU, and MHSU diagnoses. A working example using a simulated real-world dataset is presented to examine various analytical aspects, including three key dimensions of MHSU detection based on the DDDM framework, as well as temporal analysis to demonstrate the package's application for healthcare policymakers. Additionally, the limitations of the CMHSU package and potential directions for its future extension are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06435v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Soltanifar, Chel Hee Lee</dc:creator>
    </item>
    <item>
      <title>Dynamic spectral co-clustering of directed networks to unveil latent community paths in VAR-type models</title>
      <link>https://arxiv.org/abs/2502.10849</link>
      <description>arXiv:2502.10849v2 Announce Type: replace 
Abstract: Identifying network Granger causality in large vector autoregressive (VAR) models enhances explanatory power by capturing complex interdependencies among variables. Instead of constructing network structures solely through sparse estimation of coefficients, we explore latent community structures to uncover the underlying network dynamics. We propose a dynamic network framework that embeds directed connectivity within the transition matrices of VAR-type models, enabling tracking of evolving community structures over time. To incorporate network directionality, we employ degree-corrected stochastic co-block models for each season or cycle, integrating spectral co-clustering with singular vector smoothing to refine latent community transitions. For greater model parsimony, we adopt periodic VAR (PVAR) and vector heterogeneous autoregressive (VHAR) models as alternatives to high-lag VAR models. We provide theoretical justifications for the proposed methodology and demonstrate its effectiveness through applications to the cyclic evolution of US nonfarm payroll employment and the temporal progression of realized stock market volatilities. Indeed, spectral co-clustering of directed networks reveals dynamic latent community trajectories, offering deeper insights into the evolving structure of high-dimensional time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10849v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Younghoon Kim, Changryong Baek</dc:creator>
    </item>
    <item>
      <title>Bias in Gini coefficient estimation for gamma mixture populations</title>
      <link>https://arxiv.org/abs/2503.00690</link>
      <description>arXiv:2503.00690v3 Announce Type: replace 
Abstract: This paper examines the properties of the Gini coefficient estimator for gamma mixture populations and reveals the presence of bias. In contrast, we show that sampling from a gamma distribution yields an unbiased estimator, consistent with prior research (Baydil et al., 2025). We derive an explicit bias expression for the Gini coefficient in gamma mixture populations, which serves as the foundation for proposing a bias-corrected Gini estimator. We conduct a Monte Carlo simulation study to evaluate the behavior of the bias-corrected Gini estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00690v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vila, Helton Saulo</dc:creator>
    </item>
    <item>
      <title>On a fast consistent selection of nested models with possibly unnormalised probability densities</title>
      <link>https://arxiv.org/abs/2503.06331</link>
      <description>arXiv:2503.06331v2 Announce Type: replace 
Abstract: Models with unnormalized probability density functions are ubiquitous in statistics, artificial intelligence and many other fields. However, they face significant challenges in model selection if the normalizing constants are intractable. Existing methods to address this issue often incur high computational costs, either due to numerical approximations of normalizing constants or evaluation of bias corrections in information criteria. In this paper, we propose a novel and fast selection criterion, T-GIC, for nested models, allowing direct data sampling from a possibly unnormalized probability density function. T-GIC gives a consistent selection under mild regularity conditions and is computationally efficient, benefiting from a multiplying factor that depends only on the sample size and the model complexity. Extensive simulation studies and real-data applications demonstrate the efficacy of T-GIC in the selection of nested models with unnormalized probability densities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06331v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rong Bian, Kung-Sik Chan, Bing Cheng, Howell Tong</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for High-dimensional Time Series with a Directed Acyclic Graphical Structure</title>
      <link>https://arxiv.org/abs/2503.23563</link>
      <description>arXiv:2503.23563v3 Announce Type: replace 
Abstract: In multivariate time series analysis, understanding the underlying causal relationships among variables is often of interest for various applications. Directed acyclic graphs (DAGs) provide a powerful framework for representing causal dependencies. This paper proposes a novel Bayesian approach for modeling multivariate time series where conditional independencies and causal structure are encoded by a DAG. The proposed model allows structural properties such as stationarity to be easily accommodated. Given the application, we further extend the model for matrix-variate time series. We take a Bayesian approach to inference, and a ``projection-posterior'' based efficient computational algorithm is developed. The posterior convergence properties of the proposed method are established along with two identifiability results for the unrestricted structural equation models. The utility of the proposed method is demonstrated through simulation studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23563v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arkaprava Roy, Anindya Roy, Subhashis Ghosal</dc:creator>
    </item>
    <item>
      <title>Semiparametric Counterfactual Regression</title>
      <link>https://arxiv.org/abs/2504.02694</link>
      <description>arXiv:2504.02694v2 Announce Type: replace 
Abstract: We study counterfactual regression, which aims to map input features to outcomes under hypothetical scenarios that differ from those observed in the data. This is particularly useful for decision-making when adapting to sudden shifts in treatment patterns is essential. We propose a doubly robust-style estimator for counterfactual regression within a generalizable framework that accommodates a broad class of risk functions and flexible constraints, drawing on tools from semiparametric theory and stochastic optimization. Our approach uses incremental interventions to enhance adaptability while maintaining consistency with standard methods. We formulate the target estimand as the optimal solution to a stochastic optimization problem and develop an efficient estimation strategy, where we can leverage rapid development of modern optimization algorithms. We go on to analyze the rates of convergence and characterize the asymptotic distributions. Our analysis shows that the proposed estimators can achieve $\sqrt{n}$-consistency and asymptotic normality for a broad class of problems. Numerical illustrations highlight their effectiveness in adapting to unseen counterfactual scenarios while maintaining parametric convergence rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02694v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kwangho Kim</dc:creator>
    </item>
    <item>
      <title>Fused Extended Two-Way Fixed Effects for Difference-in-Differences With Staggered Adoptions</title>
      <link>https://arxiv.org/abs/2312.05985</link>
      <description>arXiv:2312.05985v4 Announce Type: replace-cross 
Abstract: To address the bias of the canonical two-way fixed effects estimator for difference-in-differences under staggered adoptions, Wooldridge (2021) proposed the extended two-way fixed effects estimator, which adds many parameters. However, this reduces efficiency. Restricting some of these parameters to be equal (for example, subsequent treatment effects within a cohort) helps, but ad hoc restrictions may reintroduce bias. We propose a machine learning estimator with a single tuning parameter, fused extended two-way fixed effects (FETWFE), that enables automatic data-driven selection of these restrictions. We prove that under an appropriate sparsity assumption FETWFE identifies the correct restrictions with probability tending to one, which improves efficiency. We also prove the consistency, oracle property, and asymptotic normality of FETWFE for several classes of heterogeneous marginal treatment effect estimators under either conditional or marginal parallel trends, and we prove the same results for conditional average treatment effects under conditional parallel trends. We provide an R package implementing fused extended two-way fixed effects, and we demonstrate FETWFE in simulation studies and an empirical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05985v4</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregory Faletto</dc:creator>
    </item>
    <item>
      <title>Asymptotics for estimating a diverging number of parameters -- with and without sparsity</title>
      <link>https://arxiv.org/abs/2411.17395</link>
      <description>arXiv:2411.17395v2 Announce Type: replace-cross 
Abstract: We consider high-dimensional estimation problems where the number of parameters diverges with the sample size. General conditions are established for consistency, uniqueness, and asymptotic normality in both unpenalized and penalized estimation settings. The conditions are weak and accommodate a broad class of estimation problems, including ones with non-convex and group structured penalties. The wide applicability of the results is illustrated through diverse examples, including generalized linear models, multi-sample inference, and stepwise estimation procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17395v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jana Gauss, Thomas Nagler</dc:creator>
    </item>
  </channel>
</rss>
