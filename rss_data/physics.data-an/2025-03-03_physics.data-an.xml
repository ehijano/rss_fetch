<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 03 Mar 2025 05:00:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Towards Ultimate NMR Resolution with Deep Learning</title>
      <link>https://arxiv.org/abs/2502.20793</link>
      <description>arXiv:2502.20793v1 Announce Type: cross 
Abstract: In multidimensional NMR spectroscopy, practical resolution is defined as the ability to distinguish and accurately determine signal positions against a background of overlapping peaks, thermal noise, and spectral artifacts. In the pursuit of ultimate resolution, we introduce Peak Probability Presentations ($P^3$)- a statistical spectral representation that assigns a probability to each spectral point, indicating the likelihood of a peak maximum occurring at that location. The mapping between the spectrum and $P^3$ is achieved using MR-Ai, a physics-inspired deep learning neural network architecture, designed to handle multidimensional NMR spectra. Furthermore, we demonstrate that MR-Ai enables coprocessing of multiple spectra, facilitating direct information exchange between datasets. This feature significantly enhances spectral quality, particularly in cases of highly sparse sampling. Performance of MR-Ai and high value of the $P^3$ are demonstrated on the synthetic data and spectra of Tau, MATL1, Calmodulin, and several other proteins.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20793v1</guid>
      <category>physics.bio-ph</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Amir Jahangiri, Tatiana Agback, Ulrika Brath, Vladislav Orekhov</dc:creator>
    </item>
    <item>
      <title>Analysis of Evolving Cortical Neuronal Networks Using Visual Informatics</title>
      <link>https://arxiv.org/abs/2502.20862</link>
      <description>arXiv:2502.20862v1 Announce Type: cross 
Abstract: Understanding the nature of the changes exhibited by evolving neuronal dynamics from high-dimensional activity data is essential for advancing neuroscience, particularly in the study of neuronal network development and the pathophysiology of neurological disorders. This work examines how advanced dimensionality reduction techniques can efficiently summarize and enhance our understanding of the development of neuronal networks over time and in response to stimulation. We develop a framework based on the Minimum-Distortion Embedding (MDE) methods and demonstrate how MDE outperforms better known benchmarks based on Principal Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE) by effectively preserving both global structures and local relationships within complex neuronal datasets. Our \emph{in silico} experiments reveal MDE's capability to capture the evolving connectivity patterns of simulated neuronal networks, illustrating a clear trajectory tracking the simulated network development. Complementary \emph{in vitro} experiments further validate MDE's advantages, highlighting its ability to identify behavioral differences and connectivity changes in neuronal cultures over a 35-day observation period. Additionally, we explore the effects of stimulation on neuronal activity, providing valuable insights into the plasticity and learning mechanisms of neuronal networks. Our findings underscore the importance of metric selection in dimensionality reduction, showing that correlation metrics yield more meaningful embeddings compared to Euclidean distance. The implications of this research extend to various areas, including the potential development of therapeutic intervention strategies for neurological disorders, and the identification of distinct phases of neuronal activity for advancing cortical-based computing devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20862v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>physics.data-an</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ho Fai Po, Akke Mats Houben, Anna-Christina Haeb, Yordan P. Raykov, Daniel Tornero, Jordi Soriano, David Saad</dc:creator>
    </item>
    <item>
      <title>Position: Solve Layerwise Linear Models First to Understand Neural Dynamical Phenomena (Neural Collapse, Emergence, Lazy/Rich Regime, and Grokking)</title>
      <link>https://arxiv.org/abs/2502.21009</link>
      <description>arXiv:2502.21009v1 Announce Type: cross 
Abstract: In physics, complex systems are often simplified into minimal, solvable models that retain only the core principles. In machine learning, layerwise linear models (e.g., linear neural networks) act as simplified representations of neural network dynamics. These models follow the dynamical feedback principle, which describes how layers mutually govern and amplify each other's evolution. This principle extends beyond the simplified models, successfully explaining a wide range of dynamical phenomena in deep neural networks, including neural collapse, emergence, lazy and rich regimes, and grokking. In this position paper, we call for the use of layerwise linear models retaining the core principles of neural dynamical phenomena to accelerate the science of deep learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21009v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yoonsoo Nam, Seok Hyeong Lee, Clementine Domine, Yea Chan Park, Charles London, Wonyl Choi, Niclas Goring, Seungjai Lee</dc:creator>
    </item>
    <item>
      <title>Supporting the development of Machine Learning for fundamental science in a federated Cloud with the AI_INFN platform</title>
      <link>https://arxiv.org/abs/2502.21266</link>
      <description>arXiv:2502.21266v1 Announce Type: cross 
Abstract: Machine Learning (ML) is driving a revolution in the way scientists design, develop, and deploy data-intensive software. However, the adoption of ML presents new challenges for the computing infrastructure, particularly in terms of provisioning and orchestrating access to hardware accelerators for development, testing, and production. The INFN-funded project AI_INFN ("Artificial Intelligence at INFN") aims at fostering the adoption of ML techniques within INFN use cases by providing support on multiple aspects, including the provision of AI-tailored computing resources. It leverages cloud-native solutions in the context of INFN Cloud, to share hardware accelerators as effectively as possible, ensuring the diversity of the Institute's research activities is not compromised. In this contribution, we provide an update on the commissioning of a Kubernetes platform designed to ease the development of GPU-powered data analysis workflows and their scalability on heterogeneous, distributed computing resources, possibly federated as Virtual Kubelets with the interLink provider.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21266v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucio Anderlini, Matteo Barbetti, Giulio Bianchini, Diego Ciangottini, Stefano Dal Pra, Diego Michelotto, Carmelo Pellegrino, Rosa Petrini, Alessandro Pascolini, Daniele Spiga</dc:creator>
    </item>
  </channel>
</rss>
