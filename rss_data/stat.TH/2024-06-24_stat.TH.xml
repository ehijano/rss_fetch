<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Jun 2024 02:36:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On the estimation rate of Bayesian PINN for inverse problems</title>
      <link>https://arxiv.org/abs/2406.14808</link>
      <description>arXiv:2406.14808v1 Announce Type: new 
Abstract: Solving partial differential equations (PDEs) and their inverse problems using Physics-informed neural networks (PINNs) is a rapidly growing approach in the physics and machine learning community. Although several architectures exist for PINNs that work remarkably in practice, our theoretical understanding of their performances is somewhat limited. In this work, we study the behavior of a Bayesian PINN estimator of the solution of a PDE from $n$ independent noisy measurement of the solution. We focus on a class of equations that are linear in their parameters (with unknown coefficients $\theta_\star$). We show that when the partial differential equation admits a classical solution (say $u_\star$), differentiable to order $\beta$, the mean square error of the Bayesian posterior mean is at least of order $n^{-2\beta/(2\beta + d)}$. Furthermore, we establish a convergence rate of the linear coefficients of $\theta_\star$ depending on the order of the underlying differential operator. Last but not least, our theoretical results are validated through extensive simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14808v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Sun, Debarghya Mukherjee, Yves Atchade</dc:creator>
    </item>
    <item>
      <title>Further Results Involving Residual and Past Extropy with their Applications</title>
      <link>https://arxiv.org/abs/2406.14985</link>
      <description>arXiv:2406.14985v1 Announce Type: new 
Abstract: In this paper we focus on the study of the monotonicity properties of the residual and the past extropy as well as on some characterization problems. We then apply the derived results to analyze further stochastic aspects of order statistics, coherent systems, and record values. Using various examples, we illustrate the applicability and significance of the results obtained. In addition, non-parametric estimators for the residual and past extropy measures are introduced. The performance of the acquired estimators has been illustrated using simulated data sets and also real data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14985v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamed Kayid</dc:creator>
    </item>
    <item>
      <title>Early stopping for conjugate gradients in statistical inverse problems</title>
      <link>https://arxiv.org/abs/2406.15001</link>
      <description>arXiv:2406.15001v1 Announce Type: new 
Abstract: We consider estimators obtained by iterates of the conjugate gradient (CG) algorithm applied to the normal equation of prototypical statistical inverse problems. Stopping the CG algorithm early induces regularisation, and optimal convergence rates of prediction and reconstruction error are established in wide generality for an ideal oracle stopping time. Based on this insight, a fully data-driven early stopping rule $\tau$ is constructed, which also attains optimal rates, provided the error in estimating the noise level is not dominant.
  The error analysis of CG under statistical noise is subtle due to its nonlinear dependence on the observations. We provide an explicit error decomposition into two terms, which shares important properties of the classical bias-variance decomposition. Together with a continuous interpolation between CG iterates, this paves the way for a comprehensive error analysis of early stopping. In particular, a general oracle-type inequality is proved for the prediction error at $\tau$. For bounding the reconstruction error, a more refined probabilistic analysis, based on concentration of self-normalised Gaussian processes, is developed. The methodology also provides some new insights into early stopping for CG in deterministic inverse problems. A numerical study for standard examples shows good results in practice for early stopping at $\tau$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15001v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laura Hucker, Markus Rei{\ss}</dc:creator>
    </item>
    <item>
      <title>Stochastic comparisons of record values based on their relative aging</title>
      <link>https://arxiv.org/abs/2406.15196</link>
      <description>arXiv:2406.15196v1 Announce Type: new 
Abstract: In this paper we examine some relative orderings of upper and lower records. It is shown that if m &gt; n, the mth upper record ages faster than the nth upper record, where the data sets come from a sequence of independent and identically distributed observations from a continuous distribution. Sufficient conditions are also obtained to see whether the mth upper record arisen from a continuous distribution ages faster in terms of the relative hazard rate than the nth upper record arisen from another continuous distribution. It is also shown that the reversed hazard rate of the mth lower record decreases faster than the reversed hazard rate of the nth lower record, when m &gt; n. Preservation property of the relative reversed hazard rate order at lower record values is investigated. Several examples are presented to examine the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15196v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamed Kayid</dc:creator>
    </item>
    <item>
      <title>On Naive Mean-Field Approximation for high-dimensional canonical GLMs</title>
      <link>https://arxiv.org/abs/2406.15247</link>
      <description>arXiv:2406.15247v1 Announce Type: new 
Abstract: We study the validity of the Naive Mean Field (NMF) approximation for canonical GLMs with product priors. This setting is challenging due to the non-conjugacy of the likelihood and the prior. Using the theory of non-linear large deviations (Austin 2019, Chatterjee, Dembo 2016, Eldan 2018), we derive sufficient conditions for the tightness of the NMF approximation to the log-normalizing constant of the posterior distribution. As a second contribution, we establish that under minor conditions on the design, any NMF optimizer is a product distribution where each component is a quadratic tilt of the prior. In turn, this suggests novel iterative algorithms for fitting the NMF optimizer to the target posterior. Finally, we establish that if the NMF optimization problem has a "well-separated maximizer", then this optimizer governs the probabilistic properties of the posterior. Specifically, we derive credible intervals with average coverage guarantees, and characterize the prediction performance on an out-of-sample datapoint in terms of this dominant optimizer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15247v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sumit Mukherjee, Jiaze Qiu, Subhabrata Sen</dc:creator>
    </item>
    <item>
      <title>From isotonic to Lipschitz regression: a new interpolative perspective on shape-restricted estimation</title>
      <link>https://arxiv.org/abs/2307.05732</link>
      <description>arXiv:2307.05732v3 Announce Type: cross 
Abstract: This manuscript seeks to bridge two seemingly disjoint paradigms of nonparametric regression estimation based on smoothness assumptions and shape constraints. The proposed approach is motivated by a conceptually simple observation: Every Lipschitz function is a sum of monotonic and linear functions. This principle is further generalized to the higher-order monotonicity and multivariate covariates. A family of estimators is proposed based on a sample-splitting procedure, which inherits desirable methodological, theoretical, and computational properties of shape-restricted estimators. Our theoretical analysis provides convergence guarantees of the estimator under heteroscedastic and heavy-tailed errors, as well as adaptive properties to the complexity of the true regression function. The generality of the proposed decomposition framework is demonstrated through new approximation results, and extensive numerical studies validate the theoretical properties and empirical evidence for the practicalities of the proposed estimation framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.05732v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenta Takatsu, Tianyu Zhang, Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>Pathological Regularization Regimes in Classification Tasks</title>
      <link>https://arxiv.org/abs/2406.14731</link>
      <description>arXiv:2406.14731v1 Announce Type: cross 
Abstract: In this paper we demonstrate the possibility of a trend reversal in binary classification tasks between the dataset and a classification score obtained from a trained model. This trend reversal occurs for certain choices of the regularization parameter for model training, namely, if the parameter is contained in what we call the pathological regularization regime. For ridge regression, we give necessary and sufficient algebraic conditions on the dataset for the existence of a pathological regularization regime. Moreover, our results provide a data science practitioner with a hands-on tool to avoid hyperparameter choices suffering from trend reversal. We furthermore present numerical results on pathological regularization regimes for logistic regression. Finally, we draw connections to datasets exhibiting Simpson's paradox, providing a natural source of pathological datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14731v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Maximilian Wiesmann, Paul Larsen</dc:creator>
    </item>
    <item>
      <title>Robust parameter estimation for partially observed second-order diffusion processes</title>
      <link>https://arxiv.org/abs/2406.14738</link>
      <description>arXiv:2406.14738v1 Announce Type: cross 
Abstract: Estimating parameters of a diffusion process given continuous-time observations of the process via maximum likelihood approaches or, online, via stochastic gradient descent or Kalman filter formulations constitutes a well-established research area. It has also been established previously that these techniques are, in general, not robust to perturbations in the data in the form of temporal correlations. While the subject is relatively well understood and appropriate modifications have been suggested in the context of multi-scale diffusion processes and their reduced model equations, we consider here an alternative setting where a second-order diffusion process in positions and velocities is only observed via its positions. In this note, we propose a simple modification to standard stochastic gradient descent and Kalman filter formulations, which eliminates the arising systematic estimation biases. The modification can be extended to standard maximum likelihood approaches and avoids computation of previously proposed correction terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14738v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Albrecht, Sebastian Reich</dc:creator>
    </item>
    <item>
      <title>Change Acceleration and Detection</title>
      <link>https://arxiv.org/abs/1710.00915</link>
      <description>arXiv:1710.00915v5 Announce Type: replace 
Abstract: A novel sequential change detection problem is proposed, in which the goal is to not only detect but also accelerate the change. Specifically, it is assumed that the sequentially collected observations are responses to treatments selected in real time. The assigned treatments determine the pre-change and post-change distributions of the responses and also influence when the change happens. The goal is to find a treatment assignment rule and a stopping rule that minimize the expected total number of observations subject to a user-specified bound on the false alarm probability. The optimal solution is obtained under a general Markovian change-point model. Moreover, an alternative procedure is proposed, whose applicability is not restricted to Markovian change-point models and whose design requires minimal computation. For a large class of change-point models, the proposed procedure is shown to achieve the optimal performance in an asymptotic sense. Finally, its performance is found in simulation studies to be comparable to the optimal, uniformly with respect to the error probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:1710.00915v5</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanglei Song, Georgios Fellouris</dc:creator>
    </item>
    <item>
      <title>Reduced-bias estimation of the residual dependence index with unnamed marginals</title>
      <link>https://arxiv.org/abs/2112.02899</link>
      <description>arXiv:2112.02899v3 Announce Type: replace 
Abstract: This paper addresses important weaknesses in current methodology for the estimation of multivariate extreme event distributions. The estimation of the residual dependence index $\eta \in (0,1]$ is notoriously problematic. We introduce a flexible class of reduced-bias estimators for this parameter, designed to ameliorate the usual problems of threshold selection through a unified approach to the familiar margins standardisation. We derive the associated asymptotic properties. The efficacy of the proposed semi-parametric inference on $\eta$ stems from a hitherto neglected exponentially decaying term in the hidden regular variation characterisation. Simulation studies to assess the performance for finite samples over a range of standard copulas indicate an improved performance, relative to the existing standard methods such as the Hill estimator. Our leading application illustrates how asymptotic independence can be discerned from monsoon-related rainfall occurrences at different locations in Ghana. The considerations involved in extending this framework to feasible inference on the extreme value index attached to domains of attraction are briefly discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.02899v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jennifer Israelsson, Emily Black, Claudia Neves, David Walshaw</dc:creator>
    </item>
    <item>
      <title>Radial Neighbors for Provably Accurate Scalable Approximations of Gaussian Processes</title>
      <link>https://arxiv.org/abs/2211.14692</link>
      <description>arXiv:2211.14692v4 Announce Type: replace 
Abstract: In geostatistical problems with massive sample size, Gaussian processes can be approximated using sparse directed acyclic graphs to achieve scalable $O(n)$ computational complexity. In these models, data at each location are typically assumed conditionally dependent on a small set of parents which usually include a subset of the nearest neighbors. These methodologies often exhibit excellent empirical performance, but the lack of theoretical validation leads to unclear guidance in specifying the underlying graphical model and sensitivity to graph choice. We address these issues by introducing radial neighbors Gaussian processes (RadGP), a class of Gaussian processes based on directed acyclic graphs in which directed edges connect every location to all of its neighbors within a predetermined radius. We prove that any radial neighbors Gaussian process can accurately approximate the corresponding unrestricted Gaussian process in Wasserstein-2 distance, with an error rate determined by the approximation radius, the spatial covariance function, and the spatial dispersion of samples. We offer further empirical validation of our approach via applications on simulated and real world data showing excellent performance in both prior and posterior approximations to the original Gaussian process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.14692v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yichen Zhu, Michele Peruzzi, Cheng Li, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>A non-backtracking method for long matrix and tensor completion</title>
      <link>https://arxiv.org/abs/2304.02077</link>
      <description>arXiv:2304.02077v5 Announce Type: replace 
Abstract: We consider the problem of low-rank rectangular matrix completion in the regime where the matrix $M$ of size $n\times m$ is ``long", i.e., the aspect ratio $m/n$ diverges to infinity. Such matrices are of particular interest in the study of tensor completion, where they arise from the unfolding of a low-rank tensor. In the case where the sampling probability is $\frac{d}{\sqrt{mn}}$, we propose a new spectral algorithm for recovering the singular values and left singular vectors of the original matrix $M$ based on a variant of the standard non-backtracking operator of a suitably defined bipartite weighted random graph, which we call a \textit{non-backtracking wedge operator}. When $d$ is above a Kesten-Stigum-type sampling threshold, our algorithm recovers a correlated version of the singular value decomposition of $M$ with quantifiable error bounds. This is the first result in the regime of bounded $d$ for weak recovery and the first for weak consistency when $d\to\infty$ arbitrarily slowly without any polylog factors. As an application, for low-CP-rank orthogonal $k$-tensor completion, we efficiently achieve weak recovery with sample size $O(n^{k/2})$ and weak consistency with sample size $\omega(n^{k/2})$. A similar result is obtained for low-multilinear-rank tensor completion with $O(n^{k/2})$ many samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.02077v5</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ludovic Stephan, Yizhe Zhu</dc:creator>
    </item>
    <item>
      <title>Estimation of Over-parameterized Models from an Auto-Modeling Perspective</title>
      <link>https://arxiv.org/abs/2206.01824</link>
      <description>arXiv:2206.01824v3 Announce Type: replace-cross 
Abstract: From a model-building perspective, we propose a paradigm shift for fitting over-parameterized models. Philosophically, the mindset is to fit models to future observations rather than to the observed sample. Technically, given an imputation method to generate future observations, we fit over-parameterized models to these future observations by optimizing an approximation of the desired expected loss function based on its sample counterpart and an adaptive $\textit{duality function}$. The required imputation method is also developed using the same estimation technique with an adaptive $m$-out-of-$n$ bootstrap approach. We illustrate its applications with the many-normal-means problem, $n &lt; p$ linear regression, and neural network-based image classification of MNIST digits. The numerical results demonstrate its superior performance across these diverse applications. While primarily expository, the paper conducts an in-depth investigation into the theoretical aspects of the topic. It concludes with remarks on some open problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.01824v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Jiang, Chuanhai Liu</dc:creator>
    </item>
    <item>
      <title>Shrinkage Methods for Treatment Choice</title>
      <link>https://arxiv.org/abs/2210.17063</link>
      <description>arXiv:2210.17063v2 Announce Type: replace-cross 
Abstract: This study examines the problem of determining whether to treat individuals based on observed covariates. The most common decision rule is the conditional empirical success (CES) rule proposed by Manski (2004), which assigns individuals to treatments that yield the best experimental outcomes conditional on the observed covariates. Conversely, using shrinkage estimators, which shrink unbiased but noisy preliminary estimates toward the average of these estimates, is a common approach in statistical estimation problems because it is well-known that shrinkage estimators have smaller mean squared errors than unshrunk estimators. Inspired by this idea, we propose a computationally tractable shrinkage rule that selects the shrinkage factor by minimizing the upper bound of the maximum regret. Then, we compare the maximum regret of the proposed shrinkage rule with that of CES and pooling rules when the parameter space is correctly specified or misspecified. Our theoretical results demonstrate that the shrinkage rule performs well in many cases and these findings are further supported by numerical experiments. Specifically, we show that the maximum regret of the shrinkage rule can be strictly smaller than that of the CES and pooling rules in certain cases when the parameter space is correctly specified. In addition, we find that the shrinkage rule is robust against misspecifications of the parameter space. Finally, we apply our method to experimental data from the National Job Training Partnership Act Study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.17063v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takuya Ishihara, Daisuke Kurisu</dc:creator>
    </item>
    <item>
      <title>Straight-Through meets Sparse Recovery: the Support Exploration Algorithm</title>
      <link>https://arxiv.org/abs/2301.13584</link>
      <description>arXiv:2301.13584v2 Announce Type: replace-cross 
Abstract: The {\it straight-through estimator} (STE) is commonly used to optimize quantized neural networks, yet its contexts of effective performance are still unclear despite empirical successes.To make a step forward in this comprehension, we apply STE to a well-understood problem: {\it sparse support recovery}. We introduce the {\it Support Exploration Algorithm} (SEA), a novel algorithm promoting sparsity, and we analyze its performance in support recovery (a.k.a. model selection) problems. SEA explores more supports than the state-of-the-art, leading to superior performance in experiments, especially when the columns of $A$ are strongly coherent.The theoretical analysis considers recovery guarantees when the linear measurements matrix $A$ satisfies the {\it Restricted Isometry Property} (RIP).The sufficient conditions of recovery are comparable but more stringent than those of the state-of-the-art in sparse support recovery. Their significance lies mainly in their applicability to an instance of the STE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.13584v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>ICML 2024, the 41st International Conference on Machine Learning, Jul 2024, Vienna, Austria</arxiv:journal_reference>
      <dc:creator>Mimoun Mohamed (QARMA, I2M), Fran\c{c}ois Malgouyres (IMT), Valentin Emiya (QARMA), Caroline Chaux (IPAL)</dc:creator>
    </item>
    <item>
      <title>Fast sampling from constrained spaces using the Metropolis-adjusted Mirror Langevin algorithm</title>
      <link>https://arxiv.org/abs/2312.08823</link>
      <description>arXiv:2312.08823v3 Announce Type: replace-cross 
Abstract: We propose a new method called the Metropolis-adjusted Mirror Langevin algorithm for approximate sampling from distributions whose support is a compact and convex set. This algorithm adds an accept-reject filter to the Markov chain induced by a single step of the Mirror Langevin algorithm (Zhang et al., 2020), which is a basic discretisation of the Mirror Langevin dynamics. Due to the inclusion of this filter, our method is unbiased relative to the target, while known discretisations of the Mirror Langevin dynamics including the Mirror Langevin algorithm have an asymptotic bias. For this algorithm, we also give upper bounds for the number of iterations taken to mix to a constrained distribution whose potential is relatively smooth, convex, and Lipschitz continuous with respect to a self-concordant mirror function. As a consequence of the reversibility of the Markov chain induced by the inclusion of the Metropolis-Hastings filter, we obtain an exponentially better dependence on the error tolerance for approximate constrained sampling. We also present numerical experiments that corroborate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.08823v3</guid>
      <category>stat.CO</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vishwak Srinivasan, Andre Wibisono, Ashia Wilson</dc:creator>
    </item>
    <item>
      <title>Sharp detection of low-dimensional structure in probability measures via dimensional logarithmic Sobolev inequalities</title>
      <link>https://arxiv.org/abs/2406.13036</link>
      <description>arXiv:2406.13036v2 Announce Type: replace-cross 
Abstract: Identifying low-dimensional structure in high-dimensional probability measures is an essential pre-processing step for efficient sampling. We introduce a method for identifying and approximating a target measure $\pi$ as a perturbation of a given reference measure $\mu$ along a few significant directions of $\mathbb{R}^{d}$. The reference measure can be a Gaussian or a nonlinear transformation of a Gaussian, as commonly arising in generative modeling. Our method extends prior work on minimizing majorizations of the Kullback--Leibler divergence to identify optimal approximations within this class of measures. Our main contribution unveils a connection between the \emph{dimensional} logarithmic Sobolev inequality (LSI) and approximations with this ansatz. Specifically, when the target and reference are both Gaussian, we show that minimizing the dimensional LSI is equivalent to minimizing the KL divergence restricted to this ansatz. For general non-Gaussian measures, the dimensional LSI produces majorants that uniformly improve on previous majorants for gradient-based dimension reduction. We further demonstrate the applicability of this analysis to the squared Hellinger distance, where analogous reasoning shows that the dimensional Poincar\'e inequality offers improved bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13036v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew T. C. Li, Tiangang Cui, Fengyi Li, Youssef Marzouk, Olivier Zahm</dc:creator>
    </item>
  </channel>
</rss>
