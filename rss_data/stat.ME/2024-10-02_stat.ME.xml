<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Oct 2024 02:11:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bayesian Calibration in a multi-output transposition context</title>
      <link>https://arxiv.org/abs/2410.00116</link>
      <description>arXiv:2410.00116v1 Announce Type: new 
Abstract: Bayesian calibration is an effective approach for ensuring that numerical simulations accurately reflect the behavior of physical systems. However, because numerical models are never perfect, a discrepancy known as model error exists between the model outputs and the observed data, and must be quantified. Conventional methods can not be implemented in transposition situations, such as when a model has multiple outputs but only one is experimentally observed. To account for the model error in this context, we propose augmenting the calibration process by introducing additional input numerical parameters through a hierarchical Bayesian model, which includes hyperparameters for the prior distribution of the calibration variables. Importance sampling estimators are used to avoid increasing computational costs. Performance metrics are introduced to assess the proposed probabilistic model and the accuracy of its predictions. The method is applied on a computer code with three outputs that models the Taylor cylinder impact test. The outputs are considered as the observed variables one at a time, to work with three different transposition situations. The proposed method is compared with other approaches that embed model errors to demonstrate the significance of the hierarchical formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00116v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gilles Defaux, C\'edric Durantin, Josselin Garnier, Baptiste Kerleguer, Guillaume Perrin, Charlie Sire</dc:creator>
    </item>
    <item>
      <title>Relative Cumulative Residual Information Measure</title>
      <link>https://arxiv.org/abs/2410.00125</link>
      <description>arXiv:2410.00125v1 Announce Type: new 
Abstract: In this paper, we develop a relative cumulative residual information (RCRI) measure that intends to quantify the divergence between two survival functions. The dynamic relative cumulative residual information (DRCRI) measure is also introduced. We establish some characterization results under the proportional hazards model assumption. Additionally, we obtained the non-parametric estimators of RCRI and DRCRI measures based on the kernel density type estimator for the survival function. The effectiveness of the estimators are assessed through an extensive Monte Carlo simulation study. We consider the data from the third Gaia data release (Gaia DR3) for demonstrating the use of the proposed measure. For this study, we have collected epoch photometry data for the objects Gaia DR3 4111834567779557376 and Gaia DR3 5090605830056251776.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00125v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Mary Andrews, Smitha S, Sudheesh K. Kattumannil</dc:creator>
    </item>
    <item>
      <title>On the posterior property of the Rician distribution</title>
      <link>https://arxiv.org/abs/2410.00142</link>
      <description>arXiv:2410.00142v1 Announce Type: new 
Abstract: The Rician distribution, a well-known statistical distribution frequently encountered in fields like magnetic resonance imaging and wireless communications, is particularly useful for describing many real phenomena such as signal process data. In this paper, we introduce objective Bayesian inference for the Rician distribution parameters, specifically the Jeffreys rule and Jeffreys prior are derived. We proved that the obtained posterior for the first priors led to an improper posterior while the Jeffreys prior led to a proper distribution. To evaluate the effectiveness of our proposed Bayesian estimation method, we perform extensive numerical simulations and compare the results with those obtained from traditional moment-based and maximum likelihood estimators. Our simulations illustrate that the Bayesian estimators derived from the Jeffreys prior provide nearly unbiased estimates, showcasing the advantages of our approach over classical techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00142v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesus Enrique Achire Quispe, Eduardo Ramos, Pedro Luiz Ramos</dc:creator>
    </item>
    <item>
      <title>Generalised mixed effects models for changepoint analysis of biomedical time series data</title>
      <link>https://arxiv.org/abs/2410.00183</link>
      <description>arXiv:2410.00183v1 Announce Type: new 
Abstract: Motivated by two distinct types of biomedical time series data, digital health monitoring and neuroimaging, we develop a novel approach for changepoint analysis that uses a generalised linear mixed model framework. The generalised linear mixed model framework lets us incorporate structure that is usually present in biomedical time series data. We embed the mixed model in a dynamic programming algorithm for detecting multiple changepoints in the fMRI data. We evaluate the performance of our proposed method across several scenarios using simulations. Finally, we show the utility of our proposed method on our two distinct motivating applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00183v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark B. Fiecas, Kathryn R. Cullen, Rebecca Killick</dc:creator>
    </item>
    <item>
      <title>Robust Emax Model Fitting: Addressing Nonignorable Missing Binary Outcome in Dose-Response Analysis</title>
      <link>https://arxiv.org/abs/2410.00259</link>
      <description>arXiv:2410.00259v1 Announce Type: new 
Abstract: The Binary Emax model is widely employed in dose-response analysis during drug development, where missing data often pose significant challenges. Addressing nonignorable missing binary responses, where the likelihood of missing data is related to unobserved outcomes, is particularly important, yet existing methods often lead to biased estimates. This issue is compounded when using the regulatory-recommended imputing as treatment failure approach, known as non-responder imputation. Moreover, the problem of separation, where a predictor perfectly distinguishes between outcome classes, can further complicate likelihood maximization. In this paper, we introduce a penalized likelihood-based method that integrates a modified Expectation-Maximization algorithm in the spirit of Ibrahim and Lipsitz to effectively manage both nonignorable missing data and separation issues. Our approach applies a noninformative Jeffreys prior to the likelihood, reducing bias in parameter estimation. Simulation studies demonstrate that our method outperforms existing methods, such as NRI, and the superiority is further supported by its application to data from a Phase II clinical trial. Additionally, we have developed an R package, ememax, to facilitate the implementation of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00259v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiangshan Zhang, Vivek Pradhan, Yuxi Zhao</dc:creator>
    </item>
    <item>
      <title>Visualization for departures from symmetry with the power-divergence-type measure in two-way contingency tables</title>
      <link>https://arxiv.org/abs/2410.00300</link>
      <description>arXiv:2410.00300v1 Announce Type: new 
Abstract: When the row and column variables consist of the same category in a two-way contingency table, it is specifically called a square contingency table. Since it is clear that the square contingency tables have an association structure, a primary objective is to examine symmetric relationships and transitions between variables. While various models and measures have been proposed to analyze these structures understanding changes between two variables in behavior at two-time points or cohorts, it is also necessary to require a detailed investigation of individual categories and their interrelationships, such as shifts in brand preferences. This paper proposes a novel approach to correspondence analysis (CA) for evaluating departures from symmetry in square contingency tables with nominal categories, using a power-divergence-type measure. The approach ensures that well-known divergences can also be visualized and, regardless of the divergence used, the CA plot consists of two principal axes with equal contribution rates. Additionally, the scaling is independent of sample size, making it well-suited for comparing departures from symmetry across multiple contingency tables. Confidence regions are also constructed to enhance the accuracy of the CA plot.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00300v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wataru Urasaki, Tomoyuki Nakagawa, Jun Tsuchida, Kouji Tahata</dc:creator>
    </item>
    <item>
      <title>The generalized Nelson--Aalen estimator by inverse probability of treatment weighting</title>
      <link>https://arxiv.org/abs/2410.00338</link>
      <description>arXiv:2410.00338v1 Announce Type: new 
Abstract: Inverse probability of treatment weighting (IPTW) has been well applied in causal inference. For time-to-event outcomes, IPTW is performed by weighting the event counting process and at-risk process, resulting in a generalized Nelson--Aalen estimator for population-level hazards. In the presence of competing events, we adopt the counterfactual cumulative incidence of a primary event as the estimated. When the propensity score is estimated, we derive the influence function of the hazard estimator, and then establish the asymptotic property of the incidence estimator. We show that the uncertainty in the estimated propensity score contributes to an additional variation in the IPTW estimator of the cumulative incidence. However, through simulation and real-data application, we find that the additional variation is usually small.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00338v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhao Deng, Rui Wang</dc:creator>
    </item>
    <item>
      <title>Covariate Adjusted Functional Mixed Membership Models</title>
      <link>https://arxiv.org/abs/2410.00370</link>
      <description>arXiv:2410.00370v1 Announce Type: new 
Abstract: Mixed membership models are a flexible class of probabilistic data representations used for unsupervised and semi-supervised learning, allowing each observation to partially belong to multiple clusters or features. In this manuscript, we extend the framework of functional mixed membership models to allow for covariate-dependent adjustments. The proposed model utilizes a multivariate Karhunen-Lo\`eve decomposition, which allows for a scalable and flexible model. Within this framework, we establish a set of sufficient conditions ensuring the identifiability of the mean, covariance, and allocation structure up to a permutation of the labels. This manuscript is primarily motivated by studies on functional brain imaging through electroencephalography (EEG) of children with autism spectrum disorder (ASD). Specifically, we are interested in characterizing the heterogeneity of alpha oscillations for typically developing (TD) children and children with ASD. Since alpha oscillations are known to change as children develop, we aim to characterize the heterogeneity of alpha oscillations conditionally on the age of the child. Using the proposed framework, we were able to gain novel information on the developmental trajectories of alpha oscillations for children with ASD and how the developmental trajectories differ between TD children and children with ASD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00370v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nicholas Marco, Damla \c{S}ent\"urk, Shafali Jeste, Charlotte DiStefano, Abigail Dickinson, Donatello Telesca</dc:creator>
    </item>
    <item>
      <title>Research Frontiers in Ambit Stochastics: In memory of Ole E. Barndorff-Nielsen</title>
      <link>https://arxiv.org/abs/2410.00566</link>
      <description>arXiv:2410.00566v1 Announce Type: new 
Abstract: This article surveys key aspects of ambit stochastics and remembers Ole E. Barndorff-Nielsen's important contributions to the foundation and advancement of this new research field over the last two decades. It also highlights some of the emerging trends in ambit stochastics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00566v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fred Espen Benth, Almut E. D. Veraart</dc:creator>
    </item>
    <item>
      <title>Asymmetric GARCH modelling without moment conditions</title>
      <link>https://arxiv.org/abs/2410.00574</link>
      <description>arXiv:2410.00574v1 Announce Type: new 
Abstract: There is a serious and long-standing restriction in the literature on heavy-tailed phenomena in that moment conditions, which are unrealistic, are almost always assumed in modelling such phenomena. Further, the issue of stability is often insufficiently addressed. To this end, we develop a comprehensive statistical inference for an asymmetric generalized autoregressive conditional heteroskedasticity model with standardized non-Gaussian symmetric stable innovation (sAGARCH) in a unified framework, covering both the stationary case and the explosive case. We consider first the maximum likelihood estimation of the model including the asymptotic properties of the estimator of the stable exponent parameter among others. We then propose a modified Kolmogorov-type test statistic for diagnostic checking, as well as those for strict stationarity and asymmetry testing. We conduct Monte Carlo simulation studies to examine the finite-sample performance of our entire statistical inference procedure. We include empirical examples of stock returns to highlight the usefulness and merits of our sAGARCH model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00574v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxin Tao, Dong Li</dc:creator>
    </item>
    <item>
      <title>Bias in mixed models when analysing longitudinal data subject to irregular observation: when should we worry about it and how can recommended visit intervals help in specifying joint models when needed?</title>
      <link>https://arxiv.org/abs/2410.00662</link>
      <description>arXiv:2410.00662v1 Announce Type: new 
Abstract: In longitudinal studies using routinely collected data, such as electronic health records (EHRs), patients tend to have more measurements when they are unwell; this informative observation pattern may lead to bias. While semi-parametric approaches to modelling longitudinal data subject to irregular observation are known to be sensitive to misspecification of the visit process, parametric models may provide a more robust alternative. Robustness of parametric models on the outcome alone has been assessed under the assumption that the visit intensity is independent of the time since the last visit, given the covariates and random effects. However, this assumption of a memoryless visit process may not be realistic in the context of EHR data. In a special case which includes memory embedded into the visit process, we derive an expression for the bias in parametric models for the outcome alone and use this to identify factors that lead to increasing bias. Using simulation studies, we show that this bias is often small in practice. We suggest diagnostics for identifying the specific cases when the outcome model may be susceptible to meaningful bias, and propose a novel joint model of the outcome and visit processes that can eliminate or reduce the bias. We apply these diagnostics and the joint model to a study of juvenile dermatomyositis. We recommend that future studies using EHR data avoid relying only on the outcome model and instead first evaluate its appropriateness with our proposed diagnostics, applying our proposed joint model if necessary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00662v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rose Garrett, Brian Feldman, Eleanor Pullenayegum</dc:creator>
    </item>
    <item>
      <title>Modeling Neural Switching via Drift-Diffusion Models</title>
      <link>https://arxiv.org/abs/2410.00781</link>
      <description>arXiv:2410.00781v1 Announce Type: new 
Abstract: Neural encoding, or neural representation, is a field in neuroscience that focuses on characterizing how information is encoded in the spiking activity of neurons. Currently, little is known about how sensory neurons can preserve information from multiple stimuli given their broad receptive fields. Multiplexing is a neural encoding theory that posits that neurons temporally switch between encoding various stimuli in their receptive field. Here, we construct a statistically falsifiable single-neuron model for multiplexing using a competition-based framework. The spike train models are constructed using drift-diffusion models, implying an integrate-and-fire framework to model the temporal dynamics of the membrane potential of the neuron. In addition to a multiplexing-specific model, we develop alternative models that represent alternative encoding theories (normalization, winner-take-all, subadditivity, etc.) with some level of abstraction. Using information criteria, we perform model comparison to determine whether the data favor multiplexing over alternative theories of neural encoding. Analysis of spike trains from the inferior colliculus of two macaque monkeys provides tenable evidence of multiplexing and offers new insight into the timescales at which switching occurs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00781v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nicholas Marco, Jennifer M. Groh, Surya T. Tokdar</dc:creator>
    </item>
    <item>
      <title>Control Variate-based Stochastic Sampling from the Probability Simplex</title>
      <link>https://arxiv.org/abs/2410.00845</link>
      <description>arXiv:2410.00845v1 Announce Type: new 
Abstract: This paper presents a control variate-based Markov chain Monte Carlo algorithm for efficient sampling from the probability simplex, with a focus on applications in large-scale Bayesian models such as latent Dirichlet allocation. Standard Markov chain Monte Carlo methods, particularly those based on Langevin diffusions, suffer from significant discretization errors near the boundaries of the simplex, which are exacerbated in sparse data settings. To address this issue, we propose an improved approach based on the stochastic Cox--Ingersoll--Ross process, which eliminates discretization errors and enables exact transition densities. Our key contribution is the integration of control variates, which significantly reduces the variance of the stochastic gradient estimator in the Cox--Ingersoll--Ross process, thereby enhancing the accuracy and computational efficiency of the algorithm. We provide a theoretical analysis showing the variance reduction achieved by the control variates approach and demonstrate the practical advantages of our method in data subsampling settings. Empirical results on large datasets show that the proposed method outperforms existing approaches in both accuracy and scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00845v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco Barile, Christopher Nemeth</dc:creator>
    </item>
    <item>
      <title>Network Science in Psychology</title>
      <link>https://arxiv.org/abs/2410.00301</link>
      <description>arXiv:2410.00301v1 Announce Type: cross 
Abstract: Social network analysis can answer research questions such as why or how individuals interact or form relationships and how those relationships impact other outcomes. Despite the breadth of methods available to address psychological research questions, social network analysis is not yet a standard practice in psychological research. To promote the use of social network analysis in psychological research, we present an overview of network methods, situating each method within the context of research studies and questions in psychology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00301v1</guid>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tracy Sweet, Selena Wang</dc:creator>
    </item>
    <item>
      <title>How should we aggregate ratings? Accounting for personal rating scales via Wasserstein barycenters</title>
      <link>https://arxiv.org/abs/2410.00865</link>
      <description>arXiv:2410.00865v1 Announce Type: cross 
Abstract: A common method of making quantitative conclusions in qualitative situations is to collect numerical ratings on a linear scale. We investigate the problem of calculating aggregate numerical ratings from individual numerical ratings and propose a new, non-parametric model for the problem. We show that, with minimal modeling assumptions, the equal-weights average is inconsistent for estimating the quality of items. Analyzing the problem from the perspective of optimal transport, we derive an alternative rating estimator, which we show is asymptotically consistent almost surely and in $L^p$ for estimating quality, with an optimal rate of convergence. Further, we generalize Kendall's W, a non-parametric coefficient of preference concordance between raters, from the special case of rankings to the more general case of arbitrary numerical ratings. Along the way, we prove Glivenko--Cantelli-type theorems for uniform convergence of the cumulative distribution functions and quantile functions for Wasserstein-2 Fr\'echet means on [0,1].</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00865v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Raban</dc:creator>
    </item>
    <item>
      <title>Identification enhanced generalised linear model estimation with nonignorable missing outcomes</title>
      <link>https://arxiv.org/abs/2204.10508</link>
      <description>arXiv:2204.10508v3 Announce Type: replace 
Abstract: Missing data often result in undesirable bias and loss of efficiency. These become substantial problems when the response mechanism is nonignorable, such that the response model depends on unobserved variables. It is necessary to estimate the joint distribution of unobserved variables and response indicators to manage nonignorable nonresponse. However, model misspecification and identification issues prevent robust estimates despite careful estimation of the target joint distribution. In this study, we modelled the distribution of the observed parts and derived sufficient conditions for model identifiability, assuming a logistic regression model as the response mechanism and generalised linear models as the main outcome model of interest. More importantly, the derived sufficient conditions are testable with the observed data and do not require any instrumental variables, which are often assumed to guarantee model identifiability but cannot be practically determined beforehand. To analyse missing data, we propose a new imputation method which incorporates verifiable identifiability using only observed data. Furthermore, we present the performance of the proposed estimators in numerical studies and apply the proposed method to two sets of real data: exit polls for the 19th South Korean election data and public data collected from the Korean Survey of Household Finances and Living Conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.10508v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenji Beppu, Jinung Choi, Kosuke Morikawa, Jongho Im</dc:creator>
    </item>
    <item>
      <title>Asymptotically Optimal Knockoff Statistics via the Masked Likelihood Ratio</title>
      <link>https://arxiv.org/abs/2212.08766</link>
      <description>arXiv:2212.08766v2 Announce Type: replace 
Abstract: In feature selection problems, knockoffs are synthetic controls for the original features. Employing knockoffs allows analysts to use nearly any variable importance measure or "feature statistic" to select features while rigorously controlling false positives. However, it is not clear which statistic maximizes power. In this paper, we argue that state-of-the-art lasso-based feature statistics often prioritize features that are unlikely to be discovered, leading to low power in real applications. Instead, we introduce masked likelihood ratio (MLR) statistics, which prioritize features according to one's ability to distinguish each feature from its knockoff. Although no single feature statistic is uniformly most powerful in all situations, we show that MLR statistics asymptotically maximize the number of discoveries under a user-specified Bayesian model of the data. (Like all feature statistics, MLR statistics always provide frequentist error control.) This result places no restrictions on the problem dimensions and makes no parametric assumptions; instead, we require a "local dependence" condition that depends only on known quantities. In simulations and three real applications, MLR statistics outperform state-of-the-art feature statistics, including in settings where the Bayesian model is misspecified. We implement MLR statistics in the python package knockpy; our implementation is often faster than computing a cross-validated lasso.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.08766v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Asher Spector, William Fithian</dc:creator>
    </item>
    <item>
      <title>A Non-Parametric Approach to Detect Patterns in Binary Sequences</title>
      <link>https://arxiv.org/abs/2306.15629</link>
      <description>arXiv:2306.15629v3 Announce Type: replace 
Abstract: In many circumstances, given an ordered sequence of one or more types of elements or symbols, the objective is to determine the existence of any randomness in the occurrence of one specific element, say type 1. This method can help detect non-random patterns, such as wins or losses in a series of games. Existing methods of tests based on total number of runs or tests based on length of longest run (Mosteller (1941)) can be used for testing the null hypothesis of randomness in the entire sequence, and not a specific type of element. Moreover, the Runs Test often yields results that contradict the patterns visualized in graphs showing, for instance, win proportions over time. This paper develops a test approach to address this problem by computing the gaps between two consecutive type 1 elements, by identifying patterns in occurrence and directional trends (increasing, decreasing, or constant), applies the exact Binomial test, Kendall's Tau, and the Siegel-Tukey test for scale problems. Further modifications suggested by Jan Vegelius(1982) have been applied in the Siegel Tukey test to adjust for tied ranks and achieve more accurate results. This approach is distribution-free and suitable for small sample sizes. Also comparisons with the conventional runs test demonstrates the superiority of the proposed method under the null hypothesis of randomness in the occurrence of type 1 elements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15629v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anushka De</dc:creator>
    </item>
    <item>
      <title>Conformal prediction with local weights: randomization enables local guarantees</title>
      <link>https://arxiv.org/abs/2310.07850</link>
      <description>arXiv:2310.07850v4 Announce Type: replace 
Abstract: In this work, we consider the problem of building distribution-free prediction intervals with finite-sample conditional coverage guarantees. Conformal prediction (CP) is an increasingly popular framework for building prediction intervals with distribution-free guarantees, but these guarantees only ensure marginal coverage: the probability of coverage is averaged over a random draw of both the training and test data, meaning that there might be substantial undercoverage within certain subpopulations. Instead, ideally, we would want to have local coverage guarantees that hold for each possible value of the test point's features. While the impossibility of achieving pointwise local coverage is well established in the literature, many variants of conformal prediction algorithm show favorable local coverage properties empirically. Relaxing the definition of local coverage can allow for a theoretical understanding of this empirical phenomenon. We aim to bridge this gap between theoretical validation and empirical performance by proving achievable and interpretable guarantees for a relaxed notion of local coverage. Building on the localized CP method of Guan (2023) and the weighted CP framework of Tibshirani et al. (2019), we propose a new method, randomly-localized conformal prediction (RLCP), which returns prediction intervals that are not only marginally valid but also achieve a relaxed local coverage guarantee and guarantees under covariate shift. Through a series of simulations and real data experiments, we validate these coverage guarantees of RLCP while comparing it with the other local conformal prediction methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07850v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rohan Hore, Rina Foygel Barber</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bayesian Adjustment of Unmeasured Confounders in Cox Proportional Hazards Models</title>
      <link>https://arxiv.org/abs/2312.02404</link>
      <description>arXiv:2312.02404v4 Announce Type: replace 
Abstract: In observational studies, unmeasured confounders present a crucial challenge in accurately estimating desired causal effects. To calculate the hazard ratio (HR) in Cox proportional hazard models for time-to-event outcomes, two-stage residual inclusion and limited information maximum likelihood are typically employed. However, these methods are known to entail difficulty in terms of potential bias of HR estimates and parameter identification. This study introduces a novel nonparametric Bayesian method designed to estimate an unbiased HR, addressing concerns that previous research methods have had. Our proposed method consists of two phases: 1) detecting clusters based on the likelihood of the exposure and outcome variables, and 2) estimating the hazard ratio within each cluster. Although it is implicitly assumed that unmeasured confounders affect outcomes through cluster effects, our algorithm is well-suited for such data structures. The proposed Bayesian estimator has good performance compared with some competitors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02404v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunichiro Orihara, Shonosuke Sugasawa, Tomohiro Ohigashi, Tomoyuki Nakagawa, Masataka Taguri</dc:creator>
    </item>
    <item>
      <title>Exploring Spatial Context: A Comprehensive Bibliography of GWR and MGWR</title>
      <link>https://arxiv.org/abs/2404.16209</link>
      <description>arXiv:2404.16209v2 Announce Type: replace 
Abstract: Local spatial models such as Geographically Weighted Regression (GWR) and Multiscale Geographically Weighted Regression (MGWR) serve as instrumental tools to capture intrinsic contextual effects through the estimates of the local intercepts and behavioral contextual effects through estimates of the local slope parameters. GWR and MGWR provide simple implementation yet powerful frameworks that could be extended to various disciplines that handle spatial data. This bibliography aims to serve as a comprehensive compilation of peer-reviewed papers that have utilized GWR or MGWR as a primary analytical method to conduct spatial analyses and acts as a useful guide to anyone searching the literature for previous examples of local statistical modeling in a wide variety of application fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16209v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Stewart Fotheringham, Chen-Lun Kao, Hanchen Yu, Sarah Bardin, Taylor Oshan, Ziqi Li, Mehak Sachdeva, Wei Luo</dc:creator>
    </item>
    <item>
      <title>Nonparametric causal inference for optogenetics: sequential excursion effects for dynamic regimes</title>
      <link>https://arxiv.org/abs/2405.18597</link>
      <description>arXiv:2405.18597v2 Announce Type: replace 
Abstract: Optogenetics is a powerful neuroscience technique for studying how neural circuit manipulation affects behavior. Standard analysis conventions discard information and severely limit the scope of the causal questions that can be probed. To address this gap, we 1) draw connections to the causal inference literature on sequentially randomized experiments, 2) propose a non-parametric framework for analyzing "open-loop" (static regime) optogenetics behavioral experiments, 3) derive extensions of history-restricted marginal structural models for dynamic treatment regimes with positivity violations for "closed-loop" designs, and 4) propose a taxonomy of identifiable causal effects that encompass a far richer collection of scientific questions compared to standard methods. From another view, our work extends "excursion effect" methods, popularized recently in the mobile health literature, to enable estimation of causal contrasts for treatment sequences in the presence of positivity violations. We describe sufficient conditions for identifiability of the proposed causal estimands, and provide asymptotic statistical guarantees for a proposed inverse probability-weighted estimator, a multiply-robust estimator (for two intervention timepoints), a framework for hypothesis testing, and a computationally scalable implementation. Finally, we apply our framework to data from a recent neuroscience study and show how it provides insight into causal effects of optogenetics on behavior that are obscured by standard analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18597v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Loewinger, Alexander W. Levis, Francisco Pereira</dc:creator>
    </item>
    <item>
      <title>When can weak latent factors be statistically inferred?</title>
      <link>https://arxiv.org/abs/2407.03616</link>
      <description>arXiv:2407.03616v3 Announce Type: replace 
Abstract: This article establishes a new and comprehensive estimation and inference theory for principal component analysis (PCA) under the weak factor model that allow for cross-sectional dependent idiosyncratic components under the nearly minimal factor strength relative to the noise level or signal-to-noise ratio. Our theory is applicable regardless of the relative growth rate between the cross-sectional dimension $N$ and temporal dimension $T$. This more realistic assumption and noticeable result require completely new technical device, as the commonly-used leave-one-out trick is no longer applicable to the case with cross-sectional dependence. Another notable advancement of our theory is on PCA inference $ - $ for example, under the regime where $N\asymp T$, we show that the asymptotic normality for the PCA-based estimator holds as long as the signal-to-noise ratio (SNR) grows faster than a polynomial rate of $\log N$. This finding significantly surpasses prior work that required a polynomial rate of $N$. Our theory is entirely non-asymptotic, offering finite-sample characterizations for both the estimation error and the uncertainty level of statistical inference. A notable technical innovation is our closed-form first-order approximation of PCA-based estimator, which paves the way for various statistical tests. Furthermore, we apply our theories to design easy-to-implement statistics for validating whether given factors fall in the linear spans of unknown latent factors, testing structural breaks in the factor loadings for an individual unit, checking whether two units have the same risk exposures, and constructing confidence intervals for systematic risks. Our empirical studies uncover insightful correlations between our test results and economic cycles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03616v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianqing Fan, Yuling Yan, Yuheng Zheng</dc:creator>
    </item>
    <item>
      <title>Improve Sensitivity Analysis Synthesizing Randomized Clinical Trials With Limited Overlap</title>
      <link>https://arxiv.org/abs/2409.07391</link>
      <description>arXiv:2409.07391v2 Announce Type: replace 
Abstract: To estimate the average treatment effect in real-world populations, observational studies are typically designed around real-world cohorts. However, even when study samples from these designs represent the population, unmeasured confounders can introduce bias. Sensitivity analysis is often used to estimate bounds for the average treatment effect without relying on the strict mathematical assumptions of other existing methods. This article introduces a new approach that improves sensitivity analysis in observational studies by incorporating randomized clinical trial data, even with limited overlap due to inclusion/exclusion criteria. Theoretical proof and simulations show that this method provides a tighter bound width than existing approaches. We also apply this method to both a trial dataset and a real-world drug effectiveness comparison dataset for practical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07391v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kuan Jiang, Wenjie Hu, Shu Yang, Xinxing Lai, Xiaohua Zhou</dc:creator>
    </item>
    <item>
      <title>Validation and Comparison of Non-Stationary Cognitive Models: A Diffusion Model Application</title>
      <link>https://arxiv.org/abs/2401.08626</link>
      <description>arXiv:2401.08626v3 Announce Type: replace-cross 
Abstract: Cognitive processes undergo various fluctuations and transient states across different temporal scales. Superstatistics are emerging as a flexible framework for incorporating such non-stationary dynamics into existing cognitive model classes. In this work, we provide the first experimental validation of superstatistics and formal comparison of four non-stationary diffusion decision models in a specifically designed perceptual decision-making task. Task difficulty and speed-accuracy trade-off were systematically manipulated to induce expected changes in model parameters. To validate our models, we assess whether the inferred parameter trajectories align with the patterns and sequences of the experimental manipulations. To address computational challenges, we present novel deep learning techniques for amortized Bayesian estimation and comparison of models with time-varying parameters. Our findings indicate that transition models incorporating both gradual and abrupt parameter shifts provide the best fit to the empirical data. Moreover, we find that the inferred parameter trajectories closely mirror the sequence of experimental manipulations. Posterior re-simulations further underscore the ability of the models to faithfully reproduce critical data patterns. Accordingly, our results suggest that the inferred non-stationary dynamics may reflect actual changes in the targeted psychological constructs. We argue that our initial experimental validation paves the way for the widespread application of superstatistics in cognitive modeling and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08626v3</guid>
      <category>q-bio.NC</category>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lukas Schumacher, Martin Schnuerch, Andreas Voss, Stefan T. Radev</dc:creator>
    </item>
    <item>
      <title>Extending the blended generalized extreme value distribution</title>
      <link>https://arxiv.org/abs/2407.06875</link>
      <description>arXiv:2407.06875v3 Announce Type: replace-cross 
Abstract: The generalized extreme value (GEV) distribution is commonly employed to help estimate the likelihood of extreme events in many geophysical and other application areas. The recently proposed blended generalized extreme value (bGEV) distribution modifies the GEV with positive shape parameter to avoid a hard lower bound that complicates fitting and inference. Here, the bGEV is extended to the GEV with negative shape parameter, avoiding a hard upper bound that is unrealistic in many applications. This extended bGEV is shown to improve on the GEV for forecasting heat and sea level extremes based on past data. Software implementing this bGEV and applying it to the example temperature and sea level data is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06875v3</guid>
      <category>stat.AP</category>
      <category>physics.ao-ph</category>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nir Y. Krakauer</dc:creator>
    </item>
    <item>
      <title>Optimizing the Induced Correlation in Omnibus Joint Graph Embeddings</title>
      <link>https://arxiv.org/abs/2409.17544</link>
      <description>arXiv:2409.17544v2 Announce Type: replace-cross 
Abstract: Theoretical and empirical evidence suggests that joint graph embedding algorithms induce correlation across the networks in the embedding space. In the Omnibus joint graph embedding framework, previous results explicitly delineated the dual effects of the algorithm-induced and model-inherent correlations on the correlation across the embedded networks. Accounting for and mitigating the algorithm-induced correlation is key to subsequent inference, as sub-optimal Omnibus matrix constructions have been demonstrated to lead to loss in inference fidelity. This work presents the first efforts to automate the Omnibus construction in order to address two key questions in this joint embedding framework: the correlation-to-OMNI problem and the flat correlation problem. In the flat correlation problem, we seek to understand the minimum algorithm-induced flat correlation (i.e., the same across all graph pairs) produced by a generalized Omnibus embedding. Working in a subspace of the fully general Omnibus matrices, we prove both a lower bound for this flat correlation and that the classical Omnibus construction induces the maximal flat correlation. In the correlation-to-OMNI problem, we present an algorithm -- named corr2Omni -- that, from a given matrix of estimated pairwise graph correlations, estimates the matrix of generalized Omnibus weights that induces optimal correlation in the embedding space. Moreover, in both simulated and real data settings, we demonstrate the increased effectiveness of our corr2Omni algorithm versus the classical Omnibus construction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17544v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Konstantinos Pantazis, Michael Trosset, William N. Frost, Carey E. Priebe, Vince Lyzinski</dc:creator>
    </item>
  </channel>
</rss>
