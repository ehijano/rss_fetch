<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Sep 2025 04:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Optimal streaming algorithm for detecting $\ell_2$ heavy hitters in random order streams</title>
      <link>https://arxiv.org/abs/2509.07286</link>
      <description>arXiv:2509.07286v1 Announce Type: new 
Abstract: Given a stream $x_1,x_2,\dots,x_n$ of items from a Universe $U$ of size $\mathsf{poly}(n)$, and a parameter $\epsilon&gt;0$, an item $i\in U$ is said to be an $\ell_2$ heavy hitter if its frequency $f_i$ in the stream is at least $\sqrt{\epsilon F_2}$, where $F_2=\sqrt{\sum_{i\in U} f_i^2}$. The classical $\mathsf{CountSketch}$ algorithm due to Charikar, Chen, and Farach-Colton [2004], was the first algorithm to detect $\ell_2$ heavy hitters using $O\left(\frac{\log^2 n}{\epsilon}\right)$ bits of space, and their algorithm is optimal for streams with deletions. For insertion-only streams, Braverman, Chestnut, Ivkin, Nelson, Wang, and Woodruff [2017] gave the $\mathsf{BPTree}$ algorithm which requires only $O\left(\frac{\log(1/\epsilon)}{\epsilon}\log n \right)$ space. Note that any algorithm requires at least $O\left(\frac{1}{\epsilon} \log n\right)$ space to output $O(1/\epsilon)$ heavy hitters in the worst case. So for constant $\epsilon$, the space usage of the $\mathsf{BPTree}$ algorithm is optimal but their bound could be sub-optimal for $\epsilon=o(1)$. In this work, we show that for random order streams, where the stream elements can be adversarial but their order of arrival is uniformly random, it is possible to achieve the optimal space bound of $O\left(\frac{1}{\epsilon} \log n\right)$ for every $\epsilon = \Omega\left(\frac{1}{2^{\sqrt{\log n}}}\right)$. We also show that for partially random order streams where only the heavy hitters are required to be uniformly distributed in the stream, it is possible to achieve the same space bound, but with an additional assumption that the algorithm is given a constant approximation to $F_2$ in advance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07286v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Santhoshini Velusamy, Huacheng Yu</dc:creator>
    </item>
    <item>
      <title>Dimension Reduction for Clustering: The Curious Case of Discrete Centers</title>
      <link>https://arxiv.org/abs/2509.07444</link>
      <description>arXiv:2509.07444v1 Announce Type: new 
Abstract: The Johnson-Lindenstrauss transform is a fundamental method for dimension reduction in Euclidean spaces, that can map any dataset of $n$ points into dimension $O(\log n)$ with low distortion of their distances. This dimension bound is tight in general, but one can bypass it for specific problems. Indeed, tremendous progress has been made for clustering problems, especially in the \emph{continuous} setting where centers can be picked from the ambient space $\mathbb{R}^d$. Most notably, for $k$-median and $k$-means, the dimension bound was improved to $O(\log k)$ [Makarychev, Makarychev and Razenshteyn, STOC 2019].
  We explore dimension reduction for clustering in the \emph{discrete} setting, where centers can only be picked from the dataset, and present two results that are both parameterized by the doubling dimension of the dataset, denoted as $\operatorname{ddim}$. The first result shows that dimension $O_{\epsilon}(\operatorname{ddim} + \log k + \log\log n)$ suffices, and is moreover tight, to guarantee that the cost is preserved within factor $1\pm\epsilon$ for every set of centers. Our second result eliminates the $\log\log n$ term in the dimension through a relaxation of the guarantee (namely, preserving the cost only for all approximately-optimal sets of centers), which maintains its usefulness for downstream applications.
  Overall, we achieve strong dimension reduction in the discrete setting, and find that it differs from the continuous setting not only in the dimension bound, which depends on the doubling dimension, but also in the guarantees beyond preserving the optimal value, such as which clusterings are preserved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07444v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaofeng H. -C. Jiang, Robert Krauthgamer, Shay Sapir, Sandeep Silwal, Di Yue</dc:creator>
    </item>
    <item>
      <title>The General Expiration Streaming Model: Diameter, $k$-Center, Counting, Sampling, and Friends</title>
      <link>https://arxiv.org/abs/2509.07587</link>
      <description>arXiv:2509.07587v1 Announce Type: new 
Abstract: An important thread in the study of data-stream algorithms focuses on settings where stream items are active only for a limited time. We introduce a new expiration model, where each item arrives with its own expiration time. The special case where items expire in the order that they arrive, which we call consistent expirations, contains the classical sliding-window model of Datar, Gionis, Indyk, and Motwani [SICOMP 2002] and its timestamp-based variant of Braverman and Ostrovsky [FOCS 2007].
  Our first set of results presents algorithms (in the expiration streaming model) for several fundamental problems, including approximate counting, uniform sampling, and weighted sampling by efficiently tracking active items without explicitly storing them all. Naturally, these algorithms have many immediate applications to other problems.
  Our second and main set of results designs algorithms (in the expiration streaming model) for the diameter and $k$-center problems, where items are points in a metric space. Our results significantly extend those known for the special case of sliding-window streams by Cohen-Addad, Schwiegelshohn, and Sohler [ICALP 2016], including also a strictly better approximation factor for the diameter in the important special case of high-dimensional Euclidean space. We develop new decomposition and coordination techniques along with a geometric dominance framework, to filter out redundant points based on both temporal and spatial proximity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07587v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lotte Blank, Sergio Cabello, MohammadTaghi Hajiaghayi, Robert Krauthgamer, Sepideh Mahabadi, Andr\'e Nusser, Jeff M. Phillips, Jonas Sauer</dc:creator>
    </item>
    <item>
      <title>Tight Bounds for Low-Error Frequency Moment Estimation and the Power of Multiple Passes</title>
      <link>https://arxiv.org/abs/2509.07599</link>
      <description>arXiv:2509.07599v1 Announce Type: new 
Abstract: Estimating the second frequency moment $F_2$ of a data stream up to a $(1 \pm \varepsilon)$ factor is a central problem in the streaming literature. For errors $\varepsilon &gt; \Omega(1/\sqrt{n})$, the tight bound $\Theta\left(\log(\varepsilon^2 n)/\varepsilon^2\right)$ was recently established by Braverman and Zamir. In this work, we complete the picture by resolving the remaining regime of small error, $\varepsilon &lt; 1/\sqrt{n}$, showing that the optimal space complexity is $\Theta\left( \min\left(n, \frac{1}{\varepsilon^2} \right) \cdot \left(1 + \left| \log(\varepsilon^2 n) \right| \right) \right)$ bits for all $\varepsilon \geq 1/n^2$, assuming a sufficiently large universe. This closes the gap between the best known $\Omega(n)$ lower bound and the straightforward $O(n \log n)$ upper bound in that range, and shows that essentially storing the entire stream is necessary for high-precision estimation.
  To derive this bound, we fully characterize the two-party communication complexity of estimating the size of a set intersection up to an arbitrary additive error $\varepsilon n$. In particular, we prove a tight $\Omega(n \log n)$ lower bound for one-way communication protocols when $\varepsilon &lt; n^{-1/2-\Omega(1)}$, in contrast to classical $O(n)$-bit protocols that use two-way communication. Motivated by this separation, we present a two-pass streaming algorithm that computes the exact histogram of a stream with high probability using only $O(n \log \log n)$ bits of space, in contrast to the $\Theta(n \log n)$ bits required in one pass even to approximate $F_2$ with small error. This yields the first asymptotic separation between one-pass and $O(1)$-passes space complexity for small frequency moment estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07599v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Naomi Green-Maimon, Or Zamir</dc:creator>
    </item>
    <item>
      <title>Proximity Graphs for Similarity Search: Fast Construction, Lower Bounds, and Euclidean Separation</title>
      <link>https://arxiv.org/abs/2509.07732</link>
      <description>arXiv:2509.07732v1 Announce Type: new 
Abstract: Proximity graph-based methods have emerged as a leading paradigm for approximate nearest neighbor (ANN) search in the system community. This paper presents fresh insights into the theoretical foundation of these methods. We describe an algorithm to build a proximity graph for $(1+\epsilon)$-ANN search that has $O((1/\epsilon)^\lambda \cdot n \log \Delta)$ edges and guarantees $(1/\epsilon)^\lambda \cdot \text{polylog }\Delta$ query time. Here, $n$ and $\Delta$ are the size and aspect ratio of the data input, respectively, and $\lambda = O(1)$ is the doubling dimension of the underlying metric space. Our construction time is near-linear to $n$, improving the $\Omega(n^2)$ bounds of all previous constructions. We complement our algorithm with lower bounds revealing an inherent limitation of proximity graphs: the number of edges needs to be at least $\Omega((1/\epsilon)^\lambda \cdot n + n \log \Delta)$ in the worst case, up to a subpolynomial factor. The hard inputs used in our lower-bound arguments are non-geometric, thus prompting the question of whether improvement is possible in the Euclidean space (a key subclass of metric spaces). We provide an affirmative answer by using geometry to reduce the graph size to $O((1/\epsilon)^\lambda \cdot n)$ while preserving nearly the same query and construction time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07732v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shangqi Lu, Yufei Tao</dc:creator>
    </item>
    <item>
      <title>Compressibility Measures and Succinct Data Structures for Piecewise Linear Approximations</title>
      <link>https://arxiv.org/abs/2509.07827</link>
      <description>arXiv:2509.07827v1 Announce Type: new 
Abstract: We study the problem of deriving compressibility measures for \emph{Piecewise Linear Approximations} (PLAs), i.e., error-bounded approximations of a set of two-dimensional {\em increasing} data points using a sequence of segments. Such approximations are widely used tools in implementing many \emph{learned data structures}, which mix learning models with traditional algorithmic design blocks to exploit regularities in the underlying data distribution, providing novel and effective space-time trade-offs.
  We introduce the first lower bounds to the cost of storing PLAs in two settings, namely {\em compression} and {\em indexing}. We then compare these compressibility measures to known data structures, and show that they are asymptotically optimal up to a constant factor from the space lower bounds. Finally, we design the first data structures for the aforementioned settings that achieve the space lower bounds plus small additive terms, which turn out to be {\em succinct} in most practical cases. Our data structures support the efficient retrieval and evaluation of a segment in the (compressed) PLA for a given $x$-value, which is a core operation in any learned data structure relying on PLAs.
  As a result, our paper offers the first theoretical analysis of the maximum compressibility achievable by PLA-based learned data structures, and provides novel storage schemes for PLAs offering strong theoretical guarantees while also suggesting simple and efficient practical implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07827v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paolo Ferragina, Filippo Lari</dc:creator>
    </item>
    <item>
      <title>Quantum algorithms for general nonlinear dynamics based on the Carleman embedding</title>
      <link>https://arxiv.org/abs/2509.07155</link>
      <description>arXiv:2509.07155v1 Announce Type: cross 
Abstract: Important nonlinear dynamics, such as those found in plasma and fluid systems, are typically hard to simulate on classical computers. Thus, if fault-tolerant quantum computers could efficiently solve such nonlinear problems, it would be a transformative change for many industries. In a recent breakthrough [Liu et al., PNAS 2021], the first efficient quantum algorithm for solving nonlinear differential equations was constructed, based on a single condition $R&lt;1$, where $R$ characterizes the ratio of nonlinearity to dissipation. This result, however, is limited to the class of purely dissipative systems with negative log-norm, which excludes application to many important problems. In this work, we correct technical issues with this and other prior analysis, and substantially extend the scope of nonlinear dynamical systems that can be efficiently simulated on a quantum computer in a number of ways. Firstly, we extend the existing results from purely dissipative systems to a much broader class of stable systems, and show that every quadratic Lyapunov function for the linearized system corresponds to an independent $R$-number criterion for the convergence of the Carlemen scheme. Secondly, we extend our stable system results to physically relevant settings where conserved polynomial quantities exist. Finally, we provide extensive results for the class of non-resonant systems. With this, we are able to show that efficient quantum algorithms exist for a much wider class of nonlinear systems than previously known, and prove the BQP-completeness of nonlinear oscillator problems of exponential size. In our analysis, we also obtain several results related to the Poincar\'{e}-Dulac theorem and diagonalization of the Carleman matrix, which could be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07155v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Jennings, Kamil Korzekwa, Matteo Lostaglio, Andrew T Sornborger, Yigit Subasi, Guoming Wang</dc:creator>
    </item>
    <item>
      <title>Discrete Effort Distribution via Regret-enabled Greedy Algorithm</title>
      <link>https://arxiv.org/abs/2503.11107</link>
      <description>arXiv:2503.11107v3 Announce Type: replace 
Abstract: This paper addresses resource allocation problem with a separable objective function under a single linear constraint, formulated as maximizing $\sum_{j=1}^{n}R_j(x_j)$ subject to $\sum_{j=1}^{n}x_j=k$ and $x_j\in\{0,\dots,m\}$. While classical dynamic programming approach solves this problem in $O(n^2m^2)$ time, we propose a regret-enabled greedy algorithm that achieves $O(n\log n)$ time when $m=O(1)$. The algorithm significantly outperforms traditional dynamic programming for small $m$. Our algorithm actually solves the problem for all $k~(0\leq k\leq nm)$ in the mentioned time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11107v3</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Song Cao, Taikun Zhu, Kai Jin</dc:creator>
    </item>
    <item>
      <title>Smaller and More Flexible Cuckoo Filters</title>
      <link>https://arxiv.org/abs/2505.05847</link>
      <description>arXiv:2505.05847v3 Announce Type: replace 
Abstract: Cuckoo filters are space-efficient approximate set membership data structures with a controllable false positive rate (FPR) and zero false negatives, similar to Bloom filters. In contrast to Bloom filters, Cuckoo filters store multi-bit fingerprints of keys in a hash table using variants of Cuckoo hashing, allowing each fingerprint to be stored at a small number of possible locations. Existing Cuckoo filters use fingerprints of $(k+3)$ bits per key and an additional space overhead factor of at least $1.05$ to achieve an FPR of $2^{-k}$. For $k=10$, this amounts to $1.365\, kn$ bits to store $n$ keys, which is better than $1.443\, kn$ bits for Bloom filters. The $+3$ for the fingerprint size is required to balance out the multiplied FPR caused by looking for the fingerprint at several locations. In the original Cuckoo filter, the number of hash table buckets is restricted to a power of 2, which may lead to much larger space overheads, up to $2.1\, (1+3/k)\, kn$ bits.
  We present two improvements of Cuckoo filters. First, we remove the restriction that the number of buckets must be a power of 2 by using a different placement strategy. Second, we reduce the space overhead factor of Cuckoo filters to $1.06 \, (1+2/k)$ by using overlapping windows instead of disjoint buckets to maintain the load threshold of the hash table, while reducing the number of alternative slots where any fingerprint may be found.
  A detailed evaluation demonstrates that the alternative memory layout based on overlapping windows decreases the size of Cuckoo filters not only in theory, but also in practice. A comparison with other state-of-the art filter types, Prefix filters and Vector Quotient filters (VQFs), shows that the reduced space overhead makes windowed Cuckoo filters the smallest filters supporting online insertions, with similarly fast queries, but longer insertion times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05847v3</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johanna Elena Schmitz, Jens Zentgraf, Sven Rahmann</dc:creator>
    </item>
    <item>
      <title>Computing in a Faulty Congested Clique</title>
      <link>https://arxiv.org/abs/2505.11430</link>
      <description>arXiv:2505.11430v2 Announce Type: replace 
Abstract: We study a Faulty Congested Clique model, in which an adversary may fail nodes in the network throughout the computation. We show that any task of $O(n\log{n})$-bit input per node can be solved in roughly $n$ rounds, where $n$ is the size of the network. This nearly matches the linear upper bound on the complexity of the non-faulty Congested Clique model for such problems, by learning the entire input, and it holds in the faulty model even with a linear number of faults.
  Our main contribution is that we establish that one can do much better by looking more closely at the computation. Given a deterministic algorithm $\mathcal{A}$ for the non-faulty Congested Clique model, we show how to transform it into an algorithm $\mathcal{A}'$ for the faulty model, with an overhead that could be as small as some logarithmic-in-$n$ factor, by considering refined complexity measures of $\mathcal{A}$.
  As an exemplifying application of our approach, we show that the $O(n^{1/3})$-round complexity of semi-ring matrix multiplication [Censor-Hillel, Kaski, Korhonen, Lenzen, Paz, Suomela, PODC 2015] remains the same up to polylog factors in the faulty model, even if the adversary can fail $99\%$ of the nodes (or any other constant fraction).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11430v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keren Censor-Hillel, Pedro Soto</dc:creator>
    </item>
    <item>
      <title>Max-Min and 1-Bounded Space Algorithms for the Bin Packing Problem</title>
      <link>https://arxiv.org/abs/2508.18718</link>
      <description>arXiv:2508.18718v2 Announce Type: replace 
Abstract: In the (1-dimensional) bin packing problem, we are asked to pack all the given items into bins, each of capacity one, so that the number of non-empty bins is minimized. Zhu~[Chaos, Solitons \&amp; Fractals 2016] proposed an approximation algorithm $MM$ that sorts the item sequence in a non-increasing order by size at the beginning, and then repeatedly packs, into the current single open bin, first as many of the largest items in the remaining sequence as possible and then as many of the smallest items in the remaining sequence as possible. In this paper we prove that the asymptotic approximation ratio of $MM$ is at most 1.5. Next, focusing on the fact that $MM$ is at the intersection of two algorithm classes, max-min algorithms and 1-bounded space algorithms, we comprehensively analyze the theoretical performance bounds of each subclass derived from the two classes. Our results include a lower bound of 1.25 for the intersection of the two classes. Furthermore, we extend the theoretical analysis over algorithm classes to the cardinality constrained bin packing problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18718v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiroshi Fujiwara, Rina Atsumi, Hiroaki Yamamoto</dc:creator>
    </item>
    <item>
      <title>Decoupling via Affine Spectral-Independence: Beck-Fiala and Koml\'os Bounds Beyond Banaszczyk</title>
      <link>https://arxiv.org/abs/2508.03961</link>
      <description>arXiv:2508.03961v2 Announce Type: replace-cross 
Abstract: The Beck-Fiala Conjecture [Discrete Appl. Math, 1981] asserts that any set system of $n$ elements with degree $k$ has combinatorial discrepancy $O(\sqrt{k})$. A substantial generalization is the Koml\'os Conjecture, which states that any $m \times n$ matrix with unit length columns has discrepancy $O(1)$.
  In this work, we resolve the Beck-Fiala Conjecture for $k \geq \log^2 n$. We also give an $\widetilde{O}(\sqrt{k} + \sqrt{\log n})$ bound for $k \leq \log^2 n$, where $\widetilde{O}(\cdot)$ hides $\mathsf{poly}(\log \log n)$ factors. These bounds improve upon the $O(\sqrt{k \log n})$ bound due to Banaszczyk [Random Struct. Algor., 1998].
  For the Komlos problem, we give an $\widetilde{O}(\log^{1/4} n)$ bound, improving upon the previous $O(\sqrt{\log n})$ bound [Random Struct. Algor., 1998]. All of our results also admit efficient polynomial-time algorithms.
  To obtain these results, we exploit a new technique of ``decoupling via affine spectral-independence'' in designing rounding algorithms. In particular, our algorithms obtain the desired colorings via a discrete Brownian motion, guided by a semidefinite program (SDP). Besides standard constraints used in prior works, we add some extra affine spectral-independence constraints, which effectively decouple the evolution of discrepancies across different rows, and allow us to better control how many rows accumulate large discrepancies at any point during the process. This new technique is quite general and may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03961v2</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikhil Bansal, Haotian Jiang</dc:creator>
    </item>
  </channel>
</rss>
