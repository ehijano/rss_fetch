<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Dec 2024 05:00:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Hierarchical Bidirectional Transition Dispersion Entropy-based Lempel-Ziv Complexity and Its Application in Fault-Bearing Diagnosis</title>
      <link>https://arxiv.org/abs/2412.11123</link>
      <description>arXiv:2412.11123v1 Announce Type: new 
Abstract: Lempel-Ziv complexity (LZC) is a key measure for detecting the irregularity and complexity of nonlinear time series and has seen various improvements in recent decades. However, existing LZC-based metrics, such as Permutation Lempel-Ziv complexity (PLZC) and Dispersion-Entropy based Lempel-Ziv complexity (DELZC), focus mainly on patterns of independent embedding vectors, often overlooking the transition patterns within the time series. To address this gap, this paper introduces a novel LZC-based method called Bidirectional Transition Dispersion Entropy-based Lempel-Ziv complexity (BT-DELZC). Leveraging Markov chain theory, this method integrates a bidirectional transition network framework with DELZC to better capture dynamic signal information. Additionally, an improved hierarchical decomposition algorithm is used to extract features from various frequency components of the time series. The proposed BT-DELZC method is first evaluated through four simulated experiments, demonstrating its robustness and effectiveness in characterizing nonlinear time series. Additionally, two fault-bearing diagnosis experiments are conducted by combining the hierarchical BT-DELZC method with various classifiers from the machine learning domain. The results indicate that BT-DELZC achieves the highest accuracy across both datasets, significantly outperforming existing methods such as LZC, PLZC, and DELZC in extracting features related to fault bearings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11123v1</guid>
      <category>physics.data-an</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Runze Jiang, Pengjian Shang</dc:creator>
    </item>
    <item>
      <title>Centrality and Universality in Scale-Free Networks</title>
      <link>https://arxiv.org/abs/2412.10406</link>
      <description>arXiv:2412.10406v1 Announce Type: cross 
Abstract: We propose a novel paradigm for modeling real-world scale-free networks, where the integration of new nodes is driven by the combined attractiveness of degree and betweenness centralities, the competition of which (expressed by a parameter $0\le p\le 1$) shapes the structure of the evolving network. We reveal the ability to seamlessly explore a vast landscape of scale-free networks, unlocking an entirely new class of complex networks that we call \textit{stars-with-filament} structure. Remarkably, the average degree $\bar k$ of these networks grows like $\log t$ to some power, where $t$ is time and the average shortest path length grows logarithmically with the system size for intermediate $p$ values, offering fresh insights into the structural dynamics of scale-free systems. Our approach is backed by a robust mean-field theory, which nicely captures the dynamics of $\bar{k}$. We further unveil a rich, $p$-dependent phase diagram, encompassing 47 real-world scale-free networks, shedding light on previously hidden patterns. This work opens exciting new avenues for understanding the universal properties of complex networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10406v1</guid>
      <category>physics.soc-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>V. Adami, S. Emdadi-Mahdimahalleh, H. J. Herrmann, M. N. Najafi</dc:creator>
    </item>
    <item>
      <title>Efficient Summation of Arbitrary Masks -- ESAM</title>
      <link>https://arxiv.org/abs/2412.10678</link>
      <description>arXiv:2412.10678v1 Announce Type: cross 
Abstract: Searches for impulsive, astrophysical transients are often highly computationally demanding. A notable example is the dedispersion process required for performing blind searches for Fast Radio Bursts (FRBs) in radio telescope data. We introduce a novel approach - Efficient Summation of Arbitrary Masks (ESAM) - which efficiently computes 1-D convolution of many arbitrary 2-D masks, and can be used to carry out dedispersion over thousands of dispersion trials efficiently. Our method matches the accuracy of the traditional brute force technique in recovering the desired Signal-to-Noise ratio (S/N) while reducing computational cost by around a factor of 10. We compare its performance with existing dedispersion algorithms, such as the Fast Dispersion Measure Transform (FDMT) algorithm, and demonstrate how ESAM provides freedom to choose arbitrary masks and further optimise computational cost versus accuracy. We explore the potential applications of ESAM beyond FRB searches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10678v1</guid>
      <category>astro-ph.IM</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vivek Gupta, Keith Bannister, Chris Flynn, Clancy James</dc:creator>
    </item>
    <item>
      <title>Event-by-event Comparison between Machine-Learning- and Transfer-Matrix-based Unfolding Methods</title>
      <link>https://arxiv.org/abs/2310.17037</link>
      <description>arXiv:2310.17037v2 Announce Type: replace 
Abstract: The unfolding of detector effects is a key aspect of comparing experimental data with theoretical predictions. In recent years, different Machine-Learning methods have been developed to provide novel features, e.g. high dimensionality or a probabilistic single-event unfolding based on generative neural networks. Traditionally, many analyses unfold detector effects using transfer-matrix--based algorithms, which are well established in low-dimensional unfolding. They yield an unfolded distribution of the total spectrum, together with its covariance matrix. This paper proposes a method to obtain probabilistic single-event unfolded distributions, together with their uncertainties and correlations, for the transfer-matrix--based unfolding. The algorithm is first validated on a toy model and then applied to pseudo-data for the $pp\rightarrow Z\gamma \gamma$ process. In both examples the performance is compared to the Machine-Learning--based single-event unfolding using an iterative approach with conditional invertible neural networks (IcINN).</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17037v2</guid>
      <category>physics.data-an</category>
      <category>hep-ex</category>
      <category>hep-ph</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathias Backes, Anja Butter, Monica Dunford, Bogdan Malaescu</dc:creator>
    </item>
    <item>
      <title>Expansion of net correlations in terms of partial correlations</title>
      <link>https://arxiv.org/abs/2404.01734</link>
      <description>arXiv:2404.01734v2 Announce Type: replace-cross 
Abstract: The marginal correlation between two variables is a measure of their linear dependence. The two original variables need not interact directly, because marginal correlation may arise from the mediation of other variables in the system. The underlying network of direct interactions can be captured by a weighted graphical model. The connection between two variables can be weighted by their partial correlation, defined as the residual correlation left after accounting for the linear effects of mediating variables. While matrix inversion can be used to obtain marginal correlations from partial correlations, in large systems this approach does not reveal how the former emerge from the latter. Here we present an expansion of marginal correlations in terms of partial correlations, which shows that the effect of mediating variables can be quantified by the weight of the paths in the graphical model that connect the original pair of variables. The expansion is proved to converge for arbitrary probability distributions. The graphical interpretation reveals a close connection between the topology of the graph and the marginal correlations. Moreover, the expansion shows how marginal correlations change when some variables are severed from the graph, and how partial correlations change when some variables are marginalised out from the description. It also establishes the minimum number of latent variables required to replicate the exact effect of a collection of variables that are marginalised out, ensuring that the partial and marginal correlations of the remaining variables remain unchanged. Notably, the number of latent variables may be significantly smaller than the number of variables that they effectively replicate. Finally, for Gaussian variables, marginal correlations are shown to be related to the efficacy with which information propagates along the paths in the graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01734v2</guid>
      <category>stat.ME</category>
      <category>cond-mat.stat-mech</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>physics.data-an</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bautista Arenaza, Sebasti\'an Risau-Gusman, In\'es Samengo</dc:creator>
    </item>
    <item>
      <title>How much longer do you have to drive than the crow has to fly?</title>
      <link>https://arxiv.org/abs/2406.06490</link>
      <description>arXiv:2406.06490v2 Announce Type: replace-cross 
Abstract: When traveling by car from one location to another, our route is constrained by the road network. The network distance between the two locations is generally longer than the geodetic distance as the crow flies. We report a systematic relation between the statistical properties of these two distances. Empirically, we find a robust scaling between network and geodetic distance distributions for a variety of large motorway networks. A simple consequence is that we typically have to drive $1.3\pm0.1$ times longer than the crow flies. This scaling is not present in standard random networks; rather, it requires non-random adjacency. We develop a set of rules to build a realistic motorway network, also consistent with the above scaling. We hypothesize that the scaling reflects a compromise between two societal needs: high efficiency and accessibility on the one hand, and limitation of costs and other burdens on the other.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06490v2</guid>
      <category>physics.soc-ph</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1038/s44260-024-00023-x</arxiv:DOI>
      <arxiv:journal_reference>npj Complexity 1, 22 (2024)</arxiv:journal_reference>
      <dc:creator>Shanshan Wang, Henrik M. Bette, Michael Schreckenberg, Thomas Guhr</dc:creator>
    </item>
    <item>
      <title>Neural information field filter</title>
      <link>https://arxiv.org/abs/2407.16502</link>
      <description>arXiv:2407.16502v2 Announce Type: replace-cross 
Abstract: We introduce neural information field filter, a Bayesian state and parameter estimation method for high-dimensional nonlinear dynamical systems given large measurement datasets. Solving such a problem using traditional methods, such as Kalman and particle filters, is computationally expensive. Information field theory is a Bayesian approach that can efficiently reconstruct dynamical model state paths and calibrate model parameters from noisy measurement data. To apply the method, we parameterize the time evolution state path using the span of a finite linear basis. The existing method has to reparameterize the state path by initial states to satisfy the initial condition. Designing an expressive yet simple linear basis before knowing the true state path is crucial for inference accuracy but challenging. Moreover, reparameterizing the state path using the initial state is easy to perform for a linear basis, but is nontrivial for more complex and expressive function parameterizations, such as neural networks. The objective of this paper is to simplify and enrich the class of state path parameterizations using neural networks for the information field theory approach. To this end, we propose a generalized physics-informed conditional prior using an auxiliary initial state. We show the existing reparameterization is a special case. We parameterize the state path using a residual neural network that consists of a linear basis function and a Fourier encoding fully connected neural network residual function. The residual function aims to correct the error of the linear basis function. To sample from the intractable posterior distribution, we develop an optimization algorithm, nested stochastic variational inference, and a sampling algorithm, nested preconditioned stochastic gradient Langevin dynamics. A series of numerical and experimental examples verify and validate the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16502v2</guid>
      <category>stat.ML</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kairui Hao, Ilias Bilionis</dc:creator>
    </item>
    <item>
      <title>Multidimensional Deconvolution with Profiling</title>
      <link>https://arxiv.org/abs/2409.10421</link>
      <description>arXiv:2409.10421v2 Announce Type: replace-cross 
Abstract: In many experimental contexts, it is necessary to statistically remove the impact of instrumental effects in order to physically interpret measurements. This task has been extensively studied in particle physics, where the deconvolution task is called unfolding. A number of recent methods have shown how to perform high-dimensional, unbinned unfolding using machine learning. However, one of the assumptions in all of these methods is that the detector response is correctly modeled in the Monte Carlo simulation. In practice, the detector response depends on a number of nuisance parameters that can be constrained with data. We propose a new algorithm called Profile OmniFold, which works in a similar iterative manner as the OmniFold algorithm while being able to simultaneously profile the nuisance parameters. We illustrate the method with a Gaussian example as a proof of concept highlighting its promising capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10421v2</guid>
      <category>hep-ph</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huanbiao Zhu, Krish Desai, Mikael Kuusela, Vinicius Mikuni, Benjamin Nachman, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>GeoConformal prediction: a model-agnostic framework of measuring the uncertainty of spatial prediction</title>
      <link>https://arxiv.org/abs/2412.08661</link>
      <description>arXiv:2412.08661v2 Announce Type: replace-cross 
Abstract: Spatial prediction is a fundamental task in geography. In recent years, with advances in geospatial artificial intelligence (GeoAI), numerous models have been developed to improve the accuracy of geographic variable predictions. Beyond achieving higher accuracy, it is equally important to obtain predictions with uncertainty measures to enhance model credibility and support responsible spatial prediction. Although geostatistic methods like Kriging offer some level of uncertainty assessment, such as Kriging variance, these measurements are not always accurate and lack general applicability to other spatial models. To address this issue, we propose a model-agnostic uncertainty assessment method called GeoConformal Prediction, which incorporates geographical weighting into conformal prediction. We applied it to two classic spatial prediction cases, spatial regression and spatial interpolation, to evaluate its reliability. First, in the spatial regression case, we used XGBoost to predict housing prices, followed by GeoConformal to calculate uncertainty. Our results show that GeoConformal achieved a coverage rate of 93.67%, while Bootstrap methods only reached a maximum coverage of 81.00% after 2000 runs. Next, we applied GeoConformal to spatial interpolation models. We found that the uncertainty obtained from GeoConformal aligned closely with the variance in Kriging. Finally, using GeoConformal, we analyzed the sources of uncertainty in spatial prediction. We found that explicitly including local features in AI models can significantly reduce prediction uncertainty, especially in areas with strong local dependence. Our findings suggest that GeoConformal holds potential not only for geographic knowledge discovery but also for guiding the design of future GeoAI models, paving the way for more reliable and interpretable spatial prediction frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08661v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiayin Lou, Peng Luo, Liqiu Meng</dc:creator>
    </item>
  </channel>
</rss>
