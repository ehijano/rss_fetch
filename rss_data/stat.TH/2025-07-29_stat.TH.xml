<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Jul 2025 01:29:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>State evolution beyond first-order methods I: Rigorous predictions and finite-sample guarantees</title>
      <link>https://arxiv.org/abs/2507.19611</link>
      <description>arXiv:2507.19611v1 Announce Type: new 
Abstract: We develop a toolbox for exact analysis of iterative algorithms on a class of high-dimensional nonconvex optimization problems with random data. While prior work has shown that low-dimensional statistics of (generalized) first-order methods can be predicted by a deterministic recursion known as state evolution, our focus is on developing such a prediction for a more general class of algorithms. We provide a state evolution for any method whose iterations are given by (possibly interleaved) first-order and saddle point updates, showing two main results. First, we establish a rigorous state evolution prediction that holds even when the updates are not coordinate-wise separable. Second, we establish finite-sample guarantees bounding the deviation of the empirical updates from the established state evolution. In the process, we develop a technical toolkit that may prove useful in related problems. One component of this toolkit is a general Hilbert space lifting technique to prove existence and uniqueness of a convenient parameterization of the state evolution. Another component of the toolkit combines a generic application of Bolthausen's conditioning method with a sequential variant of Gordon's Gaussian comparison inequality, and provides additional ingredients that enable a general finite-sample analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19611v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Celentano, Chen Cheng, Ashwin Pananjady, Kabir Aladin Verchand</dc:creator>
    </item>
    <item>
      <title>Uniform inference in linear mixed models</title>
      <link>https://arxiv.org/abs/2507.19633</link>
      <description>arXiv:2507.19633v1 Announce Type: new 
Abstract: We provide finite-sample distribution approximations, that are uniform in the parameter, for inference in linear mixed models. Focus is on variances and covariances of random effects in cases where existing theory fails because their covariance matrix is nearly or exactly singular, and hence near or at the boundary of the parameter set. Quantitative bounds on the differences between the standard normal density and those of linear combinations of the score function enable, for example, the assessment of sufficient sample size. The bounds also lead to useful asymptotic theory in settings where both the number of parameters and the number of random effects grow with the sample size. We consider models with independent clusters and ones with a possibly diverging number of crossed random effects, which are notoriously complicated. Simulations indicate the theory leads to practically relevant methods. In particular, the studied confidence regions, which are straightforward to implement, have near-nominal coverage in finite samples even when some random effects have variances near or equal to zero, or correlations near or equal to $\pm 1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19633v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karl Oskar Ekvall, Matteo Bottai</dc:creator>
    </item>
    <item>
      <title>Extreme value theory for singular subspace estimation in the matrix denoising model</title>
      <link>https://arxiv.org/abs/2507.19978</link>
      <description>arXiv:2507.19978v1 Announce Type: new 
Abstract: This paper studies fine-grained singular subspace estimation in the matrix denoising model where a deterministic low-rank signal matrix is additively perturbed by a stochastic matrix of Gaussian noise. We establish that the maximum Euclidean row norm (i.e., the two-to-infinity norm) of the aligned difference between the leading sample and population singular vectors approaches the Gumbel distribution in the large-matrix limit, under suitable signal-to-noise conditions and after appropriate centering and scaling. We apply our novel asymptotic distributional theory to test hypotheses of low-rank signal structure encoded in the leading singular vectors and their corresponding principal subspace. We provide de-biased estimators for the corresponding nuisance signal singular values and show that our proposed plug-in test statistic has desirable properties. Notably, compared to using the Frobenius norm subspace distance, our test statistic based on the two-to-infinity norm has higher power to detect structured alternatives that differ from the null in only a few matrix entries or rows. Our main results are obtained by a novel synthesis of and technical analysis involving entrywise matrix perturbation analysis, extreme value theory, saddle point approximation methods, and random matrix theory. Our contributions complement the existing literature for matrix denoising focused on minimaxity, mean squared error analysis, unitarily invariant distances between subspaces, component-wise asymptotic distributional theory, and row-wise uniform error bounds. Numerical simulations illustrate our main results and demonstrate the robustness properties of our testing procedure to non-Gaussian noise distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19978v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junhyung Chang, Joshua Cape</dc:creator>
    </item>
    <item>
      <title>A global Lipschitz stability perspective for understanding approximate approaches in Bayesian sequential learning</title>
      <link>https://arxiv.org/abs/2507.20379</link>
      <description>arXiv:2507.20379v1 Announce Type: new 
Abstract: We establish a general, non-asymptotic error analysis framework for understanding the effects of incremental approximations made by practical approaches for Bayesian sequential learning (BSL) on their long-term inference performance. Our setting covers inverse problems, state estimation, and parameter-state estimation. In these settings, we bound the difference-termed the learning error-between the unknown true posterior and the approximate posterior computed by these approaches, using three widely used distribution metrics: total variation, Hellinger, and Wasserstein distances. This framework builds on our establishment of the global Lipschitz stability of the posterior with respect to the prior across these settings. To the best of our knowledge, this is the first work to establish such global Lipschitz stability under the Hellinger and Wasserstein distances and the first general error analysis framework for approximate BSL methods.
  Our framework offers two sets of upper bounds on the learning error. The first set demonstrates the stability of general approximate BSL methods with respect to the incremental approximation process, while the second set is estimable in many practical scenarios.
  Furthermore, as an initial step toward understanding the phenomenon of learning error decay, which is sometimes observed, we identify sufficient conditions under which data assimilation leads to learning error reduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20379v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liliang Wang, Alex A. Gorodetsky</dc:creator>
    </item>
    <item>
      <title>A Generalized Cram\'er-Rao Bound Using Information Geometry</title>
      <link>https://arxiv.org/abs/2507.21022</link>
      <description>arXiv:2507.21022v1 Announce Type: new 
Abstract: In information geometry, statistical models are considered as differentiable manifolds, where each probability distribution represents a unique point on the manifold. A Riemannian metric can be systematically obtained from a divergence function using Eguchi's theory (1992); the well-known Fisher-Rao metric is obtained from the Kullback-Leibler (KL) divergence. The geometric derivation of the classical Cram\'er-Rao Lower Bound (CRLB) by Amari and Nagaoka (2000) is based on this metric. In this paper, we study a Riemannian metric obtained by applying Eguchi's theory to the Basu-Harris-Hjort-Jones (BHHJ) divergence (1998) and derive a generalized Cram\'er-Rao bound using Amari-Nagaoka's approach. There are potential applications for this bound in robust estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21022v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Satyajit Dhadumia, M. Ashok Kumar</dc:creator>
    </item>
    <item>
      <title>Location Tests with Noisy Proxies for Latent Variables</title>
      <link>https://arxiv.org/abs/2507.19696</link>
      <description>arXiv:2507.19696v1 Announce Type: cross 
Abstract: We investigate inference in a latent binary variable model where a noisy proxy of the latent variable is available, motivated by the variable perturbation effectiveness problem in single-cell CRISPR screens. The baseline approach is to ignore the perturbation effectiveness problem, while a recent proposal employs a weighted average based on the proxies. Our main goals are to determine how accurate the proxies must be in order for a weighted test to gain power over the unweighted baseline, and to develop tests that are powerful regardless of the accuracy of the proxies. To address the first goal, we compute the Pitman relative efficiency of the weighted test relative to the unweighted test, yielding an interpretable quantification of proxy quality that drives the power of the weighted test. To address the second goal, we propose two strategies. First, we propose a maximum-likelihood based approach that adapts the proxies to the data. Second, we propose an estimator of the Pitman efficiency if a "positive control outcome variable" is available (as is often the case in single-cell CRISPR screens), which facilitates an adaptive choice of whether to use the proxies at all. Our numerical simulations support the Pitman efficiency as the key quantity for determining whether the weighted test gains power over the baseline, and demonstrate that the two proposed adaptive tests can improve on both existing approaches across a range of proxy qualities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19696v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louis Deutsch, Eugene Katsevich</dc:creator>
    </item>
    <item>
      <title>Computation of Optimal Type-II Progressing Censoring Scheme Using Genetic Algorithm Approach</title>
      <link>https://arxiv.org/abs/2507.20001</link>
      <description>arXiv:2507.20001v1 Announce Type: cross 
Abstract: The experimenter must perform a legitimate search in the entire set of feasible censoring schemes to identify the optimal type II progressive censoring scheme, when applied to a life-testing experiment. Current recommendations are limited to small sample sizes. Exhaustive search strategies are not practically feasible for large sample sizes. This paper proposes a meta-heuristic algorithm based on the genetic algorithm for large sample sizes. The algorithm is found to provide optimal or near-optimal solutions for small sample sizes and large sample sizes. Our suggested optimal criterion is based on the cost function and is scale-invariant for both location-scale and log-location-scale distribution families. To investigate how inaccurate parameter values or cost coefficients may affect the optimal solution, a sensitivity analysis is also taken into account.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20001v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ujjwal Roy, Ritwik Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Discrete Gaussian Vector Fields On Meshes</title>
      <link>https://arxiv.org/abs/2507.20024</link>
      <description>arXiv:2507.20024v1 Announce Type: cross 
Abstract: Though the underlying fields associated with vector-valued environmental data are continuous, observations themselves are discrete. For example, climate models typically output grid-based representations of wind fields or ocean currents, and these are often downscaled to a discrete set of points. By treating the area of interest as a two-dimensional manifold that can be represented as a triangular mesh and embedded in Euclidean space, this work shows that discrete intrinsic Gaussian processes for vector-valued data can be developed from discrete differential operators defined with respect to a mesh. These Gaussian processes account for the geometry and curvature of the manifold whilst also providing a flexible and practical formulation that can be readily applied to any two-dimensional mesh. We show that these models can capture harmonic flows, incorporate boundary conditions, and model non-stationary data. Finally, we apply these models to downscaling stationary and non-stationary gridded wind data on the globe, and to inference of ocean currents from sparse observations in bounded domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20024v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Gillan (University of Exeter), Stefan Siegert (University of Exeter), Ben Youngman (University of Exeter)</dc:creator>
    </item>
    <item>
      <title>Lasso Penalization for High-Dimensional Beta Regression Models: Computation, Analysis, and Inference</title>
      <link>https://arxiv.org/abs/2507.20079</link>
      <description>arXiv:2507.20079v1 Announce Type: cross 
Abstract: Beta regression is commonly employed when the outcome variable is a proportion. Since its conception, the approach has been widely used in applications spanning various scientific fields. A series of extensions have been proposed over time, several of which address variable selection and penalized estimation, e.g., with an $\ell_1$-penalty (LASSO). However, a theoretical analysis of this popular approach in the context of Beta regression with high-dimensional predictors is lacking. In this paper, we aim to close this gap. A particular challenge arises from the non-convexity of the associated negative log-likelihood, which we address by resorting to a framework for analyzing stationary points in a neighborhood of the target parameter. Leveraging this framework, we derive a non-asymptotic bound on the $\ell_1$-error of such stationary points. In addition, we propose a debiasing approach to construct confidence intervals for the regression parameters. A proximal gradient algorithm is devised for optimizing the resulting penalized negative log-likelihood function. Our theoretical analysis is corroborated via simulation studies, and a real data example concerning the prediction of county-level proportions of incarceration is presented to showcase the practical utility of our methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20079v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niloofar Ramezani, Martin Slawski</dc:creator>
    </item>
    <item>
      <title>Diagonally-Weighted Generalized Method of Moments Estimation for Gaussian Mixture Modeling</title>
      <link>https://arxiv.org/abs/2507.20459</link>
      <description>arXiv:2507.20459v1 Announce Type: cross 
Abstract: Since Pearson [Philosophical Transactions of the Royal Society of London. A, 185 (1894), pp. 71-110] first applied the method of moments (MM) for modeling data as a mixture of one-dimensional Gaussians, moment-based estimation methods have proliferated. Among these methods, the generalized method of moments (GMM) improves the statistical efficiency of MM by weighting the moments appropriately. However, the computational complexity and storage complexity of MM and GMM grow exponentially with the dimension, making these methods impractical for high-dimensional data or when higher-order moments are required. Such computational bottlenecks are more severe in GMM since it additionally requires estimating a large weighting matrix. To overcome these bottlenecks, we propose the diagonally-weighted GMM (DGMM), which achieves a balance among statistical efficiency, computational complexity, and numerical stability. We apply DGMM to study the parameter estimation problem for weakly separated heteroscedastic low-rank Gaussian mixtures and design a computationally efficient and numerically stable algorithm that obtains the DGMM estimator without explicitly computing or storing the moment tensors. We implement the proposed algorithm and empirically validate the advantages of DGMM: in numerical studies, DGMM attains smaller estimation errors while requiring substantially shorter runtime than MM and GMM. The code and data will be available upon publication at https://github.com/liu-lzhang/dgmm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20459v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liu Zhang, Oscar Mickelin, Sheng Xu, Amit Singer</dc:creator>
    </item>
    <item>
      <title>Independence Testing for Mixed Data</title>
      <link>https://arxiv.org/abs/2507.20609</link>
      <description>arXiv:2507.20609v1 Announce Type: cross 
Abstract: We consider the problem of testing independence in mixed-type data that combine count variables with positive, absolutely continuous variables. We first introduce two distinct classes of test statistics in the bivariate setting, designed to test independence between the components of a bivariate mixed-type vector. These statistics are then extended to the multivariate context to accommodate: (i) testing independence between vectors of different types and possibly different dimensions, and (ii) testing total independence among all components of vectors with different types. The construction is based on the recently introduced Baringhaus-Gaigall transformation, which characterizes the joint distribution of such data. We establish the asymptotic properties of the resulting tests and, through an extensive power study, demonstrate that the proposed approach is both competitive and flexible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20609v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dana Bucalo Jeli\'c, Marija Cupari\'c, Bojana Milo\v{s}evi\'c</dc:creator>
    </item>
    <item>
      <title>Bernstein-type dimension-free concentration for self-normalised martingales</title>
      <link>https://arxiv.org/abs/2507.20982</link>
      <description>arXiv:2507.20982v1 Announce Type: cross 
Abstract: We introduce a dimension-free Bernstein-type tail inequality for self-normalised martingales normalised by their predictable quadratic variation. As applications of our result, we propose solutions to the recent open problems posed by Mussi et al. (2024), providing computationally efficient confidence sequences for logistic regression with adaptively chosen RKHS-valued covariates, and establishing instance-adaptive regret bounds in the corresponding kernelised bandit setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20982v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arya Akhavan, Amitis Shidani, Alex Ayoub, David Janz</dc:creator>
    </item>
    <item>
      <title>Symmetric Perceptrons, Number Partitioning and Lattices</title>
      <link>https://arxiv.org/abs/2501.16517</link>
      <description>arXiv:2501.16517v2 Announce Type: replace 
Abstract: The symmetric binary perceptron ($\mathrm{SBP}_{\kappa}$) problem with parameter $\kappa : \mathbb{R}_{\geq1} \to [0,1]$ is an average-case search problem defined as follows: given a random Gaussian matrix $\mathbf{A} \sim \mathcal{N}(0,1)^{n \times m}$ as input where $m \geq n$, output a vector $\mathbf{x} \in \{-1,1\}^m$ such that $$|| \mathbf{A} \mathbf{x} ||_{\infty} \leq \kappa(m/n) \cdot \sqrt{m}~.$$ The number partitioning problem ($\mathrm{NPP}_{\kappa}$) corresponds to the special case of setting $n=1$. There is considerable evidence that both problems exhibit large computational-statistical gaps.
  In this work, we show (nearly) tight average-case hardness for these problems, assuming the worst-case hardness of standard approximate shortest vector problems on lattices.
  For $\mathrm{SBP}$, for large $n$, the best that efficient algorithms have been able to achieve is $\kappa(x) = \Theta(1/\sqrt{x})$ (Bansal and Spencer, Random Structures and Algorithms 2020), which is a far cry from the statistical bound. The problem has been extensively studied in the TCS and statistics communities, and Gamarnik, Kizildag, Perkins and Xu (FOCS 2022) conjecture that Bansal-Spencer is tight: namely, $\kappa(x) = \widetilde{\Theta}(1/\sqrt{x})$ is the optimal value achieved by computationally efficient algorithms. We prove their conjecture assuming the worst-case hardness of approximating the shortest vector problem on lattices.
  For $\mathrm{NPP}$, Karmarkar and Karp's classical differencing algorithm achieves $\kappa(m) = 2^{-O(\log^2 m)}~.$ We prove that Karmarkar-Karp is nearly tight: namely, no polynomial-time algorithm can achieve $\kappa(m) = 2^{-\Omega(\log^3 m)}$, once again assuming the worst-case subexponential hardness of approximating the shortest vector problem on lattices to within a subexponential factor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16517v2</guid>
      <category>math.ST</category>
      <category>cs.CC</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neekon Vafa, Vinod Vaikuntanathan</dc:creator>
    </item>
    <item>
      <title>Early Stopping for Regression Trees</title>
      <link>https://arxiv.org/abs/2502.04709</link>
      <description>arXiv:2502.04709v3 Announce Type: replace 
Abstract: We develop early stopping rules for growing regression tree estimators. The fully data-driven stopping rule is based on monitoring the global residual norm. The best-first search and the breadth-first search algorithms together with linear interpolation give rise to generalized projection or regularization flows. A general theory of early stopping is established. Oracle inequalities for the early-stopped regression tree are derived without any smoothness assumption on the regression function, assuming the original CART splitting rule, yet with a much broader scope. The remainder terms are of smaller order than the best achievable rates for Lipschitz functions in dimension $d\ge 2$. In real and synthetic data the early stopping regression tree estimators attain the statistical performance of cost-complexity pruning while significantly reducing computational costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04709v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ratmir Miftachov, Markus Rei{\ss}</dc:creator>
    </item>
    <item>
      <title>Signal detection from spiked noise via asymmetrization</title>
      <link>https://arxiv.org/abs/2504.19450</link>
      <description>arXiv:2504.19450v3 Announce Type: replace 
Abstract: The signal plus noise model $H=S+Y$ is a fundamental model in signal detection when a low rank signal $S$ is polluted by noise $Y$. In the high-dimensional setting, one often uses the leading singular values and corresponding singular vectors of $H$ to conduct the statistical inference of the signal $S$. Especially, when $Y$ consists of iid random entries, the singular values of $S$ can be estimated from those of $H$ as long as the signal $S$ is strong enough. However, when the $Y$ entries are heteroscedastic or heavy-tailed, this standard approach may fail. Especially in this work, we consider a situation that can easily arise with heteroscedastic or heavy-tailed noise but is particularly difficult to address using the singular value approach, namely, when the noise $Y$ itself may create spiked singular values. It has been a recurring question how to distinguish the signal $S$ from the spikes in $Y$, as this seems impossible by examining the leading singular values of $H$. Inspired by the work \cite{CCF21}, we turn to study the eigenvalues of an asymmetrized model when two samples $H_1=S+Y_1$ and $H_2=S+Y_2$ are available. We show that by looking into the leading eigenvalues (in magnitude) of the asymmetrized model $H_1H_2^*$, one can easily detect $S$. We will primarily discuss the heteroscedastic case and then discuss the extension to the heavy-tailed case. As a byproduct, we also derive the fundamental result regarding the outlier of non-Hermitian random matrix in \cite{Tao} under the minimal 2nd moment condition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19450v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Zhigang Bao, Kha Man Cheong, Jaehun Lee, Yuji Li</dc:creator>
    </item>
    <item>
      <title>Robust Tail Index Estimation under Random Censoring via Minimum Density Power Divergence</title>
      <link>https://arxiv.org/abs/2507.18737</link>
      <description>arXiv:2507.18737v3 Announce Type: replace 
Abstract: We introduce a robust estimator for the tail index of a Pareto-type distribution under random right censoring, developed within the framework of the minimum density power divergence. To the best of our knowledge, this is the first approach to integrate density power divergence into the context of randomly censored extreme value models, thus opening a new path for robust inference in this setting. Under general regularity conditions, the proposed estimator is shown to be consistent and asymptotically normal. Its finite-sample behavior is thoroughly assessed through an extensive simulation study, which highlights its improved robustness and efficiency compared to existing methods. Finally, the practical relevance of the method is illustrated through an application to a real AIDS survival dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18737v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nour Elhouda Guesmia, Abdelhakim Necir, Djamel Meraghni</dc:creator>
    </item>
    <item>
      <title>Tree-structured Ising models under mean parameterization</title>
      <link>https://arxiv.org/abs/2507.18749</link>
      <description>arXiv:2507.18749v2 Announce Type: replace 
Abstract: We assess advantages of expressing tree-structured Ising models via their mean parameterization rather than their commonly chosen canonical parameterization. This includes fixedness of marginal distributions, often convenient for dependence modeling, and the dispelling of the intractable normalizing constant otherwise hindering Ising models. We derive an analytic expression for the joint probability generating function of mean-parameterized tree-structured Ising models, conferring efficient computation methods for the distribution of the sum of its constituent random variables. The mean parameterization also allows for a stochastic representation of Ising models, providing straightforward sampling methods. We furthermore show that Markov random fields with fixed Poisson marginal distributions may act as an efficient and accurate approximation for tree-structured Ising models, in the spirit of Poisson approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18749v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin C\^ot\'e, H\'el\`ene Cossette, Etienne Marceau</dc:creator>
    </item>
    <item>
      <title>Separate Exchangeability as Modeling Principle in Bayesian Nonparametrics</title>
      <link>https://arxiv.org/abs/2112.07755</link>
      <description>arXiv:2112.07755v3 Announce Type: replace-cross 
Abstract: We argue for the use of separate exchangeability as a modeling principle in Bayesian nonparametric (BNP) inference. Separate exchangeability is de facto widely applied in the Bayesian parametric case, e.g., it naturally arises in simple mixed models. However, while in some areas, such as random graphs, separate and (closely related) joint exchangeable models are widely used, they are curiously underused for several other applications in BNP. We briefly review the definition of separate exchangeability, focusing on the implications of such a definition in Bayesian modeling. We then discuss two tractable classes of models that implement separate exchangeability, which are the natural counterparts of familiar partially exchangeable BNP models.
  The first is nested random partitions for a data matrix, defining a partition of columns and nested partitions of rows, nested within column clusters. Many recent models for nested partitions implement partially exchangeable models related to variations of the well-known nested Dirichlet process. We argue that inference under such models in some cases ignores important features of the experimental setup. We obtain the separately exchangeable counterpart of such partially exchangeable partition structures.
  The second class is about setting up separately exchangeable priors for a nonparametric regression model when multiple sets of experimental units are involved. We highlight how a Dirichlet process mixture of linear models, known as ANOVA DDP, can naturally implement separate exchangeability in such regression problems. Finally, we illustrate how to perform inference under such models in two real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.07755v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giovanni Rebaudo, Qiaohui Lin, Peter Mueller</dc:creator>
    </item>
    <item>
      <title>A semi-parametric model for assessing the effect of temperature on ice accumulation rate from Antarctic ice core data</title>
      <link>https://arxiv.org/abs/2309.03782</link>
      <description>arXiv:2309.03782v2 Announce Type: replace-cross 
Abstract: In this paper, we present a semiparametric model for describing the effect of temperature on Antarctic ice accumulation on a paleoclimatic time scale. The model is motivated by sharp ups and downs in the rate of ice accumulation apparent from ice core data records, which are synchronous with movements of temperature. We prove strong consistency of the estimators under reasonable conditions. We conduct extensive simulations to assess the performance of the estimators and bootstrap based standard errors and confidence limits for the requisite range of sample sizes. Analysis of ice core data from two Antarctic locations over several hundred thousand years shows a reasonable fit. The apparent accumulation rate exhibits a thinning pattern that should facilitate the understanding of ice condensation, transformation and flow over the ages. There is a very strong linear relationship between temperature and the apparent accumulation rate adjusted for thinning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.03782v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Radhendushka Srivastava, Debasis Sengupta</dc:creator>
    </item>
    <item>
      <title>On the rates of convergence for learning with convolutional neural networks</title>
      <link>https://arxiv.org/abs/2403.16459</link>
      <description>arXiv:2403.16459v3 Announce Type: replace-cross 
Abstract: We study approximation and learning capacities of convolutional neural networks (CNNs) with one-side zero-padding and multiple channels. Our first result proves a new approximation bound for CNNs with certain constraint on the weights. Our second result gives new analysis on the covering number of feed-forward neural networks with CNNs as special cases. The analysis carefully takes into account the size of the weights and hence gives better bounds than the existing literature in some situations. Using these two results, we are able to derive rates of convergence for estimators based on CNNs in many learning problems. In particular, we establish minimax optimal convergence rates of the least squares based on CNNs for learning smooth functions in the nonparametric regression setting. For binary classification, we derive convergence rates for CNN classifiers with hinge loss and logistic loss. It is also shown that the obtained rates for classification are minimax optimal in some common settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16459v3</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunfei Yang, Han Feng, Ding-Xuan Zhou</dc:creator>
    </item>
    <item>
      <title>Non-convex matrix sensing: Breaking the quadratic rank barrier in the sample complexity</title>
      <link>https://arxiv.org/abs/2408.13276</link>
      <description>arXiv:2408.13276v4 Announce Type: replace-cross 
Abstract: For the problem of reconstructing a low-rank matrix from a few linear measurements, two classes of algorithms have been widely studied in the literature: convex approaches based on nuclear norm minimization, and non-convex approaches that use factorized gradient descent. Under certain statistical model assumptions, it is known that nuclear norm minimization recovers the ground truth as soon as the number of samples scales linearly with the number of degrees of freedom of the ground-truth. In contrast, while non-convex approaches are computationally less expensive, existing recovery guarantees assume that the number of samples scales at least quadratically with the rank $r$ of the ground-truth matrix. In this paper, we close this gap by showing that the non-convex approaches can be as efficient as nuclear norm minimization in terms of sample complexity. Namely, we consider the problem of reconstructing a positive semidefinite matrix from a few Gaussian measurements. We show that factorized gradient descent with spectral initialization converges to the ground truth at a linear rate as soon as the number of samples scales with $ \Omega (rd\kappa^2)$, where $d$ is the dimension, and $\kappa$ is the condition number of the ground truth matrix. This improves the previous rank-dependence in the sample complexity of non-convex matrix factorization from quadratic to linear. Furthermore, we extend our theory to the noisy setting, where we show that with noisy measurements, factorized gradient descent with spectral initialization converges to the minimax optimal error up to a factor linear in $\kappa$. Our proof relies on a probabilistic decoupling argument, where we show that the gradient descent iterates are only weakly dependent on the individual entries of the measurement matrices. We expect that our proof technique is of independent interest for other non-convex problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13276v4</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dominik St\"oger, Yizhe Zhu</dc:creator>
    </item>
    <item>
      <title>Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning</title>
      <link>https://arxiv.org/abs/2506.09853</link>
      <description>arXiv:2506.09853v2 Announce Type: replace-cross 
Abstract: Chain-of-Thought (CoT) prompting plays an indispensable role in endowing large language models (LLMs) with complex reasoning capabilities. However, CoT currently faces two fundamental challenges: (1) Sufficiency, which ensures that the generated intermediate inference steps comprehensively cover and substantiate the final conclusion; and (2) Necessity, which identifies the inference steps that are truly indispensable for the soundness of the resulting answer. We propose a causal framework that characterizes CoT reasoning through the dual lenses of sufficiency and necessity. Incorporating causal Probability of Sufficiency and Necessity allows us not only to determine which steps are logically sufficient or necessary to the prediction outcome, but also to quantify their actual influence on the final reasoning outcome under different intervention scenarios, thereby enabling the automated addition of missing steps and the pruning of redundant ones. Extensive experimental results on various mathematical and commonsense reasoning benchmarks confirm substantial improvements in reasoning efficiency and reduced token usage without sacrificing accuracy. Our work provides a promising direction for improving LLM reasoning performance and cost-effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09853v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiangning Yu, Zhuohan Wang, Linyi Yang, Haoxuan Li, Anjie Liu, Xiao Xue, Jun Wang, Mengyue Yang</dc:creator>
    </item>
    <item>
      <title>Local empirical Bayes correction for Bayesian modeling</title>
      <link>https://arxiv.org/abs/2506.11424</link>
      <description>arXiv:2506.11424v3 Announce Type: replace-cross 
Abstract: The James-Stein estimator has attracted much interest as a shrinkage estimator that yields better estimates than the maximum likelihood estimator. The James-Stein estimator is also very useful as an argument in favor of empirical Bayesian methods. However, for problems involving large-scale data, such as differential gene expression data, the distribution is considered a mixture distribution with different means that cannot be considered sufficiently close. Therefore, it is not appropriate to apply the James-Stein estimator. Efron (2011) proposed a local empirical Bayes correction that attempted to correct a selection bias for large-scale data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11424v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.24644/keidaironshu.68.4_161</arxiv:DOI>
      <arxiv:journal_reference>Osaka Keidai Ronshu, vol.68, no.4, pp.161-172, 2017</arxiv:journal_reference>
      <dc:creator>Yoshiko Hayashi</dc:creator>
    </item>
  </channel>
</rss>
