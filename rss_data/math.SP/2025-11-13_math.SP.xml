<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.SP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.SP</link>
    <description>math.SP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.SP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Nov 2025 05:00:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The exterior Steklov problem for Euclidean domains</title>
      <link>https://arxiv.org/abs/2511.09490</link>
      <description>arXiv:2511.09490v1 Announce Type: new 
Abstract: We investigate the Steklov eigenvalue problem in an exterior Euclidean domain. First, we present several formulations of this problem and establish the equivalences between them. Next, we examine various properties of the exterior Steklov eigenvalues and eigenfunctions. One of our main findings is an Escobar-type lower bound for the first exterior Steklov eigenvalue on convex domains in dimensions three and higher. This bound is expressed in terms of the principal curvatures of the boundary and is sharp, with equality attained for a ball. Moreover, it implies the existence of a sequence of convex domains with fixed volume and the first exterior Steklov eigenvalues tending to infinity. This contrasts with the interior case, as well as with the two-dimensional exterior case, for which we show that an analogue of the Weinstock isoperimetric inequality holds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09490v1</guid>
      <category>math.SP</category>
      <category>math.AP</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lukas Bundrock, Alexandre Girouard, Denis S. Grebenkov, Michael Levitin, Iosif Polterovich</dc:creator>
    </item>
    <item>
      <title>New perturbation bounds for low rank approximation of matrices via contour analysis</title>
      <link>https://arxiv.org/abs/2511.08875</link>
      <description>arXiv:2511.08875v1 Announce Type: cross 
Abstract: Let $A$ be an $m \times n$ matrix with rank $r$ and spectral decomposition $A = \sum _{i=1}^r \sigma_i u_i v_i^\top, $ where $\sigma_i$ are its singular values, ordered decreasingly, and $u_i, v_i$ are the corresponding left and right singular vectors. For a parameter $1 \le p \le r$, $A_p := \sum_{i=1}^p \sigma_i u_i v_i^\top$ is the best rank $p$ approximation of $A$. In practice, one often chooses $p$ to be small, leading the commonly used phrase "low-rank approximation". Low-rank approximation plays a central role in data science because it can substantially reduce the dimensionality of the original data, the matrix $A$. For a large data matrix $A$, one typically computes a rank-$p$ approximation $A_p$ for a suitably chosen small $p$, stores $A_p$, and uses it as input for further computations. The reduced dimension of $A_p$ enables faster computations and significant data compression. In practice, noise is inevitable. We often have access only to noisy data $\tilde A = A + E$, where $E$ represents the noise. Consequently, the low-rank approximation used as input in many downstream tasks is $\tilde A_p$, the best rank $p$ approximation of $\tilde A$, rather than $A_p$. Therefore, it is natural and important to estimate the error $ \| \tilde A_p - A_p \|$. In this paper, we develop a new method (based on contour analysis) to bound $\| \tilde A_p - A_p \|$. We introduce new parameters that measure the skewness between the noise matrix $E$ and the singular vectors of $A$, and exploit these to obtain notable improvements in many popular settings. This method is of independent interest and has many further applications. We focus on the case where $A$ itself has relatively low rank. This assumption is frequently met in practice, and we think that this case deserves a separate, accessible treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08875v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>math.SP</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Phuc Tran, Van Vu</dc:creator>
    </item>
  </channel>
</rss>
