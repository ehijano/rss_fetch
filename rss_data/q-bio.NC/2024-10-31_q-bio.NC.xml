<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 01 Nov 2024 02:05:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Emergence of Human-Like Attention in Self-Supervised Vision Transformers: an eye-tracking study</title>
      <link>https://arxiv.org/abs/2410.22768</link>
      <description>arXiv:2410.22768v1 Announce Type: new 
Abstract: Many models of visual attention have been proposed so far. Traditional bottom-up models, like saliency models, fail to replicate human gaze patterns, and deep gaze prediction models lack biological plausibility due to their reliance on supervised learning. Vision Transformers (ViTs), with their self-attention mechanisms, offer a new approach but often produce dispersed attention patterns if trained with supervised learning. This study explores whether self-supervised DINO (self-DIstillation with NO labels) training enables ViTs to develop attention mechanisms resembling human visual attention. Using video stimuli to capture human gaze dynamics, we found that DINO-trained ViTs closely mimic human attention patterns, while those trained with supervised learning deviate significantly. An analysis of self-attention heads revealed three distinct clusters: one focusing on foreground objects, one on entire objects, and one on the background. DINO-trained ViTs offer insight into how human overt attention and figure-ground separation develop in visual perception.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22768v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Takuto Yamamoto, Hirosato Akahoshi, Shigeru Kitazawa</dc:creator>
    </item>
    <item>
      <title>Emergent rate-based dynamics in duplicate-free populations of spiking neurons</title>
      <link>https://arxiv.org/abs/2303.05174</link>
      <description>arXiv:2303.05174v5 Announce Type: replace 
Abstract: Can Spiking Neural Networks (SNNs) approximate the dynamics of Recurrent Neural Networks (RNNs)? Arguments in classical mean-field theory based on laws of large numbers provide a positive answer when each neuron in the network has many "duplicates", i.e. other neurons with almost perfectly correlated inputs. Using a disordered network model that guarantees the absence of duplicates, we show that duplicate-free SNNs can converge to RNNs, thanks to the concentration of measure phenomenon. This result reveals a general mechanism underlying the emergence of rate-based dynamics in large SNNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.05174v5</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Valentin Schmutz, Johanni Brea, Wulfram Gerstner</dc:creator>
    </item>
    <item>
      <title>Theta and/or alpha? Neural oscillational substrates for dynamic inter-brain synchrony during mother-child cooperation</title>
      <link>https://arxiv.org/abs/2410.13669</link>
      <description>arXiv:2410.13669v2 Announce Type: replace 
Abstract: Mother-child interaction is a highly dynamic process neurally characterized by inter-brain synchrony (IBS) at {\theta} and/or {\alpha} rhythms. However, their establishment, dynamic changes, and roles in mother-child interactions remain unknown. Through dynamic analysis of dual-EEG from 40 mother-child dyads during turn-taking cooperation, we uncover that {\theta}-IBS and {\alpha}-IBS alternated with interactive behaviors, with EEG frequency-shift as a prerequisite for IBS transitions. When mothers attempt to track their children's attention and/or predict their intentions, they will adjust their EEG frequencies to align with their children's {\theta} oscillations, leading to a higher occurrence of the {\theta}-IBS state. Conversely, the {\alpha}-IBS state, accompanied by the EEG frequency-shift to the {\alpha} range, is more prominent during mother-led interactions. Further exploratory analysis reveals greater presence and stability of the {\theta}-IBS state during cooperative than non-cooperative conditions, particularly in dyads with stronger emotional attachments and more frequent interactions in their daily lives. Our findings shed light on the neural oscillational substrates underlying the IBS dynamics during mother-child interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13669v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiayang Xu, Yamin Li, Ruxin Su, Saishuang Wu, Chengcheng Wu, Haiwa Wang, Qi Zhu, Yue Fang, Fan Jiang, Shanbao Tong, Yunting Zhang, Xiaoli Guo</dc:creator>
    </item>
    <item>
      <title>Neural association between musical features and shared emotional perception while movie-watching: fMRI study</title>
      <link>https://arxiv.org/abs/2410.17300</link>
      <description>arXiv:2410.17300v2 Announce Type: replace 
Abstract: The growing use of naturalistic stimuli, such as feature films, brings research on emotions closer to ecologically valid settings within brain scanners, such as functional magnetic resonance imaging (fMRI). Music is another cultural artifact known to evoke emotions, and film soundtracks are often designed to enhance the emotional impact of the narrative. However, the neural basis of shared emotional perception during movie-watching, and its relationship to the soundtrack, remains unclear. In this study, participants watched Forrest Gump in a 3T fMRI scanner. The reported shared arousal and valence (positive and negative) was correlated with the spectrogram and tempo information extracted from the soundtrack. For neuroimaging, four regions of interest (ROIs) related to audiovisual input and narrative content were examined: the superior temporal sulcus (STS), amygdala, inferior frontal gyrus, and precuneus. Results: Strong correlations between spectral and tempo, and emotional features (Bonferroni corr). BOLD responses associated with shared positive and negative valence and high-arousal moments were compared to film events with low emotional agreement. Significant activations were found in the STS and precuneus (pFWE &lt; 0.05 peak level). No significant responses were observed in the amygdala or inferior frontal gyrus, but whole-brain analysis (pFWE &lt; 0.05 cluster extent) revealed frontal activity linked to negative valence, alongside expected occipito-temporal lobe responses to the audiovisual stimuli. The STS and precuneus were particularly active during shared emotional perception of valenced and high-arousal events, with the precuneus encoding valence from auditory information. Naturalistic stimuli show promise as diagnostic and therapeutic tools for emotion regulation deficits although challenges may arise when crafting an experimental control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17300v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonardo Muller-Rodriguez (Cambridge Institute for Music Therapy Research - CIMTR), Owen O'Daly (IoPPN - King's College London)</dc:creator>
    </item>
    <item>
      <title>Nanoscale Connectomics Annotation Standards Framework</title>
      <link>https://arxiv.org/abs/2410.22320</link>
      <description>arXiv:2410.22320v2 Announce Type: replace 
Abstract: The promise of large-scale, high-resolution datasets from Electron Microscopy (EM) and X-ray Microtomography (XRM) lies in their ability to reveal neural structures and synaptic connectivity, which is critical for understanding the brain. Effectively managing these complex and rapidly increasing datasets will enable new scientific insights, facilitate querying, and support secondary use across the neuroscience community. However, without effective neurodata standards that permit use of these data across multiple systems and workflows, these valuable and costly datasets risk being underutilized especially as they surpass petascale levels. These standards will promote data sharing through accessible interfaces, allow researchers to build on each other's work, and guide the development of tools and capabilities that are interoperable. Herein we outline a standards framework for creating and managing annotations originating and derived from high-resolution volumetric imaging and connectomic datasets, focusing on ensuring Findable, Accessible, Interoperable, and Reusable (FAIR) practices. The goal is to enhance collaborative efforts, boost the reliability of findings, and enable comparative analysis across growing datasets of different species and modalities. We have formed a global working group with academic and industry partners in the high-resolution volumetric data generation and analysis community, focused on identifying gaps in current EM and XRM data pipelines, and refining outlines and platforms for standardizing EM and XRM methods. This focus considers existing and past community approaches and includes examining neuronal entities, biological components, and associated metadata, while emphasizing adaptability and fostering collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22320v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicole K. Guittari, Miguel E. Wimbish, Patricia K. Rivlin, Mark A. Hinton, Jordan K. Matelsky, Victoria A. Rose, Jorge L. Rivera Jr., Nicole E. Stock, Brock A. Wester, Erik C. Johnson, William R. Gray-Roncal</dc:creator>
    </item>
    <item>
      <title>Emergent mechanisms for long timescales depend on training curriculum and affect performance in memory tasks</title>
      <link>https://arxiv.org/abs/2309.12927</link>
      <description>arXiv:2309.12927v3 Announce Type: replace-cross 
Abstract: Recurrent neural networks (RNNs) in the brain and in silico excel at solving tasks with intricate temporal dependencies. Long timescales required for solving such tasks can arise from properties of individual neurons (single-neuron timescale, $\tau$, e.g., membrane time constant in biological neurons) or recurrent interactions among them (network-mediated timescale). However, the contribution of each mechanism for optimally solving memory-dependent tasks remains poorly understood. Here, we train RNNs to solve $N$-parity and $N$-delayed match-to-sample tasks with increasing memory requirements controlled by $N$ by simultaneously optimizing recurrent weights and $\tau$s. We find that for both tasks RNNs develop longer timescales with increasing $N$, but depending on the learning objective, they use different mechanisms. Two distinct curricula define learning objectives: sequential learning of a single-$N$ (single-head) or simultaneous learning of multiple $N$s (multi-head). Single-head networks increase their $\tau$ with $N$ and are able to solve tasks for large $N$, but they suffer from catastrophic forgetting. However, multi-head networks, which are explicitly required to hold multiple concurrent memories, keep $\tau$ constant and develop longer timescales through recurrent connectivity. Moreover, we show that the multi-head curriculum increases training speed and network stability to ablations and perturbations, and allows RNNs to generalize better to tasks beyond their training regime. This curriculum also significantly improves training GRUs and LSTMs for large-$N$ tasks. Our results suggest that adapting timescales to task requirements via recurrent interactions allows learning more complex objectives and improves the RNN's performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.12927v3</guid>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>The Twelfth International Conference on Learning Representations (2024)</arxiv:journal_reference>
      <dc:creator>Sina Khajehabdollahi, Roxana Zeraati, Emmanouil Giannakakis, Tim Jakob Sch\"afer, Georg Martius, Anna Levina</dc:creator>
    </item>
    <item>
      <title>Control when confidence is costly</title>
      <link>https://arxiv.org/abs/2406.14427</link>
      <description>arXiv:2406.14427v2 Announce Type: replace-cross 
Abstract: We develop a version of stochastic control that accounts for computational costs of inference. Past studies identified efficient coding without control, or efficient control that neglects the cost of synthesizing information. Here we combine these concepts into a framework where agents rationally approximate inference for efficient control. Specifically, we study Linear Quadratic Gaussian (LQG) control with an added internal cost on the relative precision of the posterior probability over the world state. This creates a trade-off: an agent can obtain more utility overall by sacrificing some task performance, if doing so saves enough bits during inference. We discover that the rational strategy that solves the joint inference and control problem goes through phase transitions depending on the task demands, switching from a costly but optimal inference to a family of suboptimal inferences related by rotation transformations, each misestimate the stability of the world. In all cases, the agent moves more to think less. This work provides a foundation for a new type of rational computations that could be used by both brains and machines for efficient but computationally constrained control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14427v2</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Itzel Olivos-Castillo, Paul Schrater, Xaq Pitkow</dc:creator>
    </item>
  </channel>
</rss>
