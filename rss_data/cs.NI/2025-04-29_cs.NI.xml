<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 Apr 2025 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>GeoFINDR: Practical Approach to Verify Cloud Instances Geolocation in Multicloud</title>
      <link>https://arxiv.org/abs/2504.18685</link>
      <description>arXiv:2504.18685v1 Announce Type: new 
Abstract: In multicloud environments, where legal obligations, technical constraints and economic interests are at stake, it is of interest to stakeholders to be able to locate cloud data or the cloud instance where data are decrypted for processing, making it particularly vulnerable. This paper proposes an original and practical delay-based approach, called GeoFINDR, to locate a cloud instance, e.g. a Virtual Machine (VM), over the Internet, based on RIPE Atlas landmarks. First, the assumed threat model and assumptions are more realistic than in existing solutions, e.g. VM-scale localization in multicloud environments, a Cloud Service Provider (CSP) lying about the VM's location. Second, the originality of the approach lies in four original ideas: (1) geolocalization is performed from the VM, (2) a Greedy algorithm selects a first set LM_A of distributed audit landmarks in the vicinity of the declared area, (3) a sectorization algorithm identifies a set LM_S of other landmarks with distance delay behavior similar to that of the VM to estimate the sector of the VM, and (4) the estimated location of the VM is calculated as the barycenter position of the LM_S landmarks. An open source tool is published on GitHub and experiments show that localization accuracy can be as high as 22.1km, under unfavorable conditions where the CSP lies about the location of the VM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18685v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Said Ider, Maryline Laurent</dc:creator>
    </item>
    <item>
      <title>Contracts: A unified lens on congestion control robustness, fairness, congestion, and generality</title>
      <link>https://arxiv.org/abs/2504.18786</link>
      <description>arXiv:2504.18786v1 Announce Type: new 
Abstract: Congestion control algorithms (CCAs) operate in partially observable environments, lacking direct visibility into link capacities, or competing flows. To ensure fair sharing of network resources, CCAs communicate their fair share through observable signals. For instance, Reno's fair share is encoded as $\propto 1/\sqrt{\texttt{loss rate}}$. We call such communication mechanisms \emph{contracts}. We show that the design choice of contracts fixes key steady-state performance metrics, including robustness to errors in congestion signals, fairness, amount of congestion (e.g., delay, loss), and generality (e.g., range of supported link rates). This results in fundamental tradeoffs between these metrics. We also discover some properties of contracts that describe CCA design pitfalls that can lead to starvation (extreme unfairness). We empirically validate our findings and discuss their implications on CCA design and network measurement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18786v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anup Agarwal, Venkat Arun, Srinivasan Seshan</dc:creator>
    </item>
    <item>
      <title>Transformer-Empowered Actor-Critic Reinforcement Learning for Sequence-Aware Service Function Chain Partitioning</title>
      <link>https://arxiv.org/abs/2504.18902</link>
      <description>arXiv:2504.18902v1 Announce Type: new 
Abstract: In the forthcoming era of 6G networks, characterized by unprecedented data rates, ultra-low latency, and extensive connectivity, effective management of Virtualized Network Functions (VNFs) is essential. VNFs are software-based counterparts of traditional hardware devices that facilitate flexible and scalable service provisioning. Service Function Chains (SFCs), structured as ordered sequences of VNFs, are pivotal in orchestrating complex network services. Nevertheless, partitioning SFCs across multi-domain network infrastructures presents substantial challenges due to stringent latency constraints and limited resource availability. Conventional optimization-based methods typically exhibit low scalability, whereas existing data-driven approaches often fail to adequately balance computational efficiency with the capability to effectively account for dependencies inherent in SFCs. To overcome these limitations, we introduce a Transformer-empowered actor-critic framework specifically designed for sequence-aware SFC partitioning. By utilizing the self-attention mechanism, our approach effectively models complex inter-dependencies among VNFs, facilitating coordinated and parallelized decision-making processes. Additionally, we enhance training stability and convergence using $\epsilon$-LoPe exploration strategy as well as Asymptotic Return Normalization. Comprehensive simulation results demonstrate that the proposed methodology outperforms existing state-of-the-art solutions in terms of long-term acceptance rates, resource utilization efficiency, and scalability, while achieving rapid inference. This study not only advances intelligent network orchestration by delivering a scalable and robust solution for SFC partitioning within emerging 6G environments, but also bridging recent advancements in Large Language Models (LLMs) with the optimization of next-generation networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18902v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cyril Shih-Huan Hsu, Anestis Dalgkitsis, Chrysa Papagianni, Paola Grosso</dc:creator>
    </item>
    <item>
      <title>Scaling Data Center TCP to Terabits with Laminar</title>
      <link>https://arxiv.org/abs/2504.19058</link>
      <description>arXiv:2504.19058v1 Announce Type: new 
Abstract: Laminar is the first TCP stack designed for the reconfigurable match-action table (RMT) architecture, widely used in high-speed programmable switches and SmartNICs. Laminar reimagines TCP processing as a pipeline of simple match-action operations, enabling line-rate performance with low latency and minimal energy consumption, while maintaining compatibility with standard TCP and POSIX sockets. Leveraging novel techniques like optimistic concurrency, pseudo segment updates, and bump-in-the-wire processing, Laminar handles the transport logic, including retransmission, reassembly, flow, and congestion control, entirely within the RMT pipeline.
  We prototype Laminar on an Intel Tofino2 switch and demonstrate its scalability to terabit speeds, its flexibility, and robustness to network dynamics. Laminar reaches an unprecedented 25M pkts/sec with a single host core for streaming workloads, enough to exceed 1.6Tbps with 8K MTU. Laminar delivers RDMA-equivalent performance, saving up to 16 host CPU cores versus the TAS kernel-bypass TCP stack with short RPC workloads, while achieving 1.3$\times$ higher peak throughput at 5$\times$ lower 99.99p tail latency. A key-value store on Laminar doubles the throughput-per-watt versus TAS. Demonstrating Laminar's flexibility, we implement TCP stack extensions, including a sequencer API for a linearizable distributed shared log, a new congestion control protocol, and delayed ACKs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19058v1</guid>
      <category>cs.NI</category>
      <category>cs.OS</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rajath Shashidhara, Antoine Kaufmann, Simon Peter</dc:creator>
    </item>
    <item>
      <title>Decentralization of Generative AI via Mixture of Experts for Wireless Networks: A Comprehensive Survey</title>
      <link>https://arxiv.org/abs/2504.19660</link>
      <description>arXiv:2504.19660v1 Announce Type: new 
Abstract: Mixture of Experts (MoE) has emerged as a promising paradigm for scaling model capacity while preserving computational efficiency, particularly in large-scale machine learning architectures such as large language models (LLMs). Recent advances in MoE have facilitated its adoption in wireless networks to address the increasing complexity and heterogeneity of modern communication systems. This paper presents a comprehensive survey of the MoE framework in wireless networks, highlighting its potential in optimizing resource efficiency, improving scalability, and enhancing adaptability across diverse network tasks. We first introduce the fundamental concepts of MoE, including various gating mechanisms and the integration with generative AI (GenAI) and reinforcement learning (RL). Subsequently, we discuss the extensive applications of MoE across critical wireless communication scenarios, such as vehicular networks, unmanned aerial vehicles (UAVs), satellite communications, heterogeneous networks, integrated sensing and communication (ISAC), and mobile edge networks. Furthermore, key applications in channel prediction, physical layer signal processing, radio resource management, network optimization, and security are thoroughly examined. Additionally, we present a detailed overview of open-source datasets that are widely used in MoE-based models to support diverse machine learning tasks. Finally, this survey identifies crucial future research directions for MoE, emphasizing the importance of advanced training techniques, resource-aware gating strategies, and deeper integration with emerging 6G technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19660v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunting Xu, Jiacheng Wang, Ruichen Zhang, Changyuan Zhao, Dusit Niyato, Jiawen Kang, Zehui Xiong, Bo Qian, Haibo Zhou, Shiwen Mao, Abbas Jamalipour, Xuemin Shen, Dong In Kim</dc:creator>
    </item>
    <item>
      <title>Performance Analysis of OpenVPN on a Consumer Grade Router</title>
      <link>https://arxiv.org/abs/2504.19069</link>
      <description>arXiv:2504.19069v1 Announce Type: cross 
Abstract: Virtual Private Networks (VPNs) offer an alternative solution using Internet Protocol (IP) tunnels to create secure, encrypted communication between geographically distant networks using a common shared medium such as the Internet. They use tunneling to establish end-to-end connectivity. OpenVPN is a cross-platform, secure, highly configurable VPN solution. Security in OpenVPN is handled by the OpenSSL cryptographic library which provides strong security over a Secure Socket Layer (SSL) using standard algorithms such as Advanced Encryption Standard (AES), Blowfish, or Triple DES (3DES). The Linksys WRT54GL router is a consumer-grade router made by Linksys, a division of Cisco Systems, capable of running under Linux. The Linux-based DD-WRT open-source router firmware can run OpenVPN on the Linksys WRT54GL router. For this case study, the performance of OpenVPN is measured and analyzed using a $2^{k-p}$ fractional factorial design for 5 minus 1 factors where $k=5$ and $p=1$. The results show that the throughput is mainly limited by the encryption cipher used, and that the round-trip time (RTT) is mostly dependent on the transport protocol selected.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19069v1</guid>
      <category>cs.PF</category>
      <category>cs.NI</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael J. Hall (Washington University in St. Louis)</dc:creator>
    </item>
    <item>
      <title>Graph Reinforcement Learning for QoS-Aware Load Balancing in Open Radio Access Networks</title>
      <link>https://arxiv.org/abs/2504.19499</link>
      <description>arXiv:2504.19499v1 Announce Type: cross 
Abstract: Next-generation wireless cellular networks are expected to provide unparalleled Quality-of-Service (QoS) for emerging wireless applications, necessitating strict performance guarantees, e.g., in terms of link-level data rates. A critical challenge in meeting these QoS requirements is the prevention of cell congestion, which involves balancing the load to ensure sufficient radio resources are available for each cell to serve its designated User Equipments (UEs). In this work, a novel QoS-aware Load Balancing (LB) approach is developed to optimize the performance of Guaranteed Bit Rate (GBR) and Best Effort (BE) traffic in a multi-band Open Radio Access Network (O-RAN) under QoS and resource constraints. The proposed solution builds on Graph Reinforcement Learning (GRL), a powerful framework at the intersection of Graph Neural Network (GNN) and RL. The QoS-aware LB is modeled as a Markov Decision Process, with states represented as graphs. QoS consideration are integrated into both state representations and reward signal design. The LB agent is then trained using an off-policy dueling Deep Q Network (DQN) that leverages a GNN-based architecture. This design ensures the LB policy is invariant to the ordering of nodes (UE or cell), flexible in handling various network sizes, and capable of accounting for spatial node dependencies in LB decisions. Performance of the GRL-based solution is compared with two baseline methods. Results show substantial performance gains, including a $53\%$ reduction in QoS violations and a fourfold increase in the 5th percentile rate for BE traffic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19499v1</guid>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Omid Semiari, Hosein Nikopour, Shilpa Talwar</dc:creator>
    </item>
    <item>
      <title>Lifecycle Management of Optical Networks with Dynamic-Updating Digital Twin: A Hybrid Data-Driven and Physics-Informed Approach</title>
      <link>https://arxiv.org/abs/2504.19564</link>
      <description>arXiv:2504.19564v1 Announce Type: cross 
Abstract: Digital twin (DT) techniques have been proposed for the autonomous operation and lifecycle management of next-generation optical networks. To fully utilize potential capacity and accommodate dynamic services, the DT must dynamically update in sync with deployed optical networks throughout their lifecycle, ensuring low-margin operation. This paper proposes a dynamic-updating DT for the lifecycle management of optical networks, employing a hybrid approach that integrates data-driven and physics-informed techniques for fiber channel modeling. This integration ensures both rapid calculation speed and high physics consistency in optical performance prediction while enabling the dynamic updating of critical physical parameters for DT. The lifecycle management of optical networks, covering accurate performance prediction at the network deployment and dynamic updating during network operation, is demonstrated through simulation in a large-scale network. Up to 100 times speedup in prediction is observed compared to classical numerical methods. In addition, the fiber Raman gain strength, amplifier frequency-dependent gain profile, and connector loss between fiber and amplifier on C and L bands can be simultaneously updated. Moreover, the dynamic-updating DT is verified on a field-trial C+L-band transmission link, achieving a maximum accuracy improvement of 1.4 dB for performance estimation post-device replacement. Overall, the dynamic-updating DT holds promise for driving the next-generation optical networks towards lifecycle autonomous management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19564v1</guid>
      <category>physics.optics</category>
      <category>cs.NI</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/JSAC.2025.3543489</arxiv:DOI>
      <dc:creator>Yuchen Song, Min Zhang, Yao Zhang, Yan Shi, Shikui Shen, Xiongyan Tang, Shanguo Huang, Danshi Wang</dc:creator>
    </item>
    <item>
      <title>Automatic Configuration Protocols for Optical Quantum Networks</title>
      <link>https://arxiv.org/abs/2504.19613</link>
      <description>arXiv:2504.19613v1 Announce Type: cross 
Abstract: Before quantum networks can scale up to practical sizes, there are many deployment and configuration tasks that must be automated. Currently, quantum networking testbeds are largely manually configured: network nodes are constructed out of a combination of free-space and fiber optics before being connected to shared single-photon detectors, time-to-digital converters, and optical switches. Information about these connections must be tracked manually; mislabeling may result in experimental failure and protracted debugging sessions. In this paper, we propose protocols and algorithms to automate two such manual processes. First, we address the problem of automatically identifying connections between quantum network nodes and time-to-digital converters. Then, we turn to the more complex challenge of identifying the nodes attached to a quantum network's optical switches. Implementation of these protocols will help enable the development of other protocols necessary for quantum networks, such as network topology discovery, link quality monitoring, resource naming, and routing. We intend for this paper to serve as a roadmap for near-term implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19613v1</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amin Taherkhani, Andrew Todd, Kentaro Teramoto, Shota Nagayama, Rodney Van Meter</dc:creator>
    </item>
    <item>
      <title>On the Centralization and Regionalization of the Web</title>
      <link>https://arxiv.org/abs/2406.19569</link>
      <description>arXiv:2406.19569v2 Announce Type: replace 
Abstract: Over the past decade, Internet centralization and its implications for both people and the resilience of the Internet has become a topic of active debate. While the networking community informally agrees on the definition of centralization, we lack a formal metric for quantifying centralization, which limits research beyond descriptive analysis. In this work, we introduce a statistical measure for Internet centralization, which we use to better understand how the web is centralized across four layers of web infrastructure (hosting providers, DNS infrastructure, TLDs, and certificate authorities) in 150~countries. Our work uncovers significant geographical variation, as well as a complex interplay between centralization and sociopolitically driven regionalization. We hope that our work can serve as the foundation for more nuanced analysis to inform this important debate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19569v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gautam Akiwate, Kimberly Ruth, Rumaisa Habib, Zakir Durumeric</dc:creator>
    </item>
    <item>
      <title>Revisiting Outage for Edge Inference Systems</title>
      <link>https://arxiv.org/abs/2504.03686</link>
      <description>arXiv:2504.03686v2 Announce Type: replace 
Abstract: One of the key missions of sixth-generation (6G) mobile networks is to deploy large-scale artificial intelligence (AI) models at the network edge to provide remote-inference services for edge devices. The resultant platform, known as edge inference, will support a wide range of Internet-of-Things applications, such as autonomous driving, industrial automation, and augmented reality. Given the mission-critical and time-sensitive nature of these tasks, it is essential to design edge inference systems that are both reliable and capable of meeting stringent end-to-end (E2E) latency constraints. Existing studies, which primarily focus on communication reliability as characterized by channel outage probability, may fail to guarantee E2E performance, specifically in terms of E2E inference accuracy and latency. To address this limitation, we propose a theoretical framework that introduces and mathematically characterizes the inference outage (InfOut) probability, which quantifies the likelihood that the E2E inference accuracy falls below a target threshold. Under an E2E latency constraint, this framework establishes a fundamental tradeoff between communication overhead (i.e., uploading more sensor observations) and inference reliability as quantified by the InfOut probability. To find a tractable way to optimize this tradeoff, we derive accurate surrogate functions for InfOut probability by applying a Gaussian approximation to the distribution of the received discriminant gain. Experimental results demonstrate the superiority of the proposed design over conventional communication-centric approaches in terms of E2E inference reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03686v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhanwei Wang, Qunsong Zeng, Haotian Zheng, Kaibin Huang</dc:creator>
    </item>
    <item>
      <title>AutoRAN: Automated and Zero-Touch Open RAN Systems</title>
      <link>https://arxiv.org/abs/2504.11233</link>
      <description>arXiv:2504.11233v2 Announce Type: replace 
Abstract: [...] This paper presents AutoRAN, an automated, intent-driven framework for zero-touch provisioning of open, programmable cellular networks. Leveraging cloud-native principles, AutoRAN employs virtualization, declarative infrastructure-as-code templates, and disaggregated micro-services to abstract physical resources and protocol stacks. Its orchestration engine integrates Language Models (LLMs) to translate high-level intents into machine-readable configurations, enabling closed-loop control via telemetry-driven observability. Implemented on a multi-architecture OpenShift cluster with heterogeneous compute (x86/ARM CPUs, NVIDIA GPUs) and multi-vendor Radio Access Network (RAN) hardware (Foxconn, NI), AutoRAN automates deployment of O-RAN-compliant stacks-including OpenAirInterface, NVIDIA ARC RAN, Open5GS core, and O-RAN Software Community (OSC) RIC components-using CI/CD pipelines. Experimental results demonstrate that AutoRAN is capable of deploying an end-to-end Private 5G network in less than 60 seconds with 1.6 Gbps throughput, validating its ability to streamline configuration, accelerate testing, and reduce manual intervention with similar performance than non cloud-based implementations. With its novel LLM-assisted intent translation mechanism, and performance-optimized automation workflow for multi-vendor environments, AutoRAN has the potential of advancing the robustness of next-generation cellular supply chains through reproducible, intent-based provisioning across public and private deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11233v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefano Maxenti, Ravis Shirkhani, Maxime Elkael, Leonardo Bonati, Salvatore D'Oro, Tommaso Melodia, Michele Polese</dc:creator>
    </item>
    <item>
      <title>Estimating the Number of HTTP/3 Responses in QUIC Using Deep Learning</title>
      <link>https://arxiv.org/abs/2410.06140</link>
      <description>arXiv:2410.06140v3 Announce Type: replace-cross 
Abstract: QUIC, a new and increasingly used transport protocol, enhances TCP by offering improved security, performance, and stream multiplexing. These features, however, also impose challenges for network middle-boxes that need to monitor and analyze web traffic. This paper proposes a novel method to estimate the number of HTTP/3 responses in a given QUIC connection by an observer. This estimation reveals server behavior, client-server interactions, and data transmission efficiency, which is crucial for various applications such as designing a load balancing solution and detecting HTTP/3 flood attacks. The proposed scheme transforms QUIC connection traces into image sequences and uses machine learning (ML) models, guided by a tailored loss function, to predict response counts. Evaluations on more than seven million images-derived from 100,000 traces collected across 44,000 websites over four months-achieve up to 97% accuracy in both known and unknown server settings and 92% accuracy on previously unseen complete QUIC traces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06140v3</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.NI</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Barak Gahtan, Robert J. Shahla, Reuven Cohen, Alex M. Bronstein</dc:creator>
    </item>
    <item>
      <title>Reactive Orchestration for Hierarchical Federated Learning Under a Communication Cost Budget</title>
      <link>https://arxiv.org/abs/2412.03385</link>
      <description>arXiv:2412.03385v2 Announce Type: replace-cross 
Abstract: Deploying a Hierarchical Federated Learning (HFL) pipeline across the computing continuum (CC) requires careful organization of participants into a hierarchical structure with intermediate aggregation nodes between FL clients and the global FL server. This is challenging to achieve due to (i) cost constraints, (ii) varying data distributions, and (iii) the volatile operating environment of the CC. In response to these challenges, we present a framework for the adaptive orchestration of HFL pipelines, designed to be reactive to client churn and infrastructure-level events, while balancing communication cost and ML model accuracy. Our mechanisms identify and react to events that cause HFL reconfiguration actions at runtime, building on multi-level monitoring information (model accuracy, resource availability, resource cost). Moreover, our framework introduces a generic methodology for estimating reconfiguration costs to continuously re-evaluate the quality of adaptation actions, while being extensible to optimize for various HFL performance criteria. By extending the Kubernetes ecosystem, our framework demonstrates the ability to react promptly and effectively to changes in the operating environment, making the best of the available communication cost budget and effectively balancing costs and ML performance at runtime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03385v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivan \v{C}ili\'c, Anna Lackinger, Pantelis Frangoudis, Ivana Podnar \v{Z}arko, Alireza Furutanpey, Ilir Murturi, Schahram Dustdar</dc:creator>
    </item>
    <item>
      <title>Blank Space: Adaptive Causal Coding for Streaming Communications Over Multi-Hop Networks</title>
      <link>https://arxiv.org/abs/2502.11984</link>
      <description>arXiv:2502.11984v2 Announce Type: replace-cross 
Abstract: In this work, we introduce Blank Space AC-RLNC (BS), a novel Adaptive and Causal Network Coding (AC-RLNC) solution designed to mitigate the triplet trade-off between throughput-delay-efficiency in multi-hop networks. BS leverages the network's physical limitations considering the bottleneck from each node to the destination. In particular, BS introduces a light-computational re-encoding algorithm, called Network AC-RLNC (NET), implemented independently at intermediate nodes. NET adaptively adjusts the Forward Error Correction (FEC) rates and schedules idle periods. It incorporates two distinct suspension mechanisms: 1) Blank Space Period, accounting for the forward-channels bottleneck, and 2) No-New No-FEC approach, based on data availability. The experimental results achieve significant improvements in resource efficiency, demonstrating a 20% reduction in channel usage compared to baseline RLNC solutions. Notably, these efficiency gains are achieved while maintaining competitive throughput and delay performance, ensuring improved resource utilization does not compromise network performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11984v2</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adina Waxman, Shai Ginzach, Aviel Glam, Alejandro Cohen</dc:creator>
    </item>
  </channel>
</rss>
