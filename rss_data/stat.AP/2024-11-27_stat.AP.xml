<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Nov 2024 02:55:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Statistical Emulation of Human Operational Motions</title>
      <link>https://arxiv.org/abs/2411.16929</link>
      <description>arXiv:2411.16929v1 Announce Type: new 
Abstract: This paper addresses the critical and challenging task of developing emulators for simulating human operational motions in industrial workplaces. We conceptualize human motion as a sequence of human body shapes. Leveraging statistical shape theory, we develop statistical generative models for sequences of human (body) shapes of human workers in workplaces. Our goal is to create emulators that generate random, realistic human motions statistically consistent with past work performances, essential for simulating industrial operations involving human labor. We derive the space of shapes as a Riemannian shape manifold, modeling human motion as a continuous-time stochastic process on this manifold. Representing such processes is challenging due to the nonlinearity of the shape manifold, variability in execution rates across observations, infinite dimensionality of stochastic processes, and population variability within and across action classes. This paper studies multiple solutions to these problems. This paper proposes multiple solutions to these challenges, presenting a comprehensive framework that incorporates (1) time warping for temporal alignment of training data, (2) Riemannian geometry for tackling manifold nonlinearity, and (3) Shape- and Functional-PCA for dimension reduction leading to traditional finite-dimensional Euclidean representations. In particular, it develops the transported velocity field representation for motion sequences and imposes a Gaussian model on the resulting Euclidean spaces. It then applies these models to emulate human shape sequences from an industrial operation dataset and evaluates these simulations in multiple ways.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16929v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanliang Chen, Chiwoo Park, Anuj Srivastava</dc:creator>
    </item>
    <item>
      <title>A Machine Learning-based Anomaly Detection Framework in Life Insurance Contracts</title>
      <link>https://arxiv.org/abs/2411.17495</link>
      <description>arXiv:2411.17495v1 Announce Type: new 
Abstract: Life insurance, like other forms of insurance, relies heavily on large volumes of data. The business model is based on an exchange where companies receive payments in return for the promise to provide coverage in case of an accident. Thus, trust in the integrity of the data stored in databases is crucial. One method to ensure data reliability is the automatic detection of anomalies. While this approach is highly useful, it is also challenging due to the scarcity of labeled data that distinguish between normal and anomalous contracts or inter\-actions. This manuscript discusses several classical and modern unsupervised anomaly detection methods and compares their performance across two different datasets. In order to facilitate the adoption of these methods by companies, this work also explores ways to automate the process, making it accessible even to non-data scientists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17495v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreas Groll, Akshat Khanna, Leonid Zeldin</dc:creator>
    </item>
    <item>
      <title>Cornering in the Water: An Investigation of Dolphin Swimming Performance</title>
      <link>https://arxiv.org/abs/2411.17688</link>
      <description>arXiv:2411.17688v1 Announce Type: new 
Abstract: This article provides new insights into dolphin maneuver strategies in lap swimming tasks. However, most existing research focuses on straight-line swimming leaving the study of dolphins' corning strategies an open area. Challenges for directly analyzing dolphins' turning behavior include difficulties in motion tracking underwater and the inability to directly measure the propulsive forces. This paper provides methodology and analyses of dolphins' swimming performance during lap swimming tasks. External camera detection and internal kinematics measured from wearable bio-tags are involved in this study to support accurate localization of the animals. A particle filter, which fuses the external and internal measurements, is implemented to provide accurate estimations of the trajectories, even when they swim deep below the water's surface. Thereafter, a hydrodynamic model is constructed to calculate the thrust power and energy cost of the animals. The energetic cost during lap swimming is calculated for the comparison between different corning behaviors. The results show that the implemented particle filter can provide precise and complete trajectories of the tested dolphins, providing fundamental for statistical study of the corning behavior. From the kinematic analysis, TT01 is the fastest lap swimmer, with the highest swimming speed for the whole lap while performing a sharp turn with small deceleration. TT02 performs greater energetic efficiency than TT01 by transferring more weight at high speed. TT03 shows the highest energetic efficiency by maintaining a slow underwater motion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17688v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingkai Xia, Junhan Zhang, Ningshan Wang, Gabriel Antoniak, Nicole West, Ding Zhang, Kenneth Alex Shorter</dc:creator>
    </item>
    <item>
      <title>Fragility Index for Time-to-Event Endpoints in Single-Arm Clinical Trials</title>
      <link>https://arxiv.org/abs/2411.16938</link>
      <description>arXiv:2411.16938v1 Announce Type: cross 
Abstract: The reliability of clinical trial outcomes is crucial, especially in guiding medical decisions. In this paper, we introduce the Fragility Index (FI) for time-to-event endpoints in single-arm clinical trials - a novel metric designed to quantify the robustness of study conclusions. The FI represents the smallest number of censored observations that, when reclassified as uncensored events, causes the posterior probability of the median survival time exceeding a specified threshold to fall below a predefined confidence level. While drug effectiveness is typically assessed by determining whether the posterior probability exceeds a specified confidence level, the FI offers a complementary measure, indicating how robust these conclusions are to potential shifts in the data. Using a Bayesian approach, we develop a practical framework for computing the FI based on the exponential survival model. To facilitate the application of our method, we developed an R package fi, which provides a tool to compute the Fragility Index. Through real world case studies involving time to event data from single arms clinical trials, we demonstrate the utility of this index. Our findings highlight how the FI can be a valuable tool for assessing the robustness of survival analyses in single-arm studies, aiding researchers and clinicians in making more informed decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16938v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnab Kumar Maity, Jhanvi Garg, Cynthia Basu</dc:creator>
    </item>
    <item>
      <title>Variable selection via fused sparse-group lasso penalized multi-state models incorporating molecular data</title>
      <link>https://arxiv.org/abs/2411.17394</link>
      <description>arXiv:2411.17394v1 Announce Type: cross 
Abstract: In multi-state models based on high-dimensional data, effective modeling strategies are required to determine an optimal, ideally parsimonious model. In particular, linking covariate effects across transitions is needed to conduct joint variable selection. A useful technique to reduce model complexity is to address homogeneous covariate effects for distinct transitions. We integrate this approach to data-driven variable selection by extended regularization methods within multi-state model building. We propose the fused sparse-group lasso (FSGL) penalized Cox-type regression in the framework of multi-state models combining the penalization concepts of pairwise differences of covariate effects along with transition grouping. For optimization, we adapt the alternating direction method of multipliers (ADMM) algorithm to transition-specific hazards regression in the multi-state setting. In a simulation study and application to acute myeloid leukemia (AML) data, we evaluate the algorithm's ability to select a sparse model incorporating relevant transition-specific effects and similar cross-transition effects. We investigate settings in which the combined penalty is beneficial compared to global lasso regularization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17394v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaya Miah, Jelle J. Goeman, Hein Putter, Annette Kopp-Schneider, Axel Benner</dc:creator>
    </item>
    <item>
      <title>Learning Explainable Treatment Policies with Clinician-Informed Representations: A Practical Approach</title>
      <link>https://arxiv.org/abs/2411.17570</link>
      <description>arXiv:2411.17570v1 Announce Type: cross 
Abstract: Digital health interventions (DHIs) and remote patient monitoring (RPM) have shown great potential in improving chronic disease management through personalized care. However, barriers like limited efficacy and workload concerns hinder adoption of existing DHIs; while limited sample sizes and lack of interpretability limit the effectiveness and adoption of purely black-box algorithmic DHIs. In this paper, we address these challenges by developing a pipeline for learning explainable treatment policies for RPM-enabled DHIs. We apply our approach in the real-world setting of RPM using a DHI to improve glycemic control of youth with type 1 diabetes. Our main contribution is to reveal the importance of clinical domain knowledge in developing state and action representations for effective, efficient, and interpretable targeting policies. We observe that policies learned from clinician-informed representations are significantly more efficacious and efficient than policies learned from black-box representations. This work emphasizes the importance of collaboration between ML researchers and clinicians for developing effective DHIs in the real world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17570v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Johannes O. Ferstad, Emily B. Fox, David Scheinker, Ramesh Johari</dc:creator>
    </item>
    <item>
      <title>Can artificial intelligence predict clinical trial outcomes?</title>
      <link>https://arxiv.org/abs/2411.17595</link>
      <description>arXiv:2411.17595v1 Announce Type: cross 
Abstract: The increasing complexity and cost of clinical trials, particularly in the context of oncology and advanced therapies, pose significant challenges for drug development. This study evaluates the predictive capabilities of large language models (LLMs) such as GPT-3.5, GPT-4, and HINT in determining clinical trial outcomes. By leveraging a curated dataset of trials from ClinicalTrials.gov, we compare the models' performance using metrics including balanced accuracy, specificity, recall, and Matthews Correlation Coefficient (MCC). Results indicate that GPT-4o demonstrates robust performance in early trial phases, achieving high recall but facing limitations in specificity. Conversely, the HINT model excels in recognizing negative outcomes, particularly in later trial phases, offering a balanced approach across diverse endpoints. Oncology trials, characterized by high complexity, remain challenging for all models. Additionally, trial duration and disease categories influence predictive performance, with longer durations and complex diseases such as neoplasms reducing accuracy. This study highlights the complementary strengths of LLMs and HINT, providing insights into optimizing predictive tools for clinical trial design and risk management. Future advancements in LLMs are essential to address current gaps in handling negative outcomes and complex domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17595v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyi Jin, Lu Chen, Hongru Ding, Meijie Wang, Lun Yu</dc:creator>
    </item>
    <item>
      <title>Improvement of variables interpretability in kernel PCA</title>
      <link>https://arxiv.org/abs/2303.16682</link>
      <description>arXiv:2303.16682v2 Announce Type: replace 
Abstract: Kernel methods have been proven to be a powerful tool for the integration and analysis of highthroughput technologies generated data. Kernels offer a nonlinear version of any linear algorithm solely based on dot products. The kernelized version of Principal Component Analysis is a valid nonlinear alternative to tackle the nonlinearity of biological sample spaces. This paper proposes a novel methodology to obtain a data-driven feature importance based on the KPCA representation of the data. The proposed method, kernel PCA Interpretable Gradient (KPCA-IG), provides a datadriven feature importance that is computationally fast and based solely on linear algebra calculations. It has been compared with existing methods on three benchmark datasets. The accuracy obtained using KPCA-IG selected features is equal to or greater than the other methods' average. Also, the computational complexity required demonstrates the high efficiency of the method. An exhaustive literature search has been conducted on the selected genes from a publicly available Hepatocellular carcinoma dataset to validate the retained features from a biological point of view. The results once again remark on the appropriateness of the computed ranking. The black-box nature of kernel PCA needs new methods to interpret the original features. Our proposed methodology KPCA-IG proved to be a valid alternative to select influential variables in high-dimensional high-throughput datasets, potentially unravelling new biological and medical biomarkers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.16682v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1186/s12859-023-05404-y</arxiv:DOI>
      <arxiv:journal_reference>BMC Bioinformatics 24, 282 (2023)</arxiv:journal_reference>
      <dc:creator>Mitja Briscik (IMT), Marie-Agn\`es Dillies (IMT), S\'ebastien D\'ejean (IMT)</dc:creator>
    </item>
    <item>
      <title>Algorithmic Forecasting of Extreme Heat Waves</title>
      <link>https://arxiv.org/abs/2409.18305</link>
      <description>arXiv:2409.18305v3 Announce Type: replace 
Abstract: This paper provides some foundations for valid forecasting of rare and extreme heat waves through a better understanding of the similarities and differences between several consecutive hot days under normal circumstances and rare, extreme heat waves. We analyze AIRS data from the American Pacific Northwest and AIRS data from the Phoenix, Arizona region. A genetic algorithm is used to help determine the most promising predictors. Classification accuracy with supervised learning is excellent for the Pacific Northwest and is replicated for Phoenix. Conformal prediction sets are considered as a way to represent forecasting uncertainty. Complications caused by endogenous sampling are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18305v3</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Richard A. Berk, Amy Braverman, Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>A multi-language toolkit for the semi-automated checking of research outputs</title>
      <link>https://arxiv.org/abs/2212.02935</link>
      <description>arXiv:2212.02935v3 Announce Type: replace-cross 
Abstract: This article presents a free and open source toolkit that supports the semi-automated checking of research outputs (SACRO) for privacy disclosure within secure data environments. SACRO is a framework that applies best-practice principles-based statistical disclosure control (SDC) techniques on-the-fly as researchers conduct their analyses. SACRO is designed to assist human checkers rather than seeking to replace them as with current automated rules-based approaches. The toolkit is composed of a lightweight Python package that sits over well-known analysis tools that produce outputs such as tables, plots, and statistical models. This package adds functionality to (i) automatically identify potentially disclosive outputs against a range of commonly used disclosure tests; (ii) apply optional disclosure mitigation strategies as requested; (iii) report reasons for applying SDC; and (iv) produce simple summary documents trusted research environment staff can use to streamline their workflow and maintain auditable records. This creates an explicit change in the dynamics so that SDC is something done with researchers rather than to them, and enables more efficient communication with checkers. A graphical user interface supports human checkers by displaying the requested output and results of the checks in an immediately accessible format, highlighting identified issues, potential mitigation options, and tracking decisions made. The major analytical programming languages used by researchers (Python, R, and Stata) are supported by providing front-end packages that interface with the core Python back-end. Source code, packages, and documentation are available under MIT license at https://github.com/AI-SDC/ACRO</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.02935v3</guid>
      <category>cs.CR</category>
      <category>cs.IR</category>
      <category>cs.SE</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Richard J. Preen, Maha Albashir, Simon Davy, Jim Smith</dc:creator>
    </item>
    <item>
      <title>Hierarchical Bayesian Models to Mitigate Systematic Disparities in Prediction with Proxy Outcomes</title>
      <link>https://arxiv.org/abs/2403.00639</link>
      <description>arXiv:2403.00639v2 Announce Type: replace-cross 
Abstract: Label bias occurs when the outcome of interest is not directly observable and instead, modeling is performed with proxy labels. When the difference between the true outcome and the proxy label is correlated with predictors, this can yield systematic disparities in predictions for different groups of interest. We propose Bayesian hierarchical measurement models to address these issues. When strong prior information about the measurement process is available, our approach improves accuracy and helps with algorithmic fairness. If prior knowledge is limited, our approach allows assessment of the sensitivity of predictions to the unknown specifications of the measurement process. This can help practitioners gauge if enough substantive information is available to guarantee the desired accuracy and avoid disparate predictions when using proxy outcomes. We demonstrate our approach through practical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00639v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Mikhaeil, Andrew Gelman, Philip Greengard</dc:creator>
    </item>
  </channel>
</rss>
