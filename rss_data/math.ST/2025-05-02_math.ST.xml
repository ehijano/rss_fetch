<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 May 2025 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Bayesian approach to inverse problems in spaces of measures</title>
      <link>https://arxiv.org/abs/2505.00151</link>
      <description>arXiv:2505.00151v1 Announce Type: new 
Abstract: In this work, we develop a Bayesian framework for solving inverse problems in which the unknown parameter belongs to a space of Radon measures taking values in a separable Hilbert space. The inherent ill-posedness of such problems is addressed by introducing suitable measure-valued priors that encode prior information and promote desired sparsity properties of the parameter. Under appropriate assumptions on the forward operator and noise model, we establish the well-posedness of the Bayesian formulation by proving the existence, uniqueness, and stability of the posterior with respect to perturbations in the observed data. In addition, we also discuss computational strategies for approximating the posterior distribution. Finally, we present some examples that demonstrate the effectiveness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00151v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Phuoc-Truong Huynh</dc:creator>
    </item>
    <item>
      <title>Algebraic Constraints for Linear Acyclic Causal Models</title>
      <link>https://arxiv.org/abs/2505.00215</link>
      <description>arXiv:2505.00215v1 Announce Type: new 
Abstract: In this paper we study the space of second- and third-order moment tensors of random vectors which satisfy a Linear Non-Gaussian Acyclic Model (LiNGAM). In such a causal model each entry $X_i$ of the random vector $X$ corresponds to a vertex $i$ of a directed acyclic graph $G$ and can be expressed as a linear combination of its direct causes $\{X_j: j\to i\}$ and random noise. For any directed acyclic graph $G$, we show that a random vector $X$ arises from a LiNGAM with graph $G$ if and only if certain easy-to-construct matrices, whose entries are second- and third-order moments of $X$, drop rank. This determinantal characterization extends previous results proven for polytrees and generalizes the well-known local Markov property for Gaussian models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00215v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cole Gigliotti, Elina Robeva</dc:creator>
    </item>
    <item>
      <title>Conformal changepoint localization</title>
      <link>https://arxiv.org/abs/2505.00292</link>
      <description>arXiv:2505.00292v1 Announce Type: new 
Abstract: Changepoint localization is the problem of estimating the index at which a change occurred in the data generating distribution of an ordered list of data, or declaring that no change occurred. We present the broadly applicable CONCH (CONformal CHangepoint localization) algorithm, which uses a matrix of conformal p-values to produce a confidence interval for a (single) changepoint under the mild assumption that the pre-change and post-change distributions are each exchangeable. We exemplify the CONCH algorithm on a variety of synthetic and real-world datasets, including using black-box pre-trained classifiers to detect changes in sequences of images or text.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00292v1</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjit Dandapanthula, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>The iterated Dirichlet process and applications to Bayesian inference</title>
      <link>https://arxiv.org/abs/2505.00451</link>
      <description>arXiv:2505.00451v1 Announce Type: new 
Abstract: Consider an i.i.d. sequence of random variables, taking values in some space $S$, whose underlying distribution is unknown. In problems of Bayesian inference, one models this unknown distribution as a random measure, and the law of this random measure is the prior. When $S = \{0, 1\}$, a commonly used prior is the uniform distribution on $[0, 1]$, or more generally, the beta distribution. When $S$ is finite, the analogous choice is the Dirichlet distribution. For a general space $S$, we are led naturally to the Dirichlet process (see [Ferguson, 1973]).
  Here, we consider an array of random variables, and in so doing are led to what we call the iterated Dirichlet process (IDP). We define the IDP and then show how to compute the posterior distribution, given a finite set of observations, using the method of sequential imputation. Ordinarily, this method requires the existence of certain joint density functions, which the IDP lacks. We therefore present a new, more general proof of the validity of sequential imputation, and show that the hypotheses of our proof are satisfied by the IDP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00451v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Evan Donald, Jason Swanson</dc:creator>
    </item>
    <item>
      <title>On the Distribution of the Sample Covariance from a Matrix Normal Population</title>
      <link>https://arxiv.org/abs/2505.00470</link>
      <description>arXiv:2505.00470v1 Announce Type: new 
Abstract: This paper discusses the joint distribution of sample variances and covariances, expressed in quadratic forms in a matrix population arising in comparing the differences among groups under homogeneity of variance. One major concern of this article is to compare $K$ different populations, by assuming that the mean values of $x_{11}^{(k)}, x_{12}^{(k)}, \dots, x_{1p}^{(k)}, x_{21}^{(k)}, x_{22}^{(k)}, \dots$, $x_{2p}^{(k)},\dots, x_{n1}^{(k)},x_{n2}^{(k)},\dots,$ $x_{np}^{(k)}$ in each population are $M^{(k)}$ ($n\times p$), $k = 1,2,\dots,K$ and $M$($n\times p$) a fixed matrix, with this hypothesis $$H_0: M^{(1)} = M^{(2)} = \dots = M^{(k)} = M,$$ when the inter-group covariances are neglected and the intra-group covariances are equal. The $N$ intra-group variances and $\frac{1}{2} N (N - 1)$ intra-group covariances where $N = np$ are classified into four categories $T_{1}$, $T_{1\frac{1}{2}}$, $T_{2}$ and $T_{3}$ according to the spectral forms of the precision matrix. The joint distribution of the sample variances and covariances is derived under these four scenarios. Besides, the moment generating function and the joint distribution of latent roots are explicitly calculated. %The distribution of non-central means with known covariance is calculated as an application to the one-sample analysis of variance, with its exact power tabulated up to order two. As an application, we consider a classification problem in the discriminant analysis where the two populations should have different intra-group covariances. The distribution of the ratio of two quadratic forms is considered both in the central and non-central cases, with their exact power tabulated for different $n$ and $p$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00470v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Haoming Wang</dc:creator>
    </item>
    <item>
      <title>Bayesian Discrepancy Measure: Higher-order and Skewed approximations</title>
      <link>https://arxiv.org/abs/2505.00185</link>
      <description>arXiv:2505.00185v1 Announce Type: cross 
Abstract: The aim of this paper is to discuss both higher-order asymptotic expansions and skewed approximations for the Bayesian Discrepancy Measure for testing precise statistical hypotheses. In particular, we derive results on third-order asymptotic approximations and skewed approximations for univariate posterior distributions, also in the presence of nuisance parameters, demonstrating improved accuracy in capturing posterior shape with little additional computational cost over simple first-order approximations. For the third-order approximations, connections to frequentist inference via matching priors are highlighted. Moreover, the definition of the Bayesian Discrepancy Measure and the proposed methodology are extended to the multivariate setting, employing tractable skew-normal posterior approximations obtained via derivative matching at the mode. Accurate multivariate approximations for the Bayesian Discrepancy Measure are then derived by defining credible regions based on the Optimal Transport map, that transforms the skew-normal approximation to a standard multivariate normal distribution. The performance and practical benefits of these higher-order and skewed approximations are illustrated through two examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00185v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Bortolato, Francesco Bertolino, Monica Musio, Laura Ventura</dc:creator>
    </item>
    <item>
      <title>Inference for max-linear Bayesian networks with noise</title>
      <link>https://arxiv.org/abs/2505.00229</link>
      <description>arXiv:2505.00229v1 Announce Type: cross 
Abstract: Max-Linear Bayesian Networks (MLBNs) provide a powerful framework for causal inference in extreme-value settings; we consider MLBNs with noise parameters with a given topology in terms of the max-plus algebra by taking its logarithm. Then, we show that an estimator of a parameter for each edge in a directed acyclic graph (DAG) is distributed normally. We end this paper with computational experiments with the expectation and maximization (EM) algorithm and quadratic optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00229v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mark Adams, Kamillo Ferry, Ruriko Yoshida</dc:creator>
    </item>
    <item>
      <title>EW D-optimal Designs for Experiments with Mixed Factors</title>
      <link>https://arxiv.org/abs/2505.00629</link>
      <description>arXiv:2505.00629v1 Announce Type: cross 
Abstract: We consider EW D-optimal designs as robust designs for experiments under a general parametric model with discrete and continuous factors. When a pilot study is available, we recommend sample-based EW D-optimal designs for subsequent experiments. Otherwise, we recommend EW D-optimal designs under a prior distribution for model parameters. We propose an EW ForLion algorithm for finding EW D-optimal designs under a general parametric model, and justify that the designs found by our algorithm are EW D-optimal. To facilitate potential users in practice, we also develop a rounding algorithm that converts an approximate design with mixed factors to an exact design with prespecified grid points and the number of experimental units. By applying our algorithms for real experiments under multinomial logistic models or generalized linear models, we show that our designs are highly efficient with respect to locally D-optimal designs and more robust against parameter value misspecifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00629v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siting Lin, Yifei Huang, Jie Yang</dc:creator>
    </item>
    <item>
      <title>Locally minimax optimal and dimension-agnostic discrete argmin inference</title>
      <link>https://arxiv.org/abs/2503.21639</link>
      <description>arXiv:2503.21639v2 Announce Type: replace 
Abstract: This paper tackles a fundamental inference problem: given $n$ observations from a $d$ dimensional vector with unknown mean $\boldsymbol{\mu}$, we must form a confidence set for the index (or indices) corresponding to the smallest component of $\boldsymbol{\mu}$. By duality, we reduce this to testing, for each $r$ in $1,\ldots,d$, whether $\mu_r$ is the smallest. Based on the sample splitting and self-normalization approach of Kim and Ramdas (2024), we propose "dimension-agnostic" tests that maintain validity regardless of how $d$ scales with $n$, and regardless of arbitrary ties in $\boldsymbol{\mu}$. Notably, our validity holds under mild moment conditions, requiring little more than finiteness of a second moment, and permitting possibly strong dependence between coordinates. In addition, we establish the local minimax separation rate for this problem, which adapts to the cardinality of a confusion set, and show that the proposed tests attain this rate. Furthermore, we develop robust variants that continue to achieve the same minimax rate under heavy-tailed distributions with only finite second moments. Empirical results on simulated and real data illustrate the strong performance of our approach in terms of type I error control and power compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21639v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilmun Kim, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Time-Uniform Self-Normalized Concentration for Vector-Valued Processes</title>
      <link>https://arxiv.org/abs/2310.09100</link>
      <description>arXiv:2310.09100v2 Announce Type: replace-cross 
Abstract: Self-normalized processes arise naturally in many learning-related tasks. While self-normalized concentration has been extensively studied for scalar-valued processes, there are few results for multidimensional processes outside of the sub-Gaussian setting. In this work, we construct a general, self-normalized inequality for multivariate processes that satisfy a simple yet broad sub-$\psi$ tail condition, which generalizes assumptions based on cumulant generating functions. From this general inequality, we derive an upper law of the iterated logarithm for sub-$\psi$ vector-valued processes, which is tight up to small constants. We show how our inequality can be leveraged to derive a variety of novel, self-normalized concentration inequalities under both light and heavy-tailed observations. Further, we provide applications in prototypical statistical tasks, such as parameter estimation in online linear regression, autoregressive modeling, and bounded mean estimation via a new (multivariate) empirical Bernstein concentration inequality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.09100v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Justin Whitehouse, Zhiwei Steven Wu, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Orthogonal Causal Calibration</title>
      <link>https://arxiv.org/abs/2406.01933</link>
      <description>arXiv:2406.01933v2 Announce Type: replace-cross 
Abstract: Estimates of heterogeneous treatment effects such as conditional average treatment effects (CATEs) and conditional quantile treatment effects (CQTEs) play an important role in real-world decision making. Given this importance, one should ensure these estimates are calibrated. While there is a rich literature on calibrating estimators of non-causal parameters, very few methods have been derived for calibrating estimators of causal parameters, or more generally estimators of quantities involving nuisance parameters. In this work, we develop general algorithms for reducing the task of causal calibration to that of calibrating a standard (non-causal) predictive model.
  Throughout, we study a notion of calibration defined with respect to an arbitrary, nuisance-dependent loss $\ell$, under which we say an estimator $\theta$ is calibrated if its predictions cannot be changed on any level set to decrease loss. For losses $\ell$ satisfying a condition called universal orthogonality, we present a simple algorithm that transforms partially-observed data into generalized pseudo-outcomes and applies any off-the-shelf calibration procedure. For losses $\ell$ satisfying a weaker assumption called conditional orthogonality, we provide a similar sample splitting algorithm the performs empirical risk minimization over an appropriately defined class of functions. Convergence of both algorithms follows from a generic, two term upper bound of the calibration error of any model. We demonstrate the practical applicability of our results in experiments on both observational and synthetic data. Our results are exceedingly general, showing that essentially any existing calibration algorithm can be used in causal settings, with additional loss only arising from errors in nuisance estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01933v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Justin Whitehouse, Christopher Jung, Vasilis Syrgkanis, Bryan Wilder, Zhiwei Steven Wu</dc:creator>
    </item>
    <item>
      <title>Bayesian Model Averaging in Causal Instrumental Variable Models</title>
      <link>https://arxiv.org/abs/2504.13520</link>
      <description>arXiv:2504.13520v2 Announce Type: replace-cross 
Abstract: Instrumental variables are a popular tool to infer causal effects under unobserved confounding, but choosing suitable instruments is challenging in practice. We propose gIVBMA, a Bayesian model averaging procedure that addresses this challenge by averaging across different sets of instrumental variables and covariates in a structural equation model. Our approach extends previous work through a scale-invariant prior structure and accommodates non-Gaussian outcomes and treatments, offering greater flexibility than existing methods. The computational strategy uses conditional Bayes factors to update models separately for the outcome and treatments. We prove that this model selection procedure is consistent. By explicitly accounting for model uncertainty, gIVBMA allows instruments and covariates to switch roles and provides robustness against invalid instruments. In simulation experiments, gIVBMA outperforms current state-of-the-art methods. We demonstrate its usefulness in two empirical applications: the effects of malaria and institutions on income per capita and the returns to schooling. A software implementation of gIVBMA is available in Julia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13520v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gregor Steiner, Mark Steel</dc:creator>
    </item>
  </channel>
</rss>
