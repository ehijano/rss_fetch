<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Jul 2025 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Optimal two-phase sampling designs for generalized raking estimators with multiple parameters of interest</title>
      <link>https://arxiv.org/abs/2507.16945</link>
      <description>arXiv:2507.16945v1 Announce Type: new 
Abstract: Large observational datasets compiled from electronic health records are a valuable resource for medical research but are often affected by measurement error and misclassification. Valid statistical inference requires proper adjustment for these errors. Two-phase sampling with generalized raking (GR) estimation is an efficient solution to this problem that is robust to complex error structures. In this approach, error-prone variables are observed in a large phase 1 cohort, and a subset is selected in phase 2 for validation with error-free measurements. Previous research has studied optimal phase 2 sampling designs for inverse probability weighted (IPW) estimators in non-adaptive, multi-parameter settings, and for GR estimators in single-parameter settings. In this work, we extend these results by deriving optimal adaptive, multiwave sampling designs for IPW and GR estimators when multiple parameters are of interest. We propose several practical allocation strategies and evaluate their performance through extensive simulations and a data example from the Vanderbilt Comprehensive Care Clinic HIV Study. Our results show that independently optimizing allocation for each parameter improves efficiency over traditional case-control sampling. We also derive an integer-valued, A-optimal allocation method that typically outperforms independent optimization. Notably, we find that optimal designs for GR can differ substantially from those for IPW, and that this distinction can meaningfully affect estimator efficiency in the multiple-parameter setting. These findings offer practical guidance for future two-phase studies using error-prone data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16945v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jasper B. Yang, Bryan E. Shepherd, Thomas Lumley, Pamela A. Shaw</dc:creator>
    </item>
    <item>
      <title>Bayesian Compressed Mixed-Effects Models</title>
      <link>https://arxiv.org/abs/2507.16961</link>
      <description>arXiv:2507.16961v1 Announce Type: new 
Abstract: Penalized likelihood and quasi-likelihood methods dominate inference in high-dimensional linear mixed-effects models. Sampling-based Bayesian inference is less explored due to the computational bottlenecks introduced by the random effects covariance matrix. To address this gap, we propose the compressed mixed-effects (CME) model, which defines a quasi-likelihood using low-dimensional covariance parameters obtained via random projections of the random effects covariance. This dimension reduction, combined with a global-local shrinkage prior on the fixed effects, yields an efficient collapsed Gibbs sampler for prediction and fixed effects selection. Theoretically, when the compression dimension grows slowly relative to the number of fixed effects and observations, the Bayes risk for prediction is asymptotically negligible, ensuring accurate prediction using the CME model. Empirically, the CME model outperforms existing approaches in terms of predictive accuracy, interval coverage, and fixed-effects selection across varied simulation settings and a real-world dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16961v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sreya Sarkar, Kshitij Khare, Sanvesh Srivastava</dc:creator>
    </item>
    <item>
      <title>Modelling longitudinal polytomous animal data using Bayesian hierarchical models</title>
      <link>https://arxiv.org/abs/2507.17004</link>
      <description>arXiv:2507.17004v1 Announce Type: new 
Abstract: The analysis of longitudinal categorical data can be complex and unfeasible due to the number of parameters involved, characterised by overparameterisation leading to model non-convergence, in addition to problems related to sample size and the presence or absence of overdispersion. In this context, we introduce Bayesian hierarchical models as an alternative methodology to classical statistical techniques for analysing nominal polytomous data in longitudinal studies. The theoretical foundation is based on the use of non-informative priors and advanced computational techniques, such as Markov Chain Monte Carlo (MCMC) methods, which enable a robust and flexible data analysis framework. As a motivating example, the procedure is illustrated through an applied study in agrarian science, focusing on animal welfare, which assessed seven types of behaviours exhibited by pigs over twelve weeks. The results demonstrated the efficacy of Bayesian hierarchical models for the analysis of longitudinal nominal polytomous data. Since the computational procedures were implemented in the R software and the codes are available, this work will serve as support for those who need such analyses, especially in agricultural designs, where longitudinal categorical data are frequently encountered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17004v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Let\'icia Salvador, Gabriel Rodrigues Palma, Mariana Coelly Modesto Santos Tavares, Iran Jose Oliveira Silva, Idemauro Antonio Rodrigues de Lara</dc:creator>
    </item>
    <item>
      <title>Efficient Bayesian Inference for Spatial Point Patterns Using the Palm Likelihood</title>
      <link>https://arxiv.org/abs/2507.17065</link>
      <description>arXiv:2507.17065v1 Announce Type: new 
Abstract: Bayesian inference for spatial point patterns is often hindered computationally by intractable likelihoods. In the frequentist literature, estimating equations utilizing pseudolikelihoods have long been used for simulation-free parameter estimation. One such pseudolikelihood based on the process of differences is known as the Palm likelihood. Utilizing notions of Bayesian composite likelihoods and generalized Bayesian inference, we develop a framework for the use of Palm likelihoods in a Bayesian context. Naive implementation of the Palm likelihood results in posterior undercoverage of model parameters. We propose two approaches to remedy this issue and calibrate the resulting posterior. Numerical simulations illustrate both the efficacy of the method in terms of statistical properties and the superiority in terms of computational efficiency when compared to classical Markov chain Monte Carlo. The method is then applied to the popular \textit{Beilschmiedia pendula Lauraceae} dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17065v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kevin M. Collins, Erin M. Schliep</dc:creator>
    </item>
    <item>
      <title>Instability of inverse probability weighting methods and a remedy for non-ignorable missing data</title>
      <link>https://arxiv.org/abs/2507.17137</link>
      <description>arXiv:2507.17137v1 Announce Type: new 
Abstract: Inverse probability weighting (IPW) methods are commonly used to analyze non-ignorable missing data under the assumption of a logistic model for the missingness probability. However, solving IPW equations numerically may involve non-convergence problems when the sample size is moderate and the missingness probability is high. Moreover, those equations often have multiple roots, and identifying the best root is challenging. Therefore, IPW methods may have low efficiency or even produce biased results. We identify the pitfall in these methods pathologically: they involve the estimation of a moment-generating function, and such functions are notoriously unstable in general. As a remedy, we model the outcome distribution given the covariates of the completely observed individuals semiparametrically. After forming an induced logistic regression model for the missingness status of the outcome and covariate, we develop a maximum conditional likelihood method to estimate the underlying parameters. The proposed method circumvents the estimation of a moment-generating function and hence overcomes the instability of IPW methods. Our theoretical and simulation results show that the proposed method outperforms existing competitors greatly. Two real data examples are analyzed to illustrate the advantages of our method. We conclude that if only a parametric logistic regression is assumed but the outcome regression model is left arbitrary, then one has to be cautious in using any of the existing statistical methods in problems involving non-ignorable missing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17137v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1111/biom.13881</arxiv:DOI>
      <arxiv:journal_reference>Biometrics, 79, 3215-3226 (2023)</arxiv:journal_reference>
      <dc:creator>Pengfei Li, Jing Qin, Yukun Liu</dc:creator>
    </item>
    <item>
      <title>Local graph estimation: Interpretable network discovery for complex data</title>
      <link>https://arxiv.org/abs/2507.17172</link>
      <description>arXiv:2507.17172v1 Announce Type: new 
Abstract: Large, complex datasets often include a small set of variables of primary interest, such as clinical outcomes or known biomarkers, whose relation to the broader system is the main focus of analysis. In these situations, exhaustively estimating the entire network may obscure insights into the scientific question at hand. To address this common scenario, we introduce local graph estimation, a statistical framework that focuses on inferring substructures around target variables rather than recovering the full network of inter-variable relationships. We show that traditional graph estimation methods often fail to recover local structure, and present pathwise feature selection (PFS) as an alternative approach. PFS estimates local subgraphs by iteratively applying feature selection and propagating uncertainty along network paths. We prove that PFS provides path discovery with finite-sample false discovery control and yields highly interpretable results, even in settings with mixed variable types and nonlinear dependencies. Applied to two cancer studies -- one analyzing county-level cancer incidence and mortality across the U.S., and another integrating gene, microRNA, protein, and clinical data from The Cancer Genome Atlas -- PFS uncovers biologically plausible networks that reveal both known and novel associations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17172v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Omar Melikechi, David B. Dunson, Noureddine Melikechi, Jeffrey W. Miller</dc:creator>
    </item>
    <item>
      <title>Model-robust standardization in stepped wedge designs</title>
      <link>https://arxiv.org/abs/2507.17190</link>
      <description>arXiv:2507.17190v1 Announce Type: new 
Abstract: Stepped-wedge cluster-randomized trials (SW-CRTs) are widely used in healthcare and implementation science, providing an ethical advantage by ensuring all clusters eventually receive the intervention. The staggered rollout of treatment introduces complexities in defining and estimating treatment effect estimands, particularly under informative sizes. Traditional model-based methods, including generalized estimating equations (GEE) and linear mixed models (LMM), produce estimates that depend on implicit weighting schemes and parametric assumptions, leading to bias for different types of estimands in the presence of informative sizes. While recent methods have attempted to provide robust estimation in SW-CRTs, they are restrictive on modeling assumptions or lack of general framework for consistent estimating multiple estimands under informative size. In this article, we propose a model-robust standardization framework for SW-CRTs that generalizes existing methods from parallel-arm CRTs. We define causal estimands including horizontal-individual, horizontal-cluster, vertical-individual, and vertical-cluster average treatment effects under a super population framework and introduce an augmented standardization estimator that standardizes parametric and semiparametric working models while maintaining robustness to informative cluster size under arbitrary misspecification. We evaluate the finite-sample properties of our proposed estimators through extensive simulation studies, assessing their performance under various SW-CRT designs. Finally, we illustrate the practical application of model-robust estimation through a reanalysis of two real-world SW-CRTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17190v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Fang, Xueqi Wang, Patrick J. Heagerty, Bingkai Wang, Fan Li</dc:creator>
    </item>
    <item>
      <title>Exact, Nonparametric Sensitivity Analysis for Observational Studies of Contingency Tables</title>
      <link>https://arxiv.org/abs/2507.17207</link>
      <description>arXiv:2507.17207v1 Announce Type: new 
Abstract: In observational studies, contingency tables provide a simple and intuitive approach to study associations between categorical variables. However, any test of association in contingency tables may be biased due to unmeasured confounders. Existing sensitivity analyses that assess the impact of unmeasured confounding on association tests typically assume a binary treatment variable or impose strong parametric assumptions on the non-binary treatment variable. We overcome these limitations with an exact (i.e., non-asymptotic) and nonparametric sensitivity analysis for unmeasured confounding in $I \times J$ or $I \times J \times K$ contingency tables, accommodating both non-binary treatment and non-binary outcome. Specifically, we extend Rosenbaum's sensitivity model for generic bias and develop a general method to calculate the exact worst-case null distribution for any permutation-invariant test, which includes the chi-square test and many likelihood-based or score-based tests of association. We also propose specialized results for sub-families of permutation-invariant tests that lead to more efficient computation of the worst-case null distribution. Finally, we show that sensitivity analyses based on tests that utilize all treatment and outcome levels typically have higher power than "naive" approaches that dichotomize the categorical variables. We conclude with a re-analysis of the effect of pre-kindergarten care on math achievement using data from the Early Childhood Longitudinal Study, Kindergarten Class of 1998-1999. An R package, sensitivityIxJ, implements the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17207v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elaine K. Chiu, Hyunseung Kang</dc:creator>
    </item>
    <item>
      <title>Testing Against Tree Ordered Alternatives in One-way ANOVA</title>
      <link>https://arxiv.org/abs/2507.17229</link>
      <description>arXiv:2507.17229v1 Announce Type: new 
Abstract: The likelihood ratio test against a tree ordered alternative in one-way heteroscedastic ANOVA is considered for the first time. Bootstrap is used to implement this and two multiple comparisons based tests and shown to have very good size and power performance.
  In this paper, the problem of testing the homogeneity of mean effects against the tree ordered alternative is considered in the heteroscedastic one-way ANOVA model. The likelihood ratio test and two multiple comparison-based tests - named Max-D and Min-D are proposed and implemented using the parametric bootstrap method. An extensive simulation study shows that these tests effectively control type-I error rates for various choices of sample sizes and error variances. Further, the likelihood ratio and Max-D tests achieve very good powers in all cases. The test Min-D is seen to perform better than the other two for some specific configurations of parameters. The robustness of these tests is investigated by implementing some non-normal distributions, viz., skew-normal, Laplace, exponential, mixture-normal, and t distributions. `R' packages are developed and shared on "Github" for the ease of users. The proposed tests are illustrated on a dataset of patients undergoing psychological treatments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17229v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subha Halder, Anjana Mondal, Somesh Kumar</dc:creator>
    </item>
    <item>
      <title>A principled approach for comparing Variable Importance</title>
      <link>https://arxiv.org/abs/2507.17306</link>
      <description>arXiv:2507.17306v1 Announce Type: new 
Abstract: Variable importance measures (VIMs) aim to quantify the contribution of each input covariate to the predictability of a given output. With the growing interest in explainable AI, numerous VIMs have been proposed, many of which are heuristic in nature. This is often justified by the inherent subjectivity of the notion of importance. This raises important questions regarding usage: What makes a good VIM? How can we compare different VIMs?
  In this paper, we address these questions by: (1) proposing an axiomatic framework that bridges the gap between variable importance and variable selection. This framework formalizes the intuitive principle that features providing no additional information should not be assigned importance. It helps avoid false positives due to spurious correlations, which can arise with popular methods such as Shapley values; and (2) introducing a general pipeline for constructing VIMs, which clarifies the objective of various VIMs and thus facilitates meaningful comparisons. This approach is natural in statistics, but the literature has diverged from it.
  Finally, we provide an extensive set of examples to guide practitioners in selecting and estimating appropriate indices aligned with their specific goals and data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17306v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Angel Reyero-Lobo, Pierre Neuvial, Bertrand Thirion</dc:creator>
    </item>
    <item>
      <title>Balancing utility and cost in dynamic treatment regimes</title>
      <link>https://arxiv.org/abs/2507.17360</link>
      <description>arXiv:2507.17360v1 Announce Type: new 
Abstract: Dynamic treatment regimes (DTRs) are personalized, adaptive strategies designed to guide the sequential allocation of treatments based on individual characteristics over time. Before each treatment assignment, covariate information is collected to refine treatment decisions and enhance their effectiveness. The more information we gather, the more precise our decisions can be. However, this also leads to higher costs during the data collection phase. In this work, we propose a balanced Q-learning method that strikes a balance between the utility of the DTR and the costs associated with both treatment assignment and covariate assessment. The performance of the proposed method is demonstrated through extensive numerical studies, including simulations and a real-data application to the MIMIC-III database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17360v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kai Chen, Zhang Yuqian</dc:creator>
    </item>
    <item>
      <title>Doubly robust outlier resistant inference on causal treatment effect</title>
      <link>https://arxiv.org/abs/2507.17439</link>
      <description>arXiv:2507.17439v1 Announce Type: new 
Abstract: Outliers can severely distort causal effect estimation in observational studies, yet this issue has received limited attention in the literature. Their influence is especially pronounced in small sample sizes, where detecting and removing outliers becomes increasingly difficult. Therefore, it is essential to estimate treatment effects robustly without excluding these influential data points. To address this, we propose a doubly robust point estimator for the average treatment effect under a contaminated model that includes outliers. Robustness in outcome regression is achieved through a robust estimating equation, while covariate balancing propensity scores (CBPS) ensure resilience in propensity score modeling.
  To prevent model overfitting due to the inclusion of numerous parameters, we incorporate variable selection. All these components are unified under a penalized empirical likelihood framework. For confidence interval estimation, most existing approaches rely on asymptotic properties, which may be unreliable in finite samples. We derive an optimal finite-sample confidence interval for the average treatment effect using our proposed estimating equation, ensuring that the interval bounds remain unaffected by outliers. Through simulations and a real-world application involving hypertension data with outliers, we demonstrate that our method consistently outperforms existing approaches in both accuracy and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17439v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Joonsung Kang</dc:creator>
    </item>
    <item>
      <title>Nonparametric inference for nonstationary spatial point processes</title>
      <link>https://arxiv.org/abs/2507.17600</link>
      <description>arXiv:2507.17600v1 Announce Type: new 
Abstract: Point pattern data often exhibit features such as abrupt changes, hotspots and spatially varying dependence in local intensity. Under a Poisson process framework, these correspond to discontinuities and nonstationarity in the underlying intensity function -- features that are difficult to capture with standard modeling approaches. This paper proposes a spatial Cox process model in which nonstationarity is induced through a random partition of the spatial domain, with conditionally independent Gaussian process priors specified across the resulting regions. This construction allows for heterogeneous spatial behavior, including sharp transitions in intensity. To ensure exact inference, a discretization-free MCMC algorithm is developed to target the infinite-dimensional posterior distribution without approximation. The random partition framework also reduces the computational burden typically associated with Gaussian process models. Spatial covariates can be incorporated to account for structured variation in intensity. The proposed methodology is evaluated through synthetic examples and real-world applications, demonstrating its ability to flexibly capture complex spatial structures. The paper concludes with a discussion of potential extensions and directions for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17600v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Izabel Nolau, Fl\'avio B. Gon\c{c}alves, Dani Gamerman</dc:creator>
    </item>
    <item>
      <title>Optimal differentially private kernel learning with random projection</title>
      <link>https://arxiv.org/abs/2507.17544</link>
      <description>arXiv:2507.17544v1 Announce Type: cross 
Abstract: Differential privacy has become a cornerstone in the development of privacy-preserving learning algorithms. This work addresses optimizing differentially private kernel learning within the empirical risk minimization (ERM) framework. We propose a novel differentially private kernel ERM algorithm based on random projection in the reproducing kernel Hilbert space using Gaussian processes. Our method achieves minimax-optimal excess risk for both the squared loss and Lipschitz-smooth convex loss functions under a local strong convexity condition. We further show that existing approaches based on alternative dimension reduction techniques, such as random Fourier feature mappings or $\ell_2$ regularization, yield suboptimal generalization performance. Our key theoretical contribution also includes the derivation of dimension-free generalization bounds for objective perturbation-based private linear ERM -- marking the first such result that does not rely on noisy gradient-based mechanisms. Additionally, we obtain sharper generalization bounds for existing differentially private kernel ERM algorithms. Empirical evaluations support our theoretical claims, demonstrating that random projection enables statistically efficient and optimally private kernel learning. These findings provide new insights into the design of differentially private algorithms and highlight the central role of dimension reduction in balancing privacy and utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17544v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bonwoo Lee, Cheolwoo Park, Jeongyoun Ahn</dc:creator>
    </item>
    <item>
      <title>Rank-adaptive covariance testing with applications to genomics and neuroimaging</title>
      <link>https://arxiv.org/abs/2309.10284</link>
      <description>arXiv:2309.10284v4 Announce Type: replace 
Abstract: In biomedical studies, testing for differences in covariance offers scientific insights beyond mean differences, especially when differences are driven by complex joint behavior between features. However, when differences in joint behavior are weakly dispersed across many dimensions and arise from differences in low-rank structures within the data, as is often the case in genomics and neuroimaging, existing two-sample covariance testing methods may suffer from power loss. The Ky-Fan(k) norm, defined by the sum of the top Ky-Fan(k) singular values, is a simple and intuitive matrix norm able to capture signals caused by differences in low-rank structures between matrices, but its statistical properties in hypothesis testing have not been studied well. In this paper, we investigate the behavior of the Ky-Fan(k) norm in two-sample covariance testing. Ultimately, we propose a novel methodology, Rank-Adaptive Covariance Testing (RACT), which is able to leverage differences in low-rank structures found in the covariance matrices of two groups in order to maximize power. RACT uses permutation for statistical inference, ensuring an exact Type I error control. We validate RACT in simulation studies and evaluate its performance when testing for differences in gene expression networks between two types of lung cancer, as well as testing for covariance heterogeneity in diffusion tensor imaging (DTI) data taken on two different scanner types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10284v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David Veitch, Yinqiu He, Jun Young Park</dc:creator>
    </item>
    <item>
      <title>Asymptotic and compound e-values: multiple testing and empirical Bayes</title>
      <link>https://arxiv.org/abs/2409.19812</link>
      <description>arXiv:2409.19812v4 Announce Type: replace 
Abstract: We explicitly define the notions of (bona fide, approximate or asymptotic) compound p-values and e-values, which have been implicitly presented and used in the recent multiple testing literature. While it is known that the e-BH procedure with compound e-values controls the FDR, we show the converse: every FDR controlling procedure can be recovered by instantiating the e-BH procedure with certain compound e-values. Since compound e-values are closed under averaging, this allows for combination and derandomization of arbitrary FDR procedures. We then connect compound e-values to empirical Bayes. In particular, we use the fundamental theorem of compound decision theory to derive the log-optimal simple separable compound e-value for testing a set of point nulls against point alternatives: it is a ratio of mixture likelihoods. As one example, we construct asymptotic compound e-values for multiple t-tests, where the (nuisance) variances may be different across hypotheses. Our construction may be interpreted as a data-driven instantiation of the optimal discovery procedure, and our results provide the first type-I error guarantees for the same, along with significant power gains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19812v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolaos Ignatiadis, Ruodu Wang, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Moving sum procedure for multiple change point detection in large factor models</title>
      <link>https://arxiv.org/abs/2410.02918</link>
      <description>arXiv:2410.02918v2 Announce Type: replace 
Abstract: This paper proposes a moving sum methodology for detecting multiple change points in high-dimensional time series under a factor model, where changes are attributed to those in loadings as well as emergence or disappearance of factors. We establish the asymptotic null distribution of the proposed test for family-wise error control, and show the consistency of the procedure for multiple change point estimation. Simulation studies and an application to a large dataset of volatilities demonstrate the competitive performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02918v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Barigozzi, Haeran Cho, Lorenzo Trapani</dc:creator>
    </item>
    <item>
      <title>Conformal Survival Bands for Risk Screening under Right-Censoring</title>
      <link>https://arxiv.org/abs/2505.04568</link>
      <description>arXiv:2505.04568v3 Announce Type: replace 
Abstract: We propose a method to quantify uncertainty around individual survival distribution estimates using right-censored data, compatible with any survival model. Unlike classical confidence intervals, the survival bands produced by this method offer predictive rather than population-level inference, making them useful for personalized risk screening. For example, in a low-risk screening scenario, they can be applied to flag patients whose survival band at 12 months lies entirely above 50\%, while ensuring that at least half of flagged individuals will survive past that time on average. Our approach builds on recent advances in conformal inference and integrates ideas from inverse probability of censoring weighting and multiple testing with false discovery rate control. We provide asymptotic guarantees and show promising performance in finite samples with both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04568v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Sesia, Vladimir Svetnik</dc:creator>
    </item>
    <item>
      <title>Bayesian Generalized Nonlinear Models Offer Basis Free SINDy With Model Uncertainty</title>
      <link>https://arxiv.org/abs/2507.06776</link>
      <description>arXiv:2507.06776v2 Announce Type: replace 
Abstract: Sparse Identification of Nonlinear Dynamics (SINDy) has become a standard methodology for inferring governing equations of dynamical systems from observed data using statistical modeling. However, classical SINDy approaches rely on predefined libraries of candidate functions to model nonlinearities, which limits flexibility and excludes robust uncertainty quantification. This paper proposes Bayesian Generalized Nonlinear Models (BGNLMs) as a principled alternative for more flexible statistical modeling. BGNLMs employ spike-and-slab priors combined with binary inclusion indicators to automatically discover relevant nonlinearities without predefined basis functions. Moreover, BGNLMs quantify uncertainty in selected bases and final model predictions, enabling robust exploration of the model space. In this paper, the BGNLM framework is applied to several three-dimensional (3D) SINDy problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06776v2</guid>
      <category>stat.ME</category>
      <category>math.DS</category>
      <category>stat.CO</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>https://iwsm2025.ie/wp-content/uploads/2025/07/IWSM2025_Limerick_Proceedings.pdf</arxiv:journal_reference>
      <dc:creator>Aliaksandr Hubin</dc:creator>
    </item>
    <item>
      <title>Robust Bayesian high-dimensional variable selection and inference with the horseshoe family of priors</title>
      <link>https://arxiv.org/abs/2507.10975</link>
      <description>arXiv:2507.10975v2 Announce Type: replace 
Abstract: Frequentist robust variable selection has been extensively investigated in high-dimensional regression. Despite success, developing the corresponding statistical inference procedures remains a challenging task. Recently, tackling this challenge from a Bayesian perspective has received much attention. In literature, the two-group spike-and-slab priors that can induce exact sparsity have been demonstrated to yield valid inference in robust sparse linear models. Nevertheless, another important category of sparse priors, the horseshoe family of priors, including horseshoe, horseshoe+, and regularized horseshoe priors, has not yet been examined in robust high-dimensional regression by far. Their performance in variable selection and especially statistical inference in the presence of heavy-tailed model errors is not well understood. In this paper, we address the question by developing robust Bayesian hierarchical models utilizing the horseshoe family of priors along with an efficient Gibbs sampling scheme. We show that compared with competing methods with alternative sampling strategies such as slice sampling, our proposals lead to superior performance in variable selection, Bayesian estimation and statistical inference. In particular, our numeric studies indicate that even without imposing exact sparsity, the one-group horseshoe priors can still yield valid Bayesian credible intervals under robust high-dimensional linear regression models. Applications of the proposed and alternative methods on real data further illustrates the advantage of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10975v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Fan, Srijana Subedi, Vishmi Ridmika Dissanayake Pathiranage, Cen Wu</dc:creator>
    </item>
    <item>
      <title>Fast post-process Bayesian inference with Variational Sparse Bayesian Quadrature</title>
      <link>https://arxiv.org/abs/2303.05263</link>
      <description>arXiv:2303.05263v4 Announce Type: replace-cross 
Abstract: In applied Bayesian inference scenarios, users may have access to a large number of pre-existing model evaluations, for example from maximum-a-posteriori (MAP) optimization runs. However, traditional approximate inference techniques make little to no use of this available information. We propose the framework of post-process Bayesian inference as a means to obtain a quick posterior approximation from existing target density evaluations, with no further model calls. Within this framework, we introduce Variational Sparse Bayesian Quadrature (VSBQ), a method for post-process approximate inference for models with black-box and potentially noisy likelihoods. VSBQ reuses existing target density evaluations to build a sparse Gaussian process (GP) surrogate model of the log posterior density function. Subsequently, we leverage sparse-GP Bayesian quadrature combined with variational inference to achieve fast approximate posterior inference over the surrogate. We validate our method on challenging synthetic scenarios and real-world applications from computational neuroscience. The experiments show that VSBQ builds high-quality posterior approximations by post-processing existing optimization traces, with no further model evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.05263v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengkun Li, Gr\'egoire Clart\'e, Martin J{\o}rgensen, Luigi Acerbi</dc:creator>
    </item>
    <item>
      <title>MCMC Importance Sampling via Moreau-Yosida Envelopes</title>
      <link>https://arxiv.org/abs/2501.02228</link>
      <description>arXiv:2501.02228v2 Announce Type: replace-cross 
Abstract: Non-differentiable priors are standard in modern parsimonious Bayesian models. Lack of differentiability, however, precludes gradient-based Markov chain Monte Carlo (MCMC) for posterior sampling. Recently proposed proximal MCMC approaches can partially remedy this limitation by using a differentiable approximation, constructed via Moreau-Yosida (MY) envelopes, to make proposals. In this work, we build an importance sampling paradigm by using the MY envelope as an importance distribution. Leveraging properties of the envelope, we establish asymptotic normality of the importance sampling estimator with an explicit expression for the asymptotic covariance matrix. Since the MY envelope density is smooth, it is amenable to gradient-based samplers. We provide sufficient conditions for geometric ergodicity of Metropolis-adjusted Langevin and Hamiltonian Monte Carlo algorithms, sampling from this importance distribution. Our numerical studies show that the proposed scheme can yield lower variance estimators compared to existing proximal MCMC alternatives, and is effective in low and high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02228v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Apratim Shukla, Dootika Vats, Eric C. Chi</dc:creator>
    </item>
  </channel>
</rss>
