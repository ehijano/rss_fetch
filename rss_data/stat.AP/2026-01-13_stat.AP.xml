<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 Jan 2026 05:00:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Cauchy-Gaussian Overbound for Heavy-tailed GNSS Measurement Errors</title>
      <link>https://arxiv.org/abs/2601.07299</link>
      <description>arXiv:2601.07299v1 Announce Type: new 
Abstract: Overbounds of heavy-tailed measurement errors are essential to meet stringent navigation requirements in integrity monitoring applications. This paper proposes to leverage the bounding sharpness of the Cauchy distribution in the core and the Gaussian distribution in the tails to tightly bound heavy-tailed GNSS measurement errors. We develop a procedure to determine the overbounding parameters for both symmetric unimodal (s.u.) and not symmetric unimodal (n.s.u.) heavy-tailed errors and prove that the overbounding property is preserved through convolution. The experiment results on both simulated and real-world datasets reveal that our method can sharply bound heavy-tailed errors at both core and tail regions. In the position domain, the proposed method reduces the average vertical protection level by 15% for s.u. heavy-tailed errors compared to the single-CDF Gaussian overbound, and by 21% to 47% for n.s.u. heavy-tailed errors compared to the Navigation Discrete ENvelope and two-step Gaussian overbounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07299v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengdao Li, Penggao Yan, Weisong Wen, Li-Ta Hsu</dc:creator>
    </item>
    <item>
      <title>Bayesian Handwriting Evidence Evaluation using MANOVA via Fourier-Based Extracted Features</title>
      <link>https://arxiv.org/abs/2601.07534</link>
      <description>arXiv:2601.07534v1 Announce Type: new 
Abstract: This paper proposes a novel statistical approach that aims at the identification of valid and useful patterns in handwriting examination via Bayesian modeling. Starting from a sample of characters selected among 13 French native writers, an accurate loop reconstruction can be achieved through Fourier analysis. The contour shape of handwritten characters can be described by the first four pairs of Fourier coefficients and by the surface size. Six Bayesian models are considered for such handwritten features. These models arise from two likelihood structures: (a) a multivariate Normal model, and (b) a MANOVA model that accounts for character-level variability. For each likelihood, three different prior formulations are examined, resulting in distinct Bayesian models: (i) a conjugate Normal-Inverse-Wishart prior, (ii) a hierarchical Normal-Inverse-Wishart prior, and (iii) a Normal-LogNormal-LKJ prior specification. The hierarchical prior formulations are of primary interest because they can incorporate the between-writers variability, a distinguishing element that sets writers apart. These approaches do not allow calculation of the marginal likelihood in a closed-form expression. Therefore, bridge sampling is used to estimate it. The Bayes factor is estimated to compare the performance of the proposed models and to evaluate their efficiency for discriminating purposes. Bayesian MANOVA with Normal-LogNormal-LKJ prior showed an overall better performance, in terms of discriminatory capacity and model fitting. Finally, a sensitivity analysis for the elicitation of the prior distribution parameters is performed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07534v1</guid>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lampis Tzai, Ioannis Ntzoufras, Silvia Bozza</dc:creator>
    </item>
    <item>
      <title>An evaluation of empirical equations for assessing local scour around bridge piers using global sensitivity analysis</title>
      <link>https://arxiv.org/abs/2601.07594</link>
      <description>arXiv:2601.07594v1 Announce Type: new 
Abstract: Bridge scour is a complex phenomenon combining hydrological, geotechnical and structural processes. Bridge scour is the leading cause of bridge collapse, which can bring catastrophic consequences including the loss of life. Estimating scour on bridges is an important task for engineers assessing bridge system performance. Overestimation of scour depths during design may lead to excess spendings on construction whereas underestimation can lead to the collapse of a bridge. Many empirical equations have been developed over the years to assess scour depth at bridge piers. These equations have only been calibrated with laboratory data or very few field data. This paper compares eight equations including the UK CIRIA C742 approach to establish their accuracy using the open access USGS pier-scour database for both field and laboratory conditions. A one-at-the-time sensitivity assessment and a global sensitivity analysis were then applied to identify the most significant parameters in the eight scour equations. The paper shows that using a global approach, i.e. one where all parameters are varied simultaneously, provides more insights than a traditional one-at-the-time approach. The main findings are that the CIRIA and Froehlich equations are the most accurate equations for field conditions, and that angle of attack, pier shape and the approach flow depth are the most influential parameters. Efforts to reduce uncertainty of these three parameters would maximise increase of scour estimate precision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07594v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gianna Gavriel, Maria Pregnolato, Francesca Pianosi, Theo Tryfonas, Paul Vardanega</dc:creator>
    </item>
    <item>
      <title>The Role of Confounders and Linearity in Ecological Inference: A Reassessment</title>
      <link>https://arxiv.org/abs/2601.07668</link>
      <description>arXiv:2601.07668v1 Announce Type: new 
Abstract: Estimating conditional means using only the marginal means available from aggregate data is commonly known as the ecological inference problem (EI). We provide a reassessment of EI, including a new formalization of identification conditions and a demonstration of how these conditions fail to hold in common cases. The identification conditions reveal that, similar to causal inference, credible ecological inference requires controlling for confounders. The aggregation process itself creates additional structure to assist in estimation by restricting the conditional expectation function to be linear in the predictor variable. A linear model perspective also clarifies the differences between the EI methods commonly used in the literature, and when they lead to ecological fallacies. We provide an overview of new methodology which builds on both the identification and linearity results to flexibly control for confounders and yield improved ecological inferences. Finally, using datasets for common EI problems in which the ground truth is fortuitously observed, we show that, while covariates can help, all methods are prone to overestimating both racial polarization and nationalized partisan voting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07668v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiro Kuriwaki, Cory McCartan</dc:creator>
    </item>
    <item>
      <title>Performance of models for monitoring sustainable development goals from remote sensing: A three-level meta-regression</title>
      <link>https://arxiv.org/abs/2601.06178</link>
      <description>arXiv:2601.06178v1 Announce Type: cross 
Abstract: Machine learning (ML) is a tool to exploit remote sensing data for the monitoring and implementation of the United Nations' Sustainable Development Goals (SDGs). In this paper, we report on a meta-analysis to evaluate the performance of ML applied to remote sensing data to monitor SDGs. Specifically, we aim to 1) estimate the average performance; 2) determine the degree of heterogeneity between and within studies; and 3) assess how study features influence model performance. Using PRISMA guidelines, a search was performed across multiple academic databases to identify potentially relevant studies. A random sample of 200 was screened by three reviewers, resulting in 86 trials within 20 studies with 14 study features. Overall accuracy was the most reported performance metric. It was analyzed using double arcsine transformation and a three-level random effects model. The average overall accuracy of the best model was 0.90 [0.86, 0.92]. There was considerable heterogeneity in model performance, 64% of which was between studies. The only significant feature was the prevalence of the majority class, which explained 61% of the between-study heterogeneity. None of the other thirteen features added value to the model. The most important contributions of this paper are the following two insights. 1) Overall accuracy is the most popular performance metric, yet arguably the least insightful. Its sensitivity to class imbalance makes it necessary to normalize it, which is far from common practice. 2) The field needs to standardize the reporting. Reporting of the confusion matrix for independent test sets is the most important ingredient for between-study comparisons of ML classifiers. These findings underscore the need for robust and comparable evaluation metrics in machine learning applications to ensure reliable and actionable insights for effective SDG monitoring and policy formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06178v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Klingwort, Nina M. Leach, Joep Burger</dc:creator>
    </item>
    <item>
      <title>Time-Series Anomaly Classification for Launch Vehicle Propulsion Systems: Fast Statistical Detectors Enhancing LSTM Accuracy and Data Quality</title>
      <link>https://arxiv.org/abs/2601.06186</link>
      <description>arXiv:2601.06186v1 Announce Type: cross 
Abstract: Supporting Go/No-Go decisions prior to launch requires assessing real-time telemetry data against redline limits established during the design qualification phase. Family data from ground testing or previous flights is commonly used to detect initiating failure modes and their timing; however, this approach relies heavily on engineering judgment and is more error-prone for new launch vehicles. To address these limitations, we utilize Long-Term Short-Term Memory (LSTM) networks for supervised classification of time-series anomalies. Although, initial training labels derived from simulated anomaly data may be suboptimal due to variations in anomaly strength, anomaly settling times, and other factors. In this work, we propose a novel statistical detector based on the Mahalanobis distance and forward-backward detection fractions to adjust the supervised training labels. We demonstrate our method on digital twin simulations of a ground-stage propulsion system with 20.8 minutes of operation per trial and O(10^8) training timesteps. The statistical data relabeling improved precision and recall of the LSTM classifier by 7% and 22% respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06186v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sean P. Engelstad, Sameul R. Darr, Matthew Taliaferro, Vinay K. Goyal</dc:creator>
    </item>
    <item>
      <title>From Lagging to Leading: Validating Hard Braking Events as High-Density Indicators of Segment Crash Risk</title>
      <link>https://arxiv.org/abs/2601.06327</link>
      <description>arXiv:2601.06327v1 Announce Type: cross 
Abstract: Identifying high crash risk road segments and accurately predicting crash incidence is fundamental to implementing effective safety countermeasures. While collision data inherently reflects risk, the infrequency and inconsistent reporting of crashes present a major challenge to robust risk prediction models. The proliferation of connected vehicle technology offers a promising avenue to leverage high-density safety metrics for enhanced crash forecasting. A Hard-Braking Event (HBE), interpreted as an evasive maneuver, functions as a potent proxy for elevated driving risk due to its demonstrable correlation with underlying crash causal factors. Crucially, HBE data is significantly more readily available across the entire road network than conventional collision records. This study systematically evaluated the correlation at individual road segment level between police-reported collisions and aggregated and anonymized HBEs identified via the Google Android Auto platform, utilizing datasets from California and Virginia. Empirical evidence revealed that HBEs occur at a rate magnitudes higher than traffic crashes. Employing the state-of-the-practice Negative-Binomial regression models, the analysis established a statistically significant positive correlation between the HBE rate and the crash rate: road segments exhibiting a higher frequency of HBEs were consistently associated with a greater incidence of crashes. This sophisticated model incorporated and controlled for various confounding factors, including road type, speed profile, proximity to ramps, and road segment slope. The HBEs derived from connected vehicle technology thus provide a scalable, high-density safety surrogate metric for network-wide traffic safety assessment, with the potential to optimize safer routing recommendations and inform the strategic deployment of active safety countermeasures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06327v1</guid>
      <category>cs.OH</category>
      <category>stat.AP</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yechen Li, Shantanu Shahane, Shoshana Vasserman, Carolina Osorio, Yi-fan Chen, Ivan Kuznetsov, Kristin White, Justyna Swiatkowska, Neha Arora, Feng Guo</dc:creator>
    </item>
    <item>
      <title>The Promise of Time-Series Foundation Models for Agricultural Forecasting: Evidence from Marketing Year Average Prices</title>
      <link>https://arxiv.org/abs/2601.06371</link>
      <description>arXiv:2601.06371v1 Announce Type: cross 
Abstract: Forecasting agricultural markets remains a core challenge in business analytics, where nonlinear dynamics, structural breaks, and sparse data have historically limited the gains from increasingly complex econometric and machine learning models. As a result, a long-standing belief in the literature is that simple time-series methods often outperform more advanced alternatives. This paper provides the first systematic evidence that this belief no longer holds in the modern era of time-series foundation models (TSFMs). Using USDA ERS data from 1997-2025, we evaluate 17 forecasting approaches across four model classes, assessing monthly forecasting performance and benchmarking against Market Year Average (MYA) price predictions. This period spans multiple agricultural cycles, major policy changes, and major market disruptions, with substantial cross-commodity price volatility. Focusing on five state-of-the-art TSFMs, we show that zero-shot foundation models (with only historical prices and without any additional covariates) consistently outperform traditional time-series methods, machine learning models, and deep learning architectures trained from scratch. Among them, Time-MoE delivers the largest accuracy gains, improving forecasts by 45% (MAE) overall and by more than 50% for corn and soybeans relative to USDA benchmarks. These results point to a paradigm shift in agricultural forecasting: while earlier generations of advanced models struggled to surpass simple benchmarks, modern pre-trained foundation models achieve substantial and robust improvements, offering a scalable and powerful new framework for highstakes predictive analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06371v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Le Wang, Boyuan Zhang</dc:creator>
    </item>
    <item>
      <title>Efficient Data Reduction Via PCA-Guided Quantile Based Sampling</title>
      <link>https://arxiv.org/abs/2601.06375</link>
      <description>arXiv:2601.06375v1 Announce Type: cross 
Abstract: In large-scale statistical modeling, reducing data size through subsampling is essential for balancing computational efficiency and statistical accuracy. We propose a new method, Principal Component Analysis guided Quantile Sampling (PCA-QS), which projects data onto principal components and applies quantile-based sampling to retain representative and diverse subsets. Compared with uniform random sampling, leverage score sampling, and coreset methods, PCA-QS consistently achieves lower mean squared error and better preservation of key data characteristics, while also being computationally efficient. This approach is adaptable to a variety of data scenarios and shows strong potential for broad applications in statistical computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06375v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Foo Hui-Mean, Yuan-chin Ivan Chang</dc:creator>
    </item>
    <item>
      <title>Detecting LLM-Generated Text with Performance Guarantees</title>
      <link>https://arxiv.org/abs/2601.06586</link>
      <description>arXiv:2601.06586v1 Announce Type: cross 
Abstract: Large language models (LLMs) such as GPT, Claude, Gemini, and Grok have been deeply integrated into our daily life. They now support a wide range of tasks -- from dialogue and email drafting to assisting with teaching and coding, serving as search engines, and much more. However, their ability to produce highly human-like text raises serious concerns, including the spread of fake news, the generation of misleading governmental reports, and academic misconduct. To address this practical problem, we train a classifier to determine whether a piece of text is authored by an LLM or a human. Our detector is deployed on an online CPU-based platform https://huggingface.co/spaces/stats-powered-ai/StatDetectLLM, and contains three novelties over existing detectors: (i) it does not rely on auxiliary information, such as watermarks or knowledge of the specific LLM used to generate the text; (ii) it more effectively distinguishes between human- and LLM-authored text; and (iii) it enables statistical inference, which is largely absent in the current literature. Empirically, our classifier achieves higher classification accuracy compared to existing detectors, while maintaining type-I error control, high statistical power, and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06586v1</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongyi Zhou, Jin Zhu, Ying Yang, Chengchun Shi</dc:creator>
    </item>
    <item>
      <title>R-Estimation with Right-Censored Data</title>
      <link>https://arxiv.org/abs/2601.06685</link>
      <description>arXiv:2601.06685v1 Announce Type: cross 
Abstract: This paper considers the problem of directly generalizing the R-estimator under a linear model formulation with right-censored outcomes. We propose a natural generalization of the rank and corresponding estimating equation for the R-estimator in the case of the Wilcoxon (i.e., linear-in-ranks) score function, and show how it can respectively be exactly represented as members of the classes of estimating equations proposed in Ritov (1990) and Tsiatis (1990). We then establish analogous results for a large class of bounded nonlinear-in-ranks score functions. Asymptotics and variance estimation are obtained as straightforward consequences of these representation results. The self-consistent estimator of the residual distribution function, and the mid-cumulative distribution function (and, where needed, a generalization of it), play critical roles in these developments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06685v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Glen A. Satten, Mo Li, Ni Zhao, Robert L. Strawderman</dc:creator>
    </item>
    <item>
      <title>Explainability of Complex AI Models with Correlation Impact Ratio</title>
      <link>https://arxiv.org/abs/2601.06701</link>
      <description>arXiv:2601.06701v1 Announce Type: cross 
Abstract: Complex AI systems make better predictions but often lack transparency, limiting trustworthiness, interpretability, and safe deployment. Common post hoc AI explainers, such as LIME, SHAP, HSIC, and SAGE, are model agnostic but are too restricted in one significant regard: they tend to misrank correlated features and require costly perturbations, which do not scale to high dimensional data. We introduce ExCIR (Explainability through Correlation Impact Ratio), a theoretically grounded, simple, and reliable metric for explaining the contribution of input features to model outputs, which remains stable and consistent under noise and sampling variations. We demonstrate that ExCIR captures dependencies arising from correlated features through a lightweight single pass formulation. Experimental evaluations on diverse datasets, including EEG, synthetic vehicular data, Digits, and Cats-Dogs, validate the effectiveness and stability of ExCIR across domains, achieving more interpretable feature explanations than existing methods while remaining computationally efficient. To this end, we further extend ExCIR with an information theoretic foundation that unifies the correlation ratio with Canonical Correlation Analysis under mutual information bounds, enabling multi output and class conditioned explainability at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06701v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Poushali Sengupta, Rabindra Khadka, Sabita Maharjan, Frank Eliassen, Yan Zhang, Shashi Raj Pandey, Pedro G. Lind, Anis Yazidi</dc:creator>
    </item>
    <item>
      <title>A PDE approach for the invariant measure of stochastic oscillators with hysteresis</title>
      <link>https://arxiv.org/abs/2601.07039</link>
      <description>arXiv:2601.07039v1 Announce Type: cross 
Abstract: This paper presents a PDE approach as an alternative to Monte Carlo simulations for computing the invariant measure of a white-noise-driven bilinear oscillator with hysteresis. This model is widely used in engineering to represent highly nonlinear dynamics, such as the Bauschinger effect. The study extends the stochastic elasto-plastic framework of Bensoussan et al. [SIAM J. Numer. Anal. 47 (2009), pp. 3374--3396] from the two-dimensional elasto-perfectly-plastic oscillator to the three-dimensional bilinear elasto-plastic oscillator. By constructing an appropriate Lyapunov function, the existence of an invariant measure is established. This extension thus enables the modelling of richer hysteretic behavior and broadens the scope of PDE alternatives to Monte Carlo methods. Two applications demonstrate the method's efficiency: calculating the oscillator's threshold crossing frequency (providing an alternative to Rice's formula) and probability of serviceability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07039v1</guid>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>stat.AP</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lihong Guo, Harry L. F. Ip, Mingyang Wang</dc:creator>
    </item>
    <item>
      <title>Making Absence Visible: The Roles of Reference and Prompting in Recognizing Missing Information</title>
      <link>https://arxiv.org/abs/2601.07234</link>
      <description>arXiv:2601.07234v1 Announce Type: cross 
Abstract: Interactive systems that explain data, or support decision making often emphasize what is present while overlooking what is expected but missing. This presence bias limits users' ability to form complete mental models of a dataset or situation. Detecting absence depends on expectations about what should be there, yet interfaces rarely help users form such expectations. We present an experimental study examining how reference framing and prompting influence people's ability to recognize expected but missing categories in datasets. Participants compared distributions across three domains (energy, wealth, and regime) under two reference conditions: Global, presenting a unified population baseline, and Partial, showing several concrete exemplars. Results indicate that absence detection was higher with Partial reference than with Global reference, suggesting that partial, samples-based framing can support expectation formation and absence detection. When participants were prompted to look for what was missing, absence detection rose sharply. We discuss implications for interactive user interfaces and expectation-based visualization design, while considering cognitive trade-offs of reference structures and guided attention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07234v1</guid>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hagit Ben Shoshan, Joel Lanir, Pavel Goldstein, Osnat Mokryn</dc:creator>
    </item>
    <item>
      <title>Compounded Linear Failure Rate Distribution: Properties, Simulation and Analysis</title>
      <link>https://arxiv.org/abs/2601.07249</link>
      <description>arXiv:2601.07249v1 Announce Type: cross 
Abstract: This paper proposes a new extension of the linear failure rate (LFR) model to better capture real-world lifetime data. The model incorporates an additional shape parameter to increase flexibility. It helps model the minimum survival time from a set of LFR distributed variables. We define the model, derive certain statistical properties such as the mean residual life, the mean inactivity time, moments, quantile, order statistics and also discuss the results on stochastic orders of the proposed distribution. The proposed model has increasing, bathtub shaped and inverse bathtub shaped hazard rate function. We use the method of maximum likelihood estimation to estimate the unknown parameters. We conduct simulation studies to examine the behavior of the estimators. We also use three real datasets to evaluate the model, which turns out superior compared to classical alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07249v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suchismita Das, Akul Ameya, Cahyani Karunia Putri</dc:creator>
    </item>
    <item>
      <title>Connections as treatment: causal inference with edge interventions in networks</title>
      <link>https://arxiv.org/abs/2601.07267</link>
      <description>arXiv:2601.07267v1 Announce Type: cross 
Abstract: Causal inference has traditionally focused on interventions at the unit level. In many applications, however, the central question concerns the causal effects of connections between units, such as transportation links, social relationships, or collaborative ties. We develop a causal framework for edge interventions in networks, where treatments correspond to the presence or absence of edges. Our framework defines causal estimands under stochastic interventions on the network structure and introduces an inverse probability weighting estimator under an unconfoundedness assumption on edge assignment. We estimate edge probabilities using exponential random graph models, a widely used class of network models. We establish consistency and asymptotic normality of the proposed estimator. Finally, we apply our methodology to China's transportation network to estimate the causal impact of railroad connections on regional economic development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07267v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuli Chen, Jie Hu, Zhichao Jiang</dc:creator>
    </item>
    <item>
      <title>Penalized Likelihood Optimization for Adaptive Neighborhood Clustering in Time-to-Event Data with Group-Level Heterogeneity</title>
      <link>https://arxiv.org/abs/2601.07446</link>
      <description>arXiv:2601.07446v1 Announce Type: cross 
Abstract: The identification of patient subgroups with comparable event-risk dynamics plays a key role in supporting informed decision-making in clinical research. In such settings, it is important to account for the inherent dependence that arises when individuals are nested within higher-level units, such as hospitals. Existing survival models account for group-level heterogeneity through frailty terms but do not uncover latent patient subgroups, while most clustering methods ignore hierarchical structure and are not estimated jointly with survival outcomes. In this work, we introduce a new framework that simultaneously performs patient clustering and shared-frailty survival modeling through a penalized likelihood approach. The proposed methodology adaptively learns a patient-to-patient similarity matrix via a modified version of spectral clustering, enabling cluster formation directly from estimated risk profiles while accounting for group membership. A simulation study highlights the proposed model's ability to recover latent clusters and to correctly estimate hazard parameters. We apply our method to a large cohort of heart-failure patients hospitalized with COVID-19 between 2020 and 2021 in the Lombardy region (Italy), identifying clinically meaningful subgroups characterized by distinct risk profiles and highlighting the role of respiratory comorbidities and hospital-level variability in shaping mortality outcomes. This framework provides a flexible and interpretable tool for risk-based patient stratification in hierarchical data settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07446v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alessandra Ragni, Lara Cavinato, Francesca Ieva</dc:creator>
    </item>
    <item>
      <title>Data-Driven Strategies for Detecting and Sampling Misrepresented Subgroups</title>
      <link>https://arxiv.org/abs/2405.01342</link>
      <description>arXiv:2405.01342v2 Announce Type: replace 
Abstract: Economic policy research frequently examines population well-being, with a particular focus on the relationships between unequal living conditions, low educational attainment, and social exclusion. Sample surveys, such as EU-SILC, are widely used for this purpose and inform public policy; yet, their sampling designs may fail to adequately represent rare, hard-to-sample, or under-covered subgroups. This limitation can hinder socio-demographic analyses and evidence-based policy design. We propose a generalisable approach based on univariate and multivariate unsupervised learning techniques to detect outliers in survey data that may signal under-represented subgroups. Identified groups can then be characterised to inform targeted resampling strategies that improve survey inclusiveness. An empirical application using the 2019 EU-SILC data for the Italian region of Liguria shows that citizenship, material deprivation, large household size, and economic vulnerability are key indicators of under-representation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01342v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>G. Lancia, F. Mecatti, E. Riccomagno</dc:creator>
    </item>
    <item>
      <title>A Spatiotemporal, Quasi-experimental Causal Inference Approach to Characterize the Effects of Global Plastic Waste Export and Burning on Air Quality Using Remotely Sensed Data</title>
      <link>https://arxiv.org/abs/2503.04491</link>
      <description>arXiv:2503.04491v3 Announce Type: replace 
Abstract: Open burning of plastic waste may pose a significant threat to global health by degrading air quality, but quantitative research on this problem -- crucial for policy making -- has been stunted by lack of data. Many low- and middle-income countries, where open burning is most concerning, have little to no air quality monitoring. Here, we leverage remotely sensed data products combined with spatiotemporal causal analytic techniques to evaluate the impact of large-scale plastic waste policies on air quality. Throughout, we study Indonesia before and after 2018, when China halted its import of plastic waste, resulting in diversion of this massive waste stream to other countries. We tailor cutting-edge statistical methods to this setting, estimating effects of increased plastic waste imports on fine particulate matter (PM$_{2.5}$) near waste dump sites in Indonesia as a function of proximity to ports, an induced continuous exposure. We observe strong evidence that monthly PM$_{2.5}$increased after China's ban (2018-2019) relative to expected business-as-usual (2012-2017), with increases up to 1.68 $\mu$g/m$^3$ (95\% CI = [0.72, 2.48]) at dump sites with medium-high port proximity. Effects were more modest at sites with very high port proximity, possibly reflecting smaller increases in dumping/burning where government oversight is greater.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04491v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ellen M. Considine, Rachel C. Nethery</dc:creator>
    </item>
    <item>
      <title>Precision Dose-Finding Design for Phase I Oncology Trials by Integrating Pharmacology Data</title>
      <link>https://arxiv.org/abs/2509.05120</link>
      <description>arXiv:2509.05120v2 Announce Type: replace 
Abstract: Phase I oncology trials aim to identify a safe dose - often the maximum tolerated dose (MTD) - for subsequent studies. Conventional designs focus on population-level toxicity modeling, with recent attention on leveraging pharmacokinetic (PK) data to improve dose selection. We propose the Precision Dose-Finding (PDF) design, a novel Bayesian phase I framework that integrates individual patient PK profiles into the dose-finding process. By incorporating patient-specific PK parameters (such as volume of distribution and elimination rate), PDF models toxicity risk at the individual level, in contrast to traditional methods that ignore inter-patient variability. The trial is structured in two stages: an initial training stage to update model parameters using cohort-based dose escalation, and a subsequent test stage in which doses for new patients are chosen based on each patient's own PK-predicted toxicity probability. This two-stage approach enables truly personalized dose assignment while maintaining rigorous safety oversight. Extensive simulation studies demonstrate the feasibility of PDF and suggest that it provides improved safety and dosing precision relative to the continual reassessment method (CRM). The PDF design thus offers a refined dose-finding strategy that tailors the MTD to individual patients, aligning phase I trials with the ideals of precision medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05120v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyong Ju Lee, Yuan Ji</dc:creator>
    </item>
    <item>
      <title>A causal machine learning approach to disentangling the pressure-satisfaction pathways in depression</title>
      <link>https://arxiv.org/abs/2512.06654</link>
      <description>arXiv:2512.06654v3 Announce Type: replace 
Abstract: Purpose: Prior research has established perceived pressure and life satisfaction as important correlates of depression, yet their causal interplay remains insufficiently identified. This study aims to disentangle whether satisfaction acts as an independent protective factor or operates by buffering pressure, and to identify population-specific risk profiles across students and workers. Methods: We applied a causal machine learning framework to harmonized data from India, China, and Malaysia (total N=28,243). We integrated random forests and logistic regression with Causal Mediation Analysis and Causal Forests. To resolve theoretical ambiguity regarding directionality, we validated the causal pathway between pressure and satisfaction using numerical simulation benchmarks. Results: Pressure emerged as the dominant predictor across all cohorts. Simulation-validated analysis confirmed a causal pathway flowing from Life Satisfaction -&gt; Pressure -&gt; Depression, rejecting the reverse hypothesis. Satisfaction mitigated depression partially through pressure reduction (proportion mediated around 15.1%), rather than functioning exclusively as a direct mechanism. A distinct developmental reversal was observed: younger age predicted vulnerability in students, whereas older age predicted risk in workers. Causal forests further revealed that the depressogenic impact of pressure was significantly amplified in students with high anxiety. Conclusion: Pressure acts as the proximal bottleneck for depression risk, while life satisfaction functions as an antecedent buffer. These findings challenge uniform risk models by highlighting age-related context dependence and suggest that precision interventions targeting stress reduction and high-anxiety subgroups offer the most effective pathway for breaking the causal cycle of depression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06654v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaojin Nan</dc:creator>
    </item>
    <item>
      <title>Strategic Geosteering Workflow with Uncertainty Quantification and Deep Learning: A Case Study on the Goliat Field</title>
      <link>https://arxiv.org/abs/2210.15548</link>
      <description>arXiv:2210.15548v2 Announce Type: replace-cross 
Abstract: The real-time interpretation of the logging-while-drilling data allows us to estimate the positions and properties of the geological layers in an anisotropic subsurface environment. Robust real-time estimations capturing uncertainty can be very useful for efficient geosteering operations. However, the model errors in the prior conceptual geological models and forward simulation of the measurements can be significant factors in the unreliable estimations of the profiles of the geological layers. The model errors are specifically pronounced when using a deep-neural-network (DNN) approximation which we use to accelerate and parallelize the simulation of the measurements. This paper presents a practical workflow consisting of offline and online phases. The offline phase includes DNN training and building of an uncertain prior near-well geo-model. The online phase uses the flexible iterative ensemble smoother (FlexIES) to perform real-time assimilation of extra-deep electromagnetic data accounting for the model errors in the approximate DNN model. We demonstrate the proposed workflow on a case study for a historic well in the Goliat Field (Barents Sea). The median of our probabilistic estimation is on-par with proprietary inversion despite the approximate DNN model and regardless of the number of layers in the chosen prior. By estimating the model errors, FlexIES automatically quantifies the uncertainty in the layers' boundaries and resistivities, which is not standard for proprietary inversion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.15548v2</guid>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1190/geo2023-0576.1</arxiv:DOI>
      <arxiv:journal_reference>Geophysics (2024) 89 (6) D301-D314</arxiv:journal_reference>
      <dc:creator>Muzammil Hussain Rammay, Sergey Alyaev, David Selv{\aa}g Larsen, Reidar Brumer Bratvold, Craig Saint</dc:creator>
    </item>
    <item>
      <title>Calibration Bands for Mean Estimates within the Exponential Dispersion Family</title>
      <link>https://arxiv.org/abs/2503.18896</link>
      <description>arXiv:2503.18896v2 Announce Type: replace-cross 
Abstract: A statistical model is said to be calibrated if the resulting mean estimates perfectly match the true means of the underlying responses. Aiming for calibration is often not achievable in practice as one has to deal with finite samples of noisy observations. A weaker notion of calibration is auto-calibration. An auto-calibrated model satisfies that the expected value of the responses for a given mean estimate matches this estimate. Testing for autocalibration has only been considered recently in the literature and we propose a new approach based on calibration bands. Calibration bands denote a set of lower and upper bounds such that the probability that the true means lie simultaneously inside those bounds exceeds some given confidence level. Such bands were constructed by Yang-Barber (2019) for sub-Gaussian distributions. Dimitriadis et al. (2023) then introduced narrower bands for the Bernoulli distribution. We use the same idea in order to extend the construction to the entire exponential dispersion family that contains for example the binomial, Poisson, negative binomial, gamma and normal distributions. Moreover, we show that the obtained calibration bands allow us to construct various tests for calibration and auto-calibration, respectively. As the construction of the bands does not rely on asymptotic results, we emphasize that our tests can be used for any sample size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18896v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>{\L}ukasz Delong, Selim Gatti, Mario V. W\"uthrich</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Experiments with Latent Outcomes: Key Results and Their Implications for Design and Analysis</title>
      <link>https://arxiv.org/abs/2505.21909</link>
      <description>arXiv:2505.21909v3 Announce Type: replace-cross 
Abstract: How should researchers analyze randomized experiments in which the main outcome is latent and measured in multiple ways but each measure contains some degree of error? We first identify a critical study-specific noncomparability problem in existing methods for handling multiple measurements, which often rely on strong modeling assumptions or arbitrary standardization. Such approaches render the resulting estimands noncomparable across studies. To address the problem, we describe design-based approaches that enable researchers to identify causal parameters of interest, suggest ways that experimental designs can be augmented so as to make assumptions more credible, and discuss empirical tests of key assumptions. We show that when experimental researchers invest appropriately in multiple outcome measures, an optimally weighted scaled index of these measures enables researchers to obtain efficient and interpretable estimates of causal parameters by applying standard regression. An empirical application illustrates the gains in precision and robustness that multiple outcome measures can provide.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21909v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Fu, Donald P. Green</dc:creator>
    </item>
    <item>
      <title>Omnibus goodness-of-fit tests based on trigonometric moments</title>
      <link>https://arxiv.org/abs/2507.18591</link>
      <description>arXiv:2507.18591v2 Announce Type: replace-cross 
Abstract: We propose a new omnibus goodness-of-fit test based on trigonometric moments of probability-integral-transformed data. The test builds on the framework of the LK test introduced by Langholz and Kronmal [J. Amer. Statist. Assoc. 86 (1991), 1077-1084], but fully exploits the covariance structure of the associated trigonometric statistics. As a result, our test statistic converges under the null hypothesis to a $\chi_2^2$ distribution, even in the presence of nuisance parameters, yielding a well-calibrated rejection region. We derive the exact asymptotic covariance matrix required for normalization and propose a unified approach to computing the LK normalizing scalar. The applicability of both the proposed test and the LK test is substantially expanded by providing implementation details for 11 families of continuous distributions, covering most commonly used parametric models. Simulation studies demonstrate accurate empirical size, close to the nominal level, and strong power properties, yielding fully plug-and-play procedures. Further insight is provided by an analysis under local alternatives. The methodology is illustrated using surface temperature forecast errors from a numerical weather prediction model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18591v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alain Desgagn\'e, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>Parallel Algorithms for Structured Sparse Support Vector Machines: Application in Music Genre Classification</title>
      <link>https://arxiv.org/abs/2512.07463</link>
      <description>arXiv:2512.07463v2 Announce Type: replace-cross 
Abstract: Mathematical modelling, particularly through approaches such as structured sparse support vector machines (SS-SVM), plays a crucial role in processing data with complex feature structures, yet efficient algorithms for distributed large-scale data remain lacking. To address this gap, this paper proposes a unified optimization framework based on a consensus structure. This framework is not only applicable to various loss functions and combined regularization terms but can also be effectively extended to non-convex regularizers, demonstrating strong scalability. Building upon this framework, we develop a distributed parallel alternating direction method of multipliers (ADMM) algorithm to efficiently solve SS-SVMs under distributed data storage. To ensure convergence, we incorporate a Gaussian back-substitution technique. Additionally, for completeness, we introduce a family of sparse group Lasso support vector machine (SGL-SVM) and apply it to music information retrieval. Theoretical analysis confirms that the computational complexity of the proposed algorithm is independent of the choice of regularization terms and loss functions, underscoring the universality of the parallel approach. Experiments on both synthetic and real-world music archive datasets validate the reliability, stability, and efficiency of our algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07463v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rongmei Liang, Zizheng Liu, Xiaofei Wu, Jingwen Tu</dc:creator>
    </item>
  </channel>
</rss>
