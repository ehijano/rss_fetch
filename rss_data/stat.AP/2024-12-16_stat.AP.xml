<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Dec 2024 04:00:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Scalable magnetic resonance fingerprinting: Incremental inference of high dimensional elliptical mixtures from large data volumes</title>
      <link>https://arxiv.org/abs/2412.10173</link>
      <description>arXiv:2412.10173v1 Announce Type: new 
Abstract: Magnetic Resonance Fingerprinting (MRF) is an emerging technology with the potential to revolutionize radiology and medical diagnostics. In comparison to traditional magnetic resonance imaging (MRI), MRF enables the rapid, simultaneous, non-invasive acquisition and reconstruction of multiple tissue parameters, paving the way for novel diagnostic techniques. In the original matching approach, reconstruction is based on the search for the best matches between in vivo acquired signals and a dictionary of high-dimensional simulated signals (fingerprints) with known tissue properties. A critical and limiting challenge is that the size of the simulated dictionary increases exponentially with the number of parameters, leading to an extremely costly subsequent matching. In this work, we propose to address this scalability issue by considering probabilistic mixtures of high-dimensional elliptical distributions, to learn more efficient dictionary representations. Mixture components are modelled as flexible ellipitic shapes in low dimensional subspaces. They are exploited to cluster similar signals and reduce their dimension locally cluster-wise to limit information loss. To estimate such a mixture model, we provide a new incremental algorithm capable of handling large numbers of signals, allowing us to go far beyond the hardware limitations encountered by standard implementations. We demonstrate, on simulated and real data, that our method effectively manages large volumes of MRF data with maintained accuracy. It offers a more efficient solution for accurate tissue characterization and significantly reduces the computational burden, making the clinical application of MRF more practical and accessible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10173v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geoffroy Oudoumanessah, Thomas Coudert, Carole Lartizien, Michel Dojat, Thomas Christen, Florence Forbes</dc:creator>
    </item>
    <item>
      <title>Uncertainties in Signal Recovery from Heterogeneous and Convoluted Time Series with Principal Component Analysis</title>
      <link>https://arxiv.org/abs/2412.10175</link>
      <description>arXiv:2412.10175v1 Announce Type: new 
Abstract: Principal Component Analysis (PCA) is one of the most used tools for extracting low-dimensional representations of data, in particular for time series. Performances are known to strongly depend on the quality (amount of noise) and the quantity of data. We here investigate the impact of heterogeneities, often present in real data, on the reconstruction of low-dimensional trajectories and of their associated modes. We focus in particular on the effects of sample-to-sample fluctuations and of component-dependent temporal convolution and noise in the measurements. We derive analytical predictions for the error on the reconstructed trajectory and the confusion between the modes using the replica method in a high-dimensional setting, in which the number and the dimension of the data are comparable. We find in particular that sample-to-sample variability, is deleterious for the reconstruction of the signal trajectory, but beneficial for the inference of the modes, and that the fluctuations in the temporal convolution kernels prevent perfect recovery of the latent modes even for very weak measurement noise. Our predictions are corroborated by simulations with synthetic data for a variety of control parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10175v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mariia Legenkaia, Laurent Bourdieu, R\'emi Monasson</dc:creator>
    </item>
    <item>
      <title>Applied Statistics in the Era of Artificial Intelligence: A Review and Vision</title>
      <link>https://arxiv.org/abs/2412.10331</link>
      <description>arXiv:2412.10331v1 Announce Type: new 
Abstract: The advent of artificial intelligence (AI) technologies has significantly changed many domains, including applied statistics. This review and vision paper explores the evolving role of applied statistics in the AI era, drawing from our experiences in engineering statistics. We begin by outlining the fundamental concepts and historical developments in applied statistics and tracing the rise of AI technologies. Subsequently, we review traditional areas of applied statistics, using examples from engineering statistics to illustrate key points. We then explore emerging areas in applied statistics, driven by recent technological advancements, highlighting examples from our recent projects. The paper discusses the symbiotic relationship between AI and applied statistics, focusing on how statistical principles can be employed to study the properties of AI models and enhance AI systems. We also examine how AI can advance applied statistics in terms of modeling and analysis. In conclusion, we reflect on the future role of statisticians. Our paper aims to shed light on the transformative impact of AI on applied statistics and inspire further exploration in this dynamic field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10331v1</guid>
      <category>stat.AP</category>
      <category>cs.SI</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jie Min, Xinyi Song, Simin Zheng, Caleb B. King, Xinwei Deng, Yili Hong</dc:creator>
    </item>
    <item>
      <title>$L$-estimation of Claim Severity Models Weighted by Kumaraswamy Density</title>
      <link>https://arxiv.org/abs/2412.09830</link>
      <description>arXiv:2412.09830v1 Announce Type: cross 
Abstract: Statistical modeling of claim severity distributions is essential in insurance and risk management, where achieving a balance between robustness and efficiency in parameter estimation is critical against model contaminations. Two \( L \)-estimators, the method of trimmed moments (MTM) and the method of winsorized moments (MWM), are commonly used in the literature, but they are constrained by rigid weighting schemes that either discard or uniformly down-weight extreme observations, limiting their customized adaptability. This paper proposes a flexible robust \( L \)-estimation framework weighted by Kumaraswamy densities, offering smoothly varying observation-specific weights that preserve valuable information while improving robustness and efficiency. The framework is developed for parametric claim severity models, including Pareto, lognormal, and Fr{\'e}chet distributions, with theoretical justifications on asymptotic normality and variance-covariance structures. Through simulations and application to a U.S. indemnity loss dataset, the proposed method demonstrates superior performance over MTM, MWM, and MLE approaches, particularly in handling outliers and heavy-tailed distributions, making it a flexible and reliable alternative for loss severity modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09830v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chudamani Poudyal, Gokarna R. Aryal, Keshav Pokhrel</dc:creator>
    </item>
    <item>
      <title>De-Biasing Structure Function Estimates From Sparse Time Series of the Solar Wind: A Data-Driven Approach</title>
      <link>https://arxiv.org/abs/2412.10053</link>
      <description>arXiv:2412.10053v1 Announce Type: cross 
Abstract: Structure functions, which represent the moments of the increments of a stochastic process, are essential complementary statistics to power spectra for analysing the self-similar behaviour of a time series. However, many real-world environmental datasets, such as those collected by spacecraft monitoring the solar wind, contain gaps, which inevitably corrupt the statistics. The nature of this corruption for structure functions remains poorly understood - indeed, often overlooked. Here we simulate gaps in a large set of magnetic field intervals from Parker Solar Probe in order to characterize the behaviour of the structure function of a sparse time series of solar wind turbulence. We quantify the resultant error with regards to the overall shape of the structure function, and its slope in the inertial range. Noting the consistent underestimation of the true curve when using linear interpolation, we demonstrate the ability of an empirical correction factor to de-bias these estimates. This correction, "learnt" from the data from a single spacecraft, is shown to generalize well to data from a solar wind regime elsewhere in the heliosphere, producing smaller errors, on average, for missing fractions &gt;25%. Given this success, we apply the correction to gap-affected Voyager intervals from the inner heliosheath and local interstellar medium, obtaining spectral indices similar to those from previous studies. This work provides a tool for future studies of fragmented solar wind time series, such as those from Voyager, MAVEN, and OMNI, as well as sparsely-sampled astrophysical and geophysical processes more generally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10053v1</guid>
      <category>astro-ph.SR</category>
      <category>physics.space-ph</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Wrench, Tulasi N. Parashar</dc:creator>
    </item>
    <item>
      <title>Feature Selection for Latent Factor Models</title>
      <link>https://arxiv.org/abs/2412.10128</link>
      <description>arXiv:2412.10128v1 Announce Type: cross 
Abstract: Feature selection is crucial for pinpointing relevant features in high-dimensional datasets, mitigating the 'curse of dimensionality,' and enhancing machine learning performance. Traditional feature selection methods for classification use data from all classes to select features for each class. This paper explores feature selection methods that select features for each class separately, using class models based on low-rank generative methods and introducing a signal-to-noise ratio (SNR) feature selection criterion. This novel approach has theoretical true feature recovery guarantees under certain assumptions and is shown to outperform some existing feature selection methods on standard classification datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10128v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rittwika Kansabanik, Adrian Barbu</dc:creator>
    </item>
    <item>
      <title>High-dimensional Statistics Applications to Batch Effects in Metabolomics</title>
      <link>https://arxiv.org/abs/2412.10196</link>
      <description>arXiv:2412.10196v1 Announce Type: cross 
Abstract: Batch effects are inevitable in large-scale metabolomics. Prior to formal data analysis, batch effect correction (BEC) is applied to prevent from obscuring biological variations, and batch effect evaluation (BEE) is used for correction assessment. However, existing BEE algorithms neglect covariances between the variables, and existing BEC algorithms might fail to adequately correct the covariances. Therefore, we resort to recent advancements in high-dimensional statistics, and respectively propose "quality control-based simultaneous tests (QC-ST)" and "covariance correction (CoCo)". Validated by the simulation data, QC-ST can simultaneously detect the statistical significance of QC samples' mean vectors and covariance matrices across different batches, and has a satisfactory statistical performance in empirical sizes, empirical powers, and computational speed. Then, we apply four QC-based BEC algorithms to two large cohort datasets, and find that extreme gradient boost (XGBoost) performs best in relative standard deviation (RSD) and dispersion-ratio (D-ratio). After prepositive BEC, if QC-ST still suggests that batch effects between some two batches are significant, CoCo should be implemented. And after CoCo (if necessary), the four metrics (i.e., RSD, D-ratio, classification performance, and QC-ST) might be further improved. In summary, under the guidance of QC-ST, we can develop a matching strategy to integrate multiple BEC algorithms more rationally and flexibly, and minimize batch effects for reliable biological conclusions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10196v1</guid>
      <category>stat.ME</category>
      <category>q-bio.BM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhendong Guo</dc:creator>
    </item>
    <item>
      <title>What if we had built a prediction model with a survival super learner instead of a Cox model 10 years ago?</title>
      <link>https://arxiv.org/abs/2412.10252</link>
      <description>arXiv:2412.10252v1 Announce Type: cross 
Abstract: Objective: This study sought to compare the drop in predictive performance over time according to the modeling approach (regression versus machine learning) used to build a kidney transplant failure prediction model with a time-to-event outcome.
  Study Design and Setting: The Kidney Transplant Failure Score (KTFS) was used as a benchmark. We reused the data from which it was developed (DIVAT cohort, n=2,169) to build another prediction algorithm using a survival super learner combining (semi-)parametric and non-parametric methods. Performance in DIVAT was estimated for the two prediction models using internal validation. Then, the drop in predictive performance was evaluated in the same geographical population approximately ten years later (EKiTE cohort, n=2,329).
  Results: In DIVAT, the super learner achieved better discrimination than the KTFS, with a tAUROC of 0.83 (0.79-0.87) compared to 0.76 (0.70-0.82). While the discrimination remained stable for the KTFS, it was not the case for the super learner, with a drop to 0.80 (0.76-0.83). Regarding calibration, the survival SL overestimated graft survival at development, while the KTFS underestimated graft survival ten years later. Brier score values were similar regardless of the approach and the timing.
  Conclusion: The more flexible SL provided superior discrimination on the population used to fit it compared to a Cox model and similar discrimination when applied to a future dataset of the same population. Both methods are subject to calibration drift over time. However, weak calibration on the population used to develop the prediction model was correct only for the Cox model, and recalibration should be considered in the future to correct the calibration drift.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10252v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arthur Chatton, \'Emilie Pilote, Kevin Assob Feugo, H\'elo\"ise Cardinal, Robert W. Platt, Mireille E Schnitzer</dc:creator>
    </item>
    <item>
      <title>Use of surrogate endpoints in health technology assessment: a review of selected NICE technology appraisals in oncology</title>
      <link>https://arxiv.org/abs/2412.02380</link>
      <description>arXiv:2412.02380v2 Announce Type: replace 
Abstract: Objectives: Surrogate endpoints, used to substitute for and predict final clinical outcomes, are increasingly being used to support submissions to health technology assessment agencies. The increase in use of surrogate endpoints has been accompanied by literature describing frameworks and statistical methods to ensure their robust validation. The aim of this review was to assess how surrogate endpoints have recently been used in oncology technology appraisals by the National Institute for Health and Care Excellence (NICE) in England and Wales.
  Methods: This paper identified technology appraisals in oncology published by NICE between February 2022 and May 2023. Data are extracted on methods for the use and validation of surrogate endpoints.
  Results: Of the 47 technology appraisals in oncology available for review, 18 (38 percent) utilised surrogate endpoints, with 37 separate surrogate endpoints being discussed. However, the evidence supporting the validity of the surrogate relationship varied significantly across putative surrogate relationships with 11 providing RCT evidence, 7 providing evidence from observational studies, 12 based on clinical opinion and 7 providing no evidence for the use of surrogate endpoints.
  Conclusions: This review supports the assertion that surrogate endpoints are frequently used in oncology technology appraisals in England and Wales. Despite increasing availability of statistical methods and guidance on appropriate validation of surrogate endpoints, this review highlights that use and validation of surrogate endpoints can vary between technology appraisals which can lead to uncertainty in decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02380v2</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorna Wheaton, Sylwia Bujkiewicz</dc:creator>
    </item>
    <item>
      <title>Optimizing Heat Alert Issuance with Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2312.14196</link>
      <description>arXiv:2312.14196v3 Announce Type: replace-cross 
Abstract: A key strategy in societal adaptation to climate change is using alert systems to prompt preventative action and reduce the adverse health impacts of extreme heat events. This paper implements and evaluates reinforcement learning (RL) as a tool to optimize the effectiveness of such systems. Our contributions are threefold. First, we introduce a new publicly available RL environment enabling the evaluation of the effectiveness of heat alert policies to reduce heat-related hospitalizations. The rewards model is trained from a comprehensive dataset of historical weather, Medicare health records, and socioeconomic/geographic features. We use scalable Bayesian techniques tailored to the low-signal effects and spatial heterogeneity present in the data. The transition model uses real historical weather patterns enriched by a data augmentation mechanism based on climate region similarity. Second, we use this environment to evaluate standard RL algorithms in the context of heat alert issuance. Our analysis shows that policy constraints are needed to improve RL's initially poor performance. Third, a post-hoc contrastive analysis provides insight into scenarios where our modified heat alert-RL policies yield significant gains/losses over the current National Weather Service alert policy in the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14196v3</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25), AI for Social Impact Track. 2025</arxiv:journal_reference>
      <dc:creator>Ellen M. Considine, Rachel C. Nethery, Gregory A. Wellenius, Francesca Dominici, Mauricio Tec</dc:creator>
    </item>
    <item>
      <title>Model Selection for Causal Modeling in Missing Exposure Problems</title>
      <link>https://arxiv.org/abs/2406.12171</link>
      <description>arXiv:2406.12171v2 Announce Type: replace-cross 
Abstract: In causal inference, properly selecting the propensity score (PS) model is an important topic and has been widely investigated in observational studies. There is also a large literature focusing on the missing data problem. However, there are very few studies investigating the model selection issue for causal inference when the exposure is missing at random (MAR). In this paper, we discuss how to select both imputation and PS models, which can result in the smallest root mean squared error (RMSE) of the estimated causal effect in our simulation study. Then, we propose a new criterion, called ``rank score'' for evaluating the overall performance of both models. The simulation studies show that the full imputation plus the outcome-related PS models lead to the smallest RMSE and the rank score can help select the best models. An application study is conducted to quantify the causal effect of cardiovascular disease (CVD) on the mortality of COVID-19 patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12171v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 16 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuliang Shi, Yeying Zhu, Joel A. Dubin</dc:creator>
    </item>
  </channel>
</rss>
