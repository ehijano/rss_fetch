<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 May 2025 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>pared: Model selection using multi-objective optimization</title>
      <link>https://arxiv.org/abs/2505.21730</link>
      <description>arXiv:2505.21730v1 Announce Type: new 
Abstract: Motivation: Model selection is a ubiquitous challenge in statistics. For penalized models, model selection typically entails tuning hyperparameters to maximize a measure of fit or minimize out-of-sample prediction error. However, these criteria fail to reflect other desirable characteristics, such as model sparsity, interpretability, or smoothness. Results: We present the R package pared to enable the use of multi-objective optimization for model selection. Our approach entails the use of Gaussian process-based optimization to efficiently identify solutions that represent desirable trade-offs. Our implementation includes popular models with multiple objectives including the elastic net, fused lasso, fused graphical lasso, and group graphical lasso. Our R package generates interactive graphics that allow the user to identify hyperparameter values that result in fitted models which lie on the Pareto frontier. Availability: We provide the R package pared and vignettes illustrating its application to both simulated and real data at https://github.com/priyamdas2/pared.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21730v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Priyam Das, Sarah Robinson, Christine B. Peterson</dc:creator>
    </item>
    <item>
      <title>Adaptive Block-Based Change-Point Detection for Sparse Spatially Clustered Data with Applications in Remote Sensing Imaging</title>
      <link>https://arxiv.org/abs/2505.21814</link>
      <description>arXiv:2505.21814v1 Announce Type: new 
Abstract: We present a non-parametric change-point detection approach to detect potentially sparse changes in a time series of high-dimensional observations or non-Euclidean data objects. We target a change in distribution that occurs in a small, unknown subset of dimensions, where these dimensions may be correlated. Our work is motivated by a remote sensing application, where changes occur in small, spatially clustered regions over time. An adaptive block-based change-point detection framework is proposed that accounts for spatial dependencies across dimensions and leverages these dependencies to boost detection power and improve estimation accuracy. Through simulation studies, we demonstrate that our approach has superior performance in detecting sparse changes in datasets with spatial or local group structures. An application of the proposed method to detect activity, such as new construction, in remote sensing imagery of the Natanz Nuclear facility in Iran is presented to demonstrate the method's efficacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21814v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alan Moore, Lynna Chu, Zhengyuan Zhu</dc:creator>
    </item>
    <item>
      <title>Regression Analysis of Ordinal Panel Count Data in Recurrent Medication Non-adherence</title>
      <link>https://arxiv.org/abs/2505.21858</link>
      <description>arXiv:2505.21858v1 Announce Type: new 
Abstract: Panel count data arise in clinical trials when patients are asked to report their occurrences of events of interest periodically but the exact event times are unknown, only the count of events between two successive examinations are observed. Ordinal panel count data goes even further as the exact event counts are not observed, the only information available is rank of event counts, for example, 'never', 'sometimes' and 'always'. Currently, there is lacking of standard and efficient methods for analyzing this type of data. In this paper, we proposed a semiparametric proportional intensity model to analyze such data. We developed a maximum sieve likelihood estimation using monotone spline under the nonhomogeneous Poisson process model assumption for statistical inference. Simulation studies show that our method performs well with finite sample sizes and is relatively robust to model misspecification. In addition, we compared the proposed method with other competitors and the proposed method outperforms in various settings. Finally, we investigated the recurrence of medication non-adherence in a clinical trial on non-psychotic major depressive disorder using the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21858v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiangjie Zhou, Baosheng Liang</dc:creator>
    </item>
    <item>
      <title>Beyond data: leveraging non-empirical information and expert knowledge in Bayesian model calibration</title>
      <link>https://arxiv.org/abs/2505.21934</link>
      <description>arXiv:2505.21934v1 Announce Type: new 
Abstract: Mathematical models connect theory with the real world through data, enabling us to interpret, understand, and predict complex phenomena. However, scientific knowledge often extends beyond what can be empirically measured, offering valuable insights into complex and uncertain systems. Here, we introduce a statistical framework for calibrating mathematical models using non-empirical information. Through examples in ecology, biology, and medicine, we demonstrate how expert knowledge, scientific theory, and qualitative observations can meaningfully constrain models. In each case, these non-empirical insights guide models toward more realistic dynamics and more informed predictions than empirical data alone could achieve. Now, our understanding of the systems represented by mathematical models is not limited by the data that can be obtained; they instead sit at the edge of scientific understanding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21934v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah A. Vollert, Christopher Drovandi, Cailan Jeynes-Smith, Luz V. Pascal, Matthew P. Adams</dc:creator>
    </item>
    <item>
      <title>Data-Adaptive Automatic Threshold Calibration for Stability Selection</title>
      <link>https://arxiv.org/abs/2505.22012</link>
      <description>arXiv:2505.22012v1 Announce Type: new 
Abstract: Stability selection has gained popularity as a method for enhancing the performance of variable selection algorithms while controlling false discovery rates. However, achieving these desirable properties depends on correctly specifying the stable threshold parameter, which can be challenging. An arbitrary choice of this parameter can substantially alter the set of selected variables, as the variables' selection probabilities are inherently data-dependent. To address this issue, we propose Exclusion Automatic Threshold Selection (EATS), a data-adaptive algorithm that streamlines stability selection by automating the threshold specification process. Additionally, we introduce Automatic Threshold Selection (ATS), the motivation behind EATS. We evaluate our approach through an extensive simulation study, benchmarking across commonly used variable selection algorithms and several static stable threshold values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22012v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Huang, Samuel Muller, Garth Tarr</dc:creator>
    </item>
    <item>
      <title>Debiased distributed PCA under high dimensional spiked model</title>
      <link>https://arxiv.org/abs/2505.22015</link>
      <description>arXiv:2505.22015v1 Announce Type: new 
Abstract: We study distributed principal component analysis (PCA) in high-dimensional settings under the spiked model. In such regimes, sample eigenvectors can deviate significantly from population ones, introducing a persistent bias. Existing distributed PCA methods are sensitive to this bias, particularly when the number of machines is small. Their consistency typically relies on the number of machines tending to infinity. We propose a debiased distributed PCA algorithm that corrects the local bias before aggregation and incorporates a sparsity-detection step to adaptively handle sparse and non-sparse eigenvectors. Theoretically, we establish the consistency of our estimator under much weaker conditions compared to existing literature. In particular, our approach does not require symmetric innovations and only assumes a finite sixth moment. Furthermore, our method generally achieves smaller estimation error, especially when the number of machines is small. Empirically, extensive simulations and real data experiments demonstrate that our method consistently outperforms existing distributed PCA approaches. The advantage is especially prominent when the leading eigenvectors are sparse or the number of machines is limited. Our method and theoretical analysis are also applicable to the sample correlation matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22015v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weiming Li, Zeng Li, Siyu Wang, Yanqing Yin, Junpeng Zhu</dc:creator>
    </item>
    <item>
      <title>Random irregular histograms</title>
      <link>https://arxiv.org/abs/2505.22034</link>
      <description>arXiv:2505.22034v1 Announce Type: new 
Abstract: We propose a new method of histogram construction, providing the first fully Bayesian approach to irregular histograms. Our procedure applies Bayesian model selection to a piecewise constant model of the underlying distribution, resulting in a method that selects both the number of bins as well as their location based on the data in a fully automatic fashion. We show that the histogram estimate is consistent with respect to the Hellinger metric under mild regularity conditions, and that it attains a convergence rate equal to the minimax rate (up to a logarithmic factor) for H\"{o}lder continuous densities. Simulation studies indicate that the new method performs comparably to other histogram procedures, both for minimizing the estimation error and for identifying modes. A software implementation is included as supplementary material.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22034v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oskar H{\o}gberg Simensen, Dennis Christensen, Nils Lid Hjort</dc:creator>
    </item>
    <item>
      <title>Handling bounded response in high dimensions: a Horseshoe prior Bayesian Beta regression approach</title>
      <link>https://arxiv.org/abs/2505.22211</link>
      <description>arXiv:2505.22211v1 Announce Type: new 
Abstract: Bounded continuous responses -- such as proportions -- arise frequently in diverse scientific fields including climatology, biostatistics, and finance. Beta regression is a widely adopted framework for modeling such data, due to the flexibility of the Beta distribution over the unit interval. While Bayesian extensions of Beta regression have shown promise, existing methods are limited to low-dimensional settings and lack theoretical guarantees. In this work, we propose a novel Bayesian approach for high-dimensional sparse Beta regression framework that employs a tempered posterior. Our method incorporates the Horseshoe prior for effective shrinkage and variable selection. Most notable, we propose a novel Gibbs sampling algorithm using P\'olya-Gamma augmentation for efficient inference in Beta regression model. We also provide the first theoretical results establishing posterior consistency and convergence rates for Bayesian Beta regression. Through extensive simulation studies in both low- and high-dimensional scenarios, we demonstrate that our approach outperforms existing alternatives, offering improved estimation accuracy and model interpretability.
  Our method is implemented in the R package ``betaregbayes" available on Github.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22211v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>The Tien Mai</dc:creator>
    </item>
    <item>
      <title>Network Model Averaging Prediction for Latent Space Models by K-Fold Edge Cross-Validation</title>
      <link>https://arxiv.org/abs/2505.22289</link>
      <description>arXiv:2505.22289v1 Announce Type: new 
Abstract: In complex systems, networks represent connectivity relationships between nodes through edges. Latent space models are crucial in analyzing network data for tasks like community detection and link prediction due to their interpretability and visualization capabilities. However, when the network size is relatively small, and the true latent space dimension is considerable, the parameters in latent space models may not be estimated very well. To address this issue, we propose a Network Model Averaging (NetMA) method tailored for latent space models with varying dimensions, specifically focusing on link prediction in networks. For both single-layer and multi-layer networks, we first establish the asymptotic optimality of the proposed averaging prediction in the sense of achieving the lowest possible prediction loss. Then we show that when the candidate models contain some correct models, our method assigns all weights to the correct models. Furthermore, we demonstrate the consistency of the NetMA-based weight estimator tending to the optimal weight vector. Extensive simulation studies show that NetMA performs better than simple averaging and model selection methods, and even outperforms the "oracle" method when the real latent space dimension is relatively large. Evaluation on collaboration and virtual event networks further emphasizes the competitiveness of NetMA in link prediction performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22289v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Zhang, Jun Liao, Xinyan Fan, Kuangnan Fang, Yuhong Yang</dc:creator>
    </item>
    <item>
      <title>Estimation of the number of principal components in high-dimensional multivariate extremes</title>
      <link>https://arxiv.org/abs/2505.22437</link>
      <description>arXiv:2505.22437v1 Announce Type: new 
Abstract: For multivariate regularly random vectors of dimension $d$, the dependence structure of the extremes is modeled by the so-called angular measure. When the dimension $d$ is high, estimating the angular measure is challenging because of its complexity. In this paper, we use Principal Component Analysis (PCA) as a method for dimension reduction and estimate the number of significant principal components of the empirical covariance matrix of the angular measure under the assumption of a spiked covariance structure. Therefore, we develop Akaike Information Criteria (AIC) and Bayesian Information Criteria (BIC) to estimate the location of the spiked eigenvalue of the covariance matrix, reflecting the number of significant components, and explore these information criteria on consistency. On the one hand, we investigate the case where the dimension $d$ is fixed, and on the other hand, where the dimension $d$ converges to $\infty$ under different high-dimensional scenarios. When the dimension $d$ is fixed, we establish that the AIC is not consistent, whereas the BIC is weakly consistent. In the high-dimensional setting, with techniques from random matrix theory, we derive sufficient conditions for the AIC and the BIC to be consistent. Finally, the performance of the different AIC and BIC versions is compared in a simulation study and applied to high-dimensional precipitation data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22437v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Butsch, Vicky Fasen-Hartmann</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Community Detection and Model Selection in Blockmodels</title>
      <link>https://arxiv.org/abs/2505.22459</link>
      <description>arXiv:2505.22459v1 Announce Type: new 
Abstract: Blockmodels are a foundational tool for modeling community structure in networks, with the stochastic blockmodel (SBM), degree-corrected blockmodel (DCBM), and popularity-adjusted blockmodel (PABM) forming a natural hierarchy of increasing generality. While community detection under these models has been extensively studied, much less attention has been paid to the model selection problem, i.e., determining which model best fits a given network. Building on recent theoretical insights about the spectral geometry of these models, we propose a unified framework for simultaneous community detection and model selection across the full blockmodel hierarchy. A key innovation is the use of loss functions that serve a dual role: they act as objective functions for community detection and as test statistics for hypothesis testing. We develop a greedy algorithm to minimize these loss functions and establish theoretical guarantees for exact label recovery and model selection consistency under each model. Extensive simulation studies demonstrate that our method achieves high accuracy in both tasks, outperforming or matching state-of-the-art alternatives. Applications to five real-world networks further illustrate the interpretability and practical utility of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22459v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subhankar Bhadra, Minh Tang, Srijan Sengupta</dc:creator>
    </item>
    <item>
      <title>Modeling and estimating skewed and heavy-tailed populations via unsupervised mixture models</title>
      <link>https://arxiv.org/abs/2505.22507</link>
      <description>arXiv:2505.22507v1 Announce Type: new 
Abstract: We develop an unsupervised mixture model for non-negative, skewed and heavy-tailed data, such as losses in actuarial and risk management applications. The mixture has a lognormal component, which is usually appropriate for the body of the distribution, and a Pareto-type tail, aimed at accommodating the largest observations, since the lognormal tail often decays too fast. We show that maximum likelihood estimation can be performed by means of the EM algorithm and that the model is quite flexible in fitting data from different data-generating processes. Simulation experiments and a real-data application to automobiles claims suggest that the approach is equivalent in terms of goodness-of-fit, but easier to estimate, with respect to two existing distributions with similar features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22507v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Bee, Flavio Santi</dc:creator>
    </item>
    <item>
      <title>Bayesian Non-Parametric Inference for L\'evy Measures in State-Space Models</title>
      <link>https://arxiv.org/abs/2505.22587</link>
      <description>arXiv:2505.22587v1 Announce Type: new 
Abstract: L\'evy processes, known for their ability to model complex dynamics with skewness, heavy tails and discontinuities, play a critical role in stochastic modeling across various domains. However, inference for most L\'evy processes, whether in parametric or non-parametric settings, remains a significant challenge. In this work, we present a novel Bayesian non-parametric inference framework for inferring the L\'evy measures of subordinators and normal variance-mean (NVM) processes within a linear L\'evy state space model, a setup that significantly extends existing methodologies. We employ the Dirichlet process which further results in a Student-t mixture representation to enable inference for the L\'evy measures. An efficient augmented Markov Chain Monte Carlo algorithm is developed for this problem that ensures both accuracy and computational feasibility. The effectiveness of the method is demonstrated on synthetic and tick-level (high-frequency) financial datasets, and we show the practical utility of the inference results using the forecasting performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22587v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bill Z. Lin, Simon Godsill</dc:creator>
    </item>
    <item>
      <title>Towards the Efficient Inference by Incorporating Automated Computational Phenotypes under Covariate Shift</title>
      <link>https://arxiv.org/abs/2505.22632</link>
      <description>arXiv:2505.22632v1 Announce Type: new 
Abstract: Collecting gold-standard phenotype data via manual extraction is typically labor-intensive and slow, whereas automated computational phenotypes (ACPs) offer a systematic and much faster alternative. However, simply replacing the gold-standard with ACPs, without acknowledging their differences, could lead to biased results and misleading conclusions. Motivated by the complexity of incorporating ACPs while maintaining the validity of downstream analyses, in this paper, we consider a semi-supervised learning setting that consists of both labeled data (with gold-standard) and unlabeled data (without gold-standard), under the covariate shift framework. We develop doubly robust and semiparametrically efficient estimators that leverage ACPs for general target parameters in the unlabeled and combined populations. In addition, we carefully analyze the efficiency gains achieved by incorporating ACPs, comparing scenarios with and without their inclusion. Notably, we identify that ACPs for the unlabeled data, instead of for the labeled data, drive the enhanced efficiency gains. To validate our theoretical findings, we conduct comprehensive synthetic experiments and apply our method to multiple real-world datasets, confirming the practical advantages of our approach. \hfill{\texttt{Code}: \href{https://github.com/brucejunjin/ICML2025-ACPCS}{\faGithub}}</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22632v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>2025 ICML</arxiv:journal_reference>
      <dc:creator>Chao Ying, Jun Jin, Yi Guo, Xiudi Li, Muxuan Liang, Jiwei Zhao</dc:creator>
    </item>
    <item>
      <title>Network Generating Processes With Self Exciting Arrival Times</title>
      <link>https://arxiv.org/abs/2505.22659</link>
      <description>arXiv:2505.22659v1 Announce Type: new 
Abstract: In this paper, we propose a novel modeling framework for time-evolving networks allowing for long-term dependence in network features that update in continuous time. Dynamic network growth is functionally parameterized via the conditional intensity of a marked point process. This characterization enables flexible modeling of both the time of updates and the network updates themselves, dependent on the entire left-continuous sample path. We propose a path-dependent nonlinear marked Hawkes process as an expressive platform for modeling such data; its dynamic mark space embeds the time-evolving network. We establish stability conditions, demonstrate simulation and subsequent feasible likelihood-based inference through numerical study, and present an application to conference attendee social network data. The resulting methodology serves as a general framework that can be readily adapted to a wide range of network topologies and point process model specifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22659v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Duncan A Clark, Conor J. Kresin, Charlotte M. Jones-Todd</dc:creator>
    </item>
    <item>
      <title>PrivATE: Differentially Private Confidence Intervals for Average Treatment Effects</title>
      <link>https://arxiv.org/abs/2505.21641</link>
      <description>arXiv:2505.21641v1 Announce Type: cross 
Abstract: The average treatment effect (ATE) is widely used to evaluate the effectiveness of drugs and other medical interventions. In safety-critical applications like medicine, reliable inferences about the ATE typically require valid uncertainty quantification, such as through confidence intervals (CIs). However, estimating treatment effects in these settings often involves sensitive data that must be kept private. In this work, we present PrivATE, a novel machine learning framework for computing CIs for the ATE under differential privacy. Specifically, we focus on deriving valid privacy-preserving CIs for the ATE from observational data. Our PrivATE framework consists of three steps: (i) estimating a differentially private ATE through output perturbation; (ii) estimating the differentially private variance through a truncated output perturbation mechanism; and (iii) constructing the CIs while accounting for the uncertainty from both the estimation and privatization steps. Our PrivATE framework is model agnostic, doubly robust, and ensures valid CIs. We demonstrate the effectiveness of our framework using synthetic and real-world medical datasets. To the best of our knowledge, we are the first to derive a general, doubly robust framework for valid CIs of the ATE under ($\varepsilon$, $\delta$)-differential privacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21641v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>stat.ME</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maresa Schr\"oder, Justin Hartenstein, Stefan Feuerriegel</dc:creator>
    </item>
    <item>
      <title>STACI: Spatio-Temporal Aleatoric Conformal Inference</title>
      <link>https://arxiv.org/abs/2505.21658</link>
      <description>arXiv:2505.21658v1 Announce Type: cross 
Abstract: Fitting Gaussian Processes (GPs) provides interpretable aleatoric uncertainty quantification for estimation of spatio-temporal fields. Spatio-temporal deep learning models, while scalable, typically assume a simplistic independent covariance matrix for the response, failing to capture the underlying correlation structure. However, spatio-temporal GPs suffer from issues of scalability and various forms of approximation bias resulting from restrictive assumptions of the covariance kernel function. We propose STACI, a novel framework consisting of a variational Bayesian neural network approximation of non-stationary spatio-temporal GP along with a novel spatio-temporal conformal inference algorithm. STACI is highly scalable, taking advantage of GPU training capabilities for neural network models, and provides statistically valid prediction intervals for uncertainty quantification. STACI outperforms competing GPs and deep methods in accurately approximating spatio-temporal processes and we show it easily scales to datasets with millions of observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21658v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brandon R. Feng, David Keetae Park, Xihaier Luo, Arantxa Urdangarin, Shinjae Yoo, Brian J. Reich</dc:creator>
    </item>
    <item>
      <title>Broad Spectrum Structure Discovery in Large-Scale Higher-Order Networks</title>
      <link>https://arxiv.org/abs/2505.21748</link>
      <description>arXiv:2505.21748v1 Announce Type: cross 
Abstract: Complex systems are often driven by higher-order interactions among multiple units, naturally represented as hypergraphs. Understanding dependency structures within these hypergraphs is crucial for understanding and predicting the behavior of complex systems but is made challenging by their combinatorial complexity and computational demands. In this paper, we introduce a class of probabilistic models that efficiently represents and discovers a broad spectrum of mesoscale structure in large-scale hypergraphs. The key insight enabling this approach is to treat classes of similar units as themselves nodes in a latent hypergraph. By modeling observed node interactions through latent interactions among classes using low-rank representations, our approach tractably captures rich structural patterns while ensuring model identifiability. This allows for direct interpretation of distinct node- and class-level structures. Empirically, our model improves link prediction over state-of-the-art methods and discovers interpretable structures in diverse real-world systems, including pharmacological and social networks, advancing the ability to incorporate large-scale higher-order data into the scientific process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21748v1</guid>
      <category>cs.SI</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Hood, Caterina De Bacco, Aaron Schein</dc:creator>
    </item>
    <item>
      <title>Spectral clustering for dependent community Hawkes process models of temporal networks</title>
      <link>https://arxiv.org/abs/2505.21845</link>
      <description>arXiv:2505.21845v1 Announce Type: cross 
Abstract: Temporal networks observed continuously over time through timestamped relational events data are commonly encountered in application settings including online social media communications, financial transactions, and international relations. Temporal networks often exhibit community structure and strong dependence patterns among node pairs. This dependence can be modeled through mutual excitations, where an interaction event from a sender to a receiver node increases the possibility of future events among other node pairs.
  We provide statistical results for a class of models that we call dependent community Hawkes (DCH) models, which combine the stochastic block model with mutually exciting Hawkes processes for modeling both community structure and dependence among node pairs, respectively. We derive a non-asymptotic upper bound on the misclustering error of spectral clustering on the event count matrix as a function of the number of nodes and communities, time duration, and the amount of dependence in the model. Our result leverages recent results on bounding an appropriate distance between a multivariate Hawkes process count vector and a Gaussian vector, along with results from random matrix theory. We also propose a DCH model that incorporates only self and reciprocal excitation along with highly scalable parameter estimation using a Generalized Method of Moments (GMM) estimator that we demonstrate to be consistent for growing network size and time duration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21845v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lingfei Zhao, Hadeel Soliman, Kevin S. Xu, Subhadeep Paul</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Experiments with Latent Outcomes: Key Results and Their Implications for Design and Analysis</title>
      <link>https://arxiv.org/abs/2505.21909</link>
      <description>arXiv:2505.21909v1 Announce Type: cross 
Abstract: How should we analyze randomized experiments in which the main outcome is measured in multiple ways and each measure contains some degree of error? Since Costner (1971) and Bagozzi (1977), methodological discussions of experiments with latent outcomes have reviewed the modeling assumptions that are invoked when the quantity of interest is the average treatment effect (ATE) of a randomized intervention on a latent outcome that is measured with error. Many authors have proposed methods to estimate this ATE when multiple measures of an outcome are available. Despite this extensive literature, social scientists rarely use these modeling approaches when analyzing experimental data, perhaps because the surge of interest in experiments coincides with increased skepticism about the modeling assumptions that these methods invoke. The present paper takes a fresh look at the use of latent variable models to analyze experiments. Like the skeptics, we seek to minimize reliance on ad hoc assumptions that are not rooted in the experimental design and measurement strategy. At the same time, we think that some of the misgivings that are frequently expressed about latent variable models can be addressed by modifying the research design in ways that make the underlying assumptions defensible or testable. We describe modeling approaches that enable researchers to identify and estimate key parameters of interest, suggest ways that experimental designs can be augmented so as to make the modeling requirements more credible, and discuss empirical tests of key modeling assumptions. Simulations and an empirical application illustrate the gains in terms of precision and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21909v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Fu, Donald P. Green</dc:creator>
    </item>
    <item>
      <title>Using Experiments to Correct for Selection in Observational Studies</title>
      <link>https://arxiv.org/abs/2006.09676</link>
      <description>arXiv:2006.09676v2 Announce Type: replace 
Abstract: Researchers increasingly have access to two types of data: (i) large observational datasets where treatment (e.g., class size) is not randomized but several primary outcomes (e.g., graduation rates) and secondary outcomes (e.g., test scores) are observed and (ii) experimental data in which treatment is randomized but only secondary outcomes are observed. We develop a new method to estimate treatment effects on primary outcomes in such settings. We use the difference between the secondary outcome and its predicted value based on the experimental treatment effect to measure selection bias in the observational data. Controlling for this estimate of selection bias yields an unbiased estimate of the treatment effect on the primary outcome under a new assumption that we term latent unconfoundedness, which requires that the same confounders affect the primary and secondary outcomes. Latent unconfoundedness weakens the assumptions underlying commonly used surrogate estimators. We apply our estimator to identify the effect of third grade class size on students outcomes. Estimated impacts on test scores using OLS regressions in observational school district data have the opposite sign of estimates from the Tennessee STAR experiment. In contrast, selection-corrected estimates in the observational data replicate the experimental estimates. Our estimator reveals that reducing class sizes by 25% increases high school graduation rates by 0.7 percentage points. Controlling for observables does not change the OLS estimates, demonstrating that experimental selection correction can remove biases that cannot be addressed with standard controls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2006.09676v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Susan Athey, Raj Chetty, Guido Imbens</dc:creator>
    </item>
    <item>
      <title>B-MASTER: Scalable Bayesian Multivariate Regression Analysis for Selecting Targeted Essential Regressors to Identify the Key Genera in Microbiome-Metabolite Relation Dynamics</title>
      <link>https://arxiv.org/abs/2412.05998</link>
      <description>arXiv:2412.05998v2 Announce Type: replace 
Abstract: We introduce B-MASTER (Bayesian Multivariate regression Analysis for Selecting Targeted Essential Regressors), a fully Bayesian framework for scalable multivariate regression in high dimensions. B-MASTER is designed to identify master predictors, i.e., covariates exerting widespread influence across many outcomes, via a hybrid penalty: an L1 penalty induces elementwise sparsity, while an L2 penalty enforces groupwise shrinkage across rows of the coefficient matrix. This structure selects a parsimonious set of key covariates, enhancing interpretability. A tailored Gibbs sampler achieves scalability, with runtime growing linearly in parameter dimension and remaining stable across sample sizes; full posterior inference is feasible for models with up to four million parameters. We establish posterior consistency and contraction rate results, showing that B-MASTER concentrates around the truth at the minimax-optimal rate under sparsity. These theoretical guarantees are supported by strong empirical performance; in simulations, B-MASTER outperforms competing methods in estimation and signal recovery. Applied to microbiome-metabolomics data from colorectal cancer patients, B-MASTER reveals microbial genera that shape broad metabolite profiles, uncovering relationships missed by other methods. The proposed approach is principled, interpretable, and scalable for discovering systemic patterns in ultra-high-dimensional biomedical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05998v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Priyam Das, Tanujit Dey, Christine Peterson, Sounak Chakraborty</dc:creator>
    </item>
    <item>
      <title>Hybrid Bayesian Models for Community Detection with Application to a Colombian Conflict Network</title>
      <link>https://arxiv.org/abs/2505.19399</link>
      <description>arXiv:2505.19399v2 Announce Type: replace 
Abstract: We introduce a flexible Bayesian framework for clustering nodes in undirected binary networks, motivated by the need to uncover structural patterns in complex environments. Building on the stochastic block model, we develop two hybrid extensions: the Class-Distance Model, which governs interaction probabilities through Euclidean distances between cluster-level latent positions, and the Class-Bilinear Model, which captures more complex relational patterns via bilinear interactions. We apply this framework to a novel network derived from the Colombian armed conflict, where municipalities are connected through the co-presence of armed actors, violence, and illicit economies. The resulting clusters align with empirical patterns of territorial control and trafficking corridors, highlighting the models' capacity to recover and explain complex dynamics. Full Bayesian inference is carried out via MCMC under both finite and nonparametric clustering priors. While the main application centers on the Colombian conflict, we also assess model performance using synthetic data as well as other two benchmark datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19399v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Sosa, Eleni Dilma, Brenda Betancourt</dc:creator>
    </item>
    <item>
      <title>Improving the Variance of Differentially Private Randomized Experiments through Clustering</title>
      <link>https://arxiv.org/abs/2308.00957</link>
      <description>arXiv:2308.00957v3 Announce Type: replace-cross 
Abstract: Estimating causal effects from randomized experiments is only possible if participants are willing to disclose their potentially sensitive responses. Differential privacy, a widely used framework for ensuring an algorithms privacy guarantees, can encourage participants to share their responses without the risk of de-anonymization. However, many mechanisms achieve differential privacy by adding noise to the original dataset, which reduces the precision of causal effect estimation. This introduces a fundamental trade-off between privacy and variance when performing causal analyses on differentially private data. In this work, we propose a new differentially private mechanism, "Cluster-DP", which leverages a given cluster structure in the data to improve the privacy-variance trade-off. While our results apply to any clustering, we demonstrate that selecting higher-quality clusters, according to a quality metric we introduce, can decrease the variance penalty without compromising privacy guarantees. Finally, we evaluate the theoretical and empirical performance of our Cluster-DP algorithm on both real and simulated data, comparing it to common baselines, including two special cases of our algorithm: its unclustered version and a uniform-prior version.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.00957v3</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adel Javanmard, Vahab Mirrokni, Jean Pouget-Abadie</dc:creator>
    </item>
    <item>
      <title>Optimal convex $M$-estimation via score matching</title>
      <link>https://arxiv.org/abs/2403.16688</link>
      <description>arXiv:2403.16688v2 Announce Type: replace-cross 
Abstract: In the context of linear regression, we construct a data-driven convex loss function with respect to which empirical risk minimisation yields optimal asymptotic variance in the downstream estimation of the regression coefficients. At the population level, the negative derivative of the optimal convex loss is the best decreasing approximation of the derivative of the log-density of the noise distribution. This motivates a fitting process via a nonparametric extension of score matching, corresponding to a log-concave projection of the noise distribution with respect to the Fisher divergence. At the sample level, our semiparametric estimator is computationally efficient, and we prove that it attains the minimal asymptotic covariance among all convex $M$-estimators. As an example of a non-log-concave setting, the optimal convex loss function for Cauchy errors is Huber-like, and our procedure yields asymptotic efficiency greater than $0.87$ relative to the maximum likelihood estimator of the regression coefficients that uses oracle knowledge of this error distribution. In this sense, we provide robustness and facilitate computation without sacrificing much statistical efficiency. Numerical experiments using our accompanying R package 'asm' confirm the practical merits of our proposal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16688v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oliver Y. Feng, Yu-Chun Kao, Min Xu, Richard J. Samworth</dc:creator>
    </item>
    <item>
      <title>PUATE: Efficient Average Treatment Effect Estimation from Treated (Positive) and Unlabeled Units</title>
      <link>https://arxiv.org/abs/2501.19345</link>
      <description>arXiv:2501.19345v2 Announce Type: replace-cross 
Abstract: The estimation of average treatment effects (ATEs), defined as the difference in expected outcomes between treatment and control groups, is a central topic in causal inference. This study develops semiparametric efficient estimators for ATE in a setting where only a treatment group and an unlabeled group, consisting of units whose treatment status is unknown, are observed. This scenario constitutes a variant of learning from positive and unlabeled data (PU learning) and can be viewed as a special case of ATE estimation with missing data. For this setting, we derive the semiparametric efficiency bounds, which characterize the lowest achievable asymptotic variance for regular estimators. We then construct semiparametric efficient ATE estimators that attain these bounds. Our results contribute to the literature on causal inference with missing data and weakly supervised learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19345v2</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato, Fumiaki Kozai, Ryo Inokuchi</dc:creator>
    </item>
    <item>
      <title>Spectrally Robust Covariance Shrinkage for Hotelling's $T^2$ in High Dimension</title>
      <link>https://arxiv.org/abs/2502.02006</link>
      <description>arXiv:2502.02006v2 Announce Type: replace-cross 
Abstract: We investigate covariance shrinkage for Hotelling's $T^2$ in the regime where the data dimension $p$ and the sample size $n$ grow in a fixed ratio -- without assuming that the population covariance matrix is spiked or well-conditioned. When $p/n\to\phi \in (0,1)$, we propose a practical finite-sample shrinker that, for any maximum-entropy signal prior and any fixed significance level, (a) asymptotically maximizes power under Gaussian data, and (b) asymptotically saturates the Hanson--Wright lower bound on power in the more general sub-Gaussian case. Our approach is to formulate and solve a variational problem characterizing the optimal limiting shrinker, and to show that our finite-sample method consistently approximates this limit by extending recent local random matrix laws. Empirical studies on simulated and real-world data, including the Crawdad UMich/RSS data set, demonstrate up to a $50\%$ gain in power over leading linear and nonlinear competitors at a significance level of $10^{-4}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02006v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin D. Robinson, Van Latimer</dc:creator>
    </item>
    <item>
      <title>Robust and Conjugate Spatio-Temporal Gaussian Processes</title>
      <link>https://arxiv.org/abs/2502.02450</link>
      <description>arXiv:2502.02450v2 Announce Type: replace-cross 
Abstract: State-space formulations allow for Gaussian process (GP) regression with linear-in-time computational cost in spatio-temporal settings, but performance typically suffers in the presence of outliers. In this paper, we adapt and specialise the robust and conjugate GP (RCGP) framework of Altamirano et al. (2024) to the spatio-temporal setting. In doing so, we obtain an outlier-robust spatio-temporal GP with a computational cost comparable to classical spatio-temporal GPs. We also overcome the three main drawbacks of RCGPs: their unreliable performance when the prior mean is chosen poorly, their lack of reliable uncertainty quantification, and the need to carefully select a hyperparameter by hand. We study our method extensively in finance and weather forecasting applications, demonstrating that it provides a reliable approach to spatio-temporal modelling in the presence of outliers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02450v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Laplante, Matias Altamirano, Andrew Duncan, Jeremias Knoblauch, Fran\c{c}ois-Xavier Briol</dc:creator>
    </item>
    <item>
      <title>Smooth Sailing: Lipschitz-Driven Uncertainty Quantification for Spatial Association</title>
      <link>https://arxiv.org/abs/2502.06067</link>
      <description>arXiv:2502.06067v2 Announce Type: replace-cross 
Abstract: Estimating associations between spatial covariates and responses - rather than merely predicting responses - is central to environmental science, epidemiology, and economics. For instance, public health officials might be interested in whether air pollution has a strictly positive association with a health outcome, and the magnitude of any effect. Standard machine learning methods often provide accurate predictions but offer limited insight into covariate-response relationships. And we show that existing methods for constructing confidence (or credible) intervals for associations fail to provide nominal coverage in the face of model misspecification and distribution shift - despite both being essentially always present in spatial problems. We introduce a method that constructs valid frequentist confidence intervals for associations in spatial settings. Our method requires minimal assumptions beyond a form of spatial smoothness. In particular, we do not require model correctness or covariate overlap between training and target locations. Our approach is the first to guarantee nominal coverage in this setting and outperforms existing techniques in both real and simulated experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06067v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David R. Burt, Renato Berlinghieri, Stephen Bates, Tamara Broderick</dc:creator>
    </item>
    <item>
      <title>Robust Representation and Estimation of Barycenters and Modes of Probability Measures on Metric Spaces</title>
      <link>https://arxiv.org/abs/2505.09609</link>
      <description>arXiv:2505.09609v2 Announce Type: replace-cross 
Abstract: This paper is concerned with the problem of defining and estimating statistics for distributions on spaces such as Riemannian manifolds and more general metric spaces. The challenge comes, in part, from the fact that statistics such as means and modes may be unstable: for example, a small perturbation to a distribution can lead to a large change in Fr\'echet means on spaces as simple as a circle. We address this issue by introducing a new merge tree representation of barycenters called the barycentric merge tree (BMT), which takes the form of a measured metric graph and summarizes features of the distribution in a multiscale manner. Modes are treated as special cases of barycenters through diffusion distances. In contrast to the properties of classical means and modes, we prove that BMTs are stable -- this is quantified as a Lipschitz estimate involving optimal transport metrics. This stability allows us to derive a consistency result for approximating BMTs from empirical measures, with explicit convergence rates. We also give a provably accurate method for discretely approximating the BMT construction and use this to provide numerical examples for distributions on spheres and shape spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09609v2</guid>
      <category>math.ST</category>
      <category>math.MG</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Washington Mio, Tom Needham</dc:creator>
    </item>
  </channel>
</rss>
