<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Sep 2024 01:48:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Distribution-Based Sub-Population Selection (DSPS): A Method for in-Silico Reproduction of Clinical Trials Outcomes</title>
      <link>https://arxiv.org/abs/2409.00232</link>
      <description>arXiv:2409.00232v1 Announce Type: new 
Abstract: Background and Objective: Diabetes presents a significant challenge to healthcare due to the negative impact of poor blood sugar control on health and associated complications. Computer simulation platforms, notably exemplified by the UVA/Padova Type 1 Diabetes simulator, has emerged as a promising tool for advancing diabetes treatments by simulating patient responses in a virtual environment. The UVA Virtual Lab (UVLab) is a new simulation platform to mimic the metabolic behavior of people with Type 2 diabetes (T2D) with a large population of 6062 virtual subjects. Methods: The work introduces the Distribution-Based Population Selection (DSPS) method, a systematic approach to identifying virtual subsets that mimic the clinical behavior observed in real trials. The method transforms the sub-population selection task into a Linear Programing problem, enabling the identification of the largest representative virtual cohort. This selection process centers on key clinical outcomes in diabetes research, such as HbA1c and Fasting plasma Glucose (FPG), ensuring that the statistical properties (moments) of the selected virtual sub-population closely resemble those observed in real-word clinical trial. Results: DSPS method was applied to the insulin degludec (IDeg) arm of a phase 3 clinical trial. This method was used to select a sub-population of virtual subjects that closely mirrored the clinical trial data across multiple key metrics, including glycemic efficacy, insulin dosages, and cumulative hypoglycemia events over a 26-week period. Conclusion: The DSPS algorithm is able to select virtual sub-population within UVLab to reproduce and predict the outcomes of a clinical trial. This statistical method can bridge the gap between large population simulation platforms and previously conducted clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00232v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammadreza Ganji, Anas El Fathi Ph. D., Chiara Fabris Ph. D., Dayu Lv Ph. D., Boris Kovatchev Ph. D., Marc Breton Ph. D</dc:creator>
    </item>
    <item>
      <title>Data is missing again -- Reconstruction of power generation data using $k$-Nearest Neighbors and spectral graph theory</title>
      <link>https://arxiv.org/abs/2409.00300</link>
      <description>arXiv:2409.00300v1 Announce Type: new 
Abstract: The risk of missing data and subsequent incomplete data records at wind farms increases with the number of turbines and sensors. We propose here an imputation method that blends data-driven concepts with expert knowledge, by using the geometry of the wind farm in order to provide better estimates when performing Nearest Neighbor imputation. Our method relies on learning Laplacian eigenmaps out of the graph of the wind farm through spectral graph theory. These learned representations can be based on the wind farm layout only, or additionally account for information provided by collected data. The related weighted graph is allowed to change with time and can be tracked in an online fashion. Application to the Westermost Rough offshore wind farm shows significant improvement over approaches that do not account for the wind farm layout information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00300v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amandine Pierrot, Pierre Pinson</dc:creator>
    </item>
    <item>
      <title>A method to convert traditional fingerprint ACE / ACE-V outputs ("identification", "inconclusive", "exclusion") to Bayes factors</title>
      <link>https://arxiv.org/abs/2409.00451</link>
      <description>arXiv:2409.00451v1 Announce Type: new 
Abstract: Fingerprint examiners appear to be reluctant to adopt probabilistic reasoning, statistical models, and empirical validation. The rate of adoption of the likelihood-ratio framework by fingerprint practitioners appears to be near zero. A factor in the reluctance to adopt the likelihood-ratio framework may be a perception that it would require a radical change in practice. The present paper proposes a small step that would require minimal changes to current practice. It proposes and demonstrates a method to convert traditional fingerprint-examination outputs ("identification", "inconclusive", "exclusion") to well-calibrated Bayes factors. The method makes use of a beta-binomial model, and both uninformative and informative priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00451v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Geoffrey Stewart Morrison</dc:creator>
    </item>
    <item>
      <title>Localizing Single and Multiple Oscillatory Sources: A Frequency Divider Approach</title>
      <link>https://arxiv.org/abs/2409.00566</link>
      <description>arXiv:2409.00566v1 Announce Type: new 
Abstract: Localizing sources of troublesome oscillations, particularly forced oscillations (FOs), in power systems has received considerable attention over the last few years. This is driven in part by the massive deployment of phasor measurement units (PMUs) that capture these oscillations when they occur; and in part by the increasing incidents of FOs due to malfunctioning components, wind power fluctuations, and/or cyclic loads. Capitalizing on the frequency divider formula of [1], we develop methods to localize single and multiple oscillatory sources using bus frequency measurements. The method to localize a single oscillation source does not require knowledge of network parameters. However, the method for localizing FOs caused by multiple sources requires this knowledge. We explain the reasoning behind this knowledge difference as well as demonstrate the success of our methods for source localization in multiple test systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00566v1</guid>
      <category>stat.AP</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajasekhar Anguluri, Anamitra Pal</dc:creator>
    </item>
    <item>
      <title>Tonal coarticulation revisited: functional covariance analysis to investigate the planning of co-articulated tones by Standard Chinese speakers</title>
      <link>https://arxiv.org/abs/2409.01194</link>
      <description>arXiv:2409.01194v1 Announce Type: new 
Abstract: We aim to explain whether a stress memory task has a significant impact on tonal coarticulation. We contribute a novel approach to analyse tonal coarticulation in phonetics, where several f0 contours are compared with respect to their vibrations at higher resolution, something that in statistical terms is called variation of the second order. We identify speech recording frequency curves as functional observations and harness inspiration from the mathematical fields of functional data analysis and optimal transport. By leveraging results from these two disciplines, we make one key observation:we identify the time and frequency covariance functions as crucial features for capturing the finer effects of tonal coarticulation. This observation leads us to propose a 2 steps approach where the mean functions are modelled via Generalized Additive Models, and the residuals of such models are investigated for any structure nested at covariance level. If such structure exist, we describe the variation manifested by the covariances through covariance principal component analysis. The 2-steps approach allows to uncover any variation not explained by generalized additive modelling, as well as fill a known shortcoming of these models into incorporating complex correlation structures in the data. The proposed method is illustrated on an articulatory dataset contrasting the pronunciation non-sensical bi-syllabic combinations in the presence of a short-memory challenge</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01194v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Valentina Masarotto, Yiya Chen</dc:creator>
    </item>
    <item>
      <title>Conditional multi-step attribution for climate forcings</title>
      <link>https://arxiv.org/abs/2409.01396</link>
      <description>arXiv:2409.01396v1 Announce Type: new 
Abstract: Attribution of climate impacts to a source forcing is critical to understanding, communicating, and addressing the effects of human influence on the climate. While standard attribution methods, such as optimal fingerprinting, have been successfully applied to long-term, widespread effects such as global surface temperature warming, they often struggle in low signal-to-noise regimes, typical of short-term climate forcings or climate variables which are loosely related to the forcing. Single-step approaches, which directly relate a source forcing and final impact, are unable to utilize additional climate information to improve attribution certainty. To address this shortcoming, this paper presents a novel multi-step attribution approach which is capable of analyzing multiple variables conditionally. A connected series of climate effects are treated as dependent, and relationships found in intermediary steps of a causal pathway are leveraged to better characterize the forcing impact. This enables attribution of the forcing level responsible for the observed impacts, while equivalent single-step approaches fail. Utilizing a scalar feature describing the forcing impact, simple forcing response models, and a conditional Bayesian formulation, this method can incorporate several causal pathways to identify the correct forcing magnitude. As an exemplar of a short-term, high-variance forcing, we demonstrate this method for the 1991 eruption of Mt. Pinatubo. Results indicate that including stratospheric and surface temperature and radiative flux measurements increases attribution certainty compared to analyses derived solely from temperature measurements. This framework has potential to improve climate attribution assessments for both geoengineering projects and long-term climate change, for which standard attribution methods may fail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01396v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher R. Wentland, Michael Weylandt, Laura P. Swiler, Thomas S. Ehrmann, Diana Bull</dc:creator>
    </item>
    <item>
      <title>Estimating Treatment Effect Heterogeneity in Psychiatry: A Review and Tutorial with Causal Forests</title>
      <link>https://arxiv.org/abs/2409.01578</link>
      <description>arXiv:2409.01578v1 Announce Type: new 
Abstract: Flexible machine learning tools are being used increasingly to estimate heterogeneous treatment effects. This paper gives an accessible tutorial demonstrating the use of the causal forest algorithm, available in the R package grf. We start with a brief non-technical overview of treatment effect estimation methods with a focus on estimation in observational studies, although similar methods can be used in experimental studies. We then discuss the logic of estimating heterogeneous effects using the extension of the random forest algorithm implemented in grf. Finally, we illustrate causal forest by conducting a secondary analysis on the extent to which individual differences in resilience to high combat stress can be measured among US Army soldiers deploying to Afghanistan based on information about these soldiers available prior to deployment. Throughout we illustrate simple and interpretable exercises for both model selection and evaluation, including targeting operator characteristics curves, Qini curves, area-under-the-curve summaries, and best linear projections. A replication script with simulated data is available at github.com/grf-labs/grf/tree/master/experiments/ijmpr</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01578v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erik Sverdrup, Maria Petukhova, Stefan Wager</dc:creator>
    </item>
    <item>
      <title>Analysis of driving factors for carbon emissions in China based on ARIMA-BP model</title>
      <link>https://arxiv.org/abs/2409.00039</link>
      <description>arXiv:2409.00039v1 Announce Type: cross 
Abstract: China accounts for one-third of the world's total carbon emissions. How to reach the peak of carbon emissions by 2030 and achieve carbon neutrality by 2060 to ensure the effective realization of the "dual-carbon" target is an important policy orientation at present. Based on the provincial panel data of ARIMA-BP model, this paper shows that the effect of energy consumption intensity effect is the main factor driving the growth of carbon emissions, per capita GDP and energy consumption structure effect are the main factors to inhibit carbon emissions, and the effect of industrial structure and population size effect is relatively small. Based on the research conclusion, the policy suggestions are put forward from the aspects of energy structure, industrial structure, new quality productivity and digital economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00039v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanglin Zhao, Hao Deng, Bingkun Yuan</dc:creator>
    </item>
    <item>
      <title>Variable selection in the joint frailty model of recurrent and terminal events using Broken Adaptive Ridge regression</title>
      <link>https://arxiv.org/abs/2409.00291</link>
      <description>arXiv:2409.00291v1 Announce Type: cross 
Abstract: We introduce a novel method to simultaneously perform variable selection and estimation in the joint frailty model of recurrent and terminal events using the Broken Adaptive Ridge Regression penalty. The BAR penalty can be summarized as an iteratively reweighted squared $L_2$-penalized regression, which approximates the $L_0$-regularization method. Our method allows for the number of covariates to diverge with the sample size. Under certain regularity conditions, we prove that the BAR estimator implemented under the model framework is consistent and asymptotically normally distributed, which are known as the oracle properties in the variable selection literature. In our simulation studies, we compare our proposed method to the Minimum Information Criterion (MIC) method. We apply our method on the Medical Information Mart for Intensive Care (MIMIC-III) database, with the aim of investigating which variables affect the risks of repeated ICU admissions and death during ICU stay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00291v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Chan, Fatemeh Mahmoudi, Chel Hee Lee, Quan Long, Xuewen Lu</dc:creator>
    </item>
    <item>
      <title>Personalized Pricing Decisions Through Adversarial Risk Analysis</title>
      <link>https://arxiv.org/abs/2409.00444</link>
      <description>arXiv:2409.00444v1 Announce Type: cross 
Abstract: Pricing decisions stand out as one of the most critical tasks a company faces, particularly in today's digital economy. As with other business decision-making problems, pricing unfolds in a highly competitive and uncertain environment. Traditional analyses in this area have heavily relied on game theory and its variants. However, an important drawback of these approaches is their reliance on common knowledge assumptions, which are hardly tenable in competitive business domains. This paper introduces an innovative personalized pricing framework designed to assist decision-makers in undertaking pricing decisions amidst competition, considering both buyer's and competitors' preferences. Our approach (i) establishes a coherent framework for modeling competition mitigating common knowledge assumptions; (ii) proposes a principled method to forecast competitors' pricing and customers' purchasing decisions, acknowledging major business uncertainties; and, (iii) encourages structured thinking about the competitors' problems, thus enriching the solution process. To illustrate these properties, in addition to a general pricing template, we outline two specifications - one from the retail domain and a more intricate one from the pension fund domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00444v1</guid>
      <category>cs.GT</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Garc\'ia Rasines, Roi Naveiro, David R\'ios Insua, Sim\'on Rodr\'iguez Santana</dc:creator>
    </item>
    <item>
      <title>Bayesian nonparametric mixtures of categorical directed graphs for heterogeneous causal inference</title>
      <link>https://arxiv.org/abs/2409.00453</link>
      <description>arXiv:2409.00453v1 Announce Type: cross 
Abstract: Quantifying causal effects of exposures on outcomes, such as a treatment and a disease respectively, is a crucial issue in medical science for the administration of effective therapies. Importantly, any related causal analysis should account for all those variables, e.g. clinical features, that can act as risk factors involved in the occurrence of a disease. In addition, the selection of targeted strategies for therapy administration requires to quantify such treatment effects at personalized level rather than at population level. We address these issues by proposing a methodology based on categorical Directed Acyclic Graphs (DAGs) which provide an effective tool to infer causal relationships and causal effects between variables. In addition, we account for population heterogeneity by considering a Dirichlet Process mixture of categorical DAGs, which clusters individuals into homogeneous groups characterized by common causal structures, dependence parameters and causal effects. We develop computational strategies for Bayesian posterior inference, from which a battery of causal effects at subject-specific level is recovered. Our methodology is evaluated through simulations and applied to a dataset of breast cancer patients to investigate cardiotoxic side effects that can be induced by the administrated anticancer therapies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00453v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico Castelletti, Laura Ferrini</dc:creator>
    </item>
    <item>
      <title>Statistics of punctuation in experimental literature -- the remarkable case of "Finnegans Wake" by James Joyce</title>
      <link>https://arxiv.org/abs/2409.00483</link>
      <description>arXiv:2409.00483v1 Announce Type: cross 
Abstract: As the recent studies indicate, the structure imposed onto written texts by the presence of punctuation develops patterns which reveal certain characteristics of universality. In particular, based on a large collection of classic literary works, it has been evidenced that the distances between consecutive punctuation marks, measured in terms of the number of words, obey the discrete Weibull distribution - a discrete variant of a distribution often used in survival analysis. The present work extends the analysis of punctuation usage patterns to more experimental pieces of world literature. It turns out that the compliance of the the distances between punctuation marks with the discrete Weibull distribution typically applies here as well. However, some of the works by James Joyce are distinct in this regard - in the sense that the tails of the relevant distributions are significantly thicker and, consequently, the corresponding hazard functions are decreasing functions not observed in typical literary texts in prose. "Finnegans Wake" - the same one to which science owes the word "quarks" for the most fundamental constituents of matter - is particularly striking in this context. At the same time, in all the studied texts, the sentence lengths - representing the distances between sentence-ending punctuation marks - reveal more freedom and are not constrained by the discrete Weibull distribution. This freedom in some cases translates into long-range nonlinear correlations, which manifest themselves in multifractality. Again, a text particularly spectacular in terms of multifractality is "Finnegans Wake".</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00483v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1063/5.0203530</arxiv:DOI>
      <arxiv:journal_reference>Chaos 34, 083124 (2024)</arxiv:journal_reference>
      <dc:creator>Tomasz Stanisz, Stanis{\l}aw Dro\.zd\.z, Jaros{\l}aw Kwapie\'n</dc:creator>
    </item>
    <item>
      <title>Statistical Jump Model for Mixed-Type Data with Missing Data Imputation</title>
      <link>https://arxiv.org/abs/2409.01208</link>
      <description>arXiv:2409.01208v1 Announce Type: cross 
Abstract: In this paper, we address the challenge of clustering mixed-type data with temporal evolution by introducing the statistical jump model for mixed-type data. This novel framework incorporates regime persistence, enhancing interpretability and reducing the frequency of state switches, and efficiently handles missing data. The model is easily interpretable through its state-conditional means and modes, making it accessible to practitioners and policymakers. We validate our approach through extensive simulation studies and an empirical application to air quality data, demonstrating its superiority in inferring persistent air quality regimes compared to the traditional air quality index. Our contributions include a robust method for mixed-type temporal clustering, effective missing data management, and practical insights for environmental monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01208v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico P. Cortese, Antonio Pievatolo</dc:creator>
    </item>
    <item>
      <title>Plasmode simulation for the evaluation of causal inference methods in homophilous social networks</title>
      <link>https://arxiv.org/abs/2409.01316</link>
      <description>arXiv:2409.01316v1 Announce Type: cross 
Abstract: Typical simulation approaches for evaluating the performance of statistical methods on populations embedded in social networks may fail to capture important features of real-world networks. It can therefore be unclear whether inference methods for causal effects due to interference that have been shown to perform well in such synthetic networks are applicable to social networks which arise in the real world. Plasmode simulation studies use a real dataset created from natural processes, but with part of the data-generation mechanism known. However, given the sensitivity of relational data, many network data are protected from unauthorized access or disclosure. In such case, plasmode simulations cannot use released versions of real datasets which often omit the network links, and instead can only rely on parameters estimated from them. A statistical framework for creating replicated simulation datasets from private social network data is developed and validated. The approach consists of simulating from a parametric exponential family random graph model fitted to the network data and resampling from the observed exposure and covariate distributions to preserve the associations among these variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01316v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Vanessa McNealis, Erica E. M. Moodie, Nema Dean</dc:creator>
    </item>
    <item>
      <title>Estimating Heterogenous Treatment Effects for Survival Data with Doubly Doubly Robust Estimator</title>
      <link>https://arxiv.org/abs/2409.01412</link>
      <description>arXiv:2409.01412v1 Announce Type: cross 
Abstract: In this paper, we introduce a doubly doubly robust estimator for the average and heterogeneous treatment effect for left-truncated-right-censored (LTRC) survival data. In causal inference for survival functions in LTRC survival data, two missing data issues are noteworthy: one is the missing data of counterfactuals for causal inference, and the other is the missing data due to truncation and censoring. Based on previous research on non-parametric deep learning estimation in survival analysis, this paper proposes an algorithm to obtain an efficient estimate of the average and heterogeneous causal effect. We simulate the data and compare our methods with the marginal hazard ratio estimation, the naive plug-in estimation, and the doubly robust causal with Cox Proportional Hazard estimation and illustrate the advantages and disadvantages of the model application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01412v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guanghui Pan</dc:creator>
    </item>
    <item>
      <title>Multi-objective Bayesian optimization for Likelihood-Free inference in sequential sampling models of decision making</title>
      <link>https://arxiv.org/abs/2409.01735</link>
      <description>arXiv:2409.01735v2 Announce Type: cross 
Abstract: Joint modeling of different data sources in decision-making processes is crucial for understanding decision dynamics in consumer behavior models. Sequential Sampling Models (SSMs), grounded in neuro-cognitive principles, provide a systematic approach to combining information from multi-source data, such as those based on response times and choice outcomes. However, parameter estimation of SSMs is challenging due to the complexity of joint likelihood functions. Likelihood-Free inference (LFI) approaches enable Bayesian inference in complex models with intractable likelihoods, like SSMs, and only require the ability to simulate synthetic data from the model. Extending a popular approach to simulation efficient LFI for single-source data, we propose Multi-objective Bayesian Optimization for Likelihood-Free Inference (MOBOLFI) to estimate the parameters of SSMs calibrated using multi-source data. MOBOLFI models a multi-dimensional discrepancy between observed and simulated data, using a discrepancy for each data source. Multi-objective Bayesian Optimization is then used to ensure simulation efficient approximation of the SSM likelihood. The use of a multivariate discrepancy allows for approximations to individual data source likelihoods in addition to the joint likelihood, enabling both the detection of conflicting information and a deeper understanding of the importance of different data sources in estimating individual SSM parameters. We illustrate the advantages of our approach in comparison with the use of a single discrepancy in a simple synthetic data example and an SSM example with real-world data assessing preferences of ride-hailing drivers in Singapore to rent electric vehicles. Although we focus on applications to SSMs, our approach applies to the Likelihood-Free calibration of other models using multi-source data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01735v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Chen, Xinwei Li, Eui-Jin Kim, Prateek Bansal, David Nott</dc:creator>
    </item>
    <item>
      <title>Partial membership models for soft clustering of multivariate football player performance data</title>
      <link>https://arxiv.org/abs/2409.01874</link>
      <description>arXiv:2409.01874v1 Announce Type: cross 
Abstract: The standard mixture modelling framework has been widely used to study heterogeneous populations, by modelling them as being composed of a finite number of homogeneous sub-populations. However, the standard mixture model assumes that each data point belongs to one and only one mixture component, or cluster, but when data points have fractional membership in multiple clusters this assumption is unrealistic. It is in fact conceptually very different to represent an observation as partly belonging to multiple groups instead of belonging to one group with uncertainty. For this purpose, various soft clustering approaches, or individual-level mixture models, have been developed. In this context, Heller et al (2008) formulated the Bayesian partial membership model (PM) as an alternative structure for individual-level mixtures, which also captures partial membership in the form of attribute specific mixtures, but does not assume a factorization over attributes. Our work proposes using the PM for soft clustering of count data arising in football performance analysis and compare the results with those achieved with the mixed membership model and finite mixture model. Learning and inference are carried out using Markov chain Monte Carlo methods. The method is applied on Serie A football player data from the 2022/2023 football season, to estimate the positions on the field where the players tend to play, in addition to their primary position, based on their playing style. The application of partial membership model to football data could have practical implications for coaches, talent scouts, team managers and analysts. These stakeholders can utilize the findings to make informed decisions related to team strategy, talent acquisition, and statistical research, ultimately enhancing performance and understanding in the field of football.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01874v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emiliano Seri, Roberto Rocci, Thomas Brendan Murphy</dc:creator>
    </item>
    <item>
      <title>Bayesian CART models for aggregate claim modeling</title>
      <link>https://arxiv.org/abs/2409.01908</link>
      <description>arXiv:2409.01908v1 Announce Type: cross 
Abstract: This paper proposes three types of Bayesian CART (or BCART) models for aggregate claim amount, namely, frequency-severity models, sequential models and joint models. We propose a general framework for the BCART models applicable to data with multivariate responses, which is particularly useful for the joint BCART models with a bivariate response: the number of claims and aggregate claim amount. To facilitate frequency-severity modeling, we investigate BCART models for the right-skewed and heavy-tailed claim severity data by using various distributions. We discover that the Weibull distribution is superior to gamma and lognormal distributions, due to its ability to capture different tail characteristics in tree models. Additionally, we find that sequential BCART models and joint BCART models, which incorporate dependence between the number of claims and average severity, are beneficial and thus preferable to the frequency-severity BCART models in which independence is assumed. The effectiveness of these models' performance is illustrated by carefully designed simulations and real insurance data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01908v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yaojun Zhang, Lanpeng Ji, Georgios Aivaliotis, Charles C. Taylor</dc:creator>
    </item>
    <item>
      <title>Collective risk models with FGM dependence</title>
      <link>https://arxiv.org/abs/2209.13543</link>
      <description>arXiv:2209.13543v4 Announce Type: replace 
Abstract: We study copula-based collective risk models when the dependence structure is defined by a Farlie-Gumbel-Morgenstern (FGM) copula. By leveraging a one-to-one correspondence between the class of FGM copulas and multivariate symmetric Bernoulli distributions, we find convenient representations for the moments and Laplace-Stieltjes transform for the aggregate random variable defined from collective risk models with FGM dependence. We examine different components of this collective risk model, aiming to better understand the impact of the assumed dependence between a claim's frequency and severity. Relying on stochastic ordering, we analyze the impact of dependence on the aggregate claim amount random variable. Even if the FGM copula may only induce moderate dependence, we illustrate through numerical examples that the cumulative effect of FGM dependence can lead to substantial variations in key risk measures on aggregate random variables defined from collective risk models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.13543v4</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher Blier-Wong, H\'el\`ene Cossette, Etienne Marceau</dc:creator>
    </item>
    <item>
      <title>Inferring Mobility of Care Travel Behavior From Transit Origin-Destination Data</title>
      <link>https://arxiv.org/abs/2211.04915</link>
      <description>arXiv:2211.04915v4 Announce Type: replace 
Abstract: There are substantial differences in travel behavior by gender on public transit. Studies have concluded that these differences are largely attributable to household responsibilities typically falling disproportionately on women, leading to women being more likely to utilize transit for purposes referred to by the umbrella concept of "mobility of care". In contrast to past studies that have quantified the impact of gender using survey and qualitative data, we propose a novel data-driven workflow utilizing a combination of previously developed origin, destination, and transfer inference (ODX) based on individual transit fare card transactions, name-based gender inference, and geospatial analysis as a framework to identify mobility of care trip making. We apply this framework to data from the Washington Metropolitan Area Transit Authority (WMATA). Analyzing data from millions of journeys conducted in the first quarter of 2019, the results of this study show that our proposed workflow can identify mobility of care travel behavior, detecting times and places of interest where the share of women travelers in an equally-sampled subset (on basis of inferred gender) of transit users is 10% - 15% higher than that of men. The workflow presented in this study provides a blueprint for combining transit origin-destination data, inferred customer demographics, and geospatial analyses enabling public transit agencies to assess, at the fare card level, the gendered impacts of different policy and operational decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.04915v4</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Awad Abdelhalim, Daniela Shuman, Anson F Stewart, Kayleigh B Campbell, Mira Patel, Ines Sanchez de Madariaga, Jinhua Zhao</dc:creator>
    </item>
    <item>
      <title>Estimating the Number of Street Vendors in New York City</title>
      <link>https://arxiv.org/abs/2406.00527</link>
      <description>arXiv:2406.00527v3 Announce Type: replace 
Abstract: We estimate the number of street vendors in New York City. First, we summarize the process by which vendors receive licenses and permits to operate legally in New York City. Second, we describe a survey that was administered by the Street Vendor Project while distributing Coronavirus relief aid to vendors operating in New York City both with and without a license or permit. Third, we review ratio estimation and provide a theoretical justification based on the theory of point processes. Fourth, we use ratio estimation to calculate the total number of vendors, finding approximately 23,000 street vendors operate in New York City (20,500 mobile food vendors and 2,400 general merchandise vendors) with one third located in just six ZIP Codes (11368 (16%), 11372 (3%), and 11354 (3%) in North and West Queens and 10036 (5%), 10019 (4%), and 10001 (3%) in the Chelsea and Clinton neighborhoods of Manhattan). Finally, we evaluate the accuracy of the ratio estimator when the distribution of vendors is explained by a Poisson or Yule process, and we discuss several policy implications. In particular, our estimates suggest the American Community Survey misses the majority of New York City street vendors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00527v3</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Auerbach</dc:creator>
    </item>
    <item>
      <title>Extending the blended generalized extreme value distribution</title>
      <link>https://arxiv.org/abs/2407.06875</link>
      <description>arXiv:2407.06875v2 Announce Type: replace 
Abstract: The generalized extreme value (GEV) distribution is commonly employed to help estimate the likelihood of extreme events in many geophysical and other application areas. The recently proposed blended generalized extreme value (bGEV) distribution modifies the GEV with positive shape parameter to avoid a hard lower bound that complicates fitting and inference. Here, the bGEV is extended to the GEV with negative shape parameter, avoiding a hard upper bound that is unrealistic in many applications. This extended bGEV is shown to improve on the GEV for forecasting heat and sea level extremes based on past data. Software implementing this bGEV and applying it to the example temperature and sea level data is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06875v2</guid>
      <category>stat.AP</category>
      <category>physics.ao-ph</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nir Y. Krakauer</dc:creator>
    </item>
    <item>
      <title>Beta-CoRM: A Bayesian Approach for $n$-gram Profiles Analysis</title>
      <link>https://arxiv.org/abs/2011.11558</link>
      <description>arXiv:2011.11558v3 Announce Type: replace-cross 
Abstract: $n$-gram profiles have been successfully and widely used to analyse long sequences of potentially differing lengths for clustering or classification. Mainly, machine learning algorithms have been used for this purpose but, despite their predictive performance, these methods cannot discover hidden structures or provide a full probabilistic representation of the data. A novel class of Bayesian generative models designed for $n$-gram profiles used as binary attributes have been designed to address this. The flexibility of the proposed modelling allows to consider a straightforward approach to feature selection in the generative model. Furthermore, a slice sampling algorithm is derived for a fast inferential procedure, which is applied to synthetic and real data scenarios and shows that feature selection can improve classification accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2011.11558v3</guid>
      <category>stat.ME</category>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jos\'e A. Perusqu\'ia, Jim E. Griffin, Cristiano Villa</dc:creator>
    </item>
    <item>
      <title>Leave-group-out cross-validation for latent Gaussian models</title>
      <link>https://arxiv.org/abs/2210.04482</link>
      <description>arXiv:2210.04482v5 Announce Type: replace-cross 
Abstract: Evaluating the predictive performance of a statistical model is commonly done using cross-validation. Although the leave-one-out method is frequently employed, its application is justified primarily for independent and identically distributed observations. However, this method tends to mimic interpolation rather than prediction when dealing with dependent observations. This paper proposes a modified cross-validation for dependent observations. This is achieved by excluding an automatically determined set of observations from the training set to mimic a more reasonable prediction scenario. Also, within the framework of latent Gaussian models, we illustrate a method to adjust the joint posterior for this modified cross-validation to avoid model refitting. This new approach is accessible in the R-INLA package (www.r-inla.org).</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.04482v5</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhedong Liu, Haavard Rue</dc:creator>
    </item>
    <item>
      <title>A multi-language toolkit for supporting automated checking of research outputs</title>
      <link>https://arxiv.org/abs/2212.02935</link>
      <description>arXiv:2212.02935v2 Announce Type: replace-cross 
Abstract: This article presents the automatic checking of research outputs package acro, which assists researchers and data governance teams by automatically applying best-practice principles-based statistical disclosure control (SDC) techniques on-the-fly as researchers conduct their analyses. acro distinguishes between: research output that is safe to publish; output that requires further analysis; and output that cannot be published because it creates substantial risk of disclosing private data. This is achieved through the use of a lightweight Python wrapper that sits over well-known analysis tools that produce outputs such as tables, plots, and statistical models. This adds functionality to (i) identify potentially disclosive outputs against a range of commonly used disclosure tests; (ii) apply disclosure mitigation strategies where required; (iii) report reasons for applying SDC; and (iv) produce simple summary documents trusted research environment staff can use to streamline their workflow. The major analytical programming languages used by researchers are supported: Python, R, and Stata. The acro code and documentation are available under an MIT license at https://github.com/AI-SDC/ACRO</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.02935v2</guid>
      <category>cs.CR</category>
      <category>cs.IR</category>
      <category>cs.SE</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Richard J. Preen, Maha Albashir, Simon Davy, Jim Smith</dc:creator>
    </item>
    <item>
      <title>Scalable Estimation of Multinomial Response Models with Random Consideration Sets</title>
      <link>https://arxiv.org/abs/2308.12470</link>
      <description>arXiv:2308.12470v3 Announce Type: replace-cross 
Abstract: A common assumption in the fitting of unordered multinomial response models for $J$ mutually exclusive categories is that the responses arise from the same set of $J$ categories across subjects. However, when responses measure a choice made by the subject, it is more appropriate to condition the distribution of multinomial responses on a subject-specific consideration set, drawn from the power set of $\{1,2,\ldots,J\}$. This leads to a mixture of multinomial response models governed by a probability distribution over the $J^{\ast} = 2^J -1$ consideration sets. We introduce a novel method for estimating such generalized multinomial response models based on the fundamental result that any mass distribution over $J^{\ast}$ consideration sets can be represented as a mixture of products of $J$ component-specific inclusion-exclusion probabilities. Moreover, under time-invariant consideration sets, the conditional posterior distribution of consideration sets is sparse. These features enable a scalable MCMC algorithm for sampling the posterior distribution of parameters, random effects, and consideration sets. Under regularity conditions, the posterior distributions of the marginal response probabilities and the model parameters satisfy consistency. The methodology is demonstrated in a longitudinal data set on weekly cereal purchases that cover $J = 101$ brands, a dimension substantially beyond the reach of existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12470v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siddhartha Chib, Kenichi Shimizu</dc:creator>
    </item>
    <item>
      <title>Bivariate change point detection in movement direction and speed</title>
      <link>https://arxiv.org/abs/2402.02489</link>
      <description>arXiv:2402.02489v2 Announce Type: replace-cross 
Abstract: Biological movement patterns can sometimes be quasi linear with abrupt changes in direction and speed, as in plastids in root cells investigated here. For the analysis of such changes we propose a new stochastic model for movement along linear structures. Maximum likelihood estimators are provided, and due to serial dependencies of increments, the classical MOSUM statistic is replaced by a moving kernel estimator. Convergence of the resulting difference process and strong consistency of the variance estimator are shown. We estimate the change points and propose a graphical technique to distinguish between change points in movement direction and speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02489v2</guid>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Solveig Plomer, Theresa Ernst, Philipp Gebhardt, Enrico Schleiff, Ralph Neininger, Gaby Schneider</dc:creator>
    </item>
    <item>
      <title>Quantile balancing inverse probability weighting for non-probability samples</title>
      <link>https://arxiv.org/abs/2403.09726</link>
      <description>arXiv:2403.09726v2 Announce Type: replace-cross 
Abstract: Usage of non-statistical data sources for statistical purposes have become increasingly popular in recent years, also in official statistics. However, statistical inference based on non-probability samples is made more difficult by nature of them being biased and not representative of the target population. In this paper we propose quantile balancing inverse probability weighting estimator (QBIPW) for non-probability samples. We use the idea of Harms and Duchesne (2006) which allows to include quantile information in the estimation process so known totals and distribution for auxiliary variables are being reproduced. We discuss the estimation of the QBIPW probabilities and its variance. Our simulation study has demonstrated that the proposed estimators are robust against model mis-specification and, as a result, help to reduce bias and mean squared error. Finally, we applied the proposed method to estimate the share of vacancies aimed at Ukrainian workers in Poland using an integrated set of administrative and survey data about job vacancies</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09726v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maciej Ber\k{e}sewicz, Marcin Szymkowiak, Piotr Chlebicki</dc:creator>
    </item>
    <item>
      <title>A real-time digital twin of azimuthal thermoacoustic instabilities</title>
      <link>https://arxiv.org/abs/2404.18793</link>
      <description>arXiv:2404.18793v2 Announce Type: replace-cross 
Abstract: When they occur, azimuthal thermoacoustic oscillations can detrimentally affect the safe operation of gas turbines and aeroengines. We develop a real-time digital twin of azimuthal thermoacoustics of a hydrogen-based annular combustor. The digital twin seamlessly combines two sources of information about the system (i) a physics-based low-order model; and (ii) raw and sparse experimental data from microphones, which contain both aleatoric noise and turbulent fluctuations. First, we derive a low-order thermoacoustic model for azimuthal instabilities, which is deterministic. Second, we propose a real-time data assimilation framework to infer the acoustic pressure, the physical parameters, and the model and measurement biases simultaneously. This is the bias-regularized ensemble Kalman filter (r-EnKF), for which we find an analytical solution that solves the optimization problem. Third, we propose a reservoir computer, which infers both the model bias and measurement bias to close the assimilation equations. Fourth, we propose a real-time digital twin of the azimuthal thermoacoustic dynamics of a laboratory hydrogen-based annular combustor for a variety of equivalence ratios. We find that the real-time digital twin (i) autonomously predicts azimuthal dynamics, in contrast to bias-unregularized methods; (ii) uncovers the physical acoustic pressure from the raw data, i.e., it acts as a physics-based filter; (iii) is a time-varying parameter system, which generalizes existing models that have constant parameters, and capture only slow-varying variables. The digital twin generalizes to all equivalence ratios, which bridges the gap of existing models. This work opens new opportunities for real-time digital twinning of multi-physics problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18793v2</guid>
      <category>physics.flu-dyn</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea N\'ovoa, Nicolas Noiray, James R. Dawson, Luca Magri</dc:creator>
    </item>
    <item>
      <title>Generalization Issues in Experiments Involving Multidimensional Decisions</title>
      <link>https://arxiv.org/abs/2405.06779</link>
      <description>arXiv:2405.06779v2 Announce Type: replace-cross 
Abstract: Can the causal effects estimated in an experiment be generalized to real-world scenarios? This question lies at the heart of social science studies. External validity primarily assesses whether experimental effects persist across different settings, implicitly presuming the consistency of experimental effects with their real-life counterparts. However, we argue that this presumed consistency may not always hold, especially in experiments involving multi-dimensional decision processes, such as conjoint experiments. We introduce a formal model to elucidate how attention and salience effects lead to three types of inconsistencies between experimental findings and real-world phenomena: amplified effect magnitude, effect sign reversal, and effect importance reversal. We derive testable hypotheses from each theoretical outcome and test these hypotheses using data from various existing conjoint experiments and our own experiments. Drawing on our theoretical framework, we propose several recommendations for experimental design aimed at enhancing the generalizability of survey experiment findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06779v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Fu, Xiaojun Li</dc:creator>
    </item>
    <item>
      <title>Functional Clustering for Longitudinal Associations between Social Determinants of Health and Stroke Mortality in the US</title>
      <link>https://arxiv.org/abs/2406.10499</link>
      <description>arXiv:2406.10499v3 Announce Type: replace-cross 
Abstract: Understanding the longitudinally changing associations between Social Determinants of Health (SDOH) and stroke mortality is essential for effective stroke management. Previous studies have uncovered significant regional disparities in the relationships between SDOH and stroke mortality. However, existing studies have not utilized longitudinal associations to develop data-driven methods for regional division in stroke control. To fill this gap, we propose a novel clustering method to analyze SDOH -- stroke mortality associations in US counties. To enhance the interpretability of the clustering outcomes, we introduce a novel regularized expectation-maximization algorithm equipped with various sparsity-and-smoothness-pursued penalties, aiming at simultaneous clustering and variable selection in longitudinal associations. As a result, we can identify crucial SDOH that contribute to longitudinal changes in stroke mortality. This facilitates the clustering of US counties into different regions based on the relationships between these SDOH and stroke mortality. The effectiveness of our proposed method is demonstrated through extensive numerical studies. By applying our method to longitudinal data on SDOH and stroke mortality at the county level, we identify 18 important SDOH for stroke mortality and divide the US counties into two clusters based on these selected SDOH. Our findings unveil complex regional heterogeneity in the longitudinal associations between SDOH and stroke mortality, providing valuable insights into region-specific SDOH adjustments for mitigating stroke mortality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10499v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fangzhi Luo, Jianbin Tan, Donglan Zhang, Hui Huang, Ye Shen</dc:creator>
    </item>
    <item>
      <title>CardioLab: Laboratory Values Estimation from Electrocardiogram Features -- An Exploratory Study</title>
      <link>https://arxiv.org/abs/2407.18629</link>
      <description>arXiv:2407.18629v2 Announce Type: replace-cross 
Abstract: Introduction: Laboratory value represents a cornerstone of medical diagnostics, but suffers from slow turnaround times, and high costs and only provides information about a single point in time. The continuous estimation of laboratory values from non-invasive data such as electrocardiogram (ECG) would therefore mark a significant frontier in healthcare monitoring. Despite its transformative potential, this domain remains relatively underexplored within the medical community.
  Methods: In this preliminary study, we used a publicly available dataset (MIMIC-IV-ECG) to investigate the feasibility of inferring laboratory values from ECG features and patient demographics using tree-based models (XGBoost). We define the prediction task as a binary prediction problem of predicting whether the lab value falls into low or high abnormalities. The model performance can then be assessed using AUROC.
  Results: Our findings demonstrate promising results in the estimation of laboratory values related to different organ systems based on a small yet comprehensive set of features. While further research and validation are warranted to fully assess the clinical utility and generalizability of ECG-based estimation in healthcare monitoring, our findings lay the groundwork for future investigations into approaches to laboratory value estimation using ECG data. Such advancements hold promise for revolutionizing predictive healthcare applications, offering faster, non-invasive, and more affordable means of patient monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18629v2</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Miguel Lopez Alcaraz, Nils Strodthoff</dc:creator>
    </item>
  </channel>
</rss>
