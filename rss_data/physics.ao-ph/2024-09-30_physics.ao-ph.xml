<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.ao-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.ao-ph</link>
    <description>physics.ao-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.ao-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 30 Sep 2024 04:08:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>How do the Pacific and Atlantic Oceans synergize to modulate Southeastern United States Precipitation Variability?</title>
      <link>https://arxiv.org/abs/2409.18320</link>
      <description>arXiv:2409.18320v1 Announce Type: new 
Abstract: This study explores the mechanisms behind anomalous positive and negative rainfall events in the southeastern United States (SEUS), emphasizing the interplay between upper-level large-scale atmospheric teleconnections and the lower-level North Atlantic Subtropical High (NASH). Through a novel conditional weather regime analysis of geopotential height at both lower and upper levels across the Pacific-North America-Atlantic region, we identify distinct clusters representing persistent and recurring circulation patterns originating from the Pacific and Atlantic Oceans. Our analysis of lower-level conditional weather regimes reveals two distinct phases of the NASH that influence rainfall patterns in the SEUS region. In one phase, the weakening and eastward shift of the NASH's northern boundary reduces the central low-level jet, enhances cyclonic circulation, and increases rainfall in the SEUS. In the other phase, the excessive latent heating associated with enhanced SEUS rainfall triggers a wave train pattern that strengthens the intensity of NASH. Conversely, the opposite conditions apply during anomalous negative rainfall events. Additionally, the upper-level conditional weather regime indicates that large-scale dynamics of East Asian summer monsoons trigger the Rossby wave patterns, contributing considerably to the variability in SEUS rainfall from the upper levels. Therefore, our research highlights the crucial role of global atmospheric teleconnections at upper and lower levels in shaping SEUS precipitation patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18320v1</guid>
      <category>physics.ao-ph</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Priyanshi Singhai, Kathy Pegion, Akintomide A. Akinsanola, Bohar Singh, Thierry N. Taguela</dc:creator>
    </item>
    <item>
      <title>Robustness of AI-based weather forecasts in a changing climate</title>
      <link>https://arxiv.org/abs/2409.18529</link>
      <description>arXiv:2409.18529v1 Announce Type: new 
Abstract: Data-driven machine learning models for weather forecasting have made transformational progress in the last 1-2 years, with state-of-the-art ones now outperforming the best physics-based models for a wide range of skill scores. Given the strong links between weather and climate modelling, this raises the question whether machine learning models could also revolutionize climate science, for example by informing mitigation and adaptation to climate change or to generate larger ensembles for more robust uncertainty estimates. Here, we show that current state-of-the-art machine learning models trained for weather forecasting in present-day climate produce skillful forecasts across different climate states corresponding to pre-industrial, present-day, and future 2.9K warmer climates. This indicates that the dynamics shaping the weather on short timescales may not differ fundamentally in a changing climate. It also demonstrates out-of-distribution generalization capabilities of the machine learning models that are a critical prerequisite for climate applications. Nonetheless, two of the models show a global-mean cold bias in the forecasts for the future warmer climate state, i.e. they drift towards the colder present-day climate they have been trained for. A similar result is obtained for the pre-industrial case where two out of three models show a warming. We discuss possible remedies for these biases and analyze their spatial distribution, revealing complex warming and cooling patterns that are partly related to missing ocean-sea ice and land surface information in the training data. Despite these current limitations, our results suggest that data-driven machine learning models will provide powerful tools for climate science and transform established approaches by complementing conventional physics-based models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18529v1</guid>
      <category>physics.ao-ph</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Thomas Rackow, Nikolay Koldunov, Christian Lessig, Irina Sandu, Mihai Alexe, Matthew Chantry, Mariana Clare, Jesper Dramsch, Florian Pappenberger, Xabier Pedruzo-Bagazgoitia, Steffen Tietsche, Thomas Jung</dc:creator>
    </item>
    <item>
      <title>LUCIE: A Lightweight Uncoupled ClImate Emulator with long-term stability and physical consistency for O(1000)-member ensembles</title>
      <link>https://arxiv.org/abs/2405.16297</link>
      <description>arXiv:2405.16297v2 Announce Type: replace-cross 
Abstract: We present a lightweight, easy-to-train, low-resolution, fully data-driven climate emulator, LUCIE, that can be trained on as low as $2$ years of $6$-hourly ERA5 data. Unlike most state-of-the-art AI weather models, LUCIE remains stable and physically consistent for $100$ years of autoregressive simulation with $100$ ensemble members. Long-term mean climatology from LUCIE's simulation of temperature, wind, precipitation, and humidity matches that of ERA5 data, along with the variability. We further demonstrate how well extreme weather events and their return periods can be estimated from a large ensemble of long-term simulations. We further discuss an improved training strategy with a hard-constrained first-order integrator to suppress autoregressive error growth, a novel spectral regularization strategy to better capture fine-scale dynamics, and finally an optimization algorithm that enables data-limited (as low as $2$ years of $6$-hourly data) training of the emulator without losing stability and physical consistency. Finally, we provide a scaling experiment to compare the long-term bias of LUCIE with respect to the number of training samples. Importantly, LUCIE is an easy to use model that can be trained in just $2.4$h on a single A-100 GPU, allowing for multiple experiments that can explore important scientific questions that could be answered with large ensembles of long-term simulations, e.g., the impact of different variables on the simulation, dynamic response to external forcing, and estimation of extreme weather events, amongst others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16297v2</guid>
      <category>cs.LG</category>
      <category>physics.ao-ph</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haiwen Guan, Troy Arcomano, Ashesh Chattopadhyay, Romit Maulik</dc:creator>
    </item>
    <item>
      <title>Towards Physically Consistent Deep Learning For Climate Model Parameterizations</title>
      <link>https://arxiv.org/abs/2406.03920</link>
      <description>arXiv:2406.03920v3 Announce Type: replace-cross 
Abstract: Climate models play a critical role in understanding and projecting climate change. Due to their complexity, their horizontal resolution of about 40-100 km remains too coarse to resolve processes such as clouds and convection, which need to be approximated via parameterizations. These parameterizations are a major source of systematic errors and large uncertainties in climate projections. Deep learning (DL)-based parameterizations, trained on data from computationally expensive short, high-resolution simulations, have shown great promise for improving climate models in that regard. However, their lack of interpretability and tendency to learn spurious non-physical correlations result in reduced trust in the climate simulation. We propose an efficient supervised learning framework for DL-based parameterizations that leads to physically consistent models with improved interpretability and negligible computational overhead compared to standard supervised training. First, key features determining the target physical processes are uncovered. Subsequently, the neural network is fine-tuned using only those relevant features. We show empirically that our method robustly identifies a small subset of the inputs as actual physical drivers, therefore removing spurious non-physical relationships. This results in by design physically consistent and interpretable neural networks while maintaining the predictive performance of unconstrained black-box DL-based parameterizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03920v3</guid>
      <category>cs.LG</category>
      <category>physics.ao-ph</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Birgit K\"uhbacher, Fernando Iglesias-Suarez, Niki Kilbertus, Veronika Eyring</dc:creator>
    </item>
    <item>
      <title>CoDiCast: Conditional Diffusion Model for Weather Prediction with Uncertainty Quantification</title>
      <link>https://arxiv.org/abs/2409.05975</link>
      <description>arXiv:2409.05975v2 Announce Type: replace-cross 
Abstract: Accurate weather forecasting is critical for science and society. Yet, existing methods have not managed to simultaneously have the properties of high accuracy, low uncertainty, and high computational efficiency. On one hand, to quantify the uncertainty in weather predictions, the strategy of ensemble forecast (i.e., generating a set of diverse predictions) is often employed. However, traditional ensemble numerical weather prediction (NWP) is computationally intensive. On the other hand, most existing machine learning-based weather prediction (MLWP) approaches are efficient and accurate. Nevertheless, they are deterministic and cannot capture the uncertainty of weather forecasting. In this work, we propose CoDiCast, a conditional diffusion model to generate accurate global weather prediction, while achieving uncertainty quantification with ensemble forecasts and modest computational cost. The key idea is to simulate a conditional version of the reverse denoising process in diffusion models, which starts from pure Gaussian noise to generate realistic weather scenarios for a future time point. Each denoising step is conditioned on observations from the recent past. Ensemble forecasts are achieved by repeatedly sampling from stochastic Gaussian noise to represent uncertainty quantification. CoDiCast is trained on a decade of ERA5 reanalysis data from the European Centre for Medium-Range Weather Forecasts (ECMWF). Experimental results demonstrate that our approach outperforms several existing data-driven methods in accuracy. Our conditional diffusion model, CoDiCast, can generate 3-day global weather forecasts, at 6-hour steps and $5.625^\circ$ latitude-longitude resolution, for over 5 variables, in about 12 minutes on a commodity A100 GPU machine with 80GB memory. The open-souced code is provided at \url{https://github.com/JimengShi/CoDiCast}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05975v2</guid>
      <category>cs.LG</category>
      <category>physics.ao-ph</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jimeng Shi, Bowen Jin, Jiawei Han, Giri Narasimhan</dc:creator>
    </item>
  </channel>
</rss>
