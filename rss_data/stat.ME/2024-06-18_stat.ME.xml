<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Jun 2024 02:00:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Quick and Simple Kernel Differential Equation Regression Estimators for Data with Sparse Design</title>
      <link>https://arxiv.org/abs/2406.10308</link>
      <description>arXiv:2406.10308v1 Announce Type: new 
Abstract: Local polynomial regression of order at least one often performs poorly in regions of sparse data. Local constant regression is exceptional in this regard, though it is the least accurate method in general, especially at the boundaries of the data. Incorporating information from differential equations which may approximately or exactly hold is one way of extending the sparse design capacity of local constant regression while reducing bias and variance. A nonparametric regression method that exploits first order differential equations is introduced in this paper and applied to noisy mouse tumour growth data. Asymptotic biases and variances of kernel estimators using Taylor polynomials with different degrees are discussed. Model comparison is performed for different estimators through simulation studies under various scenarios which simulate exponential-type growth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10308v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunlei Ge, W. John Braun</dc:creator>
    </item>
    <item>
      <title>Causal inference for N-of-1 trials</title>
      <link>https://arxiv.org/abs/2406.10360</link>
      <description>arXiv:2406.10360v1 Announce Type: new 
Abstract: The aim of personalized medicine is to tailor treatment decisions to individuals' characteristics. N-of-1 trials are within-person crossover trials that hold the promise of targeting individual-specific effects. While the idea behind N-of-1 trials might seem simple, analyzing and interpreting N-of-1 trials is not straightforward. In particular, there exists confusion about the role of randomization in this design, the (implicit) target estimand, and the need for covariate adjustment. Here we ground N-of-1 trials in a formal causal inference framework and formalize intuitive claims from the N-of-1 trial literature. We focus on causal inference from a single N-of-1 trial and define a conditional average treatment effect (CATE) that represents a target in this setting, which we call the U-CATE. We discuss the assumptions sufficient for identification and estimation of the U-CATE under different causal models in which the treatment schedule is assigned at baseline. A simple mean difference is shown to be an unbiased, asymptotically normal estimator of the U-CATE in simple settings, such as when participants have stable conditions (e.g., chronic pain) and interventions have effects limited in time (no carryover). We also consider settings where carryover effects, trends over time, time-varying common causes of the outcome, and outcome-outcome effects are present. In these more complex settings, we show that a time-varying g-formula identifies the U-CATE under explicit assumptions. Finally, we analyze data from N-of-1 trials about acne symptoms. Using this example, we show how different assumptions about the underlying data generating process can lead to different analytical strategies in N-of-1 trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10360v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Piccininni, Mats J. Stensrud, Zachary Shahn, Stefan Konigorski</dc:creator>
    </item>
    <item>
      <title>Design-based variance estimation of the H\'ajek effect estimator in stratified and clustered experiments</title>
      <link>https://arxiv.org/abs/2406.10473</link>
      <description>arXiv:2406.10473v1 Announce Type: new 
Abstract: Randomized controlled trials (RCTs) are used to evaluate treatment effects. When individuals are grouped together, clustered RCTs are conducted. Stratification is recommended to reduce imbalance of baseline covariates between treatment and control. In practice, this can lead to comparisons between clusters of very different sizes. As a result, direct adjustment estimators that average differences of means within the strata may be inconsistent. We study differences of inverse probability weighted means of a treatment and a control group -- H\'ajek effect estimators -- under two common forms of stratification: small strata that increase in number; or larger strata with growing numbers of clusters in each. Under either scenario, mild conditions give consistency and asymptotic Normality. We propose a variance estimator applicable to designs with any number of strata and strata of any size. We describe a special use of the variance estimator that improves small sample performance of Wald-type confidence intervals. The H\'ajek effect estimator lends itself to covariance adjustment, and our variance estimator remains applicable. Simulations and real-world applications in children's nutrition and education confirm favorable operating characteristics, demonstrating advantages of the H\'ajek effect estimator beyond its simplicity and ease of use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10473v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinhe Wang, Ben B. Hansen</dc:creator>
    </item>
    <item>
      <title>Functional Clustering for Longitudinal Associations between County-Level Social Determinants of Health and Stroke Mortality in the US</title>
      <link>https://arxiv.org/abs/2406.10499</link>
      <description>arXiv:2406.10499v1 Announce Type: new 
Abstract: Understanding longitudinally changing associations between Social determinants of health (SDOH) and stroke mortality is crucial for timely stroke management. Previous studies have revealed a significant regional disparity in the SDOH -- stroke mortality associations. However, they do not develop data-driven methods based on these longitudinal associations for regional division in stroke control. To fill this gap, we propose a novel clustering method for SDOH -- stroke mortality associations in the US counties. To enhance interpretability and statistical efficiency of the clustering outcomes, we introduce a new class of smoothness-sparsity pursued penalties for simultaneous clustering and variable selection in the longitudinal associations. As a result, we can identify important SDOH that contribute to longitudinal changes in the stroke mortality, facilitating clustering of US counties into several regions based on how these SDOH relate to stroke mortality. The effectiveness of our proposed method is demonstrated through extensive numerical studies. By applying our method to a county-level SDOH and stroke mortality longitudinal data, we identify 18 important SDOH for stroke mortality and divide the US counties into two clusters based on these selected SDOH. Our findings unveil complex regional heterogeneity in the longitudinal associations between SDOH and stroke mortality, providing valuable insights in region-specific SDOH adjustments for mitigating stroke mortality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10499v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fangzhi Luo, Jianbin Tan, Donglan Zhang, Hui Huang, Ye Shen</dc:creator>
    </item>
    <item>
      <title>Causal Inference with Outcomes Truncated by Death and Missing Not at Random</title>
      <link>https://arxiv.org/abs/2406.10554</link>
      <description>arXiv:2406.10554v1 Announce Type: new 
Abstract: In clinical trials, principal stratification analysis is commonly employed to address the issue of truncation by death, where a subject dies before the outcome can be measured. However, in practice, many survivor outcomes may remain uncollected or be missing not at random, posing a challenge to standard principal stratification analyses. In this paper, we explore the identification, estimation, and bounds of the average treatment effect within a subpopulation of individuals who would potentially survive under both treatment and control conditions. We show that the causal parameter of interest can be identified by introducing a proxy variable that affects the outcome only through the principal strata, while requiring that the treatment variable does not directly affect the missingness mechanism. Subsequently, we propose an approach for estimating causal parameters and derive nonparametric bounds in cases where identification assumptions are violated. We illustrate the performance of the proposed method through simulation studies and a real dataset obtained from a Human Immunodeficiency Virus (HIV) study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10554v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Li, Yuan Liu, Shanshan Luo, Zhi Geng</dc:creator>
    </item>
    <item>
      <title>Producing treatment hierarchies in network meta-analysis using probabilistic models and treatment-choice criteria</title>
      <link>https://arxiv.org/abs/2406.10612</link>
      <description>arXiv:2406.10612v1 Announce Type: new 
Abstract: A key output of network meta-analysis (NMA) is the relative ranking of the treatments; nevertheless, it has attracted a lot of criticism. This is mainly due to the fact that ranking is an influential output and prone to over-interpretations even when relative effects imply small differences between treatments. To date, common ranking methods rely on metrics that lack a straightforward interpretation, while it is still unclear how to measure their uncertainty. We introduce a novel framework for estimating treatment hierarchies in NMA. At first, we formulate a mathematical expression that defines a treatment choice criterion (TCC) based on clinically important values. This TCC is applied to the study treatment effects to generate paired data indicating treatment preferences or ties. Then, we synthesize the paired data across studies using an extension of the so-called "Bradley-Terry" model. We assign to each treatment a latent variable interpreted as the treatment "ability" and we estimate the ability parameters within a regression model. Higher ability estimates correspond to higher positions in the final ranking. We further extend our model to adjust for covariates that may affect treatment selection. We illustrate the proposed approach and compare it with alternatives in two datasets: a network comparing 18 antidepressants for major depression and a network comparing 6 antihypertensives for the incidence of diabetes. Our approach provides a robust and interpretable treatment hierarchy which accounts for clinically important values and is presented alongside with uncertainty measures. Overall, the proposed framework offers a novel approach for ranking in NMA based on concrete criteria and preserves from over-interpretation of unimportant differences between treatments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10612v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Theodoros Evrenoglou, Adriani Nikolakopoulou, Guido Schwarzer, Gerta R\"ucker, Anna Chaimani</dc:creator>
    </item>
    <item>
      <title>A Laplace transform-based test for the equality of positive semidefinite matrix distributions</title>
      <link>https://arxiv.org/abs/2406.10733</link>
      <description>arXiv:2406.10733v1 Announce Type: new 
Abstract: In this paper, we present a novel test for determining equality in distribution of matrix distributions. Our approach is based on the integral squared difference of the empirical Laplace transforms with respect to the noncentral Wishart measure. We conduct an extensive power study to assess the performance of the test and determine the optimal choice of parameters. Furthermore, we demonstrate the applicability of the test on financial and non-life insurance data, illustrating its effectiveness in practical scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10733v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\v{Z}ikica Luki\'c</dc:creator>
    </item>
    <item>
      <title>Data-Adaptive Identification of Subpopulations Vulnerable to Chemical Exposures using Stochastic Interventions</title>
      <link>https://arxiv.org/abs/2406.10792</link>
      <description>arXiv:2406.10792v1 Announce Type: new 
Abstract: In environmental epidemiology, identifying subpopulations vulnerable to chemical exposures and those who may benefit differently from exposure-reducing policies is essential. For instance, sex-specific vulnerabilities, age, and pregnancy are critical factors for policymakers when setting regulatory guidelines. However, current semi-parametric methods for heterogeneous treatment effects are often limited to binary exposures and function as black boxes, lacking clear, interpretable rules for subpopulation-specific policy interventions. This study introduces a novel method using cross-validated targeted minimum loss-based estimation (TMLE) paired with a data-adaptive target parameter strategy to identify subpopulations with the most significant differential impact from simulated policy interventions that reduce exposure. Our approach is assumption-lean, allowing for the integration of machine learning while still yielding valid confidence intervals. We demonstrate the robustness of our methodology through simulations and application to NHANES data. Our analysis of NHANES data for persistent organic pollutants on leukocyte telomere length (LTL) identified age as the maximum effect modifier. Specifically, we found that exposure to 3,3',4,4',5-pentachlorobiphenyl (pcnb) consistently had a differential impact on LTL, with a one standard deviation reduction in exposure leading to a more pronounced increase in LTL among younger populations compared to older ones. We offer our method as an open-source software package, \texttt{EffectXshift}, enabling researchers to investigate the effect modification of continuous exposures. The \texttt{EffectXshift} package provides clear and interpretable results, informing targeted public health interventions and policy decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10792v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David McCoy, Wenxin Zhang, Alan Hubbard, Mark van der Laan, Alejandro Schuler</dc:creator>
    </item>
    <item>
      <title>SynthTree: Co-supervised Local Model Synthesis for Explainable Prediction</title>
      <link>https://arxiv.org/abs/2406.10962</link>
      <description>arXiv:2406.10962v1 Announce Type: new 
Abstract: Explainable machine learning (XML) has emerged as a major challenge in artificial intelligence (AI). Although black-box models such as Deep Neural Networks and Gradient Boosting often exhibit exceptional predictive accuracy, their lack of interpretability is a notable drawback, particularly in domains requiring transparency and trust. This paper tackles this core AI problem by proposing a novel method to enhance explainability with minimal accuracy loss, using a Mixture of Linear Models (MLM) estimated under the co-supervision of black-box models. We have developed novel methods for estimating MLM by leveraging AI techniques. Specifically, we explore two approaches for partitioning the input space: agglomerative clustering and decision trees. The agglomerative clustering approach provides greater flexibility in model construction, while the decision tree approach further enhances explainability, yielding a decision tree model with linear or logistic regression models at its leaf nodes. Comparative analyses with widely-used and state-of-the-art predictive models demonstrate the effectiveness of our proposed methods. Experimental results show that statistical models can significantly enhance the explainability of AI, thereby broadening their potential for real-world applications. Our findings highlight the critical role that statistical methodologies can play in advancing explainable AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10962v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Evgenii Kuriabov, Jia Li</dc:creator>
    </item>
    <item>
      <title>HEDE: Heritability estimation in high dimensions by Ensembling Debiased Estimators</title>
      <link>https://arxiv.org/abs/2406.11184</link>
      <description>arXiv:2406.11184v1 Announce Type: new 
Abstract: Estimating heritability remains a significant challenge in statistical genetics. Diverse approaches have emerged over the years that are broadly categorized as either random effects or fixed effects heritability methods. In this work, we focus on the latter. We propose HEDE, an ensemble approach to estimate heritability or the signal-to-noise ratio in high-dimensional linear models where the sample size and the dimension grow proportionally. Our method ensembles post-processed versions of the debiased lasso and debiased ridge estimators, and incorporates a data-driven strategy for hyperparameter selection that significantly boosts estimation performance. We establish rigorous consistency guarantees that hold despite adaptive tuning. Extensive simulations demonstrate our method's superiority over existing state-of-the-art methods across various signal structures and genetic architectures, ranging from sparse to relatively dense and from evenly to unevenly distributed signals. Furthermore, we discuss the advantages of fixed effects heritability estimation compared to random effects estimation. Our theoretical guarantees hold for realistic genotype distributions observed in genetic studies, where genotypes typically take on discrete values and are often well-modeled by sub-Gaussian distributed random variables. We establish our theoretical results by deriving uniform bounds, built upon the convex Gaussian min-max theorem, and leveraging universality results. Finally, we showcase the efficacy of our approach in estimating height and BMI heritability using the UK Biobank.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11184v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanke Song, Xihong Lin, Pragya Sur</dc:creator>
    </item>
    <item>
      <title>Bayesian Hierarchical Modelling of Noisy Gamma Processes: Model Formulation, Identifiability, Model Fitting, and Extensions to Unit-to-Unit Variability</title>
      <link>https://arxiv.org/abs/2406.11216</link>
      <description>arXiv:2406.11216v1 Announce Type: new 
Abstract: The gamma process is a natural model for monotonic degradation processes. In practice, it is desirable to extend the single gamma process to incorporate measurement error and to construct models for the degradation of several nominally identical units. In this paper, we show how these extensions are easily facilitated through the Bayesian hierarchical modelling framework. Following the precepts of the Bayesian statistical workflow, we show the principled construction of a noisy gamma process model. We also reparameterise the gamma process to simplify the specification of priors and make it obvious how the single gamma process model can be extended to include unit-to-unit variability or covariates. We first fit the noisy gamma process model to a single simulated degradation trace. In doing so, we find an identifiability problem between the volatility of the gamma process and the measurement error when there are only a few noisy degradation observations. However, this lack of identifiability can be resolved by including extra information in the analysis through a stronger prior or extra data that informs one of the non-identifiable parameters, or by borrowing information from multiple units. We then explore extensions of the model to account for unit-to-unit variability and demonstrate them using a crack-propagation data set with added measurement error. Lastly, we perform model selection in a fully Bayesian framework by using cross-validation to approximate the expected log probability density of new observation. We also show how failure time distributions with uncertainty intervals can be calculated for new units or units that are currently under test but are yet to fail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11216v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Leadbetter, Gabriel Gonzalez Caceres, Aloke Phatak</dc:creator>
    </item>
    <item>
      <title>Bayesian Variable Selection via Hierarchical Gaussian Process Model in Computer Experiments</title>
      <link>https://arxiv.org/abs/2406.11306</link>
      <description>arXiv:2406.11306v1 Announce Type: new 
Abstract: Identifying the active factors that have significant impacts on the output of the complex system is an important but challenging variable selection problem in computer experiments. In this paper, a Bayesian hierarchical Gaussian process model is developed and some latent indicator variables are embedded into this setting for the sake of labelling the important variables. The parameter estimation and variable selection can be processed simultaneously in a full Bayesian framework through an efficient Markov Chain Monte Carlo (MCMC) method -- Metropolis-within-Gibbs sampler. The much better performances of the proposed method compared with the related competitors are evaluated by the analysis of simulated examples and a practical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11306v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiao Yao, Ning Jianhui, Qin Hong</dc:creator>
    </item>
    <item>
      <title>Spillover Detection for Donor Selection in Synthetic Control Models</title>
      <link>https://arxiv.org/abs/2406.11399</link>
      <description>arXiv:2406.11399v1 Announce Type: new 
Abstract: Synthetic control (SC) models are widely used to estimate causal effects in settings with observational time-series data. To identify the causal effect on a target unit, SC requires the existence of correlated units that are not impacted by the intervention. Given one of these potential donor units, how can we decide whether it is in fact a valid donor - that is, one not subject to spillover effects from the intervention? Such a decision typically requires appealing to strong a priori domain knowledge specifying the units, which becomes infeasible in situations with large pools of potential donors. In this paper, we introduce a practical, theoretically-grounded donor selection procedure, aiming to weaken this domain knowledge requirement. Our main result is a Theorem that yields the assumptions required to identify donor values at post-intervention time points using only pre-intervention data. We show how this Theorem - and the assumptions underpinning it - can be turned into a practical method for detecting potential spillover effects and excluding invalid donors when constructing SCs. Importantly, we employ sensitivity analysis to formally bound the bias in our SC causal estimate in situations where an excluded donor was indeed valid, or where a selected donor was invalid. Using ideas from the proximal causal inference and instrumental variables literature, we show that the excluded donors can nevertheless be leveraged to further debias causal effect estimates. Finally, we illustrate our donor selection procedure on both simulated and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11399v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael O'Riordan, Ciar\'an M. Gilligan-Lee</dc:creator>
    </item>
    <item>
      <title>Bayesian Outcome Weighted Learning</title>
      <link>https://arxiv.org/abs/2406.11573</link>
      <description>arXiv:2406.11573v1 Announce Type: new 
Abstract: One of the primary goals of statistical precision medicine is to learn optimal individualized treatment rules (ITRs). The classification-based, or machine learning-based, approach to estimating optimal ITRs was first introduced in outcome-weighted learning (OWL). OWL recasts the optimal ITR learning problem into a weighted classification problem, which can be solved using machine learning methods, e.g., support vector machines. In this paper, we introduce a Bayesian formulation of OWL. Starting from the OWL objective function, we generate a pseudo-likelihood which can be expressed as a scale mixture of normal distributions. A Gibbs sampling algorithm is developed to sample the posterior distribution of the parameters. In addition to providing a strategy for learning an optimal ITR, Bayesian OWL provides a natural, probabilistic approach to estimate uncertainty in ITR treatment recommendations themselves. We demonstrate the performance of our method through several simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11573v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sophia Yazzourh, Nikki L. B. Freeman</dc:creator>
    </item>
    <item>
      <title>The analysis of paired comparison data in the presence of cyclicality and intransitivity</title>
      <link>https://arxiv.org/abs/2406.11584</link>
      <description>arXiv:2406.11584v1 Announce Type: new 
Abstract: A principled approach to cyclicality and intransitivity in cardinal paired comparison data is developed within the framework of graphical linear models. Fundamental to our developments is a detailed understanding and study of the parameter space which accommodates cyclicality and intransitivity. In particular, the relationships between the reduced, completely transitive model, the full, not necessarily transitive model, and all manner of intermediate models are explored for both complete and incomplete paired comparison graphs. It is shown that identifying cyclicality and intransitivity reduces to a model selection problem and a new method for model selection employing geometrical insights, unique to the problem at hand, is proposed. The large sample properties of the estimators as well as guarantees on the selected model are provided. It is thus shown that in large samples all cyclicalities and intransitivities can be identified. The method is exemplified using simulations and the analysis of an illustrative example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11584v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rahul Singh, Ori Davidov</dc:creator>
    </item>
    <item>
      <title>Bayesian regression discontinuity design with unknown cutoff</title>
      <link>https://arxiv.org/abs/2406.11585</link>
      <description>arXiv:2406.11585v1 Announce Type: new 
Abstract: Regression discontinuity design (RDD) is a quasi-experimental approach used to estimate the causal effects of an intervention assigned based on a cutoff criterion. RDD exploits the idea that close to the cutoff units below and above are similar; hence, they can be meaningfully compared. Consequently, the causal effect can be estimated only locally at the cutoff point. This makes the cutoff point an essential element of RDD. However, especially in medical applications, the exact cutoff location may not always be disclosed to the researcher, and even when it is, the actual location may deviate from the official one. As we illustrate on the application of RDD to the HIV treatment eligibility data, estimating the causal effect at an incorrect cutoff point leads to meaningless results. Moreover, since the cutoff criterion often acts as a guideline rather than as a strict rule, the location of the cutoff may be unclear from the data. The method we present can be applied both as an estimation and validation tool in RDD. We use a Bayesian approach to incorporate prior knowledge and uncertainty about the cutoff location in the causal effect estimation. At the same time, our Bayesian model LoTTA is fitted globally to the whole data, whereas RDD is a local, boundary point estimation problem. In this work we address a natural question that arises: how to make Bayesian inference more local to render a meaningful and powerful estimate of the treatment effect?</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11585v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Kowalska, Mark van de Wiel, St\'ephanie van der Pas</dc:creator>
    </item>
    <item>
      <title>A conservation law for posterior predictive variance</title>
      <link>https://arxiv.org/abs/2406.11806</link>
      <description>arXiv:2406.11806v1 Announce Type: new 
Abstract: We use the law of total variance to generate multiple expressions for the posterior predictive variance in Bayesian hierarchical models. These expressions are sums of terms involving conditional expectations and conditional variances. Since the posterior predictive variance is fixed given the hierarchical model, it represents a constant quantity that is conserved over the various expressions for it. The terms in the expressions can be assessed in absolute or relative terms to understand the main contributors to the length of prediction intervals. Also, sometimes these terms can be intepreted in the context of the hierarchical model. We show several examples, closed form and computational, to illustrate the uses of this approach in model assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11806v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Bertrand Clarke, Dean Dustin</dc:creator>
    </item>
    <item>
      <title>Improving the Validity and Practical Usefulness of AI/ML Evaluations Using an Estimands Framework</title>
      <link>https://arxiv.org/abs/2406.10366</link>
      <description>arXiv:2406.10366v1 Announce Type: cross 
Abstract: Commonly, AI or machine learning (ML) models are evaluated on benchmark datasets. This practice supports innovative methodological research, but benchmark performance can be poorly correlated with performance in real-world applications -- a construct validity issue. To improve the validity and practical usefulness of evaluations, we propose using an estimands framework adapted from international clinical trials guidelines. This framework provides a systematic structure for inference and reporting in evaluations, emphasizing the importance of a well-defined estimation target. We illustrate our proposal on examples of commonly used evaluation methodologies - involving cross-validation, clustering evaluation, and LLM benchmarking - that can lead to incorrect rankings of competing models (rank reversals) with high probability, even when performance differences are large. We demonstrate how the estimands framework can help uncover underlying issues, their causes, and potential solutions. Ultimately, we believe this framework can improve the validity of evaluations through better-aligned inference, and help decision-makers and model users interpret reported results more effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10366v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Olivier Binette, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>The data augmentation algorithm</title>
      <link>https://arxiv.org/abs/2406.10464</link>
      <description>arXiv:2406.10464v1 Announce Type: cross 
Abstract: The data augmentation (DA) algorithms are popular Markov chain Monte Carlo (MCMC) algorithms often used for sampling from intractable probability distributions. This review article comprehensively surveys DA MCMC algorithms, highlighting their theoretical foundations, methodological implementations, and diverse applications in frequentist and Bayesian statistics. The article discusses tools for studying the convergence properties of DA algorithms. Furthermore, it contains various strategies for accelerating the speed of convergence of the DA algorithms, different extensions of DA algorithms and outlines promising directions for future research. This paper aims to serve as a resource for researchers and practitioners seeking to leverage data augmentation techniques in MCMC algorithms by providing key insights and synthesizing recent developments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10464v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vivekananda Roy, Kshitij Khare, James P. Hobert</dc:creator>
    </item>
    <item>
      <title>DCDILP: a distributed learning method for large-scale causal structure learning</title>
      <link>https://arxiv.org/abs/2406.10481</link>
      <description>arXiv:2406.10481v1 Announce Type: cross 
Abstract: This paper presents a novel approach to causal discovery through a divide-and-conquer framework. By decomposing the problem into smaller subproblems defined on Markov blankets, the proposed DCDILP method first explores in parallel the local causal graphs of these subproblems. However, this local discovery phase encounters systematic challenges due to the presence of hidden confounders (variables within each Markov blanket may be influenced by external variables). Moreover, aggregating these local causal graphs in a consistent global graph defines a large size combinatorial optimization problem. DCDILP addresses these challenges by: i) restricting the local subgraphs to causal links only related with the central variable of the Markov blanket; ii) formulating the reconciliation of local causal graphs as an integer linear programming method. The merits of the approach, in both terms of causal discovery accuracy and scalability in the size of the problem, are showcased by experiments and comparisons with the state of the art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10481v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyu Dong, Mich\`ele Sebag, Kento Uemura, Akito Fujii, Shuang Chang, Yusuke Koyanagi, Koji Maruhashi</dc:creator>
    </item>
    <item>
      <title>Adaptive Experimentation When You Can't Experiment</title>
      <link>https://arxiv.org/abs/2406.10738</link>
      <description>arXiv:2406.10738v1 Announce Type: cross 
Abstract: This paper introduces the \emph{confounded pure exploration transductive linear bandit} (\texttt{CPET-LB}) problem. As a motivating example, often online services cannot directly assign users to specific control or treatment experiences either for business or practical reasons. In these settings, naively comparing treatment and control groups that may result from self-selection can lead to biased estimates of underlying treatment effects. Instead, online services can employ a properly randomized encouragement that incentivizes users toward a specific treatment. Our methodology provides online services with an adaptive experimental design approach for learning the best-performing treatment for such \textit{encouragement designs}. We consider a more general underlying model captured by a linear structural equation and formulate pure exploration linear bandits in this setting. Though pure exploration has been extensively studied in standard adaptive experimental design settings, we believe this is the first work considering a setting where noise is confounded. Elimination-style algorithms using experimental design methods in combination with a novel finite-time confidence interval on an instrumental variable style estimator are presented with sample complexity upper bounds nearly matching a minimax lower bound. Finally, experiments are conducted that demonstrate the efficacy of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10738v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yao Zhao, Kwang-Sung Jun, Tanner Fiez, Lalit Jain</dc:creator>
    </item>
    <item>
      <title>Statistical Considerations for Evaluating Treatment Effect under Various Non-proportional Hazard Scenarios</title>
      <link>https://arxiv.org/abs/2406.11043</link>
      <description>arXiv:2406.11043v1 Announce Type: cross 
Abstract: We conducted a systematic comparison of statistical methods used for the analysis of time-to-event outcomes under various proportional and nonproportional hazard (NPH) scenarios. Our study used data from recently published oncology trials to compare the Log-rank test, still by far the most widely used option, against some available alternatives, including the MaxCombo test, the Restricted Mean Survival Time Difference (dRMST) test, the Generalized Gamma Model (GGM) and the Generalized F Model (GFM). Power, type I error rate, and time-dependent bias with respect to the RMST difference, survival probability difference, and median survival time were used to evaluate and compare the performance of these methods. In addition to the real data, we simulated three hypothetical scenarios with crossing hazards chosen so that the early and late effects 'cancel out' and used them to evaluate the ability of the aforementioned methods to detect time-specific and overall treatment effects. We implemented novel metrics for assessing the time-dependent bias in treatment effect estimates to provide a more comprehensive evaluation in NPH scenarios. Recommendations under each NPH scenario are provided by examining the type I error rate, power, and time-dependent bias associated with each statistical approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11043v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Zhang, Erich J. Greene, Ondrej Blaha, Wei Wei</dc:creator>
    </item>
    <item>
      <title>Interventional Imbalanced Multi-Modal Representation Learning via $\beta$-Generalization Front-Door Criterion</title>
      <link>https://arxiv.org/abs/2406.11490</link>
      <description>arXiv:2406.11490v1 Announce Type: cross 
Abstract: Multi-modal methods establish comprehensive superiority over uni-modal methods. However, the imbalanced contributions of different modalities to task-dependent predictions constantly degrade the discriminative performance of canonical multi-modal methods. Based on the contribution to task-dependent predictions, modalities can be identified as predominant and auxiliary modalities. Benchmark methods raise a tractable solution: augmenting the auxiliary modality with a minor contribution during training. However, our empirical explorations challenge the fundamental idea behind such behavior, and we further conclude that benchmark approaches suffer from certain defects: insufficient theoretical interpretability and limited exploration capability of discriminative knowledge. To this end, we revisit multi-modal representation learning from a causal perspective and build the Structural Causal Model. Following the empirical explorations, we determine to capture the true causality between the discriminative knowledge of predominant modality and predictive label while considering the auxiliary modality. Thus, we introduce the $\beta$-generalization front-door criterion. Furthermore, we propose a novel network for sufficiently exploring multi-modal discriminative knowledge. Rigorous theoretical analyses and various empirical evaluations are provided to support the effectiveness of the innate mechanism behind our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11490v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Li, Jiangmeng Li, Fei Song, Qingmeng Zhu, Changwen Zheng, Wenwen Qiang</dc:creator>
    </item>
    <item>
      <title>Teleporter Theory: A General and Simple Approach for Modeling Cross-World Counterfactual Causality</title>
      <link>https://arxiv.org/abs/2406.11501</link>
      <description>arXiv:2406.11501v2 Announce Type: cross 
Abstract: Leveraging the development of structural causal model (SCM), researchers can establish graphical models for exploring the causal mechanisms behind machine learning techniques. As the complexity of machine learning applications rises, single-world interventionism causal analysis encounters theoretical adaptation limitations. Accordingly, cross-world counterfactual approach extends our understanding of causality beyond observed data, enabling hypothetical reasoning about alternative scenarios. However, the joint involvement of cross-world variables, encompassing counterfactual variables and real-world variables, challenges the construction of the graphical model. Twin network is a subtle attempt, establishing a symbiotic relationship, to bridge the gap between graphical modeling and the introduction of counterfactuals albeit with room for improvement in generalization. In this regard, we demonstrate the theoretical breakdowns of twin networks in certain cross-world counterfactual scenarios. To this end, we propose a novel teleporter theory to establish a general and simple graphical representation of counterfactuals, which provides criteria for determining teleporter variables to connect multiple worlds. In theoretical application, we determine that introducing the proposed teleporter theory can directly obtain the conditional independence between counterfactual variables and real-world variables from the cross-world SCM without requiring complex algebraic derivations. Accordingly, we can further identify counterfactual causal effects through cross-world symbolic derivation. We demonstrate the generality of the teleporter theory to the practical application. Adhering to the proposed theory, we build a plug-and-play module, and the effectiveness of which are substantiated by experiments on benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11501v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiangmeng Li, Bin Qin, Qirui Ji, Yi Li, Wenwen Qiang, Jianwen Cao, Fanjiang Xu</dc:creator>
    </item>
    <item>
      <title>Joint Linked Component Analysis for Multiview Data</title>
      <link>https://arxiv.org/abs/2406.11761</link>
      <description>arXiv:2406.11761v1 Announce Type: cross 
Abstract: In this work, we propose the joint linked component analysis (joint\_LCA) for multiview data. Unlike classic methods which extract the shared components in a sequential manner, the objective of joint\_LCA is to identify the view-specific loading matrices and the rank of the common latent subspace simultaneously. We formulate a matrix decomposition model where a joint structure and an individual structure are present in each data view, which enables us to arrive at a clean svd representation for the cross covariance between any pair of data views. An objective function with a novel penalty term is then proposed to achieve simultaneous estimation and rank selection. In addition, a refitting procedure is employed as a remedy to reduce the shrinkage bias caused by the penalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11761v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lin Xiao, Luo Xiao</dc:creator>
    </item>
    <item>
      <title>R-miss-tastic: a unified platform for missing values methods and workflows</title>
      <link>https://arxiv.org/abs/1908.04822</link>
      <description>arXiv:1908.04822v4 Announce Type: replace 
Abstract: Missing values are unavoidable when working with data. Their occurrence is exacerbated as more data from different sources become available. However, most statistical models and visualization methods require complete data, and improper handling of missing data results in information loss or biased analyses. Since the seminal work of Rubin (1976), a burgeoning literature on missing values has arisen, with heterogeneous aims and motivations. This led to the development of various methods, formalizations, and tools. For practitioners, it remains nevertheless challenging to decide which method is most suited for their problem, partially due to a lack of systematic covering of this topic in statistics or data science curricula.
  To help address this challenge, we have launched the "R-miss-tastic" platform, which aims to provide an overview of standard missing values problems, methods, and relevant implementations of methodologies. Beyond gathering and organizing a large majority of the material on missing data (bibliography, courses, tutorials, implementations), "R-miss-tastic" covers the development of standardized analysis workflows. Indeed, we have developed several pipelines in R and Python to allow for hands-on illustration of and recommendations on missing values handling in various statistical tasks such as matrix completion, estimation and prediction, while ensuring reproducibility of the analyses. Finally, the platform is dedicated to users who analyze incomplete data, researchers who want to compare their methods and search for an up-to-date bibliography, and also teachers who are looking for didactic materials (notebooks, video, slides).</description>
      <guid isPermaLink="false">oai:arXiv.org:1908.04822v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Imke Mayer, Aude Sportisse, Julie Josse, Nicholas Tierney, Nathalie Vialaneix</dc:creator>
    </item>
    <item>
      <title>Additive Density-on-Scalar Regression in Bayes Hilbert Spaces with an Application to Gender Economics</title>
      <link>https://arxiv.org/abs/2110.11771</link>
      <description>arXiv:2110.11771v3 Announce Type: replace 
Abstract: Motivated by research on gender identity norms and the distribution of the woman's share in a couple's total labor income, we consider functional additive regression models for probability density functions as responses with scalar covariates. To preserve nonnegativity and integration to one under vector space operations, we formulate the model for densities in a Bayes Hilbert space, which allows to not only consider continuous densities, but also, e.g., discrete or mixed densities. Mixed ones occur in our application, as the woman's income share is a continuous variable having discrete point masses at zero and one for single-earner couples. Estimation is based on a gradient boosting algorithm, allowing for potentially numerous flexible covariate effects and model selection. We develop properties of Bayes Hilbert spaces related to subcompositional coherence, yielding (odds-ratio) interpretation of effect functions and simplified estimation for mixed densities via an orthogonal decomposition. Applying our approach to data from the German Socio-Economic Panel Study (SOEP) shows a more symmetric distribution in East German than in West German couples after reunification and a smaller child penalty comparing couples with and without minor children. These West-East differences become smaller, but are persistent over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.11771v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eva-Maria Maier, Almond St\"ocker, Bernd Fitzenberger, Sonja Greven</dc:creator>
    </item>
    <item>
      <title>Estimating Heterogeneous Causal Effects of High-Dimensional Treatments: Application to Conjoint Analysis</title>
      <link>https://arxiv.org/abs/2201.01357</link>
      <description>arXiv:2201.01357v4 Announce Type: replace 
Abstract: Estimation of heterogeneous treatment effects is an active area of research. Most of the existing methods, however, focus on estimating the conditional average treatment effects of a single, binary treatment given a set of pre-treatment covariates. In this paper, we propose a method to estimate the heterogeneous causal effects of high-dimensional treatments, which poses unique challenges in terms of estimation and interpretation. The proposed approach finds maximally heterogeneous groups and uses a Bayesian mixture of regularized logistic regressions to identify groups of units who exhibit similar patterns of treatment effects. By directly modeling group membership with covariates, the proposed methodology allows one to explore the unit characteristics that are associated with different patterns of treatment effects. Our motivating application is conjoint analysis, which is a popular type of survey experiment in social science and marketing research and is based on a high-dimensional factorial design. We apply the proposed methodology to the conjoint data, where survey respondents are asked to select one of two immigrant profiles with randomly selected attributes. We find that a group of respondents with a relatively high degree of prejudice appears to discriminate against immigrants from non-European countries like Iraq. An open-source software package is available for implementing the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.01357v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Goplerud, Kosuke Imai, Nicole E. Pashley</dc:creator>
    </item>
    <item>
      <title>An extended generalized Pareto regression model for count data</title>
      <link>https://arxiv.org/abs/2210.15253</link>
      <description>arXiv:2210.15253v2 Announce Type: replace 
Abstract: The statistical modeling of discrete extremes has received less attention than their continuous counterparts in the Extreme Value Theory (EVT) literature. One approach to the transition from continuous to discrete extremes is the modeling of threshold exceedances of integer random variables by the discrete version of the generalized Pareto distribution. However, the optimal choice of thresholds defining exceedances remains a problematic issue. Moreover, in a regression framework, the treatment of the majority of non-extreme data below the selected threshold is either ignored or separated from the extremes. To tackle these issues, we expand on the concept of employing a smooth transition between the bulk and the upper tail of the distribution. In the case of zero inflation, we also develop models with an additional parameter. To incorporate possible predictors, we relate the parameters to additive smoothed predictors via an appropriate link, as in the generalized additive model (GAM) framework. A penalized maximum likelihood estimation procedure is implemented. We illustrate our modeling proposal with a real dataset of avalanche activity in the French Alps. With the advantage of bypassing the threshold selection step, our results indicate that the proposed models are more flexible and robust than competing models, such as the negative binomial distribution</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.15253v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Touqeer Ahmad, Carlo Gaetan, Philippe Naveau</dc:creator>
    </item>
    <item>
      <title>Bayesian inference for aggregated Hawkes processes</title>
      <link>https://arxiv.org/abs/2211.16552</link>
      <description>arXiv:2211.16552v3 Announce Type: replace 
Abstract: The Hawkes process, a self-exciting point process, has a wide range of applications in modeling earthquakes, social networks and stock markets. The established estimation process requires that researchers have access to the exact time stamps and spatial information. However, available data are often rounded or aggregated. We develop a Bayesian estimation procedure for the parameters of a Hawkes process based on aggregated data. Our approach is developed for temporal, spatio-temporal, and mutually exciting Hawkes processes where data are available over discrete time periods and regions. We show theoretically that the parameters of the Hawkes process are identifiable from aggregated data under general specifications. We demonstrate the method on simulated temporal and spatio-temporal data with various model specifications in the presence of one or more interacting processes, and under varying coarseness of data aggregation. Finally, we examine the internal and cross-excitation effects of airstrikes and insurgent violence events from February 2007 to June 2008, with some data aggregated by day.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.16552v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lingxiao Zhou, Georgia Papadogeorgou</dc:creator>
    </item>
    <item>
      <title>Sparse and Integrative Principal Component Analysis for Multiview Data</title>
      <link>https://arxiv.org/abs/2301.06718</link>
      <description>arXiv:2301.06718v2 Announce Type: replace 
Abstract: We consider dimension reduction of multiview data, which are emerging in scientific studies. Formulating multiview data as multi-variate data with block structures corresponding to the different views, or views of data, we estimate top eigenvectors from multiview data that have two-fold sparsity, elementwise sparsity and blockwise sparsity. We propose a Fantope-based optimization criterion with multiple penalties to enforce the desired sparsity patterns and a denoising step is employed to handle potential presence of heteroskedastic noise across different data views. An alternating direction method of multipliers (ADMM) algorithm is used for optimization. We derive the l2 convergence of the estimated top eigenvectors and establish their sparsity and support recovery properties. Numerical studies are used to illustrate the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.06718v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lin Xiao, Luo Xiao</dc:creator>
    </item>
    <item>
      <title>Data-driven uncertainty quantification for constrained stochastic differential equations and application to solar photovoltaic power forecast data</title>
      <link>https://arxiv.org/abs/2302.13133</link>
      <description>arXiv:2302.13133v3 Announce Type: replace 
Abstract: In this work, we extend the data-driven It\^{o} stochastic differential equation (SDE) framework for the pathwise assessment of short-term forecast errors to account for the time-dependent upper bound that naturally constrains the observable historical data and forecast. We propose a new nonlinear and time-inhomogeneous SDE model with a Jacobi-type diffusion term for the phenomenon of interest, simultaneously driven by the forecast and the constraining upper bound. We rigorously demonstrate the existence and uniqueness of a strong solution to the SDE model by imposing a condition for the time-varying mean-reversion parameter appearing in the drift term. The normalized forecast function is thresholded to keep such mean-reversion parameters bounded. The SDE model parameter calibration is applied to user-selected approximations of the likelihood function. Another novel contribution is estimating the unknown transition density of the forecast error process with a tailored kernel smoothing technique with the control variate method, coupling an adequate SDE to the original one. We provide a theoretical study about how to choose the optimal bandwidth. We fit the model to the 2019 photovoltaic (PV) solar power daily production and forecast data in Uruguay, computing the daily maximum solar PV production estimation. Two statistical versions of the constrained SDE model are fit, with the beta and truncated normal distributions as proxies for the transition density. Empirical results include simulations of the normalized solar PV power production and pathwise confidence bands generated through an indirect inference method. An objective comparison of optimal parametric points associated with the two selected statistical approximations is provided by applying our innovative kernel smoothing estimation technique of the transition function of the forecast error process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.13133v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Khaoula Ben Chaabane, Ahmed Kebaier, Marco Scavino, Ra\'ul Tempone</dc:creator>
    </item>
    <item>
      <title>Generating contingency tables with fixed marginal probabilities and dependence structures described by loglinear models</title>
      <link>https://arxiv.org/abs/2303.08568</link>
      <description>arXiv:2303.08568v2 Announce Type: replace 
Abstract: We present a method to generate contingency tables that follow loglinear models with prescribed marginal probabilities and dependence structures. We make use of (loglinear) Poisson regression, where the dependence structures, described using odds ratios, are implemented using an offset term. We apply this methodology to carry out simulation studies in the context of population size estimation using dual system and triple system estimators, popular in official statistics. These estimators use contingency tables that summarise the counts of elements enumerated or captured within lists that are linked. The simulation is used to investigate these estimators in the situation that the model assumptions are fulfilled, and the situation that the model assumptions are violated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.08568v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/00949655.2024.2353760</arxiv:DOI>
      <arxiv:journal_reference>Journal of Statistical Computation and Simulation, 1-16 (2024)</arxiv:journal_reference>
      <dc:creator>Ceejay Hammond, Peter G. M. van der Heijden, Paul A. Smith</dc:creator>
    </item>
    <item>
      <title>Copula-like inference for discrete bivariate distributions with rectangular supports</title>
      <link>https://arxiv.org/abs/2307.04225</link>
      <description>arXiv:2307.04225v4 Announce Type: replace 
Abstract: After reviewing a large body of literature on the modeling of bivariate discrete distributions with finite support, \cite{Gee20} made a compelling case for the use of $I$-projections in the sense of \cite{Csi75} as a sound way to attempt to decompose a bivariate probability mass function (p.m.f.) into its two univariate margins and a bivariate p.m.f.\ with uniform margins playing the role of a discrete copula. From a practical perspective, the necessary $I$-projections on Fr\'echet classes can be carried out using the iterative proportional fitting procedure (IPFP), also known as Sinkhorn's algorithm or matrix scaling in the literature. After providing conditions under which a bivariate p.m.f.\ can be decomposed in the aforementioned sense, we investigate, for starting bivariate p.m.f.s with rectangular supports, nonparametric and parametric estimation procedures as well as goodness-of-fit tests for the underlying discrete copula. Related asymptotic results are provided and build upon a differentiability result for $I$-projections on Fr\'echet classes which can be of independent interest. Theoretical results are complemented by finite-sample experiments and a data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.04225v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ivan Kojadinovic, Tommaso Martini</dc:creator>
    </item>
    <item>
      <title>Choosing a Proxy Metric from Past Experiments</title>
      <link>https://arxiv.org/abs/2309.07893</link>
      <description>arXiv:2309.07893v2 Announce Type: replace 
Abstract: In many randomized experiments, the treatment effect of the long-term metric (i.e. the primary outcome of interest) is often difficult or infeasible to measure. Such long-term metrics are often slow to react to changes and sufficiently noisy they are challenging to faithfully estimate in short-horizon experiments. A common alternative is to measure several short-term proxy metrics in the hope they closely track the long-term metric -- so they can be used to effectively guide decision-making in the near-term. We introduce a new statistical framework to both define and construct an optimal proxy metric for use in a homogeneous population of randomized experiments. Our procedure first reduces the construction of an optimal proxy metric in a given experiment to a portfolio optimization problem which depends on the true latent treatment effects and noise level of experiment under consideration. We then denoise the observed treatment effects of the long-term metric and a set of proxies in a historical corpus of randomized experiments to extract estimates of the latent treatment effects for use in the optimization problem. One key insight derived from our approach is that the optimal proxy metric for a given experiment is not apriori fixed; rather it should depend on the sample size (or effective noise level) of the randomized experiment for which it is deployed. To instantiate and evaluate our framework, we employ our methodology in a large corpus of randomized experiments from an industrial recommendation system and construct proxy metrics that perform favorably relative to several baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.07893v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nilesh Tripuraneni, Lee Richardson, Alexander D'Amour, Jacopo Soriano, Steve Yadlowsky</dc:creator>
    </item>
    <item>
      <title>Invariant Probabilistic Prediction</title>
      <link>https://arxiv.org/abs/2309.10083</link>
      <description>arXiv:2309.10083v2 Announce Type: replace 
Abstract: In recent years, there has been a growing interest in statistical methods that exhibit robust performance under distribution changes between training and test data. While most of the related research focuses on point predictions with the squared error loss, this article turns the focus towards probabilistic predictions, which aim to comprehensively quantify the uncertainty of an outcome variable given covariates. Within a causality-inspired framework, we investigate the invariance and robustness of probabilistic predictions with respect to proper scoring rules. We show that arbitrary distribution shifts do not, in general, admit invariant and robust probabilistic predictions, in contrast to the setting of point prediction. We illustrate how to choose evaluation metrics and restrict the class of distribution shifts to allow for identifiability and invariance in the prototypical Gaussian heteroscedastic linear model. Motivated by these findings, we propose a method to yield invariant probabilistic predictions, called IPP, and study the consistency of the underlying parameters. Finally, we demonstrate the empirical performance of our proposed procedure on simulated as well as on single-cell data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10083v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alexander Henzi, Xinwei Shen, Michael Law, Peter B\"uhlmann</dc:creator>
    </item>
    <item>
      <title>Conformal prediction with local weights: randomization enables local guarantees</title>
      <link>https://arxiv.org/abs/2310.07850</link>
      <description>arXiv:2310.07850v3 Announce Type: replace 
Abstract: In this work, we consider the problem of building distribution-free prediction intervals with finite-sample conditional coverage guarantees. Conformal prediction (CP) is an increasingly popular framework for building prediction intervals with distribution-free guarantees, but these guarantees only ensure marginal coverage: the probability of coverage is averaged over a random draw of both the training and test data, meaning that there might be substantial undercoverage within certain subpopulations. Instead, ideally, we would want to have local coverage guarantees that hold for each possible value of the test point's features. While the impossibility of achieving pointwise local coverage is well established in the literature, many variants of conformal prediction algorithm show favorable local coverage properties empirically. Relaxing the definition of local coverage can allow for a theoretical understanding of this empirical phenomenon. We aim to bridge this gap between theoretical validation and empirical performance by proving achievable and interpretable guarantees for a relaxed notion of local coverage. Building on the localized CP method of Guan (2023) and the weighted CP framework of Tibshirani et al. (2019), we propose a new method, randomly-localized conformal prediction (RLCP), which returns prediction intervals that are not only marginally valid but also achieve a relaxed local coverage guarantee and guarantees under covariate shift. Through a series of simulations and real data experiments, we validate these coverage guarantees of RLCP while comparing it with the other local conformal prediction methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07850v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rohan Hore, Rina Foygel Barber</dc:creator>
    </item>
    <item>
      <title>An Experimental Design for Anytime-Valid Causal Inference on Multi-Armed Bandits</title>
      <link>https://arxiv.org/abs/2311.05794</link>
      <description>arXiv:2311.05794v3 Announce Type: replace 
Abstract: Experimentation is crucial for managers to rigorously quantify the value of a change and determine if it leads to a statistically significant improvement over the status quo, thus augmenting their decision-making. Many companies now mandate that all changes undergo experimentation, presenting two challenges: (1) reducing the risk/cost of experimentation by minimizing the proportion of customers assigned to the inferior treatment and (2) increasing the experimentation velocity by enabling managers to stop experiments as soon as results are statistically significant. This paper simultaneously addresses both challenges by proposing the Mixture Adaptive Design (MAD), a new experimental design for multi-armed bandit (MAB) algorithms that enables anytime valid inference on the Average Treatment Effect (ATE) for any MAB algorithm. Intuitively, the MAB "mixes" any bandit algorithm with a Bernoulli design such that at each time step, the probability that a customer is assigned via the Bernoulli design is controlled by a user-specified deterministic sequence that can converge to zero. The sequence enables managers to directly and interpretably control the trade-off between regret minimization and inferential precision. Under mild conditions on the rate the sequence converges to zero, we provide a confidence sequence that is asymptotically anytime valid and demonstrate that the MAD is guaranteed to have a finite stopping time in the presence of a true non-zero ATE. Hence, the MAD allows managers to stop experiments early when a significant ATE is detected while ensuring valid inference, enhancing both the efficiency and reliability of adaptive experiments. Empirically, we demonstrate that the MAD achieves finite-sample anytime-validity while accurately and precisely estimating the ATE, all without incurring significant losses in reward compared to standard bandit designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05794v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Biyonka Liang, Iavor Bojinov</dc:creator>
    </item>
    <item>
      <title>False Discovery Rate Control For Structured Multiple Testing: Asymmetric Rules And Conformal Q-values</title>
      <link>https://arxiv.org/abs/2311.15322</link>
      <description>arXiv:2311.15322v4 Announce Type: replace 
Abstract: The effective utilization of structural information in data while ensuring statistical validity poses a significant challenge in false discovery rate (FDR) analyses. Conformal inference provides rigorous theory for grounding complex machine learning methods without relying on strong assumptions or highly idealized models. However, existing conformal methods have limitations in handling structured multiple testing. This is because their validity requires the deployment of symmetric rules, which assume the exchangeability of data points and permutation-invariance of fitting algorithms. To overcome these limitations, we introduce the pseudo local index of significance (PLIS) procedure, which is capable of accommodating asymmetric rules and requires only pairwise exchangeability between the null conformity scores. We demonstrate that PLIS offers finite-sample guarantees in FDR control and the ability to assign higher weights to relevant data points. Numerical results confirm the effectiveness and robustness of PLIS and show improvements in power compared to existing model-free methods in various scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15322v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zinan Zhao, Wenguang Sun</dc:creator>
    </item>
    <item>
      <title>Alternative Approaches for Estimating Highest-Density Regions</title>
      <link>https://arxiv.org/abs/2401.00245</link>
      <description>arXiv:2401.00245v2 Announce Type: replace 
Abstract: Among the variety of statistical intervals, highest-density regions (HDRs) stand out for their ability to effectively summarize a distribution or sample, unveiling its distinctive and salient features. An HDR represents the minimum size set that satisfies a certain probability coverage, and current methods for their computation require knowledge or estimation of the underlying probability distribution or density $f$. In this work, we illustrate a broader framework for computing HDRs, which generalizes the classical density quantile method introduced in the seminal paper of Hyndman (1996). The framework is based on neighbourhood measures, i.e., measures that preserve the order induced in the sample by $f$, and include the density $f$ as a special case. We explore a number of suitable distance-based measures, such as the $k$-nearest neighborhood distance, and some probabilistic variants based on copula models. An extensive comparison is provided, showing the advantages of the copula-based strategy, especially in those scenarios that exhibit complex structures (e.g., multimodalities or particular dependencies). Finally, we discuss the practical implications of our findings for estimating HDRs in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00245v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nina Deliu, Brunero Liseo</dc:creator>
    </item>
    <item>
      <title>Robust Functional Data Analysis for Stochastic Evolution Equations in Infinite Dimensions</title>
      <link>https://arxiv.org/abs/2401.16286</link>
      <description>arXiv:2401.16286v2 Announce Type: replace 
Abstract: We develop an asymptotic theory for the jump robust measurement of covariations in the context of stochastic evolution equation in infinite dimensions. Namely, we identify scaling limits for realized covariations of solution processes with the quadratic covariation of the latent random process that drives the evolution equation which is assumed to be a Hilbert space-valued semimartingale. We discuss applications to dynamically consistent and outlier-robust dimension reduction in the spirit of functional principal components and the estimation of infinite-dimensional stochastic volatility models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16286v2</guid>
      <category>stat.ME</category>
      <category>q-fin.MF</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dennis Schroers</dc:creator>
    </item>
    <item>
      <title>Data integration of non-probability and probability samples with predictive mean matching</title>
      <link>https://arxiv.org/abs/2403.13750</link>
      <description>arXiv:2403.13750v2 Announce Type: replace 
Abstract: In this paper we study predictive mean matching mass imputation estimators to integrate data from probability and non-probability samples. We consider two approaches: matching predicted to predicted ($\hat{y}-\hat{y}$~matching; PMM A) and predicted to observed ($\hat{y}-y$~matching; PMM B) values. We prove the consistency of two semi-parametric mass imputation estimators based on these approaches and derive their variance and estimators of variance. We underline the differences of our approach with the nearest neighbour approach proposed by Yang et al. (2021) and prove consistency of the PMM A estimator under model mis-specification. Our approach can be employed with non-parametric regression techniques, such as kernel regression, and the analytical expression for variance can also be applied in nearest neighbour matching for non-probability samples. We conduct extensive simulation studies in order to compare the properties of this estimator with existing approaches, discuss the selection of $k$-nearest neighbours, and study the effects of model mis-specification. The paper finishes with empirical study in integration of job vacancy survey and vacancies submitted to public employment offices (admin and online data). Open source software is available for the proposed approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13750v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Piotr Chlebicki, {\L}ukasz Chrostowski, Maciej Ber\k{e}sewicz</dc:creator>
    </item>
    <item>
      <title>Asymptotic Properties of Matthews Correlation Coefficient</title>
      <link>https://arxiv.org/abs/2405.12622</link>
      <description>arXiv:2405.12622v2 Announce Type: replace 
Abstract: Evaluating classifications is crucial in statistics and machine learning, as it influences decision-making across various fields, such as patient prognosis and therapy in critical conditions. The Matthews correlation coefficient (MCC) is recognized as a performance metric with high reliability, offering a balanced measurement even in the presence of class imbalances. Despite its importance, there remains a notable lack of comprehensive research on the statistical inference of MCC. This deficiency often leads to studies merely validating and comparing MCC point estimates, a practice that, while common, overlooks the statistical significance and reliability of results. Addressing this research gap, our paper introduces and evaluates several methods to construct asymptotic confidence intervals for the single MCC and the differences between MCCs in paired designs. Through simulations across various scenarios, we evaluate the finite-sample behavior of these methods and compare their performances. Furthermore, through real data analysis, we illustrate the potential utility of our findings in comparing binary classifiers, highlighting the possible contributions of our research in this field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12622v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuki Itaya, Jun Tamura, Kenichi Hayashi, Kouji Yamamoto</dc:creator>
    </item>
    <item>
      <title>Identifying Sample Size and Accuracy and Precision of the Estimators in Case-Crossover Designs with Distributed Lags of Heteroskedastic Time-Varying Continuous Exposures Measured with Simple or Complex Error</title>
      <link>https://arxiv.org/abs/2406.02369</link>
      <description>arXiv:2406.02369v3 Announce Type: replace 
Abstract: Understanding of sample size, statistical power, and the accuracy and precision of the estimator in epidemiological research can facilitate power and bias analyses. However, such understanding can become complicated for several reasons. First, exposures varying spatiotemporally may be heteroskedastic. Second, distributed lags of exposures may be used to identify critical exposure time-windows. Third, exposure measurement error may exist, impacting the accuracy and/or precision of the estimator that consequently affects sample size and statistical power. Fourth, research may rely on different study designs, so understanding may differ. For example, case-crossover designs as matched case-control designs, are used to estimate health effects of short-term exposures. To address these gaps, I developed approximation equations for sample size, estimates of the estimators and standard errors, including polynomials for non-linear effect estimation. With air pollution exposure estimates, I examined approximations using statistical simulations. Overall, sample size, the accuracy and precision of the estimators can be approximated based on external information about validation, without validation data in hand. For distributed lags, approximations may perform well if residual confounding due to covariate measurement errors is not severe. This condition may be difficult to identify without validation data, so validation research is recommended in identifying critical exposure time-windows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02369v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Honghyok Kim</dc:creator>
    </item>
    <item>
      <title>When Pearson $\chi^2$ and other divisible statistics are not goodness-of-fit tests</title>
      <link>https://arxiv.org/abs/2406.09195</link>
      <description>arXiv:2406.09195v2 Announce Type: replace 
Abstract: Thousands of experiments are analyzed and papers are published each year involving the statistical analysis of grouped data. While this area of statistics is often perceived - somewhat naively - as saturated, several misconceptions still affect everyday practice, and new frontiers have so far remained unexplored. Researchers must be aware of the limitations affecting their analyses and what are the new possibilities in their hands.
  Motivated by this need, the article introduces a unifying approach to the analysis of grouped data which allows us to study the class of divisible statistics - that includes Pearson's $\chi^2$, the likelihood ratio as special cases - with a fresh perspective. The contributions collected in this manuscript span from modeling and estimation to distribution-free goodness-of-fit tests.
  Perhaps the most surprising result presented here is that, in a sparse regime, all tests proposed in the literature are dominated by a class of weighted linear statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09195v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>physics.data-an</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sara Algeri, Estate V. Khmaladze</dc:creator>
    </item>
    <item>
      <title>Treatment Effects in Market Equilibrium</title>
      <link>https://arxiv.org/abs/2109.11647</link>
      <description>arXiv:2109.11647v3 Announce Type: replace-cross 
Abstract: Policy-relevant treatment effect estimation in a marketplace setting requires taking into account both the direct benefit of the treatment and any spillovers induced by changes to the market equilibrium. The standard way to address these challenges is to evaluate interventions via cluster-randomized experiments, where each cluster corresponds to an isolated market. This approach, however, cannot be used when we only have access to a single market (or a small number of markets). Here, we show how to identify and estimate policy-relevant treatment effects using a unit-level randomized trial run within a single large market. A standard Bernoulli-randomized trial allows consistent estimation of direct effects, and of treatment heterogeneity measures that can be used for welfare-improving targeting. Estimating spillovers - as well as providing confidence intervals for the direct effect - requires estimates of price elasticities, which we provide using an augmented experimental design. Our results rely on all spillovers being mediated via the (observed) prices of a finite number of traded goods, and the market power of any single unit decaying as the market gets large. We illustrate our results using a simulation calibrated to a conditional cash transfer experiment in the Philippines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.11647v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Evan Munro, Xu Kuang, Stefan Wager</dc:creator>
    </item>
    <item>
      <title>Background Modeling for Double Higgs Boson Production: Density Ratios and Optimal Transport</title>
      <link>https://arxiv.org/abs/2208.02807</link>
      <description>arXiv:2208.02807v3 Announce Type: replace-cross 
Abstract: We study the problem of data-driven background estimation, arising in the search of physics signals predicted by the Standard Model at the Large Hadron Collider. Our work is motivated by the search for the production of pairs of Higgs bosons decaying into four bottom quarks. A number of other physical processes, known as background, also share the same final state. The data arising in this problem is therefore a mixture of unlabeled background and signal events, and the primary aim of the analysis is to determine whether the proportion of unlabeled signal events is nonzero. A challenging but necessary first step is to estimate the distribution of background events. Past work in this area has determined regions of the space of collider events where signal is unlikely to appear, and where the background distribution is therefore identifiable. The background distribution can be estimated in these regions, and extrapolated into the region of primary interest using transfer learning with a multivariate classifier. We build upon this existing approach in two ways. First, we revisit this method by developing a customized residual neural network which is tailored to the structure and symmetries of collider data. Second, we develop a new method for background estimation, based on the optimal transport problem, which relies on modeling assumptions distinct from earlier work. These two methods can serve as cross-checks for each other in particle physics analyses, due to the complementarity of their underlying assumptions. We compare their performance on simulated double Higgs boson data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.02807v3</guid>
      <category>stat.AP</category>
      <category>hep-ex</category>
      <category>hep-ph</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tudor Manole, Patrick Bryant, John Alison, Mikael Kuusela, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Consider or Choose? The Role and Power of Consideration Sets</title>
      <link>https://arxiv.org/abs/2302.04354</link>
      <description>arXiv:2302.04354v3 Announce Type: replace-cross 
Abstract: Consideration sets play a crucial role in discrete choice modeling, where customers are commonly assumed to go through a two-stage decision making process. Specifically, customers are assumed to form consideration sets in the first stage and then use a second-stage choice mechanism to pick the product with the highest utility from the consideration sets. Recent studies mostly aim to propose more powerful choice mechanisms based on advanced non-parametric models to improve prediction accuracy. In contrast, this paper takes a step back from exploring more complex second-stage choice mechanisms and instead focus on how effectively we can model customer choice relying only on the first-stage consideration set formation. To this end, we study a class of nonparametric choice models that is only specified by a distribution over consideration sets and has a bounded rationality interpretation. We denote it as the consideration set model. Intriguingly, we show that this class of choice models can be characterized by the axiom of symmetric demand cannibalization, which enables complete statistical identification. We further consider the model's downstream assortment planning as an application. We first present an exact description of the optimal assortment, proving that it is revenue-ordered based on the blocks defined by the consideration sets. Despite this compelling structure, we establish that the assortment optimization problem under this model is NP-hard even to approximate. This result shows that accounting for consideration sets in the model inevitably results in inapproximability in assortment planning, even though the consideration set model uses the simplest possible uniform second-stage choice mechanism. Finally, using a real-world dataset, we show the tremendous power of the first-stage consideration sets when modeling customers' decision-making processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.04354v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi-Chun Akchen, Dmitry Mitrofanov</dc:creator>
    </item>
    <item>
      <title>Counterfactual Generative Models for Time-Varying Treatments</title>
      <link>https://arxiv.org/abs/2305.15742</link>
      <description>arXiv:2305.15742v4 Announce Type: replace-cross 
Abstract: Estimating the counterfactual outcome of treatment is essential for decision-making in public health and clinical science, among others. Often, treatments are administered in a sequential, time-varying manner, leading to an exponentially increased number of possible counterfactual outcomes. Furthermore, in modern applications, the outcomes are high-dimensional and conventional average treatment effect estimation fails to capture disparities in individuals. To tackle these challenges, we propose a novel conditional generative framework capable of producing counterfactual samples under time-varying treatment, without the need for explicit density estimation. Our method carefully addresses the distribution mismatch between the observed and counterfactual distributions via a loss function based on inverse probability re-weighting, and supports integration with state-of-the-art conditional generative models such as the guided diffusion and conditional variational autoencoder. We present a thorough evaluation of our method using both synthetic and real-world data. Our results demonstrate that our method is capable of generating high-quality counterfactual samples and outperforms the state-of-the-art baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.15742v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3637528.3671950</arxiv:DOI>
      <dc:creator>Shenghao Wu, Wenbin Zhou, Minshuo Chen, Shixiang Zhu</dc:creator>
    </item>
    <item>
      <title>Regularization of the ensemble Kalman filter using a non-parametric, non-stationary spatial model</title>
      <link>https://arxiv.org/abs/2306.14318</link>
      <description>arXiv:2306.14318v4 Announce Type: replace-cross 
Abstract: The sample covariance matrix of a random vector is a good estimate of the true covariance matrix if the sample size is much larger than the length of the vector. In high-dimensional problems, this condition is never met. As a result, in high dimensions the Ensemble Kalman Filter's (EnKF) ensemble does not contain enough information to specify the prior covariance matrix accurately. This necessitates the need for regularization of the analysis (observation update) problem. We propose a regularization technique based on a new spatial model. The model is a constrained version of the general Gaussian process convolution model. The constraints include local stationarity and smoothness of local spectra. We regularize EnKF by postulating that its prior covariances obey the spatial model. Placing a hyperprior distribution on the model parameters and using the likelihood of the prior ensemble data allows for an optimized use of both the ensemble and the hyperprior. The respective estimator is shown to be consistent. Its neural Bayes implementation proved to be both accurate and computationally efficient. In simulation experiments, the new technique led to substantially better EnKF performance than several existing techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14318v4</guid>
      <category>physics.data-an</category>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Tsyrulnikov, Arseniy Sotskiy</dc:creator>
    </item>
    <item>
      <title>When Graph Neural Network Meets Causality: Opportunities, Methodologies and An Outlook</title>
      <link>https://arxiv.org/abs/2312.12477</link>
      <description>arXiv:2312.12477v3 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) have emerged as powerful representation learning tools for capturing complex dependencies within diverse graph-structured data. Despite their success in a wide range of graph mining tasks, GNNs have raised serious concerns regarding their trustworthiness, including susceptibility to distribution shift, biases towards certain populations, and lack of explainability. Recently, integrating causal learning techniques into GNNs has sparked numerous ground-breaking studies since many GNN trustworthiness issues can be alleviated by capturing the underlying data causality rather than superficial correlations. In this survey, we comprehensively review recent research efforts on Causality-Inspired GNNs (CIGNNs). Specifically, we first employ causal tools to analyze the primary trustworthiness risks of existing GNNs, underscoring the necessity for GNNs to comprehend the causal mechanisms within graph data. Moreover, we introduce a taxonomy of CIGNNs based on the type of causal learning capability they are equipped with, i.e., causal reasoning and causal representation learning. Besides, we systematically introduce typical methods within each category and discuss how they mitigate trustworthiness risks. Finally, we summarize useful resources and discuss several future directions, hoping to shed light on new research opportunities in this emerging field. The representative papers, along with open-source data and codes, are available in https://github.com/usail-hkust/Causality-Inspired-GNNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12477v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenzhao Jiang, Hao Liu, Hui Xiong</dc:creator>
    </item>
    <item>
      <title>Heterogeneous Treatment Effects and Causal Mechanisms</title>
      <link>https://arxiv.org/abs/2404.01566</link>
      <description>arXiv:2404.01566v2 Announce Type: replace-cross 
Abstract: The credibility revolution advances the use of research designs that permit identification and estimation of causal effects. However, understanding which mechanisms produce measured causal effects remains a challenge. A dominant current approach to the quantitative evaluation of mechanisms relies on the detection of heterogeneous treatment effects with respect to pre-treatment covariates. This paper develops a framework to understand when the existence of such heterogeneous treatment effects can support inferences about the activation of a mechanism. We show first that this design cannot provide evidence of mechanism activation without an additional, generally implicit, assumption. Further, even when this assumption is satisfied, if a measured outcome is produced by a non-linear transformation of a directly-affected outcome of theoretical interest, heterogeneous treatment effects are not informative of mechanism activation. We provide novel guidance for interpretation and research design in light of these findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01566v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Fu, Tara Slough</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for Consistent Predictions in Overparameterized Nonlinear Regression</title>
      <link>https://arxiv.org/abs/2404.04498</link>
      <description>arXiv:2404.04498v2 Announce Type: replace-cross 
Abstract: The remarkable generalization performance of large-scale models has been challenging the conventional wisdom of the statistical learning theory. Although recent theoretical studies have shed light on this behavior in linear models and nonlinear classifiers, a comprehensive understanding of overparameterization in nonlinear regression models is still lacking. This study explores the predictive properties of overparameterized nonlinear regression within the Bayesian framework, extending the methodology of the adaptive prior considering the intrinsic spectral structure of the data. Posterior contraction is established for generalized linear and single-neuron models with Lipschitz continuous activation functions, demonstrating the consistency in the predictions of the proposed approach. Moreover, the Bayesian framework enables uncertainty estimation of the predictions. The proposed method was validated via numerical simulations and a real data application, showing its ability to achieve accurate predictions and reliable uncertainty estimates. This work provides a theoretical understanding of the advantages of overparameterization and a principled Bayesian approach to large nonlinear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04498v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoya Wakayama</dc:creator>
    </item>
  </channel>
</rss>
