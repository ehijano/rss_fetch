<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Jul 2025 01:43:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Mapping Dengue Vulnerability in Recife, Brazil: Socioeconomic Insights from PCA and Robust Regression</title>
      <link>https://arxiv.org/abs/2507.08814</link>
      <description>arXiv:2507.08814v1 Announce Type: new 
Abstract: Based on approximately 90,000 confirmed dengue cases reported in Recife - a major city in northeastern Brazil - between 2015 and 2024, we conducted a neighborhood-level spatial analysis. Socioeconomic and demographic indicators from the 2022 Brazilian Census were integrated to explore factors associated with the spatial distribution of dengue incidence. To address multicollinearity and reduce dimensionality, we applied Principal Component Analysis (PCA) to the explanatory variables. Using the resulting components, we built predictive models via Ordinary Least Squares (OLS), robust regression, and Random Forest algorithms. The OLS model explained 60.4% of the variance in case density (cases per square kilometer), while the robust model - more resilient to outliers - accounted for 43.2%. The Random Forest model, capturing nonlinear patterns, achieved 37.3%. Despite some localized gains from nonlinearity, linear models showed greater overall stability and interpretability. Using PCA scores, we constructed a dengue risk ranking of neighborhoods and compared it to the actual 2024 distribution, achieving an 83.5% match in relative ordering. Our findings indicate that census-based socioeconomic data, when combined with dimensionality reduction and predictive modeling, can effectively estimate urban dengue risk and guide spatially targeted public health strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08814v1</guid>
      <category>stat.AP</category>
      <category>q-bio.PE</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marc\'ilio Ferreira dos Santos</dc:creator>
    </item>
    <item>
      <title>Hierarchical Bayesian Modeling of Total Column Ozone: Unraveling Equatorial Variability over Ethiopia Using Satellite Data and Multisource Covariates</title>
      <link>https://arxiv.org/abs/2507.09046</link>
      <description>arXiv:2507.09046v1 Announce Type: new 
Abstract: Understanding the spatiotemporal dynamics of total column ozone (TCO) is critical for monitoring ultraviolet (UV) exposure and ozone trends, particularly in equatorial regions where variability remains underexplored. This study investigates monthly TCO over Ethiopia (2012-2022) using a Bayesian hierarchical model implemented via Integrated Nested Laplace Approximation (INLA). The model incorporates nine environmental covariates, capturing meteorological, stratospheric, and topographic influences alongside spatiotemporal random effects. Spatial dependence is modeled using the Stochastic Partial Differential Equation (SPDE) approach, while temporal autocorrelation is handled through an autoregressive structure. The model shows strong predictive accuracy, with correlation coefficients of 0.94 (training) and 0.91 (validation), and RMSE values of 3.91 DU and 4.45 DU, respectively. Solar radiation, stratospheric temperature, and the Quasi-Biennial Oscillation are positively associated with TCO, whereas surface temperature, precipitation, humidity, water vapor, and altitude exhibit negative associations. Random effects highlight persistent regional clusters and seasonal peaks during summer. These findings provide new insights into regional ozone behavior over complex equatorial terrains, contributing to the understanding of the equatorial ozone paradox. The approach demonstrates the utility of combining satellite observations with environmental data in data-scarce regions, supporting improved UV risk monitoring and climate-informed policy planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09046v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yassin Tesfaw Abebe, Abdu Mohammed Seid, Lassi Roininen, U. Jaya Parakash Raju, Abebaw Bizuneh Alemu</dc:creator>
    </item>
    <item>
      <title>Spatial Dependencies in Item Response Theory: Gaussian Process Priors for Geographic and Cognitive Measurement</title>
      <link>https://arxiv.org/abs/2507.09824</link>
      <description>arXiv:2507.09824v1 Announce Type: new 
Abstract: Measurement validity in Item Response Theory depends on appropriately modeling dependencies between items when these reflect meaningful theoretical structures rather than random measurement error. In ecological assessment, citizen scientists identifying species across geographic regions exhibit systematic spatial patterns in task difficulty due to environmental factors. Similarly, in Author Recognition Tests, literary knowledge organizes by genre, where familiarity with science fiction authors systematically predicts recognition of other science fiction authors. Current spatial Item Response Theory methods, represented by the 1PLUS, 2PLUS, and 3PLUS model family, address these dependencies but remain limited by (1) binary response restrictions, and (2) conditional autoregressive priors that impose rigid local correlation assumptions, preventing effective modeling of complex spatial relationships. Our proposed method, Spatial Gaussian Process Item Response Theory (SGP-IRT), addresses these limitations by replacing conditional autoregressive priors with flexible Gaussian process priors that adapt to complex dependency structures while maintaining principled uncertainty quantification. SGP-IRT accommodates polytomous responses and models spatial dependencies in both geographic and abstract cognitive spaces, where items cluster by theoretical constructs rather than physical proximity. Simulation studies demonstrate improved parameter recovery, particularly for item difficulty estimation. Empirical applications show enhanced recovery of meaningful difficulty surfaces and improved measurement precision across psychological, educational, and ecological research applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09824v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingya Huang, Soham Ghosh</dc:creator>
    </item>
    <item>
      <title>Gradient boosted multi-population mortality modelling with high-frequency data</title>
      <link>https://arxiv.org/abs/2507.09983</link>
      <description>arXiv:2507.09983v1 Announce Type: new 
Abstract: High-frequency mortality data remains an understudied yet critical research area. While its analysis can reveal short-term health impacts of climate extremes and enable more timely mortality forecasts, its complex temporal structure poses significant challenges to traditional mortality models. To leverage the power of high-frequency mortality data, this paper introduces a novel integration of gradient boosting techniques into traditional stochastic mortality models under a multi-population setting. Our key innovation lies in using the Li and Lee model as the weak learner within the gradient boosting framework, replacing conventional decision trees. Empirical studies are conducted using weekly mortality data from 30 countries (Human Mortality Database, 2015--2019). The proposed methodology not only enhances model fit by accurately capturing underlying mortality trends and seasonal patterns, but also achieves superior forecast accuracy, compared to the benchmark models. We also investigate a key challenge in multi-population mortality modelling: how to select appropriate sub-populations with sufficiently similar mortality experiences. A comprehensive clustering exercise is conducted based on mortality improvement rates and seasonal strength. The results demonstrate the robustness of our proposed model, yielding stable forecast accuracy under different clustering configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09983v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziting Miao, Han Li, Yuyu Chen</dc:creator>
    </item>
    <item>
      <title>An Accurate Discretized Approach to Parameter Estimation in the CKLS Model via the CIR Framework</title>
      <link>https://arxiv.org/abs/2507.10041</link>
      <description>arXiv:2507.10041v1 Announce Type: new 
Abstract: This paper provides insight into the estimation and asymptotic behavior of parameters in interest rate models, focusing primarily on the Cox-Ingersoll-Ross (CIR) process and its extension -- the more general Chan-Karolyi-Longstaff-Sanders (CKLS) framework ($\alpha\in[0.5,1]$). The CIR process is widely used in modeling interest rates which possess the mean reverting feature. An Extension of CIR model, CKLS model serves as a foundational case for analyzing more complex dynamics. We employ Euler-Maruyama discretization to transform the continuous-time stochastic differential equations (SDEs) of these models into a discretized form that facilitates efficient simulation and estimation of parameters using linear regression techniques. We established the strong consistency and asymptotic normality of the estimators for the drift and volatility parameters, providing a theoretical underpinning for the parameter estimation process. Additionally, we explore the boundary behavior of these models, particularly in the context of unattainability at zero and infinity, by examining the scale and speed density functions associated with generalized SDEs involving polynomial drift and diffusion terms. Furthermore, we derive sufficient conditions for the existence of a stationary distribution within the CKLS framework and the corresponding stationary density function; and discuss its dependence on model parameters for $\alpha\in[0.5,1]$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10041v1</guid>
      <category>stat.AP</category>
      <category>q-fin.ST</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sourojyoti Barick</dc:creator>
    </item>
    <item>
      <title>History Matching under Uncertainty of Geological Scenarios with Implicit Geological Realism Control with Generative Deep Learning and Graph Convolutions</title>
      <link>https://arxiv.org/abs/2507.10201</link>
      <description>arXiv:2507.10201v1 Announce Type: new 
Abstract: The graph-based variational autoencoder represents an architecture that can handle the uncertainty of different geological scenarios, such as depositional or structural, through the concept of a lowerdimensional latent space. The main difference from recent studies is utilisation of a graph-based approach in reservoir modelling instead of the more traditional lattice-based deep learning methods. We provide a solution to implicitly control the geological realism through the latent variables of a generative model and Geodesic metrics. Our experiments of AHM with synthetic dataset that consists of 3D realisations of channelised geological representations with two distinct scenarios with one and two channels shows the viability of the approach. We offer in-depth analysis of the latent space using tools such as PCA, t-SNE, and TDA to illustrate its structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10201v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gleb Shishaev, Vasily Demyanov, Daniel Arnold</dc:creator>
    </item>
    <item>
      <title>The efficiencies of pilot feasibility trials in rare diseases using Bayesian methods</title>
      <link>https://arxiv.org/abs/2507.10269</link>
      <description>arXiv:2507.10269v1 Announce Type: new 
Abstract: Pilot feasibility studies play a pivotal role in the development of clinical trials for rare diseases, where small populations and slow recruitment often threaten trial viability. While such studies are commonly used to assess operational parameters, they also offer a valuable opportunity to inform the design and analysis of subsequent definitive trials-particularly through the use of Bayesian methods. In this paper, we demonstrate how data from a single, protocol-aligned pilot study can be incorporated into a definitive trial using robust meta-analytic-predictive priors. We focus on the case of a binary efficacy outcome, motivated by a feasibility trial of intravenous immunoglobulin tapering in autoimmune inflammatory myopathies. Through simulation studies, we evaluate the operating characteristics of trials informed by pilot data, including sample size, expected trial duration, and the probability of meeting recruitment targets. Our findings highlight the operational and ethical advantages of leveraging pilot data via robust Bayesian priors, and offer practical guidance for their application in rare disease settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10269v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lara Maleyeff, Val\'erie Leclair, Shirin Golchi, Marie Hudson</dc:creator>
    </item>
    <item>
      <title>A NuSTAR study of quasi-periodic oscillations from the ultraluminous X-ray sources in M82</title>
      <link>https://arxiv.org/abs/2505.16921</link>
      <description>arXiv:2505.16921v1 Announce Type: cross 
Abstract: The study of quasi-periodic oscillations in X-ray binaries provides valuable insights into the physics of accretion around compact objects. The M82 galaxy hosts two ultraluminous X-ray sources (ULXs), one of which is suspected to harbor an intermediate-mass black hole. Using 39 NuSTAR observations acquired between 2014--2024, we investigate the aperiodic X-ray variability in M82. In particular, we study in detail the evolution of the QPO from M82 X-1 in the range 20--300 mHz. We do not find additional timing features in the data, besides a frequent broad noise component at lower frequencies. The QPO behaves similarly to other classes of low-frequency oscillations in accreting compact objects, both black holes and neutron stars.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16921v1</guid>
      <category>astro-ph.HE</category>
      <category>astro-ph.IM</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hamza El Byad, Matteo Bachetti, Silvia Columbu, Giuseppe Rodriguez, Maura Pilia, Matthew J. Middleton, Dominic J Walton, Murray Brightman, Hannah Earnshaw, Karl Forster, Brian Grefenstette, Felix F\"urst, Marianne Heida, Matteo Imbrogno, Eleonora Veronica Lai, Thomas Maccarone</dc:creator>
    </item>
    <item>
      <title>A monotone single index model for spatially-referenced multistate current status data</title>
      <link>https://arxiv.org/abs/2507.09057</link>
      <description>arXiv:2507.09057v1 Announce Type: cross 
Abstract: Assessment of multistate disease progression is commonplace in biomedical research, such as, in periodontal disease (PD). However, the presence of multistate current status endpoints, where only a single snapshot of each subject's progression through disease states is available at a random inspection time after a known starting state, complicates the inferential framework. In addition, these endpoints can be clustered, and spatially associated, where a group of proximally located teeth (within subjects) may experience similar PD status, compared to those distally located. Motivated by a clinical study recording PD progression, we propose a Bayesian semiparametric accelerated failure time model with an inverse-Wishart proposal for accommodating (spatial) random effects, and flexible errors that follow a Dirichlet process mixture of Gaussians. For clinical interpretability, the systematic component of the event times is modeled using a monotone single index model, with the (unknown) link function estimated via a novel integrated basis expansion and basis coefficients endowed with constrained Gaussian process priors. In addition to establishing parameter identifiability, we present scalable computing via a combination of elliptical slice sampling, fast circulant embedding techniques, and smoothing of hard constraints, leading to straightforward estimation of parameters, and state occupation and transition probabilities. Using synthetic data, we study the finite sample properties of our Bayesian estimates, and their performance under model misspecification. We also illustrate our method via application to the real clinical PD dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09057v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Snigdha Das, Minwoo Chae, Debdeep Pati, Dipankar Bandyopadhyay</dc:creator>
    </item>
    <item>
      <title>The Use of Variational Inference for Lifetime Data with Spatial Correlations</title>
      <link>https://arxiv.org/abs/2507.09559</link>
      <description>arXiv:2507.09559v1 Announce Type: cross 
Abstract: Lifetime data with spatial correlations are often collected for analysis in modern engineering, clinical, and medical applications. For such spatial lifetime data, statistical models usually account for the spatial dependence through spatial random effects, such as the cumulative exposure model and the proportional hazards model. For these models, the Bayesian estimation is commonly used for model inference, but often encounters computational challenges when the number of spatial locations is large. The conventional Markov Chain Monte Carlo (MCMC) methods for sampling the posterior can be time-consuming. In this case-study paper, we investigate the capability of variational inference (VI) for the model inference on spatial lifetime data, aiming for a good balance between the estimation accuracy and computational efficiency. Specifically, the VI methods with different divergence metrics are investigated for the spatial lifetime models. In the case study, the Titan GPU lifetime data and the pine tree lifetime data are used to examine the VI methods in terms of their computational advantage and estimation accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09559v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yueyao Wang, Yili Hong, Laura Freeman, Xinwei Deng</dc:creator>
    </item>
    <item>
      <title>Continental scale habitat modelling with artificial intelligence and multimodal earth observation</title>
      <link>https://arxiv.org/abs/2507.09732</link>
      <description>arXiv:2507.09732v1 Announce Type: cross 
Abstract: Habitats integrate the abiotic conditions and biophysical structures that support biodiversity and sustain nature's contributions to people. As these ecosystems face mounting pressure from human activities, accurate, high-resolution habitat maps are essential for effective conservation and restoration. Yet current maps often fall short in thematic or spatial resolution because they must (1) model several mutually exclusive habitat types that co-occur across landscapes and (2) cope with severe class imbalance that complicate multi-class training. Here, we evaluated how high-resolution remote sensing (RS) data and Artificial Intelligence (AI) tools can improve habitat classification over large geographic extents at fine thematic resolution. Using vegetation plots from the European Vegetation Archive, we modelled Level 3 EUNIS habitats across Europe and assessed multiple modelling strategies against independent validation datasets. Strategies that exploited the hierarchical nature of habitat nomenclatures resolved classification ambiguities, especially in fragmented landscapes. Integrating multi-spectral (MSI) and synthetic aperture radar (SAR) imagery, particularly through Earth Observation Foundation models, enhanced within-formation discrimination and overall performance. Finally, ensemble machine learning that corrects class imbalance boosted accuracy further. Our methodological framework is transferable beyond Europe and adaptable to other classification systems. Future research should advance temporal modelling of dynamic habitats, extend to habitat segmentation and quality assessment, and exploit next-generation EO data paired with higher-quality in-situ observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09732v1</guid>
      <category>cs.LG</category>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sara Si-Moussi, Stephan Hennekens, Sander Mucher, Stan Los, Wilfried Thuiller</dc:creator>
    </item>
    <item>
      <title>Long-Tailed Data Classification by Increasing and Decreasing Neurons During Training</title>
      <link>https://arxiv.org/abs/2507.09940</link>
      <description>arXiv:2507.09940v1 Announce Type: cross 
Abstract: In conventional deep learning, the number of neurons typically remains fixed during training. However, insights from biology suggest that the human hippocampus undergoes continuous neuron generation and pruning of neurons over the course of learning, implying that a flexible allocation of capacity can contribute to enhance performance. Real-world datasets often exhibit class imbalance situations where certain classes have far fewer samples than others, leading to significantly reduce recognition accuracy for minority classes when relying on fixed size networks.To address the challenge, we propose a method that periodically adds and removes neurons during training, thereby boosting representational power for minority classes. By retaining critical features learned from majority classes while selectively increasing neurons for underrepresented classes, our approach dynamically adjusts capacity during training. Importantly, while the number of neurons changes throughout training, the final network size and structure remain unchanged, ensuring efficiency and compatibility with deployment.Furthermore, by experiments on three different datasets and five representative models, we demonstrate that the proposed method outperforms fixed size networks and shows even greater accuracy when combined with other imbalance-handling techniques. Our results underscore the effectiveness of dynamic, biologically inspired network designs in improving performance on class-imbalanced data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09940v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taigo Sakai, Kazuhiro Hotta</dc:creator>
    </item>
    <item>
      <title>Radial Neighborhood Smoothing Recommender System</title>
      <link>https://arxiv.org/abs/2507.09952</link>
      <description>arXiv:2507.09952v1 Announce Type: cross 
Abstract: Recommender systems inherently exhibit a low-rank structure in latent space. A key challenge is to define meaningful and measurable distances in the latent space to capture user-user, item-item, user-item relationships effectively. In this work, we establish that distances in the latent space can be systematically approximated using row-wise and column-wise distances in the observed matrix, providing a novel perspective on distance estimation. To refine the distance estimation, we introduce the correction based on empirical variance estimator to account for noise-induced non-centrality. The novel distance estimation enables a more structured approach to constructing neighborhoods, leading to the Radial Neighborhood Estimator (RNE), which constructs neighborhoods by including both overlapped and partially overlapped user-item pairs and employs neighborhood smoothing via localized kernel regression to improve imputation accuracy. We provide the theoretical asymptotic analysis for the proposed estimator. We perform evaluations on both simulated and real-world datasets, demonstrating that RNE achieves superior performance compared to existing collaborative filtering and matrix factorization methods. While our primary focus is on distance estimation in latent space, we find that RNE also mitigates the ``cold-start'' problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09952v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zerui Zhang, Yumou Qiu</dc:creator>
    </item>
    <item>
      <title>A Graph Sufficiency Perspective for Neural Networks</title>
      <link>https://arxiv.org/abs/2507.10215</link>
      <description>arXiv:2507.10215v1 Announce Type: cross 
Abstract: This paper analyzes neural networks through graph variables and statistical sufficiency. We interpret neural network layers as graph-based transformations, where neurons act as pairwise functions between inputs and learned anchor points. Within this formulation, we establish conditions under which layer outputs are sufficient for the layer inputs, that is, each layer preserves the conditional distribution of the target variable given the input variable. Under dense anchor point assumptions, we prove that asymptotic sufficiency holds in the infinite-width limit and is preserved throughout training. To align more closely with practical architectures, we further show that sufficiency can be achieved with finite-width networks by assuming region-separated input distributions and constructing appropriate anchor points. Our framework covers fully connected layers, general pairwise functions, ReLU and sigmoid activations, and convolutional neural networks. This work bridges statistical sufficiency, graph-theoretic representations, and deep learning, providing a new statistical understanding of neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10215v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cencheng Shen, Yuexiao Dong</dc:creator>
    </item>
    <item>
      <title>3D Bivariate Spatial Modelling of Argo Ocean Temperature and Salinity Profiles</title>
      <link>https://arxiv.org/abs/2210.11611</link>
      <description>arXiv:2210.11611v2 Announce Type: replace 
Abstract: Variables contained within the global oceans can detect and reveal the effects of the warming climate, as the oceans absorb huge amounts of solar energy. Hence, information regarding the joint spatial distribution of ocean variables is critical for understanding the climate. In this paper, we investigate the spatial dependence structure between ocean temperature and salinity using data harvested from the Argo program and construct a bivariate spatial model for the data that cover the surface to the ocean's interior. We develop a flexible class of multivariate nonstationary covariance models defined in 3-dimensional (3D) space (longitude $\times$ latitude $\times$ depth) that allow the variances and correlation to vary with ocean depth. These models describe the joint spatial distribution of the two variables while incorporating the underlying vertical structure of the ocean. We apply this framework to temperature and salinity data from Argo floats. To manage the computational challenges posed by the large volume of the Argo data, we apply the Vecchia approximation to the likelihood functions. We demonstrate that the proposed bivariate covariance is able to describe the complex vertical cross-covariance structure between the original processes as well as their first and second-order differenciations, while existing bivariate models, including bivariate Mat\'{e}rn, poorly fit the empirical cross-covariance structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.11611v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mary Lai Salvana, Jian Cao, Mikyoung Jun</dc:creator>
    </item>
    <item>
      <title>Donor's Deferral and Return Behavior: Partial Identification from a Regression Discontinuity Design with Manipulation</title>
      <link>https://arxiv.org/abs/1910.02170</link>
      <description>arXiv:1910.02170v4 Announce Type: replace-cross 
Abstract: Volunteer labor can temporarily yield lower benefits to charities than its costs. In such instances, organizations may wish to defer volunteer donations to a later date. Exploiting a discontinuity in blood donations' eligibility criteria, we show that deferring donors reduces their future volunteerism. In our setting, medical staff manipulates donors' reported hemoglobin levels over a threshold to facilitate donation. Such manipulation invalidates standard regression discontinuity design. To circumvent this issue, we propose a procedure for obtaining partial identification bounds where manipulation is present. Our procedure is applicable in various regression discontinuity settings where the running variable is manipulated.</description>
      <guid isPermaLink="false">oai:arXiv.org:1910.02170v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Evan Rosenman, Karthik Rajkumar, Romain Gauriot, Robert Slonim</dc:creator>
    </item>
    <item>
      <title>GEEPERs: Principal Stratification using Principal Scores and Stacked Estimating Equations</title>
      <link>https://arxiv.org/abs/2212.10406</link>
      <description>arXiv:2212.10406v3 Announce Type: replace-cross 
Abstract: Principal stratification is a framework for making sense of causal effects conditioned on variables that themselves may have been affected by treatment. For instance, one component of an educational computer application is the availability of ``bottom-out'' hints that provide the answer. In evaluating a recent experimental evaluation against alternative programs without bottom-out hints, researchers may be interested in estimating separate average treatment effects for students who, if given the opportunity, would request bottom-out hints frequently, and for students who would not. Most principal stratification estimators rely on strong structural or modeling assumptions, and many require advanced statistical training to fit and check. In this paper, we introduce a new M-estimation principal effect estimator for one-way noncompliance based on a binary indicator. Estimates may be computed using conventional regressions (though the standard errors require a specialized sandwich formula) and do not rely on distributional assumptions. We present a simulation study that demonstrates the novel method's greater robustness compared to popular alternatives and illustrate the method through two real-data analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.10406v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam C. Sales, Kirk P. Vanacore, Erin R. Ottmar</dc:creator>
    </item>
    <item>
      <title>Quantification of Interdependent Emotion Dynamics in Online Interactions</title>
      <link>https://arxiv.org/abs/2408.05700</link>
      <description>arXiv:2408.05700v3 Announce Type: replace-cross 
Abstract: A growing share of human interactions now occurs online, where the expression and perception of emotions are often amplified and distorted. Yet, the interplay between different emotions and the extent to which they are driven by external stimuli or social feedback remains poorly understood. We calibrate a multivariate Hawkes self-exciting point process to model the temporal expression of six basic emotions in YouTube Live chats. This framework captures both temporal and cross-emotional dependencies while allowing us to disentangle the influence of video content (exogenous) from peer interactions (endogenous). We find that emotional expressions are up to four times more strongly driven by peer interaction than by video content. Positivity is more contagious, spreading three times more readily, whereas negativity is more memorable, lingering nearly twice as long. Moreover, we observe asymmetric cross-excitation, with negative emotions frequently triggering positive ones, a pattern consistent with trolling dynamics, but not the reverse. These findings highlight the central role of social interaction in shaping emotional dynamics online and the risks of emotional manipulation as human-chatbot interactions become increasingly realistic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05700v3</guid>
      <category>cs.SI</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yishan Luo, Didier Sornette, Sandro Claudio Lera</dc:creator>
    </item>
    <item>
      <title>Estimating Interpretable Heterogeneous Treatment Effect with Causal Subgroup Discovery in Survival Outcomes</title>
      <link>https://arxiv.org/abs/2409.19241</link>
      <description>arXiv:2409.19241v3 Announce Type: replace-cross 
Abstract: Estimating heterogeneous treatment effect (HTE) for survival outcomes has gained increasing attention, as it captures the variation in treatment efficacy across patients or subgroups in delaying disease progression. However, most existing methods focus on post-hoc subgroup identification rather than simultaneously estimating HTE and selecting relevant subgroups. In this paper, we propose an interpretable HTE estimation framework that integrates three meta-learners that simultaneously estimate CATE for survival outcomes and identify predictive subgroups. We evaluated the performance of our method through comprehensive simulation studies across various randomized clinical trial (RCT) settings. Additionally, we demonstrated its application in a large RCT for age-related macular degeneration (AMD), a polygenic progressive eye disease, to estimate the HTE of an antioxidant and mineral supplement on time-to-AMD progression and to identify genetics-based subgroups with enhanced treatment effects. Our method offers a direct interpretation of the estimated HTE and provides evidence to support precision healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19241v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Na Bo, Ying Ding</dc:creator>
    </item>
    <item>
      <title>Most Swiss-system tournaments are unfair: Evidence from chess</title>
      <link>https://arxiv.org/abs/2410.19333</link>
      <description>arXiv:2410.19333v2 Announce Type: replace-cross 
Abstract: The Swiss-system is an increasingly popular tournament format as it provides an attractive trade-off between the number of matches and ranking accuracy. However, few research consider the optimal design of Swiss-system tournaments. We contribute to this topic by empirically investigating the fairness of 52 Swiss-system chess competitions containing an odd (9 or 11) number of rounds, where about half of the players have an extra game with the white pieces. It is verified that they often enjoy a significant advantage: they are expected to score more points and have higher chances of performing above certain thresholds. A potential solution could be to organise Swiss-system tournaments with an even number of rounds and guarantee a balanced colour assignment for all players using a recently proposed pairing mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19333v2</guid>
      <category>econ.GN</category>
      <category>physics.soc-ph</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o</dc:creator>
    </item>
    <item>
      <title>Highest Posterior Density Intervals of Unimodal Distributions As Analogues to Profile Likelihood Ratio Confidence Intervals</title>
      <link>https://arxiv.org/abs/2412.06528</link>
      <description>arXiv:2412.06528v4 Announce Type: replace-cross 
Abstract: In Bayesian statistics, the highest posterior density (HPD) interval is often used to describe properties of a posterior distribution. As a method for estimating confidence intervals (CIs), the HPD has two main desirable properties. Firstly, it is the shortest interval to have a specified coverage probability. Secondly, every point inside the HPD interval has a density greater than every point outside the interval. However, the HPD interval is sometimes criticized for being transformation invariant.
  We make the case that under certain conditions the HPD interval is a natural analog to the frequentist profile likelihood ratio confidence interval (LRCI). Our main result is to derive a proof showing that under specified conditions, the HPD interval with respect to the density mode is transformation invariant for monotonic functions in a manner which is similar to a profile LRCI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06528v4</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. X. Venu</dc:creator>
    </item>
    <item>
      <title>Prune 'n Predict: Optimizing LLM Decision-making with Conformal Prediction</title>
      <link>https://arxiv.org/abs/2501.00555</link>
      <description>arXiv:2501.00555v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are empowering decision-making in several applications, including tool or API usage and answering multiple-choice questions (MCQs). However, incorrect outputs pose significant risks in high-stakes domains like healthcare and finance. To quantify LLM uncertainty and thereby mitigate these risks, recent works employ conformal prediction (CP), a model- and distribution-agnostic framework that uses LLM outputs to generate a \emph{prediction set} containing the true answer with high probability. Leveraging CP, we propose \emph{conformal revision of questions} (CROQ), which revises the question by narrowing down the available choices to those in the prediction set and asking the LLM the revised question. We expect LLMs to be more accurate on revised questions with fewer choices. Furthermore, we expect CROQ to be effective when the prediction sets from CP are small. Commonly used logit scores often lead to large sets, diminishing CROQ's effectiveness. To overcome this, we propose CP-OPT, an optimization framework to learn scores that minimize set sizes while maintaining coverage. Our extensive experiments on MMLU, ToolAlpaca, and TruthfulQA datasets with multiple LLMs show that CROQ improves accuracy over the standard inference, with more pronounced gains when paired with CP-OPT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00555v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>42nd International Conference on Machine Learning (ICML 2025)</arxiv:journal_reference>
      <dc:creator>Harit Vishwakarma, Alan Mishler, Thomas Cook, Niccol\`o Dalmasso, Natraj Raman, Sumitra Ganesh</dc:creator>
    </item>
    <item>
      <title>BiDepth: A Bidirectional-Depth Neural Network for Spatio-Temporal Prediction</title>
      <link>https://arxiv.org/abs/2501.08411</link>
      <description>arXiv:2501.08411v3 Announce Type: replace-cross 
Abstract: Accurate spatial-temporal (ST) prediction for dynamic systems, such as urban mobility and weather patterns, is crucial but hindered by complex ST correlations and the challenge of concurrently modeling long-term trends with short-term fluctuations. Existing methods often falter in these areas. This paper proposes the BiDepth Multimodal Neural Network (BDMNN), which integrates two key innovations: 1) a bidirectional depth modulation mechanism that dynamically adjusts network depth to comprehensively capture both long-term seasonality and immediate short-term events; and 2) a novel convolutional self-attention cell (CSAC). Critically, unlike many attention mechanisms that can lose spatial acuity, our CSAC is specifically designed to preserve crucial spatial relationships throughout the network, akin to standard convolutional layers, while simultaneously capturing temporal dependencies. Evaluated on real-world urban traffic and precipitation datasets, BDMNN demonstrates significant accuracy improvements, achieving a 12% Mean Squared Error (MSE) reduction in urban traffic prediction and a 15% improvement in precipitation forecasting over leading deep learning benchmarks like ConvLSTM, using comparable computational resources. These advancements offer robust ST forecasting for smart city management, disaster prevention, and resource optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08411v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sina Ehsani, Fenglian Pan, Qingpei Hu, Jian Liu</dc:creator>
    </item>
    <item>
      <title>Adapting OpenAI's CLIP Model for Few-Shot Image Inspection in Manufacturing Quality Control: An Expository Case Study with Multiple Application Examples</title>
      <link>https://arxiv.org/abs/2501.12596</link>
      <description>arXiv:2501.12596v2 Announce Type: replace-cross 
Abstract: This expository paper introduces a simplified approach to image-based quality inspection in manufacturing using OpenAI's CLIP (Contrastive Language-Image Pretraining) model adapted for few-shot learning. While CLIP has demonstrated impressive capabilities in general computer vision tasks, its direct application to manufacturing inspection presents challenges due to the domain gap between its training data and industrial applications. We evaluate CLIP's effectiveness through five case studies: metallic pan surface inspection, 3D printing extrusion profile analysis, stochastic textured surface evaluation, automotive assembly inspection, and microstructure image classification. Our results show that CLIP can achieve high classification accuracy with relatively small learning sets (50-100 examples per class) for single-component and texture-based applications. However, the performance degrades with complex multi-component scenes. We provide a practical implementation framework that enables quality engineers to quickly assess CLIP's suitability for their specific applications before pursuing more complex solutions. This work establishes CLIP-based few-shot learning as an effective baseline approach that balances implementation simplicity with robust performance, demonstrated in several manufacturing quality control applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12596v2</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fadel M. Megahed, Ying-Ju Chen, Bianca Maria Colosimo, Marco Luigi Giuseppe Grasso, L. Allison Jones-Farmer, Sven Knoth, Hongyue Sun, Inez Zwetsloot</dc:creator>
    </item>
    <item>
      <title>HiBayES: A Hierarchical Bayesian Modeling Framework for AI Evaluation Statistics</title>
      <link>https://arxiv.org/abs/2505.05602</link>
      <description>arXiv:2505.05602v3 Announce Type: replace-cross 
Abstract: As Large Language Models (LLMs) and other AI systems evolve, robustly estimating their capabilities from inherently stochastic outputs while systematically quantifying uncertainty in these estimates becomes increasingly important. Further, advanced AI evaluations often have a nested hierarchical structure, exhibit high levels of complexity, and come with high costs in testing the most advanced AI systems. To address these challenges, we introduce HiBayES, a generalizable Hierarchical Bayesian modeling framework for AI Evaluation Statistics. HiBayES supports robust inferences in classical question-answer benchmarks and advanced agentic evaluations, particularly in low-data scenarios (e.g., &lt; 20 data points per evaluation). Built on Generalized Linear Models (GLMs), Bayesian data analysis, and formal model comparison, HiBayES provides principled uncertainty quantification and robust parameter estimation. This paper offers a comprehensive introduction to HiBayES, including illustrative examples, comparisons to conventional statistical methods, and practical guidance for implementing multilevel Bayesian GLMs. Additionally, we provide a HiBayES software package [4] (Beta version) for out-of-the-box implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05602v3</guid>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lennart Luettgau, Harry Coppock, Magda Dubois, Christopher Summerfield, Cozmin Ududec</dc:creator>
    </item>
    <item>
      <title>Modern approaches to building interpretable models of the property market using machine learning on the base of mass cadastral valuation</title>
      <link>https://arxiv.org/abs/2506.15723</link>
      <description>arXiv:2506.15723v2 Announce Type: replace-cross 
Abstract: In this article, we review modern approaches to building interpretable models of property markets using machine learning on the base of mass valuation of property in the Primorye region, Russia. The researcher, lacking expertise in this topic, encounters numerous difficulties in the effort to build a good model. The main source of this is the huge difference between noisy real market data and ideal data which is very common in all types of tutorials on machine learning. This paper covers all stages of modeling: the collection of initial data, identification of outliers, the search and analysis of patterns in the data, the formation and final choice of price factors, the building of the model, and the evaluation of its efficiency. For each stage, we highlight potential issues and describe sound methods for overcoming emerging difficulties on actual examples. We show that the combination of classical linear regression with interpolation methods of geostatistics allows to build an effective model for land parcels. For flats, when many objects are attributed to one spatial point the application of geostatistical methods is difficult. Therefore we suggest linear regression with automatic generation and selection of additional rules on the base of decision trees, so called the RuleFit method. Thus we show, that despite such a strong restriction as the requirement of interpretability which is important in practical aspects, for example, legal matters, it is still possible to build effective models of real property markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15723v2</guid>
      <category>q-fin.ST</category>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Irina G. Tanashkina, Alexey S. Tanashkin, Alexander S. Maksimchuik, Anna Yu. Poshivailo</dc:creator>
    </item>
  </channel>
</rss>
