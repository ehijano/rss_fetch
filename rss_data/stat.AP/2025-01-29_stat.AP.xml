<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 Jan 2025 02:31:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Synchronized step Multilevel Markov chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2501.16538</link>
      <description>arXiv:2501.16538v1 Announce Type: new 
Abstract: In this work, we propose a new algorithm for coupling Markov chains in a Markovian multilevel Monte Carlo estimator. We apply this approach for solving Bayesian inverse problems that consist of multiple model fidelities. The coupling methodology, termed as synchronized step correlation enhancement (SYNCE), is inspired by the concept of using common random numbers in Markov chain Monte Carlo sampling. This methodology is shown to be more efficient and cost-effective than existing couplings in the literature. This improvement is achieved because SYNCE leads to higher correlation of samples obtained from level dependent posteriors than previous works. SYNCE is especially effective at coarse levels of the hierarchy where posteriors differ significantly from each other, resulting in orders of magnitude improvement in variance reduction. We first demonstrate the effectiveness of our proposed methodology by comparing it with existing algorithms on two simple examples taken from the literature. We then apply our methodology to a more complex example in the context of uncertainty quantification in subsurface flow simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16538v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjan Muchandimath, Alex Gorodetsky</dc:creator>
    </item>
    <item>
      <title>Estimating the Long-term Causal Effect of Redlining Policy on Air Pollution Exposure</title>
      <link>https://arxiv.org/abs/2501.16958</link>
      <description>arXiv:2501.16958v1 Announce Type: new 
Abstract: This study assesses the long-term effects of redlining policies (1935-1974) on present-day fine particulate matter (PM$_{2.5}$) and nitrogen dioxide (NO$_2$) air pollution levels. Redlining policies enacted in the 1930s, so there is very limited documentation of pre-treatment covariates. Consequently, traditional methods fails to sufficiently account for unmeasured confounders, potentially biasing causal interpretations. By integrating historical redlining data with 2010 PM$_{2.5}$ and NO$_2$ levels, our study aims to discern whether a causal link exists. Our study addresses challenges with a novel spatial and non-spatial latent factor framework, using the unemployment rate, house rent and percentage of Black population in 1940 U.S. Census as proxies to reconstruct pre-treatment latent socio-economic status. We establish identification of a causal effect under broad assumptions, and use Bayesian Markov Chain Monte Carlo to quantify uncertainty. We found strong evidence that historically redlined regions are exposed to higher NO$_2$ levels, with estimated effect of 0.38 ppb (95\% CI, 0.28 to 0.48). We did not observe a strong effect for PM$_{2.5}$, though we cannot dismiss the possibility that redlining once influenced PM$_{2.5}$ concentrations. Los Angeles, CA and Atlanta, GA show significant and the strongest evidence of harmful effects for both NO$_2$ and PM$_{2.5}$ among the cities studied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16958v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaodan Zhou, Shu Yang, Brian J Reich</dc:creator>
    </item>
    <item>
      <title>Detecting clinician implicit biases in diagnoses using proximal causal inference</title>
      <link>https://arxiv.org/abs/2501.16399</link>
      <description>arXiv:2501.16399v1 Announce Type: cross 
Abstract: Clinical decisions to treat and diagnose patients are affected by implicit biases formed by racism, ableism, sexism, and other stereotypes. These biases reflect broader systemic discrimination in healthcare and risk marginalizing already disadvantaged groups. Existing methods for measuring implicit biases require controlled randomized testing and only capture individual attitudes rather than outcomes. However, the "big-data" revolution has led to the availability of large observational medical datasets, like EHRs and biobanks, that provide the opportunity to investigate discrepancies in patient health outcomes. In this work, we propose a causal inference approach to detect the effect of clinician implicit biases on patient outcomes in large-scale medical data. Specifically, our method uses proximal mediation to disentangle pathway-specific effects of a patient's sociodemographic attribute on a clinician's diagnosis decision. We test our method on real-world data from the UK Biobank. Our work can serve as a tool that initiates conversation and brings awareness to unequal health outcomes caused by implicit biases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16399v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1142/9789819807024_0024</arxiv:DOI>
      <arxiv:journal_reference>Biocomputing 2025, pp. 330-345 (2024)</arxiv:journal_reference>
      <dc:creator>Kara Liu, Russ Altman, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>Ancestral Inference and Learning for Branching Processes in Random Environments</title>
      <link>https://arxiv.org/abs/2501.16526</link>
      <description>arXiv:2501.16526v1 Announce Type: cross 
Abstract: Ancestral inference for branching processes in random environments involves determining the ancestor distribution parameters using the population sizes of descendant generations. In this paper, we introduce a new methodology for ancestral inference utilizing the generalized method of moments. We demonstrate that the estimator's behavior is critically influenced by the coefficient of variation of the environment sequence. Furthermore, despite the process's evolution being heavily dependent on the offspring means of various generations, we show that the joint limiting distribution of the ancestor and offspring estimators of the mean, under appropriate centering and scaling, decouple and converge to independent Gaussian random variables when the ratio of the number of generations to the logarithm of the number of replicates converges to zero. Additionally, we provide estimators for the limiting variance and illustrate our findings through numerical experiments and data from Polymerase Chain Reaction experiments and COVID-19 data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16526v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoran Jiang, Anand N. Vidyashankar</dc:creator>
    </item>
    <item>
      <title>Quantifying Uncertainty and Variability in Machine Learning: Confidence Intervals for Quantiles in Performance Metric Distributions</title>
      <link>https://arxiv.org/abs/2501.16931</link>
      <description>arXiv:2501.16931v1 Announce Type: cross 
Abstract: Machine learning models are widely used in applications where reliability and robustness are critical. Model evaluation often relies on single-point estimates of performance metrics such as accuracy, F1 score, or mean squared error, that fail to capture the inherent variability in model performance. This variability arises from multiple sources, including train-test split, weights initialization, and hyperparameter tuning. Investigating the characteristics of performance metric distributions, rather than focusing on a single point only, is essential for informed decision-making during model selection and optimization, especially in high-stakes settings.
  How does the performance metric vary due to intrinsic uncertainty in the selected modeling approach? For example, train-test split is modified, initial weights for optimization are modified or hyperparameter tuning is done using an algorithm with probabilistic nature?
  This is shifting the focus from identifying a single best model to understanding a distribution of the performance metric that captures variability across different training conditions. By running multiple experiments with varied settings, empirical distributions of performance metrics can be generated. Analyzing these distributions can lead to more robust models that generalize well across diverse scenarios.
  This contribution explores the use of quantiles and confidence intervals to analyze such distributions, providing a more complete understanding of model performance and its uncertainty. Aimed at a statistically interested audience within the machine learning community, the suggested approaches are easy to implement and apply to various performance metrics for classification and regression problems. Given the often long training times in ML, particular attention is given to small sample sizes (in the order of 10-25).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16931v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christoph Lehmann, Yahor Paromau</dc:creator>
    </item>
    <item>
      <title>Rethinking the Win Ratio: A Causal Framework for Hierarchical Outcome Analysis</title>
      <link>https://arxiv.org/abs/2501.16933</link>
      <description>arXiv:2501.16933v1 Announce Type: cross 
Abstract: Quantifying causal effects in the presence of complex and multivariate outcomes is a key challenge to evaluate treatment effects. For \emph{hierarchical} multivarariates outcomes, the FDA recommends the Win Ratio and Generalized Pairwise Comparisons approaches \cite{Pocock2011winratio,Buyse2010}. However, as far as we know, these empirical methods lack causal or statistical foundations to justify their broader use in recent studies. To address this gap, we establish causal foundations for hierarchical comparison methods. We define related causal effect measures, and highlight that depending on the methodology used to compute Win Ratios or Net Benefits of treatments, the causal estimand targeted can be different, as proved by our consistency results. Quite dramatically, it appears that the causal estimand related to the historical estimation approach can yield reversed and incorrect treatment recommendations in heterogeneous populations, as we illustrate through striking examples. In order to compensate for this fallacy, we introduce a novel, individual-level yet identifiable causal effect measure that better approximates the ideal, non-identifiable individual-level estimand. We prove that computing Win Ratio or Net Benefits using a Nearest Neighbor pairing approach between treated and controlled patients, an approach that can be seen as an extreme form of stratification, leads to estimating this new causal estimand measure. We extend our methods to observational settings via propensity weighting, distributional regression to address the curse of dimensionality, and a doubly robust framework. We prove the consistency of our methods, and the double robustness of our augmented estimator. These methods are straightforward to implement, making them accessible to practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16933v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathieu Even, Julie Josse</dc:creator>
    </item>
    <item>
      <title>Seasonal Influenza Vaccination Hesitancy and Digital Literacy: Evidence from the European countries</title>
      <link>https://arxiv.org/abs/2501.17005</link>
      <description>arXiv:2501.17005v1 Announce Type: cross 
Abstract: This study documents the relationship between computer skills/digital literacy and influenza vaccination take-up among older adults in Europe during and after the COVID-19 pandemic. Using data from the Survey of Health, Aging and Retirement in Europe, we find a positive partial association between influenza vaccination take-up and two indicators of computer skills/digital literacy, self-assessed pre-pandemic computer skills and having used a computer at work in any pre-pandemic job. We do not estimate significant behavioural changes for individuals with better computer skills that may have been driven by spillover effects from the pandemic experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17005v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martina Celidoni, Nita Handastya, Guglielmo Weber, Nancy Zambon</dc:creator>
    </item>
    <item>
      <title>A Bayesian hierarchical mixture cure modelling framework to utilize multiple survival datasets for long-term survivorship estimates: A case study from previously untreated metastatic melanoma</title>
      <link>https://arxiv.org/abs/2401.13820</link>
      <description>arXiv:2401.13820v2 Announce Type: replace 
Abstract: Time to an event of interest over a lifetime is a central measure of the clinical benefit of an intervention used in a health technology assessment (HTA). Within the same trial, multiple end-points may also be considered. For example, overall and progression-free survival time for different drugs in oncology studies. A common challenge is when an intervention is only effective for some proportion of the population who are not clinically identifiable. Therefore, latent group membership as well as separate survival models for identified groups need to be estimated. However, follow-up in trials may be relatively short leading to substantial censoring. We present a general Bayesian hierarchical framework that can handle this complexity by exploiting the similarity of cure fractions between end-points; accounting for the correlation between them and improving the extrapolation beyond the observed data. Assuming exchangeability between cure fractions facilitates the borrowing of information between end-points. We undertake a comprehensive simulation study to evaluate the model performance under different scenarios. We also show the benefits of using our approach with a motivating example, the CheckMate 067 phase 3 trial consisting of patients with metastatic melanoma treated with first line therapy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13820v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Green, Murat Kurt, Andriy Moshyk, James Larkin, Gianluca Baio</dc:creator>
    </item>
    <item>
      <title>Fault detection in propulsion motors in the presence of concept drift</title>
      <link>https://arxiv.org/abs/2406.08030</link>
      <description>arXiv:2406.08030v3 Announce Type: replace 
Abstract: Machine learning and statistical methods can improve conventional motor protection systems, providing early warning and detection of emerging failures. Data-driven methods rely on historical data to learn how the system is expected to behave under normal circumstances. An unexpected change in the underlying system may cause a change in the statistical properties of the data, and by this alter the performance of the fault detection algorithm in terms of time to detection and false alarms. This kind of change, called \textit{concept drift}, requires adaptations to maintain constant performance. In this article, we present a machine learning approach for detecting overheating in the stator windings of marine electrical propulsion motors. Using simulated overheating faults injected into operational data, the methods are shown to provide early detection compared to conventional methods based on temperature readings and fixed limits. The proposed monitors are designed to operate for a type of concept drift observed in operational data collected from a specific class of motors in a fleet of ships. Using a mix of real and simulated concept drifts, it is shown that the proposed monitors are able to provide early detections during and after concept drifts, without the need for full model retraining.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08030v3</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Tveten, Morten Stakkeland</dc:creator>
    </item>
    <item>
      <title>The Effects of Air Pollution on Health: A Longitudinal Study of Los Angeles County Accounting for Measurement Error</title>
      <link>https://arxiv.org/abs/2410.01151</link>
      <description>arXiv:2410.01151v2 Announce Type: replace 
Abstract: This study develops a Bayesian hierarchical model to explore the effects of air pollution on respiratory and cardiovascular mortality in Los Angeles County. The model takes into account various pollutants such as PM2.5, PM10, CO, SO2, NO2 and O3, as well as a related meteorological factor: temperature. The objective is to identify the significant factors affecting selected health outcomes without including all variables in each model specification. This flexibility enables the model to capture key drivers of health risk without redundancy. To account for potential measurement error in pollution data due to imperfect monitoring or averaging, certain observed pollutant levels are treated as noise proxies for true exposure. By specifying priors for regression coefficients and measurement error parameters and estimating posterior distributions via Markov Chain Monte Carlo (MCMC) sampling, it leads to more precise and reliable estimates of the health risks associated with air pollution exposure in Los Angeles County by incorporating both the count nature of the health data and the uncertainties in pollution measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01151v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanfei Qu, David A. Stephens</dc:creator>
    </item>
    <item>
      <title>Validation of Satellite and Reanalysis Rainfall Products in Ghana and Zambia</title>
      <link>https://arxiv.org/abs/2501.14829</link>
      <description>arXiv:2501.14829v2 Announce Type: replace 
Abstract: Accurate rainfall data are crucial for effective climate services, especially in Sub-Saharan Africa, where agriculture heavily depends on rain-fed systems. However, the sparse distribution of rain-gauge networks in the region necessitates reliance on satellite and reanalysis rainfall products (REs) for rainfall estimation. This study evaluated the performance of eight REs -- CHIRPS, TAMSAT, CHIRP, ENACTS, ERA5, AgERA5, PERSIANN-CDR, and PERSIANN-CCS-CDR -- in Zambia and Ghana using a point-to-pixel validation approach. The analysis encompassed spatial consistency, annual rainfall summaries, seasonal patterns, and rainfall intensity detection across 38 ground stations. Results indicated that no single product performed optimally across all contexts, underscoring the need for application-specific recommendations. All products exhibited a high probability of detection (POD) for dry days in Zambia and northern Ghana (with 70% $&lt;$ POD $&lt;$ 100%, and 60% $&lt;$ POD $&lt;$ 85% respectively), suggesting their potential utility for drought-related studies in these areas. Conversely, all products showed limited skill in detecting heavy and violent rains (with POD close to 0%), rendering them unsuitable for extreme rainfall analysis (such as floods) in the current form. Products integrated with station data (ENACTS, CHIRPS, and TAMSAT) outperformed their counterparts under many contexts, highlighting the importance of calibration with local observations. Bias correction is strongly recommended, as varying levels of biases were evident across different rainfall summaries. A critical area for advancement is extreme rainfall detection. Future research should focus on this aspect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14829v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Bagiliko, David Stern, Denis Ndanguza, Francis Feehi Torgbor</dc:creator>
    </item>
    <item>
      <title>ForTune: Running Offline Scenarios to Estimate Impact on Business Metrics</title>
      <link>https://arxiv.org/abs/2403.00133</link>
      <description>arXiv:2403.00133v2 Announce Type: replace-cross 
Abstract: Making ideal decisions as a product leader in a web-facing company is extremely difficult. In addition to navigating the ambiguity of customer satisfaction and achieving business goals, one must also pave a path forward for ones' products and services to remain relevant, desirable, and profitable. Data and experimentation to test product hypotheses are key to informing product decisions. Online controlled experiments by A/B testing may provide the best data to support such decisions with high confidence, but can be time-consuming and expensive, especially when one wants to understand impact to key business metrics such as retention or long-term value. Offline experimentation allows one to rapidly iterate and test, but often cannot provide the same level of confidence, and cannot easily shine a light on impact on business metrics. We introduce a novel, lightweight, and flexible approach to investigating hypotheses, called scenario analysis, that aims to support product leaders' decisions using data about users and estimates of business metrics. Its strengths are that it can provide guidance on trade-offs that are incurred by growing or shifting consumption, estimate trends in long-term outcomes like retention and other important business metrics, and can generate hypotheses about relationships between metrics at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00133v2</guid>
      <category>cs.CE</category>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Georges Dupret, Konstantin Sozinov, Carmen Barcena Gonzalez, Ziggy Zacks, Amber Yuan, Benjamin Carterette, Manuel Mai, Shubham Bansal, Gwo Liang Leo Lien, Andrey Gatash, Roberto Sanchis Ojeda, Mounia Lalmas</dc:creator>
    </item>
    <item>
      <title>Differentially Private Boxplots</title>
      <link>https://arxiv.org/abs/2405.20415</link>
      <description>arXiv:2405.20415v3 Announce Type: replace-cross 
Abstract: Despite the potential of differentially private data visualization to harmonize data analysis and privacy, research in this area remains underdeveloped. Boxplots are a widely popular visualization used for summarizing a dataset and for comparison of multiple datasets. Consequentially, we introduce a differentially private boxplot. We evaluate its effectiveness for displaying location, scale, skewness and tails of a given empirical distribution. In our theoretical exposition, we show that the location and scale of the boxplot are estimated with optimal sample complexity, and the skewness and tails are estimated consistently, which is not always the case for a boxplot naively constructed from a single existing differentially private quantile algorithm. As a byproduct of this exposition, we introduce several new results concerning private quantile estimation. In simulations, we show that this boxplot performs similarly to a non-private boxplot, and it outperforms the naive boxplot. Additionally, we conduct a real data analysis of Airbnb listings, which shows that comparable analysis can be achieved through differentially private boxplot visualization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20415v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kelly Ramsay, Jairo Diaz-Rodriguez</dc:creator>
    </item>
  </channel>
</rss>
