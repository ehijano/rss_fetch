<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 15 Jan 2025 05:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Heuristics based on Adjacency Graph Packing for DCJ Distance Considering Intergenic Regions</title>
      <link>https://arxiv.org/abs/2501.07606</link>
      <description>arXiv:2501.07606v1 Announce Type: new 
Abstract: In this work, we explore heuristics for the Adjacency Graph Packing problem, which can be applied to the Double Cut and Join (DCJ) Distance Problem. The DCJ is a rearrangement operation and the distance problem considering it is a well established method for genome comparison. Our heuristics will use the structure called adjacency graph adapted to include information about intergenic regions, multiple copies of genes in the genomes, and multiple circular or linear chromosomes. The only required property from the genomes is that it must be possible to turn one into the other with DCJ operations. We propose one greedy heuristic and one heuristic based on Genetic Algorithms. Our experimental tests in artificial genomes show that the use of heuristics is capable of finding good results that are superior to a simpler random strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07606v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5753/bsb.2024.245554</arxiv:DOI>
      <dc:creator>Gabriel Siqueira, Alexsandro Oliveira Alexandrino, Andre Rodrigues Oliveira, Zanoni Dias</dc:creator>
    </item>
    <item>
      <title>Solar radiation and atmospheric CO$_2$ predict young leaf production in a moist evergreen tropical forest: Insights from 23 years</title>
      <link>https://arxiv.org/abs/2501.07620</link>
      <description>arXiv:2501.07620v1 Announce Type: new 
Abstract: Climate change impacts ecosystems worldwide, affecting animal behaviour and survival both directly and indirectly through changes such as the availability of food. For animals reliant on leaves as a primary food source, understanding how climate change influences leaf production of trees is crucial, yet this is understudied, especially in moist evergreen tropical forests. We analyzed a 23-year dataset of young leaf phenology from a moist tropical forest in Kibale National Park, Uganda, to examine seasonal and long-term patterns of 12 key tree species consumed by folivorous primates. We described phenological patterns and explored relationships between young leaf production of different tree species and climate variables. We also assessed the suitability of the Enhanced Vegetation Index (EVI) as a proxy for young leaf production in moist evergreen tropical forests. Our results showed that tree species exhibited distinct phenological patterns, with most species producing young leaves during two seasonal peaks aligned with the rainy seasons. Rainfall, cloud cover, and maximum temperature were the most informative predictors of seasonal variation in young leaf production. However, solar radiation and atmospheric CO$_2$ were most informative regarding long-term trends. EVI was strongly correlated with young leaf production within years but less effective for capturing inter-annual trends. These findings highlight the complex relationship between climate and young leaf phenology in moist evergreen tropical forests, and helps us understand the changes in food availability for tropical folivores.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07620v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura L\"uthy, Colin A. Chapman, Patrick Lauer, Patrick Omeja, Urs Kalbitzer</dc:creator>
    </item>
    <item>
      <title>Scaling Up ESM2 Architectures for Long Protein Sequences Analysis: Long and Quantized Approaches</title>
      <link>https://arxiv.org/abs/2501.07747</link>
      <description>arXiv:2501.07747v1 Announce Type: cross 
Abstract: Various approaches utilizing Transformer architectures have achieved state-of-the-art results in Natural Language Processing (NLP). Based on this success, numerous architectures have been proposed for other types of data, such as in biology, particularly for protein sequences. Notably among these are the ESM2 architectures, pre-trained on billions of proteins, which form the basis of various state-of-the-art approaches in the field. However, the ESM2 architectures have a limitation regarding input size, restricting it to 1,022 amino acids, which necessitates the use of preprocessing techniques to handle sequences longer than this limit. In this paper, we present the long and quantized versions of the ESM2 architectures, doubling the input size limit to 2,048 amino acids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07747v1</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5753/bsb.2024.244804</arxiv:DOI>
      <dc:creator>Gabriel Bianchin de Oliveira, Helio Pedrini, Zanoni Dias</dc:creator>
    </item>
    <item>
      <title>MD-Syn: Synergistic drug combination prediction based on the multidimensional feature fusion method and attention mechanisms</title>
      <link>https://arxiv.org/abs/2501.07884</link>
      <description>arXiv:2501.07884v1 Announce Type: cross 
Abstract: Drug combination therapies have shown promising therapeutic efficacy in complex diseases and have demonstrated the potential to reduce drug resistance. However, the huge number of possible drug combinations makes it difficult to screen them all in traditional experiments. In this study, we proposed MD-Syn, a computational framework, which is based on the multidimensional feature fusion method and multi-head attention mechanisms. Given drug pair-cell line triplets, MD-Syn considers one-dimensional and two-dimensional feature spaces simultaneously. It consists of a one-dimensional feature embedding module (1D-FEM), a two-dimensional feature embedding module (2D-FEM), and a deep neural network-based classifier for synergistic drug combination prediction. MD-Syn achieved the AUROC of 0.919 in 5-fold cross-validation, outperforming the state-of-the-art methods. Further, MD-Syn showed comparable results over two independent datasets. In addition, the multi-head attention mechanisms not only learn embeddings from different feature aspects but also focus on essential interactive feature elements, improving the interpretability of MD-Syn. In summary, MD-Syn is an interpretable framework to prioritize synergistic drug combination pairs with chemicals and cancer cell line gene expression profiles. To facilitate broader community access to this model, we have developed a web portal (https://labyeh104-2.life.nthu.edu.tw/) that enables customized predictions of drug combination synergy effects based on user-specified compounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07884v1</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>XinXin Ge, Yi-Ting Lee, Shan-Ju Yeh</dc:creator>
    </item>
    <item>
      <title>Early prediction of the transferability of bovine embryos from videomicroscopy</title>
      <link>https://arxiv.org/abs/2501.07945</link>
      <description>arXiv:2501.07945v1 Announce Type: cross 
Abstract: Videomicroscopy is a promising tool combined with machine learning for studying the early development of in vitro fertilized bovine embryos and assessing its transferability as soon as possible. We aim to predict the embryo transferability within four days at most, taking 2D time-lapse microscopy videos as input. We formulate this problem as a supervised binary classification problem for the classes transferable and not transferable. The challenges are three-fold: 1) poorly discriminating appearance and motion, 2) class ambiguity, 3) small amount of annotated data. We propose a 3D convolutional neural network involving three pathways, which makes it multi-scale in time and able to handle appearance and motion in different ways. For training, we retain the focal loss. Our model, named SFR, compares favorably to other methods. Experiments demonstrate its effectiveness and accuracy for our challenging biological task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07945v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>ICIP 2024 - IEEE International Conference on Image Processing, Oct 2024, Abu DHABI, United Arab Emirates</arxiv:journal_reference>
      <dc:creator>Yasmine Hachani (LACODAM), Patrick Bouthemy (SAIRPICO), Elisa Fromont (LACODAM), Sylvie Ruffini (UVSQ, INRAE), Ludivine Laffont (UVSQ, INRAE), Alline de Paula Reis (UVSQ, INRAE, ENVA)</dc:creator>
    </item>
    <item>
      <title>Avoiding subtraction and division of stochastic signals using normalizing flows: NFdeconvolve</title>
      <link>https://arxiv.org/abs/2501.08288</link>
      <description>arXiv:2501.08288v1 Announce Type: cross 
Abstract: Across the scientific realm, we find ourselves subtracting or dividing stochastic signals. For instance, consider a stochastic realization, $x$, generated from the addition or multiplication of two stochastic signals $a$ and $b$, namely $x=a+b$ or $x = ab$. For the $x=a+b$ example, $a$ can be fluorescence background and $b$ the signal of interest whose statistics are to be learned from the measured $x$. Similarly, when writing $x=ab$, $a$ can be thought of as the illumination intensity and $b$ the density of fluorescent molecules of interest. Yet dividing or subtracting stochastic signals amplifies noise, and we ask instead whether, using the statistics of $a$ and the measurement of $x$ as input, we can recover the statistics of $b$. Here, we show how normalizing flows can generate an approximation of the probability distribution over $b$, thereby avoiding subtraction or division altogether. This method is implemented in our software package, NFdeconvolve, available on GitHub with a tutorial linked in the main text.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08288v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>physics.data-an</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pedro Pessoa, Max Schweiger, Lance W. Q. Xu, Tristan Manha, Ayush Saurabh, Julian Antolin Camarena, Steve Press\'e</dc:creator>
    </item>
    <item>
      <title>TSEML: A task-specific embedding-based method for few-shot classification of cancer molecular subtypes</title>
      <link>https://arxiv.org/abs/2412.13228</link>
      <description>arXiv:2412.13228v3 Announce Type: replace 
Abstract: Molecular subtyping of cancer is recognized as a critical and challenging upstream task for personalized therapy. Existing deep learning methods have achieved significant performance in this domain when abundant data samples are available. However, the acquisition of densely labeled samples for cancer molecular subtypes remains a significant challenge for conventional data-intensive deep learning approaches. In this work, we focus on the few-shot molecular subtype prediction problem in heterogeneous and small cancer datasets, aiming to enhance precise diagnosis and personalized treatment. We first construct a new few-shot dataset for cancer molecular subtype classification and auxiliary cancer classification, named TCGA Few-Shot, from existing publicly available datasets. To effectively leverage the relevant knowledge from both tasks, we introduce a task-specific embedding-based meta-learning framework (TSEML). TSEML leverages the synergistic strengths of a model-agnostic meta-learning (MAML) approach and a prototypical network (ProtoNet) to capture diverse and fine-grained features. Comparative experiments conducted on the TCGA Few-Shot dataset demonstrate that our TSEML framework achieves superior performance in addressing the problem of few-shot molecular subtype classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13228v3</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/BIBM62325.2024.10821940</arxiv:DOI>
      <arxiv:journal_reference>2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</arxiv:journal_reference>
      <dc:creator>Ran Su, Rui Shi, Hui Cui, Ping Xuan, Chengyan Fang, Xikang Feng, Qiangguo Jin</dc:creator>
    </item>
    <item>
      <title>CREIMBO: Cross-Regional Ensemble Interactions in Multi-view Brain Observations</title>
      <link>https://arxiv.org/abs/2405.17395</link>
      <description>arXiv:2405.17395v2 Announce Type: replace-cross 
Abstract: Modern recordings of neural activity provide diverse observations of neurons across brain areas, conditions, and subjects; presenting an exciting opportunity to reveal the fundamentals of brain-wide dynamics. Current analysis methods often fail to harness the richness of such data, as they provide either uninterpretable representations or oversimplify models (e.g., by assuming stationary dynamics). Here, instead of regarding asynchronous neural recordings that lack alignment in neural identity or brain areas as a limitation, we leverage these diverse views into the brain to learn a unified model of neural dynamics. We assume that brain activity is driven by multiple hidden global sub-circuits. These sub-circuits represent global basis interactions between neural ensembles -- functional groups of neurons -- such that the time-varying decomposition of these circuits defines how the ensembles' interactions evolve over time non-stationarily. We discover the neural ensembles underlying non-simultaneous observations, along with their non-stationary evolving interactions, with our new model, CREIMBO. CREIMBO identifies the hidden composition of per-session neural ensembles through graph-driven dictionary learning and models the ensemble dynamics on a low-dimensional manifold spanned by a sparse time-varying composition of the global sub-circuits. Thus, CREIMBO disentangles overlapping temporal neural processes while preserving interpretability due to the use of a shared underlying sub-circuit basis. Moreover, CREIMBO distinguishes session-specific computations from global (session-invariant) ones by identifying session covariates and variations in sub-circuit activations. We demonstrate CREIMBO's ability to recover true components in synthetic data, and uncover meaningful brain dynamics including cross-subject neural mechanisms and inter- vs. intra-region dynamical motifs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17395v2</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noga Mudrik, Ryan Ly, Oliver Ruebel, Adam S. Charles</dc:creator>
    </item>
    <item>
      <title>AI Foundation Models for Wearable Movement Data in Mental Health Research</title>
      <link>https://arxiv.org/abs/2411.15240</link>
      <description>arXiv:2411.15240v3 Announce Type: replace-cross 
Abstract: Pretrained foundation models and transformer architectures have driven the success of large language models (LLMs) and other modern AI breakthroughs. However, similar advancements in health data modeling remain limited due to the need for innovative adaptations. Wearable movement data offers a valuable avenue for exploration, as it's a core feature in nearly all commercial smartwatches, well established in clinical and mental health research, and the sequential nature of the data shares similarities to language. We introduce the Pretrained Actigraphy Transformer (PAT), the first open source foundation model designed for time-series wearable movement data. Leveraging transformer-based architectures and novel techniques, such as patch embeddings, and pretraining on data from 29,307 participants in a national U.S. sample, PAT achieves state-of-the-art performance in several mental health prediction tasks. PAT is also lightweight and easily interpretable, making it a robust tool for mental health research.
  GitHub: https://github.com/njacobsonlab/Pretrained-Actigraphy-Transformer/</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15240v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Franklin Y. Ruan, Aiwei Zhang, Jenny Y. Oh, SouYoung Jin, Nicholas C. Jacobson</dc:creator>
    </item>
  </channel>
</rss>
