<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Feb 2026 05:00:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Market Games for Generative Models: Equilibria, Welfare, and Strategic Entry</title>
      <link>https://arxiv.org/abs/2602.17787</link>
      <description>arXiv:2602.17787v1 Announce Type: new 
Abstract: Generative model ecosystems increasingly operate as competitive multi-platform markets, where platforms strategically select models from a shared pool and users with heterogeneous preferences choose among them. Understanding how platforms interact, when market equilibria exist, how outcomes are shaped by model-providers, platforms, and user behavior, and how social welfare is affected is critical for fostering a beneficial market environment. In this paper, we formalize a three-layer model-platform-user market game and identify conditions for the existence of pure Nash equilibrium. Our analysis shows that market structure, whether platforms converge on similar models or differentiate by selecting distinct ones, depends not only on models' global average performance but also on their localized attraction to user groups. We further examine welfare outcomes and show that expanding the model pool does not necessarily increase user welfare or market diversity. Finally, we design novel best-response training schemes that allow model providers to strategically introduce new models into competitive markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17787v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiukun Wei, Min Shi, Xueru Zhang</dc:creator>
    </item>
    <item>
      <title>Robust Temporal Guarantees in Budgeted Sequential Auctions</title>
      <link>https://arxiv.org/abs/2602.17916</link>
      <description>arXiv:2602.17916v1 Announce Type: new 
Abstract: In modern advertising platforms, learning algorithms are deployed by budget-constrained bidders to maximize their accumulated value. These algorithms often offer classical utility guarantees like no-regret, i.e., the agent's utility is at least the utility achieved by some benchmark in which it is assumed that every other agent's bidding remains the same. These guarantees offer compelling properties: They are optimal against stationary competition distributions, and in unconstrained settings, the resulting empirical distribution of play induced by no-regret dynamics approximates a Coarse Correlated Equilibrium. However, no-regret algorithms are easily manipulable, and in budgeted settings, no stronger notion of regret (such as swap regret) is currently known that would limit such manipulation.
  We propose a very simple learning algorithm for budgeted sequential auctions where agents maximize their total number of wins and show that it has surprisingly appealing properties. We analyze this algorithm from two perspectives. First, we show that when an agent with a $\rho$ fraction of the total budget uses this algorithm, then she is guaranteed to win at least $\rho T - O(\sqrt T)$ of the total $T$ rounds. This result holds for adversarial behavior by the other agents, as long as they respect their own budget restrictions. Second, we examine the scenario when all the agents follow our algorithm. By the first result, every agent's total wins are proportional to her budget, up to the additive $O(\sqrt T)$ term. In addition, we show that this result holds in a much stronger sense: after an initial period of $O(\sqrt T \log T)$ rounds, every agent gets the same guarantee over any time interval. For intervals of length $O(\sqrt T)$, we show that the deviation from the desired number of wins is an additive constant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17916v1</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giannis Fikioris, Robert Kleinberg, Yoav Kolumbus, Yishay Mansour, Eva Tardos</dc:creator>
    </item>
    <item>
      <title>Pricing with a Hidden Sample</title>
      <link>https://arxiv.org/abs/2602.18038</link>
      <description>arXiv:2602.18038v1 Announce Type: new 
Abstract: We study prior-independent pricing for selling a single item to a single buyer when the seller observes only a single sample from the valuation distribution, while the buyer knows the distribution. Classical robust pricing approaches either rely on distributional statistics, which typically require many samples to estimate, or directly use revealed samples to determine prices and allocations. We show that these two regimes can be bridged by leveraging the buyer's informational advantage: pricing policies that conventionally require the seller to know statistics such as the mean, $L^\eta$-norm, or superquantile can, in our framework, be implemented using only a single hidden sample.
  We introduce hidden pricing mechanisms, in which the seller commits ex ante to a pricing rule based on a single sample that is revealed only after the buyer's participation decision. We show that every concave pricing policy can be implemented in this way. To evaluate performance guarantees, we develop a general reduction for analyzing monotone pricing policies over $\alpha$-regular distributions, enabling a tractable characterization of worst-case instances. Using this reduction, we characterize the optimal monotone hidden pricing mechanisms and compute their approximation ratios; in particular, we obtain an approximation ratio of approximately $0.79$ for monotone hazard rate (MHR) distributions. We further establish impossibility results for general concave pricing policies and for all prior-independent mechanisms. Finally, we show that our framework also applies to statistic-based robust pricing, thereby unifying sample-based and statistic-based approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18038v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhihao Gavin Tang, Yixin Tao, Shixin Wang</dc:creator>
    </item>
    <item>
      <title>Fair Orientations: Proportionality and Equitability</title>
      <link>https://arxiv.org/abs/2602.18098</link>
      <description>arXiv:2602.18098v1 Announce Type: new 
Abstract: We study the fair allocation of indivisible items under relevance constraints, where each agent has a set of relevant items and can only receive items that are relevant to them. While the relevance constraint has been studied in recent years, existing work has largely focused on envy-freeness. Our work extends this study to other key fairness criteria -- such as proportionality, equitability, and their relaxations -- in settings where the items may be goods, chores, or a mixture of both. We complement the literature by presenting a picture of the existence and computational complexity of the considered criteria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18098v1</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ankang Sun, Ruijie Wang, Bo Li</dc:creator>
    </item>
    <item>
      <title>MeDUET: Disentangled Unified Pretraining for 3D Medical Image Synthesis and Analysis</title>
      <link>https://arxiv.org/abs/2602.17901</link>
      <description>arXiv:2602.17901v1 Announce Type: cross 
Abstract: Self-supervised learning (SSL) and diffusion models have advanced representation learning and image synthesis. However, in 3D medical imaging, they remain separate: diffusion for synthesis, SSL for analysis. Unifying 3D medical image synthesis and analysis is intuitive yet challenging, as multi-center datasets exhibit dominant style shifts, while downstream tasks rely on anatomy, and site-specific style co-varies with anatomy across slices, making factors unreliable without explicit constraints. In this paper, we propose MeDUET, a 3D Medical image Disentangled UnifiEd PreTraining framework that performs SSL in the Variational Autoencoder (VAE) latent space which explicitly disentangles domain-invariant content from domain-specific style. The token demixing mechanism serves to turn disentanglement from a modeling assumption into an empirically identifiable property. Two novel proxy tasks, Mixed-Factor Token Distillation (MFTD) and Swap-invariance Quadruplet Contrast (SiQC), are devised to synergistically enhance disentanglement. Once pretrained, MeDUET is capable of (i) delivering higher fidelity, faster convergence, and improved controllability for synthesis, and (ii) demonstrating strong domain generalization and notable label efficiency for analysis across diverse medical benchmarks. In summary, MeDUET converts multi-source heterogeneity from an obstacle into a learning signal, enabling unified pretraining for 3D medical image synthesis and analysis. The code is available at https://github.com/JK-Liu7/MeDUET .</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17901v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.GT</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junkai Liu, Ling Shao, Le Zhang</dc:creator>
    </item>
    <item>
      <title>Optimal Competitive Ratio of Two-sided Online Bipartite Matching</title>
      <link>https://arxiv.org/abs/2602.18049</link>
      <description>arXiv:2602.18049v1 Announce Type: cross 
Abstract: We establish an optimal upper bound (negative result) of $\sim 0.526$ on the competitive ratio of the fractional version of online bipartite matching with two-sided vertex arrivals, matching the lower bound (positive result) achieved by Wang and Wong (ICALP 2015), and Tang and Zhang (EC 2024).</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18049v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhihao Gavin Tang</dc:creator>
    </item>
    <item>
      <title>The Complexity of Sparse Win-Lose Bimatrix Games</title>
      <link>https://arxiv.org/abs/2602.18380</link>
      <description>arXiv:2602.18380v1 Announce Type: cross 
Abstract: We prove that computing an $\epsilon$-approximate Nash equilibrium of a win-lose bimatrix game with constant sparsity is PPAD-hard for inverse-polynomial $\epsilon$. Our result holds for 3-sparse games, which is tight given that 2-sparse win-lose bimatrix games can be solved in polynomial time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18380v1</guid>
      <category>cs.CC</category>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eleni Batziou, John Fearnley, Abheek Ghosh, Rahul Savani</dc:creator>
    </item>
    <item>
      <title>Combatting Gerrymandering with Ranked Choice Voting: An experimental analysis of Multi-member Districts in the United States</title>
      <link>https://arxiv.org/abs/2107.07083</link>
      <description>arXiv:2107.07083v5 Announce Type: replace 
Abstract: Every representative democracy must specify a mechanism under which voters choose their representatives. The most common mechanism in the United States -- Winner takes all single-member districts -- both enables substantial partisan gerrymandering and constrains `fair' redistricting, preventing proportional representation in legislatures. We study the design of \textit{multi-member districts (MMDs)}, in which each district elects multiple representatives, potentially through a non-Winner takes all voting rule. We carry out large-scale empirical analyses for the U.S. House of Representatives under MMDs with different social choice functions, under algorithmically generated maps optimized for either partisan benefit or proportionality. Doing so requires efficiently incorporating predicted partisan outcomes -- under various multi-winner social choice functions -- into an algorithm that optimizes over an ensemble of maps. We find that with three-member districts using Single Transferable Vote, fairness-minded independent commissions would be able to achieve proportional outcomes in every state up to rounding, \textit{and} advantage-seeking partisans would have their power to gerrymander significantly curtailed. Simultaneously, such districts would preserve geographic cohesion. Through simulation, we find that the insights are robust to cross-party voting. In the process, we advance a rich research agenda at the intersection of social choice and computational gerrymandering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.07083v5</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikhil Garg, Wes Gurnee, David Rothschild, David Shmoys</dc:creator>
    </item>
    <item>
      <title>Ex-post Stability under Two-Sided Matching: Complexity and Characterization</title>
      <link>https://arxiv.org/abs/2411.14821</link>
      <description>arXiv:2411.14821v2 Announce Type: replace 
Abstract: A probabilistic approach to the stable matching problem has been identified as an important research area with several important open problems. When considering random matchings, ex-post stability is a fundamental stability concept. A prominent open problem is characterizing ex-post stability and establishing its computational complexity. We investigate the computational complexity of testing ex-post stability. Our central result is that when either side has ties in the preferences/priorities, testing ex-post stability is NP-complete. The result even holds if both sides have dichotomous preferences. On the positive side, we give an algorithm using an integer programming approach, that can determine a decomposition with a maximum probability of being weakly stable. We also consider stronger versions of ex-post stability (in particular robust ex-post stability and ex-post strong stability) and prove that they can be tested in polynomial time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14821v2</guid>
      <category>cs.GT</category>
      <category>cs.CC</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haris Aziz, Gergely Cs\'aji, P\'eter Bir\'o</dc:creator>
    </item>
    <item>
      <title>Fair Allocation of Improvements: When Old Endowments Shape New Assignments</title>
      <link>https://arxiv.org/abs/2504.16852</link>
      <description>arXiv:2504.16852v2 Announce Type: replace 
Abstract: This work is motivated by a common urban renewal process called Reconstruct and Divide. It involves the demolition of old buildings and the construction of new ones. Original homeowners are compensated with upgraded apartments, while surplus units are sold for profit, so theoretically it is a win-win project for all parties involved. However, many Reconstruct and Divide projects are withheld or delayed due to disagreements over the assignment of new apartments, claiming they are not fair. The goal of this research is to develop algorithms for envy-free assignment of the new apartments, possibly using monetary payments to reduce envy.
  In contrast to previous works on envy-free assignment, in our setting the envy depends also on the value of the old apartments, as people with more valuable old apartments expect to get more valuable new apartments. This presents two challenges.
  First, in some cases, no assignment and payment-vector satisfy the common fairness notions of envy-freeness and proportionality. Hence, we focus on minimizing the envy and the disproportionality (the distance between an agent's value and their proportional share). We present a strongly polynomial-time algorithm that, for a given assignment, finds a payment vector that minimizes the maximum pairwise-envy. We also present a strongly polynomial-time algorithm that computes an assignment and payment-vector that together minimize the maximum disproportionality.
  Second, directly asking the agents for their subjective valuations for their old apartments is infeasible, as it is a dominant strategy for them to report very high values for their old apartments. We introduce a novel method to elicit agents' valuations indirectly. Using this method, we identify conditions under which our Minimum Disproportionality algorithm is risk-averse truthful.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16852v2</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noga Klein Elmalem, Rica Gonen, Erel Segal-Halevi</dc:creator>
    </item>
    <item>
      <title>Stability in Online Assignment Games</title>
      <link>https://arxiv.org/abs/2510.09814</link>
      <description>arXiv:2510.09814v3 Announce Type: replace 
Abstract: The assignment game models a housing market where buyers and sellers are matched, and transaction prices are set so that the resulting allocation is stable. Shapley and Shubik showed that every stable allocation is necessarily built on a maximum social welfare matching. In practice, however, stable allocations are rarely attainable, as matchings are often sub-optimal, particularly in online settings where eagents arrive sequentially to the market. In this paper, we introduce and compare two complementary measures of instability for allocations with sub-optimal matchings, establish their connections to the optimality ratio of the underlying matching, and use this framework to study the stability performances of randomized algorithms in online assignment games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09814v3</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emile Martinez, Felipe Garrido-Lucero, Umberto Grandi</dc:creator>
    </item>
    <item>
      <title>Computing Perfect Bayesian Equilibria, with Application to Empirical Game-Theoretic Analysis</title>
      <link>https://arxiv.org/abs/2602.15233</link>
      <description>arXiv:2602.15233v3 Announce Type: replace 
Abstract: Perfect Bayesian Equilibrium (PBE) is a refinement of the Nash equilibrium for imperfect-information extensive-form games (EFGs) that enforces consistency between the two components of a solution: agents' strategy profile describing their decisions at information sets and the belief system quantifying their uncertainty over histories within an information set. We present a scalable approach for computing a PBE of an arbitrary two-player EFG. We adopt the definition of PBE enunciated by Bonanno in 2011 using a consistency concept based on the theory of belief revision due to Alchourr\'{o}n, G\"{a}rdenfors, and Makinson. Our algorithm for finding a PBE is an adaptation of Counterfactual Regret Minimization (CFR) that minimizes the expected regret at each information set given a belief system, while maintaining the necessary consistency criteria. We prove that our algorithm is correct for two-player zero-sum games and has a reasonable slowdown in time-complexity relative to classical CFR given the additional computation needed for refinement. We also experimentally demonstrate the competent performance of PBE-CFR in terms of equilibrium quality and running time on medium-to-large non-zero-sum EFGs. Finally, we investigate the effectiveness of using PBE for strategy exploration in empirical game-theoretic analysis. Specifically, we compute PBE as a meta-strategy solver (MSS) in a tree-exploiting variant of Policy Space Response Oracles (TE-PSRO). Our experiments show that PBE as an MSS leads to higher-quality empirical EFG models with complex imperfect information structures compared to MSSs based on an unrefined Nash equilibrium.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15233v3</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christine Konicki, Mithun Chakraborty, Michael P. Wellman</dc:creator>
    </item>
    <item>
      <title>A semi-parametric approach for estimating consumer valuation distributions using second price auctions</title>
      <link>https://arxiv.org/abs/2312.07882</link>
      <description>arXiv:2312.07882v3 Announce Type: replace-cross 
Abstract: We focus on online second price auctions, where bids are made sequentially, and the winning bidder pays the maximum of the second-highest bid and a seller specified starting price. For many such auctions, the seller does not see all the bids or the total number of bidders accessing the auction, and only observes the current selling prices throughout the course of the auction. We develop a novel semi-parametric approach to estimate the underlying consumer valuation distribution based on this data. Previous semi-parametric or non-parametric approaches in the literature only use the final selling price and assume knowledge of the total number of bidders. The resulting estimate, in particular, can be used by the seller to compute the optimal profit-maximizing price for the product. Our approach is free of tuning parameters, and we demonstrate its computational and statistical efficiency in a variety of simulation settings, and also on an Xbox 7-day auction dataset on eBay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07882v3</guid>
      <category>stat.ME</category>
      <category>cs.GT</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourav Mukherjee, Ziqian Yang, Rohit K Patra, Kshitij Khare</dc:creator>
    </item>
    <item>
      <title>Individual and Collective Welfare in Risk Sharing with Many States</title>
      <link>https://arxiv.org/abs/2401.07337</link>
      <description>arXiv:2401.07337v5 Announce Type: replace-cross 
Abstract: We study efficient risk sharing among risk-averse agents in an economy with a large, finite number of states. Following a random shock to an initial agreement, agents may renegotiate. If they require a minimal utility improvement to accept a new deal, we show the probability of finding a mutually acceptable allocation vanishes exponentially as the state space grows. This holds regardless of agents' degree of risk aversion. In a two-agent multiple-priors model, we find that the potential for Pareto-improving trade requires that at least one agent's set of priors has a vanishingly small measure. Our results hinge on the ``shape does not matter'' message of high-dimensional isoperimetric inequalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07337v5</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <pubDate>Mon, 23 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico Echenique, Farzad Pourbabaee</dc:creator>
    </item>
  </channel>
</rss>
