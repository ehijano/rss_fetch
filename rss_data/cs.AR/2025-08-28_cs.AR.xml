<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 29 Aug 2025 01:24:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>OmniSim: Simulating Hardware with C Speed and RTL Accuracy for High-Level Synthesis Designs</title>
      <link>https://arxiv.org/abs/2508.19299</link>
      <description>arXiv:2508.19299v1 Announce Type: new 
Abstract: High-Level Synthesis (HLS) is increasingly popular for hardware design using C/C++ instead of Register-Transfer Level (RTL). To express concurrent hardware behavior in a sequential language like C/C++, HLS tools introduce constructs such as infinite loops and dataflow modules connected by FIFOs. However, efficiently and accurately simulating these constructs at C level remains challenging. First, without hardware timing information, functional verification typically requires slow RTL synthesis and simulation, as the current approaches in commercial HLS tools. Second, cycle-accurate performance metrics, such as end-to-end latency, also rely on RTL simulation. No existing HLS tool fully overcomes the first limitation. For the second, prior work such as LightningSim partially improves simulation speed but lacks support for advanced dataflow features like cyclic dependencies and non-blocking FIFO accesses.
  To overcome both limitations, we propose OmniSim, a framework that significantly extends the simulation capabilities of both academic and commercial HLS tools. First, OmniSim enables fast and accurate simulation of complex dataflow designs, especially those explicitly declared unsupported by commercial tools. It does so through sophisticated software multi-threading, where threads are orchestrated by querying and updating a set of FIFO tables that explicitly record exact hardware timing of each FIFO access. Second, OmniSim achieves near-C simulation speed with near-RTL accuracy for both functionality and performance, via flexibly coupled and overlapped functionality and performance simulations.
  We demonstrate that OmniSim successfully simulates eleven designs previously unsupported by any HLS tool, achieving up to 35.9x speedup over traditional C/RTL co-simulation, and up to 6.61x speedup over the state-of-the-art yet less capable simulator, LightningSim, on its own benchmark suite.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19299v1</guid>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rishov Sarkar, Cong Hao</dc:creator>
    </item>
    <item>
      <title>GENIE-ASI: Generative Instruction and Executable Code for Analog Subcircuit Identification</title>
      <link>https://arxiv.org/abs/2508.19393</link>
      <description>arXiv:2508.19393v1 Announce Type: new 
Abstract: Analog subcircuit identification is a core task in analog design, essential for simulation, sizing, and layout. Traditional methods often require extensive human expertise, rule-based encoding, or large labeled datasets. To address these challenges, we propose GENIE-ASI, the first training-free, large language model (LLM)-based methodology for analog subcircuit identification. GENIE-ASI operates in two phases: it first uses in-context learning to derive natural language instructions from a few demonstration examples, then translates these into executable Python code to identify subcircuits in unseen SPICE netlists. In addition, to evaluate LLM-based approaches systematically, we introduce a new benchmark composed of operational amplifier netlists (op-amps) that cover a wide range of subcircuit variants. Experimental results on the proposed benchmark show that GENIE-ASI matches rule-based performance on simple structures (F1-score = 1.0), remains competitive on moderate abstractions (F1-score = 0.81), and shows potential even on complex subcircuits (F1-score = 0.31). These findings demonstrate that LLMs can serve as adaptable, general-purpose tools in analog design automation, opening new research directions for foundation model applications in analog design automation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19393v1</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Phuoc Pham, Arun Venkitaraman, Chia-Yu Hsieh, Andrea Bonetti, Stefan Uhlich, Markus Leibl, Simon Hofmann, Eisaku Ohbuchi, Lorenzo Servadei, Ulf Schlichtmann, Robert Wille</dc:creator>
    </item>
    <item>
      <title>RARO: Reliability-aware Conversion with Enhanced Read Performance for QLC SSDs</title>
      <link>https://arxiv.org/abs/2508.19530</link>
      <description>arXiv:2508.19530v1 Announce Type: new 
Abstract: Quad-level cell (QLC) flash offers significant benefits in cost and capacity, but its limited reliability leads to frequent read retries, which severely degrade read performance. A common strategy in high-density flash storage is to program selected blocks in a low-density mode (SLC), sacrificing some capacity to achieve higher I/O performance. This hybrid storage architecture has been widely adopted in consumer-grade storage systems. However, existing hybrid storage schemes typically focus on write performance and rely solely on data temperature for migration decisions. This often results in excessive mode switching, causing substantial capacity overhead.
  In this paper, we present RARO (Reliability-Aware Read performance Optimization), a hybrid flash management scheme designed to improve read performance with minimal capacity cost. The key insight behind RARO is that much of the read slowdown in QLC flash is caused by read retries. RARO triggers data migration only when hot data resides in QLC blocks experiencing a high number of read retries, significantly reducing unnecessary conversions and capacity loss. Moreover, RARO supports fine-grained multi-mode conversions (SLC-TLC-QLC) to further minimize capacity overhead. By leveraging real-time read retry statistics and flash characteristics, RARO mitigates over-conversion and optimizes I/O performance. Experiments on the FEMU platform demonstrate that RARO significantly improves read performance across diverse workloads, with negligible impact on usable capacity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19530v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanyun Wang, Dingcui Yu, Yina Lv, Yunpeng Song, Yumiao Zhao, Liang Shi</dc:creator>
    </item>
    <item>
      <title>Support Vector Machines Classification on Bendable RISC-V</title>
      <link>https://arxiv.org/abs/2508.19656</link>
      <description>arXiv:2508.19656v1 Announce Type: new 
Abstract: Flexible Electronics (FE) technology offers uniquecharacteristics in electronic manufacturing, providing ultra-low-cost, lightweight, and environmentally-friendly alternatives totraditional rigid electronics. These characteristics enable a rangeof applications that were previously constrained by the costand rigidity of conventional silicon technology. Machine learning (ML) is essential for enabling autonomous, real-time intelligenceon devices with smart sensing capabilities in everyday objects. However, the large feature sizes and high power consumption ofthe devices oppose a challenge in the realization of flexible ML applications. To address the above, we propose an open-source framework for developing ML co-processors for the Bendable RISC-V core. In addition, we present a custom ML accelerator architecture for Support Vector Machine (SVM), supporting both one-vs-one (OvO) and one-vs-rest (OvR) algorithms. Our ML accelerator adopts a generic, precision-scalable design, supporting 4-, 8-, and 16-bit weight representations. Experimental results demonstrate a 21x improvement in both inference execution time and energy efficiency, on average, highlighting its potential for low-power, flexible intelligence on the edge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19656v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Polykarpos Vergos, Theofanis Vergos, Florentia Afentaki, Konstantinos Balaskas, Georgios Zervakis</dc:creator>
    </item>
    <item>
      <title>New Tools, Programming Models, and System Support for Processing-in-Memory Architectures</title>
      <link>https://arxiv.org/abs/2508.19868</link>
      <description>arXiv:2508.19868v1 Announce Type: new 
Abstract: Our goal in this dissertation is to provide tools, programming models, and system support for PIM architectures (with a focus on DRAM-based solutions), to ease the adoption of PIM in current and future systems. To this end, we make at least four new major contributions.
  First, we introduce DAMOV, the first rigorous methodology to characterize memory-related data movement bottlenecks in modern workloads, and the first data movement benchmark suite. Second, we introduce MIMDRAM, a new hardware/software co-designed substrate that addresses the major current programmability and flexibility limitations of the bulk bitwise execution model of processing-using-DRAM (PUD) architectures. MIMDRAM enables the allocation and control of only the needed computing resources inside DRAM for PUD computing. Third, we introduce Proteus, the first hardware framework that addresses the high execution latency of bulk bitwise PUD operations in state-of-the-art PUD architectures by implementing a data-aware runtime engine for PUD. Proteus reduces the latency of PUD operations in three different ways: (i) Proteus concurrently executes independent in-DRAM primitives belong to a single PUD operation across DRAM arrays. (ii) Proteus dynamically reduces the bit-precision (and consequentially the latency and energy consumption) of PUD operations by exploiting narrow values (i.e., values with many leading zeros or ones). (iii) Proteus chooses and uses the most appropriate data representation and arithmetic algorithm implementation for a given PUD instruction transparently to the programmer. Fourth, we introduce DaPPA (data-parallel processing-in-memory architecture), a new programming framework that eases programmability for general-purpose PNM architectures by allowing the programmer to write efficient PIM-friendly code without the need to manage hardware resources explicitly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19868v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geraldo F. Oliveira</dc:creator>
    </item>
    <item>
      <title>When Routers, Switches and Interconnects Compute: A processing-in-interconnect Paradigm for Scalable Neuromorphic AI</title>
      <link>https://arxiv.org/abs/2508.19548</link>
      <description>arXiv:2508.19548v1 Announce Type: cross 
Abstract: Routing, switching, and the interconnect fabric are essential for large-scale neuromorphic computing. While this fabric only plays a supporting role in the process of computing, for large AI workloads it ultimately determines energy consumption and speed. In this paper, we address this bottleneck by asking: (a) What computing paradigms are inherent in existing routing, switching, and interconnect systems, and how can they be used to implement a processing-in-Interconnect (\pi^2) computing paradigm? and (b) leveraging current and future interconnect trends, how will a \pi^2 system's performance scale compared to other neuromorphic architectures? For (a), we show that operations required for typical AI workloads can be mapped onto delays, causality, time-outs, packet drop, and broadcast operations -- primitives already implemented in packet-switching and packet-routing hardware. We show that existing buffering and traffic-shaping embedded algorithms can be leveraged to implement neuron models and synaptic operations. Additionally, a knowledge-distillation framework can train and cross-map well-established neural network topologies onto $\pi^2$ without degrading generalization performance. For (b), analytical modeling shows that, unlike other neuromorphic platforms, the energy scaling of $\pi^2$ improves with interconnect bandwidth and energy efficiency. We predict that by leveraging trends in interconnect technology, a \pi^2 architecture can be more easily scaled to execute brain-scale AI inference workloads with power consumption levels in the range of hundreds of watts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19548v1</guid>
      <category>cs.NE</category>
      <category>cs.AR</category>
      <category>cs.NI</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Madhuvanthi Srivatsav R, Chiranjib Bhattacharyya, Shantanu Chakrabartty, Chetan Singh Thakur</dc:creator>
    </item>
    <item>
      <title>Demonstrator Testbed for Effective Precoding in MEO Multibeam Satellites</title>
      <link>https://arxiv.org/abs/2508.19657</link>
      <description>arXiv:2508.19657v1 Announce Type: cross 
Abstract: The use of communication satellites in medium Earth orbit (MEO) is foreseen to provide quasi-global broadband Internet connectivity in the coming networking ecosystems. Multi-user multiple-input single-output (MU-MISO) digital signal processing techniques, such as precoding, emerge as appealing technological enablers in the forward link of multi-beam satellite systems operating in full frequency reuse (FFR). However, the orbit dynamics of MEO satellites pose additional challenges that must be carefully evaluated and addressed. This work presents the design of an in-lab testbed based on software-defined radio (SDR) platforms and the corresponding adaptations required for efficient precoding in a MEO scenario. The setup incorporates a precise orbit model and the radiation pattern of a custom-designed direct radiating array (DRA). We analyze the main impairments affecting precoding performance, including Doppler shifts and payload phase noise, and propose a synchronization loop to mitigate these effects. Preliminary experimental results validate the feasibility and effectiveness of the proposed solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19657v1</guid>
      <category>eess.SP</category>
      <category>cs.AR</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jorge L. Gonz\'alez-Rios, Liz Mart\'inez Marrero, Juan Duncan, Luis M. Garc\'es-Socarr\'as, Raudel Cuiman Marquez, Juan A. V\'asquez Peralvo, Jevgenij Krivochiza, Symeon Chatzinotas, Bj\"orn Ottersten</dc:creator>
    </item>
    <item>
      <title>Exploration of Low-Power Flexible Stress Monitoring Classifiers for Conformal Wearables</title>
      <link>https://arxiv.org/abs/2508.19661</link>
      <description>arXiv:2508.19661v1 Announce Type: cross 
Abstract: Conventional stress monitoring relies on episodic, symptom-focused interventions, missing the need for continuous, accessible, and cost-efficient solutions. State-of-the-art approaches use rigid, silicon-based wearables, which, though capable of multitasking, are not optimized for lightweight, flexible wear, limiting their practicality for continuous monitoring. In contrast, flexible electronics (FE) offer flexibility and low manufacturing costs, enabling real-time stress monitoring circuits. However, implementing complex circuits like machine learning (ML) classifiers in FE is challenging due to integration and power constraints. Previous research has explored flexible biosensors and ADCs, but classifier design for stress detection remains underexplored. This work presents the first comprehensive design space exploration of low-power, flexible stress classifiers. We cover various ML classifiers, feature selection, and neural simplification algorithms, with over 1200 flexible classifiers. To optimize hardware efficiency, fully customized circuits with low-precision arithmetic are designed in each case. Our exploration provides insights into designing real-time stress classifiers that offer higher accuracy than current methods, while being low-cost, conformable, and ensuring low power and compact size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19661v1</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florentia Afentaki, Sri Sai Rakesh Nakkilla, Konstantinos Balaskas, Paula Carolina Lozano Duarte, Shiyi Jiang, Georgios Zervakis, Farshad Firouzi, Krishnendu Chakrabarty, Mehdi B. Tahoori</dc:creator>
    </item>
    <item>
      <title>Large Language Models (LLMs) for Electronic Design Automation (EDA)</title>
      <link>https://arxiv.org/abs/2508.20030</link>
      <description>arXiv:2508.20030v1 Announce Type: cross 
Abstract: With the growing complexity of modern integrated circuits, hardware engineers are required to devote more effort to the full design-to-manufacturing workflow. This workflow involves numerous iterations, making it both labor-intensive and error-prone. Therefore, there is an urgent demand for more efficient Electronic Design Automation (EDA) solutions to accelerate hardware development. Recently, large language models (LLMs) have shown remarkable advancements in contextual comprehension, logical reasoning, and generative capabilities. Since hardware designs and intermediate scripts can be represented as text, integrating LLM for EDA offers a promising opportunity to simplify and even automate the entire workflow. Accordingly, this paper provides a comprehensive overview of incorporating LLMs into EDA, with emphasis on their capabilities, limitations, and future opportunities. Three case studies, along with their outlook, are introduced to demonstrate the capabilities of LLMs in hardware design, testing, and optimization. Finally, future directions and challenges are highlighted to further explore the potential of LLMs in shaping the next-generation EDA, providing valuable insights for researchers interested in leveraging advanced AI technologies for EDA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20030v1</guid>
      <category>eess.SY</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kangwei Xu, Denis Schwachhofer, Jason Blocklove, Ilia Polian, Peter Domanski, Dirk Pfl\"uger, Siddharth Garg, Ramesh Karri, Ozgur Sinanoglu, Johann Knechtel, Zhuorui Zhao, Ulf Schlichtmann, Bing Li</dc:creator>
    </item>
    <item>
      <title>Re-thinking Memory-Bound Limitations in CGRAs</title>
      <link>https://arxiv.org/abs/2508.09570</link>
      <description>arXiv:2508.09570v2 Announce Type: replace 
Abstract: Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators commonly employed to boost performance in workloads with iterative structures. Existing research typically focuses on compiler or architecture optimizations aimed at improving CGRA performance, energy efficiency, flexibility, and area utilization, under the idealistic assumption that kernels can access all data from Scratchpad Memory (SPM). However, certain complex workloads-particularly in fields like graph analytics, irregular database operations, and specialized forms of high-performance computing (e.g., unstructured mesh simulations)-exhibit irregular memory access patterns that hinder CGRA utilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To address this challenge, we conduct a thorough analysis of the underlying causes of performance degradation, then propose a redesigned memory subsystem and refine the memory model. With both microarchitectural and theoretical optimization, our solution can effectively manage irregular memory accesses through CGRA-specific runahead execution mechanism and cache reconfiguration techniques. Our results demonstrate that we can achieve performance comparable to the original SPM-only system while requiring only 1.27% of the storage size. The runahead execution mechanism achieves an average 3.04x speedup (up to 6.91x), with cache reconfiguration technique providing an additional 6.02% improvement, significantly enhancing CGRA performance for irregular memory access patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09570v2</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3760386</arxiv:DOI>
      <arxiv:journal_reference>ACM Transactions on Embedded Computing Systems 2025</arxiv:journal_reference>
      <dc:creator>Xiangfeng Liu, Zhe Jiang, Anzhen Zhu, Xiaomeng Han, Mingsong Lyu, Qingxu Deng, Nan Guan</dc:creator>
    </item>
    <item>
      <title>Harnessing the Full Potential of RRAMs through Scalable and Distributed In-Memory Computing with Integrated Error Correction</title>
      <link>https://arxiv.org/abs/2508.13298</link>
      <description>arXiv:2508.13298v2 Announce Type: replace-cross 
Abstract: Exponential growth in global computing demand is exacerbated due to the higher-energy requirements of conventional architectures, primarily due to energy-intensive data movement. In-memory computing with Resistive Random Access Memory (RRAM) addresses this by co-integrating memory and processing, but faces significant hurdles related to device-level non-idealities and poor scalability for large computing tasks. Here, we introduce MELISO+ (In-Memory Linear Solver), a full-stack, distributed framework for energy-efficient in-memory computing. MELISO+ proposes a novel two-tier error correction mechanism to mitigate device non-idealities and develops a distributed RRAM computing framework to enable matrix computations exceeding dimensions of $65,000\times65,000$. This approach reduces first- and second-order arithmetic errors due to device non-idealities by over $90\%$, enhances energy efficiency by three to five orders of magnitude, and decreases latency 100-fold. Hence, MELISO+ allows lower-precision RRAM devices to outperform high-precision device alternatives in accuracy, energy and latency metrics. By unifying algorithm-hardware co-design with scalable architecture, MELISO+ significantly advances sustainable, high-dimensional computing suitable for applications like large language models and generative AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13298v2</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>cs.PF</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huynh Q. N. Vo, Md Tawsif Rahman Chowdhury, Paritosh Ramanan, Murat Yildirim, Gozde Tutuncuoglu</dc:creator>
    </item>
  </channel>
</rss>
