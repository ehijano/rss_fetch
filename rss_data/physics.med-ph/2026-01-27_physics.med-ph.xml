<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.med-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.med-ph</link>
    <description>physics.med-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.med-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 27 Jan 2026 05:00:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Benchmarking Deep Learning-Based Reconstruction Methods for Photoacoustic Computed Tomography with Clinically Relevant Synthetic Datasets</title>
      <link>https://arxiv.org/abs/2601.17165</link>
      <description>arXiv:2601.17165v1 Announce Type: new 
Abstract: Deep learning (DL)-based image reconstruction methods for photoacoustic computed tomography (PACT) have developed rapidly in recent years. However, most existing methods have not employed standardized datasets, and their evaluations rely on traditional image quality (IQ) metrics that may lack clinical relevance. The absence of a standardized framework for clinically meaningful IQ assessment hinders fair comparison and raises concerns about the reproducibility and reliability of reported advancements in PACT. A benchmarking framework is proposed that provides open-source, anatomically plausible synthetic datasets and evaluation strategies for DL-based acoustic inversion methods in PACT. The datasets each include over 11,000 two-dimensional (2D) stochastic breast objects with clinically relevant lesions and paired measurements at varying modeling complexity. The evaluation strategies incorporate both traditional and task-based IQ measures to assess fidelity and clinical utility. A preliminary benchmarking study is conducted to demonstrate the framework's utility by comparing DL-based and physics-based reconstruction methods. The benchmarking study demonstrated that the proposed framework enabled comprehensive, quantitative comparisons of reconstruction performance and revealed important limitations in certain DL-based methods. Although they performed well according to traditional IQ measures, they often failed to accurately recover lesions. This highlights the inadequacy of traditional metrics and motivates the need for task-based assessments. The proposed benchmarking framework enables systematic comparisons of DL-based acoustic inversion methods for 2D PACT. By integrating clinically relevant synthetic datasets with rigorous evaluation protocols, it enables reproducible, objective assessments and facilitates method development and system optimization in PACT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17165v1</guid>
      <category>physics.med-ph</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Panpan Chen, Seonyeong Park, Gangwon Jeong, Refik Mert Cam, Umberto Villa, Mark A. Anastasio</dc:creator>
    </item>
    <item>
      <title>Circularly polarized metamaterial cage for homogeneous signal-to-noise ratio enhancement in magnetic resonance imaging</title>
      <link>https://arxiv.org/abs/2601.17190</link>
      <description>arXiv:2601.17190v1 Announce Type: new 
Abstract: The signal-to-noise ratio (SNR) in magnetic resonance imaging (MRI) governs the quality of signal detection and directly impacts the clarity and reliability of the acquired images. Recent advances in metamaterials have enabled lightweight solutions with selective magnetic responses, offering a route to locally boost SNR in targeted anatomical regions but often with compromised field homogeneity. Here, a wireless metamaterial cage constructed from coaxial cables is engineered for homogeneous SNR enhancement at 3.0 T. With its cylindrical geometry and electromagnetic architecture, the device supports circularly polarized resonance through engineered phase-shifted currents, enabling selective and omnidirectional interaction with the rotating B_1^- field to achieve uniform magnetic field distribution. Integrated with the body coil, the device yields a 32-fold SNR enhancement while maintaining comparable homogeneity to the body coil alone, exhibiting only 12.07% variation within the region of interest (ROI). Benchmarking against a state-of-the-art 16-channel extremity coil further shows that the metacage achieves at least 1.94-fold and 2.24-fold higher SNR in axial and coronal planes, respectively, and exhibits substantially lower SNR variation (12.07% compared to 54.83% for the extremity coil). The results establish the metacage as a compelling platform for next-generation wireless MRI technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17190v1</guid>
      <category>physics.med-ph</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuhan Liu, Xia Zhu, Ke Wu, Stephan W. Anderson, Xin Zhang</dc:creator>
    </item>
    <item>
      <title>MlPET: A Localized Neural Network Approach for Probabilistic Post-Reconstruction PET Image Analysis Using Informed Priors</title>
      <link>https://arxiv.org/abs/2601.18021</link>
      <description>arXiv:2601.18021v1 Announce Type: new 
Abstract: We develop and evaluate MlPET, a fast localized machine learning approach for probabilistic PET image analysis addressing the noise-resolution trade-off in conventional reconstructions. MlPET replaces computationally demanding Markov chain Monte Carlo sampling with a localized neural network trained to estimate posterior mean voxel activity from small image neighborhoods. The method incorporates scanner-specific point spread functions, spatially correlated noise modeling, and flexible priors. Performance was evaluated on NEMA IEC phantom data from three PET systems (GE Discovery MI, Siemens Biograph Vision 600, and Quadra) under varying reconstruction settings and acquisition times. On phantom data, MlPET achieved contrast recovery coefficients consistently higher than standard PET and close to 1.0 (including 10 mm spheres), while reducing background noise and improving spatial definition. Effective pointspread function full width at half maximum decreased from approximately 2 mm in standard PET to below 1 mm with MlPET, a 2.5 fold reduction in blur. Comparable image quality was obtained at 40-80 s acquisition time with MlPET versus 900 s with conventional PET. MlPET provides an efficient approach for quantitative probabilistic post-reconstruction PET analysis. By combining informed priors with neural network speed, it achieves noise suppression and resolution enhancement without altering reconstruction algorithms. The method shows promise for improved small-lesion detectability and quantitative reliability in clinical PET imaging. Future studies will evaluate performance on patient data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18021v1</guid>
      <category>physics.med-ph</category>
      <category>cs.LG</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Mejer Hansen, Nana Christensen, Mikkel Vendelbo</dc:creator>
    </item>
    <item>
      <title>Automated HER2 scoring with uncertainty quantification using lensfree holography and deep learning</title>
      <link>https://arxiv.org/abs/2601.18219</link>
      <description>arXiv:2601.18219v1 Announce Type: new 
Abstract: Accurate assessment of human epidermal growth factor receptor 2 (HER2) expression is critical for breast cancer diagnosis, prognosis, and therapy selection; yet, most existing digital HER2 scoring methods rely on bulky and expensive optical systems. Here, we present a compact and cost-effective lensfree holography platform integrated with deep learning for automated HER2 scoring of immunohistochemically stained breast tissue sections. The system captures lensfree diffraction patterns of stained HER2 tissue sections under RGB laser illumination and acquires complex field information over a sample area of ~1,250 mm^2 at an effective throughput of ~84 mm^2 per minute. To enhance diagnostic reliability, we incorporated an uncertainty quantification strategy based on Bayesian Monte Carlo dropout, which provides autonomous uncertainty estimates for each prediction and supports reliable, robust HER2 scoring, with an overall correction rate of 30.4%. Using a blinded test set of 412 unique tissue samples, our approach achieved a testing accuracy of 84.9% for 4-class (0, 1+, 2+, 3+) HER2 classification and 94.8% for binary (0/1+ vs. 2+/3+) HER2 scoring with uncertainty quantification. Overall, this lensfree holography approach provides a practical pathway toward portable, high-throughput, and cost-effective HER2 scoring, particularly suited for resource-limited settings, where traditional digital pathology infrastructure is unavailable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18219v1</guid>
      <category>physics.med-ph</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Che-Yung Shen, Xilin Yang, Yuzhu Li, Leon Lenk, Aydogan Ozcan</dc:creator>
    </item>
    <item>
      <title>Fully 3D Unrolled Magnetic Resonance Fingerprinting Reconstruction via Staged Pretraining and Implicit Gridding</title>
      <link>https://arxiv.org/abs/2601.17143</link>
      <description>arXiv:2601.17143v1 Announce Type: cross 
Abstract: Magnetic Resonance Fingerprinting (MRF) enables fast quantitative imaging, yet reconstructing high-resolution 3D data remains computationally demanding. Non-Cartesian reconstructions require repeated non-uniform FFTs, and the commonly used Locally Low Rank (LLR) prior adds computational overhead and becomes insufficient at high accelerations. Learned 3D priors could address these limitations, but training them at scale is challenging due to memory and runtime demands. We propose SPUR-iG, a fully 3D deep unrolled subspace reconstruction framework that integrates efficient data consistency with a progressive training strategy. Data consistency leverages implicit GROG, which grids non-Cartesian data onto a Cartesian grid with an implicitly learned kernel, enabling FFT-based updates with minimal artifacts. Training proceeds in three stages: (1) pretraining a denoiser with extensive data augmentation, (2) greedy per-iteration unrolled training, and (3) final fine-tuning with gradient checkpointing. Together, these stages make large-scale 3D unrolled learning feasible within a reasonable compute budget. On a large in vivo dataset with retrospective undersampling, SPUR-iG improves subspace coefficient maps quality and quantitative accuracy at 1-mm isotropic resolution compared with LLR and a hybrid 2D/3D unrolled baseline. Whole-brain reconstructions complete in under 15-seconds, with up to $\times$111 speedup for 2-minute acquisitions. Notably, $T_1$ maps with our method from 30-second scans achieve accuracy on par with or exceeding LLR reconstructions from 2-minute scans. Overall, the framework improves both accuracy and speed in large-scale 3D MRF reconstruction, enabling efficient and reliable accelerated quantitative imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17143v1</guid>
      <category>eess.IV</category>
      <category>physics.med-ph</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonatan Urman, Mark Nishimura, Daniel Abraham, Xiaozhi Cao, Kawin Setsompop</dc:creator>
    </item>
    <item>
      <title>Polyurethane-Based Scintillators for Neutron and Gamma Radiation Detection in Medical and Industrial Applications</title>
      <link>https://arxiv.org/abs/2601.17463</link>
      <description>arXiv:2601.17463v1 Announce Type: cross 
Abstract: Organic scintillators using a solid polyurethane (PU) matrix have been introduced to combine the robustness of a construction material with scintillating properties that allow gamma rays and fast neutrons to be detected efficiently and at low cost. This work compares two corresponding materials, the older M600 and the more recent M700, with EJ-276D and EJ-200 representing common plastic scintillators with and without pulse-shape discrimination (PSD) capabilities, respectively. Characterization measurements were performed with small samples of 26 mm diameter and 10 mm height, which were coupled to a photomultiplier tube (PMT) and simultaneously exposed to 252Cf fission neutrons and 137Cs gamma rays. M700 turned out to provide the best PSD performance and about the same light yield as EJ-276D, while its light pulses exhibit a shorter pulse decay. An accelerated ageing process applied in between two test campaigns was too short to trigger distinct performance degradation in any of the materials, though optical degradation was visible in EJ-276D and in EJ-200 but not in the PU-based materials. Nevertheless, the extremely robust polyurethane matrix promises advantages in medical and industrial applications where resilience and long-term stability are of crucial importance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17463v1</guid>
      <category>physics.ins-det</category>
      <category>nucl-ex</category>
      <category>physics.app-ph</category>
      <category>physics.med-ph</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Olga Maiatska, Torsten D\"unnebacke, Martin Kreuels, Guntram Pausch, Falko Scherwinski, J\"urgen R. Stein</dc:creator>
    </item>
    <item>
      <title>Multi-Criteria Inverse Robustness in Radiotherapy Planning Using Semidefinite Programming</title>
      <link>https://arxiv.org/abs/2601.17750</link>
      <description>arXiv:2601.17750v1 Announce Type: cross 
Abstract: Radiotherapy planning naturally leads to a multi-criteria optimization problem which is subject to different sources of uncertainty. In order to find the desired treatment plan, a decision maker must balance these objectives as well as the level of robustness towards uncertainty against each other. This paper showcases a quantitative approach to do so, which combines the theoretical model with the ability to deal with practical challenges. To this end, the uncertainty, which can be expressed via the so-called dose-influence matrix, is modelled using interval matrices. We use inverse robustness to introduce an additional objective, which aims to maximize the volume of the uncertainty set. A multi-criteria approach allows to handle the uncertainty while keeping appropriate values of the other objective functions. We solve the resulting quadratically constrained quadratic optimization problem (QCQP) by first relaxing it to a convex semidefinite problem (SDP) and then reconstructing optimal solutions of the QCQP from solutions of the SDP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17750v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.med-ph</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Schr\"oeder, Yair Censor, Philipp S\"uss, Karl-Heinz K\"ufer</dc:creator>
    </item>
    <item>
      <title>Feedback-Based Quantum Control for Safe and Synergistic Drug Combination Design</title>
      <link>https://arxiv.org/abs/2601.18082</link>
      <description>arXiv:2601.18082v1 Announce Type: cross 
Abstract: Drug-drug interactions (DDIs) strongly affect the safety and efficacy of combination therapies. Despite the availability of large DDI databases, selecting optimal multi-drug combinations that balance safety, therapeutic benefit, and regimen size remains a challenging combinatorial optimization problem. Here, we present a quantum-control-based framework for DDI-aware drug combination optimization, in which known harmful and synergistic interactions are encoded into Ising Hamiltonians as penalties and rewards, respectively. The optimization is performed using the feedback-based quantum algorithm FALQON, a gradient-free variational approach. We study two clinically motivated tasks: the Maximum Safe Subset problem and the Synergy-Constrained Optimization problem. Numerical simulations using interaction data from Drugs.com and SYNERGxDB demonstrate efficient convergence and high-quality solutions for clinically relevant drug sets, including COVID-19 case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18082v1</guid>
      <category>quant-ph</category>
      <category>physics.chem-ph</category>
      <category>physics.comp-ph</category>
      <category>physics.med-ph</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mai Nguyen Phuong Nhi, Lan Nguyen Tran, Le Bin Ho</dc:creator>
    </item>
    <item>
      <title>SMURF: Scalable method for unsupervised reconstruction of flow in 4D flow MRI</title>
      <link>https://arxiv.org/abs/2505.12494</link>
      <description>arXiv:2505.12494v3 Announce Type: replace 
Abstract: We introduce SMURF, a scalable and unsupervised machine learning method for simultaneously segmenting vascular geometries and reconstructing velocity fields from 4D flow MRI data. SMURF models geometry and velocity fields using multilayer perceptron-based functions incorporating Fourier feature embeddings and random weight factorization to accelerate convergence. A measurement model connects these fields to the observed image magnitude and phase data. Maximum likelihood estimation and subsampling enable SMURF to process high-dimensional datasets efficiently. Evaluations on synthetic, in vitro, and in vivo datasets demonstrate SMURF's performance. On synthetic internal carotid artery aneurysm data derived from CFD, SMURF achieves a quarter-voxel segmentation accuracy across noise levels of up to 50%, outperforming the state-of-the-art segmentation method by up to double the accuracy. In an in vitro experiment on Poiseuille flow, SMURF reduces velocity reconstruction RMSE by approximately 34% compared to raw measurements. In in vivo internal carotid artery aneurysm data, SMURF attains nearly half-voxel segmentation accuracy relative to expert annotations and decreases median velocity divergence residuals by about 31%, with a 27% reduction in the interquartile range. These results indicate that SMURF is robust to noise, preserves flow structure, and identifies patient-specific morphological features. SMURF advances 4D flow MRI accuracy, potentially enhancing the diagnostic utility of 4D flow MRI in clinical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12494v3</guid>
      <category>physics.med-ph</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Atharva Hans, Abhishek Singh, Pavlos Vlachos, Ilias Bilionis</dc:creator>
    </item>
    <item>
      <title>Clinical test cases for commissioning, QA, and benchmarking of model-based dose calculation algorithms in 192Ir HDR gynecologic tandem and ring brachytherapy</title>
      <link>https://arxiv.org/abs/2507.05144</link>
      <description>arXiv:2507.05144v2 Announce Type: replace 
Abstract: Purpose: To develop clinically relevant test cases for commissioning Model-Based Dose Calculation Algorithms (MBDCAs) for 192Ir High Dose Rate (HDR) gynecologic brachytherapy following the workflow proposed by the TG-186 report and the WGDCAB report 372. Acquisition and Validation Methods: Two cervical cancer intracavitary HDR brachytherapy patient models were created, using either uniformly structured regions or realistic segmentation. The computed tomography (CT) images of the models were converted to DICOM CT images via MATLAB and imported into two Treatment Planning Systems (TPSs) with MBDCA capability. The clinical segmentation was expanded to include additional organs at risk. The actual clinical treatment plan was generally maintained, with the source replaced by a generic 192Ir HDR source. Dose to medium in medium calculations were performed using the MBDCA option of each TPS, and three different Monte Carlo (MC) simulation codes. MC results agreed within statistical uncertainty, while comparisons between MBDCA and MC dose distributions highlighted both strengths and limitations of the studied MBDCAs, suggesting potential approaches to overcome the challenges. Data Format and Usage Notes: The datasets for the developed cases are available online at http://doi.org/ 10.5281/zenodo.15720996. The DICOM files include the treatment plan for each case, TPS, and the corresponding reference MC dose data. The package also contains a TPS- and case-specific user guide for commissioning the MBDCAs, and files needed to replicate the MC simulations. Potential Applications: The provided datasets and proposed methodology offer a commissioning framework for TPSs using MBDCAs, and serve as a benchmark for brachytherapy researchers using MC methods. They also facilitate intercomparisons of MBDCA performance and provide a quality assurance resource for evaluating future TPS software updates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05144v2</guid>
      <category>physics.med-ph</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>V. Peppa, M. Robitaille, F. Akbari, S. A. Enger, R. M. Thomson, F. Mourtada, G. P. Fonseca</dc:creator>
    </item>
    <item>
      <title>An AI dose engine for fast carbon ion treatment planning</title>
      <link>https://arxiv.org/abs/2510.11271</link>
      <description>arXiv:2510.11271v2 Announce Type: replace 
Abstract: Monte Carlo (MC) simulations provide gold-standard accuracy for carbon ion therapy dose calculations but are computationally intensive. Analytical pencil beam algorithms offer speed but reduced accuracy in heterogeneous tissues. We developed the first AI-based dose engine capable of predicting absorbed dose, the alpha and beta parameters for relative biological effectiveness (RBE)- weighted optimisation in carbon ion therapy, delivering MC-level accuracy with drastically reduced computation time. We extended the transformer-based DoTA model to predict absorbed dose (C-DoTA-d), alpha (C-DoTA-alpha), and beta (C-DoTA-beta), introducing a cross-attention mechanism for alpha and beta to combine dose and energy inputs. The training dataset consisted of ~70,000 pencil beams from 187 head-and-neck patients, with ground-truth values obtained using the GPU-accelerated MC toolkit FRED. Performance was evaluated on an independent test set using gamma pass rate (1%/1 mm), depth-dose, and isodose contour Dice coefficients. MC dropout-based uncertainty analysis was performed. Median gamma pass rates exceeded 98% for all predictions (99.76% for dose, 99.14% for alpha, and 98.74% for beta), with minima above 85% in the most heterogeneous anatomies. The Dice coefficient was 0.95 for 1% isodose contours, with slightly reduced agreement in high-gradient regions. Compared to MC FRED, inference was over 400x faster (0.032 s vs. 14 s per pencil beam) while maintaining accuracy. Uncertainty analysis showed high stability, with mean standard deviations below 0.5% for all models. C-DoTA achieves MC-quality predictions of absorbed dose and RBE model parameters in ~30 milliseconds per beam. Its speed and accuracy support online adaptive planning, paving the way for more effective carbon ion therapy workflows. Future work will expand to additional anatomical sites, beam geometries, and clinical beamlines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11271v2</guid>
      <category>physics.med-ph</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasiia Quarz, Angelica De Gregorio, Gaia Franciosini, Angelo Schiavi, Zolt\'an Perk\'o, Lennart Volz, Christoph Hoog Antink, Vincenzo Patera, Marco Durante, Christian Graeff</dc:creator>
    </item>
    <item>
      <title>Predictive Dosimetry in PSMA-Targeted Radiopharmaceutical Therapies: A PBPK Modeling and Machine Learning Study</title>
      <link>https://arxiv.org/abs/2510.21054</link>
      <description>arXiv:2510.21054v2 Announce Type: replace 
Abstract: Predictive dosimetry is central to enabling personalized radiopharmaceutical therapy (RPT), particularly in prostate specific membrane antigen (PSMA) targeted theranostics. In this work, we develop a three layer computational framework that integrates physiologically based pharmacokinetic (PBPK) modeling with machine learning (ML) to predict both physical (AUC, absorbed dose) and biological (BED, EQD2) dosimetric endpoints in tumors and major organs. In the first layer, we generated 640 virtual patients using PBPK simulations of F-18, Ga-68, and Cu-64 labeled PSMA PET tracers paired with Lu-177 PSMA therapy, producing 15360 tumor and organ time activity curves (TACs) under realistic biological variability and PET-like noise. In the second layer, TACs were transformed into quantitative kinetic features and mapped to physical and biological dose metrics. In the third layer, ML models (Random Forest, Extra Trees, Ridge, Gradient Boosting, and XGBoost) were trained to predict RPT doses from PET derived features, with performance evaluated using mean absolute percentage error (MAPE) and R2. Cu-64 PSMA-617 based PET yielded the most robust predictions, achieving tumor dose MAPE as low as 8 percent and 10 to 20 percent for normal organs, while F-18 DCFPyL showed volume dependent performance and Ga-68 PSMA-11 exhibited higher variability. SHAP analysis revealed that peak uptake, clearance, and early kinetic features dominated predictive performance across organs and endpoints. This PBPK ML framework enables scalable, physiology informed predictive dosimetry and provides a foundation for trial design and patient specific treatment planning in PSMA targeted RPT. These results demonstrate that pre therapy PET can serve as a reliable surrogate for post therapy dosimetry, enabling scalable personalization of PSMA targeted RPT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21054v2</guid>
      <category>physics.med-ph</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hamid Abdollahi (Department of Radiology, University of British Columbia, Vancouver, Canada, Department of Basic and Translation Research, BC Cancer Research institute, Vancouver, Canada), James Fowler (Department of Basic and Translation Research, BC Cancer Research institute, Vancouver, Canada, Department of Physics &amp; Astronomy, University of British Columbia, Vancouver, Canada), Carlos Uribe (Department of Basic and Translation Research, BC Cancer Research institute, Vancouver, Canada, Department of Molecular Imaging and Therapy, BC Cancer, Vancouver, Canada), Arman Rahmim (Department of Radiology, University of British Columbia, Vancouver, Canada, Department of Basic and Translation Research, BC Cancer Research institute, Vancouver, Canada, Department of Physics &amp; Astronomy, University of British Columbia, Vancouver, Canada)</dc:creator>
    </item>
    <item>
      <title>Radiation Shielding Performance of Different Concrete Materials: A Systematic Review</title>
      <link>https://arxiv.org/abs/2601.10336</link>
      <description>arXiv:2601.10336v2 Announce Type: replace 
Abstract: Background: Concrete is one of the most-used material today in nuclear, medical, and industrial applications for radiation shielding due to its economic advantages and availability together with its structural performance. However, differences in the use of aggregates, density, and other additives affect radiation attenuation efficiency. It is therefore necessary to understand and compare shielding properties of various concrete formulations for the optimization of safety and performance in radiation-prone environments. Methods: Using Preferred Reporting Items for Systematic reviews and Meta-Analyses (PRISMA) guidelines, a literature search was performed across PubMed, Scopus, ScienceDirect, and Google Scholar. 17 peer-reviewed studies published between 2010 and 2025 were analysed systematically. Data was extracted based on material composition, density, radiation type, energy range, attenuation coefficients, and shielding efficiency. The obtained results were compared to find the trend in performance and optimization of the considered materials. Conclusion: Radiation shielding efficiency of concrete is dependent on its density, microstructural characteristics and type of aggregate. For superior performance in a mixed radiation field, Heavy and boron-rich additives can be added. Newly developed UHPCs and nano-engineered concretes are lightweight, durable, and environmentally friendly options for shielding materials compared to traditional ones. Further studies are needed to focus on the standardization of test methods, validation of long-term stability, and coupling computational modelling with experimental data in order to guide material design for applications featuring enhanced radiation shielding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10336v2</guid>
      <category>physics.med-ph</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christiana Subaar, Ziem Samuel Aanoneda, Sylivia Boateng, Emmanuella Konadu Amaniampong, Philimon Adjei</dc:creator>
    </item>
    <item>
      <title>Immunity to Increasing Condition Numbers of Linear Superiorization versus Linear Programming</title>
      <link>https://arxiv.org/abs/2407.18709</link>
      <description>arXiv:2407.18709v2 Announce Type: replace-cross 
Abstract: Given a family of linear constraints and a linear objective function one can consider whether to apply a Linear Programming (LP) algorithm or use a Linear Superiorization (LinSup) algorithm on this data. In the LP methodology one aims at finding an optimal point, i.e., a point that fulfills the constraints and has the minimal value of the objective function over these constraints. The Linear Superiorization approach considers the same data as linear programming problems but instead of attempting to solve those with linear programming methods it employs perturbation resilient feasibility-seeking algorithms and steers them toward a feasible point with reduced (not necessarily minimal) objective function value. This aim of the superiorization method (SM) is less demanding than aiming to reach full-fledged constrained optimality and it places more importance on reaching feasibility than on reaching optimality. Previous studies (e. g. [12]) compared LP and LinSup in terms of their respective outputs and the resources they use. This paper is a follow-up analysis of [12], where we investigate classical LP approaches and LinSup in terms of their sensitivity to condition numbers of the system of linear constraints. Condition numbers are a measure for the impact of deviations in the input data on the output of a problem and, in particular, they describe the factor of error propagation when given wrong or erroneous data. Therefore, the ability of LP and LinSup to cope with increased condition numbers, thus with ill-posed problems, is an important matter to consider which was not studied until now. We investigate experimentally the advantages and disadvantages of both LP and LinSup on exemplary problems of linear programming with multiple condition numbers and different problem dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18709v2</guid>
      <category>math.OC</category>
      <category>physics.med-ph</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Schr\"oder, Yair Censor, Philipp S\"uss, Karl-Heinz K\"ufer</dc:creator>
    </item>
    <item>
      <title>Harmonization in Magnetic Resonance Imaging: A Survey of Acquisition, Image-level, and Feature-level Methods</title>
      <link>https://arxiv.org/abs/2507.16962</link>
      <description>arXiv:2507.16962v2 Announce Type: replace-cross 
Abstract: Magnetic resonance imaging (MRI) has greatly advanced neuroscience research and clinical diagnostics. However, imaging data collected across different scanners, acquisition protocols, or imaging sites often exhibit substantial heterogeneity, known as batch effects or site effects. These non-biological sources of variability can obscure true biological signals, reduce reproducibility and statistical power, and severely impair the generalizability of learning-based models across datasets. Image harmonization is grounded in the central hypothesis that site-related biases can be eliminated or mitigated while preserving meaningful biological information, thereby improving data comparability and consistency. This review provides a comprehensive overview of key concepts, methodological advances, publicly available datasets, and evaluation metrics in the field of MRI harmonization. We systematically cover the full imaging pipeline and categorize harmonization approaches into prospective acquisition and reconstruction, retrospective image-level and feature-level methods, and traveling-subject-based techniques. By synthesizing existing methods and evidence, we revisit the central hypothesis of image harmonization and show that, although site invariance can be achieved with current techniques, further evaluation is required to verify the preservation of biological information. To this end, we summarize the remaining challenges and highlight key directions for future research, including the need for standardized validation benchmarks, improved evaluation strategies, and tighter integration of harmonization methods across the imaging pipeline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16962v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>physics.med-ph</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qinqin Yang, Firoozeh Shomal-Zadeh, Ali Gholipour</dc:creator>
    </item>
  </channel>
</rss>
