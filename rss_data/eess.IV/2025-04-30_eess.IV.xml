<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.IV updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.IV</link>
    <description>eess.IV updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.IV" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 May 2025 01:49:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>SCOPE-MRI: Bankart Lesion Detection as a Case Study in Data Curation and Deep Learning for Challenging Diagnoses</title>
      <link>https://arxiv.org/abs/2504.20405</link>
      <description>arXiv:2504.20405v1 Announce Type: new 
Abstract: While deep learning has shown strong performance in musculoskeletal imaging, existing work has largely focused on pathologies where diagnosis is not a clinical challenge, leaving more difficult problems underexplored, such as detecting Bankart lesions (anterior-inferior glenoid labral tears) on standard MRIs. Diagnosing these lesions is challenging due to their subtle imaging features, often leading to reliance on invasive MRI arthrograms (MRAs). This study introduces ScopeMRI, the first publicly available, expert-annotated dataset for shoulder pathologies, and presents a deep learning (DL) framework for detecting Bankart lesions on both standard MRIs and MRAs. ScopeMRI includes 586 shoulder MRIs (335 standard, 251 MRAs) from 558 patients who underwent arthroscopy. Ground truth labels were derived from intraoperative findings, the gold standard for diagnosis. Separate DL models for MRAs and standard MRIs were trained using a combination of CNNs and transformers. Predictions from sagittal, axial, and coronal views were ensembled to optimize performance. The models were evaluated on a 20% hold-out test set (117 MRIs: 46 MRAs, 71 standard MRIs). The models achieved an AUC of 0.91 and 0.93, sensitivity of 83% and 94%, and specificity of 91% and 86% for standard MRIs and MRAs, respectively. Notably, model performance on non-invasive standard MRIs matched or surpassed radiologists interpreting MRAs. External validation demonstrated initial generalizability across imaging protocols. This study demonstrates that DL models can achieve radiologist-level diagnostic performance on standard MRIs, reducing the need for invasive MRAs. By releasing ScopeMRI and a modular codebase for training and evaluating deep learning models on 3D medical imaging data, we aim to accelerate research in musculoskeletal imaging and support the development of new datasets for clinically challenging diagnostic tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20405v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sahil Sethi, Sai Reddy, Mansi Sakarvadia, Jordan Serotte, Darlington Nwaudo, Nicholas Maassen, Lewis Shi</dc:creator>
    </item>
    <item>
      <title>LymphAtlas- A Unified Multimodal Lymphoma Imaging Repository Delivering AI-Enhanced Diagnostic Insight</title>
      <link>https://arxiv.org/abs/2504.20454</link>
      <description>arXiv:2504.20454v1 Announce Type: new 
Abstract: This study integrates PET metabolic information with CT anatomical structures to establish a 3D multimodal segmentation dataset for lymphoma based on whole-body FDG PET/CT examinations, which bridges the gap of the lack of standardised multimodal segmentation datasets in the field of haematological malignancies. We retrospectively collected 483 examination datasets acquired between March 2011 and May 2024, involving 220 patients (106 non-Hodgkin lymphoma, 42 Hodgkin lymphoma); all data underwent ethical review and were rigorously de-identified. Complete 3D structural information was preserved during data acquisition, preprocessing and annotation, and a high-quality dataset was constructed based on the nnUNet format. By systematic technical validation and evaluation of the preprocessing process, annotation quality and automatic segmentation algorithm, the deep learning model trained based on this dataset is verified to achieve accurate segmentation of lymphoma lesions in PET/CT images with high accuracy, good robustness and reproducibility, which proves the applicability and stability of this dataset in accurate segmentation and quantitative analysis. The deep fusion of PET/CT images achieved with this dataset not only significantly improves the accurate portrayal of the morphology, location and metabolic features of tumour lesions, but also provides solid data support for early diagnosis, clinical staging and personalized treatment, and promotes the development of automated image segmentation and precision medicine based on deep learning. The dataset and related resources are available at https://github.com/SuperD0122/LymphAtlas-.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20454v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiajun Ding, Beiyao Zhu, Xiaosheng Liu, Lishen Zhang, Zhao Liu</dc:creator>
    </item>
    <item>
      <title>Full-field surrogate modeling of cardiac function encoding geometric variability</title>
      <link>https://arxiv.org/abs/2504.20479</link>
      <description>arXiv:2504.20479v1 Announce Type: new 
Abstract: Combining physics-based modeling with data-driven methods is critical to enabling the translation of computational methods to clinical use in cardiology. The use of rigorous differential equations combined with machine learning tools allows for model personalization with uncertainty quantification in time frames compatible with clinical practice. However, accurate and efficient surrogate models of cardiac function, built from physics-based numerical simulation, are still mostly geometry-specific and require retraining for different patients and pathological conditions. We propose a novel computational pipeline to embed cardiac anatomies into full-field surrogate models. We generate a dataset of electrophysiology simulations using a complex multi-scale mathematical model coupling partial and ordinary differential equations. We adopt Branched Latent Neural Maps (BLNMs) as an effective scientific machine learning method to encode activation maps extracted from physics-based numerical simulations into a neural network. Leveraging large deformation diffeomorphic metric mappings, we build a biventricular anatomical atlas and parametrize the anatomical variability of a small and challenging cohort of 13 pediatric patients affected by Tetralogy of Fallot. We propose a novel statistical shape modeling based z-score sampling approach to generate a new synthetic cohort of 52 biventricular geometries that are compatible with the original geometrical variability. This synthetic cohort acts as the training set for BLNMs. Our surrogate model demonstrates robustness and great generalization across the complex original patient cohort, achieving an average adimensional mean squared error of 0.0034. The Python implementation of our BLNM model is publicly available under MIT License at https://github.com/StanfordCBCL/BLNM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20479v1</guid>
      <category>eess.IV</category>
      <category>cs.LG</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Martinez, Beatrice Moscoloni, Matteo Salvador, Fanwei Kong, Mathias Peirlinck, Alison Lesley Marsden</dc:creator>
    </item>
    <item>
      <title>SAM-Guided Robust Representation Learning for One-Shot 3D Medical Image Segmentation</title>
      <link>https://arxiv.org/abs/2504.20501</link>
      <description>arXiv:2504.20501v1 Announce Type: new 
Abstract: One-shot medical image segmentation (MIS) is crucial for medical analysis due to the burden of medical experts on manual annotation. The recent emergence of the segment anything model (SAM) has demonstrated remarkable adaptation in MIS but cannot be directly applied to one-shot medical image segmentation (MIS) due to its reliance on labor-intensive user interactions and the high computational cost. To cope with these limitations, we propose a novel SAM-guided robust representation learning framework, named RRL-MedSAM, to adapt SAM to one-shot 3D MIS, which exploits the strong generalization capabilities of the SAM encoder to learn better feature representation. We devise a dual-stage knowledge distillation (DSKD) strategy to distill general knowledge between natural and medical images from the foundation model to train a lightweight encoder, and then adopt a mutual exponential moving average (mutual-EMA) to update the weights of the general lightweight encoder and medical-specific encoder. Specifically, pseudo labels from the registration network are used to perform mutual supervision for such two encoders. Moreover, we introduce an auto-prompting (AP) segmentation decoder which adopts the mask generated from the general lightweight model as a prompt to assist the medical-specific model in boosting the final segmentation performance. Extensive experiments conducted on three public datasets, i.e., OASIS, CT-lung demonstrate that the proposed RRL-MedSAM outperforms state-of-the-art one-shot MIS methods for both segmentation and registration tasks. Especially, our lightweight encoder uses only 3\% of the parameters compared to the encoder of SAM-Base.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20501v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jia Wang, Yunan Mei, Jiarui Liu, Xin Fan</dc:creator>
    </item>
    <item>
      <title>Quality-factor inspired deep neural network solver for solving inverse scattering problems</title>
      <link>https://arxiv.org/abs/2504.20504</link>
      <description>arXiv:2504.20504v1 Announce Type: new 
Abstract: Deep neural networks have been applied to address electromagnetic inverse scattering problems (ISPs) and shown superior imaging performances, which can be affected by the training dataset, the network architecture and the applied loss function. Here, the quality of data samples is cared and valued by the defined quality factor. Based on the quality factor, the composition of the training dataset is optimized. The network architecture is integrated with the residual connections and channel attention mechanism to improve feature extraction. A loss function that incorporates data-fitting error, physical-information constraints and the desired feature of the solution is designed and analyzed to suppress the background artifacts and improve the reconstruction accuracy. Various numerical analysis are performed to demonstrate the superiority of the proposed quality-factor inspired deep neural network (QuaDNN) solver and the imaging performance is finally verified by experimental imaging test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20504v1</guid>
      <category>eess.IV</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yutong Du, Zicheng Liu, Miao Cao, Zupeng Liang, Yali Zong, Changyou Li</dc:creator>
    </item>
    <item>
      <title>Imaging on the Edge: Mapping Object Corners and Edges with Stereo X-ray Tomography</title>
      <link>https://arxiv.org/abs/2504.20892</link>
      <description>arXiv:2504.20892v1 Announce Type: new 
Abstract: X-ray computed tomography is a powerful tool for volumetric imaging, where three-dimensional (3D) images are generated from a large number of individual X-ray projection images. Collecting the required number of low noise projection images is however time-consuming and so the technique is not currently applicable when spatial information needs to be collected with high temporal resolution, such as in the study of dynamic processes. In our previous work, inspired by stereo vision, we developed stereo X-ray imaging methods that operate with only two X-ray projection images. Previously we have shown how this allowed us to map point and line fiducial markers into 3D space at significantly faster temporal resolutions. In this paper, we make two further contributions. Firstly, instead of utilising internal fiducial markers, we demonstrate the applicability of the method to the 3D mapping of sharp object corners, a problem of interest in measuring the deformation of manufactured components under different loads. Furthermore, we demonstrate how the approach can be applied to real stereo X-ray data, even in settings where we do not have the annotated real training data that was required for the training of our previous Machine Learning approach. This is achieved by substituting the real data with a relatively simple synthetic training dataset designed to mimic key aspects of the real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20892v1</guid>
      <category>eess.IV</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenduo Shang, Thomas Blumensath</dc:creator>
    </item>
    <item>
      <title>Swapped Logit Distillation via Bi-level Teacher Alignment</title>
      <link>https://arxiv.org/abs/2504.20108</link>
      <description>arXiv:2504.20108v1 Announce Type: cross 
Abstract: Knowledge distillation (KD) compresses the network capacity by transferring knowledge from a large (teacher) network to a smaller one (student). It has been mainstream that the teacher directly transfers knowledge to the student with its original distribution, which can possibly lead to incorrect predictions. In this article, we propose a logit-based distillation via swapped logit processing, namely Swapped Logit Distillation (SLD). SLD is proposed under two assumptions: (1) the wrong prediction occurs when the prediction label confidence is not the maximum; (2) the "natural" limit of probability remains uncertain as the best value addition to the target cannot be determined. To address these issues, we propose a swapped logit processing scheme. Through this approach, we find that the swap method can be effectively extended to teacher and student outputs, transforming into two teachers. We further introduce loss scheduling to boost the performance of two teachers' alignment. Extensive experiments on image classification tasks demonstrate that SLD consistently performs best among previous state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20108v1</guid>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stephen Ekaputra Limantoro, Jhe-Hao Lin, Chih-Yu Wang, Yi-Lung Tsai, Hong-Han Shuai, Ching-Chun Huang, Wen-Huang Cheng</dc:creator>
    </item>
    <item>
      <title>Remote Sensing Imagery for Flood Detection: Exploration of Augmentation Strategies</title>
      <link>https://arxiv.org/abs/2504.20203</link>
      <description>arXiv:2504.20203v1 Announce Type: cross 
Abstract: Floods cause serious problems around the world. Responding quickly and effectively requires accurate and timely information about the affected areas. The effective use of Remote Sensing images for accurate flood detection requires specific detection methods. Typically, Deep Neural Networks are employed, which are trained on specific datasets. For the purpose of river flood detection in RGB imagery, we use the BlessemFlood21 dataset. We here explore the use of different augmentation strategies, ranging from basic approaches to more complex techniques, including optical distortion. By identifying effective strategies, we aim to refine the training process of state-of-the-art Deep Learning segmentation networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20203v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Vladyslav Polushko, Damjan Hatic, Ronald R\"osch, Thomas M\"arz, Markus Rauhut, Andreas Weinmann</dc:creator>
    </item>
    <item>
      <title>Neural Stereo Video Compression with Hybrid Disparity Compensation</title>
      <link>https://arxiv.org/abs/2504.20383</link>
      <description>arXiv:2504.20383v1 Announce Type: cross 
Abstract: Disparity compensation represents the primary strategy in stereo video compression (SVC) for exploiting cross-view redundancy. These mechanisms can be broadly categorized into two types: one that employs explicit horizontal shifting, and another that utilizes an implicit cross-attention mechanism to reduce cross-view disparity redundancy. In this work, we propose a hybrid disparity compensation (HDC) strategy that leverages explicit pixel displacement as a robust prior feature to simplify optimization and perform implicit cross-attention mechanisms for subsequent warping operations, thereby capturing a broader range of disparity information. Specifically, HDC first computes a similarity map by fusing the horizontally shifted cross-view features to capture pixel displacement information. This similarity map is then normalized into an "explicit pixel-wise attention score" to perform the cross-attention mechanism, implicitly aligning features from one view to another. Building upon HDC, we introduce a novel end-to-end optimized neural stereo video compression framework, which integrates HDC-based modules into key coding operations, including cross-view feature extraction and reconstruction (HDC-FER) and cross-view entropy modeling (HDC-EM). Extensive experiments on SVC benchmarks, including KITTI 2012, KITTI 2015, and Nagoya, which cover both autonomous driving and general scenes, demonstrate that our framework outperforms both neural and traditional SVC methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20383v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiyin Jiang, Zhenghao Chen, Minghao Han, Xingyu Zhou, Leheng Zhang, Shuhang Gu</dc:creator>
    </item>
    <item>
      <title>Ring deconvolution microscopy: exploiting symmetry for efficient spatially varying aberration correction</title>
      <link>https://arxiv.org/abs/2206.08928</link>
      <description>arXiv:2206.08928v5 Announce Type: replace 
Abstract: The most ubiquitous form of computational aberration correction for microscopy is deconvolution. However, deconvolution relies on the assumption that the point spread function is the same across the entire field-of-view. This assumption is often inadequate, but space-variant deblurring techniques generally require impractical amounts of calibration and computation. We present a new imaging pipeline that leverages symmetry to provide simple and fast spatially-varying aberration correction. Our ring deconvolution microscopy (RDM) method leverages the rotational symmetry of most microscopes and cameras, and naturally extends to sheet deconvolution in the case of lateral symmetry. We formally derive theory and algorithms for image recovery and additionally propose a neural network based on Seidel coefficients as a fast alternative, as well as extension of RDM to blind deconvolution. We demonstrate significant improvements in speed and image quality as compared to standard deconvolution and existing spatially-varying deconvolution across a diverse range of microscope modalities, including miniature microscopy, multicolor fluorescence microscopy, point-scanning multimode fiber micro-endoscopy, and light-sheet fluorescence microscopy. Our approach enables near-isotropic, subcellular resolution in each of these applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.08928v5</guid>
      <category>eess.IV</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1038/s41592-025-02684-5</arxiv:DOI>
      <dc:creator>Amit Kohli, Anastasios N. Angelopoulos, David McAllister, Esther Whang, Sixian You, Kyrollos Yanny, Federico M. Gasparoli, Bo-Jui Chang, Reto Fiolka, Laura Waller</dc:creator>
    </item>
    <item>
      <title>3D ReX: Causal Explanations in 3D Neuroimaging Classification</title>
      <link>https://arxiv.org/abs/2502.12181</link>
      <description>arXiv:2502.12181v3 Announce Type: replace 
Abstract: Explainability remains a significant problem for AI models in medical imaging, making it challenging for clinicians to trust AI-driven predictions. We introduce 3D ReX, the first causality-based post-hoc explainability tool for 3D models. 3D ReX uses the theory of actual causality to generate responsibility maps which highlight the regions most crucial to the model's decision. We test 3D ReX on a stroke detection model, providing insight into the spatial distribution of features relevant to stroke.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12181v3</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Melane Navaratnarajah, Sophie A. Martin, David A. Kelly, Nathan Blake, Hana Chockler</dc:creator>
    </item>
    <item>
      <title>RGB-Thermal Infrared Fusion for Robust Depth Estimation in Complex Environments</title>
      <link>https://arxiv.org/abs/2503.04821</link>
      <description>arXiv:2503.04821v2 Announce Type: replace 
Abstract: Depth estimation in complex real-world scenarios is a challenging task, especially when relying solely on a single modality such as visible light or thermal infrared (THR) imagery. This paper proposes a novel multimodal depth estimation model, RTFusion, which enhances depth estimation accuracy and robustness by integrating the complementary strengths of RGB and THR data. The RGB modality provides rich texture and color information, while the THR modality captures thermal patterns, ensuring stability under adverse lighting conditions such as extreme illumination. The model incorporates a unique fusion mechanism, EGFusion, consisting of the Mutual Complementary Attention (MCA) module for cross-modal feature alignment and the Edge Saliency Enhancement Module (ESEM) to improve edge detail preservation. Comprehensive experiments on the MS2 and ViViD++ datasets demonstrate that the proposed model consistently produces high-quality depth maps across various challenging environments, including nighttime, rainy, and high-glare conditions. The experimental results highlight the potential of the proposed method in applications requiring reliable depth estimation, such as autonomous driving, robotics, and augmented reality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04821v2</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zelin Meng, Takanori Fukao</dc:creator>
    </item>
    <item>
      <title>Adaptive Weight Modified Riesz Mean Filter For High-Density Salt and Pepper Noise Removal</title>
      <link>https://arxiv.org/abs/2504.18251</link>
      <description>arXiv:2504.18251v2 Announce Type: replace 
Abstract: This paper introduces a novel filter, the Adaptive Weight Modified Riesz Mean Filter (AWMRmF), designed for the effective removal of high-density salt and pepper noise (SPN). AWMRmF integrates a pixel weight function and adaptivity condition inspired by the Different Adaptive Modified Riesz Mean Filter (DAMRmF). In my simulations, I evaluated the performance of AWMRmF against established filters such as Adaptive Frequency Median Filter (AFMF), Adaptive Weighted Mean Filter (AWMF), Adaptive Cesaro Mean Filter (ACmF), Adaptive Riesz Mean Filter (ARmF), and Improved Adaptive Weighted Mean Filter (IAWMF). The assessment was conducted on 26 typical test images, varying noise levels from 60% to 95%. The findings indicate that, in terms of both Peak Signal to Noise Ratio (PSNR) and Structural Similarity (SSIM) metrics, AWMRmF outperformed other state-of-the-art filters. Furthermore, AWMRmF demonstrated superior performance in mean PSNR and SSIM results as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18251v2</guid>
      <category>eess.IV</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Jahidul Islam</dc:creator>
    </item>
    <item>
      <title>Improving Generalization in MRI-Based Deep Learning Models for Total Knee Replacement Prediction</title>
      <link>https://arxiv.org/abs/2504.19203</link>
      <description>arXiv:2504.19203v2 Announce Type: replace 
Abstract: Knee osteoarthritis (KOA) is a common joint disease that causes pain and mobility issues. While MRI-based deep learning models have demonstrated superior performance in predicting total knee replacement (TKR) and disease progression, their generalizability remains challenging, particularly when applied to imaging data from different sources. In this study, we have shown that replacing batch normalization with instance normalization, using data augmentation, and applying contrastive loss improves model generalization in a baseline deep learning model for knee osteoarthritis (KOA) prediction. We trained and evaluated our model using MRI data from the Osteoarthritis Initiative (OAI) database, considering sagittal fat-suppressed intermediate-weighted turbo spin-echo (FS-IW-TSE) images as the source domain and sagittal fat-suppressed three-dimensional (3D) dual-echo in steady state (DESS) images as the target domain. The results demonstrate a statistically significant improvement in classification accuracy across both domains, with our approach outperforming the baseline model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19203v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ehsan Karami, Hamid Soltanian-Zadeh</dc:creator>
    </item>
  </channel>
</rss>
