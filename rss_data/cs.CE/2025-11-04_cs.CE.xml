<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Nov 2025 05:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>GEDICorrect: A Scalable Python Tool for Orbit-, Beam-, and Footprint-Level GEDI Geolocation Correction</title>
      <link>https://arxiv.org/abs/2511.00319</link>
      <description>arXiv:2511.00319v1 Announce Type: new 
Abstract: Accurate geolocation is essential for the reliable use of GEDI LiDAR data in footprint-scale applications such as aboveground biomass modeling, data fusion, and ecosystem monitoring. However, residual geolocation errors arising from both systematic biases and random ISS-induced jitter can significantly affect the accuracy of derived vegetation and terrain metrics. The main goal of this study is to develop and evaluate a flexible, computationally efficient framework (GEDICorrect) that enables geolocation correction of GEDI data at the orbit, beam, and footprint levels. The framework integrates existing GEDI Simulator modules (gediRat and gediMetrics) and extends their functionality with flexible correction logic, multiple similarity metrics, adaptive footprint clustering, and optimized I/O handling. Using the Kullback--Leibler divergence as the waveform similarity metric, GEDICorrect improved canopy height (RH95) accuracy from $R^2 = 0.61$ (uncorrected) to 0.74 with the orbit-level correction, and up to $R^2 = 0.78$ with the footprint-level correction, reducing RMSE from 2.62~m ($rRMSE = 43.13\%$) to 2.12~m ($rRMSE = 34.97\%$) at the orbit level, and 2.01~m ($rRMSE = 33.05\%$) at the footprint level. Terrain elevation accuracy also improved, decreasing RMSE by 0.34~m relative to uncorrected data and by 0.37~m compared to the GEDI Simulator baseline. In terms of computational efficiency, GEDICorrect achieved a $\sim2.4\times$ speedup over the GEDI Simulator in single-process mode (reducing runtime from $\sim84$~h to $\sim35$~h) and scaled efficiently to 24 cores, completing the same task in $\sim4.3$~h -- an overall $\sim19.5\times$ improvement. GEDICorrect offers a robust and scalable solution for improving GEDI geolocation accuracy while maintaining full compatibility with standard GEDI data products.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00319v1</guid>
      <category>cs.CE</category>
      <category>physics.geo-ph</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonel Corado, S\'ergio Godinho, Carlos Alberto Silva, Juan Guerra-Hern\'andez, Francesco Val\'erioa, Teresa Gon\c{c}alves, Pedro Salgueiro</dc:creator>
    </item>
    <item>
      <title>STARC-9: A Large-scale Dataset for Multi-Class Tissue Classification for CRC Histopathology</title>
      <link>https://arxiv.org/abs/2511.00383</link>
      <description>arXiv:2511.00383v1 Announce Type: new 
Abstract: Multi-class tissue-type classification of colorectal cancer (CRC) histopathologic images is a significant step in the development of downstream machine learning models for diagnosis and treatment planning. However, existing public CRC datasets often lack morphologic diversity, suffer from class imbalance, and contain low-quality image tiles, limiting model performance and generalizability. To address these issues, we introduce STARC-9 (STAnford coloRectal Cancer), a large-scale dataset for multi-class tissue classification. STARC-9 contains 630,000 hematoxylin and eosin-stained image tiles uniformly sampled across nine clinically relevant tissue classes (70,000 tiles per class) from 200 CRC patients at the Stanford University School of Medicine. The dataset was built using a novel framework, DeepCluster++, designed to ensure intra-class diversity and reduce manual curation. First, an encoder from a histopathology-specific autoencoder extracts feature vectors from tiles within each whole-slide image. Then, K-means clustering groups morphologically similar tiles, followed by equal-frequency binning to sample diverse morphologic patterns within each class. The selected tiles are subsequently verified by expert gastrointestinal pathologists to ensure accuracy. This semi-automated process significantly reduces manual effort while producing high-quality, diverse tiles. To evaluate STARC-9, we benchmarked convolutional neural networks, transformers, and pathology-specific foundation models on multi-class CRC tissue classification and segmentation tasks, showing superior generalizability compared to models trained on existing datasets. Although we demonstrate the utility of DeepCluster++ on CRC as a pilot use-case, it is a flexible framework that can be used for constructing high-quality datasets from large WSI repositories across a wide range of cancer and non-cancer applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00383v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barathi Subramanian, Rathinaraja Jeyaraj, Mitchell Nevin Peterson, Terry Guo, Nigam Shah, Curtis Langlotz, Andrew Y. Ng, Jeanne Shen</dc:creator>
    </item>
    <item>
      <title>DeltaLag: Learning Dynamic Lead-Lag Patterns in Financial Markets</title>
      <link>https://arxiv.org/abs/2511.00390</link>
      <description>arXiv:2511.00390v1 Announce Type: new 
Abstract: The lead-lag effect, where the price movement of one asset systematically precedes that of another, has been widely observed in financial markets and conveys valuable predictive signals for trading. However, traditional lead-lag detection methods are limited by their reliance on statistical analysis methods and by the assumption of persistent lead-lag patterns, which are often invalid in dynamic market conditions. In this paper, we propose \textbf{DeltaLag}, the first end-to-end deep learning method that discovers and exploits dynamic lead-lag structures with pair-specific lag values in financial markets for portfolio construction. Specifically, DeltaLag employs a sparsified cross-attention mechanism to identify relevant lead-lag pairs. These lead-lag signals are then leveraged to extract lag-aligned raw features from the leading stocks for predicting the lagger stock's future return. Empirical evaluations show that DeltaLag substantially outperforms both fixed-lag and self-lead-lag baselines. In addition, its adaptive mechanism for identifying lead-lag relationships consistently surpasses precomputed lead-lag graphs based on statistical methods. Furthermore, DeltaLag outperforms a wide range of temporal and spatio-temporal deep learning models designed for stock prediction or time series forecasting, offering both better trading performance and enhanced interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00390v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wanyun Zhou, Saizhuo Wang, Mihai Cucuringu, Zihao Zhang, Xiang Li, Jian Guo, Chao Zhang, Xiaowen Chu</dc:creator>
    </item>
    <item>
      <title>A Meta-Cognitive Swarm Intelligence Framework for Resilient UAV Navigation in GPS-Denied and Cluttered Environments</title>
      <link>https://arxiv.org/abs/2511.00884</link>
      <description>arXiv:2511.00884v1 Announce Type: new 
Abstract: Autonomous navigation of UAV swarms in perceptually-degraded environments, where GPS is unavailable and terrain is densely cluttered, presents a critical bottleneck for real-world deployment. Existing optimization-based planners lack the resilience to avoid catastrophic convergence to local optima under such uncertainty. Inspired by principles of computational meta-cognition, this paper introduces a novel swarm intelligence framework that enables a fleet of UAVs to autonomously sense, adapt, and recover from planning failures in real-time. At its core is the Self-Learning Slime Mould Algorithm (SLSMA), which integrates three meta-cognitive layers: a situation-aware search strategy that dynamically selects between exploration and exploitation based on perceived search stagnation; a collective memory mechanism that allows the swarm to learn from and avoid previously failed trajectories; and an adaptive recovery behavior that triggers global re-exploration upon entrapment. We formulate the multi-UAV trajectory problem as a resilient planning challenge, with a cost function that penalizes not only path length and collisions but also navigational uncertainty and proximity to failure states. Extensive simulations in synthetically complex 3D worlds and against the CEC 2017 benchmark suite demonstrate the framework's superior performance. The SLSMA does not merely optimize paths; it generates resilient trajectories, demonstrating a 99.5% mission success rate and significantly outperforming state-of-the-art metaheuristics in recovery speed and solution reliability. This work provides a foundational step towards truly autonomous swarms capable of persistent operation in denied and dynamic environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00884v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathias Mankoe (Yanshan University), Fuqiang Lu (Yanshan University), Hualing Bi (Yanshan University), Abdulsalam Sibidoo Mubashiru (Kwame Nkrumah University of Science and Technology)</dc:creator>
    </item>
    <item>
      <title>Investigation of Performance and Scalability of a Quantum-Inspired Evolutionary Optimizer (QIEO) on NVIDIA GPU</title>
      <link>https://arxiv.org/abs/2511.01298</link>
      <description>arXiv:2511.01298v1 Announce Type: new 
Abstract: Quantum inspired evolutionary optimization leverages quantum computing principles like superposition, interference, and probabilistic representation to enhance classical evolutionary algorithms with improved exploration and exploitation capabilities. Implemented on NVIDIA Tesla V100 SXM2 GPUs, this study systematically investigates the performance and scalability of a GPU-accelerated Quantum Inspired Evolutionary Optimizer applied to large scale 01 Knapsack problems. By exploiting CUDA`s parallel processing capabilities, particularly through optimized memory management and thread configuration, significant speedups and efficient utilization of GPU resources is demonstrated. The analysis covers various problem sizes, kernel launch configurations, and memory models including constant, shared, global, and pinned memory, alongside extensive scaling studies. The results reveal that careful tuning of memory strategies and kernel configurations is essential for maximizing throughput and efficiency, with constant memory providing superior performance up to hardware limits. Beyond these limits, global memory and strategic tiling become necessary, albeit with some performance trade offs. The findings highlight both the promise and the practical constraints of applying QIEO on GPUs for complex combinatorial optimization, offering actionable insights for future large scale metaheuristic implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01298v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aman Mittal, Kasturi Venkata Sai Srikanth, Ferdin Sagai Don Bosco, Abhishek Singh, Rut Lineswala, Abhishek Chopra</dc:creator>
    </item>
    <item>
      <title>CSMD: Curated Multimodal Dataset for Chinese Stock Analysis</title>
      <link>https://arxiv.org/abs/2511.01318</link>
      <description>arXiv:2511.01318v1 Announce Type: new 
Abstract: The stock market is a complex and dynamic system, where it is non-trivial for researchers and practitioners to uncover underlying patterns and forecast stock movements. The existing studies for stock market analysis rely on leveraging various types of information to extract useful factors, which are highly conditional on the quality of the data used. However, the currently available resources are mainly based on the U.S. stock market in English, which is inapplicable to adapt to other countries. To address these issues, we propose CSMD, a multimodal dataset curated specifically for analyzing the Chinese stock market with meticulous processing for validated quality. In addition, we develop a lightweight and user-friendly framework LightQuant for researchers and practitioners with expertise in financial domains. Experimental results on top of our datasets and framework with various backbone models demonstrate their effectiveness compared with using existing datasets. The datasets and code are publicly available at the link: https://github.com/ECNU-CILAB/LightQuant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01318v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3746252.3761636</arxiv:DOI>
      <dc:creator>Yu Liu, Zhuoying Li, Ruifeng Yang, Fengran Mo, Cen Chen</dc:creator>
    </item>
    <item>
      <title>Solution Space Topology Guides CMTS Search</title>
      <link>https://arxiv.org/abs/2511.01701</link>
      <description>arXiv:2511.01701v1 Announce Type: new 
Abstract: A fundamental question in search-guided AI: what topology should guide Monte Carlo Tree Search (MCTS) in puzzle solving? Prior work applied topological features to guide MCTS in ARC-style tasks using grid topology -- the Laplacian spectral properties of cell connectivity -- and found no benefit. We identify the root cause: grid topology is constant across all instances. We propose measuring \emph{solution space topology} instead: the structure of valid color assignments constrained by detected pattern rules. We build this via compatibility graphs where nodes are $(cell, color)$ pairs and edges represent compatible assignments under pattern constraints.
  Our method: (1) detect pattern rules automatically with 100\% accuracy on 5 types, (2) construct compatibility graphs encoding solution space structure, (3) extract topological features (algebraic connectivity, rigidity, color structure) that vary with task difficulty, (4) integrate these features into MCTS node selection via sibling-normalized scores.
  We provide formal definitions, a rigorous selection formula, and comprehensive ablations showing that algebraic connectivity is the dominant signal. The work demonstrates that topology matters for search -- but only the \emph{right} topology. For puzzle solving, this is solution space structure, not problem space structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01701v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mirco A. Mannucci</dc:creator>
    </item>
    <item>
      <title>A Synthesizability-Guided Pipeline for Materials Discovery</title>
      <link>https://arxiv.org/abs/2511.01790</link>
      <description>arXiv:2511.01790v1 Announce Type: new 
Abstract: Computational materials discovery relies on the generation of plausible crystal structures. The plausibility is typically judged through density functional theory methods which, while typically accurate at zero Kelvin, often favor low-energy structures that are not experimentally accessible. We develop a combined compositional and structural synthesizability score which provides an accurate way of predicting which compounds can actually be synthesized in a laboratory. We use it to evaluate non-synthesized structures from the Materials Project, GNoME, and Alexandria, and identified several hundred highly synthesizable candidates. We then predict synthesis pathways, conduct corresponding experiments, and characterize the products across 16 targets, successfully synthesizing 7 of 16. The entire experimental process was completed in only three days. Our results highlight omissions in lists of known synthesized structures, deliver insights into the practical utility of current materials databases, and showcase the central role synthesizability prediction can play in materials discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01790v1</guid>
      <category>cs.CE</category>
      <category>cond-mat.mtrl-sci</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thorben Prein, Willis O'Leary, Aikaterini Flessa Savvidou, Elcha\"ima Bourneix, Joonatan E. M. Laulainen</dc:creator>
    </item>
    <item>
      <title>DynBERG: Dynamic BERT-based Graph neural network for financial fraud detection</title>
      <link>https://arxiv.org/abs/2511.00047</link>
      <description>arXiv:2511.00047v1 Announce Type: cross 
Abstract: Financial fraud detection is critical for maintaining the integrity of financial systems, particularly in decentralised environments such as cryptocurrency networks. Although Graph Convolutional Networks (GCNs) are widely used for financial fraud detection, graph Transformer models such as Graph-BERT are gaining prominence due to their Transformer-based architecture, which mitigates issues such as over-smoothing. Graph-BERT is designed for static graphs and primarily evaluated on citation networks with undirected edges. However, financial transaction networks are inherently dynamic, with evolving structures and directed edges representing the flow of money. To address these challenges, we introduce DynBERG, a novel architecture that integrates Graph-BERT with a Gated Recurrent Unit (GRU) layer to capture temporal evolution over multiple time steps. Additionally, we modify the underlying algorithm to support directed edges, making DynBERG well-suited for dynamic financial transaction analysis. We evaluate our model on the Elliptic dataset, which includes Bitcoin transactions, including all transactions during a major cryptocurrency market event, the Dark Market Shutdown. By assessing DynBERG's resilience before and after this event, we analyse its ability to adapt to significant market shifts that impact transaction behaviours. Our model is benchmarked against state-of-the-art dynamic graph classification approaches, such as EvolveGCN and GCN, demonstrating superior performance, outperforming EvolveGCN before the market shutdown and surpassing GCN after the event. Additionally, an ablation study highlights the critical role of incorporating a time-series deep learning component, showcasing the effectiveness of GRU in modelling the temporal dynamics of financial transactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00047v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Omkar Kulkarni, Rohitash Chandra</dc:creator>
    </item>
    <item>
      <title>A Streaming Sparse Cholesky Method for Derivative-Informed Gaussian Process Surrogates Within Digital Twin Applications</title>
      <link>https://arxiv.org/abs/2511.00366</link>
      <description>arXiv:2511.00366v1 Announce Type: cross 
Abstract: Digital twins are developed to model the behavior of a specific physical asset (or twin), and they can consist of high-fidelity physics-based models or surrogates. A highly accurate surrogate is often preferred over multi-physics models as they enable forecasting the physical twin future state in real-time. To adapt to a specific physical twin, the digital twin model must be updated using in-service data from that physical twin. Here, we extend Gaussian process (GP) models to include derivative data, for improved accuracy, with dynamic updating to ingest physical twin data during service. Including derivative data, however, comes at a prohibitive cost of increased covariance matrix dimension. We circumvent this issue by using a sparse GP approximation, for which we develop extensions to incorporate derivatives. Numerical experiments demonstrate that the prediction accuracy of the derivative-enhanced sparse GP method produces improved models upon dynamic data additions. Lastly, we apply the developed algorithm within a DT framework to model fatigue crack growth in an aerospace vehicle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00366v1</guid>
      <category>stat.ML</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Krishna Prasath Logakannan, Shridhar Vashishtha, Jacob Hochhalter, Shandian Zhe, Robert M. Kirby</dc:creator>
    </item>
    <item>
      <title>OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights</title>
      <link>https://arxiv.org/abs/2511.01019</link>
      <description>arXiv:2511.01019v1 Announce Type: cross 
Abstract: Artificial intelligence is transforming the sciences, yet general conversational AI systems often generate unverified "hallucinations" undermining scientific rigor. We present OceanAI, a conversational platform that integrates the natural-language fluency of open-source large language models (LLMs) with real-time, parameterized access to authoritative oceanographic data streams hosted by the National Oceanic and Atmospheric Administration (NOAA). Each query such as "What was Boston Harbor's highest water level in 2024?" triggers real-time API calls that identify, parse, and synthesize relevant datasets into reproducible natural-language responses and data visualizations. In a blind comparison with three widely used AI chat-interface products, only OceanAI produced NOAA-sourced values with original data references; others either declined to answer or provided unsupported results. Designed for extensibility, OceanAI connects to multiple NOAA data products and variables, supporting applications in marine hazard forecasting, ecosystem assessment, and water-quality monitoring. By grounding outputs and verifiable observations, OceanAI advances transparency, reproducibility, and trust, offering a scalable framework for AI-enabled decision support within the oceans. A public demonstration is available at https://oceanai.ai4ocean.xyz.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01019v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>physics.ao-ph</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bowen Chen (DK), Jayesh Gajbhar (DK), Gregory Dusek (DK), Rob Redmon (DK), Patrick Hogan (DK), Paul Liu (DK), DelWayne Bohnenstiehl (DK),  Dongkuan (DK),  Xu, Ruoying He</dc:creator>
    </item>
    <item>
      <title>Novelty and Impact of Economics Papers</title>
      <link>https://arxiv.org/abs/2511.01211</link>
      <description>arXiv:2511.01211v1 Announce Type: cross 
Abstract: We propose a framework that recasts scientific novelty not as a single attribute of a paper, but as a reflection of its position within the evolving intellectual landscape. We decompose this position into two orthogonal dimensions: \textit{spatial novelty}, which measures a paper's intellectual distinctiveness from its neighbors, and \textit{temporal novelty}, which captures its engagement with a dynamic research frontier. To operationalize these concepts, we leverage Large Language Models to develop semantic isolation metrics that quantify a paper's location relative to the full-text literature. Applying this framework to a large corpus of economics articles, we uncover a fundamental trade-off: these two dimensions predict systematically different outcomes. Temporal novelty primarily predicts citation counts, whereas spatial novelty predicts disruptive impact. This distinction allows us to construct a typology of semantic neighborhoods, identifying four archetypes associated with distinct and predictable impact profiles. Our findings demonstrate that novelty can be understood as a multidimensional construct whose different forms, reflecting a paper's strategic location, have measurable and fundamentally distinct consequences for scientific progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01211v1</guid>
      <category>econ.GN</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>cs.DL</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chaofeng Wu</dc:creator>
    </item>
    <item>
      <title>Dynamic Modeling of Precipitation in Electrolyte Systems</title>
      <link>https://arxiv.org/abs/2511.01519</link>
      <description>arXiv:2511.01519v1 Announce Type: cross 
Abstract: This study presents a dynamic modeling approach for precipitation in electrolyte systems, focusing on the crystallization of an aromatic amine through continuous processes. A novel model, integrating equilibrium and crystallization kinetics, is formulated and applied to a continuous oscillatory baffled reactor. The approach assumes rapid equilibrium establishment and is formulated as a set of differential algebraic equations. Key features include a population balance equation model to describe the particle size distribution and the modeling of dynamically changing equilibria. The predictions of the dynamic model show good agreement with the available experimental measurements. The model is aimed at aiding the transition from a batch process to continuous process by forming the basis for numerical optimization and advanced control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01519v1</guid>
      <category>physics.chem-ph</category>
      <category>cs.CE</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niklas Kemmerling, Sergio Lucia</dc:creator>
    </item>
    <item>
      <title>Disciplined Biconvex Programming</title>
      <link>https://arxiv.org/abs/2511.01813</link>
      <description>arXiv:2511.01813v1 Announce Type: cross 
Abstract: We introduce disciplined biconvex programming (DBCP), a modeling framework for specifying and solving biconvex optimization problems. Biconvex optimization problems arise in various applications, including machine learning, signal processing, computational science, and control. Solving a biconvex optimization problem in practice usually resolves to heuristic methods based on alternate convex search (ACS), which iteratively optimizes over one block of variables while keeping the other fixed, so that the resulting subproblems are convex and can be efficiently solved. However, designing and implementing an ACS solver for a specific biconvex optimization problem usually requires significant effort from the user, which can be tedious and error-prone. DBCP extends the principles of disciplined convex programming to biconvex problems, allowing users to specify biconvex optimization problems in a natural way based on a small number of syntax rules. The resulting problem can then be automatically split and transformed into convex subproblems, for which a customized ACS solver is then generated and applied. DBCP allows users to quickly experiment with different biconvex problem formulations, without expertise in convex optimization. We implement DBCP into the open source Python package dbcp, as an extension to the famous domain specific language CVXPY for convex optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01813v1</guid>
      <category>math.OC</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.MS</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Zhu, Joschka Boedecker</dc:creator>
    </item>
    <item>
      <title>GroupSHAP-Guided Integration of Financial News Keywords and Technical Indicators for Stock Price Prediction</title>
      <link>https://arxiv.org/abs/2510.23112</link>
      <description>arXiv:2510.23112v3 Announce Type: replace 
Abstract: Recent advances in finance-specific language models such as FinBERT have enabled the quantification of public sentiment into index-based measures, yet compressing diverse linguistic signals into single metrics overlooks contextual nuances and limits interpretability. To address this limitation, explainable AI techniques, particularly SHAP (SHapley Additive Explanations), have been employed to identify influential features. However, SHAP's computational cost grows exponentially with input features, making it impractical for large-scale text-based financial data. This study introduces a GRU-based forecasting framework enhanced with GroupSHAP, which quantifies contributions of semantically related keyword groups rather than individual tokens, substantially reducing computational burden while preserving interpretability. We employed FinBERT to embed news articles from 2015 to 2024, clustered them into coherent semantic groups, and applied GroupSHAP to measure each group's contribution to stock price movements. The resulting group-level SHAP variables across multiple topics were used as input features for the prediction model. Empirical results from one-day-ahead forecasting of the S&amp;P 500 index throughout 2024 demonstrate that our approach achieves a 32.2% reduction in MAE and a 40.5% reduction in RMSE compared with benchmark models without the GroupSHAP mechanism. This research presents the first application of GroupSHAP in news-driven financial forecasting, showing that grouped sentiment representations simultaneously enhance interpretability and predictive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23112v3</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>ICAIF 2025 Workshop</arxiv:journal_reference>
      <dc:creator>Minjoo Kim, Jinwoong Kim, Sangjin Park</dc:creator>
    </item>
    <item>
      <title>Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning</title>
      <link>https://arxiv.org/abs/2505.17050</link>
      <description>arXiv:2505.17050v2 Announce Type: replace-cross 
Abstract: Project-Based Learning (PBL) involves a variety of highly correlated multimodal data, making it a vital educational approach within STEM disciplines. With the rapid development of multimodal large language models (MLLMs), researchers have begun exploring their potential to enhance tasks such as information retrieval, knowledge comprehension, and data generation in educational settings. However, existing benchmarks fall short in providing both a free-form output structure and a rigorous human expert validation process, limiting their effectiveness in evaluating real-world educational tasks. Additionally, few methods have developed automated pipelines to assist with the complex responsibilities of teachers leveraging MLLMs, largely due to model hallucination and instability, which lead to unreliable implementation. To address this gap, we introduce PBLBench, a novel benchmark designed to evaluate complex reasoning grounded in domain-specific knowledge and long-context understanding, thereby challenging models with tasks that closely resemble those handled by human experts. To establish reliable ground truth, we adopt the Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise comparisons to derive structured and weighted evaluation criteria. We assess the performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that even the most advanced models achieve only 59% rank accuracy, underscoring the significant challenges presented by this benchmark. We believe PBLBench will serve as a catalyst for the development of more capable AI agents, ultimately aiming to alleviate teacher workload and enhance educational productivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17050v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <category>cs.MM</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Wu, Yanhao Jia, Qinglin Zhang, Yiran Qin, Luwei Xiao, Shuai Zhao</dc:creator>
    </item>
    <item>
      <title>Scientific Machine Learning with Kolmogorov-Arnold Networks</title>
      <link>https://arxiv.org/abs/2507.22959</link>
      <description>arXiv:2507.22959v2 Announce Type: replace-cross 
Abstract: The field of scientific machine learning, which originally utilized multilayer perceptrons (MLPs), is increasingly adopting Kolmogorov-Arnold Networks (KANs) for data encoding. This shift is driven by the limitations of MLPs, including poor interpretability, fixed activation functions, and difficulty capturing localized or high-frequency features. KANs address these issues with enhanced interpretability and flexibility, enabling more efficient modeling of complex nonlinear interactions and effectively overcoming the constraints associated with conventional MLP architectures. This review categorizes recent progress in KAN-based models across three distinct perspectives: (i) data-driven learning, (ii) physics-informed modeling, and (iii) deep-operator learning. Each perspective is examined through the lens of architectural design, training strategies, application efficacy, and comparative evaluation against MLP-based counterparts. By benchmarking KANs against MLPs, we highlight consistent improvements in accuracy, convergence, and spectral representation, clarifying KANs' advantages in capturing complex dynamics while learning more effectively. In addition to reviewing recent literature, this work also presents several comparative evaluations that clarify central characteristics of KAN modeling and hint at their potential implications for real-world applications. Finally, this review identifies critical challenges and open research questions in KAN development, particularly regarding computational efficiency, theoretical guarantees, hyperparameter tuning, and algorithm complexity. We also outline future research directions aimed at improving the robustness, scalability, and physical consistency of KAN-based frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22959v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Salah A. Faroughi, Farinaz Mostajeran, Amin Hamed Mashhadzadeh, Shirko Faroughi</dc:creator>
    </item>
    <item>
      <title>A GPU-based Compressible Combustion Solver for Applications Exhibiting Disparate Space and Time Scales</title>
      <link>https://arxiv.org/abs/2510.23993</link>
      <description>arXiv:2510.23993v2 Announce Type: replace-cross 
Abstract: High-speed chemically active flows present significant computational challenges due to their disparate space and time scales, where stiff chemistry often dominates simulation time. While modern supercomputing scientific codes achieve exascale performance by leveraging graphics processing units (GPUs), existing GPU-based compressible combustion solvers face critical limitations in memory management, load balancing, and handling the highly localized nature of chemical reactions. To this end, we present a high-performance compressible reacting flow solver built on the AMReX framework and optimized for multi-GPU settings. Our approach addresses three GPU performance bottlenecks: memory access patterns through column-major storage optimization, computational workload variability via a bulk-sparse integration strategy for chemical kinetics, and multi-GPU load distribution for adaptive mesh refinement applications. The solver adapts existing matrix-based chemical kinetics formulations to multigrid contexts. Using representative combustion applications including hydrogen-air detonations and jet in supersonic crossflow configurations, we demonstrate $2-5\times$ performance improvements over initial GPU implementations with near-ideal weak scaling across $1-96$ NVIDIA H100 GPUs. Roofline analysis reveals substantial improvements in arithmetic intensity for both convection ($\sim 10 \times$) and chemistry ($\sim 4 \times$) routines, confirming efficient utilization of GPU memory bandwidth and computational resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23993v2</guid>
      <category>cs.DC</category>
      <category>cs.CE</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anthony Carreon, Jagmohan Singh, Shivank Sharma, Shuzhi Zhang, Venkat Raman</dc:creator>
    </item>
  </channel>
</rss>
