<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Jun 2025 04:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Research on the recommendation framework of foreign enterprises from the perspective of multidimensional proximity</title>
      <link>https://arxiv.org/abs/2506.17657</link>
      <description>arXiv:2506.17657v1 Announce Type: new 
Abstract: As global economic integration progresses, foreign-funded enterprises play an increasingly crucial role in fostering local economic growth and enhancing industrial development. However, there are not many researches to deal with this aspect in recent years. This study utilizes the multidimensional proximity theory to thoroughly examine the criteria for selecting high-quality foreign-funded companies that are likely to invest in and establish factories in accordance with local conditions during the investment attraction process.First, this study leverages databases such as Wind and Osiris, along with government policy documents, to investigate foreign-funded enterprises and establish a high-quality database. Second, using a two-step method, enterprises aligned with local industrial strategies are identified. Third, a detailed analysis is conducted on key metrics, including industry revenue, concentration (measured by the Herfindahl-Hirschman Index), and geographical distance (calculated using the Haversine formula). Finally, a multi-criteria decision analysis ranks the top five companies as the most suitable candidates for local investment, with the methodology validated through a case study in a district of Beijing.The example results show that the established framework helps local governments identify high-quality foreign-funded enterprises.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17657v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Guoqiang Liang, Jiarui Xie, Mengxuan Li, Shuo Zhang</dc:creator>
    </item>
    <item>
      <title>Capacity Based Design of Slot Array Antennas</title>
      <link>https://arxiv.org/abs/2506.17845</link>
      <description>arXiv:2506.17845v1 Announce Type: new 
Abstract: Historically, the design of antenna arrays has evolved separately from Shannon theory. Shannon theory adopts a probabilistic approach in the design of communication systems, while antenna design approaches have relied on deterministic Maxwell theory alone. In this paper, we introduce a new approach to the design of antenna arrays based on information theoretic metrics. To this end, we develop a statistical model suitable for the numerical optimization of antenna systems. The model is utilized to obtain the signal-to-noise ratio (SNR), find the optimal power allocation scheme, and establish the associated Shannon capacity. We demonstrate the utility of the new approach on a connected array of slot antennas. To find the impedance matrix of the slot array, we further develop a fast numerical technique based on the analytical form of the spectrum of magnetic current. The utilized spectral approach, albeit its simplicity, shows good match compared with full wave electromagnetic simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17845v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Volodymyr Shyianov, Bamelak Tadele, Vladimir I. Okhmatovski, Faouzi Bellili, Amine Mezghani</dc:creator>
    </item>
    <item>
      <title>Area between trajectories: Insights into optimal group selection and trajectory heterogeneity in group-based trajectory modeling</title>
      <link>https://arxiv.org/abs/2506.18108</link>
      <description>arXiv:2506.18108v1 Announce Type: new 
Abstract: Group-based trajectory modeling (GBTM) is commonly used to identify longitudinal patterns in health outcomes among older adults, with determining the optimal number of groups being a crucial step. While statistically grounded criteria are primarily relied upon, clinical relevance is gradually emphasized in medicine to ensure that the identified trajectory heterogeneity appropriately reflects changes in a disease or symptom over time. However, such considerations are often judged through visual comparisons, without concrete approaches for their application. To address this, the Area Between Trajectories (ABTs) was introduced as insights for quantifying trajectory group differences. Using a simulated sleep quality dataset, GBTM was applied to build and compare models. Subsequently, ABTs was demonstrated to show how it works, while also highlighting its limitations and potential applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18108v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yi-Chen Hsiao, Chun-Yuan Chen, Mei-Fen Tang</dc:creator>
    </item>
    <item>
      <title>A Method for Rapid Area Prioritisation in Flood Disaster Response</title>
      <link>https://arxiv.org/abs/2506.18423</link>
      <description>arXiv:2506.18423v1 Announce Type: new 
Abstract: In flood disasters, decision-makers have to rapidly prioritise the areas that need assistance based on a high volume of information. While approaches that combine GIS with Bayesian networks are generally effective in integrating multiple spatial variables and can thus reduce cognitive load, existing models in the literature are not equipped to address the time pressure and information-scape that is typical in a flood. To address the lack of a model for area prioritisation in flood disaster response, we present a novel decision support system that adheres to the time and information characteristics of an ongoing flood to infer the areas with the highest risk. This decision support system is based on a novel GIS-informed Bayesian network model that reflects the challenges of decision-making for area prioritisation. By developing the model during the preparedness phase, some of the most time-consuming aspects of the decision-making process are removed from the time-critical response phase. In this way, the proposed method aims to providing rapid and transparent area prioritisation recommendations for disaster response. To illustrate our method, we present a case study of an extreme flood scenario in Cologne, Germany.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18423v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moritz Schneider, Lukas Halekotte, Tina Comes, Frank Fiedrich</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Multi-Study Robust Factor Model for Analyzing RNA Sequencing Data from Heterogeneous Sources</title>
      <link>https://arxiv.org/abs/2506.18478</link>
      <description>arXiv:2506.18478v1 Announce Type: new 
Abstract: The amount of high-dimensional large-scale RNA sequencing data derived from multiple heterogeneous sources has increased exponentially in biological science. During data collection, significant technical noise or errors may occur. To robustly extract meaningful features from this type of data, we introduce a high-dimensional multi-study robust factor model, called MultiRFM, which learns latent features and accounts for the heterogeneity among sources. MultiRFM demonstrates significantly greater robustness compared to existing multi-study factor models and is capable of estimating study-specific factors that are overlooked by single-study robust factor models. Specifically,we utilize a multivariate t-distribution to model errors, capturing potential heavy tails, and incorporate both study-shared and study-specified factors to represent common and specific information among studies. For parameter estimation, we have designed a computationally efficient variational estimation approach. A step-wise singular value ratio method is proposed to determine the discrete tuning parameters. Extensive simulation studies indicate that MultiRFM surpasses state-of-the-art methods in terms of estimation accuracy across various scenarios. Real-world applications involving two RNA sequencing datasets demonstrate that MultiRFM outperforms competing methods in model fitting, prediction, and computational efficiency, significantly facilitating downstream tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18478v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaolu Jiang, Wei Liu</dc:creator>
    </item>
    <item>
      <title>Bayesian integrative factor analysis methods, with application in nutrition and genomics data</title>
      <link>https://arxiv.org/abs/2506.18479</link>
      <description>arXiv:2506.18479v1 Announce Type: new 
Abstract: High-dimensional data are crucial in biomedical research. Integrating such data from multiple studies is a critical process that relies on the choice of advanced statistical models, enhancing statistical power, reproducibility, and scientific insight compared to analyzing each study separately. Factor analysis (FA) is a core dimensionality reduction technique that models observed data through a small set of latent factors. Bayesian extensions of FA have recently emerged as powerful tools for multi-study integration, enabling researchers to disentangle shared biological signals from study-specific variability. In this tutorial, we provide a practical and comparative guide to five advanced Bayesian integrative factor models: Perturbed Factor Analysis (PFA), Bayesian Factor Regression with non-local spike-and-slab priors (MOM-SS), Subspace Factor Analysis (SUFA), Bayesian Multi-study Factor Analysis (BMSFA), and Bayesian Combinatorial Multi-study Factor Analysis (Tetris). To contextualize these methods, we also include two benchmark approaches: standard FA applied to pooled data (Stack FA) and FA applied separately to each study (Ind FA). We evaluate all methods through extensive simulations, assessing computational efficiency and accuracy in the estimation of loadings and number of factors. To bridge theory and practice, we present a full analytical workflow, with detailed R code, demonstrating how to apply these models to real-world datasets in nutrition and genomics. This tutorial is designed to guide applied researchers through the landscape of Bayesian integrative factor analysis, offering insights and tools for extracting interpretable, robust patterns from complex multi-source data. All code and resources are available at: https://github.com/Mavis-Liang/Bayesian_integrative_FA_tutorial</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18479v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mavis Liang, Blake Hansen, Alejandra Avalos-Pacheco, Roberta De Vito</dc:creator>
    </item>
    <item>
      <title>One-sample survival tests for non-proportional hazards in oncology clinical trial</title>
      <link>https://arxiv.org/abs/2506.18608</link>
      <description>arXiv:2506.18608v1 Announce Type: new 
Abstract: In oncology, well-powered time-to-event randomized clinical trials are challenging due to a limited number of patients (e.g, pediatric cancers or personalized medicine). Last decade, many one- or two-stage designs for single-arm trials (SATs) have emerged as an alternative to overcome this issue. These designs rely on the one-sample log-rank test (OSLRT) and its modified version (mOSLRT) to compare the survival curves of an experimental and an external control group under the proportional hazards (PH) assumption that may be violated. We extend Finkelstein's formulation of OSLRT as a score test under PH by using a piecewise exponential model with change-points (CPs) for early, middle and delayed treatment effects and an accelerated hazards model for crossing hazards. The restricted mean survival time (RMST) based test is adapted to SATs and we also construct a combination test procedure (max-Combo) with correction for multiple testing. The performance of the developed tests (score tests, RMST and max-Combo tests) are evaluated through a simulation study of early, middle, delayed effects and crossing hazards. Findings show that the score tests are as conservative as the OSLRT and have the highest power when the data generation matches with the model. The max-Combo test is an interesting approach when the time-dependent relative treatment effect and/or the values of CPs are unknown. Uncertainty on the survival curve estimate of the external control group and model misspecification may have a significant impact on performance. For illustration, we apply the developed tests on three real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18608v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chlo\'e Szurewsky (U1018), Guosheng Yin (DSAS), Gw\'ena\"el Le Teuff (U1018)</dc:creator>
    </item>
    <item>
      <title>A Practical Introduction to Regression-based Causal Inference in Meteorology (II): Unmeasured confounders</title>
      <link>https://arxiv.org/abs/2506.18652</link>
      <description>arXiv:2506.18652v1 Announce Type: new 
Abstract: One obstacle to ``elevating" correlation to causation is the phenomenon of confounding, i.e., when a correlation between two variables exists because both variables are in fact caused by a third variable. The situation where the confounders are measured is examined in an earlier, accompanying article. Here, it is shown that even when the confounding variables are not measured, it is still possible to estimate the causal effect via a regression-based method that uses the notion of Instrumental Variables. Using meteorological data set, similar to that in the sister article, a number of different estimates of the causal effect are compared and contrasted. It is shown that the Instrumental Variable results based on unmeasured confounders are consistent with those of the sister article where confounders are measured.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18652v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caren Marzban, Yikun Zhang, Nicholas Bond, Michael Richman</dc:creator>
    </item>
    <item>
      <title>A Structural Causal Model for Electronic Device Reliability: From Effects to Counterfactuals</title>
      <link>https://arxiv.org/abs/2506.18663</link>
      <description>arXiv:2506.18663v1 Announce Type: new 
Abstract: Electronic devices exhibit changes in electrical resistance over time at varying rates, depending on the configuration of certain components. Since measuring overall electrical resistance requires partial disassembly, only a limited number of measurements are performed over thousands of operating hours. This leads to censored failure times, whether under natural stress or under accelerated stress conditions. To address these challenges, including device-specific failure thresholds, a parametric structural causal model is developed to extract information from both observational and experimental data, with the aim of estimating causal effects and counterfactuals, regardless of the applied stress regime. Synthetic data are used to illustrate the main findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18663v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico Mattia Stefanini (Department of Environmental Science and Policy, University of Milan, Via Celoria 2, 20133 Milan, Italy), Nedka Dechkova Nikiforova (Department of Statistics Computer Science Applications 'G. Parenti', University of Florence, viale Morgagni 59, Florence, 50134 Florence, Italy), Rossella Berni (Department of Statistics Computer Science Applications 'G. Parenti', University of Florence, viale Morgagni 59, Florence, 50134 Florence, Italy)</dc:creator>
    </item>
    <item>
      <title>A Practical Introduction to Regression-based Causal Inference in Meteorology (I): All confounders measured</title>
      <link>https://arxiv.org/abs/2506.18808</link>
      <description>arXiv:2506.18808v1 Announce Type: new 
Abstract: Whether a variable is the cause of another, or simply associated with it, is often an important scientific question. Causal Inference is the name associated with the body of techniques for addressing that question in a statistical setting. Although assessing causality is relatively straightforward in the presence of temporal information, outside of that setting - the situation considered here - it is more difficult to assess causal effects. The development of the field of causal inference has involved concepts from a wide range of topics, thereby limiting its adoption across some fields, including meteorology. However, at its core, the requisite knowledge for causal inference involves little more than basic probability theory and regression, topics familiar to most meteorologists. By focusing on these core areas, this and a companion article provide a steppingstone for the meteorology community into the field of (non-temporal) causal inference. Although some theoretical foundations are presented, the main goal is the application of a specific method, called matching, to a problem in meteorology. The data for the application are in public domain, and R code is provided as well, forming an easy path for meteorology students and researchers to enter the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18808v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caren Marzban, Yikun Zhang, Nicholas Bond, Michael Richman</dc:creator>
    </item>
    <item>
      <title>CopulaSMOTE: A Copula-Based Oversampling Approach for Imbalanced Classification in Diabetes Prediction</title>
      <link>https://arxiv.org/abs/2506.17326</link>
      <description>arXiv:2506.17326v1 Announce Type: cross 
Abstract: Diabetes mellitus poses a significant health risk, as nearly 1 in 9 people are affected by it. Early detection can significantly lower this risk. Despite significant advancements in machine learning for identifying diabetic cases, results can still be influenced by the imbalanced nature of the data. To address this challenge, our study considered copula-based data augmentation, which preserves the dependency structure when generating data for the minority class and integrates it with machine learning (ML) techniques. We selected the Pima Indian dataset and generated data using A2 copula, then applied four machine learning algorithms: logistic regression, random forest, gradient boosting, and extreme gradient boosting. Our findings indicate that XGBoost combined with A2 copula oversampling achieved the best performance improving accuracy by 4.6%, precision by 15.6%, recall by 20.4%, F1-score by 18.2% and AUC by 25.5% compared to the standard SMOTE method. Furthermore, we statistically validated our results using the McNemar test. This research represents the first known use of A2 copulas for data augmentation and serves as an alternative to the SMOTE technique, highlighting the efficacy of copulas as a statistical method in machine learning applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17326v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agnideep Aich, Md Monzur Murshed, Sameera Hewage, Amanda Mayeaux</dc:creator>
    </item>
    <item>
      <title>The Zeta Tail Distribution: A Novel Event-Count Model</title>
      <link>https://arxiv.org/abs/2506.17496</link>
      <description>arXiv:2506.17496v1 Announce Type: cross 
Abstract: We introduce the Zeta Tail(a) probability distribution as a new model for random damage-event counts in risk analysis. Although readily motivated through a natural relationship with the Geometric(p) distribution, Zeta Tail(a) has received little attention in the scholarly literature. In the present work, we begin by deriving various fundamental properties of this novel distribution. We then assess its usefulness as an alternative to Geometric(p) for event-count data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17496v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael R. Powers</dc:creator>
    </item>
    <item>
      <title>Predicting Stock Market Crash with Bayesian Generalised Pareto Regression</title>
      <link>https://arxiv.org/abs/2506.17549</link>
      <description>arXiv:2506.17549v1 Announce Type: cross 
Abstract: This paper develops a Bayesian Generalised Pareto Regression (GPR) model to forecast extreme losses in Indian equity markets, with a focus on the Nifty 50 index. Extreme negative returns, though rare, can cause significant financial disruption, and accurate modelling of such events is essential for effective risk management. Traditional Generalised Pareto Distribution (GPD) models often ignore market conditions; in contrast, our framework links the scale parameter to covariates using a log-linear function, allowing tail risk to respond dynamically to market volatility. We examine four prior choices for Bayesian regularisation of regression coefficients: Cauchy, Lasso (Laplace), Ridge (Gaussian), and Zellner's g-prior. Simulation results suggest that the Cauchy prior delivers the best trade-off between predictive accuracy and model simplicity, achieving the lowest RMSE, AIC, and BIC values. Empirically, we apply the model to large negative returns (exceeding 5%) in the Nifty 50 index. Volatility measures from the Nifty 50, S&amp;P 500, and gold are used as covariates to capture both domestic and global risk drivers. Our findings show that tail risk increases significantly with higher market volatility. In particular, both S&amp;P 500 and gold volatilities contribute meaningfully to crash prediction, highlighting global spillover and flight-to-safety effects. The proposed GPR model offers a robust and interpretable approach for tail risk forecasting in emerging markets. It improves upon traditional EVT-based models by incorporating real-time financial indicators, making it useful for practitioners, policymakers, and financial regulators concerned with systemic risk and stress testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17549v1</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourish Das</dc:creator>
    </item>
    <item>
      <title>Triadic Novelty: A Typology and Measurement Framework for Recognizing Novel Contributions in Science</title>
      <link>https://arxiv.org/abs/2506.17851</link>
      <description>arXiv:2506.17851v1 Announce Type: cross 
Abstract: Scientific progress depends on novel ideas, but current reward systems often fail to recognize them. Many existing metrics conflate novelty with popularity, privileging ideas that fit existing paradigms over those that challenge them. This study develops a theory-driven framework to better understand how different types of novelty emerge, take hold, and receive recognition. Drawing on network science and theories of discovery, we introduce a triadic typology: Pioneers, who introduce entirely new topics; Mavericks, who recombine distant concepts; and Vanguards, who reinforce weak but promising connections. We apply this typology to a dataset of 41,623 articles in the interdisciplinary field of philanthropy and nonprofit studies, linking novelty types to five-year citation counts using mixed-effects negative binomial regression. Results show that novelty is not uniformly rewarded. Pioneer efforts are foundational but often overlooked. Maverick novelty shows consistent citation benefits, particularly rewarded when it displaces prior focus. Vanguard novelty is more likely to gain recognition when it strengthens weakly connected topics, but its citation advantage diminishes as those reinforced nodes become more central. To enable fair comparison across time and domains, we introduce a simulated baseline model. These findings improve the evaluation of innovations, affecting science policy, funding, and institutional assessment practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17851v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin Ai, Richard S. Steinberg, Chao Guo, Filipi Nascimento Silva</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for Left-Truncated Log-Logistic Distributions for Time-to-event Data Analysis</title>
      <link>https://arxiv.org/abs/2506.17852</link>
      <description>arXiv:2506.17852v1 Announce Type: cross 
Abstract: Parameter estimation is a foundational step in statistical modeling, enabling us to extract knowledge from data and apply it effectively. Bayesian estimation of parameters incorporates prior beliefs with observed data to infer distribution parameters probabilistically and robustly. Moreover, it provides full posterior distributions, allowing uncertainty quantification and regularization, especially useful in small or truncated samples. Utilizing the left-truncated log-logistic (LTLL) distribution is particularly well-suited for modeling time-to-event data where observations are subject to a known lower bound such as precipitation data and cancer survival times. In this paper, we propose a Bayesian approach for estimating the parameters of the LTLL distribution with a fixed truncation point \( x_L &gt; 0 \). Given a random variable \( X \sim LL(\alpha, \beta; x_L) \), where \( \alpha &gt; 0 \) is the scale parameter and \( \beta &gt; 0 \) is the shape parameter, the likelihood function is derived based on a truncated sample \( X_1, X_2, \dots, X_N \) with \( X_i &gt; x_L \). We assume independent prior distributions for the parameters, and the posterior inference is conducted via Markov Chain Monte Carlo sampling, specifically using the Metropolis-Hastings algorithm to obtain posterior estimates \( \hat{\alpha} \) and \( \hat{\beta} \). Through simulation studies and real-world applications, we demonstrate that Bayesian estimation provides more stable and reliable parameter estimates, particularly when the likelihood surface is irregular due to left truncation. The results highlight the advantages of Bayesian inference outperform the estimation of parameter uncertainty in truncated distributions for time to event data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17852v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fahad Mostafa, Md Rejuan Haque, Md Mostafijur Rahman, Farzana Nasrin</dc:creator>
    </item>
    <item>
      <title>Constructing prediction intervals for the age distribution of deaths</title>
      <link>https://arxiv.org/abs/2506.17953</link>
      <description>arXiv:2506.17953v1 Announce Type: cross 
Abstract: We introduce a model-agnostic procedure to construct prediction intervals for the age distribution of deaths. The age distribution of deaths is an example of constrained data, which are nonnegative and have a constrained integral. A centered log-ratio transformation and a cumulative distribution function transformation are used to remove the two constraints, where the latter transformation can also handle the presence of zero counts. Our general procedure divides data samples into training, validation, and testing sets. Within the validation set, we can select an optimal tuning parameter by calibrating the empirical coverage probabilities to be close to their nominal ones. With the selected optimal tuning parameter, we then construct the pointwise prediction intervals using the same models for the holdout data in the testing set. Using Japanese age- and sex-specific life-table death counts, we assess and evaluate the interval forecast accuracy with a suite of functional time-series models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17953v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Han Lin Shang, Steven Haberman</dc:creator>
    </item>
    <item>
      <title>Identifiable Convex-Concave Regression via Sub-gradient Regularised Least Squares</title>
      <link>https://arxiv.org/abs/2506.18078</link>
      <description>arXiv:2506.18078v1 Announce Type: cross 
Abstract: We propose a novel nonparametric regression method that models complex input-output relationships as the sum of convex and concave components. The method-Identifiable Convex-Concave Nonparametric Least Squares (ICCNLS)-decomposes the target function into additive shape-constrained components, each represented via sub-gradient-constrained affine functions. To address the affine ambiguity inherent in convex-concave decompositions, we introduce global statistical orthogonality constraints, ensuring that residuals are uncorrelated with both intercept and input variables. This enforces decomposition identifiability and improves interpretability. We further incorporate L1, L2 and elastic net regularisation on sub-gradients to enhance generalisation and promote structural sparsity. The proposed method is evaluated on synthetic and real-world datasets, including healthcare pricing data, and demonstrates improved predictive accuracy and model simplicity compared to conventional CNLS and difference-of-convex (DC) regression approaches. Our results show that statistical identifiability, when paired with convex-concave structure and sub-gradient regularisation, yields interpretable models suited for forecasting, benchmarking, and policy evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18078v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Chung</dc:creator>
    </item>
    <item>
      <title>Statistical Multicriteria Evaluation of LLM-Generated Text</title>
      <link>https://arxiv.org/abs/2506.18082</link>
      <description>arXiv:2506.18082v1 Announce Type: cross 
Abstract: Assessing the quality of LLM-generated text remains a fundamental challenge in natural language processing. Current evaluation approaches often rely on isolated metrics or simplistic aggregations that fail to capture the nuanced trade-offs between coherence, diversity, fluency, and other relevant indicators of text quality. In this work, we adapt a recently proposed framework for statistical inference based on Generalized Stochastic Dominance (GSD) that addresses three critical limitations in existing benchmarking methodologies: the inadequacy of single-metric evaluation, the incompatibility between cardinal automatic metrics and ordinal human judgments, and the lack of inferential statistical guarantees. The GSD-front approach enables simultaneous evaluation across multiple quality dimensions while respecting their different measurement scales, building upon partial orders of decoding strategies, thus avoiding arbitrary weighting of the involved metrics. By applying this framework to evaluate common decoding strategies against human-generated text, we demonstrate its ability to identify statistically significant performance differences while accounting for potential deviations from the i.i.d. assumption of the sampling design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18082v1</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Esteban Garces Arias, Hannah Blocher, Julian Rodemann, Matthias A{\ss}enmacher, Christoph Jansen</dc:creator>
    </item>
    <item>
      <title>Multivariate Statistical Analysis of Exoplanet Habitability: Detection Bias and Earth Analog Identification</title>
      <link>https://arxiv.org/abs/2506.18200</link>
      <description>arXiv:2506.18200v1 Announce Type: cross 
Abstract: We present a comprehensive multivariate statistical analysis of 517 exoplanets from the NASA Exoplanet Archive to identify potentially habitable worlds and quantify detection bias in current surveys. Using eight key parameters (planetary radius, equilibrium temperature, insolation flux, density, and stellar effective temperature, radius, mass, metallicity), we developed a classification framework that successfully identifies Earth as an "Excellent Candidate" for habitability. Our analysis reveals that only 0.6% (3 planets including Earth) meet all habitability criteria under relaxed thresholds, while 75.0% exhibit "Good Star, Poor Planet" characteristics, indicating significant observational bias toward unsuitable planetary systems. Hotelling's T2 test demonstrates that potentially habitable planets are statistically significantly different from the general exoplanet population (p = 0.015). Mahalanobis distance analysis places Earth in the 69.4th percentile for statistical unusualness, confirming that Earth-like planets are genuine outliers in parameter space. We identify Kepler-22 b as a compelling Earth analog with remarkable parameter similarity, and reveal that 1.2% of planets represent "edge cases" orbiting M-dwarf stars with suitable planetary but marginal stellar conditions. These findings demonstrate systematic detection bias in exoplanet surveys and provide quantitative evidence for the rarity of Earth-like worlds while identifying high-priority targets for atmospheric characterization with JWST.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18200v1</guid>
      <category>astro-ph.EP</category>
      <category>astro-ph.IM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caleb Traxler, Samuel Townsend, Abby Mori, Grace Newman, Kaitlyn Morenzone</dc:creator>
    </item>
    <item>
      <title>Semantic similarity estimation for domain specific data using BERT and other techniques</title>
      <link>https://arxiv.org/abs/2506.18602</link>
      <description>arXiv:2506.18602v1 Announce Type: cross 
Abstract: Estimation of semantic similarity is an important research problem both in natural language processing and the natural language understanding, and that has tremendous application on various downstream tasks such as question answering, semantic search, information retrieval, document clustering, word-sense disambiguation and machine translation. In this work, we carry out the estimation of semantic similarity using different state-of-the-art techniques including the USE (Universal Sentence Encoder), InferSent and the most recent BERT, or Bidirectional Encoder Representations from Transformers, models. We use two question pairs datasets for the analysis, one is a domain specific in-house dataset and the other is a public dataset which is the Quora's question pairs dataset. We observe that the BERT model gave much superior performance as compared to the other methods. This should be because of the fine-tuning procedure that is involved in its training process, allowing it to learn patterns based on the training data that is used. This works demonstrates the applicability of BERT on domain specific datasets. We infer from the analysis that BERT is the best technique to use in the case of domain specific data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18602v1</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R. Prashanth</dc:creator>
    </item>
    <item>
      <title>100-Day Analysis of USD/IDR Exchange Rate Dynamics Around the 2025 U.S. Presidential Inauguration</title>
      <link>https://arxiv.org/abs/2506.18738</link>
      <description>arXiv:2506.18738v1 Announce Type: cross 
Abstract: Using a 100-day symmetric window around the January 2025 U.S. presidential inauguration, non-parametric statistical methods with bootstrap resampling (10,000 iterations) analyze distributional properties and anomalies. Results indicate a statistically significant 3.61\% Indonesian rupiah depreciation post-inauguration, with a large effect size (Cliff's Delta $= -0.9224$, CI: $[-0.9727, -0.8571]$). Central tendency shifted markedly, yet volatility remained stable (variance ratio $= 0.9061$, $p = 0.504$). Four significant anomalies exhibiting temporal clustering are detected. These findings provide quantitative evidence of political transition effects on emerging market currencies, highlighting implications for monetary policy and currency risk management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18738v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sandy H. S. Herho, Siti N. Kaban, Cahya Nugraha</dc:creator>
    </item>
    <item>
      <title>NFL Ghosts: A framework for evaluating defender positioning with conditional density estimation</title>
      <link>https://arxiv.org/abs/2406.17220</link>
      <description>arXiv:2406.17220v2 Announce Type: replace 
Abstract: Player attribution in American football remains an open problem due to the complex nature of twenty-two players interacting on the field, but the granularity of player tracking data provides ample opportunity for novel approaches. In this work, we introduce the first public framework to evaluate spatial and trajectory tracking data of players relative to a baseline distribution of "ghost" defenders. We demonstrate our framework in the context of modeling the nearest defender positioning at the moment of catch. In particular, we provide estimates of how much better or worse their observed positioning and trajectory compared to the expected play value of ghost defenders. Our framework leverages multi-dimensional tracking data features through flexible random forests for conditional density estimation in two ways: (1) to model the distribution of receiver yards gained enabling the estimation of within-play expected value, and (2) to model the 2D spatial distribution of baseline ghost defenders. We present novel metrics for measuring player and team performance based on tracking data, and discuss challenges that remain in extending our framework to other aspects of American football.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17220v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronald Yurko, Quang Nguyen, Konstantinos Pelechrinis</dc:creator>
    </item>
    <item>
      <title>A Synthetic Texas Power System with Time-Series Weather-Dependent Spatiotemporal Profiles</title>
      <link>https://arxiv.org/abs/2302.13231</link>
      <description>arXiv:2302.13231v3 Announce Type: replace-cross 
Abstract: We developed a synthetic Texas 123-bus backbone transmission system (TX-123BT) with spatio-temporally correlated grid profiles of solar power, wind power, dynamic line ratings and loads at one-hour resolution for five continuous years, which demonstrates unique advantages compared to conventional test cases that offer single static system profile snapshots. Three weather-dependent models are used to create the hourly wind power productions, solar power productions, and dynamic line ratings respectively. The actual historical weather information is also provided along with this dataset, which is suitable for machine learning models. Security-constrained unit commitment is conducted on TX-123BT daily grid profiles and numerical results are compared with the actual Texas system for validation. The created hourly DLR profiles can cut operating cost from USD 8.09 M to USD 7.95 M (-1.7 %), raises renewable dispatch by 1.3 %, and lowers average LMPs from USD 18.66 to USD 17.98 /MWh (-3.6 %). Two hydrogen options -- a 200 MW dual hub and a 500 MW hydrogen-energy transmission and conversion system -- reduce high-load Q3 daily costs by 13.9 % and 14.1 %, respectively. Sensitivity tests show that suppressing the high-resolution weather-driven profiles can push system cost up by as much as 15 %, demonstrating the economic weight of temporal detail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.13231v3</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jin Lu, Xingpeng Li, Hongyi Li, Taher Chegini, Carlos Gamarra, Y. C. Ethan Yang, Margaret Cook, Gavin Dillingham</dc:creator>
    </item>
    <item>
      <title>A Bayesian Non-parametric Approach to Generative Models: Integrating Variational Autoencoder and Generative Adversarial Networks using Wasserstein and Maximum Mean Discrepancy</title>
      <link>https://arxiv.org/abs/2308.14048</link>
      <description>arXiv:2308.14048v2 Announce Type: replace-cross 
Abstract: We propose a novel generative model within the Bayesian non-parametric learning (BNPL) framework to address some notable failure modes in generative adversarial networks (GANs) and variational autoencoders (VAEs)--these being overfitting in the GAN case and noisy samples in the VAE case. We will demonstrate that the BNPL framework enhances training stability and provides robustness and accuracy guarantees when incorporating the Wasserstein distance and maximum mean discrepancy measure (WMMD) into our model's loss function. Moreover, we introduce a so-called ``triple model'' that combines the GAN, the VAE, and further incorporates a code-GAN (CGAN) to explore the latent space of the VAE. This triple model design generates high-quality, diverse samples, while the BNPL framework, leveraging the WMMD loss function, enhances training stability. Together, these components enable our model to achieve superior performance across various generative tasks. These claims are supported by both theoretical analyses and empirical validation on a wide variety of datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.14048v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Forough Fazeli-Asl, Michael Minyi Zhang</dc:creator>
    </item>
    <item>
      <title>Stylized Facts of High-Frequency Bitcoin Time Series</title>
      <link>https://arxiv.org/abs/2402.11930</link>
      <description>arXiv:2402.11930v2 Announce Type: replace-cross 
Abstract: This paper analyses the high-frequency intraday Bitcoin dataset from 2019 to 2022. During this time frame, the Bitcoin market index exhibited two distinct periods, 2019-20 and 2021-22, characterized by an abrupt change in volatility. The Bitcoin price returns for both periods can be described by an anomalous diffusion process, transitioning from subdiffusion for short intervals to weak superdiffusion over longer time intervals. The characteristic features related to this anomalous behavior studied in the present paper include heavy tails, which can be described using a $q$-Gaussian distribution and correlations. When we sample the autocorrelation of absolute returns, we observe a power-law relationship, indicating time dependence in both periods initially. The ensemble autocorrelation of the returns decays rapidly. We fitted the autocorrelation with a power law to capture the decay and found that the second period experienced a slightly higher decay rate. The further study involves the analysis of endogenous effects within the Bitcoin time series, which are examined through detrending analysis. We found that both periods are multifractal and present self-similarity in the detrended probability density function (PDF). The Hurst exponent over short time intervals shifts from less than 0.5 ($\sim$ 0.42) in Period 1 to closer to 0.5 in Period 2 ($\sim$ 0.49), indicating that the market has gained efficiency over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11930v2</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yaoyue Tang, Karina Arias-Calluari, M. N. Najafi, Michael S. Harr\'e, Fernando Alonso-Marroquin</dc:creator>
    </item>
    <item>
      <title>Accurate early detection of Parkinson's disease from SPECT imaging through Convolutional Neural Networks</title>
      <link>https://arxiv.org/abs/2412.05348</link>
      <description>arXiv:2412.05348v2 Announce Type: replace-cross 
Abstract: Early and accurate detection of Parkinson's disease (PD) is a crucial diagnostic challenge carrying immense clinical significance, for effective treatment regimens and patient management. For instance, a group of subjects termed SWEDD who are clinically diagnosed as PD, but show normal Single Photon Emission Computed Tomography (SPECT) scans, change their diagnosis as non-PD after few years of follow up, and in the meantime, they are treated with PD medications which do more harm than good. In this work, machine learning models are developed using features from SPECT images to detect early PD and SWEDD subjects from normal. These models were observed to perform with high accuracy. It is inferred from the study that these diagnostic models carry potential to help PD clinicians in the diagnostic process</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05348v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.36922/AIH025040005</arxiv:DOI>
      <dc:creator>R. Prashanth</dc:creator>
    </item>
    <item>
      <title>Explainable Linear and Generalized Linear Models by the Predictions Plot</title>
      <link>https://arxiv.org/abs/2412.16980</link>
      <description>arXiv:2412.16980v2 Announce Type: replace-cross 
Abstract: Many statistics courses cover multiple linear regression, and present students with the formula of a prediction using the regressors, slopes, and an intercept. But is it really easy to see which terms have the largest effect, or to explain why the prediction of a specific case is unusually high or low? To assist with this the so-called predictions plot is proposed. Its simplicity makes it easy to interpret, and it combines much information. Its main benefit is that it helps explainability of the prediction formula as it is, without depending on how the formula was derived. The input variables can be numerical or categorical; interaction terms are also handled, and the model can be linear or generalized linear. Another display is proposed to visualize correlations between prediction terms, in a way that is tailored for this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16980v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Peter J. Rousseeuw</dc:creator>
    </item>
    <item>
      <title>Robust local empirical Bayes correction for Bayesian modeling</title>
      <link>https://arxiv.org/abs/2503.06837</link>
      <description>arXiv:2503.06837v2 Announce Type: replace-cross 
Abstract: This paper investigates a robust empirical Bayes correction for Bayesian modeling. We show the application of the model on income distribution. Income shock includes temporal and permanent shocks. We aim to eliminate temporal shock and permanent shock using two-step local empirical correction method. Our results show that only 6.7% of the observed income shocks were permanent shock, and the posterior (permanent) mean weekly income was reduced from the observed income 415 pounds to 202 pounds for the United Kingdom using the Living Costs and Food Survey in 2021-2022. Keywords: Empirical Bayes correction; Outliers; Bayesian modeling</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06837v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoshiko Hayashi</dc:creator>
    </item>
  </channel>
</rss>
