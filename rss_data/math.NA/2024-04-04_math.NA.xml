<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.NA</link>
    <description>math.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Apr 2024 04:01:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 04 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A fast cosine transformation accelerated method for predicting effective thermal conductivity</title>
      <link>https://arxiv.org/abs/2404.02433</link>
      <description>arXiv:2404.02433v1 Announce Type: new 
Abstract: Predicting effective thermal conductivity by solving a Partial Differential Equation (PDE) defined on a high-resolution Representative Volume Element (RVE) is a computationally intensive task. In this paper, we tackle the task by proposing an efficient and implementation-friendly computational method that can fully leverage the computing power offered by hardware accelerators, namely, graphical processing units (GPUs). We first employ the Two-Point Flux-Approximation scheme to discretize the PDE and then utilize the preconditioned conjugate gradient method to solve the resulting algebraic linear system. The construction of the preconditioner originates from FFT-based homogenization methods, and an engineered linear programming technique is utilized to determine the homogeneous reference parameters. The fundamental observation presented in this paper is that the preconditioner system can be effectively solved using multiple Fast Cosine Transformations (FCT) and parallel tridiagonal matrix solvers. Regarding the fact that default multiple FCTs are unavailable on the CUDA platform, we detail how to derive FCTs from FFTs with nearly optimal memory usage. Numerical experiments including the stability comparison with standard preconditioners are conducted for 3D RVEs. Our performance reports indicate that the proposed method can achieve a $5$-fold acceleration on the GPU platform over the pure CPU platform and solve the problems with $512^3$ degrees of freedom and reasonable contrast ratios in less than $30$ seconds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02433v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Changqing Ye, Shubin Fu, Eric T. Chung</dc:creator>
    </item>
    <item>
      <title>A Neural Multigrid Solver for Helmholtz Equations with High Wavenumber and Heterogeneous Media</title>
      <link>https://arxiv.org/abs/2404.02493</link>
      <description>arXiv:2404.02493v1 Announce Type: new 
Abstract: Solving high-wavenumber and heterogeneous Helmholtz equations presents a long-standing challenge in scientific computing. In this paper, we introduce a deep learning-enhanced multigrid solver to address this issue. By conducting error analysis on standard multigrid applied to a discrete Helmholtz equation, we devise a strategy to handle errors with different frequencies separately.
  For error components with frequencies distant from the wavenumber, we perform simple smoothing based on local operations at different levels to eliminate them.
  On the other hand, to address error components with frequencies near the wavenumber, we utilize another multigrid V-cycle to solve an advection-diffusion-reaction (ADR) equation at a coarse scale.
  The resulting solver, named Wave-ADR-NS, involves parameters learned through unsupervised training.
  Numerical results demonstrate that Wave-ADR-NS effectively resolves heterogeneous 2D Helmholtz equation with wavenumber up to 2000. Comparative experiments against classical multigrid preconditioners and existing deep learning-based multigrid preconditioners reveals the superior performance of Wave-ADR-NS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02493v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Cui, Kai Jiang, Shi Shu</dc:creator>
    </item>
    <item>
      <title>Space-time parallel scaling of Parareal with a Fourier Neural Operator as coarse propagator</title>
      <link>https://arxiv.org/abs/2404.02521</link>
      <description>arXiv:2404.02521v1 Announce Type: new 
Abstract: Iterative parallel-in-time algorithms like Parareal can extend scaling beyond the saturation of purely spatial parallelization when solving initial value problems. However, they require the user to build coarse models to handle the inevitably serial transport of information in time.This is a time consuming and difficult process since there is still only limited theoretical insight into what constitutes a good and efficient coarse model. Novel approaches from machine learning to solve differential equations could provide a more generic way to find coarse level models for parallel-in-time algorithms. This paper demonstrates that a physics-informed Fourier Neural Operator (PINO) is an effective coarse model for the parallelization in time of the two-asset Black-Scholes equation using Parareal. We demonstrate that PINO-Parareal converges as fast as a bespoke numerical coarse model and that, in combination with spatial parallelization by domain decomposition, it provides better overall speedup than both purely spatial parallelization and space-time parallelizaton with a numerical coarse propagator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02521v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdul Qadir Ibrahim, Sebastian G\"otschel, Daniel Ruprecht</dc:creator>
    </item>
    <item>
      <title>Adaptive hp-Polynomial Based Sparse Grid Collocation Algorithms for Piecewise Smooth Functions with Kinks</title>
      <link>https://arxiv.org/abs/2404.02556</link>
      <description>arXiv:2404.02556v1 Announce Type: new 
Abstract: High-dimensional interpolation problems appear in various applications of uncertainty quantification, stochastic optimization and machine learning. Such problems are computationally expensive and request the use of adaptive grid generation strategies like anisotropic sparse grids to mitigate the curse of dimensionality. However, it is well known that the standard dimension-adaptive sparse grid method converges very slowly or even fails in the case of non-smooth functions. For piecewise smooth functions with kinks, we construct two novel hp-adaptive sparse grid collocation algorithms that combine low-order basis functions with local support in parts of the domain with less regularity and variable-order basis functions elsewhere. Spatial refinement is realized by means of a hierarchical multivariate knot tree which allows the construction of localised hierarchical basis functions with varying order. Hierarchical surplus is used as an error indicator to automatically detect the non-smooth region and adaptively refine the collocation points there. The local polynomial degrees are optionally selected by a greedy approach or a kink detection procedure. Three numerical benchmark examples with different dimensions are discussed and comparison with locally linear and highest degree basis functions are given to show the efficiency and accuracy of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02556v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hendrik Wilka, Jens Lang</dc:creator>
    </item>
    <item>
      <title>Goal-oriented time adaptivity for port-Hamiltonian systems</title>
      <link>https://arxiv.org/abs/2404.02641</link>
      <description>arXiv:2404.02641v1 Announce Type: new 
Abstract: Port-Hamiltonian systems provide an energy-based modeling paradigm for dynamical input-state-output systems. At their core, they fulfill an energy balance relating stored, dissipated and supplied energy. To accurately resolve this energy balance in time discretizations, we propose an adaptive grid refinement technique based on a posteriori error estimation. The evaluation of the error estimator includes the computation of adjoint sensitivities. To interpret this adjoint equation as a backwards-in-time equation, we show piecewise weak differentiability of the dual variable. Then, leveraging dissipativity of the port-Hamiltonian dynamics, we present a parallelizable approximation of the underlying adjoint system in the spirit of a block-Jacobi method to efficiently compute error indicators. We illustrate the performance of the proposed scheme by means of numerical experiments showing that it yields a smaller violation of the energy balance when compared to uniform refinements and traditional step-size controlled time stepping.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02641v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andreas Bartel, Manuel Schaller</dc:creator>
    </item>
    <item>
      <title>Coarse spaces for non-symmetric two-level preconditioners based on local generalized eigenproblems</title>
      <link>https://arxiv.org/abs/2404.02758</link>
      <description>arXiv:2404.02758v1 Announce Type: new 
Abstract: Domain decomposition (DD) methods are a natural way to take advantage of parallel computers when solving large scale linear systems. Their scalability depends on the design of the coarse space used in the two-level method. The analysis of adaptive coarse spaces we present here is quite general since it applies to symmetric and non symmetric problems, to symmetric preconditioners such the additive Schwarz method (ASM) and to the non-symmetric preconditioner restricted additive Schwarz (RAS), as well as to exact or inexact subdomain solves. The coarse space is built by solving generalized eigenvalues in the subdomains and applying a well-chosen operator to the selected eigenvectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02758v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fr\'ed\'eric Nataf, Emile Parolin</dc:creator>
    </item>
    <item>
      <title>Locking-free hybrid high-order method for linear elasticity</title>
      <link>https://arxiv.org/abs/2404.02768</link>
      <description>arXiv:2404.02768v1 Announce Type: new 
Abstract: The hybrid-high order (HHO) scheme has many successful applications including linear elasticity as the first step towards computational solid mechanics. The striking advantage is the simplicity among other higher-order nonconforming schemes and its geometric flexibility as a polytopal method on the expanse of a parameter-free refined stabilization. The classical suggestion of a locking-free HHO discretization requires a split of the the reconstruction terms with an additional reconstruction of the divergence operator that might be motivated by the Stokes equations for the robust approximation in the incompressible limit, when one Lam\'e parameter $\lambda\to\infty$ becomes very large. This paper utilizes just one reconstruction operator for the linear Green strain and therefore does not rely on a split in deviatoric and spherical behavior. The a priori error analysis provides quasi-best approximation with $\lambda$-independent equivalence constants. The reliable and (up to data oscillations) efficient a posteriori error estimates are stabilization-free and $\lambda$-robust. The error analysis is carried out on simplicial meshes to allow conforming piecewise polynomials finite elements in the kernel of the stabilization terms. Numerical benchmarks provide empirical evidence for optimal convergence rates of the a posteriori error estimator in some associated adaptive mesh-refining algorithm also in the incompressible limit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02768v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carsten Carstensen, Ngoc Tien Tran</dc:creator>
    </item>
    <item>
      <title>Proper Implicit Discretization of Arbitrary-Order Robust Exact Differentiators</title>
      <link>https://arxiv.org/abs/2404.02770</link>
      <description>arXiv:2404.02770v1 Announce Type: new 
Abstract: This paper considers the implicit Euler discretization of Levant's arbitrary order robust exact differentiator in presence of sampled measurements. Existing implicit discretizations of that differentiator are shown to exhibit either unbounded bias errors or, surprisingly, discretization chattering despite the use of the implicit discretization. A new, proper implicit discretization that exhibits neither of these two detrimental effects is proposed by computing the differentiator's outputs as appropriately designed linear combinations of its state variables. A numerical differentiator implementation is discussed and closed-form stability conditions for arbitrary differentiation orders are given. The influence of bounded measurement noise and numerical approximation errors is formally analyzed. Numerical simulations confirm the obtained results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02770v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Richard Seeber</dc:creator>
    </item>
    <item>
      <title>Residual-Based a Posteriori Error Estimators for Algebraic Stabilizations</title>
      <link>https://arxiv.org/abs/2404.02804</link>
      <description>arXiv:2404.02804v1 Announce Type: new 
Abstract: In this note, we extend the analysis for the residual-based a posteriori error estimators in the energy norm defined for the algebraic flux correction (AFC) schemes [Jha20.CAMWA] to the newly proposed algebraic stabilization schemes [JK21.NM, Kn23.NA]. Numerical simulations on adaptively refined grids are performed in two dimensions showing the higher efficiency of an algebraic stabilization with similar accuracy compared with an AFC scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02804v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhinav Jha</dc:creator>
    </item>
    <item>
      <title>Characterization of Matrices Satisfying the Reverse Order Law for the Moore-Penrose Pseudoinverse</title>
      <link>https://arxiv.org/abs/2404.02843</link>
      <description>arXiv:2404.02843v1 Announce Type: new 
Abstract: We give a constructive characterization of matrices satisfying the reverse-order law for the Moore--Penrose pseudoinverse. In particular, for a given matrix $A$ we construct another matrix $B$, of arbitrary compatible size and chosen rank, in terms of the right singular vectors of $A$, such that the reverse order law for $AB$ is satisfied. Moreover, we show that any matrix satisfying this law comes from a similar construction. As a consequence, several equivalent conditions to $B^+ A^+$ being a pseudoinverse of $AB$ are given, for example $\mathcal{C}(A^*AB)=\mathcal{C}(BB^*A^*)$ or $B\left(AB\right)^+A$ being an orthogonal projection. In addition, we parameterize all possible SVD decompositions of a fixed matrix and give Greville-like equivalent conditions for $B^+A^+$ being a $\{1,2\}-$inverse of $AB$, with a geometric insight in terms of the principal angles between $\mathcal{C}(A^*)$ and $\mathcal{C}(B)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02843v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oskar K\k{e}dzierski</dc:creator>
    </item>
    <item>
      <title>On Properties of Adjoint Systems for Evolutionary PDEs</title>
      <link>https://arxiv.org/abs/2404.02320</link>
      <description>arXiv:2404.02320v1 Announce Type: cross 
Abstract: We investigate the geometric structure of adjoint systems associated with evolutionary partial differential equations at the fully continuous, semi-discrete, and fully discrete levels and the relations between these levels. We show that the adjoint system associated with an evolutionary partial differential equation has an infinite-dimensional Hamiltonian structure, which is useful for connecting the fully continuous, semi-discrete, and fully discrete levels. We subsequently address the question of discretize-then-optimize versus optimize-then-discrete for both semi-discretization and time integration, by characterizing the commutativity of discretize-then-optimize methods versus optimize-then-discretize methods uniquely in terms of an adjoint-variational quadratic conservation law. For Galerkin semi-discretizations and one-step time integration methods in particular, we explicitly construct these commuting methods by using structure-preserving discretization techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02320v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian K. Tran, Ben S. Southworth, Melvin Leok</dc:creator>
    </item>
    <item>
      <title>cppdlr: Imaginary time calculations using the discrete Lehmann representation</title>
      <link>https://arxiv.org/abs/2404.02334</link>
      <description>arXiv:2404.02334v1 Announce Type: cross 
Abstract: We introduce cppdlr, a C++ library implementing the discrete Lehmann representation (DLR) of functions in imaginary time and Matsubara frequency, such as Green's functions and self-energies. The DLR is based on a low-rank approximation of the analytic continuation kernel, and yields a compact and explicit basis consisting of exponentials in imaginary time and simple poles in Matsubara frequency. cppdlr constructs the DLR basis and associated interpolation grids, and implements standard operations. It provides a flexible yet high-level interface, facilitating the incorporation of the DLR into both small-scale applications and existing large-scale software projects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02334v1</guid>
      <category>physics.comp-ph</category>
      <category>cond-mat.str-el</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jason Kaye, Hugo U. R. Strand, Nils Wentzell</dc:creator>
    </item>
    <item>
      <title>Speed, power and cost implications for GPU acceleration of Computational Fluid Dynamics on HPC systems</title>
      <link>https://arxiv.org/abs/2404.02482</link>
      <description>arXiv:2404.02482v1 Announce Type: cross 
Abstract: Computational Fluid Dynamics (CFD) is the simulation of fluid flow undertaken with the use of computational hardware. The underlying equations are computationally challenging to solve and necessitate high performance computing (HPC) to resolve in a practical timeframe when a reasonable level of fidelity is required. The simulations are memory intensive, having previously been limited to central processing unit (CPU) solvers, as graphics processing unit (GPU) video random access memory (VRAM) was insufficient. However, with recent developments in GPU design and increases to VRAM, GPU acceleration of CPU solved workflows is now possible. At HPC scale however, many operational details are still unknown. This paper utilizes ANSYS Fluent, a leading commercial code in CFD, to investigate the compute speed, power consumption and service unit (SU) cost considerations for the GPU acceleration of CFD workflows on HPC architectures. To provide a comprehensive analysis, different CPU architectures, and GPUs have been assessed. It is seen that GPU compute speed is faster, however, the initialisation speed, power and cost performance is less clear cut. Whilst the larger A100 cards perform well with respect to power consumption, this is not observed for the V100 cards. In situations where more than one GPU is required, their adoption may not be beneficial from a power or cost perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02482v1</guid>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>cs.PF</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zachary Cooper-Baldock, Brenda Vara Almirall, Kiao Inthavong</dc:creator>
    </item>
    <item>
      <title>Analysis and approximation to parabolic optimal control problems with measure-valued controls in time</title>
      <link>https://arxiv.org/abs/2404.02546</link>
      <description>arXiv:2404.02546v1 Announce Type: cross 
Abstract: In this paper, we investigate an optimal control problem governed by parabolic equations with measure-valued controls over time. We establish the well-posedness of the optimal control problem and derive the first-order optimality condition using Clarke's subgradients, revealing a sparsity structure in time for the optimal control. Consequently, these optimal control problems represent a generalization of impulse control for evolution equations. To discretize the optimal control problem, we employ the space-time finite element method. Here, the state equation is approximated using piecewise linear and continuous finite elements in space, alongside a Petrov-Galerkin method utilizing piecewise constant trial functions and piecewise linear and continuous test functions in time. The control variable is discretized using the variational discretization concept. For error estimation, we initially derive a priori error estimates and stabilities for the finite element discretizations of the state and adjoint equations. Subsequently, we establish weak-* convergence for the control under the norm $\mathcal{M}(\bar I_c;L^2(\omega))$, with a convergence order of $O(h^\frac{1}{2}+\tau^\frac{1}{4})$ for the state.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02546v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Wei Gong, Dongdong Liang</dc:creator>
    </item>
    <item>
      <title>Some properties of a modified Hilbert transform</title>
      <link>https://arxiv.org/abs/2404.02609</link>
      <description>arXiv:2404.02609v1 Announce Type: cross 
Abstract: Recently, Steinbach et al. introduced a novel operator $\mathcal{H}_T: L^2(0,T) \to L^2(0,T)$, known as the modified Hilbert transform. This operator has shown its significance in space-time formulations related to the heat and wave equations. In this paper, we establish a direct connection between the modified Hilbert transform $\mathcal{H}_T$ and the canonical Hilbert transform $\mathcal{H}$. Specifically, we prove the relationship $\mathcal{H}_T \varphi = -\mathcal{H} \tilde{\varphi}$, where $\varphi \in L^2(0,T)$ and $\tilde{\varphi}$ is a suitable extension of $\varphi$ over the entire $\mathbb{R}$. By leveraging this crucial result, we derive some properties of $\mathcal{H}_T$, including a new inversion formula, that emerge as immediate consequences of well-established findings on $\mathcal{H}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02609v1</guid>
      <category>math.CA</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Matteo Ferrari</dc:creator>
    </item>
    <item>
      <title>MODNO: Multi Operator Learning With Distributed Neural Operators</title>
      <link>https://arxiv.org/abs/2404.02892</link>
      <description>arXiv:2404.02892v1 Announce Type: cross 
Abstract: The study of operator learning involves the utilization of neural networks to approximate operators. Traditionally, the focus has been on single-operator learning (SOL). However, recent advances have rapidly expanded this to include the approximation of multiple operators using foundation models equipped with millions or billions of trainable parameters, leading to the research of multi-operator learning (MOL). In this paper, we present a novel distributed training approach aimed at enabling a single neural operator with significantly fewer parameters to effectively tackle multi-operator learning challenges, all without incurring additional average costs. Our method is applicable to various Chen-Chen-type neural operators, such as Deep Operator Neural Networks (DON). The core idea is to independently learn the output basis functions for each operator using its dedicated data, while simultaneously centralizing the learning of the input function encoding shared by all operators using the entire dataset. Through a systematic study of five numerical examples, we compare the accuracy and cost of training a single neural operator for each operator independently versus training a MOL model using our proposed method. Our results demonstrate enhanced efficiency and satisfactory accuracy. Moreover, our approach illustrates that some operators with limited data can be more effectively constructed with the aid of data from analogous operators through MOL learning. This highlights another MOL's potential to bolster operator learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02892v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zecheng Zhang</dc:creator>
    </item>
    <item>
      <title>One-dimensional Tensor Network Recovery</title>
      <link>https://arxiv.org/abs/2207.10665</link>
      <description>arXiv:2207.10665v3 Announce Type: replace 
Abstract: We study the recovery of the underlying graphs or permutations for tensors in the tensor ring or tensor train format. Our proposed algorithms compare the matricization ranks after down-sampling, whose complexity is $O(d\log d)$ for $d$-th order tensors. We prove that our algorithms can almost surely recover the correct graph or permutation when tensor entries can be observed without noise. We further establish the robustness of our algorithms against observational noise. The theoretical results are validated by numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.10665v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziang Chen, Jianfeng Lu, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Dictionary-based model reduction for state estimation</title>
      <link>https://arxiv.org/abs/2303.10771</link>
      <description>arXiv:2303.10771v3 Announce Type: replace 
Abstract: We consider the problem of state estimation from a few linear measurements, where the state to recover is an element of the manifold $\mathcal{M}$ of solutions of a parameter-dependent equation. The state is estimated using prior knowledge on $\mathcal{M}$ coming from model order reduction. Variational approaches based on linear approximation of $\mathcal{M}$, such as PBDW, yields a recovery error limited by the Kolmogorov width of $\mathcal{M}$. To overcome this issue, piecewise-affine approximations of $\mathcal{M}$ have also been considered, that consist in using a library of linear spaces among which one is selected by minimizing some distance to $\mathcal{M}$. In this paper, we propose a state estimation method relying on dictionary-based model reduction, where a space is selected from a library generated by a dictionary of snapshots, using a distance to the manifold. The selection is performed among a set of candidate spaces obtained from a set of $\ell_1$-regularized least-squares problems. Then, in the framework of parameter-dependent operator equations (or PDEs) with affine parametrizations, we provide an efficient offline-online decomposition based on randomized linear algebra, that ensures efficient and stable computations while preserving theoretical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.10771v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anthony Nouy, Alexandre Pasco</dc:creator>
    </item>
    <item>
      <title>A hybrid neural-network and MAC scheme for Stokes interface problems</title>
      <link>https://arxiv.org/abs/2306.06333</link>
      <description>arXiv:2306.06333v3 Announce Type: replace 
Abstract: In this paper, we present a hybrid neural-network and MAC (Marker-And-Cell) scheme for solving Stokes equations with singular forces on an embedded interface in regular domains. As known, the solution variables (the pressure and velocity) exhibit non-smooth behaviors across the interface so extra discretization efforts must be paid near the interface in order to have small order of local truncation errors in finite difference schemes. The present hybrid approach avoids such additional difficulty. It combines the expressive power of neural networks with the convergence of finite difference schemes to ease the code implementation and to achieve good accuracy at the same time. The key idea is to decompose the solution into singular and regular parts. The neural network learning machinery incorporating the given jump conditions finds the singular part solution, while the standard MAC scheme is used to obtain the regular part solution with associated boundary conditions. The two- and three-dimensional numerical results show that the present hybrid method converges with second-order accuracy for the velocity and first-order accuracy for the pressure, and it is comparable with the traditional immersed interface method in literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.06333v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Che-Chia Chang, Chen-Yang Dai, Wei-Fan Hu, Te-Sheng Lin, Ming-Chih Lai</dc:creator>
    </item>
    <item>
      <title>Acceleration and restart for the randomized Bregman-Kaczmarz method</title>
      <link>https://arxiv.org/abs/2310.17338</link>
      <description>arXiv:2310.17338v2 Announce Type: replace 
Abstract: Optimizing strongly convex functions subject to linear constraints is a fundamental problem with numerous applications. In this work, we propose a block (accelerated) randomized Bregman-Kaczmarz method that only uses a block of constraints in each iteration to tackle this problem. We consider a dual formulation of this problem in order to deal in an efficient way with the linear constraints. Using convex tools, we show that the corresponding dual function satisfies the Polyak-Lojasiewicz (PL) property, provided that the primal objective function is strongly convex and verifies additionally some other mild assumptions. However, adapting the existing theory on coordinate descent methods to our dual formulation can only give us sublinear convergence results in the dual space. In order to obtain convergence results in some criterion corresponding to the primal (original) problem, we transfer our algorithm to the primal space, which combined with the PL property allows us to get linear convergence rates. More specifically, we provide a theoretical analysis of the convergence of our proposed method under different assumptions on the objective and demonstrate in the numerical experiments its superior efficiency and speed up compared to existing methods for the same problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17338v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lionel Tondji, Ion Necoara, Dirk A. Lorenz</dc:creator>
    </item>
    <item>
      <title>Helicity-conservative Physics-informed Neural Network Model for Navier-Stokes Equations</title>
      <link>https://arxiv.org/abs/2204.07497</link>
      <description>arXiv:2204.07497v3 Announce Type: replace-cross 
Abstract: We design the helicity-conservative physics-informed neural network model for the Navier-Stokes equation in the ideal case. The key is to provide an appropriate PDE model as loss function so that its neural network solutions produce helicity conservation. Physics-informed neural network model is based on the strong form of PDE. We compare the proposed Physics-informed neural network model and a relevant helicity-conservative finite element method. We arrive at the conclusion that the strong form PDE is better suited for conservation issues. We also present theoretical justifications for helicity conservation as well as supporting numerical calculations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.07497v3</guid>
      <category>physics.comp-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiwei Jia, Young Ju Lee, Ziqian Li, Zheng Lu, Ran Zhang</dc:creator>
    </item>
    <item>
      <title>On Complexity of Stability Analysis in Higher-order Ecological Networks through Tensor Decompositions</title>
      <link>https://arxiv.org/abs/2401.02023</link>
      <description>arXiv:2401.02023v2 Announce Type: replace-cross 
Abstract: Complex ecological networks are often characterized by intricate interactions that extend beyond pairwise relationships. Understanding the stability of higher-order ecological networks is salient for species coexistence, biodiversity, and community persistence. In this article, we present complexity analyses for determining the linear stability of higher-order ecological networks through tensor decompositions. We are interested in the higher-order generalized Lotka-Volterra model, which captures high-order interactions using tensors of varying orders. To efficiently compute Jacobian matrices and thus determine stability in large ecological networks, we exploit various tensor decompositions, including higher-order singular value decomposition, Canonical Polyadic decomposition, and tensor train decomposition, accompanied by in-depth computational and memory complexity analyses. We demonstrate the effectiveness of our framework with numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.02023v2</guid>
      <category>eess.SY</category>
      <category>cs.NA</category>
      <category>cs.SY</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anqi Dong, Can Chen</dc:creator>
    </item>
    <item>
      <title>Modeling Large-Scale Walking and Cycling Networks: A Machine Learning Approach Using Mobile Phone and Crowdsourced Data</title>
      <link>https://arxiv.org/abs/2404.00162</link>
      <description>arXiv:2404.00162v2 Announce Type: replace-cross 
Abstract: Walking and cycling are known to bring substantial health, environmental, and economic advantages. However, the development of evidence-based active transportation planning and policies has been impeded by significant data limitations, such as biases in crowdsourced data and representativeness issues of mobile phone data. In this study, we develop and apply a machine learning based modeling approach for estimating daily walking and cycling volumes across a large-scale regional network in New South Wales, Australia that includes 188,999 walking links and 114,885 cycling links. The modeling methodology leverages crowdsourced and mobile phone data as well as a range of other datasets on population, land use, topography, climate, etc. The study discusses the unique challenges and limitations related to all three aspects of model training, testing, and inference given the large geographical extent of the modeled networks and relative scarcity of observed walking and cycling count data. The study also proposes a new technique to identify model estimate outliers and to mitigate their impact. Overall, the study provides a valuable resource for transportation modelers, policymakers and urban planners seeking to enhance active transportation infrastructure planning and policies with advanced emerging data-driven modeling methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00162v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Meead Saberi, Tanapon Lilasathapornkit</dc:creator>
    </item>
  </channel>
</rss>
