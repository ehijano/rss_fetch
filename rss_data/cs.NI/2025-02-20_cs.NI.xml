<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Feb 2025 02:44:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Impact of Cross Technology Interference on Time Synchronization and Join Time in Low-Power Wireless Networks</title>
      <link>https://arxiv.org/abs/2502.13303</link>
      <description>arXiv:2502.13303v1 Announce Type: new 
Abstract: Low-power and low-cost wireless sensor networks enable scalable and affordable sensing and can be deployed in different environments to monitor various physical parameters. In some environments, these networks may have to coexist and interact with other systems which use the same frequency spectrum for communication. This potentially results in cross-technology interference (CTI). Dynamic channel hopping is one of the mechanisms that is currently employed to deal with CTI, but its usefulness depends on the channel selection and occupation timing. In this paper, we experimentally study the impact of CTI (caused by IEEE 802.11 networks) on time synchronization and network join time. Experiment results show that CTI can increase time drift between a child and a parent node by up to $\pm 3$ clock ticks between two synchronization intervals. Likewise, CTI affects new nodes from timely joining a network. In a simple network which does not involve multi-hop communication, the time it takes for nodes to join the network in the absence of CTI is between 40 and 70 ms (83.3\% of the time). In the presence of CTI, 96.82\% of the time, the join time is between 100 and 200 ms. In other words, the join time in the presence of CTI is about five times higher. Interestingly, not only the main spectral lobes, but also the spectral sidelobes of interfering networks impact the performance of low-power networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13303v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zegeye Mekasha Kidane, Waltenegus Dargie</dc:creator>
    </item>
    <item>
      <title>Simulative Comparison of DVB-S2X/RCS2 and 3GPP 5G NR NTN Technologies in a Geostationary Satellite Scenario</title>
      <link>https://arxiv.org/abs/2502.13704</link>
      <description>arXiv:2502.13704v1 Announce Type: new 
Abstract: Comparison between existing, well-established satellite technologies, like the Digital Video Broadcasting (DVB) satellite specifications, and the emerging Third Generation Partnership Project (3GPP) specified 5th Generation New Radio (5G NR) Non-Terrestrial Networks (NTN) is an actively discussed topic in the satellite industry standardization groups. This article presents a thorough performance comparison between DVB Second Generation Satellite Extensions (DVBS2X) and Return Channel via Satellite 2nd Generation (DVBRCS2), and NR NTN in a Geostationary Orbit (GEO) satellite scenario, using system-level simulators (SLS) for evaluation, namely Satellite Network Simulator 3 (SNS3) and ALIX 5G (TN-)NTN SLS, built on the same Network Simulator 3 (ns-3) platform. With the satellite system geometry, beam layout, and link budget aligned to use the 3GPP NTN example parameterization for a fair comparison between DVB and NR NTN, the results show that DVB-S2X consistently achieves higher spectral efficiency than the NR Physical Downlink Shared Channel (PDSCH) on the forward user link. In contrast, on the return link, the NR Physical Uplink Shared Channel (PUSCH) demonstrates better spectral efficiency at the system level. The SLS results incorporate link-level performance, obtained through link-level simulations (LLS) for different modulation and coding schemes (MCS) and waveforms supported by each technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13704v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lauri Sormunen, Tuomas Huikko, Verneri R\"onty, Erno Sepp\"anen, Sami Rantanen, Frans Laakso, Vesa Hyt\"onen, Mikko Majamaa, Jani Puttonen</dc:creator>
    </item>
    <item>
      <title>Binary VPN Traffic Detection Using Wavelet Features and Machine Learning</title>
      <link>https://arxiv.org/abs/2502.13804</link>
      <description>arXiv:2502.13804v1 Announce Type: new 
Abstract: Encrypted traffic classification faces growing challenges as encryption renders traditional deep packet inspection ineffective. This study addresses binary VPN detection, distinguishing VPN-encrypted from non-VPN traffic using wavelet transform-based features across multiple machine learning models. We analyze the impact of wavelet decomposition levels and dataset filtering on classification performance. Our results demonstrate that Random Forest (RF) achieves superior performance with an F1-score of 99%, maintaining robust accuracy even after significant dataset filtering. Neural Networks (NN) show comparable effectiveness with an F1-score of 98% when trained on wavelet level 12, while Support Vector Machines (SVM) exhibit notable sensitivity to dataset reduction, with F1-scores dropping from 90% to 85% after filtering. Comparing wavelet decomposition at levels 5 and 12, we observe improved classification performance at level 12, particularly for variable traffic types, though the marginal gains may not justify the additional computational overhead. These findings establish RF as the most reliable model for VPN traffic classification while highlighting key performance tradeoffs in feature extraction and preprocessing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13804v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yasameen Sajid Razooqi, Adrian Pekar</dc:creator>
    </item>
    <item>
      <title>A measurement-based approach to analyze the power consumption of the softwarized 5G core</title>
      <link>https://arxiv.org/abs/2502.13879</link>
      <description>arXiv:2502.13879v1 Announce Type: new 
Abstract: In light of the ever growing energy needs of the ICT sector, a value that is becoming increasingly important for a mobile network is its power consumption. However, the transition away from legacy network deployments tightly coupled with the underlying hardware and the adoption of the Network Function Virtualization (NFV) paradigm has made more difficult to accurately evaluate their energy and carbon footprint. In this paper, we propose and validate a measurement-based approach to analyze the power consumption of a virtualized 5G core network (5GC) deployment. We design an experimental testbed using commercial off-the-shelf (COTS) hardware and open-source software as a sample architecture simulating an edge computing node and supporting three different virtualization options. We make use of both hardware-based and software-based power meters to investigate the power consumption trends associated with increasing levels of traffic and multiple 5GC deployment types. The results show the feasibility of a real-time power monitoring system and highlight how deployment choices, such as virtualization framework and 5GC software, can significantly impact on the power consumption of the network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13879v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.comnet.2024.110312</arxiv:DOI>
      <arxiv:journal_reference>Computer Networks, Volume 244, 2024, 110312</arxiv:journal_reference>
      <dc:creator>Arturo Bellin, Fabrizio Granelli, Daniele Munaretto</dc:creator>
    </item>
    <item>
      <title>Integration of Agentic AI with 6G Networks for Mission-Critical Applications: Use-case and Challenges</title>
      <link>https://arxiv.org/abs/2502.13476</link>
      <description>arXiv:2502.13476v1 Announce Type: cross 
Abstract: We are in a transformative era, and advances in Artificial Intelligence (AI), especially the foundational models, are constantly in the news. AI has been an integral part of many applications that rely on automation for service delivery, and one of them is mission-critical public safety applications. The problem with AI-oriented mission-critical applications is the humanin-the-loop system and the lack of adaptability to dynamic conditions while maintaining situational awareness. Agentic AI (AAI) has gained a lot of attention recently due to its ability to analyze textual data through a contextual lens while quickly adapting to conditions. In this context, this paper proposes an AAI framework for mission-critical applications. We propose a novel framework with a multi-layer architecture to realize the AAI. We also present a detailed implementation of AAI layer that bridges the gap between network infrastructure and missioncritical applications. Our preliminary analysis shows that the AAI reduces initial response time by 5.6 minutes on average, while alert generation time is reduced by 15.6 seconds on average and resource allocation is improved by up to 13.4%. We also show that the AAI methods improve the number of concurrent operations by 40, which reduces the recovery time by up to 5.2 minutes. Finally, we highlight some of the issues and challenges that need to be considered when implementing AAI frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13476v1</guid>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sunder Ali Khowaja, Kapal Dev, Muhammad Salman Pathan, Engin Zeydan, Merouane Debbah</dc:creator>
    </item>
    <item>
      <title>MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D Medical Image Analysis</title>
      <link>https://arxiv.org/abs/2502.13524</link>
      <description>arXiv:2502.13524v1 Announce Type: cross 
Abstract: Efficient evaluation of three-dimensional (3D) medical images is crucial for diagnostic and therapeutic practices in healthcare. Recent years have seen a substantial uptake in applying deep learning and computer vision to analyse and interpret medical images. Traditional approaches, such as convolutional neural networks (CNNs) and vision transformers (ViTs), face significant computational challenges, prompting the need for architectural advancements. Recent efforts have led to the introduction of novel architectures like the ``Mamba'' model as alternative solutions to traditional CNNs or ViTs. The Mamba model excels in the linear processing of one-dimensional data with low computational demands. However, Mamba's potential for 3D medical image analysis remains underexplored and could face significant computational challenges as the dimension increases. This manuscript presents MobileViM, a streamlined architecture for efficient segmentation of 3D medical images. In the MobileViM network, we invent a new dimension-independent mechanism and a dual-direction traversing approach to incorporate with a vision-Mamba-based framework. MobileViM also features a cross-scale bridging technique to improve efficiency and accuracy across various medical imaging modalities. With these enhancements, MobileViM achieves segmentation speeds exceeding 90 frames per second (FPS) on a single graphics processing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster than the state-of-the-art deep learning models for processing 3D images with the same computational resources. In addition, experimental evaluations demonstrate that MobileViM delivers superior performance, with Dice similarity scores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024, ATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses existing models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13524v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Wei Dai, Steven Wang, Jun Liu</dc:creator>
    </item>
    <item>
      <title>GENIO: Synergizing Edge Computing with Optical Network Infrastructures</title>
      <link>https://arxiv.org/abs/2502.13657</link>
      <description>arXiv:2502.13657v1 Announce Type: cross 
Abstract: Edge computing has emerged as a paradigm to bring low-latency and bandwidth-intensive applications close to end-users. However, edge computing platforms still face challenges related to resource constraints, connectivity, and security. We present GENIO, a novel platform that integrates edge computing within existing Passive Optical Network (PON) infrastructures. GENIO enhances central offices with computational and storage resources, enabling telecom operators to leverage their existing PON networks as a distributed edge computing infrastructure. Through simulations, we show the feasibility of GENIO in supporting real-world edge scenarios, and its better performance compared to a traditional edge computing architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13657v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/MCOM.002.2400382</arxiv:DOI>
      <dc:creator>Carmine Cesarano, Alessio Foggia, Gianluca Roscigno, Luca Andreani, Roberto Natella</dc:creator>
    </item>
    <item>
      <title>Characterizing the Age of Information with Multiple Coexisting Data Streams</title>
      <link>https://arxiv.org/abs/2404.15623</link>
      <description>arXiv:2404.15623v2 Announce Type: replace 
Abstract: In this paper we analyze the distribution of the Age of Information (AoI) of a tagged data stream sharing a processor with a set of other data streams. We do so in the highly general setting in which the interarrival times pertaining to the tagged stream can have any distribution, and also the service times of both the tagged stream and the background stream are generally distributed. The packet arrival times of the background process are assumed to constitute a Poisson process, which is justified by the fact that it typically is a superposition of many relatively homogeneous streams. The first main contribution is that we derive an expression for the Laplace-Stieltjes transform of the AoI in the resulting GI+M/GI+GI/1 model. Second, we use stochastic ordering techniques to identify tight stochastic bounds on the AoI, leading to an explicit lower and upper bound on the mean AoI. In addition, when approximating the tagged stream's inter-generation times through a phase-type distribution (which can be done at any precision), we present a computational algorithm for the mean AoI. As illustrated through a sequence of numerical experiments, the analysis enables us to assess the impact of background traffic on the AoI of the tagged stream. It turns out that the upper bound on the mean AoI is remarkably close to its true value, which yields an explicit expression (in terms of the model parameters) for an accurate proxy of the AoI-minimizing generation rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15623v2</guid>
      <category>cs.NI</category>
      <category>math.PR</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoshiaki Inoue, Michel Mandjes</dc:creator>
    </item>
    <item>
      <title>Optimal Sampling under Cost for Remote Estimation of the Wiener Process over a Channel with Delay</title>
      <link>https://arxiv.org/abs/2407.21181</link>
      <description>arXiv:2407.21181v4 Announce Type: replace 
Abstract: We address the optimal sampling of a Wiener process under sampling and transmission costs, with the samples being forwarded to a remote estimator over a channel with IID delay. The goal of the estimator is to reconstruct the real-time signal by minimizing a long-term average cost that includes both the mean squared estimation error (MSE) and the costs associated with sampling and transmission from causally received samples. Rather than pursuing the conventional MMSE estimate, our objective is to derive a policy that optimally balances estimation accuracy and resource expenditure, yielding an MSE-optimal solution under explicit cost constraints. We look for optimal online strategies for both sampling and transmission. By employing Lagrange relaxation and iterative backward induction, we derive an optimal policy that balances the trade-offs between estimation accuracy and costs. We validate our approach through comprehensive simulations, evaluating various scenarios including balanced costs, high sampling costs, high transmission costs, and different transmission delay statistics. Our results demonstrate the effectiveness and robustness of the proposed joint sampling and transmission policy in maintaining lower MSE compared to conventional periodic sampling methods. The differences are particularly striking under high delay variability. We also analyze the convergence behavior of the cost function. We believe our formulation and results provide insights into the design and implementation of efficient remote estimation systems in stochastic networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21181v4</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S\"uleyman \c{C}{\i}t{\i}r, Orhan T. Yava\c{s}can, Elif Uysal</dc:creator>
    </item>
    <item>
      <title>WRF-GS: Wireless Radiation Field Reconstruction with 3D Gaussian Splatting</title>
      <link>https://arxiv.org/abs/2412.04832</link>
      <description>arXiv:2412.04832v2 Announce Type: replace 
Abstract: Wireless channel modeling plays a pivotal role in designing, analyzing, and optimizing wireless communication systems. Nevertheless, developing an effective channel modeling approach has been a longstanding challenge. This issue has been escalated due to the denser network deployment, larger antenna arrays, and wider bandwidth in 5G and beyond networks. To address this challenge, we put forth WRF-GS, a novel framework for channel modeling based on wireless radiation field (WRF) reconstruction using 3D Gaussian splatting. WRF-GS employs 3D Gaussian primitives and neural networks to capture the interactions between the environment and radio signals, enabling efficient WRF reconstruction and visualization of the propagation characteristics. The reconstructed WRF can then be used to synthesize the spatial spectrum for comprehensive wireless channel characterization. Notably, with a small number of measurements, WRF-GS can synthesize new spatial spectra within milliseconds for a given scene, thereby enabling latency-sensitive applications. Experimental results demonstrate that WRF-GS outperforms existing methods for spatial spectrum synthesis, such as ray tracing and other deep-learning approaches. Moreover, WRF-GS achieves superior performance in the channel state information prediction task, surpassing existing methods by a significant margin of more than 2.43 dB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04832v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaozheng Wen, Jingwen Tong, Yingdong Hu, Zehong Lin, Jun Zhang</dc:creator>
    </item>
    <item>
      <title>Service Function Chain Dynamic Scheduling in Space-Air-Ground Integrated Networks</title>
      <link>https://arxiv.org/abs/2502.10731</link>
      <description>arXiv:2502.10731v2 Announce Type: replace 
Abstract: As an important component of the sixth generation communication technologies, the space-air-ground integrated network (SAGIN) attracts increasing attentions in recent years. However, due to the mobility and heterogeneity of the components such as satellites and unmanned aerial vehicles in multi-layer SAGIN, the challenges of inefficient resource allocation and management complexity are aggregated. To this end, the network function virtualization technology is introduced and can be implemented via service function chains (SFCs) deployment. However, urgent unexpected tasks may bring conflicts and resource competition during SFC deployment, and how to schedule the SFCs of multiple tasks in SAGIN is a key issue. In this paper, we address the dynamic and complexity of SAGIN by presenting a reconfigurable time extension graph and further propose the dynamic SFC scheduling model. Then, we formulate the SFC scheduling problem to maximize the number of successful deployed SFCs within limited resources and time horizons. Since the problem is in the form of integer linear programming and intractable to solve, we propose the algorithm by incorporating deep reinforcement learning. Finally, simulation results show that the proposed algorithm has better convergence and performance compared to other benchmark algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10731v2</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziye Jia, Yilu Cao, Lijun He, Qihui Wu, Qiuming Zhu, Dusit Niyato, Zhu Han</dc:creator>
    </item>
    <item>
      <title>Diffusion Models as Network Optimizers: Explorations and Analysis</title>
      <link>https://arxiv.org/abs/2411.00453</link>
      <description>arXiv:2411.00453v5 Announce Type: replace-cross 
Abstract: Network optimization is a fundamental challenge in the Internet of Things (IoT) network, often characterized by complex features that make it difficult to solve these problems. Recently, generative diffusion models (GDMs) have emerged as a promising new approach to network optimization, with the potential to directly address these optimization problems. However, the application of GDMs in this field is still in its early stages, and there is a noticeable lack of theoretical research and empirical findings. In this study, we first explore the intrinsic characteristics of generative models. Next, we provide a concise theoretical proof and intuitive demonstration of the advantages of generative models over discriminative models in network optimization. Based on this exploration, we implement GDMs as optimizers aimed at learning high-quality solution distributions for given inputs, sampling from these distributions during inference to approximate or achieve optimal solutions. Specifically, we utilize denoising diffusion probabilistic models (DDPMs) and employ a classifier-free guidance mechanism to manage conditional guidance based on input parameters. We conduct extensive experiments across three challenging network optimization problems. By investigating various model configurations and the principles of GDMs as optimizers, we demonstrate the ability to overcome prediction errors and validate the convergence of generated solutions to optimal solutions. We provide code and data at https://github.com/qiyu3816/DiffSG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00453v5</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/JIOT.2025.3528955</arxiv:DOI>
      <arxiv:journal_reference>IEEE Internet of Things Journal (2025)</arxiv:journal_reference>
      <dc:creator>Ruihuai Liang, Bo Yang, Pengyu Chen, Xianjin Li, Yifan Xue, Zhiwen Yu, Xuelin Cao, Yan Zhang, M\'erouane Debbah, H. Vincent Poor, Chau Yuen</dc:creator>
    </item>
    <item>
      <title>Future Resource Bank for ISAC: Achieving Fast and Stable Win-Win Matching for Both Individuals and Coalitions</title>
      <link>https://arxiv.org/abs/2502.08118</link>
      <description>arXiv:2502.08118v4 Announce Type: replace-cross 
Abstract: Future wireless networks must support emerging applications where environmental awareness is as critical as data transmission. Integrated Sensing and Communication (ISAC) enables this vision by allowing base stations (BSs) to allocate bandwidth and power to mobile users (MUs) for communications and cooperative sensing. However, this resource allocation is highly challenging due to: (i) dynamic resource demands from MUs and resource supply from BSs, and (ii) the selfishness of MUs and BSs. To address these challenges, existing solutions rely on either real-time (online) resource trading, which incurs high overhead and failures, or static long-term (offline) resource contracts, which lack flexibility. To overcome these limitations, we propose the Future Resource Bank for ISAC, a hybrid trading framework that integrates offline and online resource allocation through a level-wise client model, where MUs and their coalitions negotiate with BSs. We introduce two mechanisms: (i) Role-Friendly Win-Win Matching (offRFW$^2$M), leveraging overbooking to establish risk-aware, stable contracts, and (ii) Effective Backup Win-Win Matching (onEBW$^2$M), which dynamically reallocates unmet demand and surplus supply. We theoretically prove stability, individual rationality, and weak Pareto optimality of these mechanisms. Through simulations, we show that our framework improves social welfare, latency, and energy efficiency compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08118v4</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Houyi Qi, Minghui Liwang, Seyyedali Hosseinalipour, Liqun Fu, Sai Zou, Wei Ni</dc:creator>
    </item>
  </channel>
</rss>
