<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 11 Oct 2024 02:26:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Multi-Stage Graph Learning for fMRI Analysis to Diagnose Neuro-Developmental Disorders</title>
      <link>https://arxiv.org/abs/2410.05342</link>
      <description>arXiv:2410.05342v1 Announce Type: new 
Abstract: The insufficient supervision limit the performance of the deep supervised models for brain disease diagnosis. It is important to develop a learning framework that can capture more information in limited data and insufficient supervision. To address these issues at some extend, we propose a multi-stage graph learning framework which incorporates 1) pretrain stage : self-supervised graph learning on insufficient supervision of the fmri data 2) fine-tune stage : supervised graph learning for brain disorder diagnosis. Experiment results on three datasets, Autism Brain Imaging Data Exchange ABIDE I, ABIDE II and ADHD with AAL1,demonstrating the superiority and generalizability of the proposed framework compared to the state of art of models.(ranging from 0.7330 to 0.9321,0.7209 to 0.9021,0.6338 to 0.6699)</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05342v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Wenjing Gao, Yuanyuan Yang, Jianrui Wei, Xuntao Yin, Xinhan Di</dc:creator>
    </item>
    <item>
      <title>Closed-Loop phase selection in EEG-TMS using Bayesian Optimization</title>
      <link>https://arxiv.org/abs/2410.05747</link>
      <description>arXiv:2410.05747v1 Announce Type: new 
Abstract: Research on transcranial magnetic stimulation (TMS) combined with encephalography feedback (EEG-TMS) has shown that the phase of the sensorimotor mu rhythm is predictive of corticospinal excitability. Thus, if the subject-specific optimal phase is known, stimulation can be timed to be more efficient. In this paper, we present a closed-loop algorithm to determine the optimal phase linked to the highest excitability with few trials. We used Bayesian optimization as an automated, online search tool in an EEG-TMS simulation experiment. From a sample of 38 participants, we selected all participants with a significant single-subject phase effect (N = 5) for simulation. We then simulated 1000 experimental sessions per participant where we used Bayesian optimization to find the optimal phase. We tested two objective functions: Fitting a sinusoid in Bayesian linear regression or Gaussian Process (GP) regression. We additionally tested adaptive sampling using a knowledge gradient as the acquisition function compared with random sampling. We evaluated the algorithm's performance in a fast optimization (100 trials) and a long-term optimization (1000 trials). For fast optimization, the Bayesian linear regression in combination with adaptive sampling gives the best results with a mean phase location accuracy of 79 % after 100 trials. With either sampling approach, Bayesian linear regression performs better than GP regression in the fast optimization. In the long-term optimization, Bayesian regression with random sampling shows the best trajectory, with a rather steep improvement and good final performance of 87 % mean phase location accuracy. We show the suitability of closed-loop Bayesian optimization for phase selection. We could increase the speed and accuracy by using prior knowledge about the expected function shape compared with traditional Bayesian optimization with GP regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05747v1</guid>
      <category>q-bio.NC</category>
      <category>eess.SP</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miriam Kirchhoff, Dania Humaidan, Ulf Ziemann</dc:creator>
    </item>
    <item>
      <title>Mechanisms and Controversies of tACS (Transcranial Alternating Current Stimulation)</title>
      <link>https://arxiv.org/abs/2410.05841</link>
      <description>arXiv:2410.05841v1 Announce Type: new 
Abstract: The use of small amplitude electric currents delivered through the scalp, termed transcranial current stimulation (tCS, tDCS / tACS when using DC / AC currents) holds considerable promise for developing safe and effective treatments for central nervous system disorders. Initially welcomed with skepticism due to significant gaps of knowledge in terms of neurophysiology and biophysical mechanisms, tCS is maturing as a technology while its mechanisms of action are gradually being elucidated. However, there remain open questions about the mechanisms of action that warrant clarification to bring tCS to its full potential. In this review focused on tACS, we make an attempt at providing an overview of the converging experimental evidence, from results obtained in various species, regarding the mechanisms of action. We also highlight the remaining points of uncertainty regarding potential confounds, and propose possible experimentally testable solutions to address those issues. Finally, we outline how a continued focus on deepening our understanding of tACS mechanisms might provide significant insights into fundamental, long-standing questions in neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05841v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Julien Modolo, Mengsen Zhang, Joan Duprez, Flavio Frohlich</dc:creator>
    </item>
    <item>
      <title>Node-reconfiguring multilayer networks of human brain function</title>
      <link>https://arxiv.org/abs/2410.05972</link>
      <description>arXiv:2410.05972v1 Announce Type: new 
Abstract: The properties of functional brain networks are heavily influenced by how the network nodes are defined. A common approach uses Regions of Interest (ROIs), which are predetermined collections of fMRI measurement voxels, as network nodes. Their definition is always a compromise, as static ROIs cannot capture the dynamics and the temporal reconfigurations of the brain areas. Consequently, the ROIs do not align with the functionally homogeneous regions, which can explain the very low functional homogeneity values observed for the ROIs. This is in violation of the underlying homogeneity assumption in functional brain network analysis pipelines and it can cause serious problems such as spurious network structure. We introduce the node-reconfiguring multilayer network model, where nodes represent ROIs with boundaries optimized for high functional homogeneity in each time window. In this representation, network layers correspond to time windows, intralayer links depict functional connectivity between ROIs, and interlayer link weights quantify overlap between ROIs on different layers. The ROI optimization approach increases functional homogeneity notably, yielding an over 10-fold increase in the fraction of ROIs with high homogeneity compared to static ROIs from the Brainnetome atlas. The optimized ROIs reorganize non-trivially at short time scales of consecutive time windows and across several windows. The amount of reorganization across time windows is connected to intralayer hubness: ROIs that show intermediate levels of reorganization have stronger intralayer links than extremely stable or unstable ROIs. Our results demonstrate that reconfiguring parcellations yield more accurate network models of brain function. This supports the ongoing paradigm shift towards the chronnectome that sees the brain as a set of sources with continuously reconfiguring spatial and connectivity profiles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05972v1</guid>
      <category>q-bio.NC</category>
      <category>physics.comp-ph</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tarmo Nurmi (Department of Computer Science, Aalto University, Helsinki, Finland), Pietro De Luca (Department of Computer Science, Aalto University, Helsinki, Finland), Mikko Kivel\"a (Department of Computer Science, Aalto University, Helsinki, Finland), Onerva Korhonen (Department of Computer Science, Aalto University, Helsinki, Finland, Faculty of Science, Forestry and Technology, University of Eastern Finland, Joensuu, Finland)</dc:creator>
    </item>
    <item>
      <title>Don't Cut Corners: Exact Conditions for Modularity in Biologically Inspired Representations</title>
      <link>https://arxiv.org/abs/2410.06232</link>
      <description>arXiv:2410.06232v1 Announce Type: new 
Abstract: Why do biological and artificial neurons sometimes modularise, each encoding a single meaningful variable, and sometimes entangle their representation of many variables? In this work, we develop a theory of when biologically inspired representations -- those that are nonnegative and energy efficient -- modularise with respect to source variables (sources). We derive necessary and sufficient conditions on a sample of sources that determine whether the neurons in an optimal biologically-inspired linear autoencoder modularise. Our theory applies to any dataset, extending far beyond the case of statistical independence studied in previous work. Rather, we show that sources modularise if their support is "sufficiently spread". From this theory, we extract and validate predictions in a variety of empirical studies on how data distribution affects modularisation in nonlinear feedforward and recurrent neural networks trained on supervised and unsupervised tasks. Furthermore, we apply these ideas to neuroscience data. First, we explain why two studies that recorded prefrontal activity in working memory tasks conflict on whether memories are encoded in orthogonal subspaces: the support of the sources differed due to a critical discrepancy in experimental protocol. Second, we use similar arguments to understand why preparatory and potent subspaces in RNN models of motor cortex are only sometimes orthogonal. Third, we study spatial and reward information mixing in entorhinal recordings, and show our theory matches data better than previous work. And fourth, we suggest a suite of surprising settings in which neurons can be (or appear) mixed selective, without requiring complex nonlinear readouts as in traditional theories. In sum, our theory prescribes precise conditions on when neural activities modularise, providing tools for inducing and elucidating modular representations in brains and machines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06232v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Will Dorrell, Kyle Hsu, Luke Hollingsworth, Jin Hwa Lee, Jiajun Wu, Chelsea Finn, Peter E Latham, Tim EJ Behrens, James CR Whittington</dc:creator>
    </item>
    <item>
      <title>Dynamics of Adaptive Continuous Attractor Neural Networks</title>
      <link>https://arxiv.org/abs/2410.06517</link>
      <description>arXiv:2410.06517v1 Announce Type: new 
Abstract: Attractor neural networks consider that neural information is stored as stationary states of a dynamical system formed by a large number of interconnected neurons. The attractor property empowers a neural system to encode information robustly, but it also incurs the difficulty of rapid update of network states, which can impair information update and search in the brain. To overcome this difficulty, a solution is to include adaptation in the attractor network dynamics, whereby the adaptation serves as a slow negative feedback mechanism to destabilize which are otherwise permanently stable states. In such a way, the neural system can, on one hand, represent information reliably using attractor states, and on the other hand, perform computations wherever rapid state updating is involved. Previous studies have shown that continuous attractor neural networks with adaptation (A-CANNs) exhibits rich dynamical behaviors accounting for various brain functions. In this paper, we present a comprehensive view of the rich diverse dynamics of A-CANNs. Moreover, we provide a unified mathematical framework to understand these different dynamical behaviors, and briefly discuss about their biological implications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06517v1</guid>
      <category>q-bio.NC</category>
      <category>physics.bio-ph</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujun Li, Tianhao Chu, Si Wu</dc:creator>
    </item>
    <item>
      <title>EEG-estimated functional connectivity, and not behavior, differentiates Parkinson's patients from health controls during the Simon conflict task</title>
      <link>https://arxiv.org/abs/2410.06534</link>
      <description>arXiv:2410.06534v1 Announce Type: new 
Abstract: Neural biomarkers that can classify or predict disease are of broad interest to the neurological and psychiatric communities. Such biomarkers can be informative of disease state or treatment efficacy, even before there are changes in symptoms and/or behavior. This work investigates EEG-estimated functional connectivity (FC) as a Parkinson's Disease (PD) biomarker. Specifically, we investigate FC mediated via neural oscillations and consider such activity during the Simons conflict task. This task yields sensory-motor conflict, and one might expect differences in behavior between PD patients and healthy controls (HCs). In addition to considering spatially focused approaches, such as FC, as a biomarker, we also consider temporal biomarkers, which are more sensitive to ongoing changes in neural activity. We find that FC, estimated from delta (1-4Hz) and theta (4-7Hz) oscillations, yields spatial FC patterns significantly better at distinguishing PD from HC than temporal features or behavior. This study reinforces that FC in spectral bands is informative of differences in brain-wide processes and can serve as a biomarker distinguishing normal brain function from that seen in disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06534v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoxiao Sun, Chongkun Zhao, Sharath Koorathota, Paul Sajda</dc:creator>
    </item>
    <item>
      <title>On the Minimal Theory of Consciousness Implicit in Active Inference</title>
      <link>https://arxiv.org/abs/2410.06633</link>
      <description>arXiv:2410.06633v1 Announce Type: new 
Abstract: The multifaceted nature of experience poses a challenge to the study of consciousness. Traditional neuroscientific approaches often concentrate on isolated facets, such as perceptual awareness or the global state of consciousness and construct a theory around the relevant empirical paradigms and findings. Theories of consciousness are, therefore, often difficult to compare; indeed, there might be little overlap in the phenomena such theories aim to explain. Here, we take a different approach: starting with active inference, a first principles framework for modelling behaviour as (approximate) Bayesian inference, and building up to a minimal theory of consciousness, which emerges from the shared features of computational models derived under active inference. We review a body of work applying active inference models to the study of consciousness and argue that there is implicit in all these models a small set of theoretical commitments that point to a minimal (and testable) theory of consciousness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06633v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher J. Whyte, Andrew W. Corcoran, Jonathan Robinson, Ryan Smith, Rosalyn J. Moran, Thomas Parr, Karl J. Friston, Anil K. Seth, Jakob Hohwy</dc:creator>
    </item>
    <item>
      <title>The multiscale self-similarity of the weighted human brain connectome</title>
      <link>https://arxiv.org/abs/2410.06739</link>
      <description>arXiv:2410.06739v1 Announce Type: new 
Abstract: Anatomical connectivity between different regions in the brain can be mapped to a network representation, the connectome, where the intensities of the links, the weights, influence its structural resilience and the functional processes it sustains. Yet, many features associated with the weights in the human brain connectome are not fully understood, particularly their multiscale organization. In this paper, we elucidate the architecture of weights, including weak ties, in multiscale hierarchical human brain connectomes reconstructed from empirical data. Our findings reveal multiscale self-similarity in the weighted statistical properties, including the ordering of weak ties, that remain consistent across the analyzed length scales of every individual and the group representatives. This phenomenon is effectively captured by a renormalization of the weighted structure applied to hyperbolic embeddings of the connectomes, based on a unique weighted geometric model that integrates links of all weights across all length scales. This eliminates the need for separate generative weighted connectivity rules for each scale or to replicate weak and strong ties at specific scales in brain connectomes. The observed symmetry represents a distinct signature of criticality in the weighted connectivity of human brain connectomes, aligning with the fractality observed in their topology, and raises important questions for future research, like the existence of a resolution threshold where the observed symmetry breaks, or whether it is preserved in cases of neurodegenerative disease or psychiatric disorder.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06739v1</guid>
      <category>q-bio.NC</category>
      <category>physics.bio-ph</category>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laia Barjuan, Muhua Zheng, M. \'Angeles Serrano</dc:creator>
    </item>
    <item>
      <title>Diagnosis and Pathogenic Analysis of Autism Spectrum Disorder Using Fused Brain Connection Graph</title>
      <link>https://arxiv.org/abs/2410.07138</link>
      <description>arXiv:2410.07138v1 Announce Type: new 
Abstract: We propose a model for diagnosing Autism spectrum disorder (ASD) using multimodal magnetic resonance imaging (MRI) data. Our approach integrates brain connectivity data from diffusion tensor imaging (DTI) and functional MRI (fMRI), employing graph neural networks (GNNs) for fused graph classification. To improve diagnostic accuracy, we introduce a loss function that maximizes inter-class and minimizes intra-class margins. We also analyze network node centrality, calculating degree, subgraph, and eigenvector centralities on a bimodal fused brain graph to identify pathological regions linked to ASD. Two non-parametric tests assess the statistical significance of these centralities between ASD patients and healthy controls. Our results reveal consistency between the tests, yet the identified regions differ significantly across centralities, suggesting distinct physiological interpretations. These findings enhance our understanding of ASD's neurobiological basis and offer new directions for clinical diagnosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07138v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lu Wei, Yi Huang, Guosheng Yin, Fode Zhang, Manxue Zhang, Bin Liu</dc:creator>
    </item>
    <item>
      <title>Neural Circuit Architectural Priors for Quadruped Locomotion</title>
      <link>https://arxiv.org/abs/2410.07174</link>
      <description>arXiv:2410.07174v1 Announce Type: new 
Abstract: Learning-based approaches to quadruped locomotion commonly adopt generic policy architectures like fully connected MLPs. As such architectures contain few inductive biases, it is common in practice to incorporate priors in the form of rewards, training curricula, imitation data, or trajectory generators. In nature, animals are born with priors in the form of their nervous system's architecture, which has been shaped by evolution to confer innate ability and efficient learning. For instance, a horse can walk within hours of birth and can quickly improve with practice. Such architectural priors can also be useful in ANN architectures for AI. In this work, we explore the advantages of a biologically inspired ANN architecture for quadruped locomotion based on neural circuits in the limbs and spinal cord of mammals. Our architecture achieves good initial performance and comparable final performance to MLPs, while using less data and orders of magnitude fewer parameters. Our architecture also exhibits better generalization to task variations, even admitting deployment on a physical robot without standard sim-to-real methods. This work shows that neural circuits can provide valuable architectural priors for locomotion and encourages future work in other sensorimotor skills.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07174v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>cs.RO</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikhil X. Bhattasali, Venkatesh Pattabiraman, Lerrel Pinto, Grace W. Lindsay</dc:creator>
    </item>
    <item>
      <title>Cumulative, Adaptive, Open-ended Change through Self-Other Reorganization: Reply to comment on 'An evolutionary process without variation and selection'</title>
      <link>https://arxiv.org/abs/2410.05294</link>
      <description>arXiv:2410.05294v1 Announce Type: cross 
Abstract: Self-Other Reorganization (SOR) is a theory of how interacting entities or individuals, each of which can be described as an autocatalytic network, collectively exhibit cumulative, adaptive, open-ended change, or evolution. Zachar et al.'s critique of SOR stems from misunderstandings; it does not weaken the arguments in (Gabora &amp; Steel, 2021). The formal framework of Reflexively Autocatalytic and foodset-derived sets (RAFs) enables us to model the process whereby, through their interactions, a set of elements become a 'collective self.' SOR shows how the RAF setting provides a means of encompassing abiogenesis and cultural evolution under the same explanatory framework and provides a plausible explanation for the origins of both evolutionary processes. Although SOR allows for detrimental stimuli (and products), there is (naturally) limited opportunity for elements that do not contribute to or reinforce a RAF to become part of it. Replication and cumulative, adaptive change in RAFs is well-established in the literature. Contrary to Zachar et al., SOR is not a pure percolation model (such as SIR); it encompasses not only learning (modeled as assimilation of foodset elements) but also creative restructuring (modeled as generation of foodset-derived elements), as well as the emergence of new structures made possible by new foodset- and foodset-derived elements. Cultural SOR is robust to degradation, and imperfect replication. Zachar et al.'s simulation contains no RAFs, and does not model SOR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05294v1</guid>
      <category>q-bio.PE</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liane Gabora, Mike Steel</dc:creator>
    </item>
    <item>
      <title>Meta-Dynamical State Space Models for Integrative Neural Data Analysis</title>
      <link>https://arxiv.org/abs/2410.05454</link>
      <description>arXiv:2410.05454v1 Announce Type: cross 
Abstract: Learning shared structure across environments facilitates rapid learning and adaptive behavior in neural systems. This has been widely demonstrated and applied in machine learning to train models that are capable of generalizing to novel settings. However, there has been limited work exploiting the shared structure in neural activity during similar tasks for learning latent dynamics from neural recordings. Existing approaches are designed to infer dynamics from a single dataset and cannot be readily adapted to account for statistical heterogeneities across recordings. In this work, we hypothesize that similar tasks admit a corresponding family of related solutions and propose a novel approach for meta-learning this solution space from task-related neural activity of trained animals. Specifically, we capture the variabilities across recordings on a low-dimensional manifold which concisely parametrizes this family of dynamics, thereby facilitating rapid learning of latent dynamics given new recordings. We demonstrate the efficacy of our approach on few-shot reconstruction and forecasting of synthetic dynamical systems, and neural recordings from the motor cortex during different arm reaching tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05454v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ayesha Vermani, Josue Nassar, Hyungju Jeon, Matthew Dowling, Il Memming Park</dc:creator>
    </item>
    <item>
      <title>Hierarchy of chaotic dynamics in random modular networks</title>
      <link>https://arxiv.org/abs/2410.06361</link>
      <description>arXiv:2410.06361v1 Announce Type: cross 
Abstract: We introduce a model of randomly connected neural populations and study its dynamics by means of the dynamical mean-field theory and simulations. Our analysis uncovers a rich phase diagram, featuring high- and low-dimensional chaotic phases, separated by a crossover region characterized by low values of the maximal Lyapunov exponent and participation ratio dimension, but with high and rapidly changing values of the Lyapunov dimension. Counterintuitively, chaos can be attenuated by either adding noise to strongly modular connectivity or by introducing modularity into random connectivity. Extending the model to include a multilevel, hierarchical connectivity reveals that a loose balance between activities across levels drives the system towards the edge of chaos.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06361v1</guid>
      <category>physics.bio-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.NE</category>
      <category>nlin.CD</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>{\L}ukasz Ku\'smierz, Ulises Pereira-Obilinovic, Zhixin Lu, Dana Mastrovito, Stefan Mihalas</dc:creator>
    </item>
    <item>
      <title>Where are the bits in atoms? A perspective on the physical origin and evolutionary nature of information</title>
      <link>https://arxiv.org/abs/2407.09567</link>
      <description>arXiv:2407.09567v2 Announce Type: replace 
Abstract: This manuscript proposes a conceptual hypothesis regarding the ontology of information, which can serve as a foundation for future empirical exploration and theoretical development. Starting from the premise that information consists of a structural pattern of matter and analysing the current understanding of the evolution of structures and complexity in the universe, we propose to redefine information as a physical structure capable of evolving and driving complexity across multiple layers of self-organization - biological, cultural, civilizational, and cybernetic. The perspective highlights how information structures replicate, vary, and evolve across different domains, speculating on the emergence of a cybernetic layer where machines could evolve autonomously. This interdisciplinary framework challenges traditional views of information and encourages further research into its role in shaping the structures of the universe, offering a new perspective on the evolution of complexity across both natural and artificial systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09567v2</guid>
      <category>q-bio.NC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>physics.hist-ph</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wouter van der Wijngaart</dc:creator>
    </item>
    <item>
      <title>Active Inference Tree Search in Large POMDPs</title>
      <link>https://arxiv.org/abs/2103.13860</link>
      <description>arXiv:2103.13860v5 Announce Type: replace-cross 
Abstract: The ability to plan ahead efficiently is key for both living organisms and artificial systems. Model-based planning and prospection are widely studied in cognitive neuroscience and artificial intelligence (AI), but from different perspectives--and with different desiderata in mind (biological realism versus scalability) that are difficult to reconcile. Here, we introduce a novel method to plan in POMDPs--Active Inference Tree Search (AcT)--that combines the normative character and biological realism of a leading planning theory in neuroscience (Active Inference) and the scalability of tree search methods in AI. This unification enhances both approaches. On the one hand, tree searches enable the biologically grounded, first principle method of active inference to be applied to large-scale problems. On the other hand, active inference provides a principled solution to the exploration-exploitation dilemma, which is often addressed heuristically in tree search methods. Our simulations show that AcT successfully navigates binary trees that are challenging for sampling-based methods, problems that require adaptive exploration, and the large POMDP problem 'RockSample'--in which AcT reproduces state-of-the-art POMDP solutions. Furthermore, we illustrate how AcT can be used to simulate neurophysiological responses (e.g., in the hippocampus and prefrontal cortex) of humans and other animals that solve large planning problems. These numerical analyses show that Active Tree Search is a principled realisation of neuroscientific and AI planning theories, which offer both biological realism and scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2103.13860v5</guid>
      <category>cs.AI</category>
      <category>math.PR</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Domenico Maisto, Francesco Gregoretti, Karl Friston, Giovanni Pezzulo</dc:creator>
    </item>
    <item>
      <title>When does compositional structure yield compositional generalization? A kernel theory</title>
      <link>https://arxiv.org/abs/2405.16391</link>
      <description>arXiv:2405.16391v2 Announce Type: replace-cross 
Abstract: Compositional generalization (the ability to respond correctly to novel combinations of familiar components) is thought to be a cornerstone of intelligent behavior. Compositionally structured (e.g. disentangled) representations are essential for this; however, the conditions under which they yield compositional generalization remain unclear. To address this gap, we present a general theory of compositional generalization in kernel models with fixed representations, a tractable framework for characterizing the impact of dataset statistics on generalization. We find that kernel models are constrained to adding up values assigned to each combination of components seen during training ("conjunction-wise additivity"). This imposes fundamental restrictions on the set of tasks these models can learn, in particular preventing them from transitively generalizing equivalence relations. Even for compositional tasks that kernel models can in principle learn, we identify novel failure modes in compositional generalization that arise from biases in the training data and affect important compositional building blocks such as symbolic addition and context dependence (memorization leak and shortcut bias). Finally, we empirically validate our theory, showing that it captures the behavior of deep neural networks (convolutional networks, residual networks, and Vision Transformers) trained on a set of compositional tasks with similarly structured data. Ultimately, this work provides a theoretical perspective on how statistical structure in the training data can affect compositional generalization, with implications for how to identify and remedy failure modes in deep learning models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16391v2</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Lippl, Kim Stachenfeld</dc:creator>
    </item>
    <item>
      <title>Tilting the Odds at the Lottery: the Interplay of Overparameterisation and Curricula in Neural Networks</title>
      <link>https://arxiv.org/abs/2406.01589</link>
      <description>arXiv:2406.01589v2 Announce Type: replace-cross 
Abstract: A wide range of empirical and theoretical works have shown that overparameterisation can amplify the performance of neural networks. According to the lottery ticket hypothesis, overparameterised networks have an increased chance of containing a sub-network that is well-initialised to solve the task at hand. A more parsimonious approach, inspired by animal learning, consists in guiding the learner towards solving the task by curating the order of the examples, i.e. providing a curriculum. However, this learning strategy seems to be hardly beneficial in deep learning applications. In this work, we undertake an analytical study that connects curriculum learning and overparameterisation. In particular, we investigate their interplay in the online learning setting for a 2-layer network in the XOR-like Gaussian Mixture problem. Our results show that a high degree of overparameterisation -- while simplifying the problem -- can limit the benefit from curricula, providing a theoretical account of the ineffectiveness of curricula in deep learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01589v2</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefano Sarao Mannelli, Yaraslau Ivashynka, Andrew Saxe, Luca Saglietti</dc:creator>
    </item>
    <item>
      <title>Population Transformer: Learning Population-level Representations of Neural Activity</title>
      <link>https://arxiv.org/abs/2406.03044</link>
      <description>arXiv:2406.03044v2 Announce Type: replace-cross 
Abstract: We present a self-supervised framework that learns population-level codes for arbitrary ensembles of neural recordings at scale. We address two key challenges in scaling models with neural time-series data: sparse and variable electrode distribution across subjects and datasets. The Population Transformer (PopT) stacks on top of pretrained representations and enhances downstream decoding by enabling learned aggregation of multiple spatially-sparse data channels. The pretrained PopT lowers the amount of data required for downstream decoding experiments, while increasing accuracy, even on held-out subjects and tasks. Compared to end-to-end methods, this approach is computationally lightweight and more interpretable, while still retaining competitive performance. We further show how our framework is generalizable to multiple time-series embeddings and neural data modalities. Beyond decoding, we interpret the pretrained PopT and fine-tuned models to show how they can be used to extract neuroscience insights from massive amounts of data. We release our code as well as a pretrained PopT to enable off-the-shelf improvements in multi-channel intracranial data decoding and interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03044v2</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geeling Chau, Christopher Wang, Sabera Talukder, Vighnesh Subramaniam, Saraswati Soedarmadji, Yisong Yue, Boris Katz, Andrei Barbu</dc:creator>
    </item>
    <item>
      <title>Predictability maximization and the origins of word order harmony</title>
      <link>https://arxiv.org/abs/2408.16570</link>
      <description>arXiv:2408.16570v5 Announce Type: replace-cross 
Abstract: We address the linguistic problem of the sequential arrangement of a head and its dependents from an information theoretic perspective. In particular, we consider the optimal placement of a head that maximizes the predictability of the sequence. We assume that dependents are statistically independent given a head, in line with the open-choice principle and the core assumptions of dependency grammar. We demonstrate the optimality of harmonic order, i.e., placing the head last maximizes the predictability of the head whereas placing the head first maximizes the predictability of dependents. We also show that postponing the head is the optimal strategy to maximize its predictability while bringing it forward is the optimal strategy to maximize the predictability of dependents. We unravel the advantages of the strategy of maximizing the predictability of the head over maximizing the predictability of dependents. Our findings shed light on the placements of the head adopted by real languages or emerging in different kinds of experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16570v5</guid>
      <category>cs.CL</category>
      <category>physics.soc-ph</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ramon Ferrer-i-Cancho</dc:creator>
    </item>
  </channel>
</rss>
