<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 May 2025 04:00:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Scalable Bernoulli factories for Bayesian inference with intractable likelihoods</title>
      <link>https://arxiv.org/abs/2505.05438</link>
      <description>arXiv:2505.05438v1 Announce Type: new 
Abstract: Bernoulli factory MCMC algorithms implement accept-reject Markov chains without explicit computation of acceptance probabilities, and are used to target posterior distributions associated with intractable likelihood models. These algorithms often mix better than alternatives based on data augmentation or acceptance probability estimation. However, we show that their computational performance typically deteriorates exponentially with data size. To address this, we propose a simple divide-and-conquer Bernoulli factory MCMC algorithm and prove that it has polynomial complexity of degree between 1 and 2, with the exact degree depending on the existence of efficient unbiased estimators of the intractable likelihood ratio. We demonstrate the effectiveness of our approach with applications to Bayesian inference in two intractable likelihood models, and observe respective polynomial cost of degree 1.2 and 1 in the data size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05438v1</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timoth\'ee Stumpf-F\'etizon, Fl\'avio B. Gon\c{c}alves</dc:creator>
    </item>
    <item>
      <title>ChatGPT for automated grading of short answer questions in mechanical ventilation</title>
      <link>https://arxiv.org/abs/2505.04645</link>
      <description>arXiv:2505.04645v1 Announce Type: cross 
Abstract: Standardised tests using short answer questions (SAQs) are common in postgraduate education. Large language models (LLMs) simulate conversational language and interpret unstructured free-text responses in ways aligning with applying SAQ grading rubrics, making them attractive for automated grading. We evaluated ChatGPT 4o to grade SAQs in a postgraduate medical setting using data from 215 students (557 short-answer responses) enrolled in an online course on mechanical ventilation (2020--2024). Deidentified responses to three case-based scenarios were presented to ChatGPT with a standardised grading prompt and rubric. Outputs were analysed using mixed-effects modelling, variance component analysis, intraclass correlation coefficients (ICCs), Cohen's kappa, Kendall's W, and Bland--Altman statistics. ChatGPT awarded systematically lower marks than human graders with a mean difference (bias) of -1.34 on a 10-point scale. ICC values indicated poor individual-level agreement (ICC1 = 0.086), and Cohen's kappa (-0.0786) suggested no meaningful agreement. Variance component analysis showed minimal variability among the five ChatGPT sessions (G-value = 0.87), indicating internal consistency but divergence from the human grader. The poorest agreement was observed for evaluative and analytic items, whereas checklist and prescriptive rubric items had less disagreement. We caution against the use of LLMs in grading postgraduate coursework. Over 60% of ChatGPT-assigned grades differed from human grades by more than acceptable boundaries for high-stakes assessments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04645v1</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tejas Jade, Alex Yartsev</dc:creator>
    </item>
    <item>
      <title>Estimating the Heritability of Longitudinal Rate-of-Change: Genetic Insights into PSA Velocity in Prostate Cancer-Free Individuals</title>
      <link>https://arxiv.org/abs/2505.04773</link>
      <description>arXiv:2505.04773v1 Announce Type: cross 
Abstract: Serum prostate-specific antigen (PSA) is widely used for prostate cancer screening. While the genetics of PSA levels has been studied to enhance screening accuracy, the genetic basis of PSA velocity, the rate of PSA change over time, remains unclear. The Prostate, Lung, Colorectal, and Ovarian (PLCO) Cancer Screening Trial, a large, randomized study with longitudinal PSA data (15,260 cancer-free males, averaging 5.34 samples per subject) and genome-wide genotype data, provides a unique opportunity to estimate PSA velocity heritability. We developed a mixed model to jointly estimate heritability of PSA levels at age 54 and PSA velocity. To accommodate the large dataset, we implemented two efficient computational approaches: a partitioning and meta-analysis strategy using average information restricted maximum likelihood (AI-REML), and a fast restricted Haseman-Elston (REHE) regression method. Simulations showed that both methods yield unbiased estimates of both heritability metrics, with AI-REML providing smaller variability in the estimation of velocity heritability than REHE. Applying AI-REML to PLCO data, we estimated heritability at 0.32 (s.e. = 0.07) for baseline PSA and 0.45 (s.e. = 0.18) for PSA velocity. These findings reveal a substantial genetic contribution to PSA velocity, supporting future genome-wide studies to identify variants affecting PSA dynamics and improve PSA-based screening.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04773v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pei Zhang, Xiaoyu Wang, Jianxin Shi, Paul S. Albert</dc:creator>
    </item>
    <item>
      <title>Aesthetics Without Semantics</title>
      <link>https://arxiv.org/abs/2505.05331</link>
      <description>arXiv:2505.05331v1 Announce Type: cross 
Abstract: While it is easy for human observers to judge an image as beautiful or ugly, aesthetic decisions result from a combination of entangled perceptual and cognitive (semantic) factors, making the understanding of aesthetic judgements particularly challenging from a scientific point of view. Furthermore, our research shows a prevailing bias in current databases, which include mostly beautiful images, further complicating the study and prediction of aesthetic responses. We address these limitations by creating a database of images with minimal semantic content and devising, and next exploiting, a method to generate images on the ugly side of aesthetic valuations. The resulting Minimum Semantic Content (MSC) database consists of a large and balanced collection of 10,426 images, each evaluated by 100 observers. We next use established image metrics to demonstrate how augmenting an image set biased towards beautiful images with ugly images can modify, or even invert, an observed relationship between image features and aesthetics valuation. Taken together, our study reveals that works in empirical aesthetics attempting to link image content and aesthetic judgements may magnify, underestimate, or simply miss interesting effects due to a limitation of the range of aesthetic values they consider.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05331v1</guid>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <category>stat.CO</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>C. Alejandro Parraga (Comp. Sci. Dept., Engineering School, Universitat Aut\`onoma de Barcelona, Computer Vision Centre, Campus UAB, Bellaterra, 08193, Barcelona, Spain), Olivier Penacchio (Comp. Sci. Dept., Engineering School, Universitat Aut\`onoma de Barcelona, School of Psychology and Neuroscience, University of St Andrews, St Andrews, Fife KY16 9JP, United Kingdom), Marcos Mu\v{n}oz Gonzalez (Comp. Sci. Dept., Engineering School, Universitat Aut\`onoma de Barcelona), Bogdan Raducanu (Computer Vision Centre, Campus UAB, Bellaterra, 08193, Barcelona, Spain), Xavier Otazu (Comp. Sci. Dept., Engineering School, Universitat Aut\`onoma de Barcelona, Computer Vision Centre, Campus UAB, Bellaterra, 08193, Barcelona, Spain)</dc:creator>
    </item>
    <item>
      <title>Particle Gibbs without the Gibbs bit</title>
      <link>https://arxiv.org/abs/2505.04611</link>
      <description>arXiv:2505.04611v2 Announce Type: replace 
Abstract: Exact parameter and trajectory inference in state-space models is typically achieved by one of two methods: particle marginal Metropolis-Hastings (PMMH) or particle Gibbs (PGibbs). PMMH is a pseudo-marginal algorithm which jointly proposes a new trajectory and parameter, and accepts or rejects both at once. PGibbs instead alternates between sampling from the trajectory, using an algorithm known as conditional sequential Monte Carlo (CSMC) and the parameter in a Hastings-within-Gibbs fashion. While particle independent Metropolis Hastings (PIMH), the parameter-free version of PMMH, is known to be statistically worse than CSMC, PGibbs can induce a slow mixing if the parameter and the state trajectory are very correlated. This has made PMMH the method of choice for many practitioners, despite theory and experiments favouring CSMC over PIMH for the parameter-free problem. In this article, we describe a formulation of PGibbs which bypasses the Gibbs step, essentially marginalizing over the trajectory distribution in a fashion similar to PMMH. This is achieved by considering the implementation of a CSMC algortihm for the state-space model integrated over the joint distribution of the current parameter and the parameter proposal. We illustrate the benefits of method on a simple example known to be challenging for PMMH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04611v2</guid>
      <category>stat.CO</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrien Corenflos</dc:creator>
    </item>
    <item>
      <title>Summarizing Bayesian Nonparametric Mixture Posterior -- Sliced Optimal Transport Metrics for Gaussian Mixtures</title>
      <link>https://arxiv.org/abs/2411.14674</link>
      <description>arXiv:2411.14674v5 Announce Type: replace-cross 
Abstract: Existing methods to summarize posterior inference for mixture models focus on identifying a point estimate of the implied random partition for clustering, with density estimation as a secondary goal (Wade and Ghahramani, 2018; Dahl et al., 2022). We propose a novel approach for summarizing posterior inference in nonparametric Bayesian mixture models, prioritizing estimation of the mixing measure (or mixture) as an inference target. One of the key features is the model-agnostic nature of the approach, which remains valid under arbitrarily complex dependence structures in the underlying sampling model. Using a decision-theoretic framework, our method identifies a point estimate by minimizing posterior expected loss. A loss function is defined as a discrepancy between mixing measures. Estimating the mixing measure implies inference on the mixture density and the random partition. Exploiting the discrete nature of the mixing measure, we use a version of sliced Wasserstein distance. We introduce two specific variants for Gaussian mixtures. The first, mixed sliced Wasserstein, applies generalized geodesic projections on the product of the Euclidean space and the manifold of symmetric positive definite matrices. The second, sliced mixture Wasserstein, leverages the linearity of Gaussian mixture measures for efficient projection</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14674v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen, Peter Mueller</dc:creator>
    </item>
  </channel>
</rss>
