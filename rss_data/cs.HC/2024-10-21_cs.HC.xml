<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Oct 2024 03:29:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Comparative Study on Accessibility for Autistic Individuals with Urban Mobility Apps</title>
      <link>https://arxiv.org/abs/2410.14033</link>
      <description>arXiv:2410.14033v1 Announce Type: new 
Abstract: Autism Spectrum Disorder (ASD) is a neurodivergent condition with a wide range of characteristics and support levels. Individuals with ASD can exhibit various combinations of traits such as difficulties in social interaction, communication, and language, alongside restricted interests and repetitive activities. Many adults with ASD live independently due to increased awareness and late diagnoses, which help them manage long-standing challenges. Predictability, clarity, and minimized sensory stimuli are crucial for the daily comfort of autistic individuals. In mobile applications, autistic users face significant cognitive overload compared to neurotypicals, resulting in higher effort and time to complete tasks. Urban mobility apps, essential for daily routines, often overlook the needs of autistic users, leading to cognitive overload issues. This study investigates the accessibility of urban mobility apps for autistic individuals using the Interfaces Accessibility Guide for Autism (GAIA). By evaluating various apps, we have identified a common gap regarding accessibility for people with Autism Spectrum Disorder (ASD). This limitation relates to the absence of a functionality that allows users on the autism spectrum to customize the characteristics of the textual and visual elements of the software, such as changing the text font, altering the font type, and adjusting text colors, as well as native audio guidance within the applications themselves. Currently, the only function in this context is for visually impaired people, which completely changes the user experience in terms of navigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14033v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danilo Monteiro Ribeiro, Felipe Vasconcelos Melo, Vitor Negromonte, Gabriel Walisson Matias, Adna Farias, Celeste Azul, Ana Paula Chaves, Kiev Gama</dc:creator>
    </item>
    <item>
      <title>Student Reflections on Self-Initiated GenAI Use in HCI Education</title>
      <link>https://arxiv.org/abs/2410.14048</link>
      <description>arXiv:2410.14048v1 Announce Type: new 
Abstract: Generative Artificial Intelligence's (GenAI) impact on Human-Computer Interaction (HCI) education and technology design is pervasive but poorly understood. This study examines how graduate students in an applied HCI course utilized GenAI tools across various stages of interactive device design. Although the course policy neither explicitly encouraged nor prohibited using GenAI, students independently integrated these tools into their work. Through conducting 12 post-class group interviews, we reveal the dual nature of GenAI: while it stimulates creativity and accelerates design iterations, it also raises concerns about shallow learning and over-reliance. Our findings indicate that GenAI's benefits are most pronounced in the Execution phase of the design process, particularly for rapid prototyping and ideation. In contrast, its use in the Discover phase and design reflection may compromise depth. This study underscores the complex role of GenAI in HCI education and offers recommendations for curriculum improvements to better prepare future designers for effectively integrating GenAI into their creative processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14048v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hauke Sandhaus, Quiquan Gu, Maria Teresa Parreira, Wendy Ju</dc:creator>
    </item>
    <item>
      <title>An AI Guide to Enhance Accessibility of Social Virtual Reality for Blind People</title>
      <link>https://arxiv.org/abs/2410.14058</link>
      <description>arXiv:2410.14058v1 Announce Type: new 
Abstract: The rapid growth of virtual reality (VR) has led to increased use of social VR platforms for interaction. However, these platforms lack adequate features to support blind and low vision (BLV) users, posing significant challenges in navigation, visual interpretation, and social interaction. One promising approach to these challenges is employing human guides in VR. However, this approach faces limitations with a lack of availability of humans to serve as guides, or the inability to customize the guidance a user receives from the human guide. We introduce an AI-powered guide to address these limitations. The AI guide features six personas, each offering unique behaviors and appearances to meet diverse user needs, along with visual interpretation and navigation assistance. We aim to use this AI guide in the future to help us understand BLV users' preferences for guide forms and functionalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14058v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jazmin Collins, Kaylah Myranda Nicholson, Yusuf Khadir, Andrea Stevenson Won, Shiri Azenkot</dc:creator>
    </item>
    <item>
      <title>Text-to-Image Representativity Fairness Evaluation Framework</title>
      <link>https://arxiv.org/abs/2410.14201</link>
      <description>arXiv:2410.14201v1 Announce Type: new 
Abstract: Text-to-Image generative systems are progressing rapidly to be a source of advertisement and media and could soon serve as image searches or artists. However, there is a significant concern about the representativity bias these models embody and how these biases can propagate in the social fabric after fine-tuning them. Therefore, continuously monitoring and evaluating these models for fairness is important. To address this issue, we propose Text-to-Image (TTI) Representativity Fairness Evaluation Framework. In this framework, we evaluate three aspects of a TTI system; diversity, inclusion, and quality. For each aspect, human-based and model-based approaches are proposed and evaluated for their ability to capture the bias and whether they can substitute each other. The framework starts by suggesting the prompts for generating the images for the evaluation based on the context and the sensitive attributes under study. Then the three aspects are evaluated using the proposed approaches. Based on the evaluation, a decision is made regarding the representativity bias within the TTI system. The evaluation of our framework on Stable Diffusion shows that the framework can effectively capture the bias in TTI systems. The results also confirm that our proposed model based-approaches can substitute human-based approaches in three out of four components with high correlation, which could potentially reduce costs and automate the process. The study suggests that continual learning of the model on more inclusive data across disadvantaged minorities such as Indians and Middle Easterners is essential to mitigate current stereotyping and lack of inclusiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14201v1</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asma Yamani, Malak Baslyman</dc:creator>
    </item>
    <item>
      <title>Harmony: A Home Agent for Responsive Management and Action Optimization with a Locally Deployed Large Language Model</title>
      <link>https://arxiv.org/abs/2410.14252</link>
      <description>arXiv:2410.14252v1 Announce Type: new 
Abstract: Since the launch of GPT-3.5, intelligent home assistant technology based on large language models (LLMs) has made significant progress. These intelligent home assistant frameworks, such as those based on high-performance LLMs like GPT-4, have greatly expanded their functional range and application scenarios by computing on the cloud, enriching user experience and diversification. In order to optimize the privacy and economy of data processing while maintaining the powerful functions of LLMs, we propose Harmony, a smart home assistant framework that uses a locally deployable small-scale LLM. Based on Llama3-8b, an open LLM that can be easily deployed on a consumer-grade PC, Harmony does not send any data to the internet during operation, ensuring local computation and privacy secured. Harmony based on Llama3-8b achieved competitive performance on our benchmark tests with the framework used in related work with GPT-4. In addition to solving the issues mentioned above, Harmony can also take actions according to the user and home status, even if the user does not issue a command. For example, when the user wants to wake up later than normal on the weekend, Harmony would open the curtains only when the user gets up or prepare the room when the user comes home without requiring user commands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14252v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ziqi Yin, Mingxin Zhang, Daisuke Kawahara</dc:creator>
    </item>
    <item>
      <title>ChartifyText: Automated Chart Generation from Data-Involved Texts via LLM</title>
      <link>https://arxiv.org/abs/2410.14331</link>
      <description>arXiv:2410.14331v1 Announce Type: new 
Abstract: Text documents with numerical values involved are widely used in various applications such as scientific research, economy, public health and journalism. However, it is difficult for readers to quickly interpret such data-involved texts and gain deep insights. To fill this research gap, this work aims to automatically generate charts to accurately convey the underlying data and ideas to readers, which is essentially a challenging task. The challenges originate from text ambiguities, intrinsic sparsity and uncertainty of data in text documents, and subjective sentiment differences. Specifically, we propose ChartifyText, a novel fully-automated approach that leverages Large Language Models (LLMs) to convert complex data-involved texts to expressive charts. It consists of two major modules: tabular data inference and expressive chart generation. The tabular data inference module employs systematic prompt engineering to guide the LLM (e.g., GPT-4) to infer table data, where data ranges, uncertainties, missing data values and corresponding subjective sentiments are explicitly considered. The expressive chart generation module augments standard charts with intuitive visual encodings and concise texts to accurately convey the underlying data and insights. We extensively evaluate the effectiveness of ChartifyText on real-world data-involved text documents through case studies, in-depth interviews with three visualization experts, and a carefully-designed user study with 15 participants. The results demonstrate the usefulness and effectiveness of ChartifyText in helping readers efficiently and effectively make sense of data-involved texts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14331v1</guid>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Songheng Zhang, Lei Wang, Toby Jia-Jun Li, Qiaomu Shen, Yixin Cao, Yong Wang</dc:creator>
    </item>
    <item>
      <title>Assessing the Impact of AR-Assisted Warnings on Roadway Workers' Stress Under Different Workload Conditions</title>
      <link>https://arxiv.org/abs/2410.14537</link>
      <description>arXiv:2410.14537v1 Announce Type: new 
Abstract: Recent data from the Federal Highway Administration highlights an alarming increase in fatalities and injuries in roadway work zones, emphasizing the need for enhanced worker safety measures. This study addresses this concern by evaluating stress levels among roadway workers equipped with AR-assisted multi-sensory warning technology during varying work intensities. The research leverages a high-fidelity Virtual Reality environment to simulate realistic work scenarios, enabling safe evaluation of high-risk situations. Unlike previous studies focusing on external factors, this research investigates the internal physiological impact on workers. Utilizing wearable sensors, the study collected physiological data, including photoplethysmography (PPG), electrodermal activity (EDA), and skin temperature (ST), to assess stress levels continuously and non-invasively. Our findings from 18 participants reveal significant differences between light- and medium-intensity activities in heart rate variability metrics. These metrics commonly used to assess autonomic nervous system function and stress levels, included mean heart rate, NN50, pNN50, and HF-HRV. By examining the relationship between AR-enabled warnings, work intensity, and stress levels, the study contributes to enhancing worker safety and well-being. The proposed methodology offers potential for active stress monitoring in the field, contributing to enhanced safety practices and worker productivity in construction sites. By providing real-time physiological data, this approach enables informed stress management and more effective hazard warning systems in roadway work zones. This research bridges a gap in understanding the physiological impacts of AR-assisted warnings on roadway workers. The insights gained from this study can inform future safety interventions and guide the development of more effective warning systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14537v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fatemeh Banani Ardecani, Amit Kumar, Omidreza Shoghli</dc:creator>
    </item>
    <item>
      <title>Experimental Validation of Light Cable-Driven Elbow-Assisting Device L-CADEL Design</title>
      <link>https://arxiv.org/abs/2410.13870</link>
      <description>arXiv:2410.13870v1 Announce Type: cross 
Abstract: This paper presents a new design of CADEL, a cable-driven elbow-assisting device, with light weighting and control improvements. The new device design is appropriate to be more portable and user-oriented solution, presenting additional facilities with respect to the original design. One of potential benefits of improved portability can be envisaged in the possibility of house and hospital usage keeping social distancing while allowing rehabilitation treatments even during a pandemic spread. Specific attention has been devoted to design main mechatronic components by developing specific kinematics models. The design process includes an implementation of specific control hardware and software. The kinematic model of the new design is formulated and features are evaluated through numerical simulations and experimental tests. An evaluation from original design highlights the proposed improvements mainly in terms of comfort, portability and user-oriented operation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13870v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s42235-021-00133-5</arxiv:DOI>
      <arxiv:journal_reference>Journal of Bionic Engineering, 2022, 19 (2), pp.416-428</arxiv:journal_reference>
      <dc:creator>Med Amine Laribi (COBRA), Marco Ceccarelli (COBRA), Juan Sandoval (COBRA), Matteo Bottin (Unipd), Giulio Rosati</dc:creator>
    </item>
    <item>
      <title>Transformers Utilization in Chart Understanding: A Review of Recent Advances &amp; Future Trends</title>
      <link>https://arxiv.org/abs/2410.13883</link>
      <description>arXiv:2410.13883v1 Announce Type: cross 
Abstract: In recent years, interest in vision-language tasks has grown, especially those involving chart interactions. These tasks are inherently multimodal, requiring models to process chart images, accompanying text, underlying data tables, and often user queries. Traditionally, Chart Understanding (CU) relied on heuristics and rule-based systems. However, recent advancements that have integrated transformer architectures significantly improved performance. This paper reviews prominent research in CU, focusing on State-of-The-Art (SoTA) frameworks that employ transformers within End-to-End (E2E) solutions. Relevant benchmarking datasets and evaluation techniques are analyzed. Additionally, this article identifies key challenges and outlines promising future directions for advancing CU solutions. Following the PRISMA guidelines, a comprehensive literature search is conducted across Google Scholar, focusing on publications from Jan'20 to Jun'24. After rigorous screening and quality assessment, 32 studies are selected for in-depth analysis. The CU tasks are categorized into a three-layered paradigm based on the cognitive task required. Recent advancements in the frameworks addressing various CU tasks are also reviewed. Frameworks are categorized into single-task or multi-task based on the number of tasks solvable by the E2E solution. Within multi-task frameworks, pre-trained and prompt-engineering-based techniques are explored. This review overviews leading architectures, datasets, and pre-training tasks. Despite significant progress, challenges remain in OCR dependency, handling low-resolution images, and enhancing visual reasoning. Future directions include addressing these challenges, developing robust benchmarks, and optimizing model efficiency. Additionally, integrating explainable AI techniques and exploring the balance between real and synthetic data are crucial for advancing CU research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13883v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mirna Al-Shetairy, Hanan Hindy, Dina Khattab, Mostafa M. Aref</dc:creator>
    </item>
    <item>
      <title>Generating Signed Language Instructions in Large-Scale Dialogue Systems</title>
      <link>https://arxiv.org/abs/2410.14026</link>
      <description>arXiv:2410.14026v1 Announce Type: cross 
Abstract: We introduce a goal-oriented conversational AI system enhanced with American Sign Language (ASL) instructions, presenting the first implementation of such a system on a worldwide multimodal conversational AI platform. Accessible through a touch-based interface, our system receives input from users and seamlessly generates ASL instructions by leveraging retrieval methods and cognitively based gloss translations. Central to our design is a sign translation module powered by Large Language Models, alongside a token-based video retrieval system for delivering instructional content from recipes and wikiHow guides. Our development process is deeply rooted in a commitment to community engagement, incorporating insights from the Deaf and Hard-of-Hearing community, as well as experts in cognitive and ASL learning sciences. The effectiveness of our signing instructions is validated by user feedback, achieving ratings on par with those of the system in its non-signing variant. Additionally, our system demonstrates exceptional performance in retrieval accuracy and text-generation quality, measured by metrics such as BERTScore. We have made our codebase and datasets publicly accessible at https://github.com/Merterm/signed-dialogue, and a demo of our signed instruction video retrieval system is available at https://huggingface.co/spaces/merterm/signed-instructions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14026v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mert \.Inan, Katherine Atwell, Anthony Sicilia, Lorna Quandt, Malihe Alikhani</dc:creator>
    </item>
    <item>
      <title>Learning Multimodal Cues of Children's Uncertainty</title>
      <link>https://arxiv.org/abs/2410.14050</link>
      <description>arXiv:2410.14050v1 Announce Type: cross 
Abstract: Understanding uncertainty plays a critical role in achieving common ground (Clark et al.,1983). This is especially important for multimodal AI systems that collaborate with users to solve a problem or guide the user through a challenging concept. In this work, for the first time, we present a dataset annotated in collaboration with developmental and cognitive psychologists for the purpose of studying nonverbal cues of uncertainty. We then present an analysis of the data, studying different roles of uncertainty and its relationship with task difficulty and performance. Lastly, we present a multimodal machine learning model that can predict uncertainty given a real-time video clip of a participant, which we find improves upon a baseline multimodal transformer model. This work informs research on cognitive coordination between human-human and human-AI and has broad implications for gesture understanding and generation. The anonymized version of our data and code will be publicly available upon the completion of the required consent forms and data sheets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14050v1</guid>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qi Cheng, Mert \.Inan, Rahma Mbarki, Grace Grmek, Theresa Choi, Yiming Sun, Kimele Persaud, Jenny Wang, Malihe Alikhani</dc:creator>
    </item>
    <item>
      <title>CausalChat: Interactive Causal Model Development and Refinement Using Large Language Models</title>
      <link>https://arxiv.org/abs/2410.14146</link>
      <description>arXiv:2410.14146v1 Announce Type: cross 
Abstract: Causal networks are widely used in many fields to model the complex relationships between variables. A recent approach has sought to construct causal networks by leveraging the wisdom of crowds through the collective participation of humans. While this can yield detailed causal networks that model the underlying phenomena quite well, it requires a large number of individuals with domain understanding. We adopt a different approach: leveraging the causal knowledge that large language models, such as OpenAI's GPT-4, have learned by ingesting massive amounts of literature. Within a dedicated visual analytics interface, called CausalChat, users explore single variables or variable pairs recursively to identify causal relations, latent variables, confounders, and mediators, constructing detailed causal networks through conversation. Each probing interaction is translated into a tailored GPT-4 prompt and the response is conveyed through visual representations which are linked to the generated text for explanations. We demonstrate the functionality of CausalChat across diverse data contexts and conduct user studies involving both domain experts and laypersons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14146v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanming Zhang, Akshith Kota, Eric Papenhausen, Klaus Mueller</dc:creator>
    </item>
    <item>
      <title>Auto Detecting Cognitive Events Using Machine Learning on Pupillary Data</title>
      <link>https://arxiv.org/abs/2410.14174</link>
      <description>arXiv:2410.14174v1 Announce Type: cross 
Abstract: Assessing cognitive workload is crucial for human performance as it affects information processing, decision making, and task execution. Pupil size is a valuable indicator of cognitive workload, reflecting changes in attention and arousal governed by the autonomic nervous system. Cognitive events are closely linked to cognitive workload as they activate mental processes and trigger cognitive responses. This study explores the potential of using machine learning to automatically detect cognitive events experienced using individuals. We framed the problem as a binary classification task, focusing on detecting stimulus onset across four cognitive tasks using CNN models and 1-second pupillary data. The results, measured by Matthew's correlation coefficient, ranged from 0.47 to 0.80, depending on the cognitive task. This paper discusses the trade-offs between generalization and specialization, model behavior when encountering unseen stimulus onset times, structural variances among cognitive tasks, factors influencing model predictions, and real-time simulation. These findings highlight the potential of machine learning techniques in detecting cognitive events based on pupil and eye movement responses, contributing to advancements in personalized learning and optimizing neurocognitive workload management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14174v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quang Dang, Murat Kucukosmanoglu, Michael Anoruo, Golshan Kargosha, Sarah Conklin, Justin Brooks</dc:creator>
    </item>
    <item>
      <title>Evaluating the evaluators: Towards human-aligned metrics for missing markers reconstruction</title>
      <link>https://arxiv.org/abs/2410.14334</link>
      <description>arXiv:2410.14334v1 Announce Type: cross 
Abstract: Animation data is often obtained through optical motion capture systems, which utilize a multitude of cameras to establish the position of optical markers. However, system errors or occlusions can result in missing markers, the manual cleaning of which can be time-consuming. This has sparked interest in machine learning-based solutions for missing marker reconstruction in the academic community. Most academic papers utilize a simplistic mean square error as the main metric. In this paper, we show that this metric does not correlate with subjective perception of the fill quality. We introduce and evaluate a set of better-correlated metrics that can drive progress in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14334v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taras Kucherenko, Derek Peristy, Judith B\"utepage</dc:creator>
    </item>
    <item>
      <title>Perception of Emotions in Human and Robot Faces: Is the Eye Region Enough?</title>
      <link>https://arxiv.org/abs/2410.14337</link>
      <description>arXiv:2410.14337v1 Announce Type: cross 
Abstract: The increased interest in developing next-gen social robots has raised questions about the factors affecting the perception of robot emotions. This study investigates the impact of robot appearances (humanlike, mechanical) and face regions (full-face, eye-region) on human perception of robot emotions. A between-subjects user study (N = 305) was conducted where participants were asked to identify the emotions being displayed in videos of robot faces, as well as a human baseline. Our findings reveal three important insights for effective social robot face design in Human-Robot Interaction (HRI): Firstly, robots equipped with a back-projected, fully animated face - regardless of whether they are more human-like or more mechanical-looking - demonstrate a capacity for emotional expression comparable to that of humans. Secondly, the recognition accuracy of emotional expressions in both humans and robots declines when only the eye region is visible. Lastly, within the constraint of only the eye region being visible, robots with more human-like features significantly enhance emotion recognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14337v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chinmaya Mishra, Gabriel Skantze, Peter Hagoort, Rinus Verdonschot</dc:creator>
    </item>
    <item>
      <title>Generative AI, Pragmatics, and Authenticity in Second Language Learning</title>
      <link>https://arxiv.org/abs/2410.14395</link>
      <description>arXiv:2410.14395v1 Announce Type: cross 
Abstract: There are obvious benefits to integrating generative AI (artificial intelligence) into language learning and teaching. Those include using AI as a language tutor, creating learning materials, or assessing learner output. However, due to how AI systems under-stand human language, based on a mathematical model using statistical probability, they lack the lived experience to be able to use language with the same social aware-ness as humans. Additionally, there are built-in linguistic and cultural biases based on their training data which is mostly in English and predominantly from Western sources. Those facts limit AI suitability for some language learning interactions. Stud-ies have clearly shown that systems such as ChatGPT often do not produce language that is pragmatically appropriate. The lack of linguistic and cultural authenticity has important implications for how AI is integrated into second language acquisition as well as in instruction targeting development of intercultural communication compe-tence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14395v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Godwin-Jones`</dc:creator>
    </item>
    <item>
      <title>Embodied Exploration of Latent Spaces and Explainable AI</title>
      <link>https://arxiv.org/abs/2410.14590</link>
      <description>arXiv:2410.14590v1 Announce Type: cross 
Abstract: In this paper, we explore how performers' embodied interactions with a Neural Audio Synthesis model allow the exploration of the latent space of such a model, mediated through movements sensed by e-textiles. We provide background and context for the performance, highlighting the potential of embodied practices to contribute to developing explainable AI systems. By integrating various artistic domains with explainable AI principles, our interdisciplinary exploration contributes to the discourse on art, embodiment, and AI, offering insights into intuitive approaches found through bodily expression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14590v1</guid>
      <category>cs.SD</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>eess.AS</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elizabeth Wilson, Mika Satomi, Alex McLean, Deva Schubert, Juan Felipe Amaya Gonzalez</dc:creator>
    </item>
    <item>
      <title>On the Use of Proxies in Political Ad Targeting</title>
      <link>https://arxiv.org/abs/2410.14617</link>
      <description>arXiv:2410.14617v1 Announce Type: cross 
Abstract: Detailed targeting of advertisements has long been one of the core offerings of online platforms. Unfortunately, malicious advertisers have frequently abused such targeting features, with results that range from violating civil rights laws to driving division, polarization, and even social unrest. Platforms have often attempted to mitigate this behavior by removing targeting attributes deemed problematic, such as inferred political leaning, religion, or ethnicity. In this work, we examine the effectiveness of these mitigations by collecting data from political ads placed on Facebook in the lead up to the 2022 U.S. midterm elections. We show that major political advertisers circumvented these mitigations by targeting proxy attributes: seemingly innocuous targeting criteria that closely correspond to political and racial divides in American society. We introduce novel methods for directly measuring the skew of various targeting criteria to quantify their effectiveness as proxies, and then examine the scale at which those attributes are used. Our findings have crucial implications for the ongoing discussion on the regulation of political advertising and emphasize the urgency for increased transparency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14617v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3686917</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the ACM on Human-Computer Interaction 8. CSCW (2024)</arxiv:journal_reference>
      <dc:creator>Piotr Sapiezynski, Levi Kaplan, Alan Mislove, Aleksandra Korolova</dc:creator>
    </item>
    <item>
      <title>A Review of the Non-Invasive Techniques for Monitoring Different Aspects of Sleep</title>
      <link>https://arxiv.org/abs/2104.12964</link>
      <description>arXiv:2104.12964v2 Announce Type: replace 
Abstract: Quality sleep is very important for a healthy life. Nowadays, many people around the world are not getting enough sleep which is having negative impacts on their lifestyles. Studies are being conducted for sleep monitoring and have now become an important tool for understanding sleep behavior. The gold standard method for sleep analysis is polysomnography (PSG) conducted in a clinical environment but this method is both expensive and complex for long-term use. With the advancements in the field of sensors and the introduction of off-the-shelf technologies, unobtrusive solutions are becoming common as alternatives for in-home sleep monitoring. Various solutions have been proposed using both wearable and non-wearable methods which are cheap and easy to use for in-home sleep monitoring. In this paper, we present a comprehensive survey of the latest research works (2015 and after) conducted in various categories of sleep monitoring including sleep stage classification, sleep posture recognition, sleep disorders detection, and vital signs monitoring. We review the latest works done using the non-invasive approach and cover both wearable and non-wearable methods. We discuss the design approaches and key attributes of the work presented and provide an extensive analysis based on 10 key factors, to give a comprehensive overview of the recent developments and trends in all four categories of sleep monitoring. We also present some publicly available datasets for different categories of sleep monitoring. In the end, we discuss several open issues and provide future research directions in the area of sleep monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.12964v2</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3491245</arxiv:DOI>
      <dc:creator>Zawar Hussain, Quan Z. Sheng, Wei Emma Zhang, Jorge Ortiz, Seyedamin Pouriyeh</dc:creator>
    </item>
    <item>
      <title>Designing Prompt Analytics Dashboards to Analyze Student-ChatGPT Interactions in EFL Writing</title>
      <link>https://arxiv.org/abs/2405.19691</link>
      <description>arXiv:2405.19691v2 Announce Type: replace 
Abstract: While ChatGPT has significantly impacted education by offering personalized resources for students, its integration into educational settings poses unprecedented risks, such as inaccuracies and biases in AI-generated content, plagiarism and over-reliance on AI, and privacy and security issues. To help teachers address such risks, we conducted a two-phase iterative design process that comprises surveys, interviews, and prototype demonstration involving six EFL (English as a Foreign Language) teachers, who integrated ChatGPT into semester-long English essay writing classes. Based on the needs identified during the initial survey and interviews, we developed a prototype of Prompt Analytics Dashboard (PAD) that integrates the essay editing history and chat logs between students and ChatGPT. Teacher's feedback on the prototype informs additional features and unmet needs for designing future PAD, which helps them (1) analyze contextual analysis of student behaviors, (2) design an overall learning loop, and (3) develop their teaching skills.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19691v2</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minsun Kim, SeonGyeom Kim, Suyoun Lee, Yoosang Yoon, Junho Myung, Haneul Yoo, Hyunseung Lim, Jieun Han, Yoonsu Kim, So-Yeon Ahn, Juho Kim, Alice Oh, Hwajung Hong, Tak Yeon Lee</dc:creator>
    </item>
    <item>
      <title>Beyond the "Industry Standard": Focusing Gender-Affirming Voice Training Technologies on Individualized Goal Exploration</title>
      <link>https://arxiv.org/abs/2410.09958</link>
      <description>arXiv:2410.09958v2 Announce Type: replace 
Abstract: Gender-affirming voice training is critical for the transition process for many transgender individuals, enabling their voice to align with their gender identity. Individualized voice goals guide and motivate the voice training journey, but existing voice training technologies fail to define clear goals. We interviewed six voice experts and ten transgender individuals with voice training experience (voice trainees), focusing on how they defined, triangulated, and used voice goals. We found that goal voice exploration involves navigation between approximate and clear goals, and continuous reevaluation throughout the voice training journey. Our study reveals how voice examples, character descriptions, and voice modification and training technologies inform goal exploration, and identifies risks of overemphasizing goals. We identified technological implications informed by the separation of voice goals and targets, and provide a framework for a voice-changer-based goal exploration tool based on brainstorming with trainees and experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09958v2</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kassie Povinelli, Hanxiu "Hazel" Zhu, Yuhang Zhao</dc:creator>
    </item>
    <item>
      <title>Practices and Challenges of Online Love-seeking Among Deaf or Hard of Hearing People: A Case Study in China</title>
      <link>https://arxiv.org/abs/2410.11810</link>
      <description>arXiv:2410.11810v3 Announce Type: replace 
Abstract: People who are deaf or hard of hearing (DHH) in China are increasingly exploring online platforms to connect with potential partners. This research explores the online dating experiences of DHH communities in China, an area that has not been extensively researched. We interviewed sixteen participants who have varying levels of hearing ability and love-seeking statuses to understand how they manage their identities and communicate with potential partners online. We find that DHH individuals made great efforts to navigate the rich modality features to seek love online. Participants used both algorithm-based dating apps and community-based platforms like forums and WeChat to facilitate initial encounters through text-based functions that minimized the need for auditory interaction, thus fostering a more equitable starting point. Community-based platforms were found to facilitate more in-depth communication and excelled in fostering trust and authenticity, providing a more secure environment for genuine relationships. Design recommendations are proposed to enhance the accessibility and inclusiveness of online dating platforms for DHH individuals in China. This research sheds light on the benefits and challenges of online dating for DHH individuals in China and provides guidance for platform developers and researchers to enhance user experience in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11810v3</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Beiyan Cao, Changyang He, Jingling Zhang, Yuru Huang, Muzhi Zhou, Mingming Fan</dc:creator>
    </item>
    <item>
      <title>Drillboards: Adaptive Visualization Dashboards for Dynamic Personalization of Visualization Experiences</title>
      <link>https://arxiv.org/abs/2410.12744</link>
      <description>arXiv:2410.12744v2 Announce Type: replace 
Abstract: We present drillboards, a technique for adaptive visualization dashboards consisting of a hierarchy of coordinated charts that the user can drill down to reach a desired level of detail depending on their expertise, interest, and desired effort. This functionality allows different users to personalize the same dashboard to their specific needs and expertise. The technique is based on a formal vocabulary of chart representations and rules for merging multiple charts of different types and data into single composite representations. The drillboard hierarchy is created by iteratively applying these rules starting from a baseline dashboard, with each consecutive operation yielding a new dashboard with fewer charts and progressively more abstract and simplified views. We also present an authoring tool for building drillboards and show how experts users can use to build up and deliver personalized experiences to a wide audience. Our evaluation asked three domain experts to author drillboards for their own datasets, which we then showed to casual end-users with favorable outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12744v2</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sungbok Shin, Inyoup Na, Niklas Elmqvist</dc:creator>
    </item>
    <item>
      <title>A simplicity bubble problem and zemblanity in digitally intermediated societies</title>
      <link>https://arxiv.org/abs/2304.10681</link>
      <description>arXiv:2304.10681v3 Announce Type: replace-cross 
Abstract: In this article, we discuss the ubiquity of Big Data and machine learning in society and propose that it evinces the need of further investigation of their fundamental limitations. We extend the ``too much information tends to behave like very little information'' phenomenon to formal knowledge about lawlike universes and arbitrary collections of computably generated datasets. This gives rise to the simplicity bubble problem, which refers to a learning algorithm equipped with a formal theory that can be deceived by a dataset to find a locally optimal model which it deems to be the global one. In the context of lawlike (computable) universes and formal learning systems, we show that there is a ceiling above which formal knowledge cannot further decrease the probability of zemblanitous findings, should the randomly generated data made available to the formal learning system be sufficiently large in comparison to their joint complexity. Zemblanity, the opposite of serendipity, is defined by an undesirable but expected finding that reveals an underlying problem or negative consequence in a given model or theory, which is in principle predictable in case the formal theory contains sufficient information. We also argue that this is an epistemological limitation that may generate unpredictable problems in digitally intermediated societies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.10681v3</guid>
      <category>cs.IT</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <category>math.IT</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felipe S. Abrah\~ao, Ricardo P. Cavassane, Michael Winter, Mariana Vitti Rodrigues, Itala M. L. D'Ottaviano</dc:creator>
    </item>
    <item>
      <title>Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded Egocentric Perception</title>
      <link>https://arxiv.org/abs/2308.05822</link>
      <description>arXiv:2308.05822v3 Announce Type: replace-cross 
Abstract: We depend on our own memory to encode, store, and retrieve our experiences. However, memory lapses can occur. One promising avenue for achieving memory augmentation is through the use of augmented reality head-mounted displays to capture and preserve egocentric videos, a practice commonly referred to as lifelogging. However, a significant challenge arises from the sheer volume of video data generated through lifelogging, as the current technology lacks the capability to encode and store such large amounts of data efficiently. Further, retrieving specific information from extensive video archives requires substantial computational power, further complicating the task of quickly accessing desired content. To address these challenges, we propose a memory augmentation agent that involves leveraging natural language encoding for video data and storing them in a vector database. This approach harnesses the power of large vision language models to perform the language encoding process. Additionally, we propose using large language models to facilitate natural language querying. Our agent underwent extensive evaluation using the QA-Ego4D dataset and achieved state-of-the-art results with a BLEU score of 8.3, outperforming conventional machine learning models that scored between 3.4 and 5.8. Additionally, we conducted a user study in which participants interacted with the human memory augmentation agent through episodic memory and open-ended questions. The results of this study show that the agent results in significantly better recall performance on episodic memory tasks compared to human participants. The results also highlight the agent's practical applicability and user acceptance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.05822v3</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junxiao Shen, John Dudley, Per Ola Kristensson</dc:creator>
    </item>
    <item>
      <title>IncidentResponseGPT: Generating Traffic Incident Response Plans with Generative Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2404.18550</link>
      <description>arXiv:2404.18550v4 Announce Type: replace-cross 
Abstract: The proposed IncidentResponseGPT framework - a novel system that applies generative artificial intelligence (AI) to potentially enhance the efficiency and effectiveness of traffic incident response. This model allows for synthesis of region-specific incident response guidelines and generates incident response plans adapted to specific area, aiming to expedite decision-making for traffic management authorities. This approach aims to accelerate incident resolution times by suggesting various recommendations (e.g. optimal rerouting strategies, estimating resource needs) to minimize the overall impact on the urban traffic network. The system suggests specific actions, including dynamic lane closures, optimized rerouting and dispatching appropriate emergency resources. We utilize the Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) to rank generated response plans based on criteria like impact minimization and resource efficiency based on their proximity to an human-proposed solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18550v4</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Artur Grigorev, Adriana-Simona Mihaita Khaled Saleh, Yuming Ou</dc:creator>
    </item>
    <item>
      <title>Long-Term Human Trajectory Prediction using 3D Dynamic Scene Graphs</title>
      <link>https://arxiv.org/abs/2405.00552</link>
      <description>arXiv:2405.00552v3 Announce Type: replace-cross 
Abstract: We present a novel approach for long-term human trajectory prediction in indoor human-centric environments, which is essential for long-horizon robot planning in these environments. State-of-the-art human trajectory prediction methods are limited by their focus on collision avoidance and short-term planning, and their inability to model complex interactions of humans with the environment. In contrast, our approach overcomes these limitations by predicting sequences of human interactions with the environment and using this information to guide trajectory predictions over a horizon of up to 60s. We leverage Large Language Models (LLMs) to predict interactions with the environment by conditioning the LLM prediction on rich contextual information about the scene. This information is given as a 3D Dynamic Scene Graph that encodes the geometry, semantics, and traversability of the environment into a hierarchical representation. We then ground these interaction sequences into multi-modal spatio-temporal distributions over human positions using a probabilistic approach based on continuous-time Markov Chains. To evaluate our approach, we introduce a new semi-synthetic dataset of long-term human trajectories in complex indoor environments, which also includes annotations of human-object interactions. We show in thorough experimental evaluations that our approach achieves a 54% lower average negative log-likelihood and a 26.5% lower Best-of-20 displacement error compared to the best non-privileged (i.e., evaluated in a zero-shot fashion on the dataset) baselines for a time horizon of 60s.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00552v3</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/LRA.2024.3482169</arxiv:DOI>
      <dc:creator>Nicolas Gorlo, Lukas Schmid, Luca Carlone</dc:creator>
    </item>
    <item>
      <title>CAAP: Context-Aware Action Planning Prompting to Solve Computer Tasks with Front-End UI Only</title>
      <link>https://arxiv.org/abs/2406.06947</link>
      <description>arXiv:2406.06947v2 Announce Type: replace-cross 
Abstract: Software robots have long been used in Robotic Process Automation (RPA) to automate mundane and repetitive computer tasks. With the advent of Large Language Models (LLMs) and their advanced reasoning capabilities, these agents are now able to handle more complex or previously unseen tasks. However, LLM-based automation techniques in recent literature frequently rely on HTML source code for input or application-specific API calls for actions, limiting their applicability to specific environments. We propose an LLM-based agent that mimics human behavior in solving computer tasks. It perceives its environment solely through screenshot images, which are then converted into text for an LLM to process. By leveraging the reasoning capability of the LLM, we eliminate the need for large-scale human demonstration data typically required for model training. The agent only executes keyboard and mouse operations on Graphical User Interface (GUI), removing the need for pre-provided APIs to function. To further enhance the agent's performance in this setting, we propose a novel prompting strategy called Context-Aware Action Planning (CAAP) prompting, which enables the agent to thoroughly examine the task context from multiple perspectives. Our agent achieves an average success rate of 94.5% on MiniWoB++ and an average task score of 62.3 on WebShop, outperforming all previous studies of agents that rely solely on screen images. This method demonstrates potential for broader applications, particularly for tasks requiring coordination across multiple applications on desktops or smartphones, marking a significant advancement in the field of automation agents. Codes and models are accessible at https://github.com/caap-agent/caap-agent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06947v2</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junhee Cho, Jihoon Kim, Daseul Bae, Jinho Choo, Youngjune Gwon, Yeong-Dae Kwon</dc:creator>
    </item>
    <item>
      <title>Learning Social Cost Functions for Human-Aware Path Planning</title>
      <link>https://arxiv.org/abs/2407.10547</link>
      <description>arXiv:2407.10547v2 Announce Type: replace-cross 
Abstract: Achieving social acceptance is one of the main goals of Social Robotic Navigation. Despite this topic has received increasing interest in recent years, most of the research has focused on driving the robotic agent along obstacle-free trajectories, planning around estimates of future human motion to respect personal distances and optimize navigation. However, social interactions in everyday life are also dictated by norms that do not strictly depend on movement, such as when standing at the end of a queue rather than cutting it. In this paper, we propose a novel method to recognize common social scenarios and modify a traditional planner's cost function to adapt to them. This solution enables the robot to carry out different social navigation behaviors that would not arise otherwise, maintaining the robustness of traditional navigation. Our approach allows the robot to learn different social norms with a single learned model, rather than having different modules for each task. As a proof of concept, we consider the tasks of queuing and respect interaction spaces of groups of people talking to one another, but the method can be extended to other human activities that do not involve motion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10547v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Eirale, Matteo Leonetti, Marcello Chiaberge</dc:creator>
    </item>
    <item>
      <title>Integrating Expert Judgment and Algorithmic Decision Making: An Indistinguishability Framework</title>
      <link>https://arxiv.org/abs/2410.08783</link>
      <description>arXiv:2410.08783v2 Announce Type: replace-cross 
Abstract: We introduce a novel framework for human-AI collaboration in prediction and decision tasks. Our approach leverages human judgment to distinguish inputs which are algorithmically indistinguishable, or "look the same" to any feasible predictive algorithm. We argue that this framing clarifies the problem of human-AI collaboration in prediction and decision tasks, as experts often form judgments by drawing on information which is not encoded in an algorithm's training data. Algorithmic indistinguishability yields a natural test for assessing whether experts incorporate this kind of "side information", and further provides a simple but principled method for selectively incorporating human feedback into algorithmic predictions. We show that this method provably improves the performance of any feasible algorithmic predictor and precisely quantify this improvement. We demonstrate the utility of our framework in a case study of emergency room triage decisions, where we find that although algorithmic risk scores are highly competitive with physicians, there is strong evidence that physician judgments provide signal which could not be replicated by any predictive algorithm. This insight yields a range of natural decision rules which leverage the complementary strengths of human experts and predictive algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08783v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rohan Alur, Loren Laine, Darrick K. Li, Dennis Shung, Manish Raghavan, Devavrat Shah</dc:creator>
    </item>
    <item>
      <title>UniAutoML: A Human-Centered Framework for Unified Discriminative and Generative AutoML with Large Language Models</title>
      <link>https://arxiv.org/abs/2410.12841</link>
      <description>arXiv:2410.12841v2 Announce Type: replace-cross 
Abstract: Automated Machine Learning (AutoML) has simplified complex ML processes such as data pre-processing, model selection, and hyper-parameter searching. However, traditional AutoML frameworks focus solely on discriminative tasks, often falling short in tackling AutoML for generative models. Additionally, these frameworks lack interpretability and user engagement during the training process, primarily due to the absence of human-centered design. It leads to a lack of transparency in final decision-making and limited user control, potentially reducing trust and adoption of AutoML methods. To address these limitations, we introduce UniAutoML, a human-centered AutoML framework that leverages Large Language Models (LLMs) to unify AutoML for both discriminative (e.g., Transformers and CNNs for classification or regression tasks) and generative tasks (e.g., fine-tuning diffusion models or LLMs). The human-centered design of UniAutoML innovatively features a conversational user interface (CUI) that facilitates natural language interactions, providing users with real-time guidance, feedback, and progress updates for better interpretability. This design enhances transparency and user control throughout the AutoML training process, allowing users to seamlessly break down or modify the model being trained. To mitigate potential risks associated with LLM generated content, UniAutoML incorporates a safety guardline that filters inputs and censors outputs. We evaluated UniAutoML's performance and usability through experiments on eight diverse datasets and user studies involving 25 participants, demonstrating that UniAutoML not only enhances performance but also improves user control and trust. Our human-centered design bridges the gap between AutoML capabilities and user understanding, making ML more accessible to a broader audience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12841v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiayi Guo, Zan Chen, Yingrui Ji, Liyun Zhang, Daqin Luo, Zhigang Li, Yiqin Shen</dc:creator>
    </item>
  </channel>
</rss>
