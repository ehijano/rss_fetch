<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Feb 2026 02:45:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Sub-optimality bounds for certainty equivalent policies in partially observed systems</title>
      <link>https://arxiv.org/abs/2602.02814</link>
      <description>arXiv:2602.02814v1 Announce Type: new 
Abstract: In this paper, we present a generalization of the certainty equivalence principle of stochastic control. One interpretation of the classical certainty equivalence principle for linear systems with output feedback and quadratic costs is as follows: the optimal action at each time is obtained by evaluating the optimal state-feedback policy of the stochastic linear system at the minimum mean square error (MMSE) estimate of the state. Motivated by this interpretation, we consider certainty equivalent policies for general (non-linear) partially observed stochastic systems that allow for any state estimate rather than restricting to MMSE estimates. In such settings, the certainty equivalent policy is not optimal. For models where the cost and the dynamics are smooth in an appropriate sense, we derive upper bounds on the sub-optimality of certainty equivalent policies. We present several examples to illustrate the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02814v1</guid>
      <category>math.OC</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Berk Bozkurt, Aditya Mahajan, Ashutosh Nayyar, Yi Ouyang</dc:creator>
    </item>
    <item>
      <title>Fast Near Time-Optimal Motion Planning for Holonomic Vehicles in Structured Environments</title>
      <link>https://arxiv.org/abs/2602.02826</link>
      <description>arXiv:2602.02826v1 Announce Type: new 
Abstract: This paper proposes a novel and efficient optimization-based method for generating near time-optimal trajectories for holonomic vehicles navigating through complex but structured environments. The approach aims to solve the problem of motion planning for planar motion systems using magnetic levitation that can be used in assembly lines, automated laboratories or clean-rooms. In these applications, time-optimal trajectories that can be computed in real-time are required to increase productivity and allow the vehicles to be reactive if needed. The presented approach encodes the environment representation using free-space corridors and represents the motion of the vehicle through such a corridor using a motion primitive. These primitives are selected heuristically and define the trajectory with a limited number of degrees of freedom, which are determined in an optimization problem. As a result, the method achieves significantly lower computation times compared to the state-of-the-art, most notably solving a full Optimal Control Problem (OCP), OMG-tools or VP-STO without significantly compromising optimality within a fixed corridor sequence. The approach is benchmarked extensively in simulation and is validated on a real-world Beckhoff XPlanar system</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02826v1</guid>
      <category>math.OC</category>
      <category>cs.RO</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louis Callens, Bastiaan Vandewal, Ibrahim Ibrahim, Jan Swevers, Wilm Decr\'e</dc:creator>
    </item>
    <item>
      <title>Fisher-Information-Based Sensor Placement for Structural Digital Twins: Analytic Results and Benchmarks</title>
      <link>https://arxiv.org/abs/2602.02981</link>
      <description>arXiv:2602.02981v1 Announce Type: new 
Abstract: High-fidelity digital twins rely on the accurate assimilation of sensor data into physics-based computational models. In structural applications, such twins aim to identify spatially distributed quantities--such as elementwise weakening fields, material parameters, or effective thermal loads--by minimizing discrepancies between measured and simulated responses subject to the governing equations of structural mechanics. While adjoint-based methods enable efficient gradient computation for these inverse problems, the quality and stability of the resulting estimates depend critically on the choice of sensor locations, measurement types, and directions.
  This paper develops a rigorous and implementation-ready framework for Fisher-information-based sensor placement in adjoint-based finite-element digital twins. Sensor configurations are evaluated using a D-optimal design criterion derived from a linearization of the measurement map, yielding a statistically meaningful measure of information content. We present matrix-free operator formulas for applying the Jacobian and its adjoint, and hence for computing Fisher-information products $Fv = J^\top R^{-1} Jv$ using only forward and adjoint solves. Building on these operator evaluations, we derive explicit sensitivity expressions for D-optimal sensor design with respect to measurement parameters and discuss practical strategies for evaluating the associated log-determinant objectives. To complement the general framework, we provide analytically tractable sensor placement results for a canonical one-dimensional structural model, clarifying the distinction between detectability and localizability and proving that D-optimal placement of multiple displacement sensors yields approximately uniform spacing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02981v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harbir Antil, Animesh Jain, Rainald L\"ohner</dc:creator>
    </item>
    <item>
      <title>Data-driven stabilization of continuous-time systems with noisy input-output data</title>
      <link>https://arxiv.org/abs/2602.02992</link>
      <description>arXiv:2602.02992v1 Announce Type: new 
Abstract: We study data-driven stabilization of continuous-time systems in autoregressive form when only noisy input-output data are available. First, we provide an operator-based characterization of the set of systems consistent with the data. Next, combining this characterization with behavioral theory, we derive a necessary and sufficient condition for the noisy data to be informative for quadratic stabilization. This condition is formulated as linear matrix inequalities, whose solution yields a stabilizing controller. Finally, we characterize data informativity for system identification in the noise-free setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02992v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masashi Wakaiki</dc:creator>
    </item>
    <item>
      <title>Cholesky factorisation, and intrinsically sparse linear quadratic regulation</title>
      <link>https://arxiv.org/abs/2602.03460</link>
      <description>arXiv:2602.03460v1 Announce Type: new 
Abstract: We classify a family of matrices of shift operators that can be factorised in a computationally tractable manner with the Cholesky algorithm. Such matrices arise in the linear quadratic regulator problem, and related areas. We use the factorisation to uncover intrinsic sparsity properties in the control laws for transportation problems with an underlying tree structure. This reveals that the optimal control can be applied in a distributed manner that is obscured by standard solution methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03460v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julia Adlercreutz, Richard Pates</dc:creator>
    </item>
    <item>
      <title>A necessary and sufficient condition for discrete-time consensus on star boundaries</title>
      <link>https://arxiv.org/abs/2602.03508</link>
      <description>arXiv:2602.03508v1 Announce Type: new 
Abstract: It is intuitive and well known, that if agents in a multi-agent system iteratively update their states in the Euclidean space as convex combinations of neighbors' states, all states eventually converge to the same value (consensus), provided the interaction graph is sufficiently connected. However, this seems to be also true in practice if the convex combinations of states are mapped or radially projected onto any unit $l_p$-sphere or even boundaries of star-convex sets, herein referred to as star boundaries. In this paper, we present insight into this matter by providing a necessary and sufficient condition for asymptotic consensus of the normalized states (directions) for strongly connected directed graphs, which is equivalent to asymptotic consensus of states when the star boundaries are the same for all agents. Furthermore, we show that when asymptotic consensus occurs, the states converge linearly and the point of convergence is continuous in the initial states. Assuming a directed strongly connected graph provides a more general setting than that considered, for example, in gradient-based consensus protocols, where symmetric graphs are assumed. Illustrative examples and a vast number of numerical simulations showcase the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03508v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Galina Sidorenko, Johan Thunberg</dc:creator>
    </item>
    <item>
      <title>Q-Learning for 3D Coverage in VCSEL-based Optical Wireless Systems</title>
      <link>https://arxiv.org/abs/2602.03526</link>
      <description>arXiv:2602.03526v1 Announce Type: new 
Abstract: Beam divergence control is a key factor in maintaining reliable coverage in indoor optical wireless communication (OWC) systems as receiver height varies.Conventional systems employ fixed divergence angles, which result in significant coverage degradation due to the non-convex tradeoff between optical power concentration and spatial spread. In this paper, we introduce a reinforcement learning (RL)-based framework for dynamic divergence adaptation in vertical-cavity surface-emitting laser (VCSEL)-based OWC networks. By continuously interacting with the environment, the RL agent autonomously learns a near-optimal mapping between receiver height and beam divergence, thereby eliminating the need for analytical modeling or computationally intensive exhaustive search. Simulation results demonstrate that the proposed approach achieves up to 92% coverage at low receiver heights and maintains robust performance under challenging conditions, enabling scalable, real-time, and energy-efficient beam control for dense VCSEL array deployments in next-generation OWC systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03526v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hossein Safi, Rizwana Ahmad, Iman Tavakkolnia, Harald Haas</dc:creator>
    </item>
    <item>
      <title>Optimizing Weighted Hodge Laplacian Flows on Simplicial Complexes</title>
      <link>https://arxiv.org/abs/2602.03763</link>
      <description>arXiv:2602.03763v1 Announce Type: new 
Abstract: Simplicial complexes are generalizations of graphs that describe higher-order network interactions among nodes in the graph. Network dynamics described by graph Laplacian flows have been widely studied in network science and control theory, and these can be generalized to simplicial complexes using Hodge Laplacians. We study weighted Hodge Laplacian flows on simplicial complexes. In particular, we develop a framework for weighted consensus dynamics based on weighted Hodge Laplacian flows and show some decomposition results for weighted Hodge Laplacians. We then show that two key spectral functions of the weighted Hodge Laplacians, the trace of the pseudoinverse and the smallest non-zero eigenvalue, are jointly convex in upper and lower simplex weights and can be formulated as semidefinite programs. Thus, globally optimal weights can be efficiently determined to optimize flows in terms of these functions. Numerical experiments demonstrate that optimal weights can substantially improve these metrics compared to uniform weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03763v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/CDC57313.2025.11312795</arxiv:DOI>
      <arxiv:journal_reference>2025 IEEE 64th Conference on Decision and Control (CDC), 5276-5281</arxiv:journal_reference>
      <dc:creator>Mathias Hudoba de Badyn, Tyler Summers</dc:creator>
    </item>
    <item>
      <title>Effective Frontiers: A Unification of Neural Scaling Laws</title>
      <link>https://arxiv.org/abs/2602.02593</link>
      <description>arXiv:2602.02593v1 Announce Type: cross 
Abstract: Neural scaling laws govern the prediction power-law improvement of test loss with respect to model capacity ($N$), datasize ($D$), and compute ($C$). However, existing theoretical explanations often rely on specific architectures or complex kernel methods, lacking intuitive universality. In this paper, we propose a unified framework that abstracts general learning tasks as the progressive coverage of patterns from a long-tail (Zipfian) distribution. We introduce the Effective Frontier ($k_\star$), a threshold in the pattern rank space that separates learned knowledge from the unlearned tail. We prove that reducible loss is asymptotically determined by the probability mass of the tail a resource-dependent frontier truncation. Based on our framework, we derive the precise scaling laws for $N$, $D$, and $C$, attributing them to capacity, coverage, and optimization bottlenecks, respectively. Furthermore, we unify these mechanisms via a Max-Bottleneck principle, demonstrating that the Kaplan and Chinchilla scaling laws are not contradictory, but equilibrium solutions to the same constrained optimization problem under different active bottlenecks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02593v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaxuan Zou, Zixuan Gong, Ye Su, Huayi Tang, Yong Liu</dc:creator>
    </item>
    <item>
      <title>Habit Formation, Labor Supply, and the Dynamics of Retirement and Annuitization</title>
      <link>https://arxiv.org/abs/2602.02816</link>
      <description>arXiv:2602.02816v2 Announce Type: cross 
Abstract: The decision to annuitize wealth in retirement planning has become increasingly complex due to rising longevity risk and changing retirement patterns, including increased labor force participation at older ages. While an extensive literature studies consumption, labor, and annuitization decisions, these elements are typically examined in isolation. This paper develops a unified stochastic control and optimal stopping framework in which habit formation and endogenous labor supply shape retirement and annuitization decisions under age-dependent mortality. We derive optimal consumption, labor, portfolio, and annuitization policies in a continuous-time lifecycle model. The solution is characterized via dynamic programming and a Hamilton-Jacobi-Bellman variational inequality. Our results reveal a rich sequence of retirement dynamics. When wealth is low relative to habit, labor is supplied defensively to protect consumption standards. As wealth increases, agents enter a work-to-retire phase in which labor is supplied at its maximum level to accelerate access to retirement. Human capital acts as a stabilizing asset, justifying a more aggressive pre-retirement investment portfolio, followed by abrupt de-risking upon annuitization. Subjective mortality beliefs are a key determinant in shaping retirement dynamics. Agents with pessimistic longevity beliefs rationally perceive annuities as unattractive, leading them to avoid or delay annuitization. This framework provides a behavior-based explanation for low annuity demand and offers guidance for retirement planning jointly linking labor supply, portfolio choice, and the timing of annuitization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02816v2</guid>
      <category>q-fin.MF</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Criscent Birungi, Cody Hyndman</dc:creator>
    </item>
    <item>
      <title>Large-Scale LLM Inference with Heterogeneous Workloads: Prefill-Decode Contention and Asymptotically Optimal Control</title>
      <link>https://arxiv.org/abs/2602.02987</link>
      <description>arXiv:2602.02987v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are rapidly becoming critical infrastructure for enterprise applications, driving unprecedented demand for GPU-based inference services. A key operational challenge arises from the two-phase nature of LLM inference: a compute-intensive \emph{prefill} phase that processes user input, followed by a memory-bound \emph{decode} phase that generates output tokens. When these phases share GPU resources, prefill tasks throttle the processing speed of concurrent decodes, creating state-dependent contention. This contention is further complicated by workload heterogeneity, as different applications exhibit vastly different input and output lengths. We develop a stochastic control framework for scheduling heterogeneous LLM workloads across large GPU clusters. We formulate LLM inference as a multiclass many-server queueing network with state-dependent service rates, grounded in empirical iteration-time measurements. We analyze the fluid approximation of this system and solve steady-state linear programs that characterize optimal resource allocation. We design gate-and-route policies that regulate prefill admission and decode routing, and prove that they are asymptotically optimal in the many-GPU limit under both bundled and separate token-pricing schemes. We further extend the framework to incorporate Service Level Indicators (SLIs) such as latency and fairness, providing a general approach to constrained scheduling. Numerical experiments calibrated to empirical iteration-time data demonstrate that our policies outperform standard serving heuristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02987v1</guid>
      <category>cs.DC</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruihan Lin, Zezhen Ding, Zean Han, Jiheng Zhang</dc:creator>
    </item>
    <item>
      <title>Dual Attainment in Multi-Period Multi-Asset Martingale Optimal Transport and Its Computation</title>
      <link>https://arxiv.org/abs/2602.02996</link>
      <description>arXiv:2602.02996v1 Announce Type: cross 
Abstract: We establish dual attainment for the multimarginal, multi-asset martingale optimal transport (MOT) problem, a fundamental question in the mathematical theory of model-independent pricing and hedging in quantitative finance. Our main result proves the existence of dual optimizers under mild regularity and irreducibility conditions, extending previous duality and attainment results from the classical and two-marginal settings to arbitrary numbers of assets and time periods. This theoretical advance provides a rigorous foundation for robust pricing and hedging of complex, path-dependent financial derivatives. To support our analysis, we present numerical experiments that demonstrate the practical solvability of large-scale discrete MOT problems using the state-of-the-art primal-dual linear programming (PDLP) algorithm. In particular, we solve multi-dimensional (or vectorial) MOT instances arising from the robust pricing of worst-of autocallable options, confirming the accuracy and feasibility of our theoretical results. Our work advances the mathematical understanding of MOT and highlights its relevance for robust financial engineering in high-dimensional and model-uncertain environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02996v1</guid>
      <category>q-fin.MF</category>
      <category>econ.TH</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>q-fin.CP</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charlie Che, Tongseok Lim, Yue Sun</dc:creator>
    </item>
    <item>
      <title>Geometry-Preserving Neural Architectures on Manifolds with Boundary</title>
      <link>https://arxiv.org/abs/2602.03082</link>
      <description>arXiv:2602.03082v1 Announce Type: cross 
Abstract: Preserving geometric structure is important in learning. We propose a unified class of geometry-aware architectures that interleave geometric updates between layers, where both projection layers and intrinsic exponential map updates arise as discretizations of projected dynamical systems on manifolds (with or without boundary). Within this framework, we establish universal approximation results for constrained neural ODEs. We also analyze architectures that enforce geometry only at the output, proving a separate universal approximation property that enables direct comparison to interleaved designs. When the constraint set is unknown, we learn projections via small-time heat-kernel limits, showing diffusion/flow-matching can be used as data-based projections. Experiments on dynamics over S^2 and SO(3), and diffusion on S^{d-1}-valued features demonstrate exact feasibility for analytic updates and strong performance for learned projections</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03082v1</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karthik Elamvazhuthi, Shiba Biswal, Kian Rosenblum, Arushi Katyal, Tianli Qu, Grady Ma, Rishi Sonthalia</dc:creator>
    </item>
    <item>
      <title>Joint Network-and-Server Congestion in Multi-Source Traffic Allocation: A Convex Formulation and Price-Based Decentralization</title>
      <link>https://arxiv.org/abs/2602.03246</link>
      <description>arXiv:2602.03246v1 Announce Type: cross 
Abstract: This paper studies an important rate allocation problem that arises in many networked and distributed systems: steady-state traffic rate allocation from multiple sources to multiple service nodes when both (i) the access-path delay on each source-node route is rate-dependent (capacity-constrained) and convex, and (ii) each service node (also capacity-constrained) experiences a load-dependent queueing delay driven by aggregate load from all sources. We show that the resulting flow-weighted end-to-end delay minimization is a convex program, yielding a global system-optimal solution characterized by KKT conditions that equalize total marginal costs (a path marginal access term plus a node congestion price) across all utilized routes. This condition admits a Wardrop-type interpretation: for each source, all utilized options equalize total marginal cost, while any option with strictly larger total marginal cost receives no flow. Building on this structure, we develop a lightweight distributed pricing-based algorithm in which each service node locally computes and broadcasts a scalar congestion price from its observed aggregate load, while each source updates its traffic split by solving a small separable convex allocation problem under the advertised prices. Numerical illustrations demonstrate convergence of the distributed iteration to the centralized optimum and highlight the trade-offs induced by jointly modeling access and service congestion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03246v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tamoghna Sarkar, Bhaskar Krishnamachari</dc:creator>
    </item>
    <item>
      <title>From Inexact Gradients to Byzantine Robustness: Acceleration and Optimization under Similarity</title>
      <link>https://arxiv.org/abs/2602.03329</link>
      <description>arXiv:2602.03329v1 Announce Type: cross 
Abstract: Standard federated learning algorithms are vulnerable to adversarial nodes, a.k.a. Byzantine failures. To solve this issue, robust distributed learning algorithms have been developed, which typically replace parameter averaging by robust aggregations. While generic conditions on these aggregations exist to guarantee the convergence of (Stochastic) Gradient Descent (SGD), the analyses remain rather ad-hoc. This hinders the development of more complex robust algorithms, such as accelerated ones. In this work, we show that Byzantine-robust distributed optimization can, under standard generic assumptions, be cast as a general optimization with inexact gradient oracles (with both additive and multiplicative error terms), an active field of research.
  This allows for instance to directly show that GD on top of standard robust aggregation procedures obtains optimal asymptotic error in the Byzantine setting. Going further, we propose two optimization schemes to speed up the convergence. The first one is a Nesterov-type accelerated scheme whose proof directly derives from accelerated inexact gradient results applied to our formulation. The second one hinges on Optimization under Similarity, in which the server leverages an auxiliary loss function that approximates the global loss. Both approaches allow to drastically reduce the communication complexity compared to previous methods, as we show theoretically and empirically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03329v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renaud Gaucher, Aymeric Dieuleveut, Hadrien Hendrikx</dc:creator>
    </item>
    <item>
      <title>Achieving Linear Speedup for Composite Federated Learning</title>
      <link>https://arxiv.org/abs/2602.03357</link>
      <description>arXiv:2602.03357v1 Announce Type: cross 
Abstract: This paper proposes FedNMap, a normal map-based method for composite federated learning, where the objective consists of a smooth loss and a possibly nonsmooth regularizer. FedNMap leverages a normal map-based update scheme to handle the nonsmooth term and incorporates a local correction strategy to mitigate the impact of data heterogeneity across clients. Under standard assumptions, including smooth local losses, weak convexity of the regularizer, and bounded stochastic gradient variance, FedNMap achieves linear speedup with respect to both the number of clients $n$ and the number of local updates $Q$ for nonconvex losses, both with and without the Polyak-{\L}ojasiewicz (PL) condition. To our knowledge, this is the first result establishing linear speedup for nonconvex composite federated learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03357v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Huang, Shi Pu</dc:creator>
    </item>
    <item>
      <title>An Approximate Ascent Approach To Prove Convergence of PPO</title>
      <link>https://arxiv.org/abs/2602.03386</link>
      <description>arXiv:2602.03386v1 Announce Type: cross 
Abstract: Proximal Policy Optimization (PPO) is among the most widely used deep reinforcement learning algorithms, yet its theoretical foundations remain incomplete. Most importantly, convergence and understanding of fundamental PPO advantages remain widely open. Under standard theory assumptions we show how PPO's policy update scheme (performing multiple epochs of minibatch updates on multi-use rollouts with a surrogate gradient) can be interpreted as approximated policy gradient ascent. We show how to control the bias accumulated by the surrogate gradients and use techniques from random reshuffling to prove a convergence theorem for PPO that sheds light on PPO's success. Additionally, we identify a previously overlooked issue in truncated Generalized Advantage Estimation commonly used in PPO. The geometric weighting scheme induces infinite mass collapse onto the longest $k$-step advantage estimator at episode boundaries. Empirical evaluations show that a simple weight correction can yield substantial improvements in environments with strong terminal signal, such as Lunar Lander.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03386v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leif Doering, Daniel Schmidt, Moritz Melcher, Sebastian Kassing, Benedikt Wille, Tilman Aach, Simon Weissmann</dc:creator>
    </item>
    <item>
      <title>Soft-Radial Projection for Constrained End-to-End Learning</title>
      <link>https://arxiv.org/abs/2602.03461</link>
      <description>arXiv:2602.03461v1 Announce Type: cross 
Abstract: Integrating hard constraints into deep learning is essential for safety-critical systems. Yet existing constructive layers that project predictions onto constraint boundaries face a fundamental bottleneck: gradient saturation. By collapsing exterior points onto lower-dimensional surfaces, standard orthogonal projections induce rank-deficient Jacobians, which nullify gradients orthogonal to active constraints and hinder optimization. We introduce Soft-Radial Projection, a differentiable reparameterization layer that circumvents this issue through a radial mapping from Euclidean space into the interior of the feasible set. This construction guarantees strict feasibility while preserving a full-rank Jacobian almost everywhere, thereby preventing the optimization stalls typical of boundary-based methods. We theoretically prove that the architecture retains the universal approximation property and empirically show improved convergence behavior and solution quality over state-of-the-art optimization- and projection-based baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03461v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>q-fin.CP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp J. Schneider, Daniel Kuhn</dc:creator>
    </item>
    <item>
      <title>A Function-Space Stability Boundary for Generalization in Interpolating Learning Systems</title>
      <link>https://arxiv.org/abs/2602.03514</link>
      <description>arXiv:2602.03514v1 Announce Type: cross 
Abstract: Modern learning systems often interpolate training data while still generalizing well, yet it remains unclear when algorithmic stability explains this behavior. We model training as a function-space trajectory and measure sensitivity to single-sample perturbations along this trajectory.
  We propose a contractive propagation condition and a stability certificate obtained by unrolling the resulting recursion. A small certificate implies stability-based generalization, while we also prove that there exist interpolating regimes with small risk where such contractive sensitivity cannot hold, showing that stability is not a universal explanation.
  Experiments confirm that certificate growth predicts generalization differences across optimizers, step sizes, and dataset perturbations. The framework therefore identifies regimes where stability explains generalization and where alternative mechanisms must account for success.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03514v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronald Katende</dc:creator>
    </item>
    <item>
      <title>Sparse Training of Neural Networks based on Multilevel Mirror Descent</title>
      <link>https://arxiv.org/abs/2602.03535</link>
      <description>arXiv:2602.03535v1 Announce Type: cross 
Abstract: We introduce a dynamic sparse training algorithm based on linearized Bregman iterations / mirror descent that exploits the naturally incurred sparsity by alternating between periods of static and dynamic sparsity pattern updates. The key idea is to combine sparsity-inducing Bregman iterations with adaptive freezing of the network structure to enable efficient exploration of the sparse parameter space while maintaining sparsity. We provide convergence guaranties by embedding our method in a multilevel optimization framework. Furthermore, we empirically show that our algorithm can produce highly sparse and accurate models on standard benchmarks. We also show that the theoretical number of FLOPs compared to SGD training can be reduced from 38% for standard Bregman iterations to 6% for our method while maintaining test accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03535v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yannick Lunk, Sebastian J. Scott, Leon Bungert</dc:creator>
    </item>
    <item>
      <title>Riemannian Neural Optimal Transport</title>
      <link>https://arxiv.org/abs/2602.03566</link>
      <description>arXiv:2602.03566v1 Announce Type: cross 
Abstract: Computational optimal transport (OT) offers a principled framework for generative modeling. Neural OT methods, which use neural networks to learn an OT map (or potential) from data in an amortized way, can be evaluated out of sample after training, but existing approaches are tailored to Euclidean geometry. Extending neural OT to high-dimensional Riemannian manifolds remains an open challenge. In this paper, we prove that any method for OT on manifolds that produces discrete approximations of transport maps necessarily suffers from the curse of dimensionality: achieving a fixed accuracy requires a number of parameters that grows exponentially with the manifold dimension. Motivated by this limitation, we introduce Riemannian Neural OT (RNOT) maps, which are continuous neural-network parameterizations of OT maps on manifolds that avoid discretization and incorporate geometric structure by construction. Under mild regularity assumptions, we prove that RNOT maps approximate Riemannian OT maps with sub-exponential complexity in the dimension. Experiments on synthetic and real datasets demonstrate improved scalability and competitive performance relative to discretization-based baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03566v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Micheli, Yueqi Cao, Anthea Monod, Samir Bhatt</dc:creator>
    </item>
    <item>
      <title>Sleep or Transmit: Dual-Mode Energy-Efficient Design for NOMA-Enabled Backscatter Networks</title>
      <link>https://arxiv.org/abs/2602.03607</link>
      <description>arXiv:2602.03607v1 Announce Type: cross 
Abstract: The rapid growth of Internet-of-Things (IoT) devices demands communication systems that are both spectrally efficient and energy frugal. Backscatter communication (BackCom) is an attractive low-power paradigm, but its spectral efficiency declines in dense deployments. This paper presents an uplink BackCom design that integrates non-orthogonal multiple access (NOMA) and maximizes system energy efficiency (EE). In a bistatic network where multiple backscatter nodes (BNs) harvest RF energy and alternate between sleep and active modes, we formulate a fractional program with coupled time, power, and reflection variables and develop a Dinkelbach-based alternating optimization (AO) algorithm with closed-form updates. Analysis reveals two operating modes depending on power availability, circuit demands and propagation conditions. Simulations show the proposed design adapts the time allocation, achieving up to 8% higher EE than fixed-power and 68% than no-sleep baselines, and delivering up to 127% EE gains over orthogonal multiple access (OMA). These results establish NOMA-enabled BackCom as a scalable, energy efficient solution for large-scale IoT deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03607v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hajar El Hassani, Mikael Gidlund</dc:creator>
    </item>
    <item>
      <title>When Should Agents Coordinate in Differentiable Sequential Decision Problems?</title>
      <link>https://arxiv.org/abs/2602.03674</link>
      <description>arXiv:2602.03674v1 Announce Type: cross 
Abstract: Multi-robot teams must coordinate to operate effectively. When a team operates in an uncoordinated manner, and agents choose actions that are only individually optimal, the team's outcome can suffer. However, in many domains, coordination requires costly communication. We explore the value of coordination in a broad class of differentiable motion-planning problems. In particular, we model coordinated behavior as a spectrum: at one extreme, agents jointly optimize a common team objective, and at the other, agents make unilaterally optimal decisions given their individual decision variables, i.e., they operate at Nash equilibria. We then demonstrate that reasoning about coordination in differentiable motion-planning problems reduces to reasoning about the second-order properties of agents' objectives, and we provide algorithms that use this second-order reasoning to determine at which times a team of agents should coordinate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03674v1</guid>
      <category>cs.MA</category>
      <category>cs.GT</category>
      <category>cs.RO</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caleb Probine, Su Ann Low, David Fridovich-Keil, Ufuk Topcu</dc:creator>
    </item>
    <item>
      <title>Input-to-State Safe Backstepping: Robust Safety-Critical Control with Unmatched Uncertainties</title>
      <link>https://arxiv.org/abs/2602.03691</link>
      <description>arXiv:2602.03691v1 Announce Type: cross 
Abstract: Guaranteeing safety in the presence of unmatched disturbances -- uncertainties that cannot be directly canceled by the control input -- remains a key challenge in nonlinear control. This paper presents a constructive approach to safety-critical control of nonlinear systems with unmatched disturbances. We first present a generalization of the input-to-state safety (ISSf) framework for systems with these uncertainties using the recently developed notion of an Optimal Decay CBF, which provides more flexibility for satisfying the associated Lyapunov-like conditions for safety. From there, we outline a procedure for constructing ISSf-CBFs for two relevant classes of systems with unmatched uncertainties: i) strict-feedback systems; ii) dual-relative-degree systems, which are similar to differentially flat systems. Our theoretical results are illustrated via numerical simulations of an inverted pendulum and planar quadrotor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03691v1</guid>
      <category>eess.SY</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max H. Cohen, Pio Ong, Aaron D. Ames</dc:creator>
    </item>
    <item>
      <title>Anytime Pretraining: Horizon-Free Learning-Rate Schedules with Weight Averaging</title>
      <link>https://arxiv.org/abs/2602.03702</link>
      <description>arXiv:2602.03702v1 Announce Type: cross 
Abstract: Large language models are increasingly trained in continual or open-ended settings, where the total training horizon is not known in advance. Despite this, most existing pretraining recipes are not anytime: they rely on horizon-dependent learning rate schedules and extensive tuning under a fixed compute budget. In this work, we provide a theoretical analysis demonstrating the existence of anytime learning schedules for overparameterized linear regression, and we highlight the central role of weight averaging - also known as model merging - in achieving the minimax convergence rates of stochastic gradient descent. We show that these anytime schedules polynomially decay with time, with the decay rate determined by the source and capacity conditions of the problem. Empirically, we evaluate 150M and 300M parameter language models trained at 1-32x Chinchilla scale, comparing constant learning rates with weight averaging and $1/\sqrt{t}$ schedules with weight averaging against a well-tuned cosine schedule. Across the full training range, the anytime schedules achieve comparable final loss to cosine decay. Taken together, our results suggest that weight averaging combined with simple, horizon-free step sizes offers a practical and effective anytime alternative to cosine learning rate schedules for large language model pretraining.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03702v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandru Meterez, Pranav Ajit Nair, Depen Morwani, Cengiz Pehlevan, Sham Kakade</dc:creator>
    </item>
    <item>
      <title>Do We Need Asynchronous SGD? On the Near-Optimality of Synchronous Methods</title>
      <link>https://arxiv.org/abs/2602.03802</link>
      <description>arXiv:2602.03802v1 Announce Type: cross 
Abstract: Modern distributed optimization methods mostly rely on traditional synchronous approaches, despite substantial recent progress in asynchronous optimization. We revisit Synchronous SGD and its robust variant, called $m$-Synchronous SGD, and theoretically show that they are nearly optimal in many heterogeneous computation scenarios, which is somewhat unexpected. We analyze the synchronous methods under random computation times and adversarial partial participation of workers, and prove that their time complexities are optimal in many practical regimes, up to logarithmic factors. While synchronous methods are not universal solutions and there exist tasks where asynchronous methods may be necessary, we show that they are sufficient for many modern heterogeneous computation scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03802v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Grigory Begunov, Alexander Tyurin</dc:creator>
    </item>
    <item>
      <title>Large Deviations in Safety-Critical Systems with Probabilistic Initial Conditions</title>
      <link>https://arxiv.org/abs/2405.13506</link>
      <description>arXiv:2405.13506v3 Announce Type: replace 
Abstract: We often rely on probabilistic measures -- e.g. event probability or expected time -- to characterize systems' safety. However, determining these quantities for extremely low-probability events is generally challenging, as standard safety methods usually struggle due to conservativeness, high-dimension scalability, tractability or numerical limitations. We address these issues by leveraging rigorous approximations grounded in the principles of Large Deviations theory. By assuming deterministic initial conditions, Large Deviations identifies a single dominant path in the low-noise limit as the most significant contributor to the rare-event probability: the instanton. We extend this result to incorporate stochastic uncertainty in the initial states, which is a common assumption in many applications. To that end, we determine an expression for the probability density of the initial states, conditioned on the unsafe rare event being observed. This expression gives access to the most probable initial conditions, as well as the most probable hitting time and path deviations, leading to the realization of the unsafe event. We demonstrate it's effectiveness by solving a high-dimensional and non-linear problem: a space collision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13506v3</guid>
      <category>math.OC</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aitor R. Gomez, Manuela L. Bujorianu, Rafal Wisniewski</dc:creator>
    </item>
    <item>
      <title>Convergence rates for ensemble-based solutions to optimal control of uncertain dynamical systems</title>
      <link>https://arxiv.org/abs/2407.18182</link>
      <description>arXiv:2407.18182v2 Announce Type: replace 
Abstract: We consider optimal control problems involving nonlinear ordinary differential equations with uncertain inputs. Using the sample average approximation, we obtain optimal control problems with ensembles of deterministic dynamical systems. Leveraging techniques for metric entropy bounds, we derive non-asymptotic Monte Carlo-type convergence rates for the ensemble-based solutions. Our theoretical framework is validated through numerical simulations on a harmonic oscillator problem and a vaccination scheduling problem for epidemic control under model parameter uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18182v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olena Melnikov, Johannes Milz</dc:creator>
    </item>
    <item>
      <title>Certainty-Equivalence Model Predictive Control: Stability, Performance, and Beyond</title>
      <link>https://arxiv.org/abs/2412.10625</link>
      <description>arXiv:2412.10625v3 Announce Type: replace 
Abstract: Handling model mismatch is a common challenge in model predictive control (MPC). While robust MPC is effective, its conservatism often makes it less desirable. Certainty-equivalence MPC (CE-MPC), which uses a nominal model, offers an appealing alternative due to its design simplicity and low computational costs. This paper investigates CE-MPC for uncertain nonlinear systems with multiplicative parametric uncertainty and input constraints that are inactive at the steady state. The primary contributions are two-fold. First, a novel perturbation analysis of the MPC value function is provided, without assuming the Lipschitz continuity of the stage cost, better tailoring the widely used quadratic cost and having broader applicability in value function approximation, learning-based MPC, and performance-driven MPC design. Second, the stability and performance analysis of CE-MPC are provided, quantifying the suboptimality of CE-MPC compared to the infinite-horizon optimal controller with perfect model knowledge. The results provide insights in how the prediction horizon and model mismatch jointly affect stability and the worst-case performance. Furthermore, the general results are specialized to linear quadratic control, and a competitive ratio bound is derived, serving as the first competitive-ratio bound for MPC of uncertain linear systems with input constraints and multiplicative uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10625v3</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Changrui Liu, Shengling Shi, Bart De Schutter</dc:creator>
    </item>
    <item>
      <title>SPLD polynomial optimization and bounded degree SOS hierarchies</title>
      <link>https://arxiv.org/abs/2502.11343</link>
      <description>arXiv:2502.11343v4 Announce Type: replace 
Abstract: In this paper, we introduce a new class of structured polynomials, called separable plus lower degree (SPLD) polynomials. The formal definition of an SPLD polynomial, which extends the concept of SPQ polynomials (Ahmadi et al. in Math Oper Res 48:1316--1343, 2023), is provided. A type of bounded degree SOS hierarchy, referred to as BSOS-SPLD, is proposed to efficiently solve optimization problems involving SPLD polynomials. Numerical experiments on several benchmark problems indicate that the proposed method yields better performance than the standard bounded degree SOS hierarchy (Lasserre et al. in EURO J Comput Optim 5:87--117, 2017). An exact SOS relaxation for a class of convex SPLD polynomial optimization problems is proposed. Finally, we present an application of SPLD polynomials to convex polynomial regression problems arising in statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11343v4</guid>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liguo Jiao, Jae Hyoung Lee, Nguyen Bui Nguyen Thao</dc:creator>
    </item>
    <item>
      <title>A Two-Timescale Primal-Dual Framework for Reinforcement Learning via Online Dual Variable Guidance</title>
      <link>https://arxiv.org/abs/2505.04494</link>
      <description>arXiv:2505.04494v2 Announce Type: replace 
Abstract: We study reinforcement learning by combining recent advances in regularized linear programming formulations with the classical theory of stochastic approximation. Motivated by the challenge of designing algorithms that leverage off-policy data while maintaining on-policy exploration, we propose PGDA-RL, a novel primal-dual Projected Gradient Descent-Ascent algorithm for solving regularized Markov Decision Processes (MDPs). PGDA-RL integrates experience replay-based gradient estimation with a two-timescale decomposition of the underlying nested optimization problem. The algorithm operates asynchronously, interacts with the environment through a single trajectory of correlated data, and updates its policy online in response to the dual variable associated with the occupancy measure of the underlying MDP. We prove that PGDA-RL converges almost surely to the optimal value function and policy of the regularized MDP. Our convergence analysis relies on tools from stochastic approximation theory and holds under weaker assumptions than those required by existing primal-dual RL approaches, notably removing the need for a simulator or a fixed behavioral policy. Under a strengthened ergodicity assumption on the underlying Markov chain, we establish a last-iterate finite-time guarantee with $\tilde{O} (k^{-2/3})$ mean-square convergence, aligning with the best-known rates for two-timescale stochastic approximation methods under Markovian sampling and biased gradient estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04494v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Axel Friedrich Wolter, Tobias Sutter</dc:creator>
    </item>
    <item>
      <title>Stability Regularized Cross-Validation</title>
      <link>https://arxiv.org/abs/2505.06927</link>
      <description>arXiv:2505.06927v2 Announce Type: replace 
Abstract: We revisit the problem of ensuring strong test set performance via cross-validation, and propose a nested k-fold cross-validation scheme that selects hyperparameters by minimizing a weighted sum of the usual cross-validation metric and an empirical model-stability measure. The weight on the stability term is itself chosen via a nested cross-validation procedure. This reduces the risk of strong validation set performance and poor test set performance due to instability. We benchmark our procedure on a suite of $13$ real-world datasets, and find that, compared to $k$-fold cross-validation over the same hyperparameters, it improves the out-of-sample MSE for sparse ridge regression and CART by $4\%$ and $2\%$ respectively on average, but has no impact on XGBoost. It also reduces the user's out-of-sample disappointment, sometimes significantly. For instance, for sparse ridge regression, the nested k-fold cross-validation error is on average $0.9\%$ lower than the test set error, while the $k$-fold cross-validation error is $21.8\%$ lower than the test error. Thus, for unstable models such as sparse regression and CART, our approach improves test set performance and reduces out-of-sample disappointment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06927v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Cory-Wright, Andr\'es G\'omez</dc:creator>
    </item>
    <item>
      <title>Long-Term Average Impulse Control with Mean Field Interactions</title>
      <link>https://arxiv.org/abs/2505.11345</link>
      <description>arXiv:2505.11345v3 Announce Type: replace 
Abstract: This paper analyzes and explicitly solves a class of long-term average impulse control problems with a specific mean-field interaction. The underlying process is a general one-dimensional diffusion with appropriate boundary behavior. The model is motivated by applications such as the optimal long-term management of renewable resources and financial portfolio management. Each individual agent seeks to maximize her long-term average reward, which consists of a running reward and income from discrete impulses, where the unit intervention price depends on the market through a stationary supply rate, the specific mean field variable to be considered. In a competitive market setting, we establish the existence of and explicitly characterize an equilibrium strategy within a large class of policies under mild conditions. Additionally, we formulate and solve the mean field control problem, in which agents cooperate with each other, aiming to realize a common maximal long-term average profit. To illustrate the theoretical results, we examine a stochastic logistic growth model and a population growth model in a stochastic environment with impulse control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11345v3</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>K. L. Helmes, R. H. Stockbridge, C. Zhu</dc:creator>
    </item>
    <item>
      <title>Queue Replacement Approach to Dynamic User Equilibrium Assignment with Route and Departure Time Choice</title>
      <link>https://arxiv.org/abs/2508.07159</link>
      <description>arXiv:2508.07159v2 Announce Type: replace 
Abstract: This study develops a hybrid analytical and numerical approach for dynamic user equilibrium (DUE) assignment with simultaneous route and departure time choice (RDTC) for homogeneous users. The core concept of the proposed approach is the generalized queue replacement principle (GQRP), which establishes an equivalence between the equilibrium queueing-delay pattern and the solution to a linear programming (LP) problem obtained by relaxing some conditions in the original DUE-RDTC problem. We first present a method for determining whether the GQRP holds. Based on the GQRP, we then develop a systematic procedure to obtain an exact DUE solution by sequentially solving two LPs: one for the equilibrium cost pattern, including queueing delays, and the other for the corresponding equilibrium flow pattern. Computational results on networks of varying scales confirm the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07159v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takara Sakai, Takashi Akamatsu, Koki Satsukawa</dc:creator>
    </item>
    <item>
      <title>Improved Stochastic Optimization of LogSumExp</title>
      <link>https://arxiv.org/abs/2509.24894</link>
      <description>arXiv:2509.24894v3 Announce Type: replace 
Abstract: The LogSumExp function, dual to the Kullback-Leibler (KL) divergence, plays a central role in many important optimization problems, including entropy-regularized optimal transport (OT) and distributionally robust optimization (DRO). In practice, when the number of exponential terms inside the logarithm is large or infinite, optimization becomes challenging since computing the gradient requires differentiating every term. We propose a novel convexity- and smoothness-preserving approximation to LogSumExp that can be efficiently optimized using stochastic gradient methods. This approximation is rooted in a sound modification of the KL divergence in the dual, resulting in a new $f$-divergence called the safe KL divergence. Our experiments and theoretical analysis of the LogSumExp-based stochastic optimization, arising in DRO and continuous OT, demonstrate the advantages of our approach over existing baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24894v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Egor Gladin, Alexey Kroshnin, Jia-Jie Zhu, Pavel Dvurechensky</dc:creator>
    </item>
    <item>
      <title>A Single-Loop Gradient Algorithm for Pessimistic Bilevel Optimization via Smooth Approximation</title>
      <link>https://arxiv.org/abs/2509.26240</link>
      <description>arXiv:2509.26240v4 Announce Type: replace 
Abstract: Bilevel optimization has garnered significant attention in the machine learning community recently, particularly regarding the development of efficient numerical methods. While substantial progress has been made in developing efficient algorithms for optimistic bilevel optimization, the study of methods for solving Pessimistic Bilevel Optimization (PBO) remains relatively less explored, especially the design of fully first-order, single-loop gradient-based algorithms. This paper aims to bridge this research gap. We first propose a novel smooth approximation to the PBO problem, using penalization and regularization techniques. Building upon this approximation, we then propose SiPBA (Single-loop Pessimistic Bilevel Algorithm), a new gradient-based method specifically designed for PBO which avoids second-order derivative information or inner-loop iterations for subproblem solving. We provide theoretical validation for the proposed smooth approximation scheme and establish theoretical convergence for the algorithm SiPBA. Numerical experiments on synthetic examples and practical applications demonstrate the effectiveness and efficiency of SiPBA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26240v4</guid>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qichao Cao, Shangzhi Zeng, Jin Zhang</dc:creator>
    </item>
    <item>
      <title>A Fast Relax-and-Round Approach to Unit Commitment for Data Center Own Generation</title>
      <link>https://arxiv.org/abs/2511.16420</link>
      <description>arXiv:2511.16420v3 Announce Type: replace 
Abstract: The rapid growth of data centers increasingly requires data center operators to "bring own generation" to complement the available utility power plants to supply all or part of data center load. This practice sharply increases the number of generators on the bulk power system and shifts operational focus toward fuel costs rather than traditional startup and runtime constraints. Conventional mixed-integer unit commitment formulations are not well suited for systems with thousands of flexible, fast-cycling units. We propose a unit commitment formulation that relaxes binary commitment decisions by allowing generators to be fractionally on, enabling the use of algorithms for continuous solvers. We then use a rounding approach to get a feasible unit commitment. For a 276-unit system, solution time decreases from 10 hours to less than a second, with no accuracy degradation. Our approach scales with no issues to tens of thousands of generators, which allows solving problems on the scale of the major North America interconnections. The bulk of computation is parallel and GPU compatible, enabling further acceleration in future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16420v3</guid>
      <category>math.OC</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaked Regev, Eve Tsybina, Slaven Peles</dc:creator>
    </item>
    <item>
      <title>Operator Splitting with Hamilton-Jacobi-based Proximals</title>
      <link>https://arxiv.org/abs/2601.22370</link>
      <description>arXiv:2601.22370v2 Announce Type: replace 
Abstract: Operator splitting algorithms are a cornerstone of modern first-order optimization, decomposing complex problems into simpler subproblems solved via proximal operators. However, most functions lack closed-form proximal operators, which has long restricted these methods to a narrow set of problems. Hamilton-Jacobi-based proximal operator (HJ-Prox) is a recent derivative-free Monte Carlo technique based on Hamilton-Jacobi PDE theory, that approximates proximal operators numerically. In this work, we introduce a unified framework for operator splitting via HJ-Prox, which allows for deployment of operator splitting even when functions are not proximable. We prove that replacing exact proximal steps with HJ-Prox in algorithms such as proximal point, proximal gradient descent, Douglas-Rachford splitting, Davis-Yin splitting, and primal-dual hybrid gradient preserves convergence guarantees under mild assumptions. Numerical experiments demonstrate HJ-Prox is competitive and effective on a wide variety of statistical learning tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22370v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas Di, Eric C. Chi, Samy Wu Fung</dc:creator>
    </item>
    <item>
      <title>Exact Solution to Data-Driven Inverse Optimization of MILPs in Finite Time via Gradient-Based Methods</title>
      <link>https://arxiv.org/abs/2405.14273</link>
      <description>arXiv:2405.14273v4 Announce Type: replace-cross 
Abstract: A data-driven inverse optimization problem (DDIOP) seeks to estimate an objective function (i.e., weights) that is consistent with observed optimal-solution data, and is important in many applications, including those involving mixed integer linear programs (MILPs). In the DDIOP for MILPs, the prediction loss on features (PLF), defined as the discrepancy between observed and predicted feature values, becomes discontinuous with respect to the weights, which makes it difficult to apply gradient-based optimization. To address this issue, we focus on a Lipschitz continuous and convex suboptimality loss. By exploiting its convex and piecewise-linear structure and the interiority of the minimum set, we show that a broad class of gradient-based optimization methods, including projected subgradient descent (PSGD), reaches the minimum suboptimality loss value in a finite number of iterations, thereby exactly solving the DDIOP for MILPs. Furthermore, as a corollary, we show that PSGD attains the minimum PLF in finitely many iterations. We also derive an upper bound on the number of iterations required for PSGD to reach finite convergence, and confirm the finite-step behavior through numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14273v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akira Kitaoka</dc:creator>
    </item>
    <item>
      <title>The Learning Approach to Games</title>
      <link>https://arxiv.org/abs/2503.00227</link>
      <description>arXiv:2503.00227v4 Announce Type: replace-cross 
Abstract: This work introduces a unified framework for analyzing games in greater depth. In the existing literature, players' strategies are typically assigned scalar values, and equilibrium concepts are used to identify compatible choices. However, this approach neglects the internal structure of players, thereby failing to accurately model observed behaviors.
  To address this limitation, we propose an abstract definition of a player, consistent with constructions in reinforcement learning. Instead of defining games as external settings, our framework defines them in terms of the players themselves. This offers a language that enables a deeper connection between games and learning. To illustrate the need for this generality, we study a simple two-player game and show that even in basic settings, a sophisticated player may adopt dynamic strategies that cannot be captured by simpler models or compatibility analysis.
  For a general definition of a player, we discuss natural conditions on its components and define competition through their behavior. In the discrete setting, we consider players whose estimates largely follow the standard framework from the literature. We explore connections to correlated equilibrium and highlight that dynamic programming naturally applies to all estimates. In the mean-field setting, we exploit symmetry to construct explicit examples of equilibria. Finally, we conclude by examining relations to reinforcement learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00227v4</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Melih \.I\c{s}eri, Erhan Bayraktar</dc:creator>
    </item>
    <item>
      <title>An Overview of Low-Rank Structures in the Training and Adaptation of Large Models</title>
      <link>https://arxiv.org/abs/2503.19859</link>
      <description>arXiv:2503.19859v3 Announce Type: replace-cross 
Abstract: The substantial computational demands of modern large-scale deep learning present significant challenges for efficient training and deployment. Recent research has revealed a widespread phenomenon wherein deep networks inherently learn low-rank structures in their weights and representations during training. This tutorial paper provides a comprehensive review of advances in identifying and exploiting these low-rank structures, bridging mathematical foundations with practical applications. We present two complementary theoretical perspectives on the emergence of low-rankness: viewing it through the optimization dynamics of gradient descent throughout training, and understanding it as a result of implicit regularization effects at convergence. Practically, these theoretical perspectives provide a foundation for understanding the success of techniques such as Low-Rank Adaptation (LoRA) in fine-tuning, inspire new parameter-efficient low-rank training strategies, and explain the effectiveness of masked training approaches like dropout and masked self-supervised learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19859v3</guid>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Balzano, Tianjiao Ding, Benjamin D. Haeffele, Soo Min Kwon, Qing Qu, Peng Wang, Zhangyang Wang, Can Yaras</dc:creator>
    </item>
    <item>
      <title>Learning non-equilibrium diffusions with Schr\"odinger bridges: from exactly solvable to simulation-free</title>
      <link>https://arxiv.org/abs/2505.16644</link>
      <description>arXiv:2505.16644v2 Announce Type: replace-cross 
Abstract: We consider the Schr\"odinger bridge problem which, given ensemble measurements of the initial and final configurations of a stochastic dynamical system and some prior knowledge on the dynamics, aims to reconstruct the "most likely" evolution of the system compatible with the data. Most existing literature assume Brownian reference dynamics, and are implicitly limited to modelling systems driven by the gradient of a potential energy. We depart from this regime and consider reference processes described by a multivariate Ornstein-Uhlenbeck process with generic drift matrix $\mathbf{A} \in \mathbb{R}^{d \times d}$. When $\mathbf{A}$ is asymmetric, this corresponds to a non-equilibrium system in which non-gradient forces are at play: this is important for applications to biological systems, which naturally exist out-of-equilibrium. In the case of Gaussian marginals, we derive explicit expressions that characterise exactly the solution of both the static and dynamic Schr\"odinger bridge. For general marginals, we propose mvOU-OTFM, a simulation-free algorithm based on flow and score matching for learning an approximation to the Schr\"odinger bridge. In application to a range of problems based on synthetic and real single cell data, we demonstrate that mvOU-OTFM achieves higher accuracy compared to competing methods, whilst being significantly faster to train.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16644v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen Y. Zhang, Michael P H Stumpf</dc:creator>
    </item>
    <item>
      <title>EVODiff: Entropy-aware Variance Optimized Diffusion Inference</title>
      <link>https://arxiv.org/abs/2509.26096</link>
      <description>arXiv:2509.26096v3 Announce Type: replace-cross 
Abstract: Diffusion models (DMs) excel in image generation but suffer from slow inference and training-inference discrepancies. Although gradient-based solvers for DMs accelerate denoising inference, they often lack theoretical foundations in information transmission efficiency. In this work, we introduce an information-theoretic perspective on the inference processes of DMs, revealing that successful denoising fundamentally reduces conditional entropy in reverse transitions. This principle leads to our key insights into the inference processes: (1) data prediction parameterization outperforms its noise counterpart, and (2) optimizing conditional variance offers a reference-free way to minimize both transition and reconstruction errors. Based on these insights, we propose an entropy-aware variance optimized method for the generative process of DMs, called EVODiff, which systematically reduces uncertainty by optimizing conditional entropy during denoising. Extensive experiments on DMs validate our insights and demonstrate that our method significantly and consistently outperforms state-of-the-art (SOTA) gradient-based solvers. For example, compared to the DPM-Solver++, EVODiff reduces the reconstruction error by up to 45.5\% (FID improves from 5.10 to 2.78) at 10 function evaluations (NFE) on CIFAR-10, cuts the NFE cost by 25\% (from 20 to 15 NFE) for high-quality samples on ImageNet-256, and improves text-to-image generation while reducing artifacts. Code is available at https://github.com/ShiguiLi/EVODiff.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26096v3</guid>
      <category>cs.CV</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shigui Li, Wei Chen, Delu Zeng</dc:creator>
    </item>
    <item>
      <title>Tight Robustness Certificates and Wasserstein Distributional Attacks for Deep Neural Networks</title>
      <link>https://arxiv.org/abs/2510.10000</link>
      <description>arXiv:2510.10000v2 Announce Type: replace-cross 
Abstract: Wasserstein distributionally robust optimization (WDRO) provides a framework for adversarial robustness, yet existing methods based on global Lipschitz continuity or strong duality often yield loose upper bounds or require prohibitive computation. We address these limitations with a primal approach and adopt a notion of exact Lipschitz certificates to tighten this upper bound of WDRO. For ReLU networks, we leverage the piecewise-affine structure on activation cells to obtain an exact tractable characterization of the corresponding WDRO problem. We further extend our analysis to modern architectures with smooth activations (e.g., GELU, SiLU), such as Transformers. Additionally, we propose novel Wasserstein Distributional Attacks (WDA, WDA++) that construct candidates for the worst-case distribution. Compared to existing attacks that are restricted to point-wise perturbations, our methods offer greater flexibility in the number and location of attack points. Extensive evaluations demonstrate that our proposed framework achieves competitive robust accuracy against state-of-the-art baselines while offering tighter certificates than existing methods. Our code is available at https://github.com/OLab-Repo/WDA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10000v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bach C. Le, Tung V. Dao, Binh T. Nguyen, Hong T. M. Chu</dc:creator>
    </item>
  </channel>
</rss>
