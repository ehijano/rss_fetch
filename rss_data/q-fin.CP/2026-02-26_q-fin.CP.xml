<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.CP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.CP</link>
    <description>q-fin.CP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.CP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Feb 2026 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>When the Rules Change: Adaptive Signal Extraction via Kalman Filtering and Markov-Switching Regimes</title>
      <link>https://arxiv.org/abs/2601.05716</link>
      <description>arXiv:2601.05716v2 Announce Type: replace 
Abstract: Most empirical microstructure research assumes that order flow--return parameters are constant, yet these relationships shift substantially across market regimes. Combining adaptive Kalman filtering, Markov-switching regime identification, and asymmetric response estimation, we characterize regime-dependent investor behavior in the Korean stock market during 2020--2024 using daily transaction data disaggregated by investor type. Three principal findings emerge: foreign investor predictive power increases several-fold during crisis periods relative to bull markets; individual investors chase momentum asymmetrically, reacting far more strongly to positive than to negative shocks; and independent information-theoretic validation corroborates both patterns. Rigorous out-of-sample testing reveals that these in-sample regularities do not generalize reliably, underscoring the need for proper validation methodology in microstructure research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05716v2</guid>
      <category>q-fin.CP</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sungwoo Kang</dc:creator>
    </item>
    <item>
      <title>Convergence of the generalization error for deep gradient flow methods for PDEs</title>
      <link>https://arxiv.org/abs/2512.25017</link>
      <description>arXiv:2512.25017v2 Announce Type: replace-cross 
Abstract: The aim of this article is to provide a firm mathematical foundation for the application of deep gradient flow methods (DGFMs) for the solution of (high-dimensional) partial differential equations (PDEs). We decompose the generalization error of DGFMs into an approximation and a training error. We first show that the solution of PDEs that satisfy reasonable and verifiable assumptions can be approximated by neural networks, thus the approximation error tends to zero as the number of neurons tends to infinity. Then, we derive the gradient flow that the training process follows in the ``wide network limit'' and analyze the limit of this flow as the training time tends to infinity. These results combined show that the generalization error of DGFMs tends to zero as the number of neurons and the training time tend to infinity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.25017v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>q-fin.CP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 26 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenguang Liu, Antonis Papapantoleon, Jasper Rou</dc:creator>
    </item>
  </channel>
</rss>
