<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Aug 2025 04:00:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Bayesian Models for Joint Selection of Features and Auto-Regressive Lags: Theory and Applications in Environmental and Financial Forecasting</title>
      <link>https://arxiv.org/abs/2508.10055</link>
      <description>arXiv:2508.10055v1 Announce Type: new 
Abstract: We develop a Bayesian framework for variable selection in linear regression with autocorrelated errors, accommodating lagged covariates and autoregressive structures. This setting occurs in time series applications where responses depend on contemporaneous or past explanatory variables and persistent stochastic shocks, including financial modeling, hydrological forecasting, and meteorological applications requiring temporal dependency capture. Our methodology uses hierarchical Bayesian models with spike-and-slab priors to simultaneously select relevant covariates and lagged error terms. We propose an efficient two-stage MCMC algorithm separating sampling of variable inclusion indicators and model parameters to address high-dimensional computational challenges. Theoretical analysis establishes posterior selection consistency under mild conditions, even when candidate predictors grow exponentially with sample size, common in modern time series with many potential lagged variables. Through simulations and real applications (groundwater depth prediction, S&amp;P 500 log returns modeling), we demonstrate substantial gains in variable selection accuracy and predictive performance. Compared to existing methods, our framework achieves lower MSPE, improved true model component identification, and greater robustness with autocorrelated noise, underscoring practical utility for model interpretation and forecasting in autoregressive settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10055v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Alokesh Manna, Sujit K. Ghosh</dc:creator>
    </item>
    <item>
      <title>Regression adjustment in covariate-adaptive randomized experiments with missing covariates</title>
      <link>https://arxiv.org/abs/2508.10061</link>
      <description>arXiv:2508.10061v1 Announce Type: new 
Abstract: Covariate-adaptive randomization is widely used in clinical trials to balance prognostic factors, and regression adjustments are often adopted to further enhance the estimation and inference efficiency. In practice, the covariates may contain missing values. Various methods have been proposed to handle the covariate missing problem under simple randomization. However, the statistical properties of the resulting average treatment effect estimators under stratified randomization, or more generally, covariate-adaptive randomization, remain unclear. To address this issue, we investigate the asymptotic properties of several average treatment effect estimators obtained by combining commonly used missingness processing procedures and regression adjustment methods. Moreover, we derive consistent variance estimators to enable valid inferences. Finally, we conduct a numerical study to evaluate the finite-sample performance of the considered estimators under various sample sizes and numbers of covariates and provide recommendations accordingly. Our analysis is model-free, meaning that the conclusions remain asymptotically valid even in cases of misspecification of the regression model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10061v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wanjia Fu, Yingying Ma, Hanzhong Liu</dc:creator>
    </item>
    <item>
      <title>Statistical methods: Basic concepts, interpretations, and cautions</title>
      <link>https://arxiv.org/abs/2508.10168</link>
      <description>arXiv:2508.10168v1 Announce Type: new 
Abstract: The study of associations and their causal explanations is a central research activity whose methodology varies tremendously across fields. Even within specialized subfields, comparisons across textbooks and journals reveals that the basics are subject to considerable variation and controversy. This variation is often obscured by the singular viewpoints presented within textbooks and journal guidelines, which may be deceptively written as if the norms they adopt are unchallenged. Furthermore, human limitations and the vastness within fields imply that no one can have expertise across all subfields and that interpretations will be severely constrained by the limitations of studies of human populations.
  The present chapter outlines an approach to statistical methods that attempts to recognize these problems from the start, rather than assume they are absent as in the claims of 'statistical significance' and 'confidence' ordinarily attached to statistical tests and interval estimates. It does so by grounding models and statistics in data description, and treating inferences from them as speculations based on assumptions that cannot be fully validated or checked using the analysis data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10168v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sander Greenland</dc:creator>
    </item>
    <item>
      <title>Incorporating Taxonomies of Cyber Incidents Into Detection Networks for Improved Detection Performance</title>
      <link>https://arxiv.org/abs/2508.10187</link>
      <description>arXiv:2508.10187v1 Announce Type: new 
Abstract: Many taxonomies exist to organize cybercrime incidents into ontological categories. We examine some of the taxonomies introduced in the literature; providing a framework, and analysis, of how best to leverage different taxonomy structures to optimize performance of detections targeting various types of threat-actor behaviors under the umbrella of precision and recall. Networks of detections are studied, and results are outlined showing properties of networks of interconnected detections. Some illustrations are provided to show how the construction of sets of detections to prevent broader types of attacks is limited by trade-offs in precision and recall under constraints. An equilibrium result is proven and validated on simulations, illustrating the existence of an optimal detection design strategy in this framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10187v1</guid>
      <category>stat.ME</category>
      <category>cs.CR</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ryan Warnick</dc:creator>
    </item>
    <item>
      <title>Examining the Association between Estimated Prevalence and Diagnostic Test Accuracy using Directed Acyclic Graphs</title>
      <link>https://arxiv.org/abs/2508.10207</link>
      <description>arXiv:2508.10207v1 Announce Type: new 
Abstract: There have been reports of correlation between estimates of prevalence and test accuracy across studies included in diagnostic meta-analyses. It has been hypothesized that this unexpected association arises because of certain biases commonly found in diagnostic accuracy studies. A theoretical explanation has not been studied systematically. In this work, we introduce directed acyclic graphs to illustrate common structures of bias in diagnostic test accuracy studies and to define the resulting data-generating mechanism behind a diagnostic meta-analysis. Using simulation studies, we examine how these common biases can produce a correlation between estimates of prevalence and index test accuracy and what factors influence its magnitude and direction. We found that an association arises either in the absence of a perfect reference test or in the presence of a covariate that simultaneously causes spectrum effect and is associated with the prevalence (confounding). We also show that the association between prevalence and accuracy can be removed by appropriate statistical methods. In the risk of bias evaluation in diagnostic meta-analyses, an observed association between estimates of prevalence and accuracy should be explored to understand its source and to adjust for latent or observed variables if possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10207v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yang Lu, Robert Platt, Nandini Dendukuri</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Comparison of the Wald, Wilson, and adjusted Wilson Confidence Intervals for Proportions</title>
      <link>https://arxiv.org/abs/2508.10223</link>
      <description>arXiv:2508.10223v1 Announce Type: new 
Abstract: The standard confidence interval for a population proportion covered in the overwhelming majority of introductory and intermediate statistics textbooks surprisingly remains the Wald confidence interval despite having a poor coverage probability, especially for small sample sizes or when the unknown population proportion is close to either 0 or 1. Using the mean coverage probability, and for some sample sizes, Agresti and Coull showed not only that the 95\% Wilson confidence interval performs better, but also showed that 95\% adjusted Wilson of type 4 confidence interval, obtained by simply adding four pseudo-observations, outperforms both the Wald and the Wilson confidence intervals. In this paper, we introduce a rainbow color code and pixel-color plots as ways to comprehensively compare the Wald, Wilson, and adjusted-Wilson of type $\epsilon$ confidence intervals across all sample sizes $n=1, 2, \dots, 1000$, population proportion values $p=0.01, 0.02, \dots, 0.99$, and for the three typical confidence levels. We show not only that adding 3 (resp., 4 and 6) pseudo-observations is the best for the 90\% (resp., 95\% and 99\%) adjusted Wilson confidence interval, but it also performs better than both the 90\% (resp., 95\% and 99\%) Wald and Wilson confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10223v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nabil Kahouadji</dc:creator>
    </item>
    <item>
      <title>Spatio-Temporal Autoregressions for High Dimensional Matrix-Valued Time Series</title>
      <link>https://arxiv.org/abs/2508.10291</link>
      <description>arXiv:2508.10291v1 Announce Type: new 
Abstract: Motivated by predicting intraday trading volume curves, we consider two spatio-temporal autoregressive models for matrix time series, in which each column may represent daily trading volume curve of one asset, and each row captures synchronized 5-minute volume intervals across multiple assets. While traditional matrix time series focus mainly on temporal evolution, our approach incorporates both spatial and temporal dynamics, enabling simultaneous analysis of interactions across multiple dimensions. The inherent endogeneity in spatio-temporal autoregressive models renders ordinary least squares estimation inconsistent. To overcome this difficulty while simultaneously estimating two distinct weight matrices with banded structure, we develop an iterated generalized Yule-Walker estimator by adapting a generalized method of moments framework based on Yule-Walker equations. Moreover, unlike conventional models that employ a single bandwidth parameter, the dual-bandwidth specification in our framework requires a new two-step, ratio-based sequential estimation procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10291v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Baojun Dou, Jing He, Sudhir Tiwari, Qiwei Yao</dc:creator>
    </item>
    <item>
      <title>Synthesizing Evidence: Data-Pooling as a Tool for Treatment Selection in Online Experiments</title>
      <link>https://arxiv.org/abs/2508.10331</link>
      <description>arXiv:2508.10331v1 Announce Type: new 
Abstract: Randomized experiments are the gold standard for causal inference but face significant challenges in business applications, including limited traffic allocation, the need for heterogeneous treatment effect estimation, and the complexity of managing overlapping experiments. These factors lead to high variability in treatment effect estimates, making data-driven policy roll out difficult. To address these issues, we introduce the data pooling treatment roll-out (DTR) framework, which enhances policy roll-out by pooling data across experiments rather than focusing narrowly on individual ones. DTR can effectively accommodate both overlapping and non-overlapping traffic scenarios, regardless of linear or nonlinear model specifications. We demonstrate the framework's robustness through a three-pronged validation: (a) theoretical analysis shows that DTR surpasses the traditional difference-in-mean and ordinary least squares methods under non-overlapping experiments, particularly when the number of experiments is large; (b) synthetic simulations confirm its adaptability in complex scenarios with overlapping traffic, rich covariates and nonlinear specifications; and (c) empirical applications to two experimental datasets from real world platforms, demonstrating its effectiveness in guiding customized policy roll-outs for subgroups within a single experiment, as well as in coordinating policy deployments across multiple experiments with overlapping scenarios. By reducing estimation variability to improve decision-making effectiveness, DTR provides a scalable, practical solution for online platforms to better leverage their experimental data in today's increasingly complex business environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10331v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenkang Peng (Philip), Chengzhang Li (Philip), Ying Rong (Philip),  Renyu (Philip),  Zhang</dc:creator>
    </item>
    <item>
      <title>Identifying Unmeasured Confounders in Panel Causal Models: A Two-Stage LM-Wald Approach</title>
      <link>https://arxiv.org/abs/2508.10342</link>
      <description>arXiv:2508.10342v1 Announce Type: new 
Abstract: Panel data are widely used in political science to draw causal inferences. However, these models often rely on the strong and untested assumption of sequential ignorability--that no unmeasured variables influence both the independent and outcome variables across time. Grounded in psychometric literature on latent variable modeling, this paper introduces the Two-Stage LM-Wald (2SLW) approach, a diagnostic tool that extends the Lagrange Multiplier (LM) and Wald tests to detect violations of this assumption in panel causal models. Using Monte Carlo simulations within the Random Intercept Cross-Lagged Panel Model (RI-CLPM), which separates within and between person effects, I demonstrate the 2SLW's ability to detect unmeasured confounding across three key scenarios: biased corrections, distorted direct effects, and altered mediation pathways. I also illustrate the approach with an empirical application to real-world panel data. By providing a practical and theoretically grounded diagnostic, the 2SLW approach enhances the robustness of causal inferences in the presence of potential time-varying confounders. Moreover, it can be readily implemented using the R package lavaan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10342v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bang Quan Zheng</dc:creator>
    </item>
    <item>
      <title>Efficient Inference for Time-to-Event Outcomes by Integrating Right-Censored and Current Status Data</title>
      <link>https://arxiv.org/abs/2508.10357</link>
      <description>arXiv:2508.10357v1 Announce Type: new 
Abstract: We propose a semiparametric data fusion framework for efficient inference on survival probabilities by integrating right-censored and current status data. Existing data fusion methods focus largely on fusing right-censored data only, while standard meta-analysis approaches are inadequate for combining right-censored and current status data, as estimators based on current status data alone typically converge at slower rates and have non-normal limiting distributions. In this work, we consider a semiparametric model under exchangeable event time distribution across data sources. We derive the canonical gradient of the survival probability at a given time, and develop a one-step estimator along with the corresponding inference procedure. Our proposed estimator is doubly robust and attains the semiparametric efficiency bound under mild conditions. Importantly, we show that incorporating current status data can lead to meaningful efficiency gains despite the slower convergence rate of current status-only estimators. We demonstrate the performance of our proposed method in simulations and discuss extensions to settings with covariate shift. We believe that this work has the potential to open new directions in data fusion methodology, particularly for settings involving mixed censoring types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10357v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiudi Li, Sijia Li</dc:creator>
    </item>
    <item>
      <title>On the implications of proportional hazards assumptions for competing risks modelling</title>
      <link>https://arxiv.org/abs/2508.10577</link>
      <description>arXiv:2508.10577v1 Announce Type: new 
Abstract: The assumption of hazard rates being proportional in covariates is widely made in empirical research and extensive research has been done to develop tests of its validity. This paper does not contribute on this end. Instead, it gives new insights on the implications of proportional hazards (PH) modelling in competing risks models. It is shown that the use of a PH model for the cause-specific hazards or subdistribution hazards can strongly restrict the class of copulas and marginal hazards for being compatible with a competing risks model. The empirical researcher should be aware that working with these models can be so restrictive that only degenerate or independent risks models are compatible. Numerical results confirm that estimates of cause-specific hazards models are not informative about patterns in the data generating process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10577v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon M. S. Lo, Ralf A. Wilke, Takeshi Emura</dc:creator>
    </item>
    <item>
      <title>Bistochastically private release of longitudinal data</title>
      <link>https://arxiv.org/abs/2508.10606</link>
      <description>arXiv:2508.10606v1 Announce Type: new 
Abstract: Although the bulk of the research in privacy and statistical disclosure control is designed for cross-sectional data, i.e. data where individuals are observed at one single point in time, longitudinal data, i.e. individuals observed over multiple periods, are increasingly collected. Such data enhance undoubtedly the possibility of statistical analysis compared to cross-sectional data, but also come with one additional layer of information, individual trajectories, that must remain practically useful in a privacy-preserving way. Few extensions, essentially k-anonymity based, of popular privacy tools have been proposed to deal with the challenges posed by longitudinal data, and these proposals are often complex. By considering randomized response, and specifically its recent bistochastic extension, in the context of longitudinal data, this paper proposes a simple approach for their anonymization. After having characterized new results on bistochastic matrices, we show that a simple relationship exists between the protection of each data set released at each period, and the protection of individuals trajectories over time. In turn, this relationship can be tuned according to desired protection and information requirements. We illustrate the application of the proposed approach by an empirical example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10606v1</guid>
      <category>stat.ME</category>
      <category>cs.CR</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Ruiz</dc:creator>
    </item>
    <item>
      <title>Encoding and inference on separable effects for sustained treatments</title>
      <link>https://arxiv.org/abs/2508.10702</link>
      <description>arXiv:2508.10702v1 Announce Type: new 
Abstract: Sustained treatment strategies are common in many domains, particularly in medicine, where many treatment are delivered repeatedly over time. The effects of adherence to a treatment strategy throughout follow-up are often more relevant to decision-makers than effects of treatment assignment or initiation. Here we consider the separable effect of sustained use of a time-varying treatment. Despite the potential usefulness of this estimand, the theory of separable effects has yet to be extended to settings with sustained treatment strategies. To derive our results, we use an unconventional encoding of time-varying treatment strategies. This allows us to obtain concise formulations of identifying assumptions with better practical properties; for example, they admit frugal graphical representations and formulations of identifying functionals. These functionals are used to motivate doubly robust semiparametrically efficient estimators. The results are applied to the Systolic Blood Pressure Intervention Trial (SPRINT), where we estimate a separable effect of modified blood pressure treatments on the risk of acute kidney injury.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10702v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ignacio Gonzalez-Perez, Kerollos Nashat Wanis, Aaron Leor Sarvet, Mats Julius Stensrud</dc:creator>
    </item>
    <item>
      <title>A Two-Step Test to Identify Zero-Inflated Biomarkers in Early-Phase Clinical Trials</title>
      <link>https://arxiv.org/abs/2508.10764</link>
      <description>arXiv:2508.10764v1 Announce Type: new 
Abstract: In early-phase clinical trials, a predictive biomarker may identify subgroups that benefit from an experimental therapy, even when the overall average treatment effect is negligible. Recently proposed nonparametric interaction tests such as the Average Kolmogorov-Smirnov Approach (AKSA) avoid prespecified biomarker cutting points and model assumptions, but their power degrades when the biomarker distribution is zero-inflated. We propose a two-step test that partitions the analysis into a spike test for biomarker-negative patients and a tail test for biomarker-positive patients, then combines the resulting p-values using Fisher's or Brown's method. This design isolates distinct sources of predictive effects, mitigates dilution, and preserves exact type I error control through permutation calibration. We derive theoretical properties showing that the proposed test retains nominal size and improves power over AKSA when predictive effects are concentrated in either the spike or tail subpopulation. Extensive simulations confirm robust type I error control under various zero-inflation rates, sample sizes, and skewed biomarker distributions. We also demonstrate consistent power gains across spike-only, tail-only, and mixed-effect scenarios. Our method provides a practical and flexible tool for early-phase trials with sparse biomarker distributions, enabling more reliable identification of predictive biomarkers to guide later-phase development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10764v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nan Miles Xi, Lin Wang</dc:creator>
    </item>
    <item>
      <title>Does fertility affect woman's labor force participation in low- and middle-income settings? Findings from a Bayesian nonparametric analysis</title>
      <link>https://arxiv.org/abs/2508.10787</link>
      <description>arXiv:2508.10787v1 Announce Type: new 
Abstract: Estimating the causal effect of fertility on women's employment is challenging because fertility and labor decisions are jointly determined. The difficulty is amplified in low- and middle-income countries, where longitudinal data are scarce. In this study, we propose a novel approach to estimating the causal effect of fertility on employment using widely available Demographic and Health Survey (DHS) observational data. Using infecundity as an instrument for family size, our approach combines principal stratification with Bayesian Additive Regression Trees to flexibly account for covariate-dependent instrument validity, work with count-valued intermediate variables, and produce estimates of causal effects and effect heterogeneity, i.e., how effects vary with covariates in the survey population. We apply the approach to DHS data from Nigeria, Senegal, and Kenya. We find in the survey sample and general population that an additional child significantly reduces employment among women in Nigeria but has no clear average effect in Senegal or Kenya. Across all three countries, however, there is strong evidence of effect heterogeneity: younger, less-educated women experience large employment penalties, while older or more advantaged women are largely unaffected. Robustness checks confirm that these findings are not sensitive to key modeling assumptions. While limitations remain due to the cross-sectional nature of the DHS data, our results illustrate how flexible non-parametric models can uncover important effect variation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10787v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Godoy Garraza, Leontine Alkema</dc:creator>
    </item>
    <item>
      <title>On the Gaussian distribution of the Mann-Kendall tau in the case of autocorrelated data</title>
      <link>https://arxiv.org/abs/2508.10842</link>
      <description>arXiv:2508.10842v1 Announce Type: new 
Abstract: Non-parametric Mann-Kendall tests for autocorrelated data rely on the assumption that the distribution of the normalized Mann-Kendall tau is Gaussian. While this assumption holds asymptotically for stationary autoregressive processes of order 1 (AR(1)) and simple moving average (SMA) processes when sampling over an increasingly long period, it often fails for finite-length time series. In such cases, the empirical distribution of the Mann-Kendall tau deviates significantly from the Gaussian distribution. To assess the validity of this assumption, we explore an alternative asymptotic framework for AR(1) and SMA processes. We prove that, along upsampling sequences, the distribution of the normalized Mann-Kendall tau does not converge to a Gaussian but instead to a bounded distribution with strictly positive variance. This asymptotic behavior suggests scaling laws which determine the conditions under which the Gaussian approximation remains valid for finite-length time series generated by stationary AR(1) and SMA processes. Using Shapiro-Wilk tests, we numerically confirm the departure from normality and establish simple, practical criteria for assessing the validity of the Gaussian assumption, which depend on both the autocorrelation structure and the series length. Finally, we illustrate these findings with examples from existing studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10842v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tristan Gamot, Nils Thibeau--Sutre, Tom J. M. Van Dooren</dc:creator>
    </item>
    <item>
      <title>On the Practical Use of Blaschke Decomposition in Nonstationary Signal Analysis</title>
      <link>https://arxiv.org/abs/2508.10861</link>
      <description>arXiv:2508.10861v1 Announce Type: new 
Abstract: The Blaschke decomposition-based algorithm, {\em Phase Dynamics Unwinding} (PDU), possesses several attractive theoretical properties, including fast convergence, effective decomposition, and multiscale analysis. However, its application to real-world signal decomposition tasks encounters notable challenges. In this work, we propose two techniques, divide-and-conquer via tapering and cumulative summation (cumsum), to handle complex trends and amplitude modulations and the mode-mixing caused by winding. The resulting method, termed {\em windowed PDU}, enhances PDU's performance in practical decomposition tasks. We validate our approach through both simulated and real-world signals, demonstrating its effectiveness across diverse scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10861v1</guid>
      <category>stat.ME</category>
      <category>math.CV</category>
      <category>physics.data-an</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ronald R. Coifman, Hau-Tieng Wu</dc:creator>
    </item>
    <item>
      <title>Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models</title>
      <link>https://arxiv.org/abs/2508.10007</link>
      <description>arXiv:2508.10007v1 Announce Type: cross 
Abstract: Hostile attribution bias is the tendency to interpret social interactions as intentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ) is commonly used to measure hostile attribution bias, and includes open-ended questions where participants describe the perceived intentions behind a negative social situation and how they would respond. While these questions provide insights into the contents of hostile attributions, they require time-intensive scoring by human raters. In this study, we assessed whether large language models can automate the scoring of AIHQ open-ended responses. We used a previously collected dataset in which individuals with traumatic brain injury (TBI) and healthy controls (HC) completed the AIHQ and had their open-ended responses rated by trained human raters. We used half of these responses to fine-tune the two models on human-generated ratings, and tested the fine-tuned models on the remaining half of AIHQ responses. Results showed that model-generated ratings aligned with human ratings for both attributions of hostility and aggression responses, with fine-tuned models showing higher alignment. This alignment was consistent across ambiguous, intentional, and accidental scenario types, and replicated previous findings on group differences in attributions of hostility and aggression responses between TBI and HC groups. The fine-tuned models also generalized well to an independent nonclinical dataset. To support broader adoption, we provide an accessible scoring interface that includes both local and cloud-based options. Together, our findings suggest that large language models can streamline AIHQ scoring in both research and clinical contexts, revealing their potential to facilitate psychological assessments across different populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10007v1</guid>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Y. Lyu, D. Combs, D. Neumann, Y. C. Leong</dc:creator>
    </item>
    <item>
      <title>Uncertainty-Aware Prediction of Parkinson's Disease Medication Needs: A Two-Stage Conformal Prediction Approach</title>
      <link>https://arxiv.org/abs/2508.10284</link>
      <description>arXiv:2508.10284v1 Announce Type: cross 
Abstract: Parkinson's Disease (PD) medication management presents unique challenges due to heterogeneous disease progression and treatment response. Neurologists must balance symptom control with optimal dopaminergic dosing based on functional disability while minimizing side effects. This balance is crucial as inadequate or abrupt changes can cause levodopa-induced dyskinesia, wearing off, and neuropsychiatric effects, significantly reducing quality of life. Current approaches rely on trial-and-error decisions without systematic predictive methods. Despite machine learning advances, clinical adoption remains limited due to reliance on point predictions that do not account for prediction uncertainty, undermining clinical trust and utility. Clinicians require not only predictions of future medication needs but also reliable confidence measures. Without quantified uncertainty, adjustments risk premature escalation to maximum doses or prolonged inadequate symptom control. We developed a conformal prediction framework anticipating medication needs up to two years in advance with reliable prediction intervals and statistical guarantees. Our approach addresses zero-inflation in PD inpatient data, where patients maintain stable medication regimens between visits. Using electronic health records from 631 inpatient admissions at University of Florida Health (2011-2021), our two-stage approach identifies patients likely to need medication changes, then predicts required levodopa equivalent daily dose adjustments. Our framework achieved marginal coverage while reducing prediction interval lengths compared to traditional approaches, providing precise predictions for short-term planning and wider ranges for long-term forecasting. By quantifying uncertainty, our approach enables evidence-based decisions about levodopa dosing, optimizing symptom control while minimizing side effects and improving life quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10284v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ricardo Diaz-Rincon, Muxuan Liang, Adolfo Ramirez-Zamora, Benjamin Shickel</dc:creator>
    </item>
    <item>
      <title>Mental Effort Estimation in Motion Exploration and Concept Generation Design Tasks using Inter-Band Relative Power Difference of EEG</title>
      <link>https://arxiv.org/abs/2508.10353</link>
      <description>arXiv:2508.10353v1 Announce Type: cross 
Abstract: Conceptual design is a cognitively complex task, especially in the engineering design of products having relative motion between components. Designers prefer sketching as a medium for conceptual design and use gestures and annotations to represent such relative motion. Literature suggests that static representations of motion in sketches may not achieve the intended functionality when realised, because it primarily depends on the designers' mental capabilities for motion simulation. Thus, it is important to understand the cognitive phenomena when designers are exploring concepts of articulated products. The current work is an attempt to understand design neurocognition by categorising the tasks and measuring the mental effort involved in these tasks using EEG. The analysis is intended to validate design intervention tools to support the conceptual design involving motion exploration. A novel EEG-based metric, inter-Band Relative Power Difference (inter-BRPD), is introduced to quantify mental effort. A design experiment is conducted with 32 participants, where they have to perform one control task and 2 focus tasks corresponding to the motion exploration task (MET) and the concept generation task (CGT), respectively. EEG data is recorded during the 3 tasks, cleaned, processed and analysed using the MNE library in Python. It is observed from the results that inter-BRPD captures the essence of mental effort with half the number of conventionally used parameters. The reliability and efficacy of the inter-BRPD metric are also statistically validated against literature-based cognitive metrics. With these new insights, the study opens up possibilities for creating support for conceptual design and its evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10353v1</guid>
      <category>cs.HC</category>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>G. Kalyan Ramana, Sumit Yempalle, Prasad S. Onkar</dc:creator>
    </item>
    <item>
      <title>A General Design-Based Framework and Estimator for Randomized Experiments</title>
      <link>https://arxiv.org/abs/2210.08698</link>
      <description>arXiv:2210.08698v3 Announce Type: replace 
Abstract: We describe a design-based framework for drawing causal inference in general randomized experiments. Causal effects are defined as linear functionals evaluated at unit-level potential outcome functions. Assumptions about the potential outcome functions are encoded as function spaces. This makes the framework expressive, allowing experimenters to formulate and investigate a wide range of causal questions, including about interference, that previously could not be investigated with design-based methods. We describe a class of estimators for estimands defined using the framework and investigate their properties. We provide necessary and sufficient conditions for unbiasedness and consistency. We also describe a class of conservative variance estimators, which facilitate the construction of confidence intervals. Finally, we provide several examples of empirical settings that previously could not be examined with design-based methods to illustrate the use of our approach in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.08698v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher Harshaw, Fredrik S\"avje, Yitan Wang</dc:creator>
    </item>
    <item>
      <title>Post-clustering Inference under Dependence</title>
      <link>https://arxiv.org/abs/2310.11822</link>
      <description>arXiv:2310.11822v2 Announce Type: replace 
Abstract: Recent work by Gao et al. (JASA 2022) has laid the foundations for post-clustering inference, establishing a theoretical framework allowing to test for differences between means of estimated clusters. Additionally, they studied the estimation of unknown parameters while controlling the selective type I error. However, their theory was developed for independent observations identically distributed as $p$-dimensional Gaussian variables, where the parameter estimation could only be performed for spherical covariance matrices. Here, we aim at extending this framework to a more convenient scenario for practical applications, where arbitrary dependence structures between observations and features are allowed. We establish sufficient conditions for extending the setting presented by Gao et al. to the general dependence framework. Moreover, we assess theoretical conditions allowing the compatible estimation of a covariance matrix. The theory is developed for hierarchical agglomerative clustering algorithms with several types of linkages, and for the $k$-means algorithm. We illustrate our method with synthetic data and real data of protein structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.11822v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Javier Gonz\'alez-Delgado, Mathis Deronzier, Juan Cort\'es, Pierre Neuvial</dc:creator>
    </item>
    <item>
      <title>Enhancing Dose Selection in Phase I Cancer Trials: Extending the Bayesian Logistic Regression Model with Non-DLT Adverse Events Integration</title>
      <link>https://arxiv.org/abs/2405.13767</link>
      <description>arXiv:2405.13767v3 Announce Type: replace 
Abstract: This paper presents the Burdened Bayesian Logistic Regression Model (BBLRM), an enhancement to the Bayesian Logistic Regression Model (BLRM) for dose-finding in phase I oncology trials. Traditionally, the BLRM determines the maximum tolerated dose (MTD) based on dose-limiting toxicities (DLTs). However, clinicians often perceive model-based designs like BLRM as complex and less conservative than rule-based designs, such as the widely used 3+3 method. To address these concerns, the BBLRM incorporates non-DLT adverse events (nDLTAEs) into the model. These events, although not severe enough to qualify as DLTs, provide additional information suggesting that higher doses might result in DLTs. In the BBLRM, an additional parameter $\delta$ is introduced to account for nDLTAEs. This parameter adjusts the toxicity probability estimates, making the model more conservative in dose escalation without compromising the accuracy in allocating the true MTD. The $\delta$ parameter is derived from the proportion of patients experiencing nDLTAEs and is tuned based on the design characteristics to balance the conservatism of the model. This approach aims to reduce the likelihood of assigning toxic doses as MTD while involving clinicians more directly in the decision-making process identifying the nDLTAEs along the study conduction. The paper includes a simulation study comparing BBLRM with more traditional versions of BLRM and a two stage Continual Reassessment Method (CRM) that incorporates nDLTAEs across various scenarios. The simulations demonstrate that BBLRM significantly reduces the selection of toxic doses as MTD without compromising the accuracy of MTD identification. These results suggest that integrating nDLTAEs into the dose-finding process can enhance the safety and acceptance of model-based designs in phase I oncology trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13767v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Nizzardo, Luca Genetti, Marco Pergher</dc:creator>
    </item>
    <item>
      <title>Incremental Causal Effect for Time to Treatment Initialization</title>
      <link>https://arxiv.org/abs/2409.13097</link>
      <description>arXiv:2409.13097v2 Announce Type: replace 
Abstract: We consider time to treatment initialization. This can commonly occur in preventive medicine, such as disease screening and vaccination; it can also occur with non-fatal health conditions such as HIV infection without the onset of AIDS; or in tech industry where items wait to be reviewed manually as abusive or not, etc. While traditional causal inference focused on `when to treat' and its effects, including their possible dependence on subject characteristics, we consider the incremental causal effect when the intensity of time to treatment initialization is intervened upon. We provide identification of the incremental causal effect without the commonly required positivity assumption, as well as an estimation framework using inverse probability weighting. We illustrate our approach via simulation, and apply it to a rheumatoid arthritis study to evaluate the incremental effect of time to start methotrexate on joint pain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13097v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Ying, Zhichen Zhao, Ronghui Xu</dc:creator>
    </item>
    <item>
      <title>Temporal Wasserstein Imputation: A Versatile Method for Time Series Imputation</title>
      <link>https://arxiv.org/abs/2411.02811</link>
      <description>arXiv:2411.02811v3 Announce Type: replace 
Abstract: Missing data can significantly hamper standard time series analysis, yet they occur frequently in applications. In this paper, we introduce temporal Wasserstein imputation, a novel method for imputing missing data in time series. Unlike most existing techniques, our approach is fully nonparametric, circumventing the need for model specification prior to imputation, making it suitable for empirical applications even with nonlinear dynamics. Its principled algorithmic implementation can seamlessly handle univariate or multivariate time series with any non-systematic missing pattern. In addition, the plausible range and side information of the missing entries (such as box constraints) can easily be incorporated. Furthermore, our method mitigates the distributional bias common among many existing approaches, ensuring more reliable downstream statistical analysis using the imputed series. We establish the convergence of an alternating minimization algorithm to critical points. We also provide conditions under which the marginal distributions of the underlying time series can be identified. Numerical experiments, including extensive simulations covering both linear and nonlinear time series and an analysis on a real-world groundwater dataset, corroborate the practical usefulness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02811v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuo-Chieh Huang, Tengyuan Liang, Ruey S. Tsay</dc:creator>
    </item>
    <item>
      <title>Multimorbidity as a multistage disease process</title>
      <link>https://arxiv.org/abs/2501.18742</link>
      <description>arXiv:2501.18742v2 Announce Type: replace 
Abstract: There is a growing proportion of people with several disease conditions ("multimorbidity"), placing increasing demands on healthcare systems. One hypothesis is that clusters of diseases may arise from shared underlying disease processes (shared "pathogenesis"), whereby the presence of one disease indicates the state of disease progression to several related disease types. This article explains how this hypothesis can be tested using observational data for disease incidence. Specifically, a multistage model is used to test whether two diseases can have a "shared stage" or "step", before either disease can occur, and how the unobserved rate of this step can be determined. The approach offers a simple method for studying multiple diseases and identifying shared underlying causes of multiple conditions, and is illustrated with published data and numerical examples. The fundamental mathematical model is analysed to compare key statistical properties such as the expectation and variance with those of independent diseases. The main results do not need an understanding of the underlying mathematics and can be appreciated by a non-expert.
  Significance: It is widely believed that there are shared underlying pathways that can lead to several disease types (shared "pathogenesis"), and this may explain observed clusters of disease types. This article shows how this hypothesis can be tested for a pair or cluster of diseases, using observational data of disease incidence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18742v2</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anthony J. Webster</dc:creator>
    </item>
    <item>
      <title>Inequality Restricted Minimum Density Power Divergence Estimation in Panel Count Data</title>
      <link>https://arxiv.org/abs/2503.21534</link>
      <description>arXiv:2503.21534v2 Announce Type: replace 
Abstract: Analysis of panel count data has garnered a considerable amount of attention in the literature, leading to the development of multiple statistical techniques. In inferential analysis, most of the works focus on leveraging estimating equations-based techniques or conventional maximum likelihood estimation. However, the robustness of these methods is largely questionable. In this paper, we present the robust density power divergence estimation for panel count data arising from nonhomogeneous Poisson processes, correlated through a latent frailty variable. In order to cope with real-world incidents, it is often desired to impose certain inequality constraints on the parameter space, giving rise to the restricted minimum density power divergence estimator. The significant contribution of this study lies in deriving its asymptotic properties. The proposed method ensures high efficiency in the model estimation while providing reliable inference despite data contamination. Moreover, the density power divergence measure is governed by a tuning parameter $\gamma$, which controls the trade-off between robustness and efficiency. To effectively determine the optimal value of $\gamma$, this study employs a generalized score-matching technique, marking considerable progress in the data analysis. Simulation studies and real data examples are provided to illustrate the performance of the estimator and to substantiate the theory developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21534v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Udita Goswami, Shuvashree Mondal</dc:creator>
    </item>
    <item>
      <title>Asymptotic Standard Errors for Reliability Coefficients in Item Response Theory</title>
      <link>https://arxiv.org/abs/2503.22924</link>
      <description>arXiv:2503.22924v2 Announce Type: replace 
Abstract: Reliability is a crucial index of measurement precision and is commonly reported in substantive research using latent variable measurement models. However, reliability coefficients, often treated as fixed values, are estimated from sample data and thus inherently subject to sampling variability. There are two categories of item response theory (IRT) reliability coefficients according to the regression framework of measurement precision (Liu, Pek, &amp; Maydeu-Olivares, 2025b): classical test theory (CTT) reliability and proportional reduction in mean squared error (PRMSE). We focus on quantifying their sampling variability in this article. Unlike existing approaches that can only handle sampling variability due to item parameter estimation, we consider a scenario in which an additional source of variability arises from substituting population moments with sample moments. We propose a general strategy for computing SEs that account for both sources of sampling variability, enabling the estimation of model-based reliability coefficients and their SEs in long tests. We apply the proposed framework to two specific reliability coefficients: the PRMSE for the latent variable and the CTT reliability for the expected a posteriori score of the latent variable. Simulation results confirm that the derived SEs accurately capture the sampling variability across various test lengths in moderate to large samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22924v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youjin Sung, Yang Liu</dc:creator>
    </item>
    <item>
      <title>Partial identification via conditional linear programs: estimation and policy learning</title>
      <link>https://arxiv.org/abs/2506.12215</link>
      <description>arXiv:2506.12215v2 Announce Type: replace 
Abstract: Many important quantities of interest are only partially identified from observable data: the data can limit them to a set of plausible values, but not uniquely determine them. This paper develops a unified framework for covariate-assisted estimation, inference, and decision making in partial identification problems where the parameter of interest satisfies a series of linear constraints, conditional on covariates. In such settings, bounds on the parameter can be written as expectations of solutions to conditional linear programs that optimize a linear function subject to linear constraints, where both the objective function and the constraints may depend on covariates and need to be estimated from data. Examples include estimands involving the joint distributions of potential outcomes, policy learning with inequality-aware value functions, and instrumental variable settings. We propose two de-biased estimators for bounds defined by conditional linear programs. The first directly solves the conditional linear programs with plugin estimates and uses output from standard LP solvers to de-bias the plugin estimate, avoiding the need for computationally demanding vertex enumeration of all possible solutions for symbolic bounds. The second uses entropic regularization to create smooth approximations to the conditional linear programs, trading a small amount of approximation error for improved estimation and computational efficiency. We establish conditions for asymptotic normality of both estimators, show that both estimators are robust to first-order errors in estimating the conditional constraints and objectives, and construct Wald-type confidence intervals for the partially identified parameters. These results also extend to policy learning problems where the value of a decision policy is only partially identified. We apply our methods to a study on the effects of Medicaid enrollment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12215v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eli Ben-Michael</dc:creator>
    </item>
    <item>
      <title>Decision Theory For Large Scale Outlier Detection Using Aleatoric Uncertainty: With a Note on Bayesian FDR</title>
      <link>https://arxiv.org/abs/2508.01988</link>
      <description>arXiv:2508.01988v3 Announce Type: replace 
Abstract: Aleatoric and Epistemic uncertainty have achieved recent attention in the literature as different sources from which uncertainty can emerge in stochastic modeling. Epistemic being intrinsic or model based notions of uncertainty, and aleatoric being the uncertainty inherent in the data. We propose a novel decision theoretic framework for outlier detection in the context of aleatoric uncertainty; in the context of Bayesian modeling. The model incorporates bayesian false discovery rate control for multiplicty adjustment, and a new generalization of Bayesian FDR is introduced. The model is applied to simulations based on temporally fluctuating outlier detection where fixing thresholds often results in poor performance due to nonstationarity, and a case study is outlined on on a novel cybersecurity detection. Cyberthreat signals are highly nonstationary; giving a credible stress test of the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01988v3</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ryan Warnick</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bayesian Multi-Treatment Mixture Cure Survival Model with Application in Pediatric Oncology</title>
      <link>https://arxiv.org/abs/2508.08975</link>
      <description>arXiv:2508.08975v3 Announce Type: replace 
Abstract: Heterogeneous treatment effect estimation is critical in oncology, particularly in multi-arm trials with overlapping therapeutic components and long-term survivors. These shared mechanisms pose a central challenge to identifying causal effects in precision medicine. We propose a novel covariate-dependent nonparametric Bayesian multi-treatment cure survival model that jointly accounts for common structures among treatments and cure fractions. Through latent link functions, our model leverages sharing among treatments through a flexible modeling approach, enabling individualized survival inference. We adopt a Bayesian route for inference and implement an efficient MCMC algorithm for approximating the posterior. Simulation studies demonstrate the method's robustness and superiority in various specification scenarios. Finally, application to the AALL0434 trial reveals clinically meaningful differences in survival across methotrexate-based regimens and their associations with different covariates, underscoring its practical utility for learning treatment effects in real-world pediatric oncology data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08975v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Chang, John Kairalla, Arkaprava Roy</dc:creator>
    </item>
    <item>
      <title>Statistical Inference on Latent Space Models for Network Data</title>
      <link>https://arxiv.org/abs/2312.06605</link>
      <description>arXiv:2312.06605v3 Announce Type: replace-cross 
Abstract: Latent space models are powerful statistical tools for modeling and understanding network data. While the importance of accounting for uncertainty in network analysis has been well recognized, the current literature predominantly focuses on point estimation and prediction, leaving the statistical inference of latent space models an open question. This work aims to fill this gap by providing a general framework to analyze the theoretical properties of the maximum likelihood estimators. In particular, we establish the uniform consistency and asymptotic distribution results for the latent space models under different edge types and link functions. Furthermore, the proposed framework enables us to generalize our results to the dependent-edge and sparse scenarios. Our theories are supported by simulation studies and have the potential to be applied in downstream inferences, such as link prediction and network testing problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.06605v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinming Li, Shihao Wu, Chengyu Cui, Gongjun Xu, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>Continuous Parallel Relaxation for Finding Diverse Solutions in Combinatorial Optimization Problems</title>
      <link>https://arxiv.org/abs/2402.02190</link>
      <description>arXiv:2402.02190v3 Announce Type: replace-cross 
Abstract: Finding the optimal solution is often the primary goal in combinatorial optimization (CO). However, real-world applications frequently require diverse solutions rather than a single optimum, particularly in two key scenarios. The first scenario occurs in real-world applications where strictly enforcing every constraint is neither necessary nor desirable. Allowing minor constraint violations can often lead to more cost-effective solutions. This is typically achieved by incorporating the constraints as penalty terms in the objective function, which requires careful tuning of penalty parameters. The second scenario involves cases where CO formulations tend to oversimplify complex real-world factors, such as domain knowledge, implicit trade-offs, or ethical considerations. To address these challenges, generating (i) penalty-diversified solutions by varying penalty intensities and (ii) variation-diversified solutions with distinct structural characteristics provides valuable insights, enabling practitioners to post-select the most suitable solution for their specific needs. However, efficiently discovering these diverse solutions is more challenging than finding a single optimal one. This study introduces Continual Parallel Relaxation Annealing (CPRA), a computationally efficient framework for unsupervised-learning (UL)-based CO solvers that generates diverse solutions within a single training run. CPRA leverages representation learning and parallelization to automatically discover shared representations, substantially accelerating the search for these diverse solutions. Numerical experiments demonstrate that CPRA outperforms existing UL-based solvers in generating these diverse solutions while significantly reducing computational costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02190v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Transactions on Machine Learning Research, 2835-8856 (2025)</arxiv:journal_reference>
      <dc:creator>Yuma Ichikawa, Hiroaki Iwashita</dc:creator>
    </item>
    <item>
      <title>Online Distributional Regression</title>
      <link>https://arxiv.org/abs/2407.08750</link>
      <description>arXiv:2407.08750v3 Announce Type: replace-cross 
Abstract: Large-scale streaming data are common in modern machine learning applications and have led to the development of online learning algorithms. Many fields, such as supply chain management, weather and meteorology, energy markets, and finance, have pivoted towards using probabilistic forecasts. This results in the need not only for accurate learning of the expected value but also for learning the conditional heteroskedasticity and conditional moments. Against this backdrop, we present a methodology for online estimation of regularized, linear distributional models. The proposed algorithm is based on a combination of recent developments for the online estimation of LASSO models and the well-known GAMLSS framework. We provide a case study on day-ahead electricity price forecasting, in which we show the competitive performance of the incremental estimation combined with strongly reduced computational effort. Our algorithms are implemented in a computationally efficient Python package ondil.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08750v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Hirsch, Jonathan Berrisch, Florian Ziel</dc:creator>
    </item>
    <item>
      <title>Interpretable Neural ODEs for Gene Regulatory Network Discovery under Perturbations</title>
      <link>https://arxiv.org/abs/2501.02409</link>
      <description>arXiv:2501.02409v3 Announce Type: replace-cross 
Abstract: Modern high-throughput biological datasets with thousands of perturbations provide the opportunity for large-scale discovery of causal graphs that represent the regulatory interactions between genes. Differentiable causal graphical models have been proposed to infer a gene regulatory network (GRN) from large scale interventional datasets, capturing the causal gene regulatory relationships from genetic perturbations. However, existing models are limited in their expressivity and scalability while failing to address the dynamic nature of biological processes such as cellular differentiation. We propose PerturbODE, a novel framework that incorporates biologically informative neural ordinary differential equations (neural ODEs) to model cell state trajectories under perturbations and derive the causal GRN from the neural ODE's parameters. We demonstrate PerturbODE's efficacy in trajectory prediction and GRN inference across simulated and real over-expression datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02409v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>q-bio.MN</category>
      <category>stat.ME</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zaikang Lin, Sei Chang, Aaron Zweig, Minseo Kang, Elham Azizi, David A. Knowles</dc:creator>
    </item>
    <item>
      <title>Distribution-free data-driven smooth tests without $\chi^2$</title>
      <link>https://arxiv.org/abs/2508.01973</link>
      <description>arXiv:2508.01973v2 Announce Type: replace-cross 
Abstract: This article demonstrates how recent developments in the theory of empirical processes allow us to construct a new family of asymptotically distribution-free smooth test statistics. Their distribution-free property is preserved even when the parameters are estimated, model selection is performed, and the sample size is only moderately large. A computationally efficient alternative to the classical parametric bootstrap is also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01973v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 15 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiangyu Zhang, Sara Algeri</dc:creator>
    </item>
  </channel>
</rss>
