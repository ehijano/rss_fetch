<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 Oct 2025 04:01:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A L-infinity Norm Synthetic Control Approach</title>
      <link>https://arxiv.org/abs/2510.26053</link>
      <description>arXiv:2510.26053v1 Announce Type: new 
Abstract: This paper reinterprets the Synthetic Control (SC) framework through the lens of weighting philosophy, arguing that the contrast between traditional SC and Difference-in-Differences (DID) reflects two distinct modeling mindsets: sparse versus dense weighting schemes. Rather than viewing sparsity as inherently superior, we treat it as a modeling choice simple but potentially fragile. We propose an L-infinity-regularized SC method that combines the strengths of both approaches. Like DID, it employs a denser weighting scheme that distributes weights more evenly across control units, enhancing robustness and reducing overreliance on a few control units. Like traditional SC, it remains flexible and data-driven, increasing the likelihood of satisfying the parallel trends assumption while preserving interpretability. We develop an interior point algorithm for efficient computation, derive asymptotic theory under weak dependence, and demonstrate strong finite-sample performance through simulations and real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26053v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Le Wang, Xin Xing, Youhui Ye</dc:creator>
    </item>
    <item>
      <title>Poisson process factorization for mutational signature analysis with genomic covariates</title>
      <link>https://arxiv.org/abs/2510.26090</link>
      <description>arXiv:2510.26090v1 Announce Type: new 
Abstract: Mutational signatures are powerful summaries of the mutational processes altering the DNA of cancer cells and are increasingly relevant as biomarkers in personalized treatments. The widespread approach to mutational signature analysis consists of decomposing the matrix of mutation counts from a sample of patients via non-negative matrix factorization (NMF) algorithms. However, by working with aggregate counts, this procedure ignores the non-homogeneous patterns of occurrence of somatic mutations along the genome, as well as the tissue-specific characteristics that notoriously influence their rate of appearance. This gap is primarily due to a lack of adequate methodologies to leverage locus-specific covariates directly in the factorization. In this paper, we address these limitations by introducing a model based on Poisson point processes to infer mutational signatures and their activities as they vary across genomic regions. Using covariate-dependent factorized intensity functions, our Poisson process factorization (PPF) generalizes the baseline NMF model to include regression coefficients that capture the effect of commonly known genomic features on the mutation rates from each latent process. Furthermore, our method relies on sparsity-inducing hierarchical priors to automatically infer the number of active latent factors in the data, avoiding the need to fit multiple models for a range of plausible ranks. We present algorithms to obtain maximum a posteriori estimates and uncertainty quantification via Markov chain Monte Carlo. We test the method on simulated data and on real data from breast cancer, using covariates on alterations in chromosomal copies, histone modifications, cell replication timing, nucleosome positioning, and DNA methylation. Our results shed light on the joint effect that epigenetic marks have on the latent processes at high resolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26090v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Zito, Giovanni Parmigiani, Jeffrey W. Miller</dc:creator>
    </item>
    <item>
      <title>Variable selection in spatial lag models using the focussed information criterion</title>
      <link>https://arxiv.org/abs/2510.26177</link>
      <description>arXiv:2510.26177v1 Announce Type: new 
Abstract: Spatial regression models have a variety of applications in several fields ranging from economics to public health. Typically, it is of interest to select important exogenous predictors of the spatially autocorrelated response variable. In this paper, we propose variable selection in linear spatial lag models by means of the focussed information criterion (FIC). The FIC-based variable selection involves the minimization of the asymptotic risk in the estimation of a certain parametric focus function of interest under potential model misspecification. We systematically investigate the key asymptotics of the maximum likelihood estimators under the sequence of locally perturbed mutually contiguous probability models. Using these results, we obtain the expressions for the bias and the variance of the estimated focus leading to the desired FIC formula. We provide practically useful focus functions that account for various spatial characteristics such as mean response, variability in the estimation and spatial spillover effects. Furthermore, we develop an averaged version of the FIC that incorporates varying covariate levels while evaluating the models. The empirical performance of the proposed methodology is demonstrated through simulations and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26177v1</guid>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sagar Pandhare, Divya Kappara, Siuli Mukhopadhyay</dc:creator>
    </item>
    <item>
      <title>Estimating heritability of survival traits using censored multiple variance component model</title>
      <link>https://arxiv.org/abs/2510.26226</link>
      <description>arXiv:2510.26226v1 Announce Type: new 
Abstract: Characterizing the genetic basis of survival traits, such as age at disease onset, is critical for risk stratification, early intervention, and elucidating biological mechanisms that can inform therapeutic development. However, time-to-event outcomes in human cohorts are frequently right-censored, complicating both the estimation and partitioning of total heritability. Modern biobanks linked to electronic health records offer the unprecedented power to dissect the genetic basis of age-at-diagnosis traits at large scale. Yet, few methods exist for estimating and partitioning the total heritability of censored survival traits. Existing methods impose restrictive distributional assumptions on genetic and environmental effects and are not scalable to large biobanks with a million subjects. We introduce a censored multiple variance component model to robustly estimate the total heritability of survival traits under right-censoring. We demonstrate through extensive simulations that the method provides accurate total heritability estimates of right-censored traits at censoring rates up to 80% given sufficient sample size. The method is computationally efficient in estimating one hundred genetic variance components of a survival trait using large-scale biobank genotype data consisting of a million subjects and a million SNPs in under nine hours, including uncertainty quantification. We apply our method to estimate the total heritability of four age-at-diagnosis traits from the UK Biobank study. Our results establish a scalable and robust framework for heritability analysis of right-censored survival traits in large-scale genetic studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26226v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Do Hyun Kim, Hua Zhou, Brendon Chau, Aubrey Jensen, Judong Shen, Devan Mehrotra, Gang Li, Jin J. Zhou</dc:creator>
    </item>
    <item>
      <title>Smoothed Quantile Estimation via Interpolation to the Mean</title>
      <link>https://arxiv.org/abs/2510.26447</link>
      <description>arXiv:2510.26447v1 Announce Type: new 
Abstract: This paper introduces a unified family of smoothed quantile estimators that continuously interpolate between classical empirical quantiles and the sample mean. The estimators q(z, h) are defined as minimizers of a regularized objective function depending on two parameters: a smoothing parameter h $\ge$ 0 and a location parameter z $\in$ R.  When h = 0 and z $\in$ (-1, 1), the estimator reduces to the empirical quantile of order $\tau$ = (1z)/2; as h $\rightarrow$ $\infty$, it converges to the sample mean for any fixed z. We establish consistency, asymptotic normality, and an explicit variance expression characterizing the efficiency-robustness trade-off induced by h.  A key geometric insight shows that for each fixed quantile level $\tau$ , the admissible parameter pairs (z, h) lie on a straight line in the parameter space, along which the population quantile remains constant while asymptotic efficiency varies. The analysis reveals two regimes: under light-tailed distributions (e.g., Gaussian), smoothing yields a monotonic but asymptotic variance reduction with no finite optimum; under heavy-tailed distributions (e.g., Laplace), a finite smoothing level h * ($\tau$ ) &gt; 0 achieves strict efficiency improvement over the classical empirical quantile. Numerical illustrations confirm these theoretical predictions and highlight how smoothing balances robustness and efficiency across quantile levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26447v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sa\"id Maanan (LPP), Azzouz Dermoune (LPP), Ahmed El Ghini</dc:creator>
    </item>
    <item>
      <title>In Defense of the Pre-Test: Valid Inference when Testing Violations of Parallel Trends for Difference-in-Differences</title>
      <link>https://arxiv.org/abs/2510.26470</link>
      <description>arXiv:2510.26470v1 Announce Type: new 
Abstract: The difference-in-differences (DID) research design is a key identification strategy which allows researchers to estimate causal effects under the parallel trends assumption. While the parallel trends assumption is counterfactual and cannot be tested directly, researchers often examine pre-treatment periods to check whether the time trends are parallel before treatment is administered. Recently, researchers have been cautioned against using preliminary tests which aim to detect violations of parallel trends in the pre-treatment period. In this paper, we argue that preliminary testing can -- and should -- play an important role within the DID research design. We propose a new and more substantively appropriate conditional extrapolation assumption, which requires an analyst to conduct a preliminary test to determine whether the severity of pre-treatment parallel trend violations falls below an acceptable level before extrapolation to the post-treatment period is justified. This stands in contrast to prior work which can be interpreted as either setting the acceptable level to be exactly zero (in which case preliminary tests lack power) or assuming that extrapolation is always justified (in which case preliminary tests are not required). Under mild assumptions on how close the actual violation is to the acceptable level, we provide a consistent preliminary test as well confidence intervals which are valid when conditioned on the result of the test. The conditional coverage of these intervals overcomes a common critique made against the use of preliminary testing within the DID research design. We use real data as well as numerical simulations to illustrate the performance of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26470v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas M. Mikhaeil, Christopher Harshaw</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Matching Decisions via Matrix Completion under Dependent Missingness</title>
      <link>https://arxiv.org/abs/2510.26478</link>
      <description>arXiv:2510.26478v1 Announce Type: new 
Abstract: This paper studies decision-making and statistical inference for two-sided matching markets via matrix completion. In contrast to the independent sampling assumed in classical matrix completion literature, the observed entries, which arise from past matching data, are constrained by matching capacity. This matching-induced dependence poses new challenges for both estimation and inference in the matrix completion framework. We propose a non-convex algorithm based on Grassmannian gradient descent and establish near-optimal entrywise convergence rates for three canonical mechanisms, i.e., one-to-one matching, one-to-many matching with one-sided random arrival, and two-sided random arrival. To facilitate valid uncertainty quantification and hypothesis testing on matching decisions, we further develop a general debiasing and projection framework for arbitrary linear forms of the reward matrix, deriving asymptotic normality with finite-sample guarantees under matching-induced dependent sampling. Our empirical experiments demonstrate that the proposed approach provides accurate estimation, valid confidence intervals, and efficient evaluation of matching policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26478v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Congyuan Duan, Wanteng Ma, Dong Xia, Kan Xu</dc:creator>
    </item>
    <item>
      <title>Discovering Causal Relationships Between Time Series With Spatial Structure</title>
      <link>https://arxiv.org/abs/2510.26485</link>
      <description>arXiv:2510.26485v1 Announce Type: new 
Abstract: Causal discovery is the subfield of causal inference concerned with estimating the structure of cause-and-effect relationships in a system of interrelated variables, as opposed to quantifying the strength of causal effects. As interest in causal discovery builds in fields such as ecology, public health, and environmental sciences where data is regularly collected with spatial and temporal structures, approaches must evolve to manage autocorrelation and complex confounding. As it stands, the few proposed causal discovery algorithms for spatiotemporal data require summarizing across locations, ignore spatial autocorrelation, and/or scale poorly to high dimensions. Here, we introduce our developing framework that extends time-series causal discovery to systems with spatial structure, building upon work on causal discovery across contexts and methods for handling spatial confounding in causal effect estimation. We close by outlining remaining gaps in the literature and directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26485v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rebecca F. Supple (School of Mathematics and Statistics, University of St Andrews, Centre for Research into Ecological and Environmental Modelling, University of St Andrews), Hannah Worthington (School of Mathematics and Statistics, University of St Andrews, Centre for Research into Ecological and Environmental Modelling, University of St Andrews), Ben Swallow (School of Mathematics and Statistics, University of St Andrews, Centre for Research into Ecological and Environmental Modelling, University of St Andrews)</dc:creator>
    </item>
    <item>
      <title>A KL-divergence based test for elliptical distribution</title>
      <link>https://arxiv.org/abs/2510.26775</link>
      <description>arXiv:2510.26775v1 Announce Type: new 
Abstract: We conduct a KL-divergence based procedure for testing elliptical distributions. The procedure simultaneously takes into account the two defining properties of an elliptically distributed random vector: independence between length and direction, and uniform distribution of the direction. The test statistic is constructed based on the $k$ nearest neighbors ($k$NN) method, and two cases are considered where the mean vector and covariance matrix are known and unknown. First-order asymptotic properties of the test statistic are rigorously established by creatively utilizing sample splitting, truncation and transformation between Euclidean space and unit sphere, while avoiding assuming Fr\'echet differentiability of any functionals. Debiasing and variance inflation are further proposed to treat the degeneration of the influence function. Numerical implementations suggest better size and power performance than the state of the art procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26775v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yin Tang, Yanyuan Ma, Bing Li</dc:creator>
    </item>
    <item>
      <title>Transferring Causal Effects using Proxies</title>
      <link>https://arxiv.org/abs/2510.25924</link>
      <description>arXiv:2510.25924v1 Announce Type: cross 
Abstract: We consider the problem of estimating a causal effect in a multi-domain setting. The causal effect of interest is confounded by an unobserved confounder and can change between the different domains. We assume that we have access to a proxy of the hidden confounder and that all variables are discrete or categorical. We propose methodology to estimate the causal effect in the target domain, where we assume to observe only the proxy variable. Under these conditions, we prove identifiability (even when treatment and response variables are continuous). We introduce two estimation techniques, prove consistency, and derive confidence intervals. The theoretical results are supported by simulation studies and a real-world example studying the causal effect of website rankings on consumer choices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25924v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuel Iglesias-Alonso, Felix Schur, Julius von K\"ugelgen, Jonas Peters</dc:creator>
    </item>
    <item>
      <title>Bias-Corrected Data Synthesis for Imbalanced Learning</title>
      <link>https://arxiv.org/abs/2510.26046</link>
      <description>arXiv:2510.26046v1 Announce Type: cross 
Abstract: Imbalanced data, where the positive samples represent only a small proportion compared to the negative samples, makes it challenging for classification problems to balance the false positive and false negative rates. A common approach to addressing the challenge involves generating synthetic data for the minority group and then training classification models with both observed and synthetic data. However, since the synthetic data depends on the observed data and fails to replicate the original data distribution accurately, prediction accuracy is reduced when the synthetic data is naively treated as the true data. In this paper, we address the bias introduced by synthetic data and provide consistent estimators for this bias by borrowing information from the majority group. We propose a bias correction procedure to mitigate the adverse effects of synthetic data, enhancing prediction accuracy while avoiding overfitting. This procedure is extended to broader scenarios with imbalanced data, such as imbalanced multi-task learning and causal inference. Theoretical properties, including bounds on bias estimation errors and improvements in prediction accuracy, are provided. Simulation results and data analysis on handwritten digit datasets demonstrate the effectiveness of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26046v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengfei Lyu, Zhengchi Ma, Linjun Zhang, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Estimation and Inference in Boundary Discontinuity Designs: Distance-Based Methods</title>
      <link>https://arxiv.org/abs/2510.26051</link>
      <description>arXiv:2510.26051v1 Announce Type: cross 
Abstract: We study the statistical properties of nonparametric distance-based (isotropic) local polynomial regression estimators of the boundary average treatment effect curve, a key causal functional parameter capturing heterogeneous treatment effects in boundary discontinuity designs. We present necessary and/or sufficient conditions for identification, estimation, and inference in large samples, both pointwise and uniformly along the boundary. Our theoretical results highlight the crucial role played by the ``regularity'' of the boundary (a one-dimensional manifold) over which identification, estimation, and inference are conducted. Our methods are illustrated with simulated data. Companion general-purpose software is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26051v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo (Rae), Rocio Titiunik (Rae),  Ruiqi (Rae),  Yu</dc:creator>
    </item>
    <item>
      <title>Bridging the Gap between Empirical Welfare Maximization and Conditional Average Treatment Effect Estimation in Policy Learning</title>
      <link>https://arxiv.org/abs/2510.26723</link>
      <description>arXiv:2510.26723v1 Announce Type: cross 
Abstract: The goal of policy learning is to train a policy function that recommends a treatment given covariates to maximize population welfare. There are two major approaches in policy learning: the empirical welfare maximization (EWM) approach and the plug-in approach. The EWM approach is analogous to a classification problem, where one first builds an estimator of the population welfare, which is a functional of policy functions, and then trains a policy by maximizing the estimated welfare. In contrast, the plug-in approach is based on regression, where one first estimates the conditional average treatment effect (CATE) and then recommends the treatment with the highest estimated outcome. This study bridges the gap between the two approaches by showing that both are based on essentially the same optimization problem. In particular, we prove an exact equivalence between EWM and least squares over a reparameterization of the policy class. As a consequence, the two approaches are interchangeable in several respects and share the same theoretical guarantees under common conditions. Leveraging this equivalence, we propose a novel regularization method for policy learning. Our findings yield a convex and computationally efficient training procedure that avoids the NP-hard combinatorial step typically required in EWM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26723v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>A Unified Theory for Causal Inference: Direct Debiased Machine Learning via Bregman-Riesz Regression</title>
      <link>https://arxiv.org/abs/2510.26783</link>
      <description>arXiv:2510.26783v1 Announce Type: cross 
Abstract: This note introduces a unified theory for causal inference that integrates Riesz regression, covariate balancing, density-ratio estimation (DRE), targeted maximum likelihood estimation (TMLE), and the matching estimator in average treatment effect (ATE) estimation. In ATE estimation, the balancing weights and the regression functions of the outcome play important roles, where the balancing weights are referred to as the Riesz representer, bias-correction term, and clever covariates, depending on the context. Riesz regression, covariate balancing, DRE, and the matching estimator are methods for estimating the balancing weights, where Riesz regression is essentially equivalent to DRE in the ATE context, the matching estimator is a special case of DRE, and DRE is in a dual relationship with covariate balancing. TMLE is a method for constructing regression function estimators such that the leading bias term becomes zero. Nearest Neighbor Matching is equivalent to Least Squares Density Ratio Estimation and Riesz Regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26783v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Mixture Modeling for Temporal Point Processes with Memory</title>
      <link>https://arxiv.org/abs/2407.03774</link>
      <description>arXiv:2407.03774v2 Announce Type: replace 
Abstract: We propose a constructive approach to building temporal point processes that incorporate dependence on their history. The dependence is modeled through the conditional density of the duration, i.e., the interval between successive event times, using a mixture of first-order conditional densities for each one of a specific number of lagged durations. Such a formulation for the conditional duration density accommodates high-order dynamics, and it thus enables flexible modeling for point processes with memory. The implied conditional intensity function admits a representation as a local mixture of first-order hazard functions. By specifying appropriate families of distributions for the first-order conditional densities, with different shapes for the associated hazard functions, we can obtain either self-exciting or self-regulating point processes. From the perspective of duration processes, we develop a method to specify a stationary marginal density. The resulting model, interpreted as a dependent renewal process, introduces high-order Markov dependence among identically distributed durations. Furthermore, we provide extensions to cluster point processes. These can describe duration clustering behaviors attributed to different factors, thus expanding the scope of the modeling framework to a wider range of applications. Regarding implementation, we develop a Bayesian approach to inference, model checking, and prediction. We investigate point process model properties analytically, and illustrate the methodology with both synthetic and real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03774v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaotian Zheng, Athanasios Kottas, Bruno Sans\'o</dc:creator>
    </item>
    <item>
      <title>Structured Lasso for convex nonparametric least squares: An application to Swedish electricity distribution networks</title>
      <link>https://arxiv.org/abs/2409.01911</link>
      <description>arXiv:2409.01911v4 Announce Type: replace 
Abstract: We study the problem of variable selection in convex nonparametric least squares (CNLS). Whereas the least absolute shrinkage and selection operator (Lasso) is a popular technique for least squares, its variable selection performance is unknown in CNLS problems. In this work, we investigate the performance of the Lasso estimator and find out it is usually unable to select variables efficiently. Exploiting the unique structure of the subgradients in CNLS, we develop a structured Lasso method by combining $\ell_1$-norm and $\ell_{\infty}$-norm. The relaxed version of the structured Lasso is proposed for achieving model sparsity and predictive performance simultaneously, where we can control the two effects--variable selection and model shrinkage--using separate tuning parameters. A Monte Carlo study is implemented to verify the finite sample performance of the proposed approaches. We also use real data from Swedish electricity distribution networks to illustrate the effects of the proposed variable selection techniques. The results from the simulation and application confirm that the proposed structured Lasso performs favorably, generally leading to sparser and more accurate predictive models, relative to the conventional Lasso methods in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01911v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiqiang Liao, Zhaonan Qu</dc:creator>
    </item>
    <item>
      <title>Efficient nonparametric estimation with difference-in-differences in the presence of network dependence and interference</title>
      <link>https://arxiv.org/abs/2502.03414</link>
      <description>arXiv:2502.03414v3 Announce Type: replace 
Abstract: Differences-in-differences (DiD) is a causal inference method for observational longitudinal data that assumes parallel expected potential outcome trajectories between treatment groups under the counterfactual scenario where all units receive a specific treatment. In this paper DiD is extended to allow for: (i) non-identically distributed treatment effects and exposure probabilities; (ii) network dependency, where outcomes, treatments, and covariates may exhibit between-unit correlation; and (iii) interference, where treatments can affect outcomes in neighboring units. The causal estimand of interest is the network averaged expected exposure effect if units received a specific exposure level, where a unit's exposure is a function of its own treatment and its neighbors' treatments. Under a conditional parallel trends assumption and suitable network dependency and heterogeneity conditions, a doubly robust estimator allowing for data-adaptive nuisance function estimation is proposed and shown to be consistent, asymptotically normal, and efficient. The proposed methods are evaluated in simulations and applied to study the effects of adopting emission control technologies in coal power plants on county-level mortality due to cardiovascular disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03414v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Jetsupphasuk, Didong Li, Michael G. Hudgens</dc:creator>
    </item>
    <item>
      <title>Definition, Identification, and Estimation of the Direct and Indirect Number Needed to Treat</title>
      <link>https://arxiv.org/abs/2504.16912</link>
      <description>arXiv:2504.16912v2 Announce Type: replace 
Abstract: The number needed to treat (NNT) is an efficacy and effect size measure commonly used in epidemiological studies and meta-analyses. The NNT was originally defined as the average number of patients needed to be treated to observe one less adverse effect. In this study, we introduce the novel direct and indirect number needed to treat (DNNT and INNT, respectively). The DNNT and the INNT are efficacy measures defined as the average number of patients that needed to be treated to benefit from the treatment's direct and indirect effects, respectively. We start by formally defining these measures using nested potential outcomes. Next, we formulate the conditions for the identification of the DNNT and INNT, as well as for the direct and indirect number needed to expose (DNNE and INNE, respectively) and the direct and indirect exposure impact number (DEIN and IEIN, respectively) in observational studies. Next, we present an estimation method with two analytical examples. A corresponding simulation study follows the examples. The simulation study illustrates that the estimators of the novel indices are consistent, and their analytical confidence intervals meet the nominal coverage rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16912v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Valentin Vancak, Arvid Sj\"olander</dc:creator>
    </item>
    <item>
      <title>Predictive Causal Inference via Spatio-Temporal Modeling and Penalized Empirical Likelihood</title>
      <link>https://arxiv.org/abs/2507.08896</link>
      <description>arXiv:2507.08896v2 Announce Type: replace 
Abstract: This study introduces an integrated framework for predictive causal inference designed to overcome limitations inherent in conventional single model approaches. Specifically, we combine a Hidden Markov Model (HMM) for spatial health state estimation with a Multi Task and Multi Graph Convolutional Network (MTGCN) for capturing temporal outcome trajectories. The framework asymmetrically treats temporal and spatial information regarding them as endogenous variables in the outcome regression, and exogenous variables in the propensity score model, thereby expanding the standard doubly robust treatment effect estimation to jointly enhance bias correction and predictive accuracy. To demonstrate its utility, we focus on clinical domains such as cancer, dementia, and Parkinson disease, where treatment effects are challenging to observe directly. Simulation studies are conducted to emulate latent disease dynamics and evaluate the model performance under varying conditions. Overall, the proposed framework advances predictive causal inference by structurally adapting to spatiotemporal complexities common in biomedical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08896v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Byunghee Lee, Hye Yeon Sin, Joonsung Kang</dc:creator>
    </item>
    <item>
      <title>Chiseling: Powerful and Valid Subgroup Selection via Interactive Machine Learning</title>
      <link>https://arxiv.org/abs/2509.19490</link>
      <description>arXiv:2509.19490v3 Announce Type: replace 
Abstract: In regression and causal inference, controlled subgroup selection aims to identify, with inferential guarantees, a subgroup (defined as a subset of the covariate space) on which the average response or treatment effect is above a given threshold. E.g., in a clinical trial, it may be of interest to find a subgroup with a positive average treatment effect. However, existing methods either lack inferential guarantees, heavily restrict the search for the subgroup, or sacrifice efficiency by naive data splitting. We propose a novel framework called chiseling that allows the analyst to interactively refine and test a candidate subgroup by iteratively shrinking it. The sole restriction is that the shrinkage direction only depends on the points outside the current subgroup, but otherwise the analyst may leverage any prior information or machine learning algorithm. Despite this flexibility, chiseling controls the probability that the discovered subgroup is null (e.g., has a non-positive average treatment effect) under minimal assumptions: for example, in randomized experiments, this inferential validity guarantee holds under only bounded moment conditions. When applied to a variety of simulated datasets and a real survey experiment, chiseling identifies substantially better subgroups than existing methods with inferential guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19490v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan Cheng, Asher Spector, Lucas Janson</dc:creator>
    </item>
    <item>
      <title>Designing a quasi-experiment to study the clinical impact of adaptive risk prediction models</title>
      <link>https://arxiv.org/abs/2510.25052</link>
      <description>arXiv:2510.25052v2 Announce Type: replace 
Abstract: Clinical risk prediction is a valuable tool for guiding healthcare interventions toward those most likely to benefit. Yet, evaluating the pairing of a risk prediction model with an intervention using randomized controlled trials presents substantial challenges, making quasi-experimental designs an attractive alternatives. Existing designs, however, assume that both the model and the decision rules used to trigger interventions (typically a risk threshold) remain fixed. This limits their utility in modern healthcare, where both are routinely updated. We introduce a regression discontinuity framework that accommodates adaptation in both the model and the risk threshold. We precisely characterize the form of interference introduced by these adaptations and exploit this structure to establish conditions for identification and thus design estimation strategies. The key idea is to define counterfactual risks-the scores patients would have received under hypothetical reorderings-thereby restoring local exchangeability and enabling valid estimation of the local average treatment effect. Our estimator leverages the fact that, although counterfactual risk vectors grow with time, they typically lie in a low-dimensional space. In simulations of cardiovascular prevention programs, we show that our method accurately recovers treatment effects even as thresholds adapt to meet operational or clinical targets and models are updated to align predicted and observed outcomes or to exclude demographic predictors such as race.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25052v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valerie Odeh-Couvertier, Gabriel Zayas-Caban, Brian Patterson, Amy Cochran</dc:creator>
    </item>
    <item>
      <title>Survey Data Integration for Distribution Function Estimation</title>
      <link>https://arxiv.org/abs/2409.14284</link>
      <description>arXiv:2409.14284v5 Announce Type: replace-cross 
Abstract: Estimates of finite population cumulativedistribution functions (CDFs) and quantiles are critical forpolicy-making, resource allocation, and public health planning. For instance, federal finance agencies may require accurate estimates of the proportion of individuals with income below the federal poverty line to determine funding eligibility, while health organizations may rely on precise quantile estimates of key health variables to guide local health interventions. Despite growing interest in survey data integration, research on the integration of probability and nonprobability samples toestimate CDFs and quantiles remains limited. In this study, we propose a novel residual-based CDF estimator that integrates information from a probability sample with data from potentially large nonprobability samples. Our approach leverages shared covariates observed in both datasets, while the response variable is available only in the nonprobability sample. Using a semiparametric approach, we train an outcome model on the nonprobability sample and incorporate model residuals with sampling weights from the probability sample to estimate the CDF of the target variable. Based on this CDF estimator, we define a quantile estimator and introduce linearization and bootstrap methods for variance estimation of both the CDF and quantile estimators. Under certain regularity conditions, we establish the asymptotic properties, including bias and variance, of the CDF estimator. Our empirical findings support the theoretical results and demonstrate the favorable performance of the proposed estimators relative to plug-in mass imputation estimators and the na\"ive estimators derived from the nonprobability sample only. A real data example is presented to illustrate the proposed estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14284v5</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy Flood, Sayed Mostafa</dc:creator>
    </item>
    <item>
      <title>Estimation and Inference in Boundary Discontinuity Designs: Location-Based Methods</title>
      <link>https://arxiv.org/abs/2505.05670</link>
      <description>arXiv:2505.05670v2 Announce Type: replace-cross 
Abstract: Boundary discontinuity designs are used to learn about causal treatment effects along a continuous assignment boundary that splits units into control and treatment groups according to a bivariate location score. We analyze the statistical properties of local polynomial treatment effect estimators employing location information for each unit. We develop pointwise and uniform estimation and inference methods for both the conditional treatment effect function at the assignment boundary as well as for transformations thereof, which aggregate information along the boundary. We illustrate our methods with an empirical application. Companion general-purpose software is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05670v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Rocio Titiunik, Ruiqi Rae Yu</dc:creator>
    </item>
    <item>
      <title>Direct Debiased Machine Learning via Bregman Divergence Minimization</title>
      <link>https://arxiv.org/abs/2510.23534</link>
      <description>arXiv:2510.23534v2 Announce Type: replace-cross 
Abstract: We develop a direct debiased machine learning framework comprising Neyman targeted estimation and generalized Riesz regression. Our framework unifies Riesz regression for automatic debiased machine learning, covariate balancing, targeted maximum likelihood estimation (TMLE), and density-ratio estimation. In many problems involving causal effects or structural models, the parameters of interest depend on regression functions. Plugging regression functions estimated by machine learning methods into the identifying equations can yield poor performance because of first-stage bias. To reduce such bias, debiased machine learning employs Neyman orthogonal estimating equations. Debiased machine learning typically requires estimation of the Riesz representer and the regression function. For this problem, we develop a direct debiased machine learning framework with an end-to-end algorithm. We formulate estimation of the nuisance parameters, the regression function and the Riesz representer, as minimizing the discrepancy between Neyman orthogonal scores computed with known and unknown nuisance parameters, which we refer to as Neyman targeted estimation. Neyman targeted estimation includes Riesz representer estimation, and we measure discrepancies using the Bregman divergence. The Bregman divergence encompasses various loss functions as special cases, where the squared loss yields Riesz regression and the Kullback-Leibler divergence yields entropy balancing. We refer to this Riesz representer estimation as generalized Riesz regression. Neyman targeted estimation also yields TMLE as a special case for regression function estimation. Furthermore, for specific pairs of models and Riesz representer estimation methods, we can automatically obtain the covariate balancing property without explicitly solving the covariate balancing objective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23534v2</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
  </channel>
</rss>
