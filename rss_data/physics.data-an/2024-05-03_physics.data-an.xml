<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 May 2024 06:12:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 03 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Statistical Method for Improving Momentum Measurement of Photon Conversions Reconstructed from Single Electrons</title>
      <link>https://arxiv.org/abs/2405.01454</link>
      <description>arXiv:2405.01454v1 Announce Type: new 
Abstract: The reconstruction of photon conversions is importantin order to improve the reconstruction efficiency of the physics measurements involving photons. However, there are significant number of conversions in which only one of the two tracks emitted electrons is reconstructed in the detector due to very asymmetric energy sharing between the electron-positron pair. The momentum determination of the parent photon can be improved by estimating the missing energy in such conversions. In this study, we propose a simple statistical method that can be used to determine the mean value of the missing energy. By using simulated minimum bias events at LHC conditions and a toy detector simulation, the performance of the method is tested for several decay channels commonly used in particle physics analyses. A considerable improvement in the mass reconstruction precision is obtained when reconstructing particles decaying to photons whose energies are less than 20 GeV.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01454v1</guid>
      <category>physics.data-an</category>
      <category>hep-ex</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahmet Bing\"ul, Zekeriya Uysal</dc:creator>
    </item>
    <item>
      <title>Network reconstruction via the minimum description length principle</title>
      <link>https://arxiv.org/abs/2405.01015</link>
      <description>arXiv:2405.01015v1 Announce Type: cross 
Abstract: A fundamental problem associated with the task of network reconstruction from dynamical or behavioral data consists in determining the most appropriate model complexity in a manner that prevents overfitting, and produces an inferred network with a statistically justifiable number of edges. The status quo in this context is based on $L_{1}$ regularization combined with cross-validation. As we demonstrate, besides its high computational cost, this commonplace approach unnecessarily ties the promotion of sparsity with weight "shrinkage". This combination forces a trade-off between the bias introduced by shrinkage and the network sparsity, which often results in substantial overfitting even after cross-validation. In this work, we propose an alternative nonparametric regularization scheme based on hierarchical Bayesian inference and weight quantization, which does not rely on weight shrinkage to promote sparsity. Our approach follows the minimum description length (MDL) principle, and uncovers the weight distribution that allows for the most compression of the data, thus avoiding overfitting without requiring cross-validation. The latter property renders our approach substantially faster to employ, as it requires a single fit to the complete data. As a result, we have a principled and efficient inference scheme that can be used with a large variety of generative models, without requiring the number of edges to be known in advance. We also demonstrate that our scheme yields systematically increased accuracy in the reconstruction of both artificial and empirical networks. We highlight the use of our method with the reconstruction of interaction networks between microbial communities from large-scale abundance samples involving in the order of $10^{4}$ to $10^{5}$ species, and demonstrate how the inferred model can be used to predict the outcome of interventions in the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01015v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>physics.data-an</category>
      <category>q-bio.PE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tiago P. Peixoto</dc:creator>
    </item>
    <item>
      <title>Multimodal reconstruction of TbCo thin film structure with Basyeian analysis of polarised neutron reflectivity</title>
      <link>https://arxiv.org/abs/2405.01243</link>
      <description>arXiv:2405.01243v1 Announce Type: cross 
Abstract: We implemented the Bayesian analysis to the polarised neutron reflectivity data. Reflectivity data from a magnetic TbCo thin film structure was studied using the bundle of a Monte-Carlo Markov-chain algorithm, likelihood estimation, and error modeling. By utilizing the Bayesian analysis, we were able to investigate the uniqueness of the solution beyond reconstructing the magnetic and structure parameters. This approach has demonstrated its expedience as several probable reconstructions were found (the multimodality case) concerning the isotopic composition of the surface cover layer. Such multimodal reconstruction emphasizes the importance of rigorous data analysis instead of the direct data fitting approach, especially in the case of poor statistically conditioned data, typical for neutron reflectivity experiments. The analysis details and the discussion on multimodality are in this article.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01243v1</guid>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>P. S. Savchenkov, K. V. Nikolaev, V. I. Bodnarchuk, A. N. Pirogov, A. V. Belushkin, S. N. Yakunin</dc:creator>
    </item>
    <item>
      <title>A hypergraph model shows the carbon reduction potential of effective space use in housing</title>
      <link>https://arxiv.org/abs/2405.01290</link>
      <description>arXiv:2405.01290v1 Announce Type: cross 
Abstract: Humans spend over 90% of their time in buildings which account for 40% of anthropogenic greenhouse gas (GHG) emissions, making buildings the leading cause of climate change. To incentivize more sustainable construction, building codes are used to enforce indoor comfort standards and maximum energy use. However, they currently only reward energy efficiency measures such as equipment or envelope upgrades and disregard the actual spatial configuration and usage. Using a new hypergraph model that encodes building floorplan organization and facilitates automatic geometry creation, we demonstrate that space efficiency outperforms envelope upgrades in terms of operational carbon emissions in 72%, 61% and 33% of surveyed buildings in Zurich, New York, and Singapore. Automatically generated floorplans for a case study in Zurich further increase access to daylight by up to 24%, revealing that auto-generated floorplans have the potential to improve the quality of residential spaces in terms of environmental performance and access to daylight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01290v1</guid>
      <category>cs.CG</category>
      <category>cs.GR</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ramon Elias Weber, Caitlin Mueller, Christoph Reinhart</dc:creator>
    </item>
    <item>
      <title>Scalable network reconstruction in subquadratic time</title>
      <link>https://arxiv.org/abs/2401.01404</link>
      <description>arXiv:2401.01404v4 Announce Type: replace-cross 
Abstract: Network reconstruction consists in determining the unobserved pairwise couplings between $N$ nodes given only observational data on the resulting behavior that is conditioned on those couplings -- typically a time-series or independent samples from a graphical model. A major obstacle to the scalability of algorithms proposed for this problem is a seemingly unavoidable quadratic complexity of $\Omega(N^2)$, corresponding to the requirement of each possible pairwise coupling being contemplated at least once, despite the fact that most networks of interest are sparse, with a number of non-zero couplings that is only $O(N)$. Here we present a general algorithm applicable to a broad range of reconstruction problems that significantly outperforms this quadratic baseline. Our algorithm relies on a stochastic second neighbor search (Dong et al., 2011) that produces the best edge candidates with high probability, thus bypassing an exhaustive quadratic search. If we rely on the conjecture that the second-neighbor search finishes in log-linear time (Baron &amp; Darling, 2020; 2022), we demonstrate theoretically that our algorithm finishes in subquadratic time, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\log N)$, but with a more typical log-linear complexity of $O(N\log^2N)$. In practice, we show that our algorithm achieves a performance that is many orders of magnitude faster than the quadratic baseline -- in a manner consistent with our theoretical analysis -- allows for easy parallelization, and thus enables the reconstruction of networks with hundreds of thousands and even millions of nodes and edges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01404v4</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tiago P. Peixoto</dc:creator>
    </item>
  </channel>
</rss>
