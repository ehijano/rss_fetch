<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Jun 2024 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Coin-Flipping In The Brain: Statistical Learning with Neuronal Assemblies</title>
      <link>https://arxiv.org/abs/2406.07715</link>
      <description>arXiv:2406.07715v1 Announce Type: new 
Abstract: How intelligence arises from the brain is a central problem in science. A crucial aspect of intelligence is dealing with uncertainty -- developing good predictions about one's environment, and converting these predictions into decisions. The brain itself seems to be noisy at many levels, from chemical processes which drive development and neuronal activity to trial variability of responses to stimuli. One hypothesis is that the noise inherent to the brain's mechanisms is used to sample from a model of the world and generate predictions. To test this hypothesis, we study the emergence of statistical learning in NEMO, a biologically plausible computational model of the brain based on stylized neurons and synapses, plasticity, and inhibition, and giving rise to assemblies -- a group of neurons whose coordinated firing is tantamount to recalling a location, concept, memory, or other primitive item of cognition. We show in theory and simulation that connections between assemblies record statistics, and ambient noise can be harnessed to make probabilistic choices between assemblies. This allows NEMO to create internal models such as Markov chains entirely from the presentation of sequences of stimuli. Our results provide a foundation for biologically plausible probabilistic computation, and add theoretical support to the hypothesis that noise is a useful component of the brain's mechanism for cognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07715v1</guid>
      <category>q-bio.NC</category>
      <category>cs.NE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Max Dabagia, Daniel Mitropolsky, Christos H. Papadimitriou, Santosh S. Vempala</dc:creator>
    </item>
    <item>
      <title>Functional voxel hierarchy and afferent capacity revealed mental state transition on dynamic correlation resting-state fMRI</title>
      <link>https://arxiv.org/abs/2406.08140</link>
      <description>arXiv:2406.08140v1 Announce Type: new 
Abstract: Voxel hierarchy on dynamic brain graphs is produced by k core percolation on functional dynamic amplitude correlation of resting-state fMRI. Directed graphs and their afferent/efferent capacities are produced by Markov modeling of the universal cover of undirected graphs simultaneously with the calculation of volume entropy. Positive and unsigned negative brain graphs were analyzed separately on sliding-window representation to underpin the visualization and quantitation of mental dynamic states with their transitions. Voxel hierarchy animation maps of positive graphs revealed abrupt changes in coreness k and kmaxcore, which we called mental state transitions. Afferent voxel capacities of the positive graphs also revealed transient modules composed of dominating voxels/independent components and their exchanges representing mental state transitions. Animation and quantification plots of voxel hierarchy and afferent capacity corroborated each other in underpinning mental state transitions and afferent module exchange on the positive directed functional connectivity graphs. We propose the use of spatiotemporal trajectories of voxels on positive dynamic graphs to construct hierarchical structures by k core percolation and quantified in- and out-flows of information of voxels by volume entropy/directed graphs to subserve diverse resting mental state transitions on resting-state fMRI graphs in normal human individuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08140v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dong Soo Lee, Hyun Joo Kim, Youngmin Huh, Yeon Koo Kang, Wonseok Whi, Hyekyoung Lee, Hyejin Kang</dc:creator>
    </item>
    <item>
      <title>Progress Towards Decoding Visual Imagery via fNIRS</title>
      <link>https://arxiv.org/abs/2406.07662</link>
      <description>arXiv:2406.07662v1 Announce Type: cross 
Abstract: We demonstrate the possibility of reconstructing images from fNIRS brain activity and start building a prototype to match the required specs. By training an image reconstruction model on downsampled fMRI data, we discovered that cm-scale spatial resolution is sufficient for image generation. We obtained 71% retrieval accuracy with 1-cm resolution, compared to 93% on the full-resolution fMRI, and 20% with 2-cm resolution. With simulations and high-density tomography, we found that time-domain fNIRS can achieve 1-cm resolution, compared to 2-cm resolution for continuous-wave fNIRS. Lastly, we share designs for a prototype time-domain fNIRS device, consisting of a laser driver, a single photon detector, and a time-to-digital converter system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07662v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michel Adamic (McGill University), Wellington Avelino (McGill University), Anna Brandenberger (Massachusetts Institute of Technology), Bryan Chiang (Stanford University), Hunter Davis (McGill University), Stephen Fay (McGill University), Andrew Gregory (McGill University), Aayush Gupta (McGill University), Raphael Hotter (McGill University), Grace Jiang (McGill University), Fiona Leng (McGill University), Stephen Polcyn (McGill University), Thomas Ribeiro (McGill University), Paul Scotti (Princeton University), Michelle Wang (McGill University), Marley Xiong (University of Waterloo), Jonathan Xu (University of Waterloo)</dc:creator>
    </item>
    <item>
      <title>A Concise Mathematical Description of Active Inference in Discrete Time</title>
      <link>https://arxiv.org/abs/2406.07726</link>
      <description>arXiv:2406.07726v1 Announce Type: cross 
Abstract: In this paper we present a concise mathematical description of active inference in discrete time. The main part of the paper serves as a general introduction to the topic, including an example illustrating the theory on action selection. In the appendix the more subtle mathematical details are discussed. This part is aimed at readers who have already studied the active inference literature but struggle to make sense of the mathematical details and derivations. Throughout the whole manuscript, special attention has been paid to adopting notation that is both precise and in line with standard mathematical texts. All equations and derivations are linked to specific equation numbers in other popular text on the topic. Furthermore, Python code is provided that implements the action selection mechanism described in this paper and is compatible with pymdp environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07726v1</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesse van Oostrum, Carlotta Langer, Nihat Ay</dc:creator>
    </item>
    <item>
      <title>Incremental Learning and Self-Attention Mechanisms Improve Neural System Identification</title>
      <link>https://arxiv.org/abs/2406.07843</link>
      <description>arXiv:2406.07843v1 Announce Type: cross 
Abstract: Convolutional neural networks (CNNs) have been shown to be the state-of-the-art approach for modeling the transfer functions of visual cortical neurons. Cortical neurons in the primary visual cortex are are sensitive to contextual information mediated by extensive horizontal and feedback connections. Standard CNNs can integrate global spatial image information to model such contextual modulation via two mechanisms: successive rounds of convolutions and a fully connected readout layer. In this paper, we find that non-local networks or self-attention (SA) mechanisms, theoretically related to context-dependent flexible gating mechanisms observed in the primary visual cortex, improve neural response predictions over parameter-matched CNNs in two key metrics: tuning curve correlation and tuning peak. We factorize networks to determine the relative contribution of each context mechanism. This reveals that information in the local receptive field is most important for modeling the overall tuning curve, but surround information is critically necessary for characterizing the tuning peak. We find that self-attention can replace subsequent spatial-integration convolutions when learned in an incremental manner, and is further enhanced in the presence of a fully connected readout layer, suggesting that the two context mechanisms are complementary. Finally, we find that learning a receptive-field-centric model with self-attention, before incrementally learning a fully connected readout, yields a more biologically realistic model in terms of center-surround contributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07843v1</guid>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaac Lin, Tianye Wang, Shang Gao, Shiming Tang, Tai Sing Lee</dc:creator>
    </item>
    <item>
      <title>Exploring Geometrical Properties of Chaotic Systems Through an Analysis of the Rulkov Neuron Maps</title>
      <link>https://arxiv.org/abs/2406.08385</link>
      <description>arXiv:2406.08385v1 Announce Type: cross 
Abstract: While extensive research has been conducted on chaos emerging from a dynamical system's temporal dynamics, the research documented in this paper examines extreme sensitivity to initial conditions in discrete-time dynamical systems from a geometrical perspective. The heart of this paper focuses on two simple neuron maps developed by Nikolai F. Rulkov in the early 2000s and the complex geometrical structures that emerge from them. Beginning with a conversational introduction to the geometry of chaos, this paper integrates mathematics, physics, neurobiology, computational modeling, and electrochemistry to present original research that provides a novel perspective on how types of geometrical sensitivity to initial conditions appear in discrete-time neuron systems.
  This paper was developed in the Thomas Jefferson High School for Science and Technology Quantum Lab as part of a senior research project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08385v1</guid>
      <category>nlin.CD</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brandon B. Le, Nivika A. Gandhi</dc:creator>
    </item>
    <item>
      <title>Surprise! Using Physiological Stress for Allostatic Regulation Under the Active Inference Framework [Pre-Print]</title>
      <link>https://arxiv.org/abs/2406.08471</link>
      <description>arXiv:2406.08471v1 Announce Type: cross 
Abstract: Allostasis proposes that long-term viability of a living system is achieved through anticipatory adjustments of its physiology and behaviour: emphasising physiological and affective stress as an adaptive state of adaptation that minimizes long-term prediction errors. More recently, the active inference framework (AIF) has also sought to explain action and long-term adaptation through the minimization of future errors (free energy), through the learning of statistical contingencies of the world, offering a formalism for allostatic regulation. We suggest that framing prediction errors through the lens of biological hormonal dynamics proposed by allostasis offers a way to integrate these two models together in a biologically-plausible manner. In this paper, we describe our initial work in developing a model that grounds prediction errors (surprisal) into the secretion of a physiological stress hormone (cortisol) acting as an adaptive, allostatic mediator on a homeostatically-controlled physiology. We evaluate this using a computational model in simulations using an active inference agent endowed with an artificial physiology, regulated through homeostatic and allostatic control in a stochastic environment. Our results find that allostatic functions of cortisol (stress), secreted as a function of prediction errors, provide adaptive advantages to the agent's long-term physiological regulation. We argue that the coupling of information-theoretic prediction errors to low-level, biological hormonal dynamics of stress can provide a computationally efficient model to long-term regulation for embodied intelligent systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08471v1</guid>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Imran Khan, Robert Lowe</dc:creator>
    </item>
    <item>
      <title>Why Do Animals Need Shaping? A Theory of Task Composition and Curriculum Learning</title>
      <link>https://arxiv.org/abs/2402.18361</link>
      <description>arXiv:2402.18361v2 Announce Type: replace 
Abstract: Diverse studies in systems neuroscience begin with extended periods of curriculum training known as `shaping' procedures. These involve progressively studying component parts of more complex tasks, and can make the difference between learning a task quickly, slowly or not at all. Despite the importance of shaping to the acquisition of complex tasks, there is as yet no theory that can help guide the design of shaping procedures, or more fundamentally, provide insight into its key role in learning. Modern deep reinforcement learning systems might implicitly learn compositional primitives within their multilayer policy networks. Inspired by these models, we propose and analyse a model of deep policy gradient learning of simple compositional reinforcement learning tasks. Using the tools of statistical physics, we solve for exact learning dynamics and characterise different learning strategies including primitives pre-training, in which task primitives are studied individually before learning compositional tasks. We find a complex interplay between task complexity and the efficacy of shaping strategies. Overall, our theory provides an analytical understanding of the benefits of shaping in a class of compositional tasks and a quantitative account of how training protocols can disclose useful task primitives, ultimately yielding faster and more robust learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18361v2</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin Hwa Lee, Stefano Sarao Mannelli, Andrew Saxe</dc:creator>
    </item>
  </channel>
</rss>
