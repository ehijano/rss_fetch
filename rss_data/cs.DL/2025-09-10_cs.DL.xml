<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Sep 2025 04:07:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Causal evidence of racial and institutional biases in accessing paywalled articles and scientific data</title>
      <link>https://arxiv.org/abs/2509.08299</link>
      <description>arXiv:2509.08299v1 Announce Type: new 
Abstract: Scientific progress fundamentally depends on researchers' ability to access and build upon the work of others. Yet, a majority of published work remains behind expensive paywalls, limiting access to universities that can afford subscriptions. Furthermore, even when articles are accessible, the underlying datasets could be restricted, available only through a "reasonable request" to the authors. One way researchers could overcome these barriers is by relying on informal channels, such as emailing authors directly, to obtain paywalled articles or restricted datasets. However, whether these informal channels are hindered by racial and/or institutional biases remains unknown. Here, we combine qualitative semi-structured interviews, large-scale observational analysis, and two randomized audit experiments to examine racial and institutional disparities in access to scientific knowledge. Our analysis of 250 million articles reveals that researchers in the Global South cite paywalled papers and upon-request datasets at significantly lower rates than their Global North counterparts, and that these access gaps are associated with reduced knowledge breadth and scholarly impact. To interrogate the mechanisms underlying this phenomenon, we conduct two randomized email audit studies in which fictional PhD students differing in racial background and institutional affiliation request access to paywalled articles (N = 18,000) and datasets (N = 11,840). We find that racial identity more strongly predicts response rate to paywalled article requests compared to institutional affiliation, whereas institutional affiliation played a larger role in shaping access to datasets. These findings reveal how informal gatekeeping can perpetuate structural inequities in science, highlighting the need for stronger data-sharing mandates and more equitable open access policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08299v1</guid>
      <category>cs.DL</category>
      <category>cs.CY</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hazem Ibrahim, Fengyuan Liu, Khalid Mengal, Aaron R. Kaufman, Yasir Zaki, Talal Rahwan</dc:creator>
    </item>
    <item>
      <title>A Use Case Lens on Digital Cultural Heritage</title>
      <link>https://arxiv.org/abs/2509.08710</link>
      <description>arXiv:2509.08710v1 Announce Type: new 
Abstract: This article proposes a novel methodological approach for developing use cases for CH e-infrastuctures documented using Jupyter Notebooks (JNs), enabling transparency and reproducibility. We also address the present problem of use cases that are not consistently documented to cover all key aspects that are derived from the use case literature review outside of CH field to define a useful use case.
  Purpose. Our primary objective is to explore the practices around creating and analysing use cases related to digital cultural heritage. Our review of the literature showed a substantial deviation in the depth and coverage of use cases and revealed the need for a more robust and consistent approach to creating use cases in a digital heritage context. We developed a framework to develop use cases to support the ongoing efforts to expand the use of eInfrastructures in the digital heritage domain as a first step.
  Design/methodology/approach. Our research design combines desk research of existing literature and analysing examples of use cases documented in projects. We examine the challenges and inconsistencies in the current practice of use case production in digital heritage. Finally, we synthesize a systematic process to generate use cases which is illustrated by five example use cases within the context.
  Our work impacts directly such infrastructures and communities as the International GLAM Labs Community, AI for Libraries, Archives, and Museums (AI4LAM) and Time Machine Organisation. This work advances the use of data research infrastructures within communities of researchers, scholars, students, GLAM (Galleries, Libraries, Archives, and Museums) institutions, and Cultural Heritage and Cultural and Creative Industries (CCIs).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08710v1</guid>
      <category>cs.DL</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gustavo Candela, Milena Dobreva, Henk Alkemade, Olga Holownia, Mahendra Mahey, Sarah Ames, Karen Renaud, Ines Vodopivec, Benjamin Charles Germain Lee, Thomas Padilla, Steven Claeyssens, Isto Huvila, Beth Knazook</dc:creator>
    </item>
    <item>
      <title>The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist Systems</title>
      <link>https://arxiv.org/abs/2509.08713</link>
      <description>arXiv:2509.08713v1 Announce Type: cross 
Abstract: AI scientist systems, capable of autonomously executing the full research workflow from hypothesis generation and experimentation to paper writing, hold significant potential for accelerating scientific discovery. However, the internal workflow of these systems have not been closely examined. This lack of scrutiny poses a risk of introducing flaws that could undermine the integrity, reliability, and trustworthiness of their research outputs. In this paper, we identify four potential failure modes in contemporary AI scientist systems: inappropriate benchmark selection, data leakage, metric misuse, and post-hoc selection bias. To examine these risks, we design controlled experiments that isolate each failure mode while addressing challenges unique to evaluating AI scientist systems. Our assessment of two prominent open-source AI scientist systems reveals the presence of several failures, across a spectrum of severity, which can be easily overlooked in practice. Finally, we demonstrate that access to trace logs and code from the full automated workflow enables far more effective detection of such failures than examining the final paper alone. We thus recommend journals and conferences evaluating AI-generated research to mandate submission of these artifacts alongside the paper to ensure transparency, accountability, and reproducibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08713v1</guid>
      <category>cs.AI</category>
      <category>cs.DL</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziming Luo, Atoosa Kasirzadeh, Nihar B. Shah</dc:creator>
    </item>
    <item>
      <title>Whose Name Comes Up? Auditing LLM-Based Scholar Recommendations</title>
      <link>https://arxiv.org/abs/2506.00074</link>
      <description>arXiv:2506.00074v2 Announce Type: replace-cross 
Abstract: This paper evaluates the performance of six open-weight LLMs (llama3-8b, llama3.1-8b, gemma2-9b, mixtral-8x7b, llama3-70b, llama3.1-70b) in recommending experts in physics across five tasks: top-k experts by field, influential scientists by discipline, epoch, seniority, and scholar counterparts. The evaluation examines consistency, factuality, and biases related to gender, ethnicity, academic popularity, and scholar similarity. Using ground-truth data from the American Physical Society and OpenAlex, we establish scholarly benchmarks by comparing model outputs to real-world academic records. Our analysis reveals inconsistencies and biases across all models. mixtral-8x7b produces the most stable outputs, while llama3.1-70b shows the highest variability. Many models exhibit duplication, and some, particularly gemma2-9b and llama3.1-8b, struggle with formatting errors. LLMs generally recommend real scientists, but accuracy drops in field-, epoch-, and seniority-specific queries, consistently favoring senior scholars. Representation biases persist, replicating gender imbalances (reflecting male predominance), under-representing Asian scientists, and over-representing White scholars. Despite some diversity in institutional and collaboration networks, models favor highly cited and productive scholars, reinforcing the rich-getricher effect while offering limited geographical representation. These findings highlight the need to improve LLMs for more reliable and equitable scholarly recommendations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00074v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.DL</category>
      <category>cs.IR</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Daniele Barolo, Chiara Valentin, Fariba Karimi, Luis Gal\'arraga, Gonzalo G. M\'endez, Lisette Esp\'in-Noboa</dc:creator>
    </item>
    <item>
      <title>SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP</title>
      <link>https://arxiv.org/abs/2509.07801</link>
      <description>arXiv:2509.07801v2 Announce Type: replace-cross 
Abstract: Structured information extraction from scientific literature is crucial for capturing core concepts and emerging trends in specialized fields. While existing datasets aid model development, most focus on specific publication sections due to domain complexity and the high cost of annotating scientific texts. To address this limitation, we introduce SciNLP - a specialized benchmark for full-text entity and relation extraction in the Natural Language Processing (NLP) domain. The dataset comprises 60 manually annotated full-text NLP publications, covering 7,072 entities and 1,826 relations. Compared to existing research, SciNLP is the first dataset providing full-text annotations of entities and their relationships in the NLP domain. To validate the effectiveness of SciNLP, we conducted comparative experiments with similar datasets and evaluated the performance of state-of-the-art supervised models on this dataset. Results reveal varying extraction capabilities of existing models across academic texts of different lengths. Cross-comparisons with existing datasets show that SciNLP achieves significant performance improvements on certain baseline models. Using models trained on SciNLP, we implemented automatic construction of a fine-grained knowledge graph for the NLP domain. Our KG has an average node degree of 3.2 per entity, indicating rich semantic topological information that enhances downstream applications. The dataset is publicly available at https://github.com/AKADDC/SciNLP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07801v2</guid>
      <category>cs.CL</category>
      <category>cs.DL</category>
      <category>cs.IR</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Decheng Duan, Yingyi Zhang, Jitong Peng, Chengzhi Zhang</dc:creator>
    </item>
  </channel>
</rss>
