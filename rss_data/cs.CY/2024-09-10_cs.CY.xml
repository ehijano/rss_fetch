<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Sep 2024 01:47:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The limits of progress in the digital era</title>
      <link>https://arxiv.org/abs/2409.05082</link>
      <description>arXiv:2409.05082v1 Announce Type: new 
Abstract: The concept of progress clearly percolates the activities in science, technology, economy and society. It is a driving vector (probably the main vector) of our daily activity as researchers. The InThisGen initiative, proudly displayed in places across the University of Berkeley campus, and its headline lemma (what can we change in a single generation?) are clear exponents of the underlying assumption that progress is not only possible but also desirable. But about the concept of progress two major concerns arise. First of all, progress means some kind of going forward, that is a direction in a journey. But deciding the way in the route clearly implies that we are explicit or implicitly defining the goals, as individuals and as society. That is, the concept of progress has a set of underlying values. Additionally, the conceptual paradigm in scientific research (and probably in the whole spirit of our times) it is assuming some kind of endless progress. It is true that many technological innovations and their subsequent impact on society have found resistance, from Luddites to ecologist movements. But the last 150 years (the age of our university) have been witness of an enormous and general increase in knowledge, wealth and welfare, showing how progress can be sustained in the long-term and positively influence the human beings and the society. In this contribution will try to discuss these bounds, addressing the limits of materials, scientific knowledge and technological know-how. We will mainly focus on the limitations in technological knowledge in the software design, a key aspect of the digital era. Our main thesis, which will be addressed through the paper, is that there are intrinsic limits to technological knowledge and the concept of progress should take them into account.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05082v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Joaquin Luque</dc:creator>
    </item>
    <item>
      <title>Beyond One-Time Validation: A Framework for Adaptive Validation of Prognostic and Diagnostic AI-based Medical Devices</title>
      <link>https://arxiv.org/abs/2409.04794</link>
      <description>arXiv:2409.04794v1 Announce Type: cross 
Abstract: Prognostic and diagnostic AI-based medical devices hold immense promise for advancing healthcare, yet their rapid development has outpaced the establishment of appropriate validation methods. Existing approaches often fall short in addressing the complexity of practically deploying these devices and ensuring their effective, continued operation in real-world settings. Building on recent discussions around the validation of AI models in medicine and drawing from validation practices in other fields, a framework to address this gap is presented. It offers a structured, robust approach to validation that helps ensure device reliability across differing clinical environments. The primary challenges to device performance upon deployment are discussed while highlighting the impact of changes related to individual healthcare institutions and operational processes. The presented framework emphasizes the importance of repeating validation and fine-tuning during deployment, aiming to mitigate these issues while being adaptable to challenges unforeseen during device development. The framework is also positioned within the current US and EU regulatory landscapes, underscoring its practical viability and relevance considering regulatory requirements. Additionally, a practical example demonstrating potential benefits of the framework is presented. Lastly, guidance on assessing model performance is offered and the importance of involving clinical stakeholders in the validation and fine-tuning process is discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04794v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Hellmeier, Kay Brosien, Carsten Eickhoff, Alexander Meyer</dc:creator>
    </item>
    <item>
      <title>Centralized Selection with Preferences in the Presence of Biases</title>
      <link>https://arxiv.org/abs/2409.04897</link>
      <description>arXiv:2409.04897v1 Announce Type: cross 
Abstract: This paper considers the scenario in which there are multiple institutions, each with a limited capacity for candidates, and candidates, each with preferences over the institutions. A central entity evaluates the utility of each candidate to the institutions, and the goal is to select candidates for each institution in a way that maximizes utility while also considering the candidates' preferences. The paper focuses on the setting in which candidates are divided into multiple groups and the observed utilities of candidates in some groups are biased--systematically lower than their true utilities. The first result is that, in these biased settings, prior algorithms can lead to selections with sub-optimal true utility and significant discrepancies in the fraction of candidates from each group that get their preferred choices. Subsequently, an algorithm is presented along with proof that it produces selections that achieve near-optimal group fairness with respect to preferences while also nearly maximizing the true utility under distributional assumptions. Further, extensive empirical validation of these results in real-world and synthetic settings, in which the distributional assumptions may not hold, are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04897v1</guid>
      <category>cs.DS</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>econ.TH</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L. Elisa Celis, Amit Kumar, Nisheeth K. Vishnoi, Andrew Xu</dc:creator>
    </item>
    <item>
      <title>Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis</title>
      <link>https://arxiv.org/abs/2409.05292</link>
      <description>arXiv:2409.05292v1 Announce Type: cross 
Abstract: The world is currently experiencing an outbreak of mpox, which has been declared a Public Health Emergency of International Concern by WHO. No prior work related to social media mining has focused on the development of a dataset of Instagram posts about the mpox outbreak. The work presented in this paper aims to address this research gap and makes two scientific contributions to this field. First, it presents a multilingual dataset of 60,127 Instagram posts about mpox, published between July 23, 2022, and September 5, 2024. The dataset, available at https://dx.doi.org/10.21227/7fvc-y093, contains Instagram posts about mpox in 52 languages. For each of these posts, the Post ID, Post Description, Date of publication, language, and translated version of the post (translation to English was performed using the Google Translate API) are presented as separate attributes in the dataset. After developing this dataset, sentiment analysis, hate speech detection, and anxiety or stress detection were performed. This process included classifying each post into (i) one of the sentiment classes, i.e., fear, surprise, joy, sadness, anger, disgust, or neutral, (ii) hate or not hate, and (iii) anxiety/stress detected or no anxiety/stress detected. These results are presented as separate attributes in the dataset. Second, this paper presents the results of performing sentiment analysis, hate speech analysis, and anxiety or stress analysis. The variation of the sentiment classes - fear, surprise, joy, sadness, anger, disgust, and neutral were observed to be 27.95%, 2.57%, 8.69%, 5.94%, 2.69%, 1.53%, and 50.64%, respectively. In terms of hate speech detection, 95.75% of the posts did not contain hate and the remaining 4.25% of the posts contained hate. Finally, 72.05% of the posts did not indicate any anxiety/stress, and the remaining 27.95% of the posts represented some form of anxiety/stress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05292v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nirmalya Thakur</dc:creator>
    </item>
    <item>
      <title>Online Advertisements with LLMs: Opportunities and Challenges</title>
      <link>https://arxiv.org/abs/2311.07601</link>
      <description>arXiv:2311.07601v4 Announce Type: replace 
Abstract: This paper explores the potential for leveraging Large Language Models (LLM) in the realm of online advertising systems. We introduce a general framework for LLM advertisement, consisting of modification, bidding, prediction, and auction modules. Different design considerations for each module are presented. These design choices are evaluated and discussed based on essential desiderata required to maintain a sustainable system. Further fundamental questions regarding practicality, efficiency, and implementation challenges are raised for future research. Finally, we exposit how recent approaches on mechanism design for LLM can be framed in our unified perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07601v4</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soheil Feizi, MohammadTaghi Hajiaghayi, Keivan Rezaei, Suho Shin</dc:creator>
    </item>
    <item>
      <title>Resolving Ethics Trade-offs in Implementing Responsible AI</title>
      <link>https://arxiv.org/abs/2401.08103</link>
      <description>arXiv:2401.08103v4 Announce Type: replace 
Abstract: While the operationalisation of high-level AI ethics principles into practical AI/ML systems has made progress, there is still a theory-practice gap in managing tensions between the underlying AI ethics aspects. We cover five approaches for addressing the tensions via trade-offs, ranging from rudimentary to complex. The approaches differ in the types of considered context, scope, methods for measuring contexts, and degree of justification. None of the approaches is likely to be appropriate for all organisations, systems, or applications. To address this, we propose a framework which consists of: (i) proactive identification of tensions, (ii) prioritisation and weighting of ethics aspects, (iii) justification and documentation of trade-off decisions. The proposed framework aims to facilitate the implementation of well-rounded AI/ML systems that are appropriate for potential regulatory requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08103v4</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/CAI59869.2024.00215</arxiv:DOI>
      <arxiv:journal_reference>IEEE Conference on Artificial Intelligence, 2024</arxiv:journal_reference>
      <dc:creator>Conrad Sanderson, Emma Schleiger, David Douglas, Petra Kuhnert, Qinghua Lu</dc:creator>
    </item>
    <item>
      <title>A+AI: Threats to Society, Remedies, and Governance</title>
      <link>https://arxiv.org/abs/2409.02219</link>
      <description>arXiv:2409.02219v2 Announce Type: replace 
Abstract: This document focuses on the threats, especially near-term threats, that Artificial Intelligence (AI) brings to society. Most of the threats discussed here can result from any algorithmic process, not just AI; in addition, defining AI is notoriously difficult. For both reasons, it is important to think of "A+AI": Algorithms and Artificial Intelligence.
  In addition to the threats, this paper discusses countermeasures to them, and it includes a table showing which countermeasures are likely to mitigate which threats. Thoughtful governance could manage the risks without seriously impeding progress; in fact, chances are it would accelerate progress by reducing the social chaos that would otherwise be likely. The paper lists specific actions government should take as soon as possible, namely:
  * Require all social media platforms accessible in the U.S. to offer users verification that their accounts are owned by citizens, and to display every account's verification status
  * Establish regulations to require that all products created or significantly modified with A+AI be clearly labeled as such; to restrict use of generative AI to create likenesses of persons; and to require creators of generative AI software to disclose materials used to train their software and to compensate the creators of any copyrighted material used
  * Fund a crash project of research on mitigating the threats
  * Fund educational campaigns to raise awareness of the threats</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02219v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Don Byrd</dc:creator>
    </item>
    <item>
      <title>Know, Grow, and Protect Net Worth: Using ML for Asset Protection by Preventing Overdraft Fees</title>
      <link>https://arxiv.org/abs/2302.02455</link>
      <description>arXiv:2302.02455v2 Announce Type: replace-cross 
Abstract: When a customer overdraws their bank account and their balance is negative they are assessed an overdraft fee. Americans pay approximately \$15 billion in unnecessary overdraft fees a year, often in \$35 increments; users of the Mint personal finance app pay approximately \$250 million in fees a year in particular. These overdraft fees are an excessive financial burden and lead to cascading overdraft fees trapping customers in financial hardship. To address this problem, we have created an ML-driven overdraft early warning system (ODEWS) that assesses a customer's risk of overdrafting within the next week using their banking and transaction data in the Mint app. At-risk customers are sent an alert so they can take steps to avoid the fee, ultimately changing their behavior and financial habits. The system deployed resulted in a \$3 million savings in overdraft fees for Mint customers compared to a control group. Moreover, the methodology outlined here is part of a greater effort to provide ML-driven personalized financial advice to help our members know, grow, and protect their net worth, ultimately, achieving their financial goals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.02455v2</guid>
      <category>stat.ML</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3637528.3671628</arxiv:DOI>
      <dc:creator>Avishek Kumar, Tyson Silver</dc:creator>
    </item>
    <item>
      <title>Quantifying Misalignment Between Agents: Towards a Sociotechnical Understanding of Alignment</title>
      <link>https://arxiv.org/abs/2406.04231</link>
      <description>arXiv:2406.04231v2 Announce Type: replace-cross 
Abstract: Existing work on the alignment problem has focused mainly on (1) qualitative descriptions of the alignment problem; (2) attempting to align AI actions with human interests by focusing on value specification and learning; and/or (3) focusing on a single agent or on humanity as a monolith. Recent sociotechnical approaches highlight the need to understand complex misalignment among multiple human and AI agents. We address this gap by adapting a computational social science model of human contention to the alignment problem. Our model quantifies misalignment in large, diverse agent groups with potentially conflicting goals across various problem areas. Misalignment scores in our framework depend on the observed agent population, the domain in question, and conflict between agents' weighted preferences. Through simulations, we demonstrate how our model captures intuitive aspects of misalignment across different scenarios. We then apply our model to two case studies, including an autonomous vehicle setting, showcasing its practical utility. Our approach offers enhanced explanatory power for complex sociotechnical environments and could inform the design of more aligned AI systems in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04231v2</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aidan Kierans, Avijit Ghosh, Hananel Hazan, Shiri Dori-Hacohen</dc:creator>
    </item>
    <item>
      <title>J\"ager: Automated Telephone Call Traceback</title>
      <link>https://arxiv.org/abs/2409.02839</link>
      <description>arXiv:2409.02839v2 Announce Type: replace-cross 
Abstract: Unsolicited telephone calls that facilitate fraud or unlawful telemarketing continue to overwhelm network users and the regulators who prosecute them. The first step in prosecuting phone abuse is traceback -- identifying the call originator. This fundamental investigative task currently requires hours of manual effort per call. In this paper, we introduce J\"ager, a distributed secure call traceback system. J\"ager can trace a call in a few seconds, even with partial deployment, while cryptographically preserving the privacy of call parties, carrier trade secrets like peers and call volume, and limiting the threat of bulk analysis. We establish definitions and requirements of secure traceback, then develop a suite of protocols that meet these requirements using witness encryption, oblivious pseudorandom functions, and group signatures. We prove these protocols secure in the universal composibility framework. We then demonstrate that J\"ager has low compute and bandwidth costs per call, and these costs scale linearly with call volume. J\"ager provides an efficient, secure, privacy-preserving system to revolutionize telephone abuse investigation with minimal costs to operators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02839v2</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3658644.3690290</arxiv:DOI>
      <dc:creator>David Adei, Varun Madathil, Sathvik Prasad, Bradley Reaves, Alessandra Scafuro</dc:creator>
    </item>
  </channel>
</rss>
