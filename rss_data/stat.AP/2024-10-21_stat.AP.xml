<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 Oct 2024 04:00:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Unfolding the Network of Peer Grades: A Latent Variable Approach</title>
      <link>https://arxiv.org/abs/2410.14296</link>
      <description>arXiv:2410.14296v1 Announce Type: new 
Abstract: Peer grading is an educational system in which students assess each other's work. It is commonly applied under Massive Open Online Course (MOOC) and offline classroom settings. With this system, instructors receive a reduced grading workload, and students enhance their understanding of course materials by grading others' work. Peer grading data have a complex dependence structure, for which all the peer grades may be dependent. This complex dependence structure is due to a network structure of peer grading, where each student can be viewed as a vertex of the network, and each peer grade serves as an edge connecting one student as a grader to another student as an examinee. This paper introduces a latent variable model framework for analyzing peer grading data and develops a fully Bayesian procedure for its statistical inference. This framework has several advantages. First, when aggregating multiple peer grades, the average score and other simple summary statistics fail to account for grader effects and, thus, can be biased. The proposed approach produces more accurate model parameter estimates and, therefore, more accurate aggregated grades, by modeling the heterogeneous grading behavior with latent variables. Second, the proposed method provides a way to assess each student's performance as a grader, which may be used to identify a pool of reliable graders or generate feedback to help students improve their grading. Third, our model may further provide insights into the peer grading system by answering questions such as whether a student who performs better in coursework also tends to be a more reliable grader. Finally, thanks to the Bayesian approach, uncertainty quantification is straightforward when inferring the student-specific latent variables as well as the structural parameters of the model. The proposed method is applied to two real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14296v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giuseppe Mignemi, Yunxiao Chen, Irini Moustaki</dc:creator>
    </item>
    <item>
      <title>Predicting the trajectory of intracranial pressure in patients with traumatic brain injury: evaluation of a foundation model for time series</title>
      <link>https://arxiv.org/abs/2410.14333</link>
      <description>arXiv:2410.14333v1 Announce Type: new 
Abstract: Patients with traumatic brain injury (TBI) often experience pathological increases in intracranial pressure (ICP), leading to intracranial hypertension (tIH), a common and serious complication. Early warning of an impending rise in ICP could potentially improve patient outcomes by enabling preemptive clinical intervention. However, the limited availability of patient data poses a challenge in developing reliable prediction models. In this study, we aim to determine whether foundation models, which leverage transfer learning, may offer a promising solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14333v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Florian D. van Leeuwen, Shubhayu Bhattacharyay, Alex Carriero, Ethan Jacob Moyer, Richard Moberg</dc:creator>
    </item>
    <item>
      <title>Model Validation Practice in Banking: A Structured Approach</title>
      <link>https://arxiv.org/abs/2410.13877</link>
      <description>arXiv:2410.13877v1 Announce Type: cross 
Abstract: This paper presents a comprehensive overview of model validation practices and advancement in the banking industry based on the experience of managing Model Risk Management (MRM) since the inception of regulatory guidance SR11-7/OCC11-12 over a decade ago. Model validation in banking is a crucial process designed to ensure that predictive models, which are often used for credit risk, fraud detection, and capital planning, operate reliably and meet regulatory standards. This practice ensures that models are conceptually sound, produce valid outcomes, and are consistently monitored over time. Model validation in banking is a multi-faceted process with three key components: conceptual soundness evaluation, outcome analysis, and on-going monitoring to ensure that the models are not only designed correctly but also perform reliably and consistently in real-world environments. Effective validation helps banks mitigate risks, meet regulatory requirements, and maintain trust in the models that underpin critical business decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13877v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Agus Sudjianto, Aijun Zhang</dc:creator>
    </item>
    <item>
      <title>Modeling Zero-Inflated Correlated Dental Data through Gaussian Copulas and Approximate Bayesian Computation</title>
      <link>https://arxiv.org/abs/2410.13949</link>
      <description>arXiv:2410.13949v1 Announce Type: cross 
Abstract: We develop a new longitudinal count data regression model that accounts for zero-inflation and spatio-temporal correlation across responses. This project is motivated by an analysis of Iowa Fluoride Study (IFS) data, a longitudinal cohort study with data on caries (cavity) experience scores measured for each tooth across five time points. To that end, we use a hurdle model for zero-inflation with two parts: the presence model indicating whether a count is non-zero through logistic regression and the severity model that considers the non-zero counts through a shifted Negative Binomial distribution allowing overdispersion. To incorporate dependence across measurement occasion and teeth, these marginal models are embedded within a Gaussian copula that introduces spatio-temporal correlations. A distinct advantage of this formulation is that it allows us to determine covariate effects with population-level (marginal) interpretations in contrast to mixed model choices. Standard Bayesian sampling from such a model is infeasible, so we use approximate Bayesian computing for inference. This approach is applied to the IFS data to gain insight into the risk factors for dental caries and the correlation structure across teeth and time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13949v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anish Mukherjee, Jeremy T. Gaskins, Shoumi Sarkar, Steven Levy, Somnath Datta</dc:creator>
    </item>
    <item>
      <title>A Statistical Machine Learning Approach for Adapting Reduced-Order Models using Projected Gaussian Process</title>
      <link>https://arxiv.org/abs/2410.14090</link>
      <description>arXiv:2410.14090v1 Announce Type: cross 
Abstract: The Proper Orthogonal Decomposition (POD) computes the optimal basis modes that span a low-dimensional subspace where the Reduced-Order Models (ROMs) reside. Because a governing equation is often parameterized by a set of parameters, challenges immediately arise when one would like to investigate how systems behave differently over the parameter space (in design, control, uncertainty quantification and real-time operations). In this case, the POD basis needs to be updated so as to adapt ROM that accurately captures the variation of a system's behavior over its parameter space. This paper proposes a Projected Gaussian Process (pGP) and formulate the problem of adapting POD basis as a supervised statistical learning problem, for which the goal is to learn a mapping from the parameter space to the Grassmann Manifold that contains the optimal vector subspaces. A mapping is firstly found between the Euclidean space and the horizontal space of an orthogonal matrix that spans a reference subspace in the Grassmann Manifold. Then, a second mapping from the horizontal space to the Grassmann Manifold is established through the Exponential/Logarithm maps between the manifold and its tangent space. Finally, given a new parameter, the conditional distribution of a vector can be found in the Euclidean space using the Gaussian Process (GP) regression, and such a distribution is projected to the Grassmann Manifold that yields the optimal subspace for the new parameter. The proposed statistical learning approach allows us to optimally estimate model parameters given data (i.e., the prediction/interpolation becomes problem-specific), and quantify the uncertainty associated with the prediction. Numerical examples are presented to demonstrate the advantages of the proposed pGP for adapting POD basis against parameter changes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14090v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>stat.AP</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiao Liu, Xinchao Liu</dc:creator>
    </item>
    <item>
      <title>Skill vs. Chance Quantification for Popular Card &amp; Board Games</title>
      <link>https://arxiv.org/abs/2410.14363</link>
      <description>arXiv:2410.14363v1 Announce Type: cross 
Abstract: We consider a few online and offline games under actual playing conditions. Generally it is expected that initially a player obtains additional skill with experience of playing more number of games and then it should finally saturate. This phase is identified when a player, with the experience of very few games, loses more when she plays against players with much longer history. Then the winning proportion curve moves up and finally it saturates. We benchmark our analysis and discussion against Chess, the most skilled one among the games we consider here. We use proprietary data from actual games (online and offline) as well as experiments for our statistical analysis. In this regard, we show that Rummy has stronger skill and learning effects. Ludo has similar characteristics as Rummy, but at a weaker level. Similarly, a game that is perceived as almost no skill such as Teen Patti indeed presents much less skill in the analysis. In the next section we describe the game structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14363v1</guid>
      <category>cs.GT</category>
      <category>stat.AP</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tathagata Banerjee, Anushka De, Subhamoy Maitra, Diganta Mukherjee</dc:creator>
    </item>
    <item>
      <title>Asymptotic non-linear shrinkage formulas for weighted sample covariance</title>
      <link>https://arxiv.org/abs/2410.14420</link>
      <description>arXiv:2410.14420v1 Announce Type: cross 
Abstract: We compute asymptotic non-linear shrinkage formulas for covariance and precision matrix estimators for weighted sample covariances, in the spirit of Ledoit and P\'ech\'e. We detail explicitly the formulas for exponentially-weighted sample covariances. Those new tools pave a way for applying non-linear shrinkage methods on weighted sample covariance. We show experimentally the performance of the asymptotic shrinkage formulas. Finally, we test the robustness of the theory to a heavy-tailed distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14420v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Benoit Oriol</dc:creator>
    </item>
    <item>
      <title>Individualized Multi-Treatment Response Curves Estimation using RBF-net with Shared Neurons</title>
      <link>https://arxiv.org/abs/2401.16571</link>
      <description>arXiv:2401.16571v5 Announce Type: replace-cross 
Abstract: Heterogeneous treatment effect estimation is an important problem in precision medicine. Specific interests lie in identifying the differential effect of different treatments based on some external covariates. We propose a novel non-parametric treatment effect estimation method in a multi-treatment setting. Our non-parametric modeling of the response curves relies on radial basis function (RBF)-nets with shared hidden neurons. Our model thus facilitates modeling commonality among the treatment outcomes. The estimation and inference schemes are developed under a Bayesian framework using thresholded best linear projections and implemented via an efficient Markov chain Monte Carlo algorithm, appropriately accommodating uncertainty in all aspects of the analysis. The numerical performance of the method is demonstrated through simulation experiments. Applying our proposed method to MIMIC data, we obtain several interesting findings related to the impact of different treatment strategies on the length of ICU stay and 12-hour SOFA score for sepsis patients who are home-discharged.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16571v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Chang, Arkaprava Roy</dc:creator>
    </item>
    <item>
      <title>Differentially Private Boxplots</title>
      <link>https://arxiv.org/abs/2405.20415</link>
      <description>arXiv:2405.20415v2 Announce Type: replace-cross 
Abstract: Despite the potential of differentially private data visualization to harmonize data analysis and privacy, research in this area remains relatively underdeveloped. Boxplots are a widely popular visualization used for summarizing a dataset and for comparison of multiple datasets. Consequentially, we introduce a differentially private boxplot. We evaluate its effectiveness for displaying location, scale, skewness and tails of a given empirical distribution. In our theoretical exposition, we show that the location and scale of the boxplot are estimated with optimal sample complexity, and the skewness and tails are estimated consistently. In simulations, we show that this boxplot performs similarly to a non-private boxplot, and it outperforms a boxplot naively constructed from existing differentially private quantile algorithms. Additionally, we conduct a real data analysis of Airbnb listings, which shows that comparable analysis can be achieved through differentially private boxplot visualization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20415v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kelly Ramsay, Jairo Diaz-Rodriguez</dc:creator>
    </item>
    <item>
      <title>Experimenting on Markov Decision Processes with Local Treatments</title>
      <link>https://arxiv.org/abs/2407.19618</link>
      <description>arXiv:2407.19618v2 Announce Type: replace-cross 
Abstract: Utilizing randomized experiments to evaluate the effect of short-term treatments on the short-term outcomes has been well understood and become the golden standard in industrial practice. However, as service systems become increasingly dynamical and personalized, much focus is shifting toward maximizing long-term cumulative outcomes, such as customer lifetime value, through lifetime exposure to interventions. To bridge this gap, we investigate the randomized experiments within dynamical systems modeled as Markov Decision Processes (MDPs). Our goal is to assess the impact of treatment and control policies on long-term cumulative rewards from relatively short-term observations. We first develop optimal inference techniques for assessing the effects of general treatment patterns. Furthermore, recognizing that many real-world treatments tend to be fine-grained and localized for practical efficiency and operational convenience, we then propose methods to harness this localized structure by sharing information on the non-targeted states. Our new estimator effectively overcomes the variance lower bound for general treatments while matching the more stringent lower bound incorporating the local treatment structure. Furthermore, our estimator can optimally achieve a linear reduction with the number of test arms for a major part of the variance. Finally, we explore scenarios with perfect knowledge of the control arm and design estimators that further improve inference efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19618v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuze Chen, David Simchi-Levi, Chonghuan Wang</dc:creator>
    </item>
    <item>
      <title>Sparse Causal Effect Estimation using Two-Sample Summary Statistics in the Presence of Unmeasured Confounding</title>
      <link>https://arxiv.org/abs/2410.12300</link>
      <description>arXiv:2410.12300v2 Announce Type: replace-cross 
Abstract: Observational genome-wide association studies are now widely used for causal inference in genetic epidemiology. To maintain privacy, such data is often only publicly available as summary statistics, and often studies for the endogenous covariates and the outcome are available separately. This has necessitated methods tailored to two-sample summary statistics. Current state-of-the-art methods modify linear instrumental variable (IV) regression -- with genetic variants as instruments -- to account for unmeasured confounding. However, since the endogenous covariates can be high dimensional, standard IV assumptions are generally insufficient to identify all causal effects simultaneously. We ensure identifiability by assuming the causal effects are sparse and propose a sparse causal effect two-sample IV estimator, spaceTSIV, adapting the spaceIV estimator by Pfister and Peters (2022) for two-sample summary statistics. We provide two methods, based on L0- and L1-penalization, respectively. We prove identifiability of the sparse causal effects in the two-sample setting and consistency of spaceTSIV. The performance of spaceTSIV is compared with existing two-sample IV methods in simulations. Finally, we showcase our methods using real proteomic and gene-expression data for drug-target discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12300v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shimeng Huang, Niklas Pfister, Jack Bowden</dc:creator>
    </item>
  </channel>
</rss>
