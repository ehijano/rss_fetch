<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 Apr 2025 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Machine learning accelerates fuel cell life testing</title>
      <link>https://arxiv.org/abs/2504.18835</link>
      <description>arXiv:2504.18835v1 Announce Type: new 
Abstract: Accelerated life testing (ALT) can significantly reduce the economic, time, and labor costs of life testing in the process of equipment, device, and material research and development (R&amp;D), and improve R&amp;D efficiency. This paper proposes a performance characterization data prediction (PCDP) method and a life prediction-driven ALT (LP-ALT) method to accelerate the life test of polymer electrolyte membrane fuel cells (PEMFCs). The PCDP method can accurately predict different PCD using only four impedances (real and imaginary) corresponding to a high frequency and a medium frequency, greatly shortening the measurement time of offline PCD and reducing the difficulty of life testing. The test results on an open source life test dataset containing 42 PEMFCs show that compared with the determination coefficient (R^2) results of predicted aging indicators, including limiting current, total mass transport resistance, and electrochemically active surface area, and crossover current, obtained based on the measured PCD, the R^2 results of predicted aging indicators based on the predicted PCD is only reduced by 0.05, 0.05, 0.06, and 0.06, respectively. The LP-ALT method can shorten the life test time through early life prediction. Test results on the same open-source life test dataset of PEMFCs show that the acceleration ratio of the LP-ALT method can reach 30 times under the premise of ensuring that the minimum R^2 of the prediction results of different aging indicators, including limiting current, total mass transport resistance, and electrochemically active surface area, is not less than 0.9. Combining the different performance characterization data predicted by the PCDP method and the life prediction of the LP-ALT method, the diagnosis and prognosis of PEMFCs and their components can be achieved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18835v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanbin Zhao, Hao Liu, Zhihua Deng, Haoyi Jiang, Zhenfei Ling, Zhiyang Liu, Xingkai Wang, Tong Li, Xiaoping Ouyang</dc:creator>
    </item>
    <item>
      <title>Glider Path Design and Control for Reconstructing Three-Dimensional Structures of Oceanic Mesoscale Eddies</title>
      <link>https://arxiv.org/abs/2504.18936</link>
      <description>arXiv:2504.18936v1 Announce Type: new 
Abstract: Underwater gliders offer effective means in oceanic surveys with a major task in reconstructing the three-dimensional hydrographic field of a mesoscale eddy. This paper considers three key issues in the hydrographic reconstruction of mesoscale eddies with the sampled data from the underwater gliders. It first proposes using the Thin Plate Spline (TPS) as the interpolation method for the reconstruction with a blocking scheme to speed up the computation. It then formulates a procedure for selecting glider path design that minimizes the reconstruction errors among a set of pathway formations. Finally we provide a glider path control procedure to guide the glider to follow to designed pathways as much as possible in the presence of ocean current. A set of optimization algorithms are experimented and several with robust glider control performance on a simulated eddy are identified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18936v1</guid>
      <category>stat.AP</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wu Su, Xiaoyuan E, Zhao Jing, Song Xi Chen</dc:creator>
    </item>
    <item>
      <title>Near-real-time flood inundation monitoring by Bayesian analysis for change point problems for Sentinel-1 time series</title>
      <link>https://arxiv.org/abs/2504.19526</link>
      <description>arXiv:2504.19526v1 Announce Type: new 
Abstract: Near real-time flood monitoring is crucial for disaster response, yet existing methods face significant limitations in training data requirements and cloud cover interference. Here we present a novel approach using Bayesian analysis for change point problems (BCP) applied to Sentinel-1 SAR time series data, which automatically detects temporal discontinuities in backscatter patterns to distinguish flood inundation from permanent water bodies without requiring training data or ancillary information. We validate our method using the UrbanSARFloods benchmark dataset across three diverse geographical contexts (Weihui, China; Jubba, Somalia; and NovaKakhovka, Ukraine). Our BCP approach achieves F1 scores ranging from 0.41 to 0.76 (IoU: 0.25-0.61), significantly outperforming both OTSU thresholding (F1: 0.03-0.12, IoU: 0.02-0.08) and Siamese convolutional neural network approaches (F1: 0.08-0.34, IoU: 0.05-0.24). Further analysis reveals exceptional performance in open areas with F1 scores of 0.47-0.81 (IoU: 0.31-0.68) and high recall (0.36-0.84), contrasted with substantially lower performance in urban areas (F1: 0.00-0.01, IoU: 0.00-0.01), indicating a common challenge across current flood detection methods in urban environments. The proposed method's ability to process raw SAR data directly with minimal preprocessing enables integration into operational early warning systems for rapid flood mapping, particularly in agricultural and open landscapes where it demonstrates the strongest performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19526v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Narumasa Tsutsumida, Tomohiro Tanaka, Nifat Sultana</dc:creator>
    </item>
    <item>
      <title>A Study on the Optimal Design of Isothermal Experiments in Predictive Microbiology</title>
      <link>https://arxiv.org/abs/2504.19541</link>
      <description>arXiv:2504.19541v1 Announce Type: new 
Abstract: This study addresses from the Optimal Experimental Design perspective the use of the isothermal experimentation procedure to precisely estimate the parameters defining models used in predictive microbiology. Starting from a case study set out in the literature, and taking the Baranyi model as the primary model, and the Ratkowsky square-root model as the secondary, D- and c-optimal designs are provided for isothermal experiments, taking the temperature both as a value fixed by the experimenter and as a variable to be designed. The designs calculated show that those commonly used in practice are not efficient enough to estimate the parameters of the secondary model, leading to greater uncertainty in the predictions made via these models. Finally, an analysis is carried out to determine the effect on the efficiency of the possible reduction in the final experimental time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19541v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alba Mu\~noz del R\'io, V\'ictor Casero-Alonso, Mariano Amo-Salas</dc:creator>
    </item>
    <item>
      <title>Bias correction in treatment effect estimates following data-driven biomarker cutoff selection</title>
      <link>https://arxiv.org/abs/2504.19776</link>
      <description>arXiv:2504.19776v1 Announce Type: new 
Abstract: Predictive biomarkers are playing an essential role in precision medicine. Identifying an optimal cutoff to select patient subsets with greater benefit from treatment is critical and more challenging for predictive biomarkers measured with a continuous scale. It is a common practice to perform exploratory subset analysis in early-stage studies to select the cutoff. However, data-driven cutoff selection will often cause bias in treatment effect estimates and lead to over-optimistic expectations in the future phase III trial. In this study, we first conducted extensive simulations to investigate factors influencing the bias, including the cutoff selection rule, the number of candidates cutoffs, the magnitude of the predictive effect, and sample sizes. Our insights emphasize the importance of accounting for bias and uncertainties caused by small sample sizes and data-driven selection procedures in Go/No Go decision-making, and population and sample size determination for phase III studies. Secondly, we evaluated the performance of Bootstrap Bias Correction and the Approximate Bayesian Computation (ABC) method for bias correction through extensive simulations. We conclude by providing a recommendation for the application of the two approaches in clinical practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19776v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chi Zhang, Wei Shi, Spencer Woody, Qing Liu</dc:creator>
    </item>
    <item>
      <title>Marginal expected shortfall: Systemic risk measurement under dependence uncertainty</title>
      <link>https://arxiv.org/abs/2504.19953</link>
      <description>arXiv:2504.19953v1 Announce Type: new 
Abstract: Measuring the contribution of a bank or an insurance company to the overall systemic risk of the market is an important issue, especially in the aftermath of the 2007-2009 financial crisis and the financial downturn of 2020. In this paper, we derive the worst-case and best-case bounds for marginal expected shortfall (MES) -- a key measure of systemic risk contribution -- under the assumption of known marginal distributions for individual companies' risks but an unknown dependence structure. We further derive improved bounds for the MES risk measure when partial information on companies' risk exposures -- and hence their dependence -- is available. To capture this partial information, we utilize three commonly used background risk models: the additive, minimum-based, and multiplicative factor models. Finally, we present an alternative set of improved MES bounds based on a linear regression relationship between individual companies' risks and overall market risk, consistent with the assumptions of the Capital Asset Pricing Model in finance and the Weighted Insurance Pricing Model in insurance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19953v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinghui Chen, Edward Furman, X. Sheldon Lin</dc:creator>
    </item>
    <item>
      <title>Fairness Is More Than Algorithms: Racial Disparities in Time-to-Recidivism</title>
      <link>https://arxiv.org/abs/2504.18629</link>
      <description>arXiv:2504.18629v1 Announce Type: cross 
Abstract: Racial disparities in recidivism remain a persistent challenge within the criminal justice system, increasingly exacerbated by the adoption of algorithmic risk assessment tools. Past works have primarily focused on bias induced by these tools, treating recidivism as a binary outcome. Limited attention has been given to non-algorithmic factors (including socioeconomic ones) in driving racial disparities from a systemic perspective. To that end, this work presents a multi-stage causal framework to investigate the advent and extent of disparities by considering time-to-recidivism rather than a simple binary outcome. The framework captures interactions among races, the algorithm, and contextual factors. This work introduces the notion of counterfactual racial disparity and offers a formal test using survival analysis that can be conducted with observational data to assess if differences in recidivism arise from algorithmic bias, contextual factors, or their interplay. In particular, it is formally established that if sufficient statistical evidence for differences across racial groups is observed, it would support rejecting the null hypothesis that non-algorithmic factors (including socioeconomic ones) do not affect recidivism. An empirical study applying this framework to the COMPAS dataset reveals that short-term recidivism patterns do not exhibit racial disparities when controlling for risk scores. However, statistically significant disparities emerge with longer follow-up periods, particularly for low-risk groups. This suggests that factors beyond algorithmic scores, possibly structural disparities in housing, employment, and social support, may accumulate and exacerbate recidivism risks over time. This underscores the need for policy interventions extending beyond algorithmic improvements to address broader influences on recidivism trajectories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18629v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jessy Xinyi Han, Kristjan Greenewald, Devavrat Shah</dc:creator>
    </item>
    <item>
      <title>Which kind of research papers influence policymaking</title>
      <link>https://arxiv.org/abs/2504.18889</link>
      <description>arXiv:2504.18889v1 Announce Type: cross 
Abstract: This study examines the use of evidence in policymaking by analysing a range of journal and article attributes, as well as online engagement metrics. It employs a large-scale citation analysis of nearly 150,000 articles covering diverse policy topics. The findings highlight that scholarly citations exert the strongest positive influence on policy citations. Articles from journals with a higher citation impact and larger Mendeley readership are cited more frequently in policy documents. Other online engagements, such as news and blog mentions, also boost policy citations, while mentions on social media X have a negative effect. The finding that highly cited and widely read papers are also frequently referenced in policy documents likely reflects the perception among policymakers that such research is more trustworthy. In contrast, papers that derive their influence primarily from social media tend to be cited less often in policy contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18889v1</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo Dorta-Gonz\'alez</dc:creator>
    </item>
    <item>
      <title>Effect of perceived preprint effectiveness and research intensity on posting behaviour</title>
      <link>https://arxiv.org/abs/2504.18896</link>
      <description>arXiv:2504.18896v1 Announce Type: cross 
Abstract: Open science is increasingly recognised worldwide, with preprint posting emerging as a key strategy. This study explores the factors influencing researchers' adoption of preprint publication, particularly the perceived effectiveness of this practice and research intensity indicators such as publication and review frequency. Using open data from a comprehensive survey with 5,873 valid responses, we conducted regression analyses to control for demographic variables. Researchers' productivity, particularly the number of journal articles and books published, greatly influences the frequency of preprint deposits. The perception of the effectiveness of preprints follows this. Preprints are viewed positively in terms of early access to new research, but negatively in terms of early feedback. Demographic variables, such as gender and the type of organisation conducting the research, do not have a significant impact on the production of preprints when other factors are controlled for. However, the researcher's discipline, years of experience and geographical region generally have a moderate effect on the production of preprints. These findings highlight the motivations and barriers associated with preprint publication and provide insights into how researchers perceive the benefits and challenges of this practice within the broader context of open science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18896v1</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo Dorta-Gonz\'alez, Mar\'ia Isabel Dorta-Gonz\'alez</dc:creator>
    </item>
    <item>
      <title>Factor Analysis with Correlated Topic Model for Multi-Modal Data</title>
      <link>https://arxiv.org/abs/2504.18914</link>
      <description>arXiv:2504.18914v1 Announce Type: cross 
Abstract: Integrating various data modalities brings valuable insights into underlying phenomena. Multimodal factor analysis (FA) uncovers shared axes of variation underlying different simple data modalities, where each sample is represented by a vector of features. However, FA is not suited for structured data modalities, such as text or single cell sequencing data, where multiple data points are measured per each sample and exhibit a clustering structure. To overcome this challenge, we introduce FACTM, a novel, multi-view and multi-structure Bayesian model that combines FA with correlated topic modeling and is optimized using variational inference. Additionally, we introduce a method for rotating latent factors to enhance interpretability with respect to binary features. On text and video benchmarks as well as real-world music and COVID-19 datasets, we demonstrate that FACTM outperforms other methods in identifying clusters in structured data, and integrating them with simple modalities via the inference of shared, interpretable factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18914v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ma{\l}gorzata {\L}az\k{e}cka, Ewa Szczurek</dc:creator>
    </item>
    <item>
      <title>Global Climate Model Bias Correction Using Deep Learning</title>
      <link>https://arxiv.org/abs/2504.19145</link>
      <description>arXiv:2504.19145v1 Announce Type: cross 
Abstract: Climate change affects ocean temperature, salinity and sea level, impacting monsoons and ocean productivity. Future projections by Global Climate Models based on shared socioeconomic pathways from the Coupled Model Intercomparison Project (CMIP) are widely used to understand the effects of climate change. However, CMIP models have significant bias compared to reanalysis in the Bay of Bengal for the time period when both projections and reanalysis are available. For example, there is a 1.5C root mean square error (RMSE) in the sea surface temperature (SST) projections of the climate model CNRM-CM6 compared to the Ocean Reanalysis System (ORAS5). We develop a suite of data-driven deep learning models for bias correction of climate model projections and apply it to correct SST projections of the Bay of Bengal. We propose the use of three different deep neural network architectures: convolutional encoder-decoder UNet, Bidirectional LSTM and ConvLSTM. We also use a baseline linear regression model and the Equi-Distant Cumulative Density Function (EDCDF) bias correction method for comparison and evaluating the impact of the new deep learning models. All bias correction models are trained using pairs of monthly CMIP6 projections and the corresponding month's ORAS5 as input and output. Historical data (1950-2014) and future projection data (2015-2020) of CNRM-CM6 are used for training and validation, including hyperparameter tuning. Testing is performed on future projection data from 2021 to 2024. Detailed analysis of the three deep neural models has been completed. We found that the UNet architecture trained using a climatology-removed CNRM-CM6 projection as input and climatology-removed ORAS5 as output gives the best bias-corrected projections. Our novel deep learning-based method for correcting CNRM-CM6 data has a 15% reduction in RMSE compared EDCDF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19145v1</guid>
      <category>physics.ao-ph</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhishek Pasula, Deepak N. Subramani</dc:creator>
    </item>
    <item>
      <title>Selective randomization inference for subgroup effects with continuous biomarkers</title>
      <link>https://arxiv.org/abs/2504.19380</link>
      <description>arXiv:2504.19380v1 Announce Type: cross 
Abstract: Randomization tests are a popular method for testing causal effects in clinical trials with finite-sample validity. In the presence of heterogeneous treatment effects, it is often of interest to select a subgroup that benefits from the treatment, frequently by choosing a cutoff for a continuous biomarker. However, selecting the cutoff and testing the effect on the same data may fail to control the type I error. To address this, we propose using "self-contained" methods for selecting biomarker-based subgroups (cutoffs) and applying conditioning to construct valid randomization tests for the subgroup effect. Compared to sample-splitting-based randomization tests, our proposal is fully deterministic, uses the entire selected subgroup for inference, and is thus more powerful. Moreover, we demonstrate scenarios where our procedure achieves power comparable to a randomization test with oracle knowledge of the benefiting subgroup. In addition, our procedure is as computationally efficient as standard randomization tests. Empirically, we illustrate the effectiveness of our method on simulated datasets and the German Breast Cancer Study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19380v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zijun Gao</dc:creator>
    </item>
    <item>
      <title>Analyzing distortion riskmetrics and weighted entropy for unimodal and symmetric distributions under partial information constraints</title>
      <link>https://arxiv.org/abs/2504.19725</link>
      <description>arXiv:2504.19725v1 Announce Type: cross 
Abstract: In this paper, we develop the lower and upper bounds of worst-case distortion riskmetrics and weighted entropy for unimodal, and symmetric unimodal distributions when mean and variance information are available. We also consider the sharp upper bounds of distortion riskmetrics and weighted entropy for symmetric distribution under known mean and variance. These results are applied to (weighted) entropies, shortfalls and other risk measures. Specifically, entropies include cumulative Tsallis past entropy, cumulative residual Tsallis entropy of order {\alpha}, extended Gini coefficient, fractional generalized cumulative residual entropy, and fractional generalized cumulative entropy. Shortfalls include extended Gini shortfall, Gini shortfall, shortfall of cumulative residual entropy, and shortfall of cumulative residual Tsallis entropy. Other risk measures include nth-order expected shortfall, dual power principle and proportional hazard principle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19725v1</guid>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Baishuai Zuo, Chuancun Yin</dc:creator>
    </item>
    <item>
      <title>SILENT: A New Lens on Statistics in Software Timing Side Channels</title>
      <link>https://arxiv.org/abs/2504.19821</link>
      <description>arXiv:2504.19821v1 Announce Type: cross 
Abstract: Cryptographic research takes software timing side channels seriously. Approaches to mitigate them include constant-time coding and techniques to enforce such practices. However, recent attacks like Meltdown [42], Spectre [37], and Hertzbleed [70] have challenged our understanding of what it means for code to execute in constant time on modern CPUs. To ensure that assumptions on the underlying hardware are correct and to create a complete feedback loop, developers should also perform \emph{timing measurements} as a final validation step to ensure the absence of exploitable side channels. Unfortunately, as highlighted by a recent study by Jancar et al. [30], developers often avoid measurements due to the perceived unreliability of the statistical analysis and its guarantees.
  In this work, we combat the view that statistical techniques only provide weak guarantees by introducing a new algorithm for the analysis of timing measurements with strong, formal statistical guarantees, giving developers a reliable analysis tool. Specifically, our algorithm (1) is non-parametric, making minimal assumptions about the underlying distribution and thus overcoming limitations of classical tests like the t-test, (2) handles unknown data dependencies in measurements, (3) can estimate in advance how many samples are needed to detect a leak of a given size, and (4) allows the definition of a negligible leak threshold $\Delta$, ensuring that acceptable non-exploitable leaks do not trigger false positives, without compromising statistical soundness. We demonstrate the necessity, effectiveness, and benefits of our approach on both synthetic benchmarks and real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19821v1</guid>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Dunsche, Patrick Bastian, Marcel Maehren, Nurullah Erinola, Robert Merget, Nicolai Bissantz, Holger Dette, J\"org Schwenk</dc:creator>
    </item>
    <item>
      <title>The impact of COVID-19 on building energetics</title>
      <link>https://arxiv.org/abs/2504.19921</link>
      <description>arXiv:2504.19921v1 Announce Type: cross 
Abstract: Buildings are responsible for a significant portion of global energy demand and GHG emissions. Using the Massachusetts Institute of Technology campus as a case study, we find that, similar to the baseline metabolism of biological organisms, large buildings are on average $25\%$ more energetically efficient per unit size than smaller buildings. This suggests that institutions can be perceived as super populations with buildings as units (organisms) following standard metabolic relationships. Importantly, the relative efficiency of larger buildings progressively increased to $34\%$ until 2020. However, the COVID-19 pandemic acted as a major shock, disrupting this trend and leading to a reversal to the expected $25\%$ baseline level. This suggests that energetic adaptations are contingent on relatively stable conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19921v1</guid>
      <category>physics.soc-ph</category>
      <category>nlin.AO</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yu-Hsuan Hsu, Sara Beery, Christopher P. Kempes, Mingzhen Lu, Serguei Saavedra</dc:creator>
    </item>
    <item>
      <title>Enhancing short-term traffic prediction by integrating trends and fluctuations with attention mechanism</title>
      <link>https://arxiv.org/abs/2504.19967</link>
      <description>arXiv:2504.19967v1 Announce Type: cross 
Abstract: Traffic flow prediction is a critical component of intelligent transportation systems, yet accurately forecasting traffic remains challenging due to the interaction between long-term trends and short-term fluctuations. Standard deep learning models often struggle with these challenges because their architectures inherently smooth over fine-grained fluctuations while focusing on general trends. This limitation arises from low-pass filtering effects, gate biases favoring stability, and memory update mechanisms that prioritize long-term information retention. To address these shortcomings, this study introduces a hybrid deep learning framework that integrates both long-term trend and short-term fluctuation information using two input features processed in parallel, designed to capture complementary aspects of traffic flow dynamics. Further, our approach leverages attention mechanisms, specifically Bahdanau attention, to selectively focus on critical time steps within traffic data, enhancing the model's ability to predict congestion and other transient phenomena. Experimental results demonstrate that features learned from both branches are complementary, significantly improving the goodness-of-fit statistics across multiple prediction horizons compared to a baseline model. Notably, the attention mechanism enhances short-term forecast accuracy by directly targeting immediate fluctuations, though challenges remain in fully integrating long-term trends. This framework can contribute to more effective congestion mitigation and urban mobility planning by advancing the robustness and precision of traffic prediction models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19967v1</guid>
      <category>cs.ET</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adway Das, Agnimitra Sengupta, S. Ilgin Guler</dc:creator>
    </item>
    <item>
      <title>Spatial von-Mises Fisher Regression for Directional Data</title>
      <link>https://arxiv.org/abs/2207.08321</link>
      <description>arXiv:2207.08321v4 Announce Type: replace 
Abstract: Spatially varying directional data are routinely observed in several modern applications such as meteorology, biology, geophysics, engineering, etc. However, only a few approaches are available for covariate-dependent statistical analysis for such data. To address this gap, we propose a novel generalized linear model to analyze such that using a von Mises Fisher (vMF) distributed error structure. Using a novel link function that relies on the transformation between Cartesian and spherical coordinates, we regress the vMF-distributed directional data on the external covariates. This regression model enables us to quantify the impact of external factors on the observed directional data. Furthermore, we impose the spatial dependence using an autoregressive model, appropriately accounting for the directional dependence in the outcome. This novel specification renders computational efficiency and flexibility. In addition, a comprehensive Bayesian inferential toolbox is thoroughly developed and applied to our analysis. Subsequently, employing our regression model on the Alzheimer's Disease Neuroimaging Initiative (ADNI) data, we gain new insights into the relationship between cognitive impairment and the orientations of brain fibers, along with examining empirical efficacy through simulation experiments. The code for implementing our proposed method is available on GitHub: https://github.com/lanzhouBWH/Spatial_VMF_Regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.08321v4</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhou Lan, Arkaprava Roy</dc:creator>
    </item>
    <item>
      <title>A Compounded Burr Probability Distribution for Fitting Heavy-Tailed Data with Applications to Biological Networks</title>
      <link>https://arxiv.org/abs/2407.04465</link>
      <description>arXiv:2407.04465v3 Announce Type: replace 
Abstract: Complex biological networks, encompassing metabolic pathways, gene regulatory systems, and protein-protein interaction networks, often exhibit scale-free structures characterized by heavy-tailed degree distributions. However, empirical studies reveal significant deviations from ideal power law behavior, underscoring the need for more flexible and accurate probabilistic models. In this work, we propose the Compounded Burr (CBurr) distribution, a novel four parameter family derived by compounding the Burr distribution with a discrete mixing process. This model is specifically designed to capture both the body and tail behavior of real-world network degree distributions with applications to biological networks. We rigorously derive its statistical properties, including moments, hazard and risk functions, and tail behavior, and develop an efficient maximum likelihood estimation framework. The CBurr model demonstrates broad applicability to networks with complex connectivity patterns, particularly in biological, social, and technological domains. Extensive experiments on large-scale biological network datasets show that CBurr consistently outperforms classical power-law, log-normal, and other heavy-tailed models across the full degree spectrum. By providing a statistically grounded and interpretable framework, the CBurr model enhances our ability to characterize the structural heterogeneity of biological networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04465v3</guid>
      <category>stat.AP</category>
      <category>cs.SI</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tanujit Chakraborty, Swarup Chattopadhyay, Suchismita Das, Shraddha M. Naik, Chittaranjan Hens</dc:creator>
    </item>
    <item>
      <title>Unveiling Sleep Dysregulation in Chronic Fatigue Syndrome with and without Fibromyalgia Through Bayesian Networks</title>
      <link>https://arxiv.org/abs/2503.10271</link>
      <description>arXiv:2503.10271v2 Announce Type: replace 
Abstract: Chronic Fatigue Syndrome (CFS) and Fibromyalgia (FM) often co-occur as medically unexplained conditions linked to disrupted physiological regulation, including altered sleep. Building on the work of Kishi et al. (2011), who identified differences in sleep-stage transitions in women with CFS and CFS+FM, we exploited the same strictly controlled clinical cohort using a Bayesian Network (BN) to quantify detailed patterns of sleep and its dynamics. Our BN confirmed that sleep transitions are best described as a second-order process (Yetton et al., 2018), achieving a next-stage predictive accuracy of 70.6%, validated on two independent data sets with domain shifts (60.1-69.8% accuracy). Notably, we demonstrated that sleep dynamics can reveal the actual diagnoses. Our BN successfully differentiated healthy, CFS, and CFS+FM individuals, achieving an AUROC of 75.4%. Using interventions, we quantified sleep alterations attributable specifically to CFS and CFS+FM, identifying changes in stage prevalence, durations, and first- and second-order transitions. These findings reveal novel markers for CFS and CFS+FM in early-to-mid-adulthood women, offering insights into their physiological mechanisms and supporting their clinical differentiation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10271v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michal Bechny, Marco Scutari, Julia van der Meer, Francesca Faraci, St\'ephane Meystre, Benjamin H. Natelson, Akifumi Kishi</dc:creator>
    </item>
    <item>
      <title>Peer-to-Peer Basis Risk Management for Renewable Production Parametric Insurance</title>
      <link>https://arxiv.org/abs/2504.09660</link>
      <description>arXiv:2504.09660v2 Announce Type: replace 
Abstract: The financial viability of renewable energy projects is challenged by the variability and unpredictability of production due to weather fluctuations. This paper proposes a novel risk management framework combining parametric insurance and peer-to-peer (P2P) risk sharing to address production uncertainty in solar electricity generation. We first design a weather-based parametric insurance scheme to protect against forecast errors, recalibrated at the site level to mitigate geographical basis risk. To handle residual mismatches between insurance payouts and actual losses, we introduce a complementary P2P mechanism that redistributes the remaining basis risk among participants. The method leverages physically based simulation models to reconstruct day-ahead forecasts and realized productions, integrating climate data and solar farm characteristics. A second-order theoretical approximation links heterogeneous local models to a shared weather index, making risk sharing operationally feasible. In an empirical application to 50 German solar farms, our approach reduces the volatility of production losses by 55\%, demonstrating its potential to stabilize revenues and strengthen the resilience of renewable investments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09660v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fallou Niakh, Alicia Bassi\`ere, Michel Denuit, Christian Robert</dc:creator>
    </item>
    <item>
      <title>Sliced Elastic Distance for Evaluating Amplitude and Phase Differences in Precipitation Models</title>
      <link>https://arxiv.org/abs/2307.08685</link>
      <description>arXiv:2307.08685v3 Announce Type: replace-cross 
Abstract: Climate model evaluation plays a crucial role in ensuring the accuracy of climatological predictions. However, existing statistical evaluation methods often overlook time misalignment of events in a system's evolution, which can lead to a failure in identifying specific model deficiencies. This issue is particularly relevant for climate variables that involve time-sensitive events such as the monsoon season. To more comprehensively evaluate climate fields, we introduce a new vector-valued metric, the sliced elastic distance, through kernel convolution-derived slices. This metric simultaneously and separately accounts for spatial and temporal variability by decomposing the total distance between model simulations and observational data into three components: amplitude differences, timing variability, and bias (translation). We use the sliced elastic distance to assess CMIP6 precipitation simulations against observational data, evaluating amplitude and phase distances at both global and regional scales. In addition, we conduct a detailed phase analysis of the Indian Summer Monsoon to quantify timing biases in the onset and retreat of the monsoon season across the CMIP6 models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.08685v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert C. Garrett, Trevor Harris, Zhuo Wang, Bo Li</dc:creator>
    </item>
    <item>
      <title>Influence analyses of "designs" for evaluating inconsistency in network meta-analysis</title>
      <link>https://arxiv.org/abs/2406.16485</link>
      <description>arXiv:2406.16485v2 Announce Type: replace-cross 
Abstract: Network meta-analysis is an evidence synthesis method for comparing the effectiveness of multiple available treatments. To justify evidence synthesis, consistency is an important assumption; however, existing methods founded on statistical testing can be substantially limited in statistical power or have several drawbacks when handling multi-arm studies. Moreover, inconsistency can be theoretically explained as design-by-treatment interactions, and the primary purpose of such analyses is to prioritize the further investigation of specific "designs" to explore sources of bias and other issues that might influence the overall results. In this article, we propose an alternative framework for evaluating inconsistency using influence diagnostics methods, which enable the influence of individual designs on the overall results to be quantitatively evaluated. We provide four new methods, the averaged studentized residual, MDFFITS, {\Phi}_d, and {\Xi}_d, to quantify the influence of individual designs through a "leave-one-design-out" analysis framework. We also propose a simple summary measure, the O-value, for prioritizing designs and interpreting these influential analyses in a straightforward manner. Furthermore, we propose another testing approach based on the leave-one-design-out analysis framework. By applying the new methods to a network meta-analysis of antihypertensive drugs and performing simulation studies, we demonstrate that the new methods accurately located potential sources of inconsistency. The proposed methods provide new insights into alternatives to existing test-based methods, especially the quantification of the influence of individual designs on the overall network meta-analysis results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16485v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kotaro Sasaki, Hisashi Noma</dc:creator>
    </item>
    <item>
      <title>GeoConformal prediction: a model-agnostic framework of measuring the uncertainty of spatial prediction</title>
      <link>https://arxiv.org/abs/2412.08661</link>
      <description>arXiv:2412.08661v3 Announce Type: replace-cross 
Abstract: Spatial prediction is a fundamental task in geography. In recent years, with advances in geospatial artificial intelligence (GeoAI), numerous models have been developed to improve the accuracy of geographic variable predictions. Beyond achieving higher accuracy, it is equally important to obtain predictions with uncertainty measures to enhance model credibility and support responsible spatial prediction. Although geostatistic methods like Kriging offer some level of uncertainty assessment, such as Kriging variance, these measurements are not always accurate and lack general applicability to other spatial models. To address this issue, we propose a model-agnostic uncertainty assessment method called GeoConformal Prediction, which incorporates geographical weighting into conformal prediction. We applied it to two classic spatial prediction cases, spatial regression and spatial interpolation, to evaluate its reliability. First, in the spatial regression case, we used XGBoost to predict housing prices, followed by GeoConformal to calculate uncertainty. Our results show that GeoConformal achieved a coverage rate of 93.67%, while Bootstrap methods only reached a maximum coverage of 81.00% after 2000 runs. Next, we applied GeoConformal to spatial interpolation models. We found that the uncertainty obtained from GeoConformal aligned closely with the variance in Kriging. Finally, using GeoConformal, we analyzed the sources of uncertainty in spatial prediction. We found that explicitly including local features in AI models can significantly reduce prediction uncertainty, especially in areas with strong local dependence. Our findings suggest that GeoConformal holds potential not only for geographic knowledge discovery but also for guiding the design of future GeoAI models, paving the way for more reliable and interpretable spatial prediction frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08661v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiayin Lou, Peng Luo, Liqiu Meng</dc:creator>
    </item>
    <item>
      <title>Bayesianize Fuzziness in the Statistical Analysis of Fuzzy Data</title>
      <link>https://arxiv.org/abs/2501.18974</link>
      <description>arXiv:2501.18974v2 Announce Type: replace-cross 
Abstract: Fuzzy data, prevalent in social sciences and other fields, capture uncertainties arising from subjective evaluations and measurement imprecision. Despite significant advancements in fuzzy statistics, a unified inferential regression-based framework remains undeveloped. Hence, we propose a novel approach for analyzing bounded fuzzy variables within a regression framework. Building on the premise that fuzzy data result from a process analogous to statistical coarsening, we introduce a conditional probabilistic approach that links observed fuzzy statistics (e.g., mode, spread) to the underlying, unobserved statistical model, which depends on external covariates. The inferential problem is addressed using Approximate Bayesian methods, mainly through a Gibbs sampler incorporating a quadratic approximation of the posterior distribution. Simulation studies and applications involving external validations are employed to evaluate the effectiveness of the proposed approach for fuzzy data analysis. By reintegrating fuzzy data analysis into a more traditional statistical framework, this work provides a significant step toward enhancing the interpretability and applicability of fuzzy statistical methods in many applicative contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18974v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonio Calcagn\`i, Przemys{\l}aw Grzegorzewski, Maciej Romaniuk</dc:creator>
    </item>
    <item>
      <title>Conformal Calibration: Ensuring the Reliability of Black-Box AI in Wireless Systems</title>
      <link>https://arxiv.org/abs/2504.09310</link>
      <description>arXiv:2504.09310v3 Announce Type: replace-cross 
Abstract: AI is poised to revolutionize telecommunication networks by boosting efficiency, automation, and decision-making. However, the black-box nature of most AI models introduces substantial risk, possibly deterring adoption by network operators. These risks are not addressed by the current prevailing deployment strategy, which typically follows a best-effort train-and-deploy paradigm. This paper reviews conformal calibration, a general framework that moves beyond the state of the art by adopting computationally lightweight, advanced statistical tools that offer formal reliability guarantees without requiring further training or fine-tuning. Conformal calibration encompasses pre-deployment calibration via uncertainty quantification or hyperparameter selection; online monitoring to detect and mitigate failures in real time; and counterfactual post-deployment performance analysis to address "what if" diagnostic questions after deployment. By weaving conformal calibration into the AI model lifecycle, network operators can establish confidence in black-box AI models as a dependable enabling technology for wireless systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09310v3</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Osvaldo Simeone, Sangwoo Park, Matteo Zecchin</dc:creator>
    </item>
  </channel>
</rss>
