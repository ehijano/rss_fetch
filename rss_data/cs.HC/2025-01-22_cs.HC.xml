<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Jan 2025 05:00:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Modeling Changes in Individuals' Cognitive Self-Esteem With and Without Access To Search Tools</title>
      <link>https://arxiv.org/abs/2501.10517</link>
      <description>arXiv:2501.10517v1 Announce Type: new 
Abstract: Search engines, as cognitive partners, reshape how individuals evaluate their cognitive abilities. This study examines how search tool access influences cognitive self-esteem (CSE)-users' self-perception of cognitive abilities -- through the lens of transactive memory systems. Using a within-subject design with 164 participants, we found that CSE significantly inflates when users have access to search tools, driven by cognitive offloading. Participants with lower initial CSE exhibited greater shifts, highlighting individual differences. Search self-efficacy mediated the relationship between prior search experience and CSE, emphasizing the role of users' past interactions. These findings reveal opportunities for search engine design: interfaces that promote awareness of cognitive offloading and foster self-reflection can support accurate metacognitive evaluations, reducing overreliance on external tools. This research contributes to HCI by demonstrating how interactive systems shape cognitive self-perception, offering actionable insights for designing human-centered tools that balance user confidence and cognitive independence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10517v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahir Akgun, Sacip Toker</dc:creator>
    </item>
    <item>
      <title>An Empirical Study to Understand How Students Use ChatGPT for Writing Essays</title>
      <link>https://arxiv.org/abs/2501.10551</link>
      <description>arXiv:2501.10551v1 Announce Type: new 
Abstract: As large language models (LLMs) advance and become widespread, students increasingly turn to systems like ChatGPT for assistance with writing tasks. Educators are concerned with students' usage of ChatGPT beyond cheating; using ChatGPT may reduce their critical engagement with writing, hindering students' learning processes. The negative or positive impact of using LLM-powered tools for writing will depend on how students use them; however, how students use ChatGPT remains largely unknown, resulting in a limited understanding of its impact on learning. To better understand how students use these tools, we conducted an online study $(n=70)$ where students were given an essay-writing task using a custom platform we developed to capture the queries they made to ChatGPT. To characterize their ChatGPT usage, we categorized each of the queries students made to ChatGPT. We then analyzed the relationship between ChatGPT usage and a variety of other metrics, including students' self-perception, attitudes towards AI, and the resulting essay itself. We found that factors such as gender, race, and perceived self-efficacy can help predict different AI usage patterns. Additionally, we found that different usage patterns were associated with varying levels of enjoyment and perceived ownership over the essay. The results of this study contribute to discussions about how writing education should incorporate generative AI-powered tools in the classroom.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10551v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Jelson, Daniel Manesh, Alice Jang, Daniel Dunlap, Sang Won Lee</dc:creator>
    </item>
    <item>
      <title>Observe, Ask, Intervene: Designing AI Agents for More Inclusive Meetings</title>
      <link>https://arxiv.org/abs/2501.10553</link>
      <description>arXiv:2501.10553v1 Announce Type: new 
Abstract: Video conferencing meetings are more effective when they are inclusive, but inclusion often hinges on meeting leaders' and/or co-facilitators' practices. AI systems can be designed to improve meeting inclusion at scale by moderating negative meeting behaviors and supporting meeting leaders. We explored this design space by conducting $9$ user-centered ideation sessions, instantiating design insights in a prototype ``virtual co-host'' system, and testing the system in a formative exploratory lab study ($n=68$ across $12$ groups, $18$ interviews). We found that ideation session participants wanted AI agents to ask questions before intervening, which we formalized as the ``Observe, Ask, Intervene'' (OAI) framework. Participants who used our prototype preferred OAI over fully autonomous intervention, but rationalized away the virtual co-host's critical feedback. From these findings, we derive guidelines for designing AI agents to influence behavior and mediate group work. We also contribute methodological and design guidelines specific to mitigating inequitable meeting participation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10553v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713838</arxiv:DOI>
      <dc:creator>Mo Houtti, Moyan Zhou, Loren Terveen, Stevie Chancellor</dc:creator>
    </item>
    <item>
      <title>Identifying the Desired Word Suggestion in Simultaneous Audio</title>
      <link>https://arxiv.org/abs/2501.10568</link>
      <description>arXiv:2501.10568v1 Announce Type: new 
Abstract: We explore a method for presenting word suggestions for non-visual text input using simultaneous voices. We conduct two perceptual studies and investigate the impact of different presentations of voices on a user's ability to detect which voice, if any, spoke their desired word. Our sets of words simulated the word suggestions of a predictive keyboard during real-world text input. We find that when voices are simultaneous, user accuracy decreases significantly with each added word suggestion. However, adding a slight 0.15 s delay between the start of each subsequent word allows two simultaneous words to be presented with no significant decrease in accuracy compared to presenting two words sequentially (84% simultaneous versus 86% sequential). This allows two word suggestions to be presented to the user 32% faster than sequential playback without decreasing accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10568v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dylan Gaines, Keith Vertanen</dc:creator>
    </item>
    <item>
      <title>Algorithmic Derivation of Human Spatial Navigation Indices From Eye Movement Data</title>
      <link>https://arxiv.org/abs/2501.10696</link>
      <description>arXiv:2501.10696v1 Announce Type: new 
Abstract: Spatial navigation is a complex cognitive function involving sensory inputs, such as visual, auditory, and proprioceptive information, to understand and move within space. This ability allows humans to create mental maps, navigate through environments, and process directional cues, crucial for exploring new places and finding one's way in unfamiliar surroundings. This study takes an algorithmic approach to extract indices relevant to human spatial navigation using eye movement data. Leveraging electrooculography signals, we analyzed statistical features and applied feature engineering techniques to study eye movements during navigation tasks. The proposed work combines signal processing and machine learning approaches to develop indices for navigation and orientation, spatial anxiety, landmark recognition, path survey, and path route. The analysis yielded five subscore indices with notable accuracy. Among these, the navigation and orientation subscore achieved an R2 score of 0.72, while the landmark recognition subscore attained an R2 score of 0.50. Additionally, statistical features highly correlated with eye movement metrics, including blinks, saccades, and fixations, were identified. The findings of this study can lead to more cognitive assessments and enable early detection of spatial navigation impairments, particularly among individuals at risk of cognitive decline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10696v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sobhan Teymouri, Fatemeh Alizadehziri, Mobina Zibandehpoor, Mehdi Delrobaei</dc:creator>
    </item>
    <item>
      <title>Human-like Nonverbal Behavior with MetaHumans in Real-World Interaction Studies: An Architecture Using Generative Methods and Motion Capture</title>
      <link>https://arxiv.org/abs/2501.10713</link>
      <description>arXiv:2501.10713v1 Announce Type: new 
Abstract: Socially interactive agents are gaining prominence in domains like healthcare, education, and service contexts, particularly virtual agents due to their inherent scalability. To facilitate authentic interactions, these systems require verbal and nonverbal communication through e.g., facial expressions and gestures. While natural language processing technologies have rapidly advanced, incorporating human-like nonverbal behavior into real-world interaction contexts is crucial for enhancing the success of communication, yet this area remains underexplored. One barrier is creating autonomous systems with sophisticated conversational abilities that integrate human-like nonverbal behavior. This paper presents a distributed architecture using Epic Games MetaHuman, combined with advanced conversational AI and camera-based user management, that supports methods like motion capture, handcrafted animation, and generative approaches for nonverbal behavior. We share insights into a system architecture designed to investigate nonverbal behavior in socially interactive agents, deployed in a three-week field study in the Deutsches Museum Bonn, showcasing its potential in realistic nonverbal behavior research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10713v1</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oliver Chojnowski, Alexander Eberhard, Michael Schiffmann, Ana M\"uller, Anja Richert</dc:creator>
    </item>
    <item>
      <title>Improving External Communication of Automated Vehicles Using Bayesian Optimization</title>
      <link>https://arxiv.org/abs/2501.10792</link>
      <description>arXiv:2501.10792v1 Announce Type: new 
Abstract: The absence of a human operator in automated vehicles (AVs) may require external Human-Machine Interfaces (eHMIs) to facilitate communication with other road users in uncertain scenarios, for example, regarding the right of way. Given the plethora of adjustable parameters, balancing visual and auditory elements is crucial for effective communication with other road users. With N=37 participants, this study employed multi-objective Bayesian optimization to enhance eHMI designs and improve trust, safety perception, and mental demand. By reporting the Pareto front, we identify optimal design trade-offs. This research contributes to the ongoing standardization efforts of eHMIs, supporting broader adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10792v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3714187</arxiv:DOI>
      <dc:creator>Mark Colley, Pascal Jansen, Mugdha Keskar, Enrico Rukzio</dc:creator>
    </item>
    <item>
      <title>"Auntie, Please Don't Fall for Those Smooth Talkers": How Chinese Younger Family Members Safeguard Seniors from Online Fraud</title>
      <link>https://arxiv.org/abs/2501.10803</link>
      <description>arXiv:2501.10803v1 Announce Type: new 
Abstract: Online fraud substantially harms individuals and seniors are disproportionately targeted. While family is crucial for seniors, little research has empirically examined how they protect seniors against fraud. To address this gap, we employed an inductive thematic analysis of 124 posts and 16,872 comments on RedNote (Xiaohongshu), exploring the family support ecosystem for senior-targeted online fraud in China. We develop a taxonomy of senior-targeted online fraud from a familial perspective, revealing younger members often spot frauds hard for seniors to detect, such as unusual charges. Younger family members fulfill multiple safeguarding roles, including preventative measures, fraud identification, fraud persuasion, loss recovery, and education. They also encounter numerous challenges, such as seniors' refusal of help and considerable mental and financial stress. Drawing on these, we develop a conceptual framework to characterize family support in senior-targeted fraud, and outline implications for researchers and practitioners to consider the broader stakeholder ecosystem and cultural aspects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10803v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yue Deng, Changyang He, Yixin Zou, Bo Li</dc:creator>
    </item>
    <item>
      <title>A Survey on Conceptual model of Enterprise ontology</title>
      <link>https://arxiv.org/abs/2501.10847</link>
      <description>arXiv:2501.10847v1 Announce Type: new 
Abstract: Enterprise ontology serves as a foundational framework for semantically comprehending the nature of organizations and the essential components that uphold their integrity. The systematic and conceptual understanding of organizations has garnered significant attention from researchers due to its pivotal role in various domains, including business modeling, enterprise architecture, business process management, context-aware systems, application development, interoperability across diverse systems and platforms, knowledge management, organizational learning and innovation, and conflict resolution within organizations. Achieving a consensus on the concepts related to the fundamental elements that constitute an organization is therefore critical. This study aims to conduct a comprehensive analysis and comparison of existing conceptual models of enterprises as documented in scholarly articles published over the past decade. We discuss the strengths and weaknesses of each model and introduce a robust framework for their evaluation. To facilitate this evaluation, we propose several pertinent criteria derived from established methodologies for assessing ontologies. Furthermore, we identify contemporary challenges and issues that have been overlooked in prior studies, offering insights and suggestions for future research directions in enterprise modeling. This article ultimately presents a roadmap for enhancing the systematic understanding of organizations through refined enterprise ontology frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10847v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zeinab Rajabi, Seyed Mohsen Rahnamafard</dc:creator>
    </item>
    <item>
      <title>SMARTe-VR: Student Monitoring and Adaptive Response Technology for e-learning in Virtual Reality</title>
      <link>https://arxiv.org/abs/2501.10977</link>
      <description>arXiv:2501.10977v1 Announce Type: new 
Abstract: This work introduces SMARTe-VR, a platform for student monitoring in an immersive virtual reality environment designed for online education. SMARTe-VR is aimed to gather data for adaptive learning, focusing on facial biometrics and learning metadata. The platform allows instructors to create tailored learning sessions with video lectures, featuring an interface with an Auto QA system to evaluate understanding, interaction tools (e.g., textbook highlighting and lecture tagging), and real-time feedback. Additionally, we release a dataset containing 5 research challenges with data from 10 users in VR-based TOEIC sessions. This dataset, spanning over 25 hours, includes facial features, learning metadata, 450 responses, question difficulty levels, concept tags, and understanding labels. Alongside the database, we present preliminary experiments using Item Response Theory models, adapted for understanding detection using facial features. Two architectures were explored: a Temporal Convolutional Network for local features and a Multilayer Perceptron for global features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10977v1</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberto Daza, Lin Shengkai, Aythami Morales, Julian Fierrez, Katashi Nagao</dc:creator>
    </item>
    <item>
      <title>One Does Not Simply Meme Alone: Evaluating Co-Creativity Between LLMs and Humans in the Generation of Humor</title>
      <link>https://arxiv.org/abs/2501.11433</link>
      <description>arXiv:2501.11433v1 Announce Type: new 
Abstract: Collaboration has been shown to enhance creativity leading to more innovative and effective outcomes While previous research has explored the abilities of Large Language Models LLMs to serve as cocreative partners in tasks like writing poetry or creating narratives the collaborative potential of LLMs in humorrich and culturally nuanced domains remains an open question To address this gap we conducted a user study to explore the potential of LLMs in cocreating memesa humordriven and culturally specific form of creative expression We conducted a user study with three groups of 50 participants each a humanonly group creating memes without AI assistance a humanAI collaboration group interacting with a stateoftheart LLM model and an AIonly group where the LLM autonomously generated memes We assessed the quality of the generated memes through crowdsourcing with each meme rated on creativity humor and shareability Our results showed that LLM assistance increased the number of ideas generated and reduced the effort participants felt However it did not improve the quality of the memes when humans were collaborated with LLM Interestingly memes created entirely by AI performed better than both humanonly and humanAI collaborative memes in all areas on average However when looking at the topperforming memes humancreated ones were better in humor while humanAI collaborations stood out in creativity and shareability These findings highlight the complexities of humanAI collaboration in creative tasks While AI can boost productivity and create content that appeals to a broad audience human creativity remains crucial for content that connects on a deeper level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11433v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3708359.3712094</arxiv:DOI>
      <dc:creator>Zhikun Wu (KTH Royal Institute of Technology), Thomas Weber (LMU Munich), Florian M\"uller (TU Darmstadt)</dc:creator>
    </item>
    <item>
      <title>Governance of Generative AI in Creative Work: Consent, Credit, Compensation, and Beyond</title>
      <link>https://arxiv.org/abs/2501.11457</link>
      <description>arXiv:2501.11457v1 Announce Type: new 
Abstract: Since the emergence of generative AI, creative workers have spoken up about the career-based harms they have experienced arising from this new technology. A common theme in these accounts of harm is that generative AI models are trained on workers' creative output without their consent and without giving credit or compensation to the original creators.
  This paper reports findings from 20 interviews with creative workers in three domains: visual art and design, writing, and programming. We investigate the gaps between current AI governance strategies, what creative workers want out of generative AI governance, and the nuanced role of creative workers' consent, compensation and credit for training AI models on their work. Finally, we make recommendations for how generative AI can be governed and how operators of generative AI systems might more ethically train models on creative output in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11457v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lin Kyi, Amruta Mahuli, M. Six Silberman, Reuben Binns, Jun Zhao, Asia J. Biega</dc:creator>
    </item>
    <item>
      <title>A Hands-free Spatial Selection and Interaction Technique using Gaze and Blink Input with Blink Prediction for Extended Reality</title>
      <link>https://arxiv.org/abs/2501.11540</link>
      <description>arXiv:2501.11540v1 Announce Type: new 
Abstract: Gaze-based interaction techniques have created significant interest in the field of spatial interaction. Many of these methods require additional input modalities, such as hand gestures (e.g., gaze coupled with pinch). Those can be uncomfortable and difficult to perform in public or limited spaces, and pose challenges for users who are unable to execute pinch gestures. To address these aspects, we propose a novel, hands-free Gaze+Blink interaction technique that leverages the user's gaze and intentional eye blinks. This technique enables users to perform selections by executing intentional blinks. It facilitates continuous interactions, such as scrolling or drag-and-drop, through eye blinks coupled with head movements. So far, this concept has not been explored for hands-free spatial interaction techniques. We evaluated the performance and user experience (UX) of our Gaze+Blink method with two user studies and compared it with Gaze+Pinch in a realistic user interface setup featuring common menu interaction tasks. Study 1 demonstrated that while Gaze+Blink achieved comparable selection speeds, it was prone to accidental selections resulting from unintentional blinks. In Study 2 we explored an enhanced technique employing a deep learning algorithms for filtering out unintentional blinks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11540v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim Rolff, Jenny Gabel, Lauren Zerbin, Niklas Hypki, Susanne Schmidt, Markus Lappe, Frank Steinicke</dc:creator>
    </item>
    <item>
      <title>The Data-Expectation Gap: A Vocabulary Describing Experiential Qualities of Data Inaccuracies in Smartwatches</title>
      <link>https://arxiv.org/abs/2501.11556</link>
      <description>arXiv:2501.11556v1 Announce Type: new 
Abstract: Many users of wrist-worn wearable fitness trackers encounter the data-expectation gap - mismatches between data and expectations. While we know such discrepancies exist, we are no closer to designing technologies that can address their negative effects. This is largely because encounters with mismatches are typically treated unidimensionally, while they may differ in context and implications. This treatment does not allow the design of human-data interaction (HDI) mechanisms accounting for temporal, social, emotional, and other factors potentially influencing the perception of mismatches. To address this problem, we present a vocabulary that describes the breadth and context-bound character of encounters with the data-expectation gap, drawing from findings from two studies. Our work contributes to Personal Informatics research providing knowledge on how encounters with the data-expectation gap are embedded in people's daily lives, and a vocabulary encapsulating this knowledge, which can be used when designing HDI experiences in wearable fitness trackers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11556v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dimitra Dritsa, Steven Houben</dc:creator>
    </item>
    <item>
      <title>Spatial Bar: Exploring Window Switching Techniques for Large Virtual Displays</title>
      <link>https://arxiv.org/abs/2501.11754</link>
      <description>arXiv:2501.11754v1 Announce Type: new 
Abstract: Virtual displays provided through head-worn displays (HWDs) offer users large screen space for productivity, but managing this space effectively presents challenges. This paper explores how to enhance window-switching strategies for virtual displays by leveraging eye tracking provided by HWDs and underutilized spaces around the main display area. We investigate the efficiency and usability of different cursor behaviors and selection modes in a Spatial Bar interface for window-switching tasks in augmented reality environments. Results show gaze coupled with teleport led to the quickest window-switching times, particularly in tasks where the original cursor position or the target window was far from the Spatial Bar.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11754v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonardo Pavanatto, Jens Grubert, Doug Bowman</dc:creator>
    </item>
    <item>
      <title>Everyone's Privacy Matters! An Analysis of Privacy Leakage from Real-World Facial Images on Twitter and Associated User Behaviors</title>
      <link>https://arxiv.org/abs/2501.11756</link>
      <description>arXiv:2501.11756v1 Announce Type: new 
Abstract: Online users often post facial images of themselves and other people on online social networks (OSNs) and other Web 2.0 platforms, which can lead to potential privacy leakage of people whose faces are included in such images. There is limited research on understanding face privacy in social media while considering user behavior. It is crucial to consider privacy of subjects and bystanders separately. This calls for the development of privacy-aware face detection classifiers that can distinguish between subjects and bystanders automatically. This paper introduces such a classifier trained on face-based features, which outperforms the two state-of-the-art methods with a significant margin (by 13.1% and 3.1% for OSN images, and by 17.9% and 5.9% for non-OSN images). We developed a semi-automated framework for conducting a large-scale analysis of the face privacy problem by using our novel bystander-subject classifier. We collected 27,800 images, each including at least one face, shared by 6,423 Twitter users. We then applied our framework to analyze this dataset thoroughly. Our analysis reveals eight key findings of different aspects of Twitter users' real-world behaviors on face privacy, and we provide quantitative and qualitative results to better explain these findings. We share the practical implications of our study to empower online platforms and users in addressing the face privacy problem efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11756v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3710967</arxiv:DOI>
      <dc:creator>Yuqi Niu, Weidong Qiu, Peng Tang, Lifan Wang, Shuo Chen, Shujun Li, Nadin Kokciyan, Ben Niu</dc:creator>
    </item>
    <item>
      <title>Human-AI Collaborative Game Testing with Vision Language Models</title>
      <link>https://arxiv.org/abs/2501.11782</link>
      <description>arXiv:2501.11782v1 Announce Type: new 
Abstract: As modern video games become increasingly complex, traditional manual testing methods are proving costly and inefficient, limiting the ability to ensure high-quality game experiences. While advancements in Artificial Intelligence (AI) offer the potential to assist human testers, the effectiveness of AI in truly enhancing real-world human performance remains underexplored. This study investigates how AI can improve game testing by developing and experimenting with an AI-assisted workflow that leverages state-of-the-art machine learning models for defect detection. Through an experiment involving 800 test cases and 276 participants of varying backgrounds, we evaluate the effectiveness of AI assistance under four conditions: with or without AI support, and with or without detailed knowledge of defects and design documentation. The results indicate that AI assistance significantly improves defect identification performance, particularly when paired with detailed knowledge. However, challenges arise when AI errors occur, negatively impacting human decision-making. Our findings show the importance of optimizing human-AI collaboration and implementing strategies to mitigate the effects of AI inaccuracies. By this research, we demonstrate AI's potential and problems in enhancing efficiency and accuracy in game testing workflows and offers practical insights for integrating AI into the testing process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11782v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Boran Zhang, Muhan Xu, Zhijun Pan</dc:creator>
    </item>
    <item>
      <title>How Developers Choose Debugging Strategies for Challenging Web Application Defects</title>
      <link>https://arxiv.org/abs/2501.11792</link>
      <description>arXiv:2501.11792v1 Announce Type: new 
Abstract: Effective debugging is a crucial aspect of software development, demanding problem-solving skills, expertise, and appropriate tools. Although previous research has studied expert developers' debugging strategies, the specific factors influencing strategy choice in complex scenarios remain underexplored. To investigate these contextual factors, we conducted two studies. First, we surveyed 35 developers to identify experiences with challenging debugging problems and contextual complexities. Second, we held semi-structured interviews with 16 experienced developers to gain deeper insight into strategic reasoning for complex debugging tasks. Insights from both groups enriched our understanding of debugging strategies at different expertise levels. We found that contextual factors interact in complex ways, and combinations of factors influence strategy choice, evolving throughout the debugging process. Hypothesis making is the baseline for debugging, with experience and code familiarity crucial for strategy selection. Our results show a gap between learning and effectively practicing strategies in challenging contexts, highlighting the need for carefully designed debugging tools and educational frameworks that align with problem contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11792v1</guid>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maryam Arab, Jenny T. Liang, Valentina Hong, Thomas D. LaToza</dc:creator>
    </item>
    <item>
      <title>Light My Way: Developing and Exploring a Multimodal Interface to Assist People With Visual Impairments to Exit Highly Automated Vehicles</title>
      <link>https://arxiv.org/abs/2501.11801</link>
      <description>arXiv:2501.11801v1 Announce Type: new 
Abstract: The introduction of Highly Automated Vehicles (HAVs) has the potential to increase the independence of blind and visually impaired people (BVIPs). However, ensuring safety and situation awareness when exiting these vehicles in unfamiliar environments remains challenging. To address this, we conducted an interactive workshop with N=5 BVIPs to identify their information needs when exiting an HAV and evaluated three prior-developed low-fidelity prototypes. The insights from this workshop guided the development of PathFinder, a multimodal interface combining visual, auditory, and tactile modalities tailored to BVIP's unique needs. In a three-factorial within-between-subject study with N=16 BVIPs, we evaluated PathFinder against an auditory-only baseline in urban and rural scenarios. PathFinder significantly reduced mental demand and maintained high perceived safety in both scenarios, while the auditory baseline led to lower perceived safety in the urban scenario compared to the rural one. Qualitative feedback further supported PathFinder's effectiveness in providing spatial orientation during exiting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11801v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713454</arxiv:DOI>
      <dc:creator>Luca-Maxim Meinhardt, Lina Wilke, Maryam Elhaidary, Julia von Abel, Paul Fink, Michael Rietzler, Mark Colley, Enrico Rukzio</dc:creator>
    </item>
    <item>
      <title>Automating High Quality RT Planning at Scale</title>
      <link>https://arxiv.org/abs/2501.11803</link>
      <description>arXiv:2501.11803v1 Announce Type: new 
Abstract: Radiotherapy (RT) planning is complex, subjective, and time-intensive. Advances in artificial intelligence (AI) promise to improve its precision, efficiency, and consistency, but progress is often limited by the scarcity of large, standardized datasets. To address this, we introduce the Automated Iterative RT Planning (AIRTP) system, a scalable solution for generating high-quality treatment plans. This scalable solution is designed to generate substantial volumes of consistently high-quality treatment plans, overcoming a key obstacle in the advancement of AI-driven RT planning. Our AIRTP pipeline adheres to clinical guidelines and automates essential steps, including organ-at-risk (OAR) contouring, helper structure creation, beam setup, optimization, and plan quality improvement, using AI integrated with RT planning software like Eclipse of Varian. Furthermore, a novel approach for determining optimization parameters to reproduce 3D dose distributions, i.e. a method to convert dose predictions to deliverable treatment plans constrained by machine limitations. A comparative analysis of plan quality reveals that our automated pipeline produces treatment plans of quality comparable to those generated manually, which traditionally require several hours of labor per plan. Committed to public research, the first data release of our AIRTP pipeline includes nine cohorts covering head-and-neck and lung cancer sites to support an AAPM 2025 challenge. This data set features more than 10 times the number of plans compared to the largest existing well-curated public data set to our best knowledge. Repo:{https://github.com/RiqiangGao/GDP-HMM_AAPMChallenge}</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11803v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Riqiang Gao, Mamadou Diallo, Han Liu, Anthony Magliari, Jonathan Sackett, Wilko Verbakel, Sandra Meyers, Masoud Zarepisheh, Rafe Mcbeth, Simon Arberet, Martin Kraus, Florin C. Ghesu, Ali Kamen</dc:creator>
    </item>
    <item>
      <title>Scrolling in the Deep: Analysing Contextual Influences on Intervention Effectiveness during Infinite Scrolling on Social Media</title>
      <link>https://arxiv.org/abs/2501.11814</link>
      <description>arXiv:2501.11814v1 Announce Type: new 
Abstract: Infinite scrolling on social media platforms is designed to encourage prolonged engagement, leading users to spend more time than desired, which can provoke negative emotions. Interventions to mitigate infinite scrolling have shown initial success, yet users become desensitized due to the lack of contextual relevance. Understanding how contextual factors influence intervention effectiveness remains underexplored. We conducted a 7-day user study (N=72) investigating how these contextual factors affect users' reactance and responsiveness to interventions during infinite scrolling. Our study revealed an interplay, with contextual factors such as being at home, sleepiness, and valence playing significant roles in the intervention's effectiveness. Low valence coupled with being at home slows down the responsiveness to interventions, and sleepiness lowers reactance towards interventions, increasing user acceptance of the intervention. Overall, our work contributes to a deeper understanding of user responses toward interventions and paves the way for developing more effective interventions during infinite scrolling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11814v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713187</arxiv:DOI>
      <dc:creator>Luca-Maxim Meinhardt, Maryam Elhaidary, Mark Colley, Michael Rietzler, Jan Ole Rixen, Aditya Kumar Purohit, Enrico Rukzio</dc:creator>
    </item>
    <item>
      <title>Fly Away: Evaluating the Impact of Motion Fidelity on Optimized User Interface Design via Bayesian Optimization in Automated Urban Air Mobility Simulations</title>
      <link>https://arxiv.org/abs/2501.11829</link>
      <description>arXiv:2501.11829v1 Announce Type: new 
Abstract: Automated Urban Air Mobility (UAM) can improve passenger transportation and reduce congestion, but its success depends on passenger trust. While initial research addresses passengers' information needs, questions remain about how to simulate air taxi flights and how these simulations impact users and interface requirements. We conducted a between-subjects study (N=40), examining the influence of motion fidelity in Virtual-Reality-simulated air taxi flights on user effects and interface design. Our study compared simulations with and without motion cues using a 3-Degrees-of-Freedom motion chair. Optimizing the interface design across six objectives, such as trust and mental demand, we used multi-objective Bayesian optimization to determine the most effective design trade-offs. Our results indicate that motion fidelity decreases users' trust, understanding, and acceptance, highlighting the need to consider motion fidelity in future UAM studies to approach realism. However, minimal evidence was found for differences or equality in the optimized interface designs, suggesting personalized interface designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11829v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713288</arxiv:DOI>
      <dc:creator>Luca-Maxim Meinhardt, Clara Schramm, Pascal Jansen, Mark Colley, Enrico Rukzio</dc:creator>
    </item>
    <item>
      <title>Large Language Models with Human-In-The-Loop Validation for Systematic Review Data Extraction</title>
      <link>https://arxiv.org/abs/2501.11840</link>
      <description>arXiv:2501.11840v1 Announce Type: new 
Abstract: Systematic reviews are time-consuming endeavors. Historically speaking, knowledgeable humans have had to screen and extract data from studies before it can be analyzed. However, large language models (LLMs) hold promise to greatly accelerate this process. After a pilot study which showed great promise, we investigated the use of freely available LLMs for extracting data for systematic reviews. Using three different LLMs, we extracted 24 types of data, 9 explicitly stated variables and 15 derived categorical variables, from 112 studies that were included in a published scoping review. Overall we found that Gemini 1.5 Flash, Gemini 1.5 Pro, and Mistral Large 2 performed reasonably well, with 71.17%, 72.14%, and 62.43% of data extracted being consistent with human coding, respectively. While promising, these results highlight the dire need for a human-in-the-loop (HIL) process for AI-assisted data extraction. As a result, we present a free, open-source program we developed (AIDE) to facilitate user-friendly, HIL data extraction with LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11840v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noah L. Schroeder, Chris Davis Jaldi, Shan Zhang</dc:creator>
    </item>
    <item>
      <title>Webvs. LLMs: An Empirical Study of Learning Behaviors of CS2 Students</title>
      <link>https://arxiv.org/abs/2501.11935</link>
      <description>arXiv:2501.11935v1 Announce Type: new 
Abstract: LLMs such as ChatGPT have been widely adopted by students in higher education as tools for learning programming and related concepts. However, it remains unclear how effective students are and what strategies students use while learning with LLMs. Since the majority of students' experiences in online self-learning have come through using search engines such as Google, evaluating AI tools in this context can help us address these gaps. In this mixed methods research, we conducted an exploratory within-subjects study to understand how CS2 students learn programming concepts using both LLMs as well as traditional online methods such as educational websites and videos to examine how students approach learning within and across both scenarios. We discovered that students found it easier to learn a more difficult concept using traditional methods than using ChatGPT. We also found that students ask fewer follow-ups and use more keyword-based queries for search engines while their prompts to LLMs tend to explicitly ask for information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11935v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aayush Kumar, Daniel Prol, Amin Alipour, Sruti Srinivasa Ragavan</dc:creator>
    </item>
    <item>
      <title>Conversation Progress Guide : UI System for Enhancing Self-Efficacy in Conversational AI</title>
      <link>https://arxiv.org/abs/2501.12001</link>
      <description>arXiv:2501.12001v1 Announce Type: new 
Abstract: In this study, we introduce the Conversation Progress Guide (CPG), a system designed for text-based conversational AI interactions that provides a visual interface to represent progress. Users often encounter failures when interacting with conversational AI, which can negatively affect their self-efficacy-an individual's belief in their capabilities, reducing their willingness to engage with these services. The CPG offers visual feedback on task progress, providing users with mastery experiences, a key source of self-efficacy. To evaluate the system's effectiveness, we conducted a user study assessing how the integration of the CPG influences user engagement and self-efficacy. Results demonstrate that users interacting with a conversational AI enhanced by the CPG showed significant improvements in self-efficacy measures compared to those using a conventional conversational AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12001v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daeun Jeong, Sungbok Shin, Jongwook Jeong</dc:creator>
    </item>
    <item>
      <title>Contextualizing Recommendation Explanations with LLMs: A User Study</title>
      <link>https://arxiv.org/abs/2501.12152</link>
      <description>arXiv:2501.12152v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly prevalent in recommender systems, where LLMs can be used to generate personalized recommendations. Here, we examine how different LLM-generated explanations for movie recommendations affect users' perceptions of cognitive, affective, and utilitarian needs and consumption intentions. In a pre-registered, between-subject online experiment (N=759) and follow-up interviews (N=30), we compare (a) LLM-generated generic explanations, and (b) LLM-generated contextualized explanations. Our findings show that contextualized explanations (i.e., explanations that incorporate users' past behaviors) effectively meet users' cognitive needs while increasing users' intentions to watch recommended movies. However, adding explanations offers limited benefits in meeting users' utilitarian and affective needs, raising concerns about the proper design and implications of LLM-generated explanations. Qualitative insights from interviews reveal that referencing users' past preferences enhances trust and understanding but can feel excessive if overused. Furthermore, users with more active and positive engagement with the recommender system and movie-watching get substantial gains from contextualized explanations. Overall, our research clarifies how LLM-generated recommendations influence users' motivations and behaviors, providing valuable insights for the future development of user-centric recommender systems, a key element in social media platforms and online ecosystems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12152v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanjun Feng, Stefan Feuerriegel, Yash Raj Shrestha</dc:creator>
    </item>
    <item>
      <title>LLM-Assisted Knowledge Graph Completion for Curriculum and Domain Modelling in Personalized Higher Education Recommendations</title>
      <link>https://arxiv.org/abs/2501.12300</link>
      <description>arXiv:2501.12300v1 Announce Type: new 
Abstract: While learning personalization offers great potential for learners, modern practices in higher education require a deeper consideration of domain models and learning contexts, to develop effective personalization algorithms. This paper introduces an innovative approach to higher education curriculum modelling that utilizes large language models (LLMs) for knowledge graph (KG) completion, with the goal of creating personalized learning-path recommendations. Our research focuses on modelling university subjects and linking their topics to corresponding domain models, enabling the integration of learning modules from different faculties and institutions in the student's learning path. Central to our approach is a collaborative process, where LLMs assist human experts in extracting high-quality, fine-grained topics from lecture materials. We develop a domain, curriculum, and user models for university modules and stakeholders. We implement this model to create the KG from two study modules: Embedded Systems and Development of Embedded Systems Using FPGA. The resulting KG structures the curriculum and links it to the domain models. We evaluate our approach through qualitative expert feedback and quantitative graph quality metrics. Domain experts validated the relevance and accuracy of the model, while the graph quality metrics measured the structural properties of our KG. Our results show that the LLM-assisted graph completion approach enhances the ability to connect related courses across disciplines to personalize the learning experience. Expert feedback also showed high acceptance of the proposed collaborative approach for concept extraction and classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12300v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hasan Abu-Rasheed, Constance Jumbo, Rashed Al Amin, Christian Weber, Veit Wiese, Roman Obermaisser, Madjid Fathi</dc:creator>
    </item>
    <item>
      <title>Expertise elevates AI usage: experimental evidence comparing laypeople and professional artists</title>
      <link>https://arxiv.org/abs/2501.12374</link>
      <description>arXiv:2501.12374v1 Announce Type: new 
Abstract: Novel capacities of generative AI to analyze and generate cultural artifacts raise inevitable questions about the nature and value of artistic education and human expertise. Has AI already leveled the playing field between professional artists and laypeople, or do trained artistic expressive capacity, curation skills and experience instead enhance the ability to use these new tools? In this pre-registered study, we conduct experimental comparisons between 50 active artists and a demographically matched sample of laypeople. We designed two tasks to approximate artistic practice for testing their capabilities in both faithful and creative image creation: replicating a reference image, and moving as far away as possible from it. We developed a bespoke platform where participants used a modern text-to-image model to complete both tasks. We also collected and compared participants' sentiments towards AI. On average, artists produced more faithful and creative outputs than their lay counterparts, although only by a small margin. While AI may ease content creation, professional expertise is still valuable - even within the confined space of generative AI itself. Finally, we also explored how well an exemplary vision-capable large language model (GPT-4o) would complete the same tasks, if given the role of an image generation agent, and found it performed on par in copying but outperformed even artists in the creative task. The very best results were still produced by humans in both tasks. These outcomes highlight the importance of integrating artistic skills with AI training to prepare artists and other visual professionals for a technologically evolving landscape. We see a potential in collaborative synergy with generative AI, which could reshape creative industries and education in the arts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12374v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas F. Eisenmann, Andres Karjus, Mar Canet Sola, Levin Brinkmann, Bramantyo Ibrahim Supriyatno, Iyad Rahwan</dc:creator>
    </item>
    <item>
      <title>Handwriting Anomalies and Learning Disabilities through Recurrent Neural Networks and Geometric Pattern Analysis</title>
      <link>https://arxiv.org/abs/2405.07238</link>
      <description>arXiv:2405.07238v2 Announce Type: cross 
Abstract: Dyslexia and dysgraphia are learning disabilities that profoundly impact reading, writing, and language processing capabilities. Dyslexia primarily affects reading, manifesting as difficulties in word recognition and phonological processing, where individuals struggle to connect sounds with their corresponding letters. Dysgraphia, on the other hand, affects writing skills, resulting in difficulties with letter formation, spacing, and alignment. The coexistence of dyslexia and dysgraphia complicates diagnosis, requiring a nuanced approach capable of adapting to these complexities while accurately identifying and differentiating between the disorders. This study utilizes advanced geometrical patterns and recurrent neural networks (RNN) to identify handwriting anomalies indicative of dyslexia and dysgraphia. Handwriting is first standardized, followed by feature extraction that focuses on baseline deviations, letter connectivity, stroke thickness, and other anomalies. These features are then fed into an RNN-based autoencoder to identify irregularities. Initial results demonstrate the ability of this RNN model to achieve state-of-art performance on combined dyslexia and dysgraphia detection, while showing the challenges associated with complex pattern adaptation of deep-learning to a diverse corpus of about 33,000 writing samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07238v2</guid>
      <category>q-bio.QM</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vasileios Alevizos, Sabrina Edralin, Akebu Simasiku, Dimitra Malliarou, Antonis Messinis, George Papakostas, Clark Xu, Zongliang Yue</dc:creator>
    </item>
    <item>
      <title>Creative Loss: Ambiguity, Uncertainty and Indeterminacy</title>
      <link>https://arxiv.org/abs/2501.10369</link>
      <description>arXiv:2501.10369v1 Announce Type: cross 
Abstract: This article evaluates how creative uses of machine learning can address three adjacent terms: ambiguity, uncertainty and indeterminacy. Through the progression of these concepts it reflects on increasing ambitions for machine learning as a creative partner, illustrated with research from Unit 21 at the Bartlett School of Architecture, UCL. Through indeterminacy are potential future approaches to machine learning and design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10369v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tom Holberton</dc:creator>
    </item>
    <item>
      <title>The Generative AI Ethics Playbook</title>
      <link>https://arxiv.org/abs/2501.10383</link>
      <description>arXiv:2501.10383v1 Announce Type: cross 
Abstract: The Generative AI Ethics Playbook provides guidance for identifying and mitigating risks of machine learning systems across various domains, including natural language processing, computer vision, and generative AI. This playbook aims to assist practitioners in diagnosing potential harms that may arise during the design, development, and deployment of datasets and models. It offers concrete strategies and resources for mitigating these risks, to help minimize negative impacts on users and society. Drawing on current best practices in both research and ethical considerations, this playbook aims to serve as a comprehensive resource for AI/ML practitioners. The intended audience of this playbook includes machine learning researchers, engineers, and practitioners who are involved in the creation and implementation of generative and multimodal models (e.g., text-to-text, image-to-image, text-to-image, text-to-video).
  Specifically, we provide transparency/documentation checklists, topics of interest, common questions, examples of harms through case studies, and resources and strategies to mitigate harms throughout the Generative AI lifecycle. This playbook was made collaboratively over the course of 16 months through extensive literature review of over 100 resources and peer-reviewed articles, as well as through an initial group brainstorming session with 18 interdisciplinary AI ethics experts from industry and academia, and with additional feedback from 8 experts (5 of whom were in the initial brainstorming session).
  We note that while this playbook provides examples, discussion, and harm mitigation strategies, research in this area is ongoing. Our playbook aims to be a practically useful survey, taking a high-level view rather than aiming for covering the entire existing body of research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10383v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jessie J. Smith, Wesley Hanwen Deng, William H. Smith, Maarten Sap, Nicole DeCario, Jesse Dodge</dc:creator>
    </item>
    <item>
      <title>Nirvana AI Governance: How AI Policymaking Is Committing Three Old Fallacies</title>
      <link>https://arxiv.org/abs/2501.10384</link>
      <description>arXiv:2501.10384v1 Announce Type: cross 
Abstract: This research applies Harold Demsetz's concept of the nirvana approach to the realm of AI governance and debunks three common fallacies in various AI policy proposals--"the grass is always greener on the other side," "free lunch," and "the people could be different." Through this, I expose fundamental flaws in the current AI regulatory proposal. First, some commentators intuitively believe that people are more reliable than machines and that government works better in risk control than companies' self-regulation, but they do not fully compare the differences between the status quo and the proposed replacements. Second, when proposing some regulatory tools, some policymakers and researchers do not realize and even gloss over the fact that harms and costs are also inherent in their proposals. Third, some policy proposals are initiated based on a false comparison between the AI-driven world, where AI does lead to some risks, and an entirely idealized world, where no risk exists at all. However, the appropriate approach is to compare the world where AI causes risks to the real world where risks are everywhere, but people can live well with these risks. The prevalence of these fallacies in AI governance underscores a broader issue: the tendency to idealize potential solutions without fully considering their real-world implications. This idealization can lead to regulatory proposals that are not only impractical but potentially harmful to innovation and societal progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10384v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Zhang</dc:creator>
    </item>
    <item>
      <title>CodEv: An Automated Grading Framework Leveraging Large Language Models for Consistent and Constructive Feedback</title>
      <link>https://arxiv.org/abs/2501.10421</link>
      <description>arXiv:2501.10421v1 Announce Type: cross 
Abstract: Grading programming assignments is crucial for guiding students to improve their programming skills and coding styles. This study presents an automated grading framework, CodEv, which leverages Large Language Models (LLMs) to provide consistent and constructive feedback. We incorporate Chain of Thought (CoT) prompting techniques to enhance the reasoning capabilities of LLMs and ensure that the grading is aligned with human evaluation. Our framework also integrates LLM ensembles to improve the accuracy and consistency of scores, along with agreement tests to deliver reliable feedback and code review comments. The results demonstrate that the framework can yield grading results comparable to human evaluators, by using smaller LLMs. Evaluation and consistency tests of the LLMs further validate our approach, confirming the reliability of the generated scores and feedback.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10421v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>En-Qi Tseng, Pei-Cing Huang, Chan Hsu, Peng-Yi Wu, Chan-Tung Ku, Yihuang Kang</dc:creator>
    </item>
    <item>
      <title>Perception-Guided EEG Analysis: A Deep Learning Approach Inspired by Level of Detail (LOD) Theory</title>
      <link>https://arxiv.org/abs/2501.10428</link>
      <description>arXiv:2501.10428v1 Announce Type: cross 
Abstract: Objective: This study explores a novel deep learning approach for EEG analysis and perceptual state guidance, inspired by Level of Detail (LOD) theory. The goal is to improve perceptual state identification accuracy and advance personalized psychological therapy. Methods: Portable EEG devices and music rhythm signals were used for data collection. LOD theory was applied to dynamically adjust EEG signal processing, extracting core perceptual features. A Unity-based software system integrated EEG data with audio materials. The deep learning model combined a CNN for feature extraction and classification, and a DQN for reinforcement learning to optimize rhythm adjustments. Results: The CNN achieved 94.05% accuracy in perceptual state classification. The DQN guided subjects to target states with a 92.45% success rate, averaging 13.2 rhythm cycles. However, only 50% of users reported psychological alignment with the target state, indicating room for improvement. Discussion: The results validate the potential of LOD-based EEG biofeedback. Limitations include dataset source, label subjectivity, and reward function optimization. Future work will expand to diverse subjects, incorporate varied musical elements, and refine reward functions for better generalization and personalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10428v1</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>BG Tong</dc:creator>
    </item>
    <item>
      <title>Adapting Large Language Models for Character-based Augmentative and Alternative Communication</title>
      <link>https://arxiv.org/abs/2501.10582</link>
      <description>arXiv:2501.10582v1 Announce Type: cross 
Abstract: Users of Augmentative and Alternative Communication (AAC) may write letter-by-letter via an interface that uses a character language model. However, most state-of-the-art large pretrained language models predict subword tokens of variable length. We investigate how to practically use such models to make accurate and efficient character predictions. We fine-tune models using a large dataset of sentences we curated in which each sentence is rated according to how useful it might be for spoken or written AAC communication. We find that using an algorithm to produce character predictions from a subword large language model provides more accurate predictions than adding a classification layer or using a byte-level model. We also find that our domain adaptation curriculum is effective at improving model performance on simple, conversational text.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10582v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dylan Gaines, Keith Vertanen</dc:creator>
    </item>
    <item>
      <title>Litrepl: Literate Paper Processor Promoting Transparency More Than Reproducibility</title>
      <link>https://arxiv.org/abs/2501.10738</link>
      <description>arXiv:2501.10738v1 Announce Type: cross 
Abstract: Litrepl is a lightweight text processing tool designed to recognize and evaluate code sections within Markdown or Latex documents. This functionality is useful for both batch document section evaluation and interactive coding within a text editor, provided a straightforward integration is established. Inspired by Project Jupyter, Litrepl aims to facilitate the creation of research documents. In the light of recent developments in software deployment, however, we have shifted our focus from informal reproducibility to enhancing transparency in communication with programming language interpreters, by either eliminating or clearly exposing mutable states within the communication process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10738v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sergei Mironov</dc:creator>
    </item>
    <item>
      <title>Fine-Grained Appropriate Reliance: Human-AI Collaboration with a Multi-Step Transparent Decision Workflow for Complex Task Decomposition</title>
      <link>https://arxiv.org/abs/2501.10909</link>
      <description>arXiv:2501.10909v1 Announce Type: cross 
Abstract: In recent years, the rapid development of AI systems has brought about the benefits of intelligent services but also concerns about security and reliability. By fostering appropriate user reliance on an AI system, both complementary team performance and reduced human workload can be achieved. Previous empirical studies have extensively analyzed the impact of factors ranging from task, system, and human behavior on user trust and appropriate reliance in the context of one-step decision making. However, user reliance on AI systems in tasks with complex semantics that require multi-step workflows remains under-explored. Inspired by recent work on task decomposition with large language models, we propose to investigate the impact of a novel Multi-Step Transparent (MST) decision workflow on user reliance behaviors. We conducted an empirical study (N = 233) of AI-assisted decision making in composite fact-checking tasks (i.e., fact-checking tasks that entail multiple sub-fact verification steps). Our findings demonstrate that human-AI collaboration with an MST decision workflow can outperform one-step collaboration in specific contexts (e.g., when advice from an AI system is misleading). Further analysis of the appropriate reliance at fine-grained levels indicates that an MST decision workflow can be effective when users demonstrate a relatively high consideration of the intermediate steps. Our work highlights that there is no one-size-fits-all decision workflow that can help obtain optimal human-AI collaboration. Our insights help deepen the understanding of the role of decision workflows in facilitating appropriate reliance. We synthesize important implications for designing effective means to facilitate appropriate reliance on AI systems in composite tasks, positioning opportunities for the human-centered AI and broader HCI communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10909v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gaole He, Patrick Hemmer, Michael V\"ossing, Max Schemmer, Ujwal Gadiraju</dc:creator>
    </item>
    <item>
      <title>Decomposing and Fusing Intra- and Inter-Sensor Spatio-Temporal Signal for Multi-Sensor Wearable Human Activity Recognition</title>
      <link>https://arxiv.org/abs/2501.10917</link>
      <description>arXiv:2501.10917v1 Announce Type: cross 
Abstract: Wearable Human Activity Recognition (WHAR) is a prominent research area within ubiquitous computing. Multi-sensor synchronous measurement has proven to be more effective for WHAR than using a single sensor. However, existing WHAR methods use shared convolutional kernels for indiscriminate temporal feature extraction across each sensor variable, which fails to effectively capture spatio-temporal relationships of intra-sensor and inter-sensor variables. We propose the DecomposeWHAR model consisting of a decomposition phase and a fusion phase to better model the relationships between modality variables. The decomposition creates high-dimensional representations of each intra-sensor variable through the improved Depth Separable Convolution to capture local temporal features while preserving their unique characteristics. The fusion phase begins by capturing relationships between intra-sensor variables and fusing their features at both the channel and variable levels. Long-range temporal dependencies are modeled using the State Space Model (SSM), and later cross-sensor interactions are dynamically captured through a self-attention mechanism, highlighting inter-sensor spatial correlations. Our model demonstrates superior performance on three widely used WHAR datasets, significantly outperforming state-of-the-art models while maintaining acceptable computational efficiency. Our codes and supplementary materials are available at https://github.com/Anakin2555/DecomposeWHAR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10917v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyu Xie, Haoxuan Li, Chunyuan Zheng, Haonan Yuan, Guorui Liao, Jun Liao, Li Liu</dc:creator>
    </item>
    <item>
      <title>The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs</title>
      <link>https://arxiv.org/abs/2501.10970</link>
      <description>arXiv:2501.10970v1 Announce Type: cross 
Abstract: The "LLM-as-a-judge" paradigm employs Large Language Models (LLMs) as annotators and evaluators in tasks traditionally performed by humans. LLM annotations are widely used, not only in NLP research but also in fields like medicine, psychology, and social science. Despite their role in shaping study results and insights, there is no standard or rigorous procedure to determine whether LLMs can replace human annotators. In this paper, we propose a novel statistical procedure -- the Alternative Annotator Test (alt-test) -- that requires only a modest subset of annotated examples to justify using LLM annotations. Additionally, we introduce a versatile and interpretable measure for comparing LLM judges. To demonstrate our procedure, we curated a diverse collection of ten datasets, consisting of language and vision-language tasks, and conducted experiments with six LLMs and four prompting techniques. Our results show that LLMs can sometimes replace humans with closed-source LLMs (such as GPT-4o), outperforming open-source LLMs, and that prompting techniques yield judges of varying quality. We hope this study encourages more rigorous and reliable practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10970v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Nitay Calderon, Roi Reichart, Rotem Dror</dc:creator>
    </item>
    <item>
      <title>Conversation Routines: A Prompt Engineering Framework for Task-Oriented Dialog Systems</title>
      <link>https://arxiv.org/abs/2501.11613</link>
      <description>arXiv:2501.11613v1 Announce Type: cross 
Abstract: This study introduces Conversation Routines (CR), a structured prompt engineering framework for developing task-oriented dialog systems using Large Language Models (LLMs). While LLMs demonstrate remarkable natural language understanding capabilities, engineering them to reliably execute complex business workflows remains challenging. The proposed CR framework enables the development of Conversation Agentic Systems (CAS) through natural language specifications, embedding task-oriented logic within LLM prompts. This approach provides a systematic methodology for designing and implementing complex conversational workflows while maintaining behavioral consistency. We demonstrate the framework's effectiveness through two proof of concept implementations: a Train Ticket Booking System and an Interactive Troubleshooting Copilot. These case studies validate CR's capability to encode sophisticated behavioral patterns and decision logic while preserving natural conversational flexibility. Results show that CR enables domain experts to design conversational workflows in natural language while leveraging custom enterprise functionalities (tools) developed by software engineers, creating an efficient division of responsibilities where developers focus on core API implementation and domain experts handle conversation design. While the framework shows promise in accessibility and adaptability, we identify key challenges including computational overhead, non-deterministic behavior, and domain-specific logic optimization. Future research directions include enhancing system robustness, improving scalability for complex multi-agent interactions, and addressing the identified limitations across diverse business applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11613v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <category>cs.PL</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giorgio Robino</dc:creator>
    </item>
    <item>
      <title>Who is to Blame: A Comprehensive Review of Challenges and Opportunities in Designer-Developer Collaboration</title>
      <link>https://arxiv.org/abs/2501.11748</link>
      <description>arXiv:2501.11748v1 Announce Type: cross 
Abstract: Software development relies on effective collaboration between Software Development Engineers (SDEs) and User eXperience Designers (UXDs) to create software products of high quality and usability. While this collaboration issue has been explored over the past decades, anecdotal evidence continues to indicate the existence of challenges in their collaborative efforts. To understand this gap, we first conducted a systematic literature review (SLR) of 45 papers published since 2004, uncovering three key collaboration challenges and two main categories of potential best practices. We then analyzed designer and developer forums and discussions from one open-source software repository to assess how the challenges and practices manifest in the status quo. Our findings have broad applicability for collaboration in software development, extending beyond the partnership between SDEs and UXDs. The suggested best practices and interventions also act as a reference for future research, assisting in the development of dedicated collaboration tools for SDEs and UXDs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11748v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3711105</arxiv:DOI>
      <dc:creator>Shutong Zhang, Tianyu Zhang, Jinghui Cheng, Shurui Zhou</dc:creator>
    </item>
    <item>
      <title>Connection-Coordination Rapport (CCR) Scale: A Dual-Factor Scale to Measure Human-Robot Rapport</title>
      <link>https://arxiv.org/abs/2501.11887</link>
      <description>arXiv:2501.11887v1 Announce Type: cross 
Abstract: Robots, particularly in service and companionship roles, must develop positive relationships with people they interact with regularly to be successful. These positive human-robot relationships can be characterized as establishing "rapport," which indicates mutual understanding and interpersonal connection that form the groundwork for successful long-term human-robot interaction. However, the human-robot interaction research literature lacks scale instruments to assess human-robot rapport in a variety of situations. In this work, we developed the 18-item Connection-Coordination Rapport (CCR) Scale to measure human-robot rapport. We first ran Study 1 (N = 288) where online participants rated videos of human-robot interactions using a set of candidate items. Our Study 1 results showed the discovery of two factors in our scale, which we named "Connection" and "Coordination." We then evaluated this scale by running Study 2 (N = 201) where online participants rated a new set of human-robot interaction videos with our scale and an existing rapport scale from virtual agents research for comparison. We also validated our scale by replicating a prior in-person human-robot interaction study, Study 3 (N = 44), and found that rapport is rated significantly greater when participants interacted with a responsive robot (responsive condition) as opposed to an unresponsive robot (unresponsive condition). Results from these studies demonstrate high reliability and validity for the CCR scale, which can be used to measure rapport in both first-person and third-person perspectives. We encourage the adoption of this scale in future studies to measure rapport in a variety of human-robot interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11887v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ting-Han Lin, Hannah Dinner, Tsz Long Leung, Bilge Mutlu, J. Gregory Trafton, Sarah Sebo</dc:creator>
    </item>
    <item>
      <title>Evaluating Efficiency and Engagement in Scripted and LLM-Enhanced Human-Robot Interactions</title>
      <link>https://arxiv.org/abs/2501.12128</link>
      <description>arXiv:2501.12128v1 Announce Type: cross 
Abstract: To achieve natural and intuitive interaction with people, HRI frameworks combine a wide array of methods for human perception, intention communication, human-aware navigation and collaborative action. In practice, when encountering unpredictable behavior of people or unexpected states of the environment, these frameworks may lack the ability to dynamically recognize such states, adapt and recover to resume the interaction. Large Language Models (LLMs), owing to their advanced reasoning capabilities and context retention, present a promising solution for enhancing robot adaptability. This potential, however, may not directly translate to improved interaction metrics. This paper considers a representative interaction with an industrial robot involving approach, instruction, and object manipulation, implemented in two conditions: (1) fully scripted and (2) including LLM-enhanced responses. We use gaze tracking and questionnaires to measure the participants' task efficiency, engagement, and robot perception. The results indicate higher subjective ratings for the LLM condition, but objective metrics show that the scripted condition performs comparably, particularly in efficiency and focus during simple tasks. We also note that the scripted condition may have an edge over LLM-enhanced responses in terms of response latency and energy consumption, especially for trivial and repetitive interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12128v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tim Schreiter, Jens V. R\"uppel, Rishi Hazra, Andrey Rudenko, Martin Magnusson, Achim J. Lilienthal</dc:creator>
    </item>
    <item>
      <title>MyDigiTwin: A Privacy-Preserving Framework for Personalized Cardiovascular Risk Prediction and Scenario Exploration</title>
      <link>https://arxiv.org/abs/2501.12193</link>
      <description>arXiv:2501.12193v1 Announce Type: cross 
Abstract: Cardiovascular disease (CVD) remains a leading cause of death, and primary prevention through personalized interventions is crucial. This paper introduces MyDigiTwin, a framework that integrates health digital twins with personal health environments to empower patients in exploring personalized health scenarios while ensuring data privacy. MyDigiTwin uses federated learning to train predictive models across distributed datasets without transferring raw data, and a novel data harmonization framework addresses semantic and format inconsistencies in health data. A proof-of-concept demonstrates the feasibility of harmonizing and using cohort data to train privacy-preserving CVD prediction models. This framework offers a scalable solution for proactive, personalized cardiovascular care and sets the stage for future applications in real-world healthcare settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12193v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>H\'ector Cadavid, Hyunho Mo, Bauke Arends, Katarzyna Dziopa, Esther E. Bron, Daniel Bos, Sonja Georgievska, Pim van der Harst</dc:creator>
    </item>
    <item>
      <title>Improving robot understanding using conversational AI: demonstration and feasibility study</title>
      <link>https://arxiv.org/abs/2501.12214</link>
      <description>arXiv:2501.12214v1 Announce Type: cross 
Abstract: Explanations constitute an important aspect of successful human robot interactions and can enhance robot understanding. To improve the understanding of the robot, we have developed four levels of explanation (LOE) based on two questions: what needs to be explained, and why the robot has made a particular decision. The understandable robot requires a communicative action when there is disparity between the human s mental model of the robot and the robots state of mind. This communicative action was generated by utilizing a conversational AI platform to generate explanations. An adaptive dialog was implemented for transition from one LOE to another. Here, we demonstrate the adaptive dialog in a collaborative task with errors and provide results of a feasibility study with users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12214v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shikhar Kumar, Yael Edan</dc:creator>
    </item>
    <item>
      <title>Regressor-Guided Image Editing Regulates Emotional Response to Reduce Online Engagement</title>
      <link>https://arxiv.org/abs/2501.12289</link>
      <description>arXiv:2501.12289v1 Announce Type: cross 
Abstract: Emotions are known to mediate the relationship between users' content consumption and their online engagement, with heightened emotional intensity leading to increased engagement. Building on this insight, we propose three regressor-guided image editing approaches aimed at diminishing the emotional impact of images. These include (i) a parameter optimization approach based on global image transformations known to influence emotions, (ii) an optimization approach targeting the style latent space of a generative adversarial network, and (iii) a diffusion-based approach employing classifier guidance and classifier-free guidance. Our findings demonstrate that approaches can effectively alter the emotional properties of images while maintaining high visual quality. Optimization-based methods primarily adjust low-level properties like color hues and brightness, whereas the diffusion-based approach introduces semantic changes, such as altering appearance or facial expressions. Notably, results from a behavioral study reveal that only the diffusion-based approach successfully elicits changes in viewers' emotional responses while preserving high perceived image quality. In future work, we will investigate the impact of these image adaptations on internet user behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12289v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Gebhardt, Robin Willardt, Seyedmorteza Sadat, Chih-Wei Ning, Andreas Brombach, Jie Song, Otmar Hilliges, Christian Holz</dc:creator>
    </item>
    <item>
      <title>UI-TARS: Pioneering Automated GUI Interaction with Native Agents</title>
      <link>https://arxiv.org/abs/2501.12326</link>
      <description>arXiv:2501.12326v1 Announce Type: cross 
Abstract: This paper introduces UI-TARS, a native GUI agent model that solely perceives the screenshots as input and performs human-like interactions (e.g., keyboard and mouse operations). Unlike prevailing agent frameworks that depend on heavily wrapped commercial models (e.g., GPT-4o) with expert-crafted prompts and workflows, UI-TARS is an end-to-end model that outperforms these sophisticated frameworks. Experiments demonstrate its superior performance: UI-TARS achieves SOTA performance in 10+ GUI agent benchmarks evaluating perception, grounding, and GUI task execution. Notably, in the OSWorld benchmark, UI-TARS achieves scores of 24.6 with 50 steps and 22.7 with 15 steps, outperforming Claude (22.0 and 14.9 respectively). In AndroidWorld, UI-TARS achieves 46.6, surpassing GPT-4o (34.5). UI-TARS incorporates several key innovations: (1) Enhanced Perception: leveraging a large-scale dataset of GUI screenshots for context-aware understanding of UI elements and precise captioning; (2) Unified Action Modeling, which standardizes actions into a unified space across platforms and achieves precise grounding and interaction through large-scale action traces; (3) System-2 Reasoning, which incorporates deliberate reasoning into multi-step decision making, involving multiple reasoning patterns such as task decomposition, reflection thinking, milestone recognition, etc. (4) Iterative Training with Reflective Online Traces, which addresses the data bottleneck by automatically collecting, filtering, and reflectively refining new interaction traces on hundreds of virtual machines. Through iterative training and reflection tuning, UI-TARS continuously learns from its mistakes and adapts to unforeseen situations with minimal human intervention. We also analyze the evolution path of GUI agents to guide the further development of this domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12326v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, Guang Shi</dc:creator>
    </item>
    <item>
      <title>ChartGPT: Leveraging LLMs to Generate Charts from Abstract Natural Language</title>
      <link>https://arxiv.org/abs/2311.01920</link>
      <description>arXiv:2311.01920v2 Announce Type: replace 
Abstract: The use of natural language interfaces (NLIs) to create charts is becoming increasingly popular due to the intuitiveness of natural language interactions. One key challenge in this approach is to accurately capture user intents and transform them to proper chart specifications. This obstructs the wide use of NLI in chart generation, as users' natural language inputs are generally abstract (i.e., ambiguous or under-specified), without a clear specification of visual encodings. Recently, pre-trained large language models (LLMs) have exhibited superior performance in understanding and generating natural language, demonstrating great potential for downstream tasks. Inspired by this major trend, we propose ChartGPT, generating charts from abstract natural language inputs. However, LLMs are struggling to address complex logic problems. To enable the model to accurately specify the complex parameters and perform operations in chart generation, we decompose the generation process into a step-by-step reasoning pipeline, so that the model only needs to reason a single and specific sub-task during each run. Moreover, LLMs are pre-trained on general datasets, which might be biased for the task of chart generation. To provide adequate visualization knowledge, we create a dataset consisting of abstract utterances and charts and improve model performance through fine-tuning. We further design an interactive interface for ChartGPT that allows users to check and modify the intermediate outputs of each step. The effectiveness of the proposed system is evaluated through quantitative evaluations and a user study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01920v2</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TVCG.2024.3368621</arxiv:DOI>
      <dc:creator>Yuan Tian, Weiwei Cui, Dazhen Deng, Xinjing Yi, Yurun Yang, Haidong Zhang, Yingcai Wu</dc:creator>
    </item>
    <item>
      <title>What Lies Beneath? Exploring the Impact of Underlying AI Model Updates in AI-Infused Systems</title>
      <link>https://arxiv.org/abs/2311.10652</link>
      <description>arXiv:2311.10652v3 Announce Type: replace 
Abstract: AI models are constantly evolving, with new versions released frequently. Human-AI interaction guidelines encourage notifying users about changes in model capabilities, ideally supported by thorough benchmarking. However, as AI systems integrate into domain-specific workflows, exhaustive benchmarking can become impractical, often resulting in silent or minimally communicated updates. This raises critical questions: Can users notice these updates? What cues do they rely on to distinguish between models? How do such changes affect their behavior and task performance? We address these questions through two studies in the context of facial recognition for historical photo identification: an online experiment examining users' ability to detect model updates, followed by a diary study exploring perceptions in a real-world deployment. Our findings highlight challenges in noticing AI model updates, their impact on downstream user behavior and performance, and how they lead users to develop divergent folk theories. Drawing on these insights, we discuss strategies for effectively communicating model updates in AI-infused systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.10652v3</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713751</arxiv:DOI>
      <dc:creator>Vikram Mohanty, Jude Lim, Kurt Luther</dc:creator>
    </item>
    <item>
      <title>Supporting Industry Computing Researchers in Assessing, Articulating, and Addressing the Potential Negative Societal Impact of Their Work</title>
      <link>https://arxiv.org/abs/2408.01057</link>
      <description>arXiv:2408.01057v3 Announce Type: replace 
Abstract: Recent years have witnessed increasing calls for computing researchers to grapple with the societal impacts of their work. Tools such as impact assessments have gained prominence as a method to uncover potential impacts, and a number of publication venues now encourage authors to include an impact statement in their submissions. Despite this push, little is known about the way researchers assess, articulate, and address the potential negative societal impact of their work -- especially in industry settings, where research outcomes are often quickly integrated into products. In addition, while there are nascent efforts to support researchers in this task, there remains a dearth of empirically-informed tools and processes. Through interviews with 25 industry computing researchers across different companies and research areas, we first identify four key factors that influence how they grapple with (or choose not to grapple with) the societal impact of their research. To develop an effective impact assessment template tailored to industry computing researchers' needs, we conduct an iterative co-design process with these 25 industry researchers and an additional 16 researchers and practitioners with prior experience and expertise in reviewing and developing impact assessments or broad responsible computing practices. Through the co-design process, we develop 10 design considerations to facilitate the effective design, development, and adaptation of an impact assessment template for use in industry research settings and beyond, as well as our own ``Societal Impact Assessment'' template with concrete scaffolds. We explore the effectiveness of this template through a user study with 15 industry research interns, revealing both its strengths and limitations. Finally, we discuss the implications for future researchers and organizations seeking to foster more responsible research practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01057v3</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3711076</arxiv:DOI>
      <arxiv:journal_reference>Proc. ACM Hum.-Comput. Interact. 9, 2, Article CSCW 2025</arxiv:journal_reference>
      <dc:creator>Wesley Hanwen Deng, Solon Barocas, Jennifer Wortman Vaughan</dc:creator>
    </item>
    <item>
      <title>Why So Serious? Exploring Timely Humorous Comments in AAC Through AI-Powered Interfaces</title>
      <link>https://arxiv.org/abs/2410.16634</link>
      <description>arXiv:2410.16634v3 Announce Type: replace 
Abstract: People with disabilities that affect their speech, often use speech-generating devices (SGD), commonly referred to as Augmentative and Alternative Communication (AAC) technology. This technology enables practical conversation; however, there has been a growing interest in extending AAC to support more expressive forms of conversation such as humor. In this paper, we study how to extend AAC technology to support a subset of humorous expression: delivering timely humorous comments -- witty remarks -- through AI-powered interfaces. We conducted seven qualitative interviews with AAC users and performed thematic analysis to gain in-depth insights about their experiences and challenges with AAC technology, and the role humor plays for them. We developed four simple AI-powered interfaces designed to support users in creating timely humorous comments during real-time conversations. Through a user study with five AAC users, we explored how to effectively support the delivery of well-timed humorous remarks. We conclude with a discussion of recommendations for interface design based on both studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16634v3</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias Weinberg, Kowe Kadoma, Ricardo E. Gonzalez Penuela, Stephanie Valencia, Thijs Roumen</dc:creator>
    </item>
    <item>
      <title>Using Confidence Scores to Improve Eyes-free Detection of Speech Recognition Errors</title>
      <link>https://arxiv.org/abs/2410.20564</link>
      <description>arXiv:2410.20564v2 Announce Type: replace 
Abstract: Conversational systems rely heavily on speech recognition to interpret and respond to user commands and queries. Despite progress on speech recognition accuracy, errors may still sometimes occur and can significantly affect the end-user utility of such systems. While visual feedback can help detect errors, it may not always be practical, especially for people who are blind or low-vision. In this study, we investigate ways to improve error detection by manipulating the audio output of the transcribed text based on the recognizer's confidence level in its result. Our findings show that selectively slowing down the audio when the recognizer exhibited uncertainty led to a 12% relative increase in participants' ability to detect errors compared to uniformly slowing the audio. It also reduced the time it took participants to listen to the recognition result and decide if there was an error by 11%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20564v2</guid>
      <category>cs.HC</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sadia Nowrin, Keith Vertanen</dc:creator>
    </item>
    <item>
      <title>Over-the-Air Federated Adaptive Data Analysis: Preserving Accuracy via Opportunistic Differential Privacy</title>
      <link>https://arxiv.org/abs/2411.15948</link>
      <description>arXiv:2411.15948v2 Announce Type: replace 
Abstract: Adaptive data analysis (ADA) involves a dynamic interaction between an analyst and a dataset owner, where the analyst submits queries sequentially, adapting them based on previous answers. This process can become adversarial, as the analyst may attempt to overfit by targeting non-generalizable patterns in the data. To counteract this, the dataset owner introduces randomization techniques, such as adding noise to the responses. This noise not only helps prevent overfitting, but also enhances data privacy. However, it must be carefully calibrated to ensure that the statistical reliability of the responses is not compromised. In this paper, we extend the ADA problem to the context of distributed datasets. Specifically, we consider a scenario where a potentially adversarial analyst interacts with multiple distributed responders through adaptive queries. We assume the responses are subject to noise, introduced by the channel connecting the responders and the analyst. We demonstrate how this noise can be opportunistically leveraged through a federated mechanism to enhance the generalizability of ADA, thereby increasing the number of query-response interactions between the analyst and the responders. We illustrate that the careful tuning of the transmission amplitude based on the theoretically achievable bounds can significantly impact the number of accurately answerable queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15948v2</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Amir Hossein Hadavi, Mohammad M. Mojahedian, Mohammad Reza Aref</dc:creator>
    </item>
    <item>
      <title>Connecting through Comics: Design and Evaluation of Cube, an Arts-Based Digital Platform for Trauma-Impacted Youth</title>
      <link>https://arxiv.org/abs/2412.09834</link>
      <description>arXiv:2412.09834v3 Announce Type: replace 
Abstract: This paper explores the design, development and evaluation of a digital platform that aims to assist young people who have experienced trauma in understanding and expressing their emotions and fostering social connections. Integrating principles from expressive arts and narrative-based therapies, we collaborate with lived experts to iteratively design a novel, user-centered digital tool for young people to create and share comics that represent their experiences. Specifically, we conduct a series of nine workshops with N=54 trauma-impacted youth and young adults to test and refine our tool, beginning with three workshops using low-fidelity prototypes, followed by six workshops with Cube, a web version of the tool. A qualitative analysis of workshop feedback and empathic relations analysis of artifacts provides valuable insights into the usability and potential impact of the tool, as well as the specific needs of young people who have experienced trauma. Our findings suggest that the integration of expressive and narrative therapy principles into Cube can offer a unique avenue for trauma-impacted young people to process their experiences, more easily communicate their emotions, and connect with supportive communities. We end by presenting implications for the design of social technologies that aim to support the emotional well-being and social integration of youth and young adults who have faced trauma.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09834v3</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ila Kumar, Jocelyn Shen, Craig Ferguson, Rosalind W Picard</dc:creator>
    </item>
    <item>
      <title>Keeping Score: A Quantitative Analysis of How the CHI Community Appreciates Its Milestones</title>
      <link>https://arxiv.org/abs/2501.02456</link>
      <description>arXiv:2501.02456v2 Announce Type: replace 
Abstract: The ACM CHI Conference has a tradition of citing its intellectual heritage. At the same time, we know CHI is highly diverse and evolving. In this highly dynamic context, it is not clear how the CHI community continues to appreciate its milestones (within and outside of CHI). We present an investigation into how the community's citations to milestones have evolved over 43 years of CHI Proceedings (1981-2024). Forgetting curves plotted for each year suggest that milestones are slowly fading from the CHI community's collective memory. However, the picture is more nuanced when we trace citations to the top-cited milestones over time. We identify three distinct types of milestones cited at CHI, a typology of milestone contributions, and define the Milestone Coefficient as a metric to assess the impact of milestone papers on a continuous scale. Further, we provide empirical evidence of a Matthew effect at CHI. We discuss the broader ramifications for the CHI community and the field of HCI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02456v2</guid>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713464</arxiv:DOI>
      <dc:creator>Jonas Oppenlaender, Simo Hosio</dc:creator>
    </item>
    <item>
      <title>HistoryPalette: Supporting Exploration and Reuse of Past Alternatives in Image Generation and Editing</title>
      <link>https://arxiv.org/abs/2501.04163</link>
      <description>arXiv:2501.04163v2 Announce Type: replace 
Abstract: All creative tasks require creators to iteratively produce, select, and discard potentially useful ideas. Now, creativity tools include generative AI features (e.g., Photoshop Generative Fill) that increase the number of alternatives creators consider due to rapid experiments with text prompts and random generations. Creators use tedious manual systems for organizing their prior ideas by saving file versions or hiding layers, but they lack the support they want for reusing prior alternatives in personal work or in communication with others. We present HistoryPalette, a system that supports exploration and reuse of prior designs in generative image creation and editing. Using HistoryPalette, creators and their collaborators explore a "palette" of prior design alternatives organized by spatial position, topic category, and creation time. HistoryPalette enables creators to quickly preview and reuse their prior work. In creative professional and client collaborator user studies, participants generated and edited images by exploring and reusing past design alternatives with HistoryPalette.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04163v2</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karim Benharrak, Amy Pavel</dc:creator>
    </item>
    <item>
      <title>Exploring the distribution of connectivity weights in resting-state EEG networks</title>
      <link>https://arxiv.org/abs/2501.07394</link>
      <description>arXiv:2501.07394v2 Announce Type: replace 
Abstract: The resting-state brain networks (RSNs) reflects the functional connectivity patterns between brain modules, providing essential foundations for decoding intrinsic neural information within the brain. It serves as one of the primary tools for describing the spatial dynamics of the brain using various neuroimaging techniques, such as electroencephalography (EEG) and magnetoencephalography (MEG). However, the distribution rules or potential modes of functional connectivity weights in the resting state remain unclear. In this context, we first start from simulation, using forward solving model to generate scalp EEG with four channel densities (19, 32, 64, 128). Subsequently, we construct scalp brain networks using five coupling measures, aiming to explore whether different channel density or coupling measures affect the distribution pattern of functional connectivity weights. Next, we quantify the distribution pattern by calculating the skewness, kurtosis, and Shannon entropy of the functional connectivity network weights. Finally, the results of the simulation were validated in a normative database. We observed that: 1) The functional connection weights exhibit a right-skewed distribution, and are not influenced by channel density or coupling measures; 2) The functional connection weights exhibit a relatively uniform distribution, with the potential for volume conduction to affect the degree of uniformity in the distribution; 3) Networks constructed using coupling measures influenced by volume conduction exhibit significant correlations between the average connection weight and measures of skewness, kurtosis, and Shannon entropy. This study contributes to a deeper understanding of RSNs, providing valuable insights for research in the field of neuroscience, and holds promise for being associated with brain cognition and disease diagnosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07394v2</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiang Hu, Xiao Gong, Xiaolong Huang, Jie Ruan, Pedro Antonio Valdes-Sosa</dc:creator>
    </item>
    <item>
      <title>Redefining Affordance via Computational Rationality</title>
      <link>https://arxiv.org/abs/2501.09233</link>
      <description>arXiv:2501.09233v3 Announce Type: replace 
Abstract: Affordances, a foundational concept in human-computer interaction and design, have traditionally been explained by direct-perception theories, which assume that individuals perceive action possibilities directly from the environment. However, these theories fall short of explaining how affordances are perceived, learned, refined, or misperceived, and how users choose between multiple affordances in dynamic contexts. This paper introduces a novel affordance theory grounded in Computational Rationality, positing that humans construct internal representations of the world based on bounded sensory inputs. Within these internal models, affordances are inferred through two core mechanisms: feature recognition and hypothetical motion trajectories. Our theory redefines affordance perception as a decision-making process, driven by two components: confidence (the perceived likelihood of successfully executing an action) and predicted utility (the expected value of the outcome). By balancing these factors, individuals make informed decisions about which actions to take. Our theory frames affordances perception as dynamic, continuously learned, and refined through reinforcement and feedback. We validate the theory via thought experiments and demonstrate its applicability across diverse types of affordances (e.g., physical, digital, social). Beyond clarifying and generalizing the understanding of affordances across contexts, our theory serves as a foundation for improving design communication and guiding the development of more adaptive and intuitive systems that evolve with user capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09233v3</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3708359.3712114</arxiv:DOI>
      <dc:creator>Yi-Chi Liao, Christian Holz</dc:creator>
    </item>
    <item>
      <title>Narratives of War: Ukrainian Memetic Warfare on Twitter</title>
      <link>https://arxiv.org/abs/2309.08363</link>
      <description>arXiv:2309.08363v3 Announce Type: replace-cross 
Abstract: The 2022 Russian invasion of Ukraine has seen an intensification in the use of social media by governmental actors in cyber warfare. Wartime communication via memes has been a successful strategy used not only by independent accounts such as @uamemesforces, but also-for the first time in a full-scale interstate war-by official Ukrainian government accounts such as @Ukraine and @DefenceU. We study this prominent example of memetic warfare through the lens of its narratives, and find them to be a key component of success: tweets with a 'victim' narrative garner twice as many retweets. However, malevolent narratives focusing on the enemy resonate more than those about heroism or victims with countries providing more assistance to Ukraine. Our findings present a nuanced examination of Ukraine's influence operations and of the worldwide response to it, thus contributing new insights into the evolution of socio-technical systems in times of war.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08363v3</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3711037</arxiv:DOI>
      <arxiv:journal_reference>ACM SIGCHI Conference on Computer-Supported Cooperative Work &amp; Social Computing (CSCW) 2025</arxiv:journal_reference>
      <dc:creator>Yelena Mejova, Arthur Capozzi, Corrado Monti, Gianmarco De Francisci Morales</dc:creator>
    </item>
    <item>
      <title>Building Workflows for Interactive Human in the Loop Automated Experiment (hAE) in STEM-EELS</title>
      <link>https://arxiv.org/abs/2404.07381</link>
      <description>arXiv:2404.07381v2 Announce Type: replace-cross 
Abstract: Exploring the structural, chemical, and physical properties of matter on the nano- and atomic scales has become possible with the recent advances in aberration-corrected electron energy-loss spectroscopy (EELS) in scanning transmission electron microscopy (STEM). However, the current paradigm of STEM-EELS relies on the classical rectangular grid sampling, in which all surface regions are assumed to be of equal a priori interest. This is typically not the case for real-world scenarios, where phenomena of interest are concentrated in a small number of spatial locations. One of foundational problems is the discovery of nanometer- or atomic scale structures having specific signatures in EELS spectra. Here we systematically explore the hyperparameters controlling deep kernel learning (DKL) discovery workflows for STEM-EELS and identify the role of the local structural descriptors and acquisition functions on the experiment progression. In agreement with actual experiment, we observe that for certain parameter combinations the experiment path can be trapped in the local minima. We demonstrate the approaches for monitoring automated experiment in the real and feature space of the system and monitor knowledge acquisition of the DKL model. Based on these, we construct intervention strategies, thus defining human-in the loop automated experiment (hAE). This approach can be further extended to other techniques including 4D STEM and other forms of spectroscopic imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07381v2</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Utkarsh Pratiush, Kevin M. Roccapriore, Yongtao Liu, Gerd Duscher, Maxim Ziatdinov, Sergei V. Kalinin</dc:creator>
    </item>
    <item>
      <title>Visual Evaluative AI: A Hypothesis-Driven Tool with Concept-Based Explanations and Weight of Evidence</title>
      <link>https://arxiv.org/abs/2407.04710</link>
      <description>arXiv:2407.04710v2 Announce Type: replace-cross 
Abstract: This paper presents Visual Evaluative AI, a decision aid that provides positive and negative evidence from image data for a given hypothesis. This tool finds high-level human concepts in an image and generates the Weight of Evidence (WoE) for each hypothesis in the decision-making process. We apply and evaluate this tool in the skin cancer domain by building a web-based application that allows users to upload a dermatoscopic image, select a hypothesis and analyse their decisions by evaluating the provided evidence. Further, we demonstrate the effectiveness of Visual Evaluative AI on different concept-based explanation approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04710v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thao Le, Tim Miller, Ruihan Zhang, Liz Sonenberg, Ronal Singh</dc:creator>
    </item>
    <item>
      <title>Tactile Displays Driven by Projected Light</title>
      <link>https://arxiv.org/abs/2410.05494</link>
      <description>arXiv:2410.05494v3 Announce Type: replace-cross 
Abstract: Tactile displays that lend tangible form to digital content could transform computing interactions. However, achieving the resolution, speed, and dynamic range needed for perceptual fidelity remains challenging. We present a tactile display that directly converts projected light into visible tactile patterns via a photomechanical surface populated with millimeter-scale optotactile pixels. The pixels transduce incident light into mechanical displacements through photostimulated thermal gas expansion, yielding millimeter scale displacements with response times of 2 to 100 milliseconds. Employing projected light for power transmission and addressing renders these displays highly scalable. We demonstrate optically driven displays with up to 1,511 addressable pixels -- several times more pixels than any prior tactile display attaining comparable performance. Perceptual studies confirm that these displays can reproduce diverse spatiotemporal tactile patterns with high fidelity. This research establishes a foundation for practical, versatile high-resolution tactile displays driven by light.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05494v3</guid>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <category>physics.optics</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Linnander, Dustin Goetz, Gregory Reardon, Vijay Kumar, Elliot Hawkes, Yon Visell</dc:creator>
    </item>
    <item>
      <title>On Explaining Recommendations with Large Language Models: A Review</title>
      <link>https://arxiv.org/abs/2411.19576</link>
      <description>arXiv:2411.19576v2 Announce Type: replace-cross 
Abstract: The rise of Large Language Models (LLMs), such as LLaMA and ChatGPT, has opened new opportunities for enhancing recommender systems through improved explainability. This paper provides a systematic literature review focused on leveraging LLMs to generate explanations for recommendations -- a critical aspect for fostering transparency and user trust. We conducted a comprehensive search within the ACM Guide to Computing Literature, covering publications from the launch of ChatGPT (November 2022) to the present (November 2024). Our search yielded 232 articles, but after applying inclusion criteria, only six were identified as directly addressing the use of LLMs in explaining recommendations. This scarcity highlights that, despite the rise of LLMs, their application in explainable recommender systems is still in an early stage. We analyze these select studies to understand current methodologies, identify challenges, and suggest directions for future research. Our findings underscore the potential of LLMs improving explanations of recommender systems and encourage the development of more transparent and user-centric recommendation explanation solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19576v2</guid>
      <category>cs.IR</category>
      <category>cs.HC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3389/fdata.2024.1505284</arxiv:DOI>
      <dc:creator>Alan Said</dc:creator>
    </item>
  </channel>
</rss>
