<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Jun 2025 04:08:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bridging Equilibrium and Kinetics Prediction with a Data-Weighted Neural Network Model of Methane Steam Reforming</title>
      <link>https://arxiv.org/abs/2506.17224</link>
      <description>arXiv:2506.17224v1 Announce Type: new 
Abstract: Hydrogen's role is growing as an energy carrier, increasing the need for efficient production, with methane steam reforming being the most widely used technique. This process is crucial for applications like fuel cells, where hydrogen is converted into electricity, pushing for reactor miniaturization and optimized process control through numerical simulations. Existing models typically address either kinetic or equilibrium regimes, limiting their applicability. Here we show a surrogate model capable of unifying both regimes. An artificial neural network trained on a comprehensive dataset that includes experimental data from kinetic and equilibrium experiments, interpolated data, and theoretical data derived from theoretical models for each regime. Data augmentation and assigning appropriate weights to each data type enhanced training. After evaluating Bayesian Optimization and Random Sampling, the optimal model demonstrated high predictive accuracy for the composition of the post-reaction mixture under varying operating parameters, indicated by a mean squared error of 0.000498 and strong Pearson correlation coefficients of 0.927. The network's ability to provide continuous derivatives of its predictions makes it particularly useful for process modeling and optimization. The results confirm the surrogate model's robustness for simulating methane steam reforming in both kinetic and equilibrium regimes, making it a valuable tool for design and process optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17224v1</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zofia Pizo\'n, Shinji Kimijima, Grzegorz Brus</dc:creator>
    </item>
    <item>
      <title>Variational Quantum Latent Encoding for Topology Optimization</title>
      <link>https://arxiv.org/abs/2506.17487</link>
      <description>arXiv:2506.17487v1 Announce Type: new 
Abstract: A variational framework for structural topology optimization is developed, integrating quantum and classical latent encoding strategies within a coordinate-based neural decoding architecture. In this approach, a low-dimensional latent vector, generated either by a variational quantum circuit or sampled from a Gaussian distribution, is mapped to a higher-dimensional latent space via a learnable projection layer. This enriched representation is then decoded into a high-resolution material distribution using a neural network that takes both the latent vector and Fourier-mapped spatial coordinates as input. The optimization is performed directly on the latent parameters, guided solely by physics-based objectives such as compliance minimization and volume constraints evaluated through finite element analysis, without requiring any precomputed datasets or supervised training. Quantum latent vectors are constructed from the expectation values of Pauli observables measured on parameterized quantum circuits, providing a structured and entangled encoding of information. The classical baseline uses Gaussian-sampled latent vectors projected in the same manner. The proposed variational formulation enables the generation of diverse and physically valid topologies by exploring the latent space through sampling or perturbation, in contrast to traditional optimization methods that yield a single deterministic solution. Numerical experiments show that both classical and quantum encodings produce high-quality structural designs. However, quantum encodings demonstrate advantages in several benchmark cases in terms of compliance and design diversity. These results highlight the potential of quantum circuits as an effective and scalable tool for physics-constrained topology optimization and suggest promising directions for applying near-term quantum hardware in structural design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17487v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alireza Tabarraei</dc:creator>
    </item>
    <item>
      <title>A predictor-corrector scheme for approximating signed distances using finite element methods</title>
      <link>https://arxiv.org/abs/2506.17830</link>
      <description>arXiv:2506.17830v1 Announce Type: new 
Abstract: In this article, we introduce a finite element method designed for the robust computation of approximate signed distance functions to arbitrary boundaries in two and three dimensions. Our method employs a novel prediction-correction approach, involving first the solution of a linear diffusion-based prediction problem, followed by a nonlinear minimization-based correction problem associated with the Eikonal equation. The prediction step efficiently generates a suitable initial guess, significantly facilitating convergence of the nonlinear correction step. A key strength of our approach is its ability to handle complex interfaces and initial level set functions with arbitrary steep or flat regions, a notable challenge for existing techniques. Through several representative examples, including classical geometries and more complex shapes such as star domains and three-dimensional tori, we demonstrate the accuracy, efficiency, and robustness of the method, validating its broad applicability for reinitializing diverse level set functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17830v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amina El Bachari, Johann Rannou, Vladislav A. Yastrebov, Pierre Kerfriden, Susanne Claus</dc:creator>
    </item>
    <item>
      <title>Learning from the Storm: A Multivariate Machine Learning Approach to Predicting Hurricane-Induced Economic Losses</title>
      <link>https://arxiv.org/abs/2506.17964</link>
      <description>arXiv:2506.17964v1 Announce Type: new 
Abstract: Florida is particularly vulnerable to hurricanes, which frequently cause substantial economic losses. While prior studies have explored specific contributors to hurricane-induced damage, few have developed a unified framework capable of integrating a broader range of influencing factors to comprehensively assess the sources of economic loss. In this study, we propose a comprehensive modeling framework that categorizes contributing factors into three key components: (1) hurricane characteristics, (2) water-related environmental factors, and (3) socioeconomic factors of affected areas. By integrating multi-source data and aggregating all variables at the finer spatial granularity of the ZIP Code Tabulation Area (ZCTA) level, we employ machine learning models to predict economic loss, using insurance claims as indicators of incurred damage. Beyond accurate loss prediction, our approach facilitates a systematic assessment of the relative importance of each component, providing practical guidance for disaster mitigation, risk assessment, and the development of adaptive urban strategies in coastal and storm-exposed areas. Our code is now available at: https://github.com/LabRAI/Hurricane-Induced-Economic-Loss-Prediction</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17964v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bolin Shen, Eren Erman Ozguven, Yue Zhao, Guang Wang, Yiqun Xie, Yushun Dong</dc:creator>
    </item>
    <item>
      <title>A phase field model for hydraulic fracture: Drucker-Prager driving force and a hybrid coupling strategy</title>
      <link>https://arxiv.org/abs/2506.18161</link>
      <description>arXiv:2506.18161v1 Announce Type: new 
Abstract: Recent years have seen a significant interest in using phase field approaches to model hydraulic fracture, so as to optimise a process that is key to industries such as petroleum engineering, mining and geothermal energy extraction. Here, we present a novel theoretical and computational phase field framework to simulate hydraulic fracture. The framework is general and versatile, in that it allows for improved treatments of the coupling between fluid flow and the phase field, and encompasses a universal description of the fracture driving force. Among others, this allows us to bring two innovations to the phase field hydraulic fracture community: (i) a new hybrid coupling approach to handle the fracture-fluid flow interplay, offering enhanced accuracy and flexibility; and (ii) a Drucker-Prager-based strain energy decomposition, extending the simulation of hydraulic fracture to materials exhibiting asymmetric tension-compression fracture behaviour (such as shale rocks) and enabling the prediction of geomechanical phenomena such as fault reactivation and stick-slip behaviour. Four case studies are addressed to illustrate these additional modelling capabilities and bring insight into permeability coupling, cracking behaviour, and multiaxial conditions in hydraulic fracturing simulations. The codes developed are made freely available to the community and can be downloaded from {https://mechmat.web.ox.ac.uk/</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18161v1</guid>
      <category>cs.CE</category>
      <category>physics.app-ph</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Y. Navidtehrani, C. Beteg\'on, J. Vallejos, E. Mart\'inez-Pa\~neda</dc:creator>
    </item>
    <item>
      <title>Measuring Fractal Dimension using Discrete Global Grid Systems</title>
      <link>https://arxiv.org/abs/2506.18175</link>
      <description>arXiv:2506.18175v1 Announce Type: new 
Abstract: This study builds a bridge between two well-studied but distant topics: fractal dimension and Discrete Global Grid System (DGGS). DGGSs are used as covering sets for geospatial vector data to calculate the Minkowski-Bouligand dimension. Using the method on synthetic data yields results within 1% of their theoretical fractal dimensions. A case study on opaque cloud fields obtained from satellite images gives fractal dimension in agreement with that available in the literature. The proposed method alleviates the problems of arbitrary grid placement and orientation, as well as the progression of cell sizes of the covering sets for geospatial data. Using DGGSs further ensure that intersections of the covering sets with the geospatial vector having large geographic extents are calculated by taking the curvature of the earth into account. This paper establishes the validity of DGGSs as covering sets theoretically and discusses desirable properties of DGGSs suitable for this purpose.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18175v1</guid>
      <category>cs.CE</category>
      <category>math.DS</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Pramit Ghosh</dc:creator>
    </item>
    <item>
      <title>Conservative data-driven finite element formulation</title>
      <link>https://arxiv.org/abs/2506.18206</link>
      <description>arXiv:2506.18206v1 Announce Type: new 
Abstract: This paper presents a new data-driven finite element framework derived with mixed finite element formulation. The standard approach to diffusion problems requires the solution of the mathematical equations that describe both the conservation law and the constitutive relations, where the latter is traditionally obtained after fitting experimental data to simplified material models. To exploit all available information and avoid bias in the material model, we follow a data-driven approach. While the conservation laws and boundary conditions are satisfied by means of the finite element method, the experimental data is used directly in the numerical simulations, avoiding the need of fitting material model parameters. In order to satisfy the conservation law a priori in the strong sense, we introduce a mixed finite element formulation. This relaxes the regularity requirements on approximation spaces while enforcing continuity of the normal flux component across all of the inner boundaries. This weaker mixed formulation provides a posteriori error indicators tailored for this data-driven approach, enabling adaptive hp-refinement. The relaxed regularity of the approximation spaces makes it easier to observe how the variation in the datasets results in the non-uniqueness of the solution, which can be quantified to predict the uncertainty of the results. The capabilities of the formulation are demonstrated in an example of the nonlinear heat transfer in nuclear graphite using synthetically generated material datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18206v1</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adriana Kulikov\'a (Glasgow Computational Engineering Centre), Andrei G. Shvarts (Glasgow Computational Engineering Centre), {\L}ukasz Kaczmarczyk (Glasgow Computational Engineering Centre), Chris J. Pearce (Glasgow Computational Engineering Centre)</dc:creator>
    </item>
    <item>
      <title>Exact Conditional Score-Guided Generative Modeling for Amortized Inference in Uncertainty Quantification</title>
      <link>https://arxiv.org/abs/2506.18227</link>
      <description>arXiv:2506.18227v1 Announce Type: new 
Abstract: We propose an efficient framework for amortized conditional inference by leveraging exact conditional score-guided diffusion models to train a non-reversible neural network as a conditional generative model. Traditional normalizing flow methods require reversible architectures, which can limit their expressiveness and efficiency. Although diffusion models offer greater flexibility, they often suffer from high computational costs during inference. To combine the strengths of both approaches, we introduce a two-stage method. First, we construct a training-free conditional diffusion model by analytically deriving an exact score function under a Gaussian mixture prior formed from samples of the underlying joint distribution. This exact conditional score model allows us to efficiently generate noise-labeled data, consisting of initial diffusion Gaussian noise and posterior samples conditioned on various observation values, by solving a reverse-time ordinary differential equation. Second, we use this noise-labeled data to train a feedforward neural network that maps noise and observations directly to posterior samples, eliminating the need for reversibility or iterative sampling at inference time. The resulting model provides fast, accurate, and scalable conditional sampling for high-dimensional and multi-modal posterior distributions, making it well-suited for uncertainty quantification tasks, e.g., parameter estimation of complex physical systems. We demonstrate the effectiveness of our approach through a series of numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18227v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zezhong Zhang, Caroline Tatsuoka, Dongbin Xiu, Guannan Zhang</dc:creator>
    </item>
    <item>
      <title>Neural-operator element method: Efficient and scalable finite element method enabled by reusable neural operators</title>
      <link>https://arxiv.org/abs/2506.18427</link>
      <description>arXiv:2506.18427v1 Announce Type: new 
Abstract: The finite element method (FEM) is a well-established numerical method for solving partial differential equations (PDEs). However, its mesh-based nature gives rise to substantial computational costs, especially for complex multiscale simulations. Emerging machine learning-based methods (e.g., neural operators) provide data-driven solutions to PDEs, yet they present challenges, including high training cost and low model reusability. Here, we propose the neural-operator element method (NOEM) by synergistically combining FEM with operator learning to address these challenges. NOEM leverages neural operators (NOs) to simulate subdomains where a large number of finite elements would be required if FEM was used. In each subdomain, an NO is used to build a single element, namely a neural-operator element (NOE). NOEs are then integrated with standard finite elements to represent the entire solution through the variational framework. Thereby, NOEM does not necessitate dense meshing and offers efficient simulations. We demonstrate the accuracy, efficiency, and scalability of NOEM by performing extensive and systematic numerical experiments, including nonlinear PDEs, multiscale problems, PDEs on complex geometries, and discontinuous coefficient fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18427v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weihang Ouyang, Yeonjong Shin, Si-Wei Liu, Lu Lu</dc:creator>
    </item>
    <item>
      <title>Virtual failure assessment diagrams for hydrogen transmission pipelines</title>
      <link>https://arxiv.org/abs/2506.18554</link>
      <description>arXiv:2506.18554v1 Announce Type: new 
Abstract: We combine state-of-the-art thermo-metallurgical welding process modelling with coupled diffusion-elastic-plastic phase field fracture simulations to predict the failure states of hydrogen transport pipelines. This enables quantitatively resolving residual stress states and the role of brittle, hard regions of the weld such as the heat affected zone (HAZ). Failure pressures can be efficiently quantified as a function of asset state (existing defects), materials and weld procedures adopted, and hydrogen purity. Importantly, simulations spanning numerous relevant conditions (defect size and orientations) are used to build \emph{Virtual} Failure Assessment Diagrams (FADs), enabling a straightforward uptake of this mechanistic approach in fitness-for-service assessment. Model predictions are in very good agreement with FAD approaches from the standards but show that the latter are not conservative when resolving the heterogeneous nature of the weld microstructure. Appropriate, \emph{mechanistic} FAD safety factors are established that account for the role of residual stresses and hard, brittle weld regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18554v1</guid>
      <category>cs.CE</category>
      <category>cond-mat.mtrl-sci</category>
      <category>physics.app-ph</category>
      <category>physics.chem-ph</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J. Wijnen, J. Parker, M. Gagliano, E. Mart\'inez-Pa\~neda</dc:creator>
    </item>
    <item>
      <title>A Physics-Informed Neural Network Framework for Simulating Creep Buckling in Growing Viscoelastic Biological Tissues</title>
      <link>https://arxiv.org/abs/2506.18565</link>
      <description>arXiv:2506.18565v1 Announce Type: new 
Abstract: Modeling viscoelastic behavior is crucial in engineering and biomechanics, where materials undergo time-dependent deformations, including stress relaxation, creep buckling and biological tissue development. Traditional numerical methods, like the finite element method, often require explicit meshing, artificial perturbations or embedding customised programs to capture these phenomena, adding computational complexity. In this study, we develop an energy-based physics-informed neural network (PINN) framework using an incremental approach to model viscoelastic creep, stress relaxation, buckling, and growth-induced morphogenesis. Physics consistency is ensured by training neural networks to minimize the systems potential energy functional, implicitly satisfying equilibrium and constitutive laws. We demonstrate that this framework can naturally capture creep buckling without pre-imposed imperfections, leveraging inherent training dynamics to trigger instabilities. Furthermore, we extend our framework to biological tissue growth and morphogenesis, predicting both uniform expansion and differential growth-induced buckling in cylindrical structures. Results show that the energy-based PINN effectively predicts viscoelastic instabilities, post-buckling evolution and tissue morphological evolution, offering a promising alternative to traditional methods. This study demonstrates that PINN can be a flexible robust tool for modeling complex, time-dependent material behavior, opening possible applications in structural engineering, soft materials, and tissue development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18565v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongya Lin, Jinshuai Bai, Shuang Li, Xindong Chen, Bo Li, Xi-Qiao Feng</dc:creator>
    </item>
    <item>
      <title>Communication Architecture for Autonomous Power-to-X Platforms: Enhancing Inspection and Operation With Legged Robots and 5G</title>
      <link>https://arxiv.org/abs/2506.18572</link>
      <description>arXiv:2506.18572v1 Announce Type: new 
Abstract: Inspection and maintenance of offshore platforms are associated with high costs, primarily due to the significant personnel requirements and challenging operational conditions. This paper first presents a classification of Power to X platforms. Building upon this foundation, a communication architecture is proposed to enable monitoring, control, and teleoperation for a Power to X platform. To reduce the demand for human labor, a robotic system is integrated to autonomously perform inspection and maintenance tasks. The implementation utilizes a quadruped robot. Remote monitoring, control, and teleoperation of the robot are analyzed within the context of a 5G standalone network. As part of the evaluation, aspects such as availability and latency are recorded, compared, and critically assessed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18572v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Frank, Falk Dettinger, Daniel Dittler, Pascal H\"abig, Nasser Jazdi, Kai Hufendiek, Michael Weyrich</dc:creator>
    </item>
    <item>
      <title>A Study of Dynamic Stock Relationship Modeling and S&amp;P500 Price Forecasting Based on Differential Graph Transformer</title>
      <link>https://arxiv.org/abs/2506.18717</link>
      <description>arXiv:2506.18717v1 Announce Type: new 
Abstract: Stock price prediction is vital for investment decisions and risk management, yet remains challenging due to markets' nonlinear dynamics and time-varying inter-stock correlations. Traditional static-correlation models fail to capture evolving stock relationships. To address this, we propose a Differential Graph Transformer (DGT) framework for dynamic relationship modeling and price prediction. Our DGT integrates sequential graph structure changes into multi-head self-attention via a differential graph mechanism, adaptively preserving high-value connections while suppressing noise. Causal temporal attention captures global/local dependencies in price sequences. We further evaluate correlation metrics (Pearson, Mutual Information, Spearman, Kendall's Tau) across global/local/dual scopes as spatial-attention priors. Using 10 years of S&amp;P 500 closing prices (z-score normalized; 64-day sliding windows), DGT with spatial priors outperformed GRU baselines (RMSE: 0.24 vs. 0.87). Kendall's Tau global matrices yielded optimal results (MAE: 0.11). K-means clustering revealed "high-volatility growth" and "defensive blue-chip" stocks, with the latter showing lower errors (RMSE: 0.13) due to stable correlations. Kendall's Tau and Mutual Information excelled in volatile sectors. This study innovatively combines differential graph structures with Transformers, validating dynamic relationship modeling and identifying optimal correlation metrics/scopes. Clustering analysis supports tailored quantitative strategies. Our framework advances financial time-series prediction through dynamic modeling and cross-asset interaction analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18717v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linyue Hu, Qi Wang</dc:creator>
    </item>
    <item>
      <title>Towards Real-time Structural Dynamics Simulation with Graph-based Digital Twin Modelling</title>
      <link>https://arxiv.org/abs/2506.18724</link>
      <description>arXiv:2506.18724v1 Announce Type: new 
Abstract: Precise and timely simulation of a structure's dynamic behavior is crucial for evaluating its performance and assessing its health status. Traditional numerical methods are often limited by high computational costs and low efficiency, while deep learning approaches offer a promising alternative. However, these data-driven methods still face challenges, such as limited physical interpretability and difficulty in adapting to diverse structural configurations. To address these issues, this study proposes a graph-based digital twin modelling (GDTM) framework to simulate structural dynamic responses across various spatial topologies. In this framework, the adjacency matrix explicitly represents the spatial relationships between structural vertices, enhancing the model's physical interpretability. The effectiveness of the proposed framework was validated through comprehensive numerical and experimental studies. The results demonstrate that the framework accurately simulated structural dynamics across different topological configurations, with Normalized Mean-Squared Error (NMSE) values consistently below 0.005 in numerical simulations and 0.0015 in experimental validations. Furthermore, the framework achieved over 80-fold improvements in computational efficiency compared to traditional finite element methods (FEM). This research promotes the practical application of graph-based structural dynamics modelling, which has the potential to significantly advance structural performance evaluation and health monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18724v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jun Zhang, Tong Zhang, Ying Wang</dc:creator>
    </item>
    <item>
      <title>Skeletal Reaction Models for Gasoline Surrogate Combustion</title>
      <link>https://arxiv.org/abs/2506.18853</link>
      <description>arXiv:2506.18853v1 Announce Type: new 
Abstract: Skeletal reaction models are derived for a four-component gasoline surrogate model via an instantaneous local sensitivity analysis technique. The sensitivities of the species mass fractions and the temperature with respect to the reaction rates are estimated by a reduced-order modeling (ROM) methodology. Termed "implicit time-dependent basis CUR (implicit TDB-CUR)," this methodology is based on the CUR matrix decomposition and incorporates implicit time integration for evolving the bases. The estimated sensitivities are subsequently analyzed to develop skeletal reaction models with a fully automated procedure. The 1389-species gasoline surrogate model developed at Lawrence Livermore National Laboratory (LLNL) is selected as the detailed kinetics model. The skeletal reduction procedure is applied to this model in a zero-dimensional constant-pressure reactor over a wide range of initial conditions. The performances of the resulting skeletal models are appraised by comparison against the results via the LLNL detailed model, and also predictions via other skeletal models. Two new skeletal models are developed consisting of 679 and 494 species, respectively. The first is an alternative to an existing model with the same number of species. The predictions with this model reproduces the detailed models vital flame results with less than 1% errors. The errors via the second model are less than 10%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18853v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yinmin Liu, Hessam Babaee, Peyman Givi, Daniel Livescu, Arash Nouri</dc:creator>
    </item>
    <item>
      <title>Design, Implementation, and Analysis of Fair Faucets for Blockchain Ecosystems</title>
      <link>https://arxiv.org/abs/2506.17236</link>
      <description>arXiv:2506.17236v1 Announce Type: cross 
Abstract: The present dissertation addresses the problem of fairly distributing shared resources in non-commercial blockchain networks. Blockchains are distributed systems that order and timestamp records of a given network of users, in a public, cryptographically secure, and consensual way. The records, which may in kind be events, transaction orders, sets of rules for structured transactions etc. are placed within well-defined datastructures called blocks, and they are linked to each other by the virtue of cryptographic pointers, in a total ordering which represents their temporal relations of succession. The ability to operate on the blockchain, and/or to contribute a record to the content of a block are shared resources of the blockchain systems. In commercial networks, these resources are exchanged in return for fiat money, and consequently, fairness is not a relevant problem in terms of computer engineering. In non-commercial networks, however, monetary solutions are not available, by definition. The present non-commercial blockchain networks employ trivial distribution mechanisms called faucets, which offer fixed amounts of free tokens (called cryptocurrencies) specific to the given network. This mechanism, although simple and efficient, is prone to denial of service (DoS) attacks and cannot address the fairness problem. In the present dissertation, the faucet mechanism is adapted for fair distribution, in line with Max-min Fairness scheme. In total, we contributed 6 distinct Max-min Fair algorithms as efficient blockchain faucets. The algorithms we contribute are resistant to DoS attacks, low-cost in terms of blockchain computation economics, and they also allow for different user weighting policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17236v1</guid>
      <category>cs.CR</category>
      <category>cs.CE</category>
      <category>cs.DC</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Serdar Metin</dc:creator>
    </item>
    <item>
      <title>Transformers Beyond Order: A Chaos-Markov-Gaussian Framework for Short-Term Sentiment Forecasting of Any Financial OHLC timeseries Data</title>
      <link>https://arxiv.org/abs/2506.17244</link>
      <description>arXiv:2506.17244v1 Announce Type: cross 
Abstract: Short-term sentiment forecasting in financial markets (e.g., stocks, indices) is challenging due to volatility, non-linearity, and noise in OHLC (Open, High, Low, Close) data. This paper introduces a novel CMG (Chaos-Markov-Gaussian) framework that integrates chaos theory, Markov property, and Gaussian processes to improve prediction accuracy. Chaos theory captures nonlinear dynamics; the Markov chain models regime shifts; Gaussian processes add probabilistic reasoning. We enhance the framework with transformer-based deep learning models to capture temporal patterns efficiently. The CMG Framework is designed for fast, resource-efficient, and accurate forecasting of any financial instrument's OHLC time series. Unlike traditional models that require heavy infrastructure and instrument-specific tuning, CMG reduces overhead and generalizes well. We evaluate the framework on market indices, forecasting sentiment for the next trading day's first quarter. A comparative study against statistical, ML, and DL baselines trained on the same dataset with no feature engineering shows CMG consistently outperforms in accuracy and efficiency, making it valuable for analysts and financial institutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17244v1</guid>
      <category>q-fin.ST</category>
      <category>cs.CE</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arif Pathan</dc:creator>
    </item>
    <item>
      <title>Pix2Geomodel: A Next-Generation Reservoir Geomodeling with Property-to-Property Translation</title>
      <link>https://arxiv.org/abs/2506.17747</link>
      <description>arXiv:2506.17747v1 Announce Type: cross 
Abstract: Accurate geological modeling is critical for reservoir characterization, yet traditional methods struggle with complex subsurface heterogeneity, and they have problems with conditioning to observed data. This study introduces Pix2Geomodel, a novel conditional generative adversarial network (cGAN) framework based on Pix2Pix, designed to predict reservoir properties (facies, porosity, permeability, and water saturation) from the Rotliegend reservoir of the Groningen gas field. Utilizing a 7.6 million-cell dataset from the Nederlandse Aardolie Maatschappij, accessed via EPOS-NL, the methodology included data preprocessing, augmentation to generate 2,350 images per property, and training with a U-Net generator and PatchGAN discriminator over 19,000 steps. Evaluation metrics include pixel accuracy (PA), mean intersection over union (mIoU), frequency weighted intersection over union (FWIoU), and visualizations assessed performance in masked property prediction and property-to-property translation tasks. Results demonstrated high accuracy for facies (PA 0.88, FWIoU 0.85) and water saturation (PA 0.96, FWIoU 0.95), with moderate success for porosity (PA 0.70, FWIoU 0.55) and permeability (PA 0.74, FWIoU 0.60), and robust translation performance (e.g., facies-to-facies PA 0.98, FWIoU 0.97). The framework captured spatial variability and geological realism, as validated by variogram analysis, and calculated the training loss curves for the generator and discriminator for each property. Compared to traditional methods, Pix2Geomodel offers enhanced fidelity in direct property mapping. Limitations include challenges with microstructural variability and 2D constraints, suggesting future integration of multi-modal data and 3D modeling (Pix2Geomodel v2.0). This study advances the application of generative AI in geoscience, supporting improved reservoir management and open science initiatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17747v1</guid>
      <category>physics.geo-ph</category>
      <category>cs.CE</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Abdulrahman Al-Fakih, Ardiansyah Koeshidayatullah, Nabil A. Saraih, Tapan Mukerji, Rayan Kanfar, Abdulmohsen Alali, SanLinn I. Kaka</dc:creator>
    </item>
    <item>
      <title>Six Decades Post-Discovery of Taylor's Power Law: From Ecological and Statistical Universality, Through Prime Number Distributions and Tipping-Point Signals, to Heterogeneity and Stability of Complex Networks</title>
      <link>https://arxiv.org/abs/2506.18154</link>
      <description>arXiv:2506.18154v1 Announce Type: cross 
Abstract: First discovered by L. R. Taylor (1961, Nature), Taylor's Power Law (TPL) correlates the mean (M) population abundances and the corresponding variances (V) across a set of insect populations using a power function (V=aM^b). TPL has demonstrated its 'universality' across numerous fields of sciences, social sciences, and humanities. This universality has inspired two main prongs of exploration: one from mathematicians and statisticians, who might instinctively respond with a convergence theorem similar to the central limit theorem of the Gaussian distribution, and another from biologists, ecologists, physicists, etc., who are more interested in potential underlying ecological or organizational mechanisms. Over the past six decades, TPL studies have produced a punctuated landscape with three relatively distinct periods (1960s-1980s; 1990s-2000s, and 2010s-2020s) across the two prongs of abstract and physical worlds. Eight themes have been identified and reviewed on this landscape, including population spatial aggregation and ecological mechanisms, TPL and skewed statistical distributions, mathematical/statistical mechanisms of TPL, sample vs. population TPL, population stability, synchrony, and early warning signals for tipping points, TPL on complex networks, TPL in macrobiomes, and in microbiomes. Three future research directions including fostering reciprocal interactions between the two prongs, heterogeneity measuring, and exploration in the context of evolution. The significance of TPL research includes practically, population fluctuations captured by TPL are relevant for agriculture, forestry, fishery, wildlife-conservation, epidemiology, tumor heterogeneity, earthquakes, social inequality, stock illiquidity, financial stability, tipping point events, etc.; theoretically, TPL is one form of power laws, which are related to phase transitions, universality, scale-invariance, etc.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18154v1</guid>
      <category>q-bio.OT</category>
      <category>cs.CE</category>
      <category>cs.IT</category>
      <category>cs.SI</category>
      <category>math.IT</category>
      <category>q-bio.PE</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator> Zhanshan (Sam),  Ma, R. A. J. Taylor</dc:creator>
    </item>
    <item>
      <title>Airalogy: AI-empowered universal data digitization for research automation</title>
      <link>https://arxiv.org/abs/2506.18586</link>
      <description>arXiv:2506.18586v1 Announce Type: cross 
Abstract: Research data are the foundation of Artificial Intelligence (AI)-driven science, yet current AI applications remain limited to a few fields with readily available, well-structured, digitized datasets. Achieving comprehensive AI empowerment across multiple disciplines is still out of reach. Present-day research data collection is often fragmented, lacking unified standards, inefficiently managed, and difficult to share. Creating a single platform for standardized data digitization needs to overcome the inherent challenge of balancing between universality (supporting the diverse, ever-evolving needs of various disciplines) and standardization (enforcing consistent formats to fully enable AI). No existing platform accommodates both facets. Building a truly multidisciplinary platform requires integrating scientific domain knowledge with sophisticated computing skills. Researchers often lack the computational expertise to design customized and standardized data recording methods, whereas platform developers rarely grasp the intricate needs of multiple scientific domains. These gaps impede research data standardization and hamper AI-driven progress. In this study, we address these challenges by developing Airalogy (https://airalogy.com), the world's first AI- and community-driven platform that balances universality and standardization for digitizing research data across multiple disciplines. Airalogy represents entire research workflows using customizable, standardized data records and offers an advanced AI research copilot for intelligent Q&amp;A, automated data entry, analysis, and research automation. Already deployed in laboratories across all four schools of Westlake University, Airalogy has the potential to accelerate and automate scientific innovation in universities, industry, and the global research community-ultimately benefiting humanity as a whole.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18586v1</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zijie Yang, Qiji Zhou, Fang Guo, Sijie Zhang, Yexun Xi, Jinglei Nie, Yudian Zhu, Liping Huang, Chou Wu, Yonghe Xia, Xiaoyu Ma, Yingming Pu, Panzhong Lu, Junshu Pan, Mingtao Chen, Tiannan Guo, Yanmei Dou, Hongyu Chen, Anping Zeng, Jiaxing Huang, Tian Xu, Yue Zhang</dc:creator>
    </item>
    <item>
      <title>Energy-consistent integration of mechanical systems based on Livens principle</title>
      <link>https://arxiv.org/abs/2312.02825</link>
      <description>arXiv:2312.02825v3 Announce Type: replace 
Abstract: In this work we make use of Livens principle (sometimes also referred to as Hamilton-Pontryagin principle) in order to obtain a novel structure-preserving integrator for mechanical systems. In contrast to the canonical Hamiltonian equations of motion, the Euler-Lagrange equations pertaining to Livens principle circumvent the need to invert the mass matrix. This is an essential advantage with respect to singular mass matrices, which can yield severe difficulties for the modelling and simulation of multibody systems. Moreover, Livens principle unifies both Lagrangian and Hamiltonian viewpoints on mechanics. Additionally, the present framework avoids the need to set up the system's Hamiltonian. The novel scheme algorithmically conserves a general energy function and aims at the preservation of momentum maps corresponding to symmetries of the system. We present an extension to mechanical systems subject to holonomic constraints. The performance of the newly devised method is studied in representative examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02825v3</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11044-024-10001-9</arxiv:DOI>
      <arxiv:journal_reference>Multibody Syst Dyn 63, 303-340 (2025)</arxiv:journal_reference>
      <dc:creator>Philipp L. Kinon, Peter Betsch</dc:creator>
    </item>
    <item>
      <title>Embedded Model Form Uncertainty Quantification with Measurement Noise for Bayesian Model Calibration</title>
      <link>https://arxiv.org/abs/2410.12037</link>
      <description>arXiv:2410.12037v2 Announce Type: replace 
Abstract: A key factor in ensuring the accuracy of computer simulations that model physical systems is the proper calibration of their parameters based on real-world observations or experimental data. Inevitably, uncertainties arise, and Bayesian methods provide a robust framework for quantifying and propagating these uncertainties to model predictions. Nevertheless, Bayesian methods paired with inexact models usually produce predictions unable to represent the observed datapoints. Additionally, the quantified uncertainties of these overconfident models cannot be propagated to other Quantities of Interest (QoIs) reliably. A promising solution involves embedding a model inadequacy term in the inference parameters, allowing the quantified model form uncertainty to influence non-observed QoIs. This paper introduces a more interpretable framework for embedding the model inadequacy compared to existing methods. To overcome the limitations of current approaches, we adapt the existing likelihood models to properly account for noise in the measurements and propose two new formulations designed to address their shortcomings. Moreover, we evaluate the performance of this inadequacy-embedding approach in the presence of discrepancies between measurements and model predictions, including noise and outliers. Particular attention is given to how the uncertainty associated with the model inadequacy term propagates to the QoIs, enabling a more comprehensive statistical analysis of prediction's reliability. Finally, the proposed approach is applied to estimate the uncertainty in the predicted heat flux from a transient thermal simulation using temperature observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12037v2</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Andr\'es Arcones, Martin Weiser, Phaedon-Stelios Koutsourelakis, J\"org F. Unger</dc:creator>
    </item>
    <item>
      <title>Resilience-based post disaster recovery optimization for infrastructure system via Deep Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2410.18577</link>
      <description>arXiv:2410.18577v2 Announce Type: replace 
Abstract: Infrastructure systems are critical in modern communities but are highly susceptible to various natural and man-made disasters. Efficient post-disaster recovery requires repair-scheduling approaches under the limitation of capped resources that need to be shared across the system. Existing approaches, including component ranking methods, greedy evolutionary algorithms, and data-driven machine learning models, face various limitations when tested within such a context. To tackle these issues, we propose a novel approach to optimize post-disaster recovery of infrastructure systems by leveraging Deep Reinforcement Learning (DRL) methods and incorporating a specialized resilience metric to lead the optimization. The system topology is represented adopting a graph-based structure, where the system's recovery process is formulated as a sequential decision-making problem. Deep Q-learning algorithms are employed to learn optimal recovery strategies by mapping system states to specific actions, as for instance which component ought to be repaired next, with the goal of maximizing long-term recovery from a resilience-oriented perspective. To demonstrate the efficacy of our proposed approach, we implement this scheme on the example of post-earthquake recovery optimization for an electrical substation system. We assess different deep Q-learning algorithms to this end, namely vanilla Deep Q-Networks (DQN), Double DQN(DDQN), Duel DQN, and duel DDQN, demonstrating superiority of the DDQN for the considered problem. A further comparative analysis against baseline methods during testing reveals the superior performance of the proposed method in terms of both optimization effect and computational cost, rendering this an attractive approach in the context of resilience enhancement and rapid response and recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18577v2</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huangbin Liang, Beatriz Moya, Francisco Chinesta, Eleni Chatzi</dc:creator>
    </item>
    <item>
      <title>Rethinking Cancer Gene Identification through Graph Anomaly Analysis</title>
      <link>https://arxiv.org/abs/2412.17240</link>
      <description>arXiv:2412.17240v2 Announce Type: replace 
Abstract: Graph neural networks (GNNs) have shown promise in integrating protein-protein interaction (PPI) networks for identifying cancer genes in recent studies. However, due to the insufficient modeling of the biological information in PPI networks, more faithfully depiction of complex protein interaction patterns for cancer genes within the graph structure remains largely unexplored. This study takes a pioneering step toward bridging biological anomalies in protein interactions caused by cancer genes to statistical graph anomaly. We find a unique graph anomaly exhibited by cancer genes, namely weight heterogeneity, which manifests as significantly higher variance in edge weights of cancer gene nodes within the graph. Additionally, from the spectral perspective, we demonstrate that the weight heterogeneity could lead to the "flattening out" of spectral energy, with a concentration towards the extremes of the spectrum. Building on these insights, we propose the HIerarchical-Perspective Graph Neural Network (HIPGNN) that not only determines spectral energy distribution variations on the spectral perspective, but also perceives detailed protein interaction context on the spatial perspective. Extensive experiments are conducted on two reprocessed datasets STRINGdb and CPDB, and the experimental results demonstrate the superiority of HIPGNN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17240v2</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1609/aaai.v39i12.33436</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the AAAI Conference on Artificial Intelligence. 2025, 39(12): 13161-13169</arxiv:journal_reference>
      <dc:creator>Yilong Zang, Lingfei Ren, Yue Li, Zhikang Wang, David Antony Selby, Zheng Wang, Sebastian Josef Vollmer, Hongzhi Yin, Jiangning Song, Junhang Wu</dc:creator>
    </item>
    <item>
      <title>Mechanics Informatics: A paradigm for efficiently learning constitutive models</title>
      <link>https://arxiv.org/abs/2501.08314</link>
      <description>arXiv:2501.08314v2 Announce Type: replace 
Abstract: Efficient and accurate learning of constitutive laws is crucial for accurately predicting the mechanical behavior of materials under complex loading conditions. Accurate model calibration hinges on a delicate interplay between the information embedded in experimental data and the parameters that define our constitutive models.The information encoded in the parameters of the constitutive model must be complemented by the information in the data used for calibration. This interplay raises fundamental questions: How can we quantify the information content of test data? How much information does a single test convey? Also, how much information is required to accurately learn a constitutive model? To address these questions, we introduce mechanics informatics, a paradigm for efficient and accurate constitutive model learning. At its core is the stress state entropy, a metric for quantifying the information content of experimental data. Using this framework, we analyzed specimen geometries with varying information content for learning an anisotropic inelastic law. Specimens with limited information enabled accurate identification of a few parameters sensitive to the information in the data. Furthermore, we optimized specimen design by incorporating stress state entropy into a Bayesian optimization scheme. This led to the design of cruciform specimens with maximized entropy for accurate parameter identification. Conversely, minimizing entropy in Peirs shear specimens yielded a uniform shear stress state, showcasing the framework's flexibility in tailoring designs for specific experimental goals. Finally, we addressed experimental uncertainties, demonstrated the potential of transfer learning for replacing challenging testing protocols with simpler alternatives, and extension of the framework to different material laws.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08314v2</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jmps.2025.106239</arxiv:DOI>
      <dc:creator>Royal C. Ihuaenyi, Wei Li, Martin Z. Bazant, Juner Zhu</dc:creator>
    </item>
    <item>
      <title>Graph Neural Networks in Supply Chain Analytics and Optimization: Concepts, Perspectives, Dataset and Benchmarks</title>
      <link>https://arxiv.org/abs/2411.08550</link>
      <description>arXiv:2411.08550v2 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) have recently gained traction in transportation, bioinformatics, language and image processing, but research on their application to supply chain management remains limited. Supply chains are inherently graph-like, making them ideal for GNN methodologies, which can optimize and solve complex problems. The barriers include a lack of proper conceptual foundations, familiarity with graph applications in SCM, and real-world benchmark datasets for GNN-based supply chain research. To address this, we discuss and connect supply chains with graph structures for effective GNN application, providing detailed formulations, examples, mathematical definitions, and task guidelines. Additionally, we present a multi-perspective real-world benchmark dataset from a leading FMCG company in Bangladesh, focusing on supply chain planning. We discuss various supply chain tasks using GNNs and benchmark several state-of-the-art models on homogeneous and heterogeneous graphs across six supply chain analytics tasks. Our analysis shows that GNN-based models consistently outperform statistical Machine Learning and other Deep Learning models by around 10-30% in regression, 10-30% in classification and detection tasks, and 15-40% in anomaly detection tasks on designated metrics. With this work, we lay the groundwork for solving supply chain problems using GNNs, supported by conceptual discussions, methodological insights, and a comprehensive dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08550v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Azmine Toushik Wasi, MD Shafikul Islam, Adipto Raihan Akib, Mahathir Mohammad Bappy</dc:creator>
    </item>
    <item>
      <title>Evolutionary Optimization of Physics-Informed Neural Networks: Evo-PINN Frontiers and Opportunities</title>
      <link>https://arxiv.org/abs/2501.06572</link>
      <description>arXiv:2501.06572v3 Announce Type: replace-cross 
Abstract: Deep learning models trained on finite data lack a complete understanding of the physical world. On the other hand, physics-informed neural networks (PINNs) are infused with such knowledge through the incorporation of mathematically expressible laws of nature into their training loss function. By complying with physical laws, PINNs provide advantages over purely data-driven models in limited-data regimes and present as a promising route towards Physical AI. This feature has propelled them to the forefront of scientific machine learning, a domain characterized by scarce and costly data. However, the vision of accurate physics-informed learning comes with significant challenges. This work examines PINNs for the first time in terms of model optimization and generalization, shedding light on the need for new algorithmic advances to overcome issues pertaining to the training speed, precision, and generalizability of today's PINN models. Of particular interest are gradient-free evolutionary algorithms (EAs) for optimizing the uniquely complex loss landscapes arising in PINN training. Methods synergizing gradient descent and EAs for discovering bespoke neural architectures and balancing multiple terms in physics-informed learning objectives are positioned as important avenues for future research. Another exciting track is to cast evolutionary as a meta-learner of generalizable PINN models. To substantiate these proposed avenues, we further highlight results from recent literature to showcase the early success of such approaches in addressing the aforementioned challenges in PINN optimization and generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06572v3</guid>
      <category>cs.NE</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jian Cheng Wong, Abhishek Gupta, Chin Chun Ooi, Pao-Hsiung Chiu, Jiao Liu, Yew-Soon Ong</dc:creator>
    </item>
    <item>
      <title>PC-SRGAN: Physically Consistent Super-Resolution Generative Adversarial Network for General Transient Simulations</title>
      <link>https://arxiv.org/abs/2505.06502</link>
      <description>arXiv:2505.06502v2 Announce Type: replace-cross 
Abstract: Machine Learning, particularly Generative Adversarial Networks (GANs), has revolutionised Super Resolution (SR). However, generated images often lack physical meaningfulness, which is essential for scientific applications. Our approach, PC-SRGAN, enhances image resolution while ensuring physical consistency for interpretable simulations. PC-SRGAN significantly improves both the Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure compared to conventional methods, even with limited training data (e.g., only 13% of training data required for SRGAN). Beyond SR, PC-SRGAN augments physically meaningful machine learning, incorporating numerically justified time integrators and advanced quality metrics. These advancements promise reliable and causal machine-learning models in scientific domains. A significant advantage of PC-SRGAN over conventional SR techniques is its physical consistency, which makes it a viable surrogate model for time-dependent problems. PC-SRGAN advances scientific machine learning, offering improved accuracy and efficiency for image processing, enhanced process understanding, and broader applications to scientific research. We publicly release the complete source code at https://github.com/hasan-rakibul/PC-SRGAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06502v2</guid>
      <category>eess.IV</category>
      <category>cs.CE</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Rakibul Hasan, Pouria Behnoudfar, Dan MacKinlay, Thomas Poulet</dc:creator>
    </item>
    <item>
      <title>Accurate and scalable exchange-correlation with deep learning</title>
      <link>https://arxiv.org/abs/2506.14665</link>
      <description>arXiv:2506.14665v3 Announce Type: replace-cross 
Abstract: Density Functional Theory (DFT) is the most widely used electronic structure method for predicting the properties of molecules and materials. Although DFT is, in principle, an exact reformulation of the Schr\"odinger equation, practical applications rely on approximations to the unknown exchange-correlation (XC) functional. Most existing XC functionals are constructed using a limited set of increasingly complex, hand-crafted features that improve accuracy at the expense of computational efficiency. Yet, no current approximation achieves the accuracy and generality for predictive modeling of laboratory experiments at chemical accuracy -- typically defined as errors below 1 kcal/mol. In this work, we present Skala, a modern deep learning-based XC functional that bypasses expensive hand-designed features by learning representations directly from data. Skala achieves chemical accuracy for atomization energies of small molecules while retaining the computational efficiency typical of semi-local DFT. This performance is enabled by training on an unprecedented volume of high-accuracy reference data generated using computationally intensive wavefunction-based methods. Notably, Skala systematically improves with additional training data covering diverse chemistry. By incorporating a modest amount of additional high-accuracy data tailored to chemistry beyond atomization energies, Skala achieves accuracy competitive with the best-performing hybrid functionals across general main group chemistry, at the cost of semi-local DFT. As the training dataset continues to expand, Skala is poised to further enhance the predictive power of first-principles simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14665v3</guid>
      <category>physics.chem-ph</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giulia Luise, Chin-Wei Huang, Thijs Vogels, Derk P. Kooi, Sebastian Ehlert, Stephanie Lanius, Klaas J. H. Giesbertz, Amir Karton, Deniz Gunceler, Megan Stanley, Wessel P. Bruinsma, Lin Huang, Xinran Wei, Jos\'e Garrido Torres, Abylay Katbashev, Rodrigo Chavez Zavaleta, B\'alint M\'at\'e, S\'ekou-Oumar Kaba, Roberto Sordillo, Yingrong Chen, David B. Williams-Young, Christopher M. Bishop, Jan Hermann, Rianne van den Berg, Paola Gori-Giorgi</dc:creator>
    </item>
  </channel>
</rss>
