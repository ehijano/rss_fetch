<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Nov 2025 05:01:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Parameter Estimation and Seasonal Modification of the Fractional Poisson Process with Application to Vorticity Extremes over the North Atlantic</title>
      <link>https://arxiv.org/abs/2511.08081</link>
      <description>arXiv:2511.08081v1 Announce Type: new 
Abstract: The fractional Poisson process (FPP) generalizes the standard Poisson process by replacing exponentially distributed return times with Mittag-Leffler distributed ones with an extra tail parameter, allowing for greater flexibility. The FPP has been applied in various fields, such as modeling occurrences of extratropical cyclones in meteorology and solar flares in physics. We propose a new estimation method for the parameters of the FPP, based on minimizing the distance between the empirical and the theoretical distribution at selected quantiles. We conduct an extensive simulation study to evaluate the advantages and limitations of the new estimation method and to compare it with several competing estimators, some of which have not yet been examined in the Mittag-Leffler setting. To enhance the applicability of the FPP in real-world scenarios, particularly in meteorology, we propose a method for incorporating seasonality into the FPP through distance-based weighting. We then analyze the return times of relative vorticity extremes in the North Atlantic-European region using our seasonal modeling approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08081v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Merle Mendel, Roland Fried</dc:creator>
    </item>
    <item>
      <title>The impact of Women's empowerment on childhood vaccination coverage in Nigeria: a spatio-temporal analysis</title>
      <link>https://arxiv.org/abs/2511.08262</link>
      <description>arXiv:2511.08262v1 Announce Type: new 
Abstract: Immunization remains one of the most effective public health interventions, substantially reducing childhood morbidity and mortality worldwide. Yet, gender disparity and women's disempowerment continue to hinder access to vaccination services in low- and middle-income countries. In Nigeria, variations in social norms and cultural values shape gender roles, limiting women's autonomy in healthcare decisions and household participation. These constraints contribute to spatial differences in immunization uptake. Using data from four waves of the Nigeria Demographic and Health Survey, we developed two empowerment indices capturing women's participation in household decision-making and their ability to decide on personal healthcare needs. A structured spatiotemporal statistical model was applied to assess how much of the observed vaccination disparities could be attributed to women's empowerment and to predict vaccination outcomes at the third administrative level. We examined five indicators: Bacillus Calmette-Guerin (BCG), zero-dose, complete DPT, MCV-1 (first dose of measles-containing vaccine), and all-basic vaccination coverage. Model validation involved comparing empirical estimates with projections at the second administrative level. Results indicate that empowerment related to household participation and healthcare autonomy generally increases vaccination uptake, though the magnitude of effects varies geographically, particularly among highly empowered women. Despite ongoing national efforts to close immunization gaps, the study highlights the need for context-specific strategies that enhance women's decision-making power and community engagement to reduce regional disparities and improve overall vaccination coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08262v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ezra Gayawan, Osafu Augustine Egbon, Edson Utazi, Jamila Abubakar Umar, Caroline Trotter</dc:creator>
    </item>
    <item>
      <title>ANOVATS: A subsampling-based test to detect differences among short time series in marine studies</title>
      <link>https://arxiv.org/abs/2511.08070</link>
      <description>arXiv:2511.08070v1 Announce Type: cross 
Abstract: Assessing marine ecosystems is important for understanding the impacts of climate change and human activity, as well as for maintaining healthy oceans and ecosystems. In marine science, it is common for biologists and geologists to identify regional differences based on expert knowledge, frequently through data visualization. However, time series data collected through surveys in marine studies typically span only a few decades, limiting the applicability of classical time series methods. Additionally, without expert knowledge, detecting significant differences becomes challenging. To address these issues, we introduce ANOVATS (ANOVA for small-sample time series data), a subsampling-based method to detect regional differences in small-sample time series data with a fixed number of groups. This method bypasses the need for spectral density estimation, which requires a large number of time points in the data. Furthermore, after detecting differences in homogeneity across all areas using the ANOVATS procedure, we devised a simple ANOVATS post hoc procedure to group the areas. Finally, we demonstrate the effectiveness of our method by analyzing zooplankton biomass data collected in different strata of the North Sea, showing its ability to quantify differences in species between geographical areas without relying on prior biological or geographical knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08070v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuichi Goto, Hiroko Kato Solvang, Masanobu Taniguchi, Tone Falkenhaug</dc:creator>
    </item>
    <item>
      <title>gemlib: Probabilistic programming for epidemic models</title>
      <link>https://arxiv.org/abs/2511.08124</link>
      <description>arXiv:2511.08124v1 Announce Type: cross 
Abstract: gemlib is a Python library for defining, simulating, and calibrating Markov state-transition models. Stochastic models are often computationally intensive, making them impractical to use in pandemic response efforts despite their favourable interpretations compared to their deterministic counterparts. gemlib decomposes state-transition models into three key ingredients which succinctly encapsulate the model and are sufficient for executing the subsequent computational routines. Simulation is performed using implementations of Gillespie's algorithm for continuous-time models and a generic Tau-leaping algorithm for discrete time models. gemlib models integrate seamlessly with Markov Chain Monte Carlo samplers as they provide a target distribution for the inference algorithm. Algorithms are implemented using the machine learning computational frameworks JAX and TensorFlow Probability, thus taking advantage of modern hardware to accelerate computation. This abstracts away computational concerns from modellers, allowing them to focus on developing and testing different model structures or assumptions. The gemlib library enables users to rapidly implement and calibrate stochastic epidemic models with the flexibility and robustness required to support decision during an emerging outbreak.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08124v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alin Morariu, Jess Bridgen, Chris Jewell</dc:creator>
    </item>
    <item>
      <title>Bayesian modelling and computation utilising directed cycles in multiple network data</title>
      <link>https://arxiv.org/abs/2111.07840</link>
      <description>arXiv:2111.07840v3 Announce Type: replace 
Abstract: Modelling multiple network data is crucial for addressing a wide range of applied research questions. However, there are many challenges, both theoretical and computational, to address. Network cycles are often of particular interest in many applications; for example in ecology a largely unexplored area has been how to incorporate network cycles within the inferential framework in an explicit way. The recently developed Spherical Network Family of models (SNF) offers a flexible formulation for modelling multiple network data that permits any type of metric. This has opened up the possibility to formulate network models that focus on network properties hitherto not possible or practical to consider. In this article we propose a novel network distance metric that measures similarities between networks with respect to their cycles, and incorporates this within the SNF model to allow inferences that explicitly capture information on cycles. These network motifs are of particular interest in ecological studies aimed at understanding competitive and hierarchical interactions. We further propose a novel computational framework to allow posterior inferences from the intractable SNF model for moderate-sized networks. Lastly, we apply the resulting methodology to a set of ecological network data studying aggressive interactions between species of fish. We show our model is able to make cogent inferences concerning the cycle behaviour amongst the species, and beyond those possible from a model that does not consider this network motif.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.07840v3</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anastasia Mantziou, Sally Keith, David M. P. Jacoby, Simon Lunagomez, Robin Mitra</dc:creator>
    </item>
    <item>
      <title>K-Contact Distance for Noisy Nonhomogeneous Spatial Point Data with application to Repeating Fast Radio Burst sources</title>
      <link>https://arxiv.org/abs/2410.12146</link>
      <description>arXiv:2410.12146v2 Announce Type: replace 
Abstract: This paper introduces an approach to analyze nonhomogeneous Poisson processes (NHPP) observed with noise, focusing on previously unstudied second-order characteristics of the noisy process. Utilizing a hierarchical Bayesian model with noisy data, we estimate hyperparameters governing a physically motivated NHPP intensity. Simulation studies demonstrate the reliability of this methodology in accurately estimating hyperparameters. Leveraging the posterior distribution, we then infer the probability of detecting a certain number of events within a given radius, the $k$-contact distance. We demonstrate our methodology with an application to observations of fast radio bursts (FRBs) detected by the Canadian Hydrogen Intensity Mapping Experiment's FRB Project (CHIME/FRB). This approach allows us to identify repeating FRB sources by bounding or directly simulating the probability of observing $k$ physically independent sources within some radius in the detection domain, or the $\textit{probability of coincidence}$ ($P_{\text{C}}$). The new methodology improves the repeater detection $P_{\text{C}}$ in 91% of cases when applied to the largest sample of previously classified observations, with a median improvement factor (existing metric over $P_{\text{C}}$ from our methodology) of $\sim$ 4800.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12146v2</guid>
      <category>stat.AP</category>
      <category>astro-ph.IM</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. M. Cook (David A. Dunlap Department of Astronomy &amp; Astrophysics, University of Toronto, Toronto, ON, Dunlap Institute for Astronomy &amp; Astrophysics, University of Toronto, Toronto, ON), Dayi Li (Department of Statistical Science, University of Toronto, Toronto, ON), Gwendolyn M. Eadie (Department of Statistical Science, University of Toronto, Toronto, ON, David A. Dunlap Department of Astronomy &amp; Astrophysics, University of Toronto, Toronto, ON), David C. Stenning (Department of Statistics and Actuarial Science, Simon Fraser University, Burnaby, BC), Paul Scholz (Department of Physics and Astronomy, York University, Toronto, ON, Dunlap Institute for Astronomy &amp; Astrophysics, University of Toronto, Toronto, ON), Derek Bingham (Department of Statistics and Actuarial Science, Simon Fraser University, Burnaby, BC), Radu Craiu (Department of Statistical Science, University of Toronto, Toronto, ON), B. M. Gaensler (Department of Astronomy and Astrophysics, University of California, Santa Cruz, Santa Cruz, CA, David A. Dunlap Department of Astronomy &amp; Astrophysics, University of Toronto, Toronto, ON), Kiyoshi W. Masui (MIT Kavli Institute for Astrophysics and Space Research, Massachusetts Institute of Technology, Cambridge, MA, Department of Physics, Massachusetts Institute of Technology, Cambridge, MA), Ziggy Pleunis (Anton Pannekoek Institute for Astronomy, University of Amsterdam, Amsterdam, ASTRON, Netherlands Institute for Radio Astronomy, Dwingeloo), Antonio Herrera-Martin (David A. Dunlap Department of Astronomy &amp; Astrophysics, University of Toronto, Toronto, ON, Department of Statistical Science, University of Toronto, Toronto, ON), Ronniy C. Joseph (Trottier Space Institute, McGill University, Montr\'eal, QC, Department of Physics, McGill University, Montr\'eal, QC), Ayush Pandhi (David A. Dunlap Department of Astronomy &amp; Astrophysics, University of Toronto, Toronto, ON, Dunlap Institute for Astronomy &amp; Astrophysics, University of Toronto, Toronto, ON), Aaron B. Pearlman (Department of Physics, McGill University, Montr\'eal, QC, Department of Physics, McGill University, Montr\'eal, QC), J. Xavier Prochaska (Department of Astronomy and Astrophysics, University of California, Santa Cruz, Santa Cruz, CA)</dc:creator>
    </item>
    <item>
      <title>A data-driven merit order: Learning a fundamental electricity price model</title>
      <link>https://arxiv.org/abs/2501.02963</link>
      <description>arXiv:2501.02963v2 Announce Type: replace 
Abstract: Electricity price forecasting approaches generally fall into two categories: data-driven models, which learn from historical patterns, or fundamental models, which simulate market mechanisms. We propose a novel and highly efficient data-driven merit order model that integrates both paradigms.The model embeds the classical expert-based merit order as a nested special case, allowing all key parameters, such as plant efficiencies, bidding behavior, and available capacities, to be estimated directly from historical data, rather than assumed.We further enhance the model with critical embedded extensions such as hydro power, cross-border flows and corrections for underreported capacities, which considerably improve forecasting accuracy.Applied to the German day-ahead market, our model outperforms both classic fundamental and state-of-the-art machine learning models. It retains the interpretability of fundamental models, offering insights into marginal technologies, fuel switches, and dispatch patterns, elements which are typically inaccessible to black-box machine learning approaches.This transparency and high computational efficiency make it a promising new direction for electricity price modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02963v2</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Ghelasi, Florian Ziel</dc:creator>
    </item>
    <item>
      <title>Expectiles as basis risk-optimal payment schemes in parametric insurance</title>
      <link>https://arxiv.org/abs/2505.02607</link>
      <description>arXiv:2505.02607v2 Announce Type: replace 
Abstract: Payments in parametric insurance solutions are linked to an index and thus decoupled from policyholders' true losses. While this principle has appealing operational benefits compared to traditional indemnity coverage, i.e. is very efficient and cost effective, a downside is the discrepancy between payouts and actual damage, called basis risk. We show that in an asymmetrically weighted mean square error framework, the basis risk-minimizing payment schemes for pure parametric and parametric index insurance contracts can be expressed as conditional expectiles of policyholders' true loss given a compensation-triggering incident. We provide connections to stochastic orderings and demonstrate that regression approaches allow easy implementation in practice. Our results are visualized in parametric coverage for cyber risks and agricultural insurance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02607v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Markus Johannes Maier, Matthias Scherer</dc:creator>
    </item>
    <item>
      <title>Constructing Evidence-Based Tailoring Variables for Adaptive Interventions</title>
      <link>https://arxiv.org/abs/2506.03054</link>
      <description>arXiv:2506.03054v2 Announce Type: replace-cross 
Abstract: Background: Adaptive interventions provide a guide for how ongoing information about individuals should be used to decide whether and how to modify type, amount, delivery modality or timing of treatment, to improve intervention effectiveness while reducing cost and burden. The variables that inform treatment modification decisions are called tailoring variables. Specifying a tailoring variable for an intervention requires describing what should be measured, when to measure it, when this measure should be used to make decisions, and what cutoffs should be used in making decisions. These questions are causal and prescriptive (what to do, when), not merely predictive, raising important tradeoffs between specificity versus sensitivity, and between waiting for sufficient information versus intervening quickly. Purpose: There is little specific guidance in the literature on how to empirically choose tailoring variables, including cutoffs, measurement times, and decision times. Methods: We review possible approaches for comparing potential tailoring variables and propose a framework for systematically developing tailoring variables. Results: Although secondary observational data can be used to select tailoring variables, additional assumptions are needed. A specifically designed experiment for optimization (an optimization randomized clinical trial), e.g., a multi-arm randomized trial, sequential multiple assignment randomized trial, factorial experiment, or hybrid design, may provide a more direct way to answer these questions. Conclusions: Using randomization directly to inform tailoring variables provides the most direct causal evidence, but requires more effort and resources than secondary data analysis. More research on how best to design tailoring variables for effective, scalable interventions is needed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03054v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John J. Dziak, Inbal Nahum-Shani</dc:creator>
    </item>
    <item>
      <title>A Realistic Evaluation of Cross-Frequency Transfer Learning and Foundation Forecasting Models</title>
      <link>https://arxiv.org/abs/2509.19465</link>
      <description>arXiv:2509.19465v2 Announce Type: replace-cross 
Abstract: Cross-frequency transfer learning (CFTL) has emerged as a popular framework for curating large-scale time series datasets to pre-train foundation forecasting models (FFMs). Although CFTL has shown promise, current benchmarking practices fall short of accurately assessing its performance. This shortcoming stems from many factors: an over-reliance on small-scale evaluation datasets; inadequate treatment of sample size when computing summary statistics; reporting of suboptimal statistical models; and failing to account for non-negligible risks of overlap between pre-training and test datasets. To address these limitations, we introduce a unified reimplementation of widely-adopted neural forecasting networks, adapting them for the CFTL setup; we pre-train only on proprietary and synthetic data, being careful to prevent test leakage; and we evaluate on 15 large, diverse public forecast competition datasets. Our empirical analysis reveals that statistical models' accuracy is frequently underreported. Notably, we confirm that statistical models and their ensembles consistently outperform existing FFMs by more than 8.2% in sCRPS, and by more than 20% MASE, across datasets. However, we also find that synthetic dataset pre-training does improve the accuracy of a FFM by 7% percent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19465v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kin G. Olivares, Malcolm Wolff, Tatiana Konstantinova, Shankar Ramasubramanian, Boris Oreshkin, Andrew Gordon Wilson, Andres Potapczynski, Willa Potosnak, Michael W. Mahoney, Mengfei Cao, Dmitry Efimov</dc:creator>
    </item>
    <item>
      <title>Sub-exponential Growth of New Words and Names Online: A Piecewise Power-Law Model</title>
      <link>https://arxiv.org/abs/2511.04106</link>
      <description>arXiv:2511.04106v3 Announce Type: replace-cross 
Abstract: The diffusion of ideas and language in society has conventionally been described by S-shaped models, such as the logistic curve. However, the role of sub-exponential growth -- a slower-than-exponential pattern known in epidemiology -- has been largely overlooked in broader social phenomena. Here, we present a piecewise power-law model to characterize complex growth curves with a few parameters. We systematically analyzed a large-scale dataset of approximately one billion Japanese blog articles linked to Wikipedia vocabulary, and observed consistent patterns in web search trend data (English, Spanish, and Japanese). Our analysis of 2,963 items, selected for reliable estimation (e.g., sufficient duration/peak, monotonic growth), reveals that 1,625 (55%) diffusion patterns without abrupt level shifts were adequately described by one or two segments. For single-segment curves, we found that (i) the mode of the shape parameter $\alpha$ was near 0.5, indicating prevalent sub-exponential growth; (ii) the peak diffusion scale is primarily determined by the growth rate $R$, with minor contributions from $\alpha$ or the duration $T$; and (iii) $\alpha$ showed a tendency to vary with the nature of the topic, being smaller for niche/local topics and larger for widely shared ones. Furthermore, a micro-behavioral model of outward (stranger) vs. inward (community) contact suggests that $\alpha$ can be interpreted as an index of the preference for outward-oriented communication. These findings suggest that sub-exponential growth is a common pattern of social diffusion, and our model provides a practical framework for consistently describing, comparing, and interpreting complex and diverse growth curves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04106v3</guid>
      <category>physics.soc-ph</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hayafumi Watanabe</dc:creator>
    </item>
  </channel>
</rss>
