<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Apr 2025 02:02:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Probabilistic Crop Yields Forecasts With Spatio-Temporal Conditional Copula Using Extreme Weather Covariates</title>
      <link>https://arxiv.org/abs/2503.22807</link>
      <description>arXiv:2503.22807v1 Announce Type: new 
Abstract: We introduce a novel forecasting model for crop yields that explicitly accounts for spatio-temporal dependence and the influence of extreme weather and climatic events. Our approach combines Bayesian Structural Time Series for modeling marginal crop yields, ensuring a more robust quantification of uncertainty given the typically short historical records. To capture dynamic dependencies between regions, we develop a time-varying conditional copula model, where the copula parameter evolves over time as a function of its previous lag and extreme weather covariates. Unlike traditional approaches that treat climatic factors as fixed inputs, we incorporate dynamic Generalized Extreme Value models to characterize extreme weather events, enabling a more accurate reflection of their impact on crop yields. Furthermore, to ensure scalability for large-scale applications, we build on the existing Partitioning Around Medoids clustering algorithm and introduce a novel dissimilarity measure that integrates both spatial and copula-based dependence, enabling an effective reduction of the dimensionality in the dependence structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22807v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marie Michaelides, M\'elina Mailhot, Yongkun Li</dc:creator>
    </item>
    <item>
      <title>Divide-and-conquer with finite sample sizes: valid and efficient possibilistic inference</title>
      <link>https://arxiv.org/abs/2503.22812</link>
      <description>arXiv:2503.22812v1 Announce Type: new 
Abstract: Divide-and-conquer methods use large-sample approximations to provide frequentist guarantees when each block of data is both small enough to facilitate efficient computation and large enough to support approximately valid inferences. When the overall sample size is small or moderate, likely no suitable division of the data meets both requirements, hence the resulting inference lacks validity guarantees. We propose a new approach, couched in the inferential model framework, that is fully conditional in a Bayesian sense and provably valid in a frequentist sense. The main insight is that existing divide-and-conquer approaches make use of a Gaussianity assumption twice: first in the construction of an estimator, and second in the approximation to its sampling distribution. Our proposal is to retain the first Gaussianity assumption, using a Gaussian working likelihood, but to replace the second with a validification step that uses the sampling distributions of the block summaries determined by the posited model. This latter step, a type of probability-to-possibility transform, is key to the reliability guarantees enjoyed by our approach, which are uniquely general in the divide-and-conquer literature. In addition to finite-sample validity guarantees, our proposed approach is also asymptotically efficient like the other divide-and-conquer solutions available in the literature. Our computational strategy leverages state-of-the-art black-box likelihood emulators. We demonstrate our method's performance via simulations and highlight its flexibility with an analysis of median PM2.5 in Maryborough, Queensland, during the 2023 Australian bushfire season.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22812v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emily C. Hector, Leonardo Cella, Ryan Martin</dc:creator>
    </item>
    <item>
      <title>Density-valued time series: Nonparametric density-on-density regression</title>
      <link>https://arxiv.org/abs/2503.22904</link>
      <description>arXiv:2503.22904v1 Announce Type: new 
Abstract: This paper is concerned with forecasting probability density functions. Density functions are nonnegative and have a constrained integral; they thus do not constitute a vector space. Implementing unconstrained functional time-series forecasting methods is problematic for such nonlinear and constrained data. A novel forecasting method is developed based on a nonparametric function-on-function regression, where both the response and the predictor are probability density functions. Through a series of Monte-Carlo simulation studies, we evaluate the finite-sample performance of our nonparametric regression estimator. Using French departmental COVID19 data and age-specific period life tables in the United States, we assess and compare finite-sample forecast accuracy between the proposed and several existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22904v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fr\'ed\'eric Ferraty, Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>Asymptotic Standard Errors for Reliability Coefficients in Item Response Theory</title>
      <link>https://arxiv.org/abs/2503.22924</link>
      <description>arXiv:2503.22924v1 Announce Type: new 
Abstract: Reliability is a crucial index of measurement precision and is commonly reported in substantive research using latent variable measurement models. However, reliability coefficients, often treated as fixed values, are estimated from sample data and thus inherently subject to sampling variability. Building on the recently proposed regression framework of reliability (Liu, Pek, &amp; Maydeu-Olivares, 2025), this study classifies reliability coefficients in item response theory (IRT) into classical test theory (CTT) reliability and proportional reduction in mean squared error (PRMSE) and derives their asymptotic standard errors (SEs) to quantify this variability. Unlike existing approaches (e.g., Andersson &amp; Xin, 2018), which requires computing population moments, we focus on cases using sample moments instead. This allows for estimation of reliability and SEs in long tests. By incorporating sampling variability arising from both item parameter estimation and the use of sample moments, we present a more general strategy for computing SEs of model-based reliability coefficients. Simulation results confirm that the derived SEs accurately capture the sampling variability across various test lengths in moderate to large samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22924v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youjin Sung, Yang Liu</dc:creator>
    </item>
    <item>
      <title>Improving Transportability of Regression Calibration Under the Main/External Validation Study Design</title>
      <link>https://arxiv.org/abs/2503.22933</link>
      <description>arXiv:2503.22933v1 Announce Type: new 
Abstract: In epidemiology, obtaining accurate individual exposure measurements can be costly and challenging. Thus, these measurements are often subject to error. Regression calibration with a validation study is widely employed as a study design and analysis method to correct for measurement error in the main study due to its broad applicability and simple implementation. However, relying on an external validation study to assess the measurement error process carries the risk of introducing bias into the analysis. Specifically, if the parameters of regression calibration model estimated from the external validation study are not transportable to the main study, the subsequent estimated parameter describing the exposure-disease association will be biased. In this work, we improve the regression calibration method for linear regression models using an external validation study. Unlike the original approach, our proposed method ensures that the regression calibration model is transportable by estimating the parameters in the measurement error generating process using the external validation study and obtaining the remaining parameter values in the regression calibration model directly from the main study. This guarantees that parameter values in the regression calibration model will be applicable to the main study. We derived the theoretical properties of our proposed method. The simulation results show that the proposed method effectively reduces bias and maintains nominal confidence interval coverage. We applied this method to data from the Health Professionals Follow-Up Study (main study) and the Men's Lifestyle Validation Study (external validation study) to assess the effects of dietary intake on body weight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22933v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zexiang Li, Donna Spiegelman, Molin Wang, Zuoheng Wang, Xin Zhou</dc:creator>
    </item>
    <item>
      <title>TITE-STEIN: Time-to-event Simple Toxicity and Efficacy Interval Design to Accelerate Phase I/II Trials</title>
      <link>https://arxiv.org/abs/2503.22957</link>
      <description>arXiv:2503.22957v1 Announce Type: new 
Abstract: Oncology dose-finding trials are shifting from identifying the maximum tolerated dose (MTD) to determining the optimal biological dose (OBD), driven by the need for efficient methods that consider both toxicity and efficacy. This is particularly important for novel therapies, such as immunotherapies and molecularly targeted therapies, which often exhibit non-monotonic dose-efficacy curves. However, making timely adaptive dosing decisions is challenging due to the rapid patient accrual rate and the late-onset toxicity and/or efficacy outcomes associated with these therapies. The Simple Toxicity and Efficacy Interval (STEIN) design has demonstrated strong performance in accommodating diverse dose-efficacy patterns and incorporating both toxicity and efficacy outcomes to select the OBD. However, the rapid accrual of patients and the often-delayed onset of toxicity and/or efficacy pose challenges to timely adaptive dose decisions. To address these challenges, we propose TITE-STEIN, a model-assisted design that incorporates time-to-event (TITE) outcomes for toxicity and/or efficacy, by extending STEIN. In this article, we demonstrate that TITE-STEIN significantly shortens trial duration compared to STEIN. Furthermore, by integrating an OBD verification procedure during OBD selection, TITE-STEIN effectively mitigates the risk of exposing patients to inadmissible doses when the OBD does not exist. Extensive simulations demonstrate that TITE-STEIN outperforms existing TITE designs, including TITE-BONI12, TITE-BOIN-ET, LO-TC, and Joint TITE-CRM, by selecting the OBD more accurately, allocating more patients to it, and improving overdose control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22957v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hao Sun, Jieqi Tu, Revathi Ananthakrishnan, Eunhee Kim</dc:creator>
    </item>
    <item>
      <title>Neural Bayes inference for complex bivariate extremal dependence models</title>
      <link>https://arxiv.org/abs/2503.23156</link>
      <description>arXiv:2503.23156v1 Announce Type: new 
Abstract: Likelihood-free approaches are appealing for performing inference on complex dependence models, either because it is not possible to formulate a likelihood function, or its evaluation is very computationally costly. This is the case for several models available in the multivariate extremes literature, particularly for the most flexible tail models, including those that interpolate between the two key dependence classes of `asymptotic dependence' and `asymptotic independence'. We focus on approaches that leverage neural networks to approximate Bayes estimators. In particular, we explore the properties of neural Bayes estimators for parameter inference for several flexible but computationally expensive models to fit, with a view to aiding their routine implementation. Owing to the absence of likelihood evaluation in the inference procedure, classical information criteria such as the Bayesian information criterion cannot be used to select the most appropriate model. Instead, we propose using neural networks as neural Bayes classifiers for model selection. Our goal is to provide a toolbox for simple, fast fitting and comparison of complex extreme-value dependence models, where the best model is selected for a given data set and its parameters subsequently estimated using neural Bayes estimation. We apply our classifiers and estimators to analyse the pairwise extremal behaviour of changes in horizontal geomagnetic field fluctuations at three different locations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23156v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L\'idia M. Andr\'e, Jennifer L. Wadsworth, Rapha\"el Huser</dc:creator>
    </item>
    <item>
      <title>Active Learning for Finite Element Simulations with Adaptive Non-Stationary Kernel Function</title>
      <link>https://arxiv.org/abs/2503.23158</link>
      <description>arXiv:2503.23158v1 Announce Type: new 
Abstract: Simulating complex physical processes across a domain of input parameters can be very computationally expensive. Multi-fidelity surrogate modeling can resolve this issue by integrating cheaper simulations with the expensive ones in order to obtain better predictions at a reasonable cost. We are specifically interested in computer experiments that involve the use of finite element methods with a real-valued tuning parameter that determines the fidelity of the numerical output. In these cases, integrating this fidelity parameter in the analysis enables us to make inference on fidelity levels that have not been observed yet. Such models have been developed, and we propose a new adaptive non-stationary kernel function which more accurately reflects the behavior of computer simulation outputs. In addition, we aim to create a sequential design based on the integrated mean squared prediction error (IMSPE) to identify the best design points across input parameters and fidelity parameter, while taking into account the computational cost associated with the fidelity parameter. We illustrate this methodology through synthetic examples and applications to finite element analysis. An $\textsf{R}$ package for the proposed methodology is provided in an open repository.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23158v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Romain Boutelet, Chih-Li Sung</dc:creator>
    </item>
    <item>
      <title>Optimal Change Point Detection and Inference in the Spectral Density of General Time Series Models</title>
      <link>https://arxiv.org/abs/2503.23211</link>
      <description>arXiv:2503.23211v1 Announce Type: new 
Abstract: This paper addresses the problem of detecting change points in the spectral density of time series, motivated by EEG analysis of seizure patients. Seizures disrupt coherence and functional connectivity, necessitating precise detection. Departing from traditional parametric approaches, we utilize the Wold decomposition, representing general time series as autoregressive processes with infinite lags, which are truncated and estimated around the change point. Our detection procedure employs an initial estimator that systematically searches across time points. We examine the localization error and its dependence on time series properties and sample size. To enhance accuracy, we introduce an optimal rate method with an asymptotic distribution, facilitating the construction of confidence intervals. The proposed method effectively identifies seizure onset in EEG data and extends to event detection in video data. Comprehensive numerical experiments demonstrate its superior performance compared to existing techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23211v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sepideh Mosaferi, Abolfazl Safikhani, Peiliang Bai</dc:creator>
    </item>
    <item>
      <title>A Kolmogorov-Zurbenko Fourier Transform Band-pass Filter Extension for Time Series Analysis</title>
      <link>https://arxiv.org/abs/2503.23493</link>
      <description>arXiv:2503.23493v1 Announce Type: new 
Abstract: This research introduces a novel extension, called the Extended Kolmogorov-Zurbenko Fourier Transform (EKZFT), to an existing class of band-pass filters first introduced by Kolmogorov and Zurbenko. Their original Kolmogorov-Zurbenko Fourier Transform (KZFT) is a useful tool in time series and spatio-temporal analysis, Fourier analysis, and related statistical analysis fields. Example uses of these filters include separating frequencies, filtering portions of the spectra, reconstructing seasonality, investigating periodic signals, and reducing noise. KZFT filters have many practical applications across a wide range of disciplines including health, social, natural, and physical sciences. KZFT filters are band-pass filters defined by three arguments: the length of the filter window; the number of iterations; and the central frequency of the band-pass filter. However, the KZFT filter is limited in design to only positive odd integer widow lengths inherited from the time series. Therefore, for any combination of the other KZFT filter arguments, there is only a relatively small, discrete, selection of possible filter window lengths in a range determined by the size of the dataset. This limits the utility of KZFT filters for many of the stated uses. The proposed EKZFT filter allows a continuous selection of filter window length arguments over the same range, offering improved control, increased functionality, and wider practical use of this band-pass filter. An example application of the EKZFT in a data simulation is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23493v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for High-dimensional Time Series with a Directed Acyclic Graphical Structure</title>
      <link>https://arxiv.org/abs/2503.23563</link>
      <description>arXiv:2503.23563v1 Announce Type: new 
Abstract: In multivariate time series analysis, understanding the underlying causal relationships among variables is often of interest for various applications. Directed acyclic graphs (DAGs) provide a powerful framework for representing causal dependencies. This paper proposes a novel Bayesian approach for modeling multivariate time series where conditional independencies and causal structure are encoded by a DAG. The proposed model allows structural properties such as stationarity to be easily accommodated. Given the application, we further extend the model for matrix-variate time series. We take a Bayesian approach to inference, and a ``projection-posterior'' based efficient computational algorithm is developed. The posterior convergence properties of the proposed method are established along with two identifiability results for the unrestricted structural equation models. The utility of the proposed method is demonstrated through simulation studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23563v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arkaprava Roy, Anindya Roy, Subhashis Ghosal</dc:creator>
    </item>
    <item>
      <title>Penalized function-on-function linear quantile regression</title>
      <link>https://arxiv.org/abs/2503.23624</link>
      <description>arXiv:2503.23624v1 Announce Type: new 
Abstract: We introduce a novel function-on-function linear quantile regression model to characterize the entire conditional distribution of a functional response for a given functional predictor. Tensor cubic $B$-splines expansion is used to represent the regression parameter functions, where a derivative-free optimization algorithm is used to obtain the estimates. Quadratic roughness penalties are applied to the coefficients to control the smoothness of the estimates. The optimal degree of smoothness depends on the quantile of interest. An automatic grid-search algorithm based on the Bayesian information criterion is used to estimate the optimum values of the smoothing parameters. Via a series of Monte-Carlo experiments and an empirical data analysis using Mary River flow data, we evaluate the estimation and predictive performance of the proposed method, and the results are compared favorably with several existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23624v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ufuk Beyaztas, Han Lin Shang, Semanur Saricam</dc:creator>
    </item>
    <item>
      <title>Nonparametric function-on-scalar regression using deep neural networks</title>
      <link>https://arxiv.org/abs/2503.23935</link>
      <description>arXiv:2503.23935v1 Announce Type: new 
Abstract: We focus on nonlinear Function-on-Scalar regression, where the predictors are scalar variables, and the responses are functional data. Most existing studies approximate the hidden nonlinear relationships using linear combinations of basis functions, such as splines. However, in classical nonparametric regression, it is known that these approaches lack adaptivity, particularly when the true function exhibits high spatial inhomogeneity or anisotropic smoothness. To capture the complex structure behind data adaptively, we propose a simple adaptive estimator based on a deep neural network model. The proposed estimator is straightforward to implement using existing deep learning libraries, making it accessible for practical applications. Moreover, we derive the convergence rates of the proposed estimator for the anisotropic Besov spaces, which consist of functions with varying smoothness across dimensions. Our theoretical analysis shows that the proposed estimator mitigates the curse of dimensionality when the true function has high anisotropic smoothness, as shown in the classical nonparametric regression. Numerical experiments demonstrate the superior adaptivity of the proposed estimator, outperforming existing methods across various challenging settings. Moreover, the proposed method is applied to analyze ground reaction force data in the field of sports medicine, demonstrating more efficient estimation compared to existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23935v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazunori Takeshita, Yoshikazu Terada</dc:creator>
    </item>
    <item>
      <title>Testing for integer integration in functional time series</title>
      <link>https://arxiv.org/abs/2503.23960</link>
      <description>arXiv:2503.23960v1 Announce Type: new 
Abstract: We develop a statistical testing procedure to examine whether the curve-valued time series of interest is integrated of order d for an integer d. The proposed procedure can distinguish between integer-integrated time series and fractionally-integrated ones, and it has broad applicability in practice. Monte Carlo simulation experiments show that the proposed testing procedure performs reasonably well. We apply our methodology to Canadian yield curve data and French sub-national age-specific mortality data. We find evidence that these time series are mostly integrated of order one, while some have fractional orders exceeding or falling below one.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23960v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Won-Ki Seo, Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>A Robust Extrinsic Single-index Model for Spherical Data</title>
      <link>https://arxiv.org/abs/2503.24003</link>
      <description>arXiv:2503.24003v1 Announce Type: new 
Abstract: Regression with a spherical response is challenging due to the absence of linear structure, making standard regression models inadequate. Existing methods, mainly parametric, lack the flexibility to capture the complex relationship induced by spherical curvature, while methods based on techniques from Riemannian geometry often suffer from computational difficulties. The non-Euclidean structure further complicates robust estimation, with very limited work addressing this issue, despite the common presence of outliers in directional data. This article introduces a new semi-parametric approach, the extrinsic single-index model (ESIM) and its robust estimation, to address these limitations. We establish large-sample properties of the proposed estimator with a wide range of loss functions and assess their robustness using the influence function and standardized influence function. Specifically, we focus on the robustness of the exponential squared loss (ESL), demonstrating comparable efficiency and superior robustness over least squares loss under high concentration. We also examine how the tuning parameter for the ESL balances efficiency and robustness, providing guidance on its optimal choice. The computational efficiency and robustness of our methods are further illustrated via simulations and applications to geochemical compositional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24003v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Houren Hong, Janice L. Scealy, Andrew T. A. Wood, Yanrong Yang</dc:creator>
    </item>
    <item>
      <title>Using directed acyclic graphs to determine whether multiple imputation or subsample multiple imputation estimates of an exposure-outcome association are unbiased</title>
      <link>https://arxiv.org/abs/2503.24035</link>
      <description>arXiv:2503.24035v1 Announce Type: new 
Abstract: Background: Missing data is a pervasive problem in epidemiology, with complete records analyses (CRA) or multiple imputation (MI) the most common methods to deal with incomplete data. MI is valid when incomplete variables are independent of response indicators, conditional on complete variables - however, this can be hard to assess with multiple incomplete variables. Previous literature has shown that MI may be valid in subsamples of the data, even if not necessarily valid in the full dataset. Current guidance on how to decide whether MI is appropriate is lacking.
  Methods: We develop an algorithm that is sufficient to indicate when MI will estimate an exposure-outcome coefficient without bias and show how to implement this using directed acyclic graphs (DAGs). We extend the algorithm to investigate whether MI applied to a subsample of the data, in which some variables and complete and the remaining are imputed, will be unbiased for the same estimand. We demonstrate the algorithm by applying it to several simple examples and a more complex real-life example.
  Conclusions: Multiple incomplete variables are common in practice. Assessing the plausibility of each of CRA and MI estimating an exposure-outcome association without bias is crucial in analysing and interpreting results. Our algorithm provides researchers with the tools to decide whether (and how) to use MI in practice. Further work could focus on the likely size and direction of biases, and the impact of different missing data patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24035v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paul Madley-Dowd, Rachael A. Hughes, Maya B. Mathur, Jon Heron, Kate Tilling</dc:creator>
    </item>
    <item>
      <title>Joint model for zero-inflated data combining fishery-dependent and fishery-independent sources</title>
      <link>https://arxiv.org/abs/2503.24079</link>
      <description>arXiv:2503.24079v1 Announce Type: new 
Abstract: Accurately identifying spatial patterns of species distribution is crucial for scientific insight and societal benefit, aiding our understanding of species fluctuations. The increasing quantity and quality of ecological datasets present heightened statistical challenges, complicating spatial species dynamics comprehension. Addressing the complex task of integrating multiple data sources to enhance spatial fish distribution understanding in marine ecology, this study introduces a pioneering five-layer Joint model. The model adeptly integrates fishery-independent and fishery-dependent data, accommodating zero-inflated data and distinct sampling processes. A comprehensive simulation study evaluates the model performance across various preferential sampling scenarios and sample sizes, elucidating its advantages and challenges. Our findings highlight the model's robustness in estimating preferential parameters, emphasizing differentiation between presence-absence and biomass observations. Evaluation of estimation of spatial covariance and prediction performance underscores the model's reliability. Augmenting sample sizes reduces parameter estimation variability, aligning with the principle that increased information enhances certainty. Assessing the contribution of each data source reveals successful integration, providing a comprehensive representation of biomass patterns. Empirical validation within a real-world context further solidifies the model's efficacy in capturing species' spatial distribution. This research advances methodologies for integrating diverse datasets with different sampling natures further contributing to a more informed understanding of spatial dynamics of marine species.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24079v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Daniela Silva, Raquel Menezes, Gon\c{c}alo Ara\'ujo, Renato Rosa, Ana Moreno, Alexandra Silva, Susana Garrido</dc:creator>
    </item>
    <item>
      <title>Optimizing PCA for Health and Care Research: A Reliable Approach to Component Selection</title>
      <link>https://arxiv.org/abs/2503.24248</link>
      <description>arXiv:2503.24248v1 Announce Type: new 
Abstract: PCA is widely used in health and care research to analyze complex HD datasets, such as patient health records, genetic data, and medical imaging. By reducing dimensionality, PCA helps identify key patterns and trends, which can aid in disease diagnosis, treatment optimization, and the discovery of new biomarkers. However, the primary goal of any dimensional reduction technique is to reduce the dimensionality in a data set while keeping the essential information and variability. There are a few ways to do this in practice, such as the Kaiser-Guttman criterion, Cattell's Scree Test, and the percent cumulative variance approach. Unfortunately, the results of these methods are entirely different. That means using inappropriate methods to find the optimal number of PCs retained in PCA may lead to misinterpreted and inaccurate results in PCA and PCA-related health and care research applications. This contradiction becomes even more pronounced in HD settings where n &lt; p, making it even more critical to determine the best approach. Therefore, it is necessary to identify the issues of different techniques to select the optimal number of PCs retained in PCA. Kaiser-Guttman criterion retains fewer PCs, causing overdispersion, while Cattell's scree test retains more PCs, compromising reliability. The percentage of cumulative variation criterion offers greater stability, consistently selecting the optimal number of components. Therefore, the Pareto chart, which shows both the cumulative percentage and the cut-off point for retained PCs, provides the most reliable method of selecting components, ensuring stability and enhancing PCA effectiveness, particularly in health-related research applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24248v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nuwan Weeraratne, Lyn Hunt, Jason Kurz</dc:creator>
    </item>
    <item>
      <title>Selective Inference in Graphical Models via Maximum Likelihood</title>
      <link>https://arxiv.org/abs/2503.24311</link>
      <description>arXiv:2503.24311v1 Announce Type: new 
Abstract: The graphical lasso is a widely used algorithm for fitting undirected Gaussian graphical models. However, for inference on functionals of edge values in the learned graph, standard tools lack formal statistical guarantees, such as control of the type I error rate. In this paper, we introduce a selective inference method for asymptotically valid inference after graphical lasso selection with added randomization. We obtain a selective likelihood, conditional on the event of selection, through a change of variable on the known density of the randomization variables. Our method enables interval estimation and hypothesis testing for a wide range of functionals of edge values in the learned graph using the conditional maximum likelihood estimate. Our numerical studies show that introducing a small amount of randomization: (i) greatly increases power and yields substantially shorter intervals compared to other conditional inference methods, including data splitting; (ii) ensures intervals of bounded length in high-dimensional settings where data splitting is infeasible due to insufficient samples for inference; (iii) enables inference for a wide range of inferential targets in the learned graph, including measures of node influence and connectivity between nodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24311v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sofia Guglielmini, Gerda Claeskens, Snigdha Panigrahi</dc:creator>
    </item>
    <item>
      <title>Modeling Maximum drawdown Records with Piecewise Deterministic Markov Processe in Capital Markets</title>
      <link>https://arxiv.org/abs/2503.23221</link>
      <description>arXiv:2503.23221v1 Announce Type: cross 
Abstract: We propose to model the records of the maximum Drawdown in capital markets by means a Piecewise Deterministic Markov Process (PDMP). We derive statistical results such as the mean and variance that describes the sequence of maximum Drawdown records. In addition, we developed a simulation study and techniques for estimating the parameters governing the stochastic process, using a practical example in the capital market to illustrate the procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23221v1</guid>
      <category>q-fin.RM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rolando Rubilar-Torrealba, Lisandro Fermin, Soledad Torres</dc:creator>
    </item>
    <item>
      <title>Reinterpreting demand estimation</title>
      <link>https://arxiv.org/abs/2503.23524</link>
      <description>arXiv:2503.23524v1 Announce Type: cross 
Abstract: This paper connects the literature on demand estimation to the literature on causal inference by interpreting nonparametric structural assumptions as restrictions on counterfactual outcomes. It offers nontrivial and equivalent restatements of key demand estimation assumptions in the Neyman-Rubin potential outcomes model, for both settings with market-level data (Berry and Haile, 2014) and settings with demographic-specific market shares (Berry and Haile, 2024). This exercise helps bridge the literatures on structural estimation and on causal inference by separating notational and linguistic differences from substantive ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23524v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen</dc:creator>
    </item>
    <item>
      <title>In-silico biological discovery with large perturbation models</title>
      <link>https://arxiv.org/abs/2503.23535</link>
      <description>arXiv:2503.23535v1 Announce Type: cross 
Abstract: Data generated in perturbation experiments link perturbations to the changes they elicit and therefore contain information relevant to numerous biological discovery tasks -- from understanding the relationships between biological entities to developing therapeutics. However, these data encompass diverse perturbations and readouts, and the complex dependence of experimental outcomes on their biological context makes it challenging to integrate insights across experiments. Here, we present the Large Perturbation Model (LPM), a deep-learning model that integrates multiple, heterogeneous perturbation experiments by representing perturbation, readout, and context as disentangled dimensions. LPM outperforms existing methods across multiple biological discovery tasks, including in predicting post-perturbation transcriptomes of unseen experiments, identifying shared molecular mechanisms of action between chemical and genetic perturbations, and facilitating the inference of gene-gene interaction networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23535v1</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Djordje Miladinovic, Tobias H\"oppe, Mathieu Chevalley, Andreas Georgiou, Lachlan Stuart, Arash Mehrjou, Marcus Bantscheff, Bernhard Sch\"olkopf, Patrick Schwab</dc:creator>
    </item>
    <item>
      <title>Finite sample valid confidence sets of mode</title>
      <link>https://arxiv.org/abs/2503.23711</link>
      <description>arXiv:2503.23711v1 Announce Type: cross 
Abstract: Estimating the mode of a unimodal distribution is a classical problem in statistics. Although there are several approaches for point-estimation of mode in the literature, very little has been explored about the interval-estimation of mode. Our work proposes a collection of novel methods of obtaining finite sample valid confidence set of the mode of a unimodal distribution. We analyze the behaviour of the width of the proposed confidence sets under some regularity assumptions of the density about the mode and show that the width of these confidence sets shrink to zero near optimally. Simply put, we show that it is possible to build finite sample valid confidence sets for the mode that shrink to a singleton as sample size increases. We support the theoretical results by showing the performance of the proposed methods on some synthetic data-sets. We believe that our confidence sets can be improved both in construction and in terms of rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23711v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manit Paul, Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>Multivariate Species Sampling Models</title>
      <link>https://arxiv.org/abs/2503.24004</link>
      <description>arXiv:2503.24004v1 Announce Type: cross 
Abstract: Species sampling processes have long served as the framework for studying random discrete distributions. However, their statistical applicability is limited when partial exchangeability is assumed as probabilistic invariance for the observables. Despite numerous discrete models for partially exchangeable observations, a unifying framework is currently missing, leaving many questions about the induced learning mechanisms unanswered in this setting. To fill this gap, we consider the natural extension of species sampling models to a multivariate framework, obtaining a general class of models characterized by their partially exchangeable partition probability function. A notable subclass, named regular multivariate species sampling models, exists among these models. In the subclass, dependence across processes is accurately captured by the correlation among them: a correlation of one equals full exchangeability and a null correlation corresponds to independence. Regular multivariate species sampling models encompass discrete processes for partial exchangeable data used in Bayesian models, thereby highlighting their core distributional properties and providing a means for developing new models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24004v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Beatrice Franzolini, Antonio Lijoi, Igor Pr\"unster, Giovanni Rebaudo</dc:creator>
    </item>
    <item>
      <title>New Statistical Framework for Extreme Error Probability in High-Stakes Domains for Reliable Machine Learning</title>
      <link>https://arxiv.org/abs/2503.24262</link>
      <description>arXiv:2503.24262v1 Announce Type: cross 
Abstract: Machine learning is vital in high-stakes domains, yet conventional validation methods rely on averaging metrics like mean squared error (MSE) or mean absolute error (MAE), which fail to quantify extreme errors. Worst-case prediction failures can have substantial consequences, but current frameworks lack statistical foundations for assessing their probability. In this work a new statistical framework, based on Extreme Value Theory (EVT), is presented that provides a rigorous approach to estimating worst-case failures. Applying EVT to synthetic and real-world datasets, this method is shown to enable robust estimation of catastrophic failure probabilities, overcoming the fundamental limitations of standard cross-validation. This work establishes EVT as a fundamental tool for assessing model reliability, ensuring safer AI deployment in new technologies where uncertainty quantification is central to decision-making or scientific analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24262v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Umberto Michelucci, Francesca Venturini</dc:creator>
    </item>
    <item>
      <title>Noise-Induced Randomization in Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2004.09458</link>
      <description>arXiv:2004.09458v5 Announce Type: replace 
Abstract: Regression discontinuity designs assess causal effects in settings where treatment is determined by whether an observed running variable crosses a pre-specified threshold. Here we propose a new approach to identification, estimation, and inference in regression discontinuity designs that uses knowledge about exogenous noise (e.g., measurement error) in the running variable. In our strategy, we weight treated and control units to balance a latent variable of which the running variable is a noisy measure. Our approach is driven by effective randomization provided by the noise in the running variable, and complements standard formal analyses that appeal to continuity arguments while ignoring the stochastic nature of the assignment mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2004.09458v5</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/biomet/asaf003</arxiv:DOI>
      <dc:creator>Dean Eckles, Nikolaos Ignatiadis, Stefan Wager, Han Wu</dc:creator>
    </item>
    <item>
      <title>Assessing small area estimates via bootstrap-weighted k-Nearest-Neighbor artificial populations</title>
      <link>https://arxiv.org/abs/2306.15607</link>
      <description>arXiv:2306.15607v3 Announce Type: replace 
Abstract: National Forest Inventories (NFIs) monitor forest attributes across a variety of spatial and temporal scales in a given country. Increased interest in reporting and management at smaller scales has driven NFIs to investigate and adopt small area estimation (SAE) due to the promise of increased precision at these scales. However, comparing and evaluating SAE models for a given application is inherently difficult. Typically, many areas lack enough data to check unit-level modeling assumptions or to assess unit-level predictions empirically; and no ground truth is available for checking area-level estimates. Design-based simulation from artificial populations can help with each of these issues, but only if the artificial populations realistically represent the application at hand and are not built using assumptions that inherently favor one SAE model over another. In this paper, we borrow ideas from random hot deck, approximate Bayesian bootstrap (ABB), and k Nearest Neighbor (kNN) imputation methods to propose a kNN-based approximation to ABB (KBAABB), for generating an artificial population when rich unit-level auxiliary data is available. We introduce diagnostic checks on the process of building the artificial population, and we demonstrate how to use such an artificial population for design-based simulation studies to compare and evaluate SAE models, using real data from the United States Department of Agriculture, Forest Service, Forest Inventory and Analysis Program, the NFI of the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15607v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grayson W. White, Jerzy A. Wieczorek, Zachariah W. Cody, Emily X. Tan, Jacqueline O. Chistolini, Kelly S. McConville, Tracey S. Frescino, Gretchen G. Moisen</dc:creator>
    </item>
    <item>
      <title>Incorporating Auxiliary Variables to Improve the Efficiency of Time-Varying Treatment Effect Estimation</title>
      <link>https://arxiv.org/abs/2306.17260</link>
      <description>arXiv:2306.17260v3 Announce Type: replace 
Abstract: Contextual sensing and delivery of digital interventions to improve health outcomes have gained significant traction in behavioral and psychiatric studies. Micro-randomized trials (MRTs) are a common experimental design for obtaining data-driven evidence on the effectiveness of digital interventions where each individual is repeatedly randomized to receive treatments over numerous time points. Throughout the study, individual characteristics and contextual factors around randomization are collected, with some prespecified as moderators for assessing time-varying causal effect moderation. However, many additional measurements beyond these moderators often go underutilized. Some of these may influence treatment randomization or known to strongly moderate the treatment effect. Incorporating such auxiliary information into the estimation procedure can reduce chance imbalances and improve asymptotic estimation efficiency. In this work, we propose a method to adjust for auxiliary variables in consistently estimating time-varying intervention effects. The approach can also be extended to include post-treatment auxiliary variables when evaluating lagged treatment effects. Under specific conditions, local efficiency gains are guaranteed. We demonstrate the method's utility through simulation studies and an analysis of data from the Intern Health Study (NeCamp et al., 2020).</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.17260v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jieru Shi, Zhenke Wu, Walter Dempsey</dc:creator>
    </item>
    <item>
      <title>Generalized Multilevel Functional Principal Component Analysis with Application to NHANES Active Inactive Patterns</title>
      <link>https://arxiv.org/abs/2311.14054</link>
      <description>arXiv:2311.14054v4 Announce Type: replace 
Abstract: Between 2011 and 2014 NHANES collected objectively measured physical activity data using wrist-worn accelerometers for tens of thousands of individuals for up to seven days. In this study, we analyze minute-level indicators of being active, which can be viewed as binary (since each minute is either active or inactive), multilevel (because there are multiple days of data for each participant), and functional data (because the within-day measurements can be viewed as a function of time). To identify both within- and between-participant directions of variation in these data, we introduce Generalized Multilevel Functional Principal Component Analysis (GM-FPCA), an approach based on the dimension reduction of the linear predictor. Our results indicate that specific activity patterns captured by GM-FPCA are strongly associated with mortality risk. Extensive simulation studies demonstrate that GM-FPCA accurately estimates model parameters, is computationally stable, and scales up with the number of study participants, visits, and observations per visit. R code for implementing the method is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.14054v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xinkai Zhou, Julia Wrobel, Ciprian M. Crainiceanu, Andrew Leroux</dc:creator>
    </item>
    <item>
      <title>Product Centered Dirichlet Processes for Bayesian Multiview Clustering</title>
      <link>https://arxiv.org/abs/2312.05365</link>
      <description>arXiv:2312.05365v4 Announce Type: replace 
Abstract: While there is an immense literature on Bayesian methods for clustering, the multiview case has received little attention. This problem focuses on obtaining distinct but statistically dependent clusterings in a common set of entities for different data types. For example, clustering patients into subgroups with subgroup membership varying according to the domain of the patient variables. A challenge is how to model the across-view dependence between the partitions of patients into subgroups. The complexities of the partition space make standard methods to model dependence, such as correlation, infeasible. In this article, we propose CLustering with Independence Centering (CLIC), a clustering prior that uses a single parameter to explicitly model dependence between clusterings across views. CLIC is induced by the product centered Dirichlet process (PCDP), a novel hierarchical prior that bridges between independent and equivalent partitions. We show appealing theoretic properties, provide a finite approximation and prove its accuracy, present a marginal Gibbs sampler for posterior computation, and derive closed form expressions for the marginal and joint partition distributions for the CLIC model. On synthetic data and in an application to epidemiology, CLIC accurately characterizes view-specific partitions while providing inference on the dependence level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05365v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Dombowsky, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Inference for Cumulative Incidences and Treatment Effects in Randomized Controlled Trials with Time-to-Event Outcomes under ICH E9 (R1)</title>
      <link>https://arxiv.org/abs/2401.14684</link>
      <description>arXiv:2401.14684v4 Announce Type: replace 
Abstract: In randomized controlled trials (RCTs) that focus on time-to-event outcomes, intercurrent events can arise in two ways: as semi-competing events, which modify the hazard of the primary outcome events, or as competing events, which make the definition of the primary outcome events unclear. Although five strategies have been proposed in the ICH E9 (R1) addendum to address intercurrent events in RCTs, these strategies are not easily applicable to time-to-event outcomes when aiming for causal interpretations. In this study, we show how to define, estimate, and make inferences concerning objectives that have causal interpretations within these contexts. Specifically, we derive the mathematical formulations of the causal estimands corresponding to the five strategies and clarify the data structure needed to identify these causal estimands. Furthermore, we introduce nonparametric methods for estimating and making inferences about these causal estimands, including the asymptotic variance of estimators and hypothesis tests. Finally, we illustrate our methods using data from the LEADER Trial, which aims to investigate the effect of liraglutide on cardiovascular outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14684v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhao Deng, Shasha Han, Xiao-Hua Zhou</dc:creator>
    </item>
    <item>
      <title>Constructing Bayesian Optimal Designs for Discrete Choice Experiments by Simulated Annealing</title>
      <link>https://arxiv.org/abs/2402.18533</link>
      <description>arXiv:2402.18533v2 Announce Type: replace 
Abstract: Discrete choice experiments (DCEs) investigate the attributes that influence individuals' choices when selecting among various options. To enhance the quality of the estimated choice models, researchers opt for Bayesian optimal designs that utilize existing information about the attributes' preferences. Given the nonlinear nature of choice models, the construction of an appropriate design requires efficient algorithms. Among these, the coordinate-exchange (CE) algorithm is commonly employed for constructing designs based on the MNL model. However, as a hill-climbing method, the CE algorithm tends to quickly converge to local optima, potentially limiting the quality of the resulting designs. We propose the use of a simulated annealing (SA) algorithm to construct Bayesian optimal designs. This algorithm accepts both superior and inferior solutions, avoiding premature convergence and allowing a more thorough exploration of potential solutions. Consequently, it ultimately obtains higher-quality choice designs compared to the CE algorithm. Our work represents the first application of an SA algorithm in constructing Bayesian optimal designs for DCEs. Through extensive computational experiments, we demonstrate that the SA designs generally outperform the CE designs in terms of statistical efficiency, especially when the prior preference information is highly uncertain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18533v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yicheng Mao, Roselinde Kessels, Tom van der Zanden</dc:creator>
    </item>
    <item>
      <title>Towards a turnkey approach to unbiased Monte Carlo estimation of smooth functions of expectations</title>
      <link>https://arxiv.org/abs/2403.20313</link>
      <description>arXiv:2403.20313v3 Announce Type: replace 
Abstract: Given a smooth function $f$, we develop a general approach to turn Monte Carlo samples with expectation $m$ into an unbiased estimate of $f(m)$. Specifically, we develop estimators that are based on randomly truncating the Taylor series expansion of $f$ and estimating the coefficients of the truncated series. We derive their properties and propose a strategy to set their tuning parameters -- which depend on $m$ -- automatically, with a view to make the whole approach simple to use. We develop our methods for the specific functions $f(x)=\log x$ and $f(x)=1/x$, as they arise in several statistical applications such as maximum likelihood estimation of latent variable models and Bayesian inference for un-normalised models. Detailed numerical studies are performed for a range of applications to determine how competitive and reliable the proposed approach is.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20313v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Chopin, Francesca R. Crucinio, Sumeetpal S. Singh</dc:creator>
    </item>
    <item>
      <title>Model-free Change-point Detection using AUC of a Classifier</title>
      <link>https://arxiv.org/abs/2404.06995</link>
      <description>arXiv:2404.06995v2 Announce Type: replace 
Abstract: In contemporary data analysis, it is increasingly common to work with non-stationary complex datasets. These datasets typically extend beyond the classical low-dimensional Euclidean space, making it challenging to detect shifts in their distribution without relying on strong structural assumptions. This paper proposes a novel offline change-point detection method that leverages classifiers developed in the statistics and machine learning community. With suitable data splitting, the test statistic is constructed through sequential computation of the Area Under the Curve (AUC) of a classifier, which is trained on data segments on both ends of the sequence. It is shown that the resulting AUC process attains its maxima at the true change-point location, which facilitates the change-point estimation. The proposed method is characterized by its complete nonparametric nature, significant versatility, considerable flexibility, and absence of stringent assumptions on the underlying data or any distributional shifts. Theoretically, we derive the limiting pivotal distribution of the proposed test statistic under null, as well as the asymptotic behaviors under both local and fixed alternatives. The localization rate of the change-point estimator is also provided. Extensive simulation studies and the analysis of two real-world datasets illustrate the superior performance of our approach compared to existing model-free change-point detection methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06995v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rohit Kanrar, Feiyu Jiang, Zhanrui Cai</dc:creator>
    </item>
    <item>
      <title>Uncertainty quantification for intervals</title>
      <link>https://arxiv.org/abs/2408.16381</link>
      <description>arXiv:2408.16381v2 Announce Type: replace 
Abstract: Data following an interval structure are increasingly prevalent in many scientific applications. In medicine, clinical events are often monitored between two clinical visits, making the exact time of the event unknown and generating outcomes with a range format. As interest in automating healthcare decisions grows, uncertainty quantification via predictive regions becomes essential for developing reliable and trustworthy predictive algorithms. However, the statistical literature currently lacks a general methodology for interval targets, especially when these outcomes are incomplete due to censoring. We propose an uncertainty quantification algorithm for interval responses and establish its theoretical properties using empirical process arguments based on a newly developed class of functions specifically designed for these interval data structures. Although this paper primarily focuses on deriving predictive regions for interval-censored data, the approach can also be applied to other statistical modeling tasks, such as goodness-of-fit assessments. Finally, the applicability of the method is demonstrated through simulations, showing up to a 60\% improvement in conditional coverage. Our new algorithm is also applied to various biomedical contexts, including two clinical examples: i) sleep duration and its association with cardiovascular diseases, and ii) survival time in relation to physical activity levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16381v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Carlos Garc\'ia Meixide, Michael R. Kosorok, Marcos Matabuena</dc:creator>
    </item>
    <item>
      <title>Doubly unfolded adjacency spectral embedding of dynamic multiplex graphs</title>
      <link>https://arxiv.org/abs/2410.09810</link>
      <description>arXiv:2410.09810v2 Announce Type: replace 
Abstract: Many real-world networks evolve dynamically over time and present different types of connections between nodes, often called layers. In this work, we propose a latent position model for these objects, called the dynamic multiplex random dot product graph (DMPRDPG), which uses an inner product between layer-specific and time-specific latent representations of the nodes to obtain edge probabilities. We further introduce a computationally efficient spectral embedding method for estimation of DMPRDPG parameters, called doubly unfolded adjacency spectral embedding (DUASE). The DUASE estimates are proved to be both consistent and asymptotically normally distributed. A key strength of our method is the encoding of time-specific node representations and layer-specific effects in separate latent spaces, which allows the model to capture complex behaviors while maintaining relatively low dimensionality. The embedding method we propose can also be efficiently used for subsequent inference tasks. In particular, we highlight the use of the ISOMAP algorithm in conjunction with DUASE as a way to efficiently capture trends and global changepoints within a network, and the use of DUASE for graph clustering. Applications on real-world networks describing geopolitical interactions between countries and financial news reporting demonstrate practical uses of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09810v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Maximilian Baum, Francesco Sanna Passino, Axel Gandy</dc:creator>
    </item>
    <item>
      <title>Relational Graph in Vector Autoregression: A Case Study on the Effect of the Great Recession on Connectivity of Economic Indicators</title>
      <link>https://arxiv.org/abs/2410.22617</link>
      <description>arXiv:2410.22617v2 Announce Type: replace 
Abstract: Under a high-dimensional vector autoregressive (VAR) model, we propose a way of efficiently estimating both the stationary graph structure between the nodal time series and their temporal dynamics. The framework is then used to make inferences on the change in interdependencies between several economic indicators due to the impact of the Great Recession, the financial crisis that lasted from 2007 through 2009. There are several key advantages of the proposed framework; (1) it develops a reparametrized VAR likelihood that can be used in general high-dimensional VAR problems, (2) it strictly maintains causality of the estimated process, making inference on stationary features more meaningful and (3) it is computationally efficient due to the reduced rank structure of the parameterization. We apply the methodology to the seasonally adjusted quarterly economic indicators available in the FRED-QD database of the Federal Reserve. The analysis essentially confirms much of the prevailing knowledge about the impact of the Great Recession on different economic indicators. At the same time, it provides deeper insight into the nature and extent of the impact on the interplay of the different indicators. We also contribute to the theory of Bayesian VAR by showing the consistency of the posterior under sparse priors for the parameters of the reduced rank formulation of the VAR process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22617v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arkaprava Roy, Anindya Roy, Subhashis Ghosal</dc:creator>
    </item>
    <item>
      <title>New technique for parameter estimation and improved fits to experimental data for a set of compound Poisson distributions</title>
      <link>https://arxiv.org/abs/2502.03237</link>
      <description>arXiv:2502.03237v2 Announce Type: replace 
Abstract: Compound Poisson distributions have been employed by many authors to fit experimental data, typically via the method of moments or maximum likelihood estimation. We propose a new technique and apply it to several sets of published data. It yields better fits than those obtained by the original authors for a set of widely employed compound Poisson distributions (in some cases, significantly better). The technique employs the power spectrum (the absolute square of the characteristic function). The new idea is suggested as a useful addition to the tools for parameter estimation of compound Poisson distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03237v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S. R. Mane</dc:creator>
    </item>
    <item>
      <title>Bayesian Deep Latent Class Regression</title>
      <link>https://arxiv.org/abs/2503.17531</link>
      <description>arXiv:2503.17531v2 Announce Type: replace 
Abstract: High-dimensional categorical data arise in diverse scientific domains and are often accompanied by covariates. Latent class regression models are routinely used in such settings, reducing dimensionality by assuming conditional independence of the categorical variables given a single latent class that depends on covariates through a logistic regression model. However, such methods become unreliable as the dimensionality increases. To address this, we propose a flexible family of deep latent class models. Our model satisfies key theoretical properties, including identifiability and posterior consistency, and we establish a Bayes oracle clustering property that ensures robustness against the curse of dimensionality. We develop efficient posterior computation methods, validate them through simulation studies, and apply our model to joint species distribution modeling in ecology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17531v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuren Zhou, Yuqi Gu, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>A Restricted Latent Class Hidden Markov Model for Polytomous Responses, Polytomous Attributes, and Covariates: Identifiability and Application</title>
      <link>https://arxiv.org/abs/2503.20940</link>
      <description>arXiv:2503.20940v2 Announce Type: replace 
Abstract: We introduce a restricted latent class exploratory model for longitudinal data with ordinal attributes and respondent-specific covariates. Responses follow a hidden Markov model where the probability of a particular latent state at a time point is conditional on values at the previous time point of the respondent's covariates and latent state. We prove that the model is identifiable, state a Bayesian formulation, and demonstrate its efficacy in a variety of scenarios through a simulation study. As a real-world demonstration, we apply the model to response data from a mathematics examination, and compare the results to a previously published confirmatory analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20940v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Alan Wayman, Steven Andrew Culpepper, Jeff Douglas, Jesse Bowers</dc:creator>
    </item>
    <item>
      <title>Inference in stochastic differential equations using the Laplace approximation: Demonstration and examples</title>
      <link>https://arxiv.org/abs/2503.21358</link>
      <description>arXiv:2503.21358v2 Announce Type: replace 
Abstract: We consider the problem of estimating states and parameters in a model based on a system of coupled stochastic differential equations, based on noisy discrete-time data. Special attention is given to nonlinear dynamics and state-dependent diffusivity, where transition densities are not available in closed form. Our technique adds states between times of observations, approximates transition densities using, e.g., the Euler-Maruyama method and eliminates unobserved states using the Laplace approximation. Using case studies, we demonstrate that transition probabilities are well approximated, and that inference is computationally feasible. We discuss limitations and potential extensions of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21358v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Uffe H{\o}gsbro Thygesen, Kasper Kristensen</dc:creator>
    </item>
    <item>
      <title>Correlation Improves Group Testing: Modeling Concentration-Dependent Test Errors</title>
      <link>https://arxiv.org/abs/2111.07517</link>
      <description>arXiv:2111.07517v5 Announce Type: replace-cross 
Abstract: Population-wide screening is a powerful tool for controlling infectious diseases. Group testing enables such screening despite limited resources. Viral concentration of pooled samples are often positively correlated, either because prevalence and sample collection are influenced by location, or through intentional enhancement via pooling samples according to risk/household. Such correlation is known to improve efficiency under fixed test sensitivity. However, in reality, a test's sensitivity depends on the concentration of the analyte (e.g., viral RNA), as in the so-called dilution effect, where sensitivity decreases for larger pools. We show that concentration-dependent test error alters correlation's effect under the most widely-used group testing procedure, the two-stage Dorfman procedure. We prove that when test sensitivity increases with concentration, pooling correlated samples together (correlated pooling) achieves asymptotically higher sensitivity than independently pooling the samples (naive pooling). In contrast, in the concentration-independent case, correlation does not affect sensitivity. Moreover, with concentration-dependent errors, correlation can degrade test efficiency compared to naive pooling whereas under concentration-independent errors, correlation always improves efficiency. We propose an alternative measure of test resource usage, the number of positives found per test consumed, which we argue is better aligned with infection control, and show that correlated pooling outperforms naive pooling on this measure. In simulation, we show that the effect of correlation under realistic concentration-dependent test error meaningfully differs from correlation's effect assuming fixed sensitivity. Our findings underscore the importance for policy-makers of using models that incorporate naturally-occurring correlation and of considering ways of strengthening this correlation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.07517v5</guid>
      <category>stat.AP</category>
      <category>physics.soc-ph</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jiayue Wan, Yujia Zhang, Peter I. Frazier</dc:creator>
    </item>
    <item>
      <title>Asymptotic theory for Bayesian inference and prediction: from the ordinary to a conditional Peaks-Over-Threshold method</title>
      <link>https://arxiv.org/abs/2310.06720</link>
      <description>arXiv:2310.06720v2 Announce Type: replace-cross 
Abstract: The Peaks Over Threshold (POT) method is the most popular statistical method for the analysis of univariate extremes. Even though there is a rich applied literature on Bayesian inference for the POT, the asymptotic theory for such proposals is missing. Even more importantly, the ambitious and challenging problem of predicting future extreme events according to a proper predictive statistical approach has received no attention to date. In this paper we fill this gap by developing the asymptotic theory of posterior distributions (consistency, contraction rates, asymptotic normality and asymptotic coverage of credible intervals) and prediction within the Bayesian framework in the POT context. We extend this asymptotic theory to account for cases where the focus is on the tail properties of the conditional distribution of a response variable given a vector of random covariates. To enable accurate predictions of extreme events more severe than those previously observed, we derive the posterior predictive distribution as an estimator of the conditional distribution of an out-of-sample random variable, given that it exceeds a sufficiently high threshold. We establish Wasserstein consistency of the posterior predictive distribution under both the unconditional and covariate-conditional approaches and derive its contraction rates. Simulations show the good performances of the proposed Bayesian inferential methods. The analysis of the change in the frequency of financial crises over time shows the utility of our methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06720v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cl\'ement Dombry, Simone A. Padoan, Stefano Rizzelli</dc:creator>
    </item>
    <item>
      <title>Detection of evolutionary shifts in variance under an Ornsten-Uhlenbeck model</title>
      <link>https://arxiv.org/abs/2312.17480</link>
      <description>arXiv:2312.17480v2 Announce Type: replace-cross 
Abstract: Abrupt environmental changes can lead to evolutionary shifts in not only the optimal trait value, but also the rate of adaptation and the diffusion variance in trait evolution. While several methods exist for detecting shifts in optimal values, few explicitly model shifts in both evolutionary variance and adaptation rates. We use a multi-optima and multi-variance Ornstein-Uhlenbeck (OU) process model to describe trait evolution with shifts in both optimal value and diffusion variance and analyze how covariance between species is affected when shifts in variance occur along the phylogeny. We propose a new method that simultaneously detects shifts in both variance and optimal values by formulating the problem as a variable selection task using an L1-penalized loss function. Our method is implemented in the R package ShiVa (Detection of evolutionary Shifts in Variance). Through simulations, we compare ShiVa with methods that only consider shifts in optimal values (l1ou; PhylogeneticEM), and PCMFit. Our method demonstrates improved predictive ability and significantly reduces false positives in detecting optimal value shifts when variance shifts are present. When only shifts in optimal value occur, our method performs comparably to existing approaches. Applying ShiVa to empirical data from cordylid lizards , we find that it outperforms l1ou and PhylogeneticEM, achieving the highest log-likelihood and lowest BIC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17480v2</guid>
      <category>q-bio.PE</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wensha Zhang, Lam Si Tung Ho, Toby Kenney</dc:creator>
    </item>
    <item>
      <title>Identification by non-Gaussianity in structural threshold and smooth transition vector autoregressive models</title>
      <link>https://arxiv.org/abs/2404.19707</link>
      <description>arXiv:2404.19707v4 Announce Type: replace-cross 
Abstract: Linear structural vector autoregressive models can be identified statistically without imposing restrictions on the model if the shocks are mutually independent and at most one of them is Gaussian. We show that this result extends to structural threshold and smooth transition vector autoregressive models incorporating a time-varying impact matrix defined as a weighted sum of the impact matrices of the regimes. We also discuss the problem of labelling the shocks, estimation of the parameters, and stationarity the model. The introduced methods are implemented to the accompanying R package sstvars. Our empirical application studies the effects of the climate policy uncertainty shock on the U.S. macroeconomy. In a structural logistic smooth transition vector autoregressive model with two regimes, we find that a positive climate policy uncertainty shock decreases production and increases inflation in times of both low and high economic policy uncertainty, but its inflationary effects are stronger in the periods of high economic policy uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19707v4</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Savi Virolainen</dc:creator>
    </item>
    <item>
      <title>Theoretical Foundations of Conformal Prediction</title>
      <link>https://arxiv.org/abs/2411.11824</link>
      <description>arXiv:2411.11824v2 Announce Type: replace-cross 
Abstract: This book is about conformal prediction and related inferential techniques that build on permutation tests and exchangeability. These techniques are useful in a diverse array of tasks, including hypothesis testing and providing uncertainty quantification guarantees for machine learning systems. Much of the current interest in conformal prediction is due to its ability to integrate into complex machine learning workflows, solving the problem of forming prediction sets without any assumptions on the form of the data generating distribution. Since contemporary machine learning algorithms have generally proven difficult to analyze directly, conformal prediction's main appeal is its ability to provide formal, finite-sample guarantees when paired with such methods.
  The goal of this book is to teach the reader about the fundamental technical arguments that arise when researching conformal prediction and related questions in distribution-free inference. Many of these proof strategies, especially the more recent ones, are scattered among research papers, making it difficult for researchers to understand where to look, which results are important, and how exactly the proofs work. We hope to bridge this gap by curating what we believe to be some of the most important results in the literature and presenting their proofs in a unified language, with illustrations, and with an eye towards pedagogy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11824v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasios N. Angelopoulos, Rina Foygel Barber, Stephen Bates</dc:creator>
    </item>
    <item>
      <title>To Study Properties of a Known Procedure in Adaptive Sequential Sampling Design</title>
      <link>https://arxiv.org/abs/2412.17791</link>
      <description>arXiv:2412.17791v2 Announce Type: replace-cross 
Abstract: We consider the procedure proposed by Bhandari et al. (2009) in the context of two-treatment clinical trials, with the objective of minimizing the applications of the less effective drug to the least number of patients. Our focus is on an adaptive sequential procedure that is both simple and intuitive. Our findings show that the number of applications of the less effective drug is a finite random variable whose all moments are also finite. In contrast, Bhandari et al. (2009) observed that this number increases logarithmically with the total sample size. We attribute this discrepancy to differences in their choice of starting sample size and the method of analysis employed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17791v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sampurna Kundu, Jayant Jha, Subir Kumar Bhandari</dc:creator>
    </item>
    <item>
      <title>Learning dynamical systems with hit-and-run random feature maps</title>
      <link>https://arxiv.org/abs/2501.06661</link>
      <description>arXiv:2501.06661v2 Announce Type: replace-cross 
Abstract: We show how random feature maps can be used to forecast dynamical systems with excellent forecasting skill. We consider the tanh activation function and judiciously choose the internal weights in a data-driven manner such that the resulting features explore the nonlinear, non-saturated regions of the activation function. We introduce skip connections and construct a deep variant of random feature maps by combining several units. To mitigate the curse of dimensionality, we introduce localization where we learn local maps, employing conditional independence. Our modified random feature maps provide excellent forecasting skill for both single trajectory forecasts as well as long-time estimates of statistical properties, for a range of chaotic dynamical systems with dimensions up to 512. In contrast to other methods such as reservoir computers which require extensive hyperparameter tuning, we effectively need to tune only a single hyperparameter, and are able to achieve state-of-the-art forecast skill with much smaller networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06661v2</guid>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pinak Mandal, Georg A. Gottwald</dc:creator>
    </item>
    <item>
      <title>Improving variable selection properties by leveraging external data</title>
      <link>https://arxiv.org/abs/2502.15584</link>
      <description>arXiv:2502.15584v2 Announce Type: replace-cross 
Abstract: Sparse high-dimensional signal recovery is only possible under certain conditions on the number of parameters, sample size, signal strength and underlying sparsity. We show that leveraging external information, as possible with data integration or transfer learning, allows to push these mathematical limits. Specifically, we consider external information that allows splitting parameters into blocks, first in a simplified case, the Gaussian sequence model, and then in the general linear regression setting. We show how external information dependent, block-based, $\ell_0$ penalties attain model selection consistency under milder conditions than standard $\ell_0$ penalties, and they also attain faster model recovery rates. We first provide results for oracle-based $\ell_0$ penalties that have access to perfect sparsity and signal strength information. Subsequently, we propose an empirical Bayes data analysis method that does not require oracle information and for which efficient computation is possible via standard MCMC techniques. Our results provide a mathematical basis to justify the use of data integration methods in high-dimensional structural learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15584v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Rognon-Vael, David Rossell, Piotr Zwiernik</dc:creator>
    </item>
    <item>
      <title>Bayesian analysis of restricted mean survival time adjusted for covariates using pseudo-observations</title>
      <link>https://arxiv.org/abs/2503.05225</link>
      <description>arXiv:2503.05225v3 Announce Type: replace-cross 
Abstract: The difference in restricted mean survival time (RMST) is a clinically meaningful measure to quantify treatment effect in randomized controlled trials, especially when the proportional hazards assumption does not hold. Several frequentist methods exist to estimate RMST adjusted for covariates based on modeling and integrating the survival function. A more natural approach may be a regression model on RMST using pseudo-observations, which allows for a direct estimation without modeling the survival function. Only a few Bayesian methods exist, and each requires a model of the survival function. We developed a new Bayesian method that combines the use of pseudo-observations with the generalized method of moments. This offers RMST estimation adjusted for covariates without the need to model the survival function, making it more attractive than existing Bayesian methods. A simulation study was conducted with different time-dependent treatment effects (early, delayed, and crossing survival) and covariate effects, showing that our approach provides valid results, aligns with existing methods, and shows improved precision after covariate adjustment. For illustration, we applied our approach to a phase III trial in prostate cancer, providing estimates of the treatment effect on RMST, comparable to existing methods. In addition, our approach provided the effect of other covariates on RMST and determined the posterior probability of the difference in RMST exceeds any given time threshold for any covariate, allowing for nuanced and interpretable results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05225v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'ea Orsini (CESP, U1018), Emmanuel Lesaffre (KU Leuven), Guosheng Yin (DSAS), Caroline Brard (U1018), David Dejardin (U1018), Gw\'ena\"el Le Teuff (U1018)</dc:creator>
    </item>
    <item>
      <title>Beyond Group Means and Into the World of Individuals: A Distributional Spotlight for Experimental Effects on Individuals</title>
      <link>https://arxiv.org/abs/2503.17390</link>
      <description>arXiv:2503.17390v2 Announce Type: replace-cross 
Abstract: Traditionally, experimental effects on humans are investigated at the group level. In this work, we present a distributional ``spotlight'' to investigate experimental effects at the individual level. Specifically, we estimate the effects on individuals through the changes in the probability distributions of their experimental data across conditions. We test this approach on Reaction Time (RT) data from 10 individuals in a visual search task, examining the effects of (1) information set sizes and (2) the presence or absence of a target on their processing speed. The changes in individuals' RT distributions are measured using three approaches: (i) direct measurements of distributional changes are compared against the changes captured by two established models of RT: (ii) the ex-Gaussian distribution and (iii) the Drift-Diffusion model. We find that direct measurement of distributional changes provides the clearest view of the effects on individuals and highlights the presence of two sub-groups based on the effects experienced: one that shows neither effect and the other showing only the target-presence effect. Moreover, the intra-individual changes across conditions (i.e., the experimental effects) appear much smaller than the inter-individual differences (i.e., the random effects). Generally, these results highlight the merits of going beyond group means and examining the effects on individuals, as well as the effectiveness of the distributional spotlight in such pursuits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17390v2</guid>
      <category>physics.soc-ph</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roussel Rahman</dc:creator>
    </item>
    <item>
      <title>Transition probabilities for stochastic differential equations using the Laplace approximation: Analysis of the continuous-time limit</title>
      <link>https://arxiv.org/abs/2503.21399</link>
      <description>arXiv:2503.21399v2 Announce Type: replace-cross 
Abstract: We recently proposed a method for estimation of states and parameters in stochastic differential equations, which included intermediate time points between observations and used the Laplace approximation to integrate out these intermediate states. In this paper, we establish a Laplace approximation for the transition probabilities in the continuous-time limit where the computational time step between intermediate states vanishes. Our technique views the driving Brownian motion as a control, casts the problem as one of minimum effort control between two states, and employs a Girsanov shift of probability measure as well as a weak noise approximation to obtain the Laplace approximation. We demonstrate the technique with examples; one where the approximation is exact due to a property of coordinate transforms, and one where contributions from non-near paths impair the approximation. We assess the order of discrete-time scheme, and demonstrate the Strang splitting leads to higher order and accuracy than Euler-type discretization. Finally, we investigate numerically how the accuracy of the approximation depends on the noise intensity and the length of the time interval.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21399v2</guid>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Uffe H{\o}gsbro Thygesen</dc:creator>
    </item>
  </channel>
</rss>
