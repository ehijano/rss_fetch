<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 06 Nov 2024 02:53:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Covariance Analysis of Impulsive Streaking</title>
      <link>https://arxiv.org/abs/2411.01729</link>
      <description>arXiv:2411.01729v1 Announce Type: new 
Abstract: A comprehensive framework of modeling covariance in angular streaking experiments is presented. Within the impulsive streaking regime, the displacement of electron momentum distribution (MD) provides a tight connection between the dressing-free MD and the dressed MD. Such connection establishes universal structures in the composition of streaking covariance that are common across different MDs, regardless of their exact shape. Building on this robust framework, we have developed methods for retrieving temporal information from angular streaking measurements. By providing a detailed understanding of the covariance structure in angular streaking experiments, our work enables more accurate and robust temporal measurements in a wide range of experimental scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01729v1</guid>
      <category>physics.data-an</category>
      <category>physics.atom-ph</category>
      <category>physics.optics</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jun Wang, Zhaoheng Guo, Erik Isele, Philip H. Bucksbaum, Agostino Marinelli, James P. Cryan, Taran Driver</dc:creator>
    </item>
    <item>
      <title>Designing a Dataset for Convolutional Neural Networks to Predict Space Groups Consistent with Extinction Laws</title>
      <link>https://arxiv.org/abs/2411.00803</link>
      <description>arXiv:2411.00803v1 Announce Type: cross 
Abstract: In this paper, we utilize a dataset composed of one-dimensional powder diffraction patterns to train Convolutional Neural Networks for predicting space groups. We used a new strategy to design the dataset, the diffraction pattern was calculated based the lattice parameters and the Extinction Laws, instead of the traditional strategy that generating it from the crystallographic database. This paper demonstrated that the new strategy is more reasonable than the traditional one. As a result, the model trained on the cubic and tetragonal training set from the newly designed dataset achieves prediction accuracy that matches the theoretical maximums calculated based on Extinction Laws. This result demonstrates that the machine learning based prediction can be physically reasonable and reliable. Additionally, the model trained on our new designed dataset shows better generalization capability than the one trained on a traditionally designed dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00803v1</guid>
      <category>cs.NE</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Wang, Jiajun Zhong, Yikun Li, Junrong Zhang, Rong Du</dc:creator>
    </item>
    <item>
      <title>Re-thinking Richardson-Lucy without Iteration Cutoffs: Physically Motivated Bayesian Deconvolution</title>
      <link>https://arxiv.org/abs/2411.00991</link>
      <description>arXiv:2411.00991v1 Announce Type: cross 
Abstract: Richardson-Lucy deconvolution is widely used to restore images from degradation caused by the broadening effects of a point spread function and corruption by photon shot noise, in order to recover an underlying object. In practice, this is achieved by iteratively maximizing a Poisson emission likelihood. However, the RL algorithm is known to prefer sparse solutions and overfit noise, leading to high-frequency artifacts. The structure of these artifacts is sensitive to the number of RL iterations, and this parameter is typically hand-tuned to achieve reasonable perceptual quality of the inferred object. Overfitting can be mitigated by introducing tunable regularizers or other ad hoc iteration cutoffs in the optimization as otherwise incorporating fully realistic models can introduce computational bottlenecks. To resolve these problems, we present Bayesian deconvolution, a rigorous deconvolution framework that combines a physically accurate image formation model avoiding the challenges inherent to the RL approach. Our approach achieves deconvolution while satisfying the following desiderata:
  I deconvolution is performed in the spatial domain (as opposed to the frequency domain) where all known noise sources are accurately modeled and integrated in the spirit of providing full probability distributions over the density of the putative object recovered;
  II the probability distribution is estimated without making assumptions on the sparsity or continuity of the underlying object;
  III unsupervised inference is performed and converges to a stable solution with no user-dependent parameter tuning or iteration cutoff;
  IV deconvolution produces strictly positive solutions; and
  V implementation is amenable to fast, parallelizable computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00991v1</guid>
      <category>cs.CV</category>
      <category>astro-ph.IM</category>
      <category>physics.bio-ph</category>
      <category>physics.data-an</category>
      <category>physics.optics</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zachary H. Hendrix, Peter T. Brown, Tim Flanagan, Douglas P. Shepherd, Ayush Saurabh, Steve Press\'e</dc:creator>
    </item>
    <item>
      <title>Are EEG functional networks really describing the brain? A comparison with other information-processing complex systems</title>
      <link>https://arxiv.org/abs/2411.01522</link>
      <description>arXiv:2411.01522v1 Announce Type: cross 
Abstract: Functional networks representing human brain dynamics have become a standard tool in neuroscience, providing an accessible way of depicting the computation performed by the brain in healthy and pathological conditions. Yet, these networks share multiple characteristics with those representing other natural and man-made complex systems, leading to the question of whether they are actually capturing the uniqueness of the human brain. By resorting to a large set of data representing multiple financial, technological, social, and natural complex systems, and by relying on Deep Learning classification models, we show how they are highly similar. We specifically reach the conclusion that, under some general reconstruction methodological choices, it is as difficult to understand whether a network represents a human brain or a financial market, as to diagnose a major pathology. This suggests that functional networks are describing information processing mechanisms that are common across complex systems; but that are not currently defining the uniqueness of the human mind. We discuss the consequence of these findings for neuroscience and complexity science in general, and suggest future avenues for exploring this interesting topic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01522v1</guid>
      <category>q-bio.NC</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sofia Gil-Rodrigo, Ra\'ul L\'opez-Mart\'in, G\"orsev Yener, Jan R. Wiersema, Bahar G\"untekin, Massimiliano Zanin</dc:creator>
    </item>
    <item>
      <title>Towards certification: A complete statistical validation pipeline for supervised learning in industry</title>
      <link>https://arxiv.org/abs/2411.02075</link>
      <description>arXiv:2411.02075v1 Announce Type: cross 
Abstract: Methods of Machine and Deep Learning are gradually being integrated into industrial operations, albeit at different speeds for different types of industries. The aerospace and aeronautical industries have recently developed a roadmap for concepts of design assurance and integration of neural network-related technologies in the aeronautical sector. This paper aims to contribute to this paradigm of AI-based certification in the context of supervised learning, by outlining a complete validation pipeline that integrates deep learning, optimization and statistical methods. This pipeline is composed by a directed graphical model of ten steps. Each of these steps is addressed by a merging key concepts from different contributing disciplines (from machine learning or optimization to statistics) and adapting them to an industrial scenario, as well as by developing computationally efficient algorithmic solutions. We illustrate the application of this pipeline in a realistic supervised problem arising in aerostructural design: predicting the likelikood of different stress-related failure modes during different airflight maneuvers based on a (large) set of features characterising the aircraft internal loads and geometric parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02075v1</guid>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Lacasa, Abel Pardo, Pablo Arbelo, Miguel S\'anchez, Pablo Yeste, Noelia Bascones, Alejandro Mart\'inez-Cava, Gonzalo Rubio, Ignacio G\'omez, Eusebio Valero, Javier de Vicente</dc:creator>
    </item>
    <item>
      <title>Building robust surrogate models of laser-plasma interactions using large scale PIC simulation</title>
      <link>https://arxiv.org/abs/2411.02079</link>
      <description>arXiv:2411.02079v1 Announce Type: cross 
Abstract: As the repetition rates of ultra-high intensity lasers increase, simulations used for the prediction of experimental results may need to be augmented with machine learning to keep up. In this paper, the usage of gaussian process regression in producing surrogate models of laser-plasma interactions from particle-in-cell simulations is investigated. Such a model retains the characteristic behaviour of the simulations but allows for faster on-demand results and estimation of statistical noise. A demonstrative model of Bremsstrahlung emission by hot electrons from a femtosecond timescale laser pulse in the $10^{20} - 10^{23}\;\mathrm{Wcm}^{-2}$ intensity range is produced using 800 simulations of such a laser-solid interaction from 1D hybrid-PIC. While the simulations required 84,000 CPU-hours to generate, subsequent training occurs on the order of a minute on a single core and prediction takes only a fraction of a second. The model trained on this data is then compared against analytical expectations. The efficiency of training the model and its subsequent ability to distinguish types of noise within the data are analysed, and as a result error bounds on the model are defined.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02079v1</guid>
      <category>physics.plasm-ph</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Smith, Christopher Ridgers, Kate Lancaster, Chris Arran, Stuart Morris</dc:creator>
    </item>
    <item>
      <title>Neural optical flow for planar and stereo PIV</title>
      <link>https://arxiv.org/abs/2411.02373</link>
      <description>arXiv:2411.02373v1 Announce Type: cross 
Abstract: Neural optical flow (NOF) offers improved accuracy and robustness over existing OF methods for particle image velocimetry (PIV). Unlike other OF techniques, which rely on discrete displacement fields, NOF parameterizes the physical velocity field using a continuous neural-implicit representation. This formulation enables efficient data assimilation and ensures consistent regularization across views for stereo PIV. The neural-implicit architecture provides significant data compression and supports a space-time formulation, facilitating the analysis of both steady and unsteady flows. NOF incorporates a differentiable, nonlinear image-warping operator that relates particle motion to intensity changes between frames. Discrepancies between the advected intensity field and observed images form the data loss, while soft constraints, such as Navier-Stokes residuals, enhance accuracy and enable direct pressure inference from PIV images. Additionally, mass continuity can be imposed as a hard constraint for both 2D and 3D flows. Implicit regularization is achieved by tailoring the network's expressivity to match a target flow's spectral characteristics. Results from synthetic planar and stereo PIV datasets, as well as experimental planar data, demonstrate NOF's effectiveness compared to state-of-the-art wavelet-based OF and CC methods. Additionally, we highlight its potential broader applicability to techniques like background-oriented schlieren, molecular tagging velocimetry, and other advanced measurement systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02373v1</guid>
      <category>physics.flu-dyn</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew I. Masker, Ke Zhou, Joseph P. Molnar, Samuel J. Grauer</dc:creator>
    </item>
    <item>
      <title>Graph Frequency Features of Cancer Gene Co-Expression Networks</title>
      <link>https://arxiv.org/abs/2311.06747</link>
      <description>arXiv:2311.06747v2 Announce Type: replace-cross 
Abstract: Complex gene interactions play a significant role in cancer progression, driving cellular behaviors that contribute to tumor growth, invasion, and metastasis. Gene co-expression networks model the functional connectivity between genes under various biological conditions. Understanding the system-level evolution of these networks in cancer is critical for elucidating disease mechanisms and informing the development of targeted therapies. While previous studies have primarily focused on structural differences between cancer and normal cell co-expression networks, this study applies graph frequency analysis to cancer transcriptomic signals defined on gene co-expression networks, highlighting the graph spectral characteristics of cancer systems. Using a range of graph frequency filters, we showed that cancer cells display distinctive patterns in the graph frequency content of their gene transcriptomic signals, effectively distinguishing between cancer types and stages. The transformation of the original gene feature space into the graph spectral space captured more intricate cancer properties, as validated by significantly higher F-statistic scores for graph frequency-filtered gene features compared to those in the original space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.06747v2</guid>
      <category>q-bio.MN</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Radwa Adel, Ercan Engin Kuruoglu</dc:creator>
    </item>
    <item>
      <title>Global Optimisation of Black-Box Functions with Generative Models in the Wasserstein Space</title>
      <link>https://arxiv.org/abs/2407.11917</link>
      <description>arXiv:2407.11917v3 Announce Type: replace-cross 
Abstract: We propose a new uncertainty estimator for gradient-free optimisation of black-box simulators using deep generative surrogate models. Optimisation of these simulators is especially challenging for stochastic simulators and higher dimensions. To address these issues, we utilise a deep generative surrogate approach to model the black box response for the entire parameter space. We then leverage this knowledge to estimate the proposed uncertainty based on the Wasserstein distance - the Wasserstein uncertainty. This approach is employed in a posterior agnostic gradient-free optimisation algorithm that minimises regret over the entire parameter space. A series of tests were conducted to demonstrate that our method is more robust to the shape of both the black box function and the stochastic response of the black box than state-of-the-art methods, such as efficient global optimisation with a deep Gaussian process surrogate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11917v3</guid>
      <category>cs.LG</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3233/FAIA240765</arxiv:DOI>
      <dc:creator>Tigran Ramazyan, Mikhail Hushchyn, Denis Derkach</dc:creator>
    </item>
    <item>
      <title>On marginals and profiled posteriors for cosmological parameter estimation</title>
      <link>https://arxiv.org/abs/2408.02063</link>
      <description>arXiv:2408.02063v2 Announce Type: replace-cross 
Abstract: With several examples and in an analysis of the Pantheon+ supernova sample we discuss the properties of the marginal posterior distribution versus the profiled posterior distribution -- the profile likelihood in a Bayesian disguise. We investigate whether maximisation, as used for the profiling, or integration, as used for the marginalisation, is more appropriate. To report results we recommend the marginal posterior distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02063v2</guid>
      <category>astro-ph.CO</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 05 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Kerscher, Jochen Weller</dc:creator>
    </item>
  </channel>
</rss>
