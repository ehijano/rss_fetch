<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Oct 2025 04:01:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Data-driven Calibration Sample Selection and Forecast Combination in Electricity Price Forecasting: An Application of the ARHNN Method</title>
      <link>https://arxiv.org/abs/2510.15011</link>
      <description>arXiv:2510.15011v1 Announce Type: new 
Abstract: Calibration sample selection and forecast combination are two simple yet powerful tools used in forecasting. They can be combined with a variety of models to significantly improve prediction accuracy, at the same time offering easy implementation and low computational complexity. While their effectiveness has been repeatedly confirmed in prior scientific literature, the topic is still underexplored in the field of electricity price forecasting. In this research article we apply the Autoregressive Hybrid Nearest Neighbors (ARHNN) method to three long-term time series describing the German, Spanish and New England electricity markets. We show that it outperforms popular literature benchmarks in terms of forecast accuracy by up to 10%. We also propose two simplified variants of the method, granting a vast decrease in computation time with only minor loss of prediction accuracy. Finally, we compare the forecasts' performance in a battery storage system trading case study. We find that using a forecast-driven strategy can achieve up to 80% of theoretical maximum profits while trading, demonstrating business value in practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15011v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomasz Serafin, Weronika Nitka</dc:creator>
    </item>
    <item>
      <title>Bayesian Additive Regression Trees (BART) in Food Authenticity: A Classification Approach to Food Fraud Detection</title>
      <link>https://arxiv.org/abs/2510.15105</link>
      <description>arXiv:2510.15105v1 Announce Type: new 
Abstract: Feature engineering plays a critical role in handling hyperspectral data and is essential for identifying key wavelengths in food fraud detection. This study employs Bayesian Additive Regression Trees (BART), a flexible machine learning approach, to discriminate and classify samples of olive oil based on their level of purity. Leveraging its built-in variable selection mechanism, we employ BART to effectively identify the most representative spectral features and to capture the complex interactions among variables. We use network representation to illustrate our findings, highlighting the competitiveness of our proposed methodology. Results demonstrate that when principal component analysis is used for dimensionality reduction, BART outperforms state-of-the-art models, achieving a classification accuracy of 96.8\% under default settings, which further improves to 97.2\% after hyperparameter tuning. If we leverage a variable selection procedure within BART, the model achieves perfect classification performance on this dataset, improving upon previous optimal results both in terms of accuracy and interpretability. Our results demonstrate that three key wavelengths, 1160.71 nm, 1328.57 nm, and 1389.29 nm, play a central role in discriminating the olive oil samples, thus highlighting an application of our methodology in the context of food quality. Further analysis reveals that these variables do not function independently but rather interact synergistically to achieve accurate classification, and improved detection speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15105v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengxiang Zhu, Riccardo Rastelli</dc:creator>
    </item>
    <item>
      <title>AI and analytics in sports: Leveraging BERTopic to map the past and chart the future</title>
      <link>https://arxiv.org/abs/2510.15487</link>
      <description>arXiv:2510.15487v1 Announce Type: new 
Abstract: Purpose: The purpose of this study is to map the body of scholarly literature at the intersection of artificial intelligence (AI), analytics and sports and thereafter, leverage the insights generated to chart guideposts for future research. Design/methodology/approach: The study carries out systematic literature review (SLR). Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) protocol is leveraged to identify 204 journal articles pertaining to utilization of AI and analytics in sports published during 2002 to 2024. We follow it up with extraction of the latent topics from sampled articles by leveraging the topic modelling technique of BERTopic. Findings: The study identifies the following as predominant areas of extant research on usage of AI and analytics in sports: performance modelling, physical and mental health, social media sentiment analysis, and tactical tracking. Each extracted topic is further examined in terms of its relative prominence, representative studies, and key term associations. Drawing on these insights, the study delineates promising avenues for future inquiry. Research limitations/implications: The study offers insights to academicians and sports administrators on transformational impact of AI and analytics in sports. Originality/value: The study introduces BERTopic as a novel approach for extracting latent structures in sports research, thereby advancing both scholarly understanding and the methodological toolkit of the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15487v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manit Mishra</dc:creator>
    </item>
    <item>
      <title>Residual Kriging for Regional-Scale Canopy Height Mapping: Insights into GEDI-Induced Anisotropies and Sparse Sampling</title>
      <link>https://arxiv.org/abs/2510.15572</link>
      <description>arXiv:2510.15572v1 Announce Type: new 
Abstract: Quantifying aboveground biomass (AGB) is essential in the context of global climate change. Canopy height, which is related to AGB, can be mapped using machine learning models trained with multi-source spatial data and GEDI measurements. In this study, a comparative analysis of canopy height estimates derived from two models is presented: a U-Net deep learning model (CHNET) and a Random Forest algorithm (RFH). Both models were trained using GEDI lidar data and utilized multi-source inputs, including optical, radar, and environmental data. While CHNET can leverage its convolutional architecture to account for spatial correlations, we observed that it does not fully incorporate all the spatial autocorrelation present in GEDI canopy height measurements. By conducting a spatial analysis of the models' residuals, we also identified that GEDI data acquisition parameters, particularly the variability in laser beam energy combined with the azimuthal directions of the observation tracks, introduce spatial inconsistencies in the measurements in the form of periodic patterns. To address these anisotropies, we considered exclusively GEDI power beams, and we conducted our spatial autocorrelation analysis in the GEDI track azimuthal direction. Next, we employed the residual kriging (RK) spatial interpolation technique to account for the spatial autocorrelation of canopy heights and improve the accuracies of CHNET and RFH estimates. Adding RK corrections improved the performance of both CHNET and RFH, with more substantial gains observed for RFH. The corrections appeared to be localized around the GEDI sample points and the density of usable GEDI information is therefore an important factor in the effectiveness of spatial interpolation. Furthermore, our findings reveal that a Random Forest model combined with spatial interpolation can deliver performance comparable to that of a U-Net model alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15572v1</guid>
      <category>stat.AP</category>
      <category>physics.geo-ph</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kamel Lahssini, Guerric le Maire, Nicolas Baghdadi, Ibrahim Fayad</dc:creator>
    </item>
    <item>
      <title>Temporal Functional Factor Analysis of Brain Connectivity</title>
      <link>https://arxiv.org/abs/2510.15580</link>
      <description>arXiv:2510.15580v1 Announce Type: new 
Abstract: Many analyses of functional magnetic resonance imaging (fMRI) examine functional connectivity (FC), or the statistical dependencies among distant brain regions. These analyses are typically exploratory, guiding future confirmatory research. In this work, we present an approach based on factor analysis (FA) that is well-suited to studying FC. FA is appealing in this context because its flexible model assumptions permit a guided investigation of its target subspace consistent with the exploratory role of connectivity analyses. However, applying FA to fMRI data poses three problems: (1) its target subspace captures short-range spatial dependencies that should be treated as noise, (2) it requires factorization of a massive spatial covariance, and (3) it overlooks temporal dependencies in the data. To address these limitations, we develop a factor model within the framework of functional data analysis--a field which views certain data as arising from smooth underlying curves. The proposed approach (1) uses matrix completion techniques to filter short-range spatial dependencies out of its target subspace, (2) employs a distributed algorithm for factorizing large-scale covariance matrices, and (3) leverages functional regression to exploit temporal dynamics. Together, these innovations yield a comprehensive and scalable method for studying FC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15580v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyle Stanley, Nicole Lazar, Matthew Reimherr</dc:creator>
    </item>
    <item>
      <title>A nonstationary seasonal Dynamic Factor Model: an application to temperature time series from the state of Minas Gerais</title>
      <link>https://arxiv.org/abs/2510.15667</link>
      <description>arXiv:2510.15667v1 Announce Type: new 
Abstract: In many scientific fields, such as agriculture, temperature time series are of interest both as explanatory variables and as objects of study in their own right. However, at the state level, incorporating information from all possible locations in an analysis can be overwhelming, while using a summary measure, such as the state-wide average temperature, can result in significant information loss. In this context, using Dynamic Factor Models (DFMs) provides a compelling alternative for analyzing such multivariate time series, as they allow for the extraction of a small number of common factors that capture the majority of the variability in the data. Given that temperature series are typically seasonal, this study applies a nonstationary seasonal DFM to analyze a multivariate temperature time series from the state of Minas Gerais. The results show that the data can be effectively represented by two seasonal factors: the first captures the general seasonal pattern of the state, while the second contrasts the months of highest annual temperatures between two distinct regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15667v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Davi Oliveira Chaves, Chang Chiann, Pedro Alberto Morettin</dc:creator>
    </item>
    <item>
      <title>Incorporating estimands into meta-analyses of clinical trials</title>
      <link>https://arxiv.org/abs/2510.15762</link>
      <description>arXiv:2510.15762v1 Announce Type: new 
Abstract: The estimand framework is increasingly established to pose research questions in confirmatory clinical trials. In evidence synthesis, the uptake of estimands has been modest, and the PICO (Population, Intervention, Comparator, Outcome) framework is more often applied. While PICOs and estimands have overlapping elements, the estimand framework explicitly considers different strategies for intercurrent events. We propose a pragmatic framework for the use of estimands in meta-analyses of clinical trials, highlighting the value of estimands to systematically identify and mitigate key sources of quantitative heterogeneity, and to enhance the applicability or external validity of pooled estimates. Focus is placed on the role of strategies for intercurrent events, within the specific context of meta-analyses for health technology assessment. We apply the estimand framework to a network meta-analysis of clinical trials, comparing the efficacy of semaglutide versus dulaglutide in type 2 diabetes. We explore the impact of a treatment policy strategy for treatment discontinuation or initiation of rescue medication versus a hypothetical strategy for the corresponding intercurrent events. The specification of different target estimands at the meta-analytical level allows us to be explicit about the source of heterogeneity, the intercurrent event strategy, driving any potential differences in results. We advocate for the integration of estimands into the planning of meta-analyses, while acknowledging that potential challenges exist in the absence of subject-level data. Estimands can complement PICOs to strengthen communication between stakeholders about what evidence syntheses seek to demonstrate, and to ensure that the generated evidence is maximally relevant to healthcare decision-makers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15762v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonio Remiro-Az\'ocar, Pepa Polavieja, Emmanuelle Boutmy, Alessandro Ghiretti, Lise Lotte Nystrup Husemoen, Khadija Rerhou Rantell, Tatsiana Vaitsiakhovich, David M. Phillippo, Jay J. H. Park, Helle Lynggaard, Robert Bauer, Antonia Morga</dc:creator>
    </item>
    <item>
      <title>Enhanced Renewable Energy Forecasting using Context-Aware Conformal Prediction</title>
      <link>https://arxiv.org/abs/2510.15780</link>
      <description>arXiv:2510.15780v1 Announce Type: new 
Abstract: Accurate forecasting is critical for reliable power grid operations, particularly as the share of renewable generation, such as wind and solar, continues to grow. Given the inherent uncertainty and variability in renewable generation, probabilistic forecasts have become essential for informed operational decisions. However, such forecasts frequently suffer from calibration issues, potentially degrading decision-making performance. Building on recent advances in Conformal Predictions, this paper introduces a tailored calibration framework that constructs context-aware calibration sets using a novel weighting scheme. The proposed framework improves the quality of probabilistic forecasts at the site and fleet levels, as demonstrated by numerical experiments on large-scale datasets covering several systems in the United States. The results demonstrate that the proposed approach achieves higher forecast reliability and robustness for renewable energy applications compared to existing baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15780v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alireza Moradi, Mathieu Tanneau, Reza Zandehshahvar, Pascal Van Hentenryck</dc:creator>
    </item>
    <item>
      <title>Beyond PCA: Manifold Dimension Estimation via Local Graph Structure</title>
      <link>https://arxiv.org/abs/2510.15141</link>
      <description>arXiv:2510.15141v1 Announce Type: cross 
Abstract: Local principal component analysis (Local PCA) has proven to be an effective tool for estimating the intrinsic dimension of a manifold. More recently, curvature-adjusted PCA (CA-PCA) has improved upon this approach by explicitly accounting for the curvature of the underlying manifold, rather than assuming local flatness. Building on these insights, we propose a general framework for manifold dimension estimation that captures the manifold's local graph structure by integrating PCA with regression-based techniques. Within this framework, we introduce two representative estimators: quadratic embedding (QE) and total least squares (TLS). Experiments on both synthetic and real-world datasets demonstrate that these methods perform competitively with, and often outperform, state-of-the-art alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15141v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zelong Bi, Pierre Lafaye de Micheaux</dc:creator>
    </item>
    <item>
      <title>Neural Posterior Estimation for Cataloging Astronomical Images from the Legacy Survey of Space and Time</title>
      <link>https://arxiv.org/abs/2510.15315</link>
      <description>arXiv:2510.15315v1 Announce Type: cross 
Abstract: The Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST) will commence full-scale operations in 2026, yielding an unprecedented volume of astronomical images. Constructing an astronomical catalog, a table of imaged stars, galaxies, and their properties, is a fundamental step in most scientific workflows based on astronomical image data. Traditional deterministic cataloging methods lack statistical coherence as cataloging is an ill-posed problem, while existing probabilistic approaches suffer from computational inefficiency, inaccuracy, or the inability to perform inference with multiband coadded images, the primary output format for LSST images. In this article, we explore a recently developed Bayesian inference method called neural posterior estimation (NPE) as an approach to cataloging. NPE leverages deep learning to achieve both computational efficiency and high accuracy. When evaluated on the DC2 Simulated Sky Survey -- a highly realistic synthetic dataset designed to mimic LSST data -- NPE systematically outperforms the standard LSST pipeline in light source detection, flux measurement, star/galaxy classification, and galaxy shape measurement. Additionally, NPE provides well-calibrated posterior approximations. These promising results, obtained using simulated data, illustrate the potential of NPE in the absence of model misspecification. Although some degree of model misspecification is inevitable in the application of NPE to real LSST images, there are a variety of strategies to mitigate its effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15315v1</guid>
      <category>astro-ph.IM</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yicun Duan, Xinyue Li, Camille Avestruz, Jeffrey Regier</dc:creator>
    </item>
    <item>
      <title>Dynamic Spatial Treatment Effects as Continuous Functionals: Theory and Evidence from Healthcare Access</title>
      <link>https://arxiv.org/abs/2510.15324</link>
      <description>arXiv:2510.15324v1 Announce Type: cross 
Abstract: I develop a continuous functional framework for spatial treatment effects grounded in Navier-Stokes partial differential equations. Rather than discrete treatment parameters, the framework characterizes treatment intensity as continuous functions $\tau(\mathbf{x}, t)$ over space-time, enabling rigorous analysis of boundary evolution, spatial gradients, and cumulative exposure. Empirical validation using 32,520 U.S. ZIP codes demonstrates exponential spatial decay for healthcare access ($\kappa = 0.002837$ per km, $R^2 = 0.0129$) with detectable boundaries at 37.1 km. The framework successfully diagnoses when scope conditions hold: positive decay parameters validate diffusion assumptions near hospitals, while negative parameters correctly signal urban confounding effects. Heterogeneity analysis reveals 2-13 $\times$ stronger distance effects for elderly populations and substantial education gradients. Model selection strongly favors logarithmic decay over exponential ($\Delta \text{AIC} &gt; 10,000$), representing a middle ground between exponential and power-law decay. Applications span environmental economics, banking, and healthcare policy. The continuous functional framework provides predictive capability ($d^*(t) = \xi^* \sqrt{t}$), parameter sensitivity ($\partial d^*/\partial \nu$), and diagnostic tests unavailable in traditional difference-in-differences approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15324v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Adaptive Influence Diagnostics in High-Dimensional Regression</title>
      <link>https://arxiv.org/abs/2510.15618</link>
      <description>arXiv:2510.15618v1 Announce Type: cross 
Abstract: An adaptive Cook's distance (ACD) for diagnosing influential observations in high-dimensional single-index models with multicollinearity and outlier contamination is proposed. ACD is a model-free technique built on sparse local linear gradients to temper leverage effects. In simulations spanning low- and high-dimensional design settings with strong correlation, ACD based on LASSO (ACD-LASSO) and SCAD (ACD-SCAD) penalties reduced masking and swamping relative to classical Cook's distance and local influence as well as the DF-Model and Case-Weight adjusted solution for LASSO. Trimming points flagged by ACD stabilizes variable selection while preserving core signals. Applications to two datasets--the 1960 US cities pollution study and a high-dimensional riboflavin genomics experiment show consistent gains in selection stability and interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15618v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdul-Nasah Soale, Adewale Lukman</dc:creator>
    </item>
    <item>
      <title>Robust Estimation of Polyserial Correlation</title>
      <link>https://arxiv.org/abs/2510.15632</link>
      <description>arXiv:2510.15632v1 Announce Type: cross 
Abstract: The association between a continuous and an ordinal variable is commonly modeled through the polyserial correlation model. However, this model, which is based on a partially-latent normality assumption, may be misspecified in practice, due to, for example (but not limited to), outliers or careless responses. We demonstrate that the typically used maximum likelihood (ML) estimator is highly susceptible to such misspecification: One single observation not generated by partially-latent normality can suffice to produce arbitrarily poor estimates. As a remedy, we propose a novel estimator of the polyserial correlation model designed to be robust against the adverse effects of observations discrepant to that model. The estimator achieves robustness by implicitly downweighting such observations; the ensuing weights constitute a useful tool for pinpointing potential sources of model misspecification. We show that the proposed estimator generalizes ML and is consistent as well as asymptotically Gaussian. As price for robustness, some efficiency must be sacrificed, but substantial robustness can be gained while maintaining more than 98% of ML efficiency. We demonstrate our estimator's robustness and practical usefulness in simulation experiments and an empirical application in personality psychology where our estimator helps identify outliers. Finally, the proposed methodology is implemented in free open-source software.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15632v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Welz</dc:creator>
    </item>
    <item>
      <title>Tides Need STEMMED: A Locally Operating Spatio-Temporal Mutually Exciting Point Process with Dynamic Network for Improving Opioid Overdose Death Prediction</title>
      <link>https://arxiv.org/abs/2211.07570</link>
      <description>arXiv:2211.07570v2 Announce Type: replace 
Abstract: We develop a Spatio-TEMporal Mutually Exciting point process with Dynamic network (STEMMED), i.e., a point process network wherein each node models a unique community-drug event stream with a dynamic mutually-exciting structure, accounting for influences from other nodes. We show that STEMMED can be decomposed node-by-node, suggesting a tractable distributed learning procedure. Simulation shows that this learning algorithm can accurately recover known parameters of STEMMED, especially for small networks and long data-horizons. Next, we turn this node-by-node decomposition into an online cooperative multi-period forecasting framework, which is asymptotically robust to operational errors, to facilitate Opioid-related overdose death (OOD) trends forecasting among neighboring communities. In our numerical study, we parameterize STEMMED using individual-level OOD data and county-level demographics in Massachusetts. For any node, we observe that OODs within the same drug class from nearby locations have the greatest influence on future OOD trends. Furthermore, the expected proportion of OODs triggered by historical events varies greatly across counties, ranging between 30%-70%. Finally, in a practical online forecasting setting, STEMMED-based cooperative framework reduces prediction error by 60% on average, compared to well-established forecasting models. Leveraging the growing abundance of public health surveillance data, STEMMED can provide accurate forecasts of local OOD trends and highlight complex interactions between OODs across communities and drug types. Moreover, STEMMED enhances synergies between local and federal government entities, which is critical to designing impactful policy interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.07570v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Che-Yi Liao, Gian-Gabriel P. Garcia, Kamran Paynabar, Zheng Dong, Yao Xie, Mohammad S. Jalali</dc:creator>
    </item>
    <item>
      <title>Bayesian Signal Matching for Transfer Learning in ERP-Based Brain Computer Interface</title>
      <link>https://arxiv.org/abs/2401.07111</link>
      <description>arXiv:2401.07111v3 Announce Type: replace 
Abstract: An Event-Related Potential (ERP)-based Brain-Computer Interface (BCI) Speller System assists people with disabilities to communicate by decoding electroencephalogram (EEG) signals. A P300-ERP embedded in EEG signals arises in response to a rare, but relevant event (target) among a series of irrelevant events (non-target). Different machine learning methods have constructed binary classifiers to detect target events, known as calibration. The existing calibration strategy uses data from participants themselves with lengthy training time. Participants feel bored and distracted, which causes biased P300 estimation and decreased prediction accuracy. To resolve this issue, we propose a Bayesian signal matching (BSM) framework to calibrate EEG signals from a new participant using data from source participants. BSM specifies the joint distribution of stimulus-specific EEG signals among source participants via a Bayesian hierarchical mixture model. We apply the inference strategy. If source and new participants are similar, they share the same set of model parameters; otherwise, they keep their own sets of model parameters; we predict on the testing data using parameters of the baseline cluster directly. Our hierarchical framework can be generalized to other base classifiers with parametric forms. We demonstrate the advantages of BSM using simulations and focus on the real data analysis among participants with neuro-degenerative diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07111v3</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianwen Ma, Jane E. Huggins, Jian Kang</dc:creator>
    </item>
    <item>
      <title>Low-rank bilinear autoregressive models for three-way criminal activity tensors</title>
      <link>https://arxiv.org/abs/2505.01166</link>
      <description>arXiv:2505.01166v2 Announce Type: replace 
Abstract: Criminal activity data are typically available via a three-way tensor encoding the reported frequencies of different crime categories across time and space. The challenges that arise in the design of interpretable, yet realistic, model-based representations of the complex dependencies within and across these three dimensions have led to an increasing adoption of black-box predictive strategies. Although this perspective has proved successful in producing accurate forecasts guiding targeted interventions, the lack of interpretable model-based characterizations of the dependence structures underlying criminal activity tensors prevents from inferring the cascading effects of these interventions across the different dimensions. We address this gap through the design of a low-rank bilinear autoregressive model which achieves comparable predictive performance to black-box strategies, while allowing interpretable inference on the dependence structures of criminal activity reports across crime categories, time, and space. This representation incorporates the time dimension via an autoregressive construction, accounting for spatial effects and dependencies among crime categories through a separable low-rank bilinear formulation. When applied to Chicago police reports, the proposed model showcases remarkable predictive performance and also reveals interpretable dependence structures unveiling fundamental crime dynamics. These results facilitate the design of more refined intervention policies informed by cascading effects of the policy itself.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01166v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregor Zens, Carlos D\'iaz, Daniele Durante, Eleonora Patacchini</dc:creator>
    </item>
    <item>
      <title>Rethinking Nonstationarity in Time Series: A Deterministic Trend Perspective</title>
      <link>https://arxiv.org/abs/2506.07987</link>
      <description>arXiv:2506.07987v2 Announce Type: replace 
Abstract: This paper challenges the dominance of stochastic trend models by introducing the Seasonal-Trend-Stationary ARMA (STSA) framework, which models univariate nonstationary time series as stationary fluctuations around deterministic trend and seasonal components, incorporating a finite number of structural breaks. We propose methods for estimating the locations and numbers of breaks using a modified dynamic programming algorithm and a sequential prediction-interval procedure, and we outline strategies for specifying and estimating the full model. Empirical analysis of U.S. Retail Sales (2004-2025) shows that the STSA model accurately identifies structural breaks corresponding to major economic events, including the Global Financial Crisis and the COVID-19 downturn. The decomposition into trend, seasonal, and ARMA components provides a realistic and interpretable representation of the underlying dynamics of the economic cycle, offering insights into the pace of growth within each regime, recurring seasonal patterns, and the persistence of short-term fluctuations. Although designed primarily for interpretability, STSA substantially outperforms Prophet and achieves forecasting accuracy comparable to state-of-the-art stochastic trend models (ARIMA, ETS, TBATS, Theta) on the M4 Competition monthly dataset, with particular advantages for series exhibiting abrupt structural changes, where stochastic models typically struggle to adapt.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07987v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhandos Abdikhadir, and Terence Tai Leung Chong</dc:creator>
    </item>
    <item>
      <title>Shotgun DNA sequencing evidence: sample-specific and unknown genotyping error probabilities</title>
      <link>https://arxiv.org/abs/2509.26112</link>
      <description>arXiv:2509.26112v2 Announce Type: replace 
Abstract: Many forensic genetic trace samples are of too low quality to obtain short tandem repeat (STR) DNA profiles as the nuclear DNA they contain is highly degraded (e.g., telogen hairs). Instead, performing shotgun DNA sequencing of such samples can provide valuable information on, e.g., single nucleotide polymorphism (SNP) markers. As a result, shotgun sequencing is starting to gain more attention in forensic genetics and statistical models to correctly interpret such evidence, including properly accounting for sequencing errors, are needed. One such model is the wgsLR model by Andersen et. al. (2025) that enabled evaluating the evidential strength of a comparison between the genotypes in the trace sample and reference sample assuming a single-source contribution to both samples. This paper extends the wgsLR model to allow for different (asymmetric) genotyping error probabilities (e.g., from a low quality trace sample and a high quality reference sample). The model was also extended to be able to handle unknown genotyping error probabilities via a prior distribution. The sensitivity of the wgsLR model against overdispersion was also investigated and it was found that it is robust against it. It was also found that integrating out unknown genotyping error probability of the trace sample gave concordant weight of evidence (WoE) under both the hypotheses (that the same individual was the donor of both trace and reference sample and that two different individuals were the donors of the trace and reference sample). It was found that it is more conservative to use a too small trace sample genotyping error probability rather than a too high genotyping error probability as the latter can explain genotype inconsistencies by errors rather than due to two different individuals being the donors of the trace sample and reference sample. The extensions of the model are implemented in the R package wgsLR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26112v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikkel Meyer Andersen</dc:creator>
    </item>
    <item>
      <title>Cross-Population Amplitude Coupling in High-Dimensional Oscillatory Neural Time Series</title>
      <link>https://arxiv.org/abs/2105.03508</link>
      <description>arXiv:2105.03508v3 Announce Type: replace-cross 
Abstract: Neural oscillations have long been considered important markers of interaction across brain regions, yet identifying coordinated oscillatory activity from high-dimensional multiple-electrode recordings remains challenging. We sought to quantify time-varying covariation of oscillatory amplitudes across two brain regions, during a memory task, based on local field potentials recorded from 96 electrodes in each region. We extended Canonical Correlation Analysis (CCA) to multiple time series through the cross-correlation of latent time series. This, however, introduces a large number of possible lead-lag cross-correlations across the two regions. To manage that high dimensionality we developed rigorous statistical procedures aimed at finding a small number of dominant lead-lag effects. The method correctly identified ground truth structure in realistic simulation-based settings. When we used it to analyze local field potentials recorded from prefrontal cortex and visual area V4 we obtained highly plausible results. The new statistical methodology could also be applied to other slowly-varying high-dimensional time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.03508v3</guid>
      <category>stat.ME</category>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heejong Bong, Val\'erie Ventura, Eric A. Yttri, Matthew A. Smith, Robert E. Kass</dc:creator>
    </item>
    <item>
      <title>Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in LLMs</title>
      <link>https://arxiv.org/abs/2506.13593</link>
      <description>arXiv:2506.13593v4 Announce Type: replace-cross 
Abstract: We introduce time-to-unsafe-sampling, a novel safety measure for generative models, defined as the number of generations required by a large language model (LLM) to trigger an unsafe (e.g., toxic) response. While providing a new dimension for prompt-adaptive safety evaluation, quantifying time-to-unsafe-sampling is challenging: unsafe outputs are often rare in well-aligned models and thus may not be observed under any feasible sampling budget. To address this challenge, we frame this estimation problem as one of survival analysis. We build on recent developments in conformal prediction and propose a novel calibration technique to construct a lower predictive bound (LPB) on the time-to-unsafe-sampling of a given prompt with rigorous coverage guarantees. Our key technical innovation is an optimized sampling-budget allocation scheme that improves sample efficiency while maintaining distribution-free guarantees. Experiments on both synthetic and real data support our theoretical results and demonstrate the practical utility of our method for safety risk assessment in generative AI models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13593v4</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hen Davidov, Shai Feldman, Gilad Freidkin, Yaniv Romano</dc:creator>
    </item>
    <item>
      <title>Optimal and Efficient Sample Size Re-estimation: A Dynamic Cost Framework</title>
      <link>https://arxiv.org/abs/2509.16636</link>
      <description>arXiv:2509.16636v2 Announce Type: replace-cross 
Abstract: Adaptive sample size re-estimation (SSR) is a well-established strategy for improving the efficiency and flexibility of clinical trials. Its central challenge is determining whether, and by how much, to increase the sample size at an interim analysis. This decision requires a rational framework for balancing the potential gain in statistical power against the risk and cost of further investment. Prevailing optimization approaches, such as the Jennison and Turnbull (JT) method, address this by maximizing power for a fixed cost per additional participant. While statistically efficient, this paradigm assumes the cost of enrolling another patient is constant, regardless of whether the interim evidence is promising or weak. This can lead to impractical recommendations and inefficient resource allocation, particularly in weak-signal scenarios.
  We reframe SSR as a decision problem under dynamic costs, where the effective cost of additional enrollment reflects the interim strength of evidence. Within this framework, we derive two novel rules: (i) a likelihood-ratio based rule, shown to be Pareto optimal in achieving smaller average sample size under the null without loss of power under the alternative; and (ii) a return-on-investment (ROI) rule that directly incorporates economic considerations by linking SSR decisions to expected net benefit. To unify existing methods, we further establish a representation theorem demonstrating that a broad class of SSR rules can be expressed through implicit dynamic cost functions, providing a common analytical foundation for their comparison. Simulation studies calibrated to Phase III trial settings confirm that dynamic-cost approaches improve resource allocation relative to fixed-cost methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16636v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 20 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Jin, Cai Wu, Qiqi Deng</dc:creator>
    </item>
  </channel>
</rss>
