<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.IV updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.IV</link>
    <description>eess.IV updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.IV" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 Oct 2025 01:49:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Deep learning approach for flow visualization in background-oriented schlieren</title>
      <link>https://arxiv.org/abs/2509.25201</link>
      <description>arXiv:2509.25201v1 Announce Type: new 
Abstract: Diffractive optical element based background oriented schlieren (BOS) is a popular technique for quantitative flow visualization. This technique relies on encoding spatial density variations of the test medium in the form of an optical fringe pattern; and hence, its accuracy is directly influenced by the quality of fringe pattern demodulation. We introduce a robust deep learning assisted subspace method which enables reliable fringe pattern demodulation even in the presence of severe noise and uneven fringe distortions in recorded BOS fringe patterns. The method's effectiveness to handle fringe pattern artifacts is demonstrated via rigorous numerical simulations. Furthermore, the method's practical applicability is experimentally validated using real-world BOS images obtained from a liquid diffusion process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25201v1</guid>
      <category>eess.IV</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1364/AO.572042</arxiv:DOI>
      <arxiv:journal_reference>Applied Optics, 64, 7938-7947 (2025)</arxiv:journal_reference>
      <dc:creator>Viren S. Ram, Tullio de Rubeis, Dario Ambrosini, Rajshekhar Gannavarpu</dc:creator>
    </item>
    <item>
      <title>Evaluating the Impact of Radiographic Noise on Chest X-ray Semantic Segmentation and Disease Classification Using a Scalable Noise Injection Framework</title>
      <link>https://arxiv.org/abs/2509.25265</link>
      <description>arXiv:2509.25265v1 Announce Type: new 
Abstract: Deep learning models are increasingly used for radiographic analysis, but their reliability is challenged by the stochastic noise inherent in clinical imaging. A systematic, cross-task understanding of how different noise types impact these models is lacking. Here, we evaluate the robustness of state-of-the-art convolutional neural networks (CNNs) to simulated quantum (Poisson) and electronic (Gaussian) noise in two key chest X-ray tasks: semantic segmentation and pulmonary disease classification. Using a novel, scalable noise injection framework, we applied controlled, clinically-motivated noise severities to common architectures (UNet, DeepLabV3, FPN; ResNet, DenseNet, EfficientNet) on public datasets (Landmark, ChestX-ray14). Our results reveal a stark dichotomy in task robustness. Semantic segmentation models proved highly vulnerable, with lung segmentation performance collapsing under severe electronic noise (Dice Similarity Coefficient drop of 0.843), signifying a near-total model failure. In contrast, classification tasks demonstrated greater overall resilience, but this robustness was not uniform. We discovered a differential vulnerability: certain tasks, such as distinguishing Pneumothorax from Atelectasis, failed catastrophically under quantum noise (AUROC drop of 0.355), while others were more susceptible to electronic noise. These findings demonstrate that while classification models possess a degree of inherent robustness, pixel-level segmentation tasks are far more brittle. The task- and noise-specific nature of model failure underscores the critical need for targeted validation and mitigation strategies before the safe clinical deployment of diagnostic AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25265v1</guid>
      <category>eess.IV</category>
      <category>cs.LG</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Derek Jiu, Kiran Nijjer, Nishant Chinta, Ryan Bui, Ben Liu, Kevin Zhu</dc:creator>
    </item>
    <item>
      <title>Position-Blind Ptychography: Viability of image reconstruction via data-driven variational inference</title>
      <link>https://arxiv.org/abs/2509.25269</link>
      <description>arXiv:2509.25269v1 Announce Type: new 
Abstract: In this work, we present and investigate the novel blind inverse problem of position-blind ptychography, i.e., ptychographic phase retrieval without any knowledge of scan positions, which then must be recovered jointly with the image. The motivation for this problem comes from single-particle diffractive X-ray imaging, where particles in random orientations are illuminated and a set of diffraction patterns is collected. If one uses a highly focused X-ray beam, the measurements would also become sensitive to the beam positions relative to each particle and therefore ptychographic, but these positions are also unknown. We investigate the viability of image reconstruction in a simulated, simplified 2-D variant of this difficult problem, using variational inference with modern data-driven image priors in the form of score-based diffusion models. We find that, with the right illumination structure and a strong prior, one can achieve reliable and successful image reconstructions even under measurement noise, in all except the most difficult evaluated imaging scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25269v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.optics</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Welker, Lorenz Kuger, Tim Roith, Berthy Feng, Martin Burger, Timo Gerkmann, Henry Chapman</dc:creator>
    </item>
    <item>
      <title>Anatomy-DT: A Cross-Diffusion Digital Twin for Anatomical Evolution</title>
      <link>https://arxiv.org/abs/2509.25280</link>
      <description>arXiv:2509.25280v1 Announce Type: new 
Abstract: Accurately modeling the spatiotemporal evolution of tumor morphology from baseline imaging is a pre-requisite for developing digital twin frameworks that can simulate disease progression and treatment response. Most existing approaches primarily characterize tumor growth while neglecting the concomitant alterations in adjacent anatomical structures. In reality, tumor evolution is highly non-linear and heterogeneous, shaped not only by therapeutic interventions but also by its spatial context and interaction with neighboring tissues. Therefore, it is critical to model tumor progression in conjunction with surrounding anatomy to obtain a comprehensive and clinically relevant understanding of disease dynamics. We introduce a mathematically grounded framework that unites mechanistic partial differential equations with differentiable deep learning. Anatomy is represented as a multi-class probability field on the simplex and evolved by a cross-diffusion reaction-diffusion system that enforces inter-class competition and exclusivity. A differentiable implicit-explicit scheme treats stiff diffusion implicitly while handling nonlinear reaction and event terms explicitly, followed by projection back to the simplex. To further enhance global plausibility, we introduce a topology regularizer that simultaneously enforces centerline preservation and penalizes region overlaps. The approach is validated on synthetic datasets and a clinical dataset. On synthetic benchmarks, our method achieves state-of-the-art accuracy while preserving topology, and also demonstrates superior performance on the clinical dataset. By integrating PDE dynamics, topology-aware regularization, and differentiable solvers, this work establishes a principled path toward anatomy-to-anatomy generation for digital twins that are visually realistic, anatomically exclusive, and topologically consistent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25280v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moinak Bhattacharya, Gagandeep Singh, Prateek Prasanna</dc:creator>
    </item>
    <item>
      <title>Neural Fields for Highly Accelerated 2D Cine Phase Contrast MRI</title>
      <link>https://arxiv.org/abs/2509.25388</link>
      <description>arXiv:2509.25388v1 Announce Type: new 
Abstract: 2D cine phase contrast (CPC) MRI provides quantitative information on blood velocity and flow within the human vasculature. However, data acquisition is time-consuming, motivating the reconstruction of the velocity field from undersampled measurements to reduce scan times. In this work, we propose using neural fields to parametrize the complex-valued images, leveraging their inductive bias for the reconstruction of the velocity data. Additionally, to mitigate the inherent over-smoothing of neural fields, we introduce a simple voxel-based postprocessing step. We validate our method numerically in Cartesian and radial k-space with both high and low temporal resolution data. Our approach achieves accurate reconstructions at high acceleration factors, with low errors even at 16x and 32x undersampling, and consistently outperforms classical locally low-rank regularized voxel-based methods in both flow estimates and anatomical depiction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25388v1</guid>
      <category>eess.IV</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo Arratia, Martin J. Graves, Mary McLean, Carolin Pirkl, Carola-Bibiane Sch\"onlieb, Timo Schirmer, Florian Wiesinger, Matthias J. Ehrhardt</dc:creator>
    </item>
    <item>
      <title>Enhanced Template-based Intra Mode Derivation with Adaptive Block Vector Replacement</title>
      <link>https://arxiv.org/abs/2509.25668</link>
      <description>arXiv:2509.25668v1 Announce Type: new 
Abstract: Intra prediction is a crucial component in traditional video coding frameworks, aiming to eliminate spatial redundancy within frames. In recent years, an increasing number of decoder-side adaptive mode derivation methods have been adopted into Enhanced Compression Model (ECM). However, these methods predominantly rely on adjacent spatial information for intra mode decision-making, overlooking potential similarity patterns in non-adjacent spatial regions, thereby limiting intra prediction efficiency. To address this limitation, this paper proposes a template-based intra mode derivation approach enhanced by block vector-based prediction. The adaptive block vector replacement strategy effectively expands the reference scope of the existing template-based intra mode derivation mode to non-adjacent spatial information, thereby enhancing prediction efficiency. Extensive experiments demonstrate that our strategy achieves 0.082% Bj{\o}ntegaard delta rate (BD-rate) savings for Y components under the All Intra (AI) configuration compared to ECM-16.1 while maintaining identical encoding/decoding complexity, and delivers an additional 0.25% BD-rate savings for Y components on screen content sequences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25668v1</guid>
      <category>eess.IV</category>
      <category>cs.MM</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jiaqi Zhang, Jiaye Fu, Chuanmin Jia, Siwei Ma, Karam Naser, Thierry Dumas, Saurabh Puri, Milos Radosavljevic</dc:creator>
    </item>
    <item>
      <title>Multi-modal Liver Segmentation and Fibrosis Staging Using Real-world MRI Images</title>
      <link>https://arxiv.org/abs/2509.26061</link>
      <description>arXiv:2509.26061v1 Announce Type: new 
Abstract: Liver fibrosis represents the accumulation of excessive extracellular matrix caused by sustained hepatic injury. It disrupts normal lobular architecture and function, increasing the chances of cirrhosis and liver failure. Precise staging of fibrosis for early diagnosis and intervention is often invasive, which carries risks and complications. To address this challenge, recent advances in artificial intelligence-based liver segmentation and fibrosis staging offer a non-invasive alternative. As a result, the CARE 2025 Challenge aimed for automated methods to quantify and analyse liver fibrosis in real-world scenarios, using multi-centre, multi-modal, and multi-phase MRI data. This challenge included tasks of precise liver segmentation (LiSeg) and fibrosis staging (LiFS). In this study, we developed an automated pipeline for both tasks across all the provided MRI modalities. This pipeline integrates pseudo-labelling based on multi-modal co-registration, liver segmentation using deep neural networks, and liver fibrosis staging based on shape, textural, appearance, and directional (STAD) features derived from segmentation masks and MRI images. By solely using the released data with limited annotations, our proposed pipeline demonstrated excellent generalisability for all MRI modalities, achieving top-tier performance across all competition subtasks. This approach provides a rapid and reproducible framework for quantitative MRI-based liver fibrosis assessment, supporting early diagnosis and clinical decision-making. Code is available at https://github.com/YangForever/care2025_liver_biodreamer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26061v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Zhou, Kunhao Yuan, Ye Wei, Jishizhan Chen</dc:creator>
    </item>
    <item>
      <title>Ordinal Label-Distribution Learning with Constrained Asymmetric Priors for Imbalanced Retinal Grading</title>
      <link>https://arxiv.org/abs/2509.26146</link>
      <description>arXiv:2509.26146v1 Announce Type: new 
Abstract: Diabetic retinopathy grading is inherently ordinal and long-tailed, with minority stages being scarce, heterogeneous, and clinically critical to detect accurately. Conventional methods often rely on isotropic Gaussian priors and symmetric loss functions, misaligning latent representations with the task's asymmetric nature. We propose the Constrained Asymmetric Prior Wasserstein Autoencoder (CAP-WAE), a novel framework that addresses these challenges through three key innovations. Our approach employs a Wasserstein Autoencoder (WAE) that aligns its aggregate posterior with a asymmetric prior, preserving the heavy-tailed and skewed structure of minority classes. The latent space is further structured by a Margin-Aware Orthogonality and Compactness (MAOC) loss to ensure grade-ordered separability. At the supervision level, we introduce a direction-aware ordinal loss, where a lightweight head predicts asymmetric dispersions to generate soft labels that reflect clinical priorities by penalizing under-grading more severely. Stabilized by an adaptive multi-task weighting scheme, our end-to-end model requires minimal tuning. Across public DR benchmarks, CAP-WAE consistently achieves state-of-the-art Quadratic Weighted Kappa, accuracy, and macro-F1, surpassing both ordinal classification and latent generative baselines. t-SNE visualizations further reveal that our method reshapes the latent manifold into compact, grade-ordered clusters with reduced overlap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26146v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nagur Shareef Shaik, Teja Krishna Cherukuri, Adnan Masood, Ehsan Adeli, Dong Hye Ye</dc:creator>
    </item>
    <item>
      <title>GastroViT: A Vision Transformer Based Ensemble Learning Approach for Gastrointestinal Disease Classification with Grad CAM &amp; SHAP Visualization</title>
      <link>https://arxiv.org/abs/2509.26502</link>
      <description>arXiv:2509.26502v1 Announce Type: new 
Abstract: The gastrointestinal (GI) tract of humans can have a wide variety of aberrant mucosal abnormality findings, ranging from mild irritations to extremely fatal illnesses. Prompt identification of gastrointestinal disorders greatly contributes to arresting the progression of the illness and improving therapeutic outcomes. This paper presents an ensemble of pre-trained vision transformers (ViTs) for accurately classifying endoscopic images of the GI tract to categorize gastrointestinal problems and illnesses. ViTs, attention-based neural networks, have revolutionized image recognition by leveraging the transformative power of the transformer architecture, achieving state-of-the-art (SOTA) performance across various visual tasks. The proposed model was evaluated on the publicly available HyperKvasir dataset with 10,662 images of 23 different GI diseases for the purpose of identifying GI tract diseases. An ensemble method is proposed utilizing the predictions of two pre-trained models, MobileViT_XS and MobileViT_V2_200, which achieved accuracies of 90.57% and 90.48%, respectively. All the individual models are outperformed by the ensemble model, GastroViT, with an average precision, recall, F1 score, and accuracy of 69%, 63%, 64%, and 91.98%, respectively, in the first testing that involves 23 classes. The model comprises only 20 million (M) parameters, even without data augmentation and despite the highly imbalanced dataset. For the second testing with 16 classes, the scores are even higher, with average precision, recall, F1 score, and accuracy of 87%, 86%, 87%, and 92.70%, respectively. Additionally, the incorporation of explainable AI (XAI) methods such as Grad-CAM (Gradient Weighted Class Activation Mapping) and SHAP (Shapley Additive Explanations) enhances model interpretability, providing valuable insights for reliable GI diagnosis in real-world settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26502v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sumaiya Tabassum, Md. Faysal Ahamed, Hafsa Binte Kibria, Md. Nahiduzzaman, Julfikar Haider, Muhammad E. H. Chowdhury, Mohammad Tariqul Islam</dc:creator>
    </item>
    <item>
      <title>VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes</title>
      <link>https://arxiv.org/abs/2509.25339</link>
      <description>arXiv:2509.25339v2 Announce Type: cross 
Abstract: Is basic visual understanding really solved in state-of-the-art VLMs? We present VisualOverload, a slightly different visual question answering (VQA) benchmark comprising 2,720 question-answer pairs, with privately held ground-truth responses. Unlike prior VQA datasets that typically focus on near global image understanding, VisualOverload challenges models to perform simple, knowledge-free vision tasks in densely populated (or, overloaded) scenes. Our dataset consists of high-resolution scans of public-domain paintings that are populated with multiple figures, actions, and unfolding subplots set against elaborately detailed backdrops. We manually annotated these images with questions across six task categories to probe for a thorough understanding of the scene. We hypothesize that current benchmarks overestimate the performance of VLMs, and encoding and reasoning over details is still a challenging task for them, especially if they are confronted with densely populated scenes. Indeed, we observe that even the best model (o3) out of 37 tested models only achieves 19.6% accuracy on our hardest test split and overall 69.5% accuracy on all questions. Beyond a thorough evaluation, we complement our benchmark with an error analysis that reveals multiple failure modes, including a lack of counting skills, failure in OCR, and striking logical inconsistencies under complex tasks. Altogether, VisualOverload exposes a critical gap in current vision models and offers a crucial resource for the community to develop better models.
  Benchmark: http://paulgavrikov.github.io/visualoverload</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25339v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Gavrikov, Wei Lin, M. Jehanzeb Mirza, Soumya Jahagirdar, Muhammad Huzaifa, Sivan Doveh, Serena Yeung-Levy, James Glass, Hilde Kuehne</dc:creator>
    </item>
    <item>
      <title>World Model for AI Autonomous Navigation in Mechanical Thrombectomy</title>
      <link>https://arxiv.org/abs/2509.25518</link>
      <description>arXiv:2509.25518v1 Announce Type: cross 
Abstract: Autonomous navigation for mechanical thrombectomy (MT) remains a critical challenge due to the complexity of vascular anatomy and the need for precise, real-time decision-making. Reinforcement learning (RL)-based approaches have demonstrated potential in automating endovascular navigation, but current methods often struggle with generalization across multiple patient vasculatures and long-horizon tasks. We propose a world model for autonomous endovascular navigation using TD-MPC2, a model-based RL algorithm. We trained a single RL agent across multiple endovascular navigation tasks in ten real patient vasculatures, comparing performance against the state-of-the-art Soft Actor-Critic (SAC) method. Results indicate that TD-MPC2 significantly outperforms SAC in multi-task learning, achieving a 65% mean success rate compared to SAC's 37%, with notable improvements in path ratio. TD-MPC2 exhibited increased procedure times, suggesting a trade-off between success rate and execution speed. These findings highlight the potential of world models for improving autonomous endovascular navigation and lay the foundation for future research in generalizable AI-driven robotic interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25518v1</guid>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <category>eess.IV</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-032-05114-1_65</arxiv:DOI>
      <arxiv:journal_reference>MICCAI 2025. Lecture Notes in Computer Science, vol 15968 (2026)</arxiv:journal_reference>
      <dc:creator>Harry Robertshaw, Han-Ru Wu, Alejandro Granados, Thomas C Booth</dc:creator>
    </item>
    <item>
      <title>AttentionViG: Cross-Attention-Based Dynamic Neighbor Aggregation in Vision GNNs</title>
      <link>https://arxiv.org/abs/2509.25570</link>
      <description>arXiv:2509.25570v1 Announce Type: cross 
Abstract: Vision Graph Neural Networks (ViGs) have demonstrated promising performance in image recognition tasks against Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). An essential part of the ViG framework is the node-neighbor feature aggregation method. Although various graph convolution methods, such as Max-Relative, EdgeConv, GIN, and GraphSAGE, have been explored, a versatile aggregation method that effectively captures complex node-neighbor relationships without requiring architecture-specific refinements is needed. To address this gap, we propose a cross-attention-based aggregation method in which the query projections come from the node, while the key projections come from its neighbors. Additionally, we introduce a novel architecture called AttentionViG that uses the proposed cross-attention aggregation scheme to conduct non-local message passing. We evaluated the image recognition performance of AttentionViG on the ImageNet-1K benchmark, where it achieved SOTA performance. Additionally, we assessed its transferability to downstream tasks, including object detection and instance segmentation on MS COCO 2017, as well as semantic segmentation on ADE20K. Our results demonstrate that the proposed method not only achieves strong performance, but also maintains efficiency, delivering competitive accuracy with comparable FLOPs to prior vision GNN architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25570v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>eess.IV</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hakan Emre Gedik, Andrew Martin, Mustafa Munir, Oguzhan Baser, Radu Marculescu, Sandeep P. Chinchali, Alan C. Bovik</dc:creator>
    </item>
    <item>
      <title>YOLO-Based Defect Detection for Metal Sheets</title>
      <link>https://arxiv.org/abs/2509.25659</link>
      <description>arXiv:2509.25659v1 Announce Type: cross 
Abstract: In this paper, we propose a YOLO-based deep learning (DL) model for automatic defect detection to solve the time-consuming and labor-intensive tasks in industrial manufacturing. In our experiments, the images of metal sheets are used as the dataset for training the YOLO model to detect the defects on the surfaces and in the holes of metal sheets. However, the lack of metal sheet images significantly degrades the performance of detection accuracy. To address this issue, the ConSinGAN is used to generate a considerable amount of data. Four versions of the YOLO model (i.e., YOLOv3, v4, v7, and v9) are combined with the ConSinGAN for data augmentation. The proposed YOLOv9 model with ConSinGAN outperforms the other YOLO models with an accuracy of 91.3%, and a detection time of 146 ms. The proposed YOLOv9 model is integrated into manufacturing hardware and a supervisory control and data acquisition (SCADA) system to establish a practical automated optical inspection (AOI) system. Additionally, the proposed automated defect detection is easily applied to other components in industrial manufacturing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25659v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/IST63414.2024.10759237</arxiv:DOI>
      <arxiv:journal_reference>Proc. 2024 IEEE Int. Conf. Imaging Systems and Techniques (IST), Tokyo, Japan, Oct. 2024</arxiv:journal_reference>
      <dc:creator>Po-Heng Chou, Chun-Chi Wang, Wei-Lung Mao</dc:creator>
    </item>
    <item>
      <title>Field Calibration of Hyperspectral Cameras for Terrain Inference</title>
      <link>https://arxiv.org/abs/2509.25663</link>
      <description>arXiv:2509.25663v1 Announce Type: cross 
Abstract: Intra-class terrain differences such as water content directly influence a vehicle's ability to traverse terrain, yet RGB vision systems may fail to distinguish these properties. Evaluating a terrain's spectral content beyond red-green-blue wavelengths to the near infrared spectrum provides useful information for intra-class identification. However, accurate analysis of this spectral information is highly dependent on ambient illumination. We demonstrate a system architecture to collect and register multi-wavelength, hyperspectral images from a mobile robot and describe an approach to reflectance calibrate cameras under varying illumination conditions. To showcase the practical applications of our system, HYPER DRIVE, we demonstrate the ability to calculate vegetative health indices and soil moisture content from a mobile robot platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25663v1</guid>
      <category>cs.RO</category>
      <category>eess.IV</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nathaniel Hanson, Benjamin Pyatski, Samuel Hibbard, Gary Lvov, Oscar De La Garza, Charles DiMarzio, Kristen L. Dorsey, Ta\c{s}k{\i}n Pad{\i}r</dc:creator>
    </item>
    <item>
      <title>Brain Tumor Classification on MRI in Light of Molecular Markers</title>
      <link>https://arxiv.org/abs/2409.19583</link>
      <description>arXiv:2409.19583v4 Announce Type: replace 
Abstract: In research findings, co-deletion of the 1p/19q gene is associated with clinical outcomes in low-grade gliomas. The ability to predict 1p19q status is critical for treatment planning and patient follow-up. This study aims to utilize a specially MRI-based convolutional neural network for brain cancer detection. Although public networks such as RestNet and AlexNet can effectively diagnose brain cancers using transfer learning, the model includes quite a few weights that have nothing to do with medical images. As a result, the diagnostic results are unreliable by the transfer learning model. To deal with the problem of trustworthiness, we create the model from the ground up, rather than depending on a pre-trained model. To enable flexibility, we combined convolution stacking with a dropout and full connect operation, it improved performance by reducing overfitting. During model training, we also supplement the given dataset and inject Gaussian noise. We use three--fold cross-validation to train the best selection model. Comparing InceptionV3, VGG16, and MobileNetV2 fine-tuned with pre-trained models, our model produces better results. On an validation set of 125 codeletion vs. 31 not codeletion images, the proposed network achieves 96.37\% percent F1-score, 97.46\% percent precision, and 96.34\% percent recall when classifying 1p/19q codeletion and not codeletion images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19583v4</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Springer Nature - Book Series: Transactions on Computational Science &amp; Computational Intelligence, 2022</arxiv:journal_reference>
      <dc:creator>Jun Liu, Geng Yuan, Weihao Zeng, Hao Tang, Wenbin Zhang, Xue Lin, XiaoLin Xu, Dong Huang, Yanzhi Wang</dc:creator>
    </item>
    <item>
      <title>tCURLoRA: Tensor CUR Decomposition Based Low-Rank Parameter Adaptation and Its Application in Medical Image Segmentation</title>
      <link>https://arxiv.org/abs/2501.02227</link>
      <description>arXiv:2501.02227v3 Announce Type: replace 
Abstract: Transfer learning, by leveraging knowledge from pre-trained models, has significantly enhanced the performance of target tasks. However, as deep neural networks scale up, full fine-tuning introduces substantial computational and storage challenges in resource-constrained environments, limiting its widespread adoption. To address this, parameter-efficient fine-tuning (PEFT) methods have been developed to reduce computational complexity and storage requirements by minimizing the number of updated parameters. While matrix decomposition-based PEFT methods, such as LoRA, show promise, they struggle to fully capture the high-dimensional structural characteristics of model weights. In contrast, high-dimensional tensors offer a more natural representation of neural network weights, allowing for a more comprehensive capture of higher-order features and multi-dimensional interactions. In this paper, we propose tCURLoRA, a novel fine-tuning method based on tensor CUR decomposition. By concatenating pre-trained weight matrices into a three-dimensional tensor and applying tensor CUR decomposition, we update only the lower-order tensor components during fine-tuning, effectively reducing computational and storage overhead. Experimental results demonstrate that tCURLoRA outperforms existing PEFT methods in medical image segmentation tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02227v3</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-032-05325-1_55</arxiv:DOI>
      <dc:creator>Guanghua He, Wangang Cheng, Hancan Zhu, Xiaohao Cai, Gaohang Yu</dc:creator>
    </item>
    <item>
      <title>GOUHFI: a novel contrast- and resolution-agnostic segmentation tool for Ultra-High Field MRI</title>
      <link>https://arxiv.org/abs/2505.11445</link>
      <description>arXiv:2505.11445v2 Announce Type: replace 
Abstract: Recently, Ultra-High Field MRI (UHF-MRI) has become more available and one of the best tools to study the brain. One common step in quantitative neuroimaging is to segment the brain into several regions, which has been done using software packages like FreeSurfer , FastSurferVINN or SynthSeg. However, the differences between UHF-MRI and 1.5T or 3T images are such that the automatic segmentation techniques optimized at these field strengths usually produce unsatisfactory segmentation results for UHF images. Thus, it has been particularly challenging to perform region-based quantitative analyses as typically done with 1.5-3T data, underscoring the crucial need for developing new automatic segmentation techniques designed to handle UHF images. Hence, we propose a novel Deep Learning (DL)-based segmentation technique called GOUHFI: Generalized and Optimized segmentation tool for Ultra-High Field Images, designed to segment UHF images of various contrasts and resolutions. For training, we used a total of 206 label maps from datasets acquired at 3T, 7T and 9.4T. In contrast to most DL strategies, we used a domain randomization approach, where synthetic images were used to train a 3D U-Net. GOUHFI was tested on seven different datasets and compared to existing techniques like FastSurferVINN,SynthSeg and CEREBRUM-7T. GOUHFI was able to segment the six contrasts and seven resolutions tested at 3T, 7T and 9.4T. Average Dice scores of 0.90, 0.90 and 0.93 were computed against the ground truth segmentations at 3T, 7T and 9.4T, respectively. Ultimately, GOUHFI is a promising new segmentation tool, being the first of its kind proposing a contrast- and resolution-agnostic alternative for UHF-MRI without requiring fine-tuning or retraining, making it the forthcoming alternative for neuroscientists working with UHF-MRI or even lower field strengths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11445v2</guid>
      <category>eess.IV</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc-Antoine Fortin, Anne Louise Kristoffersen, Michael Staff Larsen, Laurent Lamalle, Ruediger Stirnberg, Paal Erik Goa</dc:creator>
    </item>
    <item>
      <title>Unified Cross-Modal Image Synthesis with Hierarchical Mixture of Product-of-Experts</title>
      <link>https://arxiv.org/abs/2410.19378</link>
      <description>arXiv:2410.19378v2 Announce Type: replace-cross 
Abstract: We propose a deep mixture of multimodal hierarchical variational auto-encoders called MMHVAE that synthesizes missing images from observed images in different modalities. MMHVAE's design focuses on tackling four challenges: (i) creating a complex latent representation of multimodal data to generate high-resolution images; (ii) encouraging the variational distributions to estimate the missing information needed for cross-modal image synthesis; (iii) learning to fuse multimodal information in the context of missing data; (iv) leveraging dataset-level information to handle incomplete data sets at training time. Extensive experiments are performed on the challenging problem of pre-operative brain multi-parametric magnetic resonance and intra-operative ultrasound imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19378v2</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Reuben Dorent, Nazim Haouchine, Alexandra Golby, Sarah Frisken, Tina Kapur, William Wells</dc:creator>
    </item>
    <item>
      <title>Multi-temporal crack segmentation in concrete structures using deep learning approaches</title>
      <link>https://arxiv.org/abs/2411.04620</link>
      <description>arXiv:2411.04620v2 Announce Type: replace-cross 
Abstract: Cracks are among the earliest indicators of deterioration in concrete structures. Early automatic detection of these cracks can significantly extend the lifespan of critical infrastructures, such as bridges, buildings, and tunnels, while simultaneously reducing maintenance costs and facilitating efficient structural health monitoring. This study investigates whether leveraging multi-temporal data for crack segmentation can enhance segmentation quality. Therefore, we compare a Swin UNETR trained on multi-temporal data with a U-Net trained on mono-temporal data to assess the effect of temporal information compared with conventional single-epoch approaches. To this end, a multi-temporal dataset comprising 1356 images, each with 32 sequential crack propagation images, was created. After training the models, experiments were conducted to analyze their generalization ability, temporal consistency, and segmentation quality. The multi-temporal approach consistently outperformed its mono-temporal counterpart, achieving an IoU of $82.72\%$ and a F1-score of $90.54\%$, representing a significant improvement over the mono-temporal model's IoU of $76.69\%$ and F1-score of $86.18\%$, despite requiring only half of the trainable parameters. The multi-temporal model also displayed a more consistent segmentation quality, with reduced noise and fewer errors. These results suggest that temporal information significantly enhances the performance of segmentation models, offering a promising solution for improved crack detection and the long-term monitoring of concrete structures, even with limited sequential data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04620v2</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5194/isprs-annals-X-G-2025-341-2025</arxiv:DOI>
      <arxiv:journal_reference>ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., X-G-2025, 341-348</arxiv:journal_reference>
      <dc:creator>Said Harb, Pedro Achanccaray, Mehdi Maboudi, Markus Gerke</dc:creator>
    </item>
    <item>
      <title>Taming Diffusion Transformer for Efficient Mobile Video Generation in Seconds</title>
      <link>https://arxiv.org/abs/2507.13343</link>
      <description>arXiv:2507.13343v2 Announce Type: replace-cross 
Abstract: Diffusion Transformers (DiT) have shown strong performance in video generation tasks, but their high computational cost makes them impractical for resource-constrained devices like smartphones, and practical on-device generation is even more challenging. In this work, we propose a series of novel optimizations to significantly accelerate video generation and enable practical deployment on mobile platforms. First, we employ a highly compressed variational autoencoder (VAE) to reduce the dimensionality of the input data without sacrificing visual quality. Second, we introduce a KD-guided, sensitivity-aware tri-level pruning strategy to shrink the model size to suit mobile platforms while preserving critical performance characteristics. Third, we develop an adversarial step distillation technique tailored for DiT, which allows us to reduce the number of inference steps to four. Combined, these optimizations enable our model to achieve approximately 15 frames per second (FPS) generation speed on an iPhone 16 Pro Max, demonstrating the feasibility of efficient, high-quality video generation on mobile devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13343v2</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yushu Wu, Yanyu Li, Anil Kag, Ivan Skorokhodov, Willi Menapace, Ke Ma, Arpit Sahni, Ju Hu, Aliaksandr Siarohin, Dhritiman Sagar, Yanzhi Wang, Sergey Tulyakov</dc:creator>
    </item>
    <item>
      <title>Keep It Real: Challenges in Attacking Compression-Based Adversarial Purification</title>
      <link>https://arxiv.org/abs/2508.05489</link>
      <description>arXiv:2508.05489v2 Announce Type: replace-cross 
Abstract: Previous work has suggested that preprocessing images through lossy compression can defend against adversarial perturbations, but comprehensive attack evaluations have been lacking. In this paper, we construct strong white-box and adaptive attacks against various compression models and identify a critical challenge for attackers: high realism in reconstructed images significantly increases attack difficulty. Through rigorous evaluation across multiple attack scenarios, we demonstrate that compression models capable of producing realistic, high-fidelity reconstructions are substantially more resistant to our attacks. In contrast, low-realism compression models can be broken. Our analysis reveals that this is not due to gradient masking. Rather, realistic reconstructions maintaining distributional alignment with natural images seem to offer inherent robustness. This work highlights a significant obstacle for future adversarial attacks and suggests that developing more effective techniques to overcome realism represents an essential challenge for comprehensive security evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05489v2</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Samuel R\"aber, Till Aczel, Andreas Plesner, Roger Wattenhofer</dc:creator>
    </item>
    <item>
      <title>MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs</title>
      <link>https://arxiv.org/abs/2509.11662</link>
      <description>arXiv:2509.11662v3 Announce Type: replace-cross 
Abstract: We propose MindVL, a multimodal large language model (MLLMs) trained on Ascend NPUs. The training of state-of-the-art MLLMs is often confined to a limited set of hardware platforms and relies heavily on massive, undisclosed data recipes, which hinders reproducibility and open research. To change the common perception that Ascend hardware is unsuitable for efficient full-stage MLLM training, we introduce MindSpeed-MLLM, a highly efficient training framework that supports stable and high-performance training of large-scale Dense and Mixture-of-Experts (MoE) models on Ascend hardware. Based on this, we provide a systematic and open description of the data production methods and mixing strategies for all training stages. Furthermore, we present MindVL, a data-efficient multimodal large language model trained end-to-end on Ascend NPUs. In addition, we find that averaging weights from checkpoints trained with different sequence lengths is particularly effective and yields further gains when combined with test-time resolution search. Our experiments demonstrate superior data efficiency: MindVL-8B matches the performance of Qwen2.5VL-7B using only 10\% of its training data, while our MoE model, MindVL-671B-A37B, matches Qwen2.5VL-72B using only 3\% of the Qwen2.5VL training data, and achieves comparable performance with other leading multimodal MoE models. Our work provides the community with a valuable hardware alternative, open data recipes, and effective performance-enhancing techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11662v3</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>eess.IV</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Feilong Chen, Yijiang Liu, Yi Huang, Hao Wang, Miren Tian, Ya-Qi Yu, Minghui Liao, Jihao Wu</dc:creator>
    </item>
  </channel>
</rss>
