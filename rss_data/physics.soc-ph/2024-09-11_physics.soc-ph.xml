<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.soc-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.soc-ph</link>
    <description>physics.soc-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.soc-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Sep 2024 01:46:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Emerging properties of the degree distribution in large non-growing networks</title>
      <link>https://arxiv.org/abs/2409.06099</link>
      <description>arXiv:2409.06099v1 Announce Type: new 
Abstract: The degree distribution is a key statistical indicator in network theory, often used to understand how information spreads across connected nodes. In this paper, we focus on non-growing networks formed through a rewiring algorithm and develop kinetic Boltzmann-type models to capture the emergence of degree distributions that characterize both preferential attachment networks and random networks. Under a suitable mean-field scaling, these models reduce to a Fokker-Planck-type partial differential equation with an affine diffusion coefficient, that is consistent with a well-established master equation for discrete rewiring processes. We further analyze the convergence to equilibrium for this class of Fokker-Planck equations, demonstrating how different regimes -- ranging from exponential to algebraic rates -- depend on network parameters. Our results provide a unified framework for modeling degree distributions in non-growing networks and offer insights into the long-time behavior of such systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06099v1</guid>
      <category>physics.soc-ph</category>
      <category>nlin.AO</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jonathan Franceschi, Lorenzo Pareschi, Mattia Zanella</dc:creator>
    </item>
    <item>
      <title>Uncovering the inherited vulnerability of electric distribution networks</title>
      <link>https://arxiv.org/abs/2409.06194</link>
      <description>arXiv:2409.06194v1 Announce Type: new 
Abstract: Research on the vulnerability of electric networks with a complex network approach has produced significant results in the last decade, especially for transmission networks. These studies have shown that there are causal relations between certain structural properties of networks and their vulnerabilities, leading to an inherent weakness. The purpose of present work was twofold: to test the hypotheses already examined on evolving transmission networks and to gain a deeper understanding on the nature of these inherent weaknesses. For this, historical models of a medium-voltage distribution network supply area were reconstructed and analysed. Topological efficiency of the networks was calculated against node and edge removals of different proportions. We found that the tolerance of the evolving grid remained practically unchanged during the examined period, implying that the increase in size is dominantly caused by the connection of geographically and spatially constrained supply areas and not by an evolutionary process. We also show that probability density functions of centrality metrics, typically connected to vulnerability, show only minor variation during the early evolution of the examined distribution network, and in many cases resemble the properties of the modern days.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06194v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>B\'alint Hartmann, Tam\'as Soha, Michelle T. Cirunay, T\'imea Erdei</dc:creator>
    </item>
    <item>
      <title>Quantum-like approaches unveil the intrinsic limits of predictability in compartmental models</title>
      <link>https://arxiv.org/abs/2409.06438</link>
      <description>arXiv:2409.06438v1 Announce Type: new 
Abstract: Obtaining accurate forecasts for the evolution of epidemic outbreaks from deterministic compartmental models represents a major theoretical challenge. Recently, it has been shown that these models typically exhibit trajectories' degeneracy, as different sets of epidemiological parameters yield comparable predictions at early stages of the outbreak but disparate future epidemic scenarios. Here we use the Doi-Peliti approach and extend the classical deterministic SIS and SIR models to a quantum-like formalism to explore whether the uncertainty of epidemic forecasts is also shaped by the stochastic nature of epidemic processes. This approach allows getting a probabilistic ensemble of trajectories, revealing that epidemic uncertainty is not uniform across time, being maximal around the epidemic peak and vanishing at both early and very late stages of the outbreak. Our results therefore show that, independently of the models' complexity, the stochasticity of contagion and recover processes poses a natural constraint for the uncertainty of epidemic forecasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06438v1</guid>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jos\'e Alejandro Rojas-Venegas, Pablo Gallarta-S\'aenz, Rafael G. Hurtado, Jes\'us G\'omez-Garde\~nes, David Soriano-Pa\~nos</dc:creator>
    </item>
    <item>
      <title>Memory and Personality in Ideological Polarization: The Politico-physics of Mnemomatter</title>
      <link>https://arxiv.org/abs/2409.06660</link>
      <description>arXiv:2409.06660v1 Announce Type: new 
Abstract: We used physical agents with deep memories of past events and left/right ideologies but different fixed personalities to study what drives the polarization of the dynamic population ideology. We find that agents have a critical memory depth below which complete ideology polarization of the collective cannot occur and above which it is inevitable. However, depending on the details of the personalities, the ideologies polarization can be static or dynamic in time, even chaotic. Thus, agents with different personalities and levels of memory (mnemomatter) can serve as a physics analogue of the ideology dynamics among ideological beings, illuminating how decisions influenced by individual memories of past interactions can shape and influence subsequent ideology polarization. Each constituent agent harbors a private stack memory and an onboard microcomputer/controller which both measures and controls its physical spin handedness, which is a proxy for ideology. The agent's decision to change or retain its current spin is determined by each agent's private algorithm for decisions (the personality) and the time-weighted stack history of present and previous interactions. Depending on a given agent's personality for evaluating its memory and experiences, an agent can act as a curmudgeon who never changes its ideology, a pushover who always accepts change, a contrarian who always does the opposite of what is expected, an opportunist who weighs recent events more heavily than past events in making decisions, and a traditionalist who weighs past events more heavily than recent events in decision making. We develop a field theory which maps agent ideological polarization over into a dynamic potential landscape. Perhaps such applications of physics-based systems to political systems will help us to understand the ideological instability observed in the world today.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06660v1</guid>
      <category>physics.soc-ph</category>
      <category>cond-mat.soft</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shengkai Li, Trung V. Phan, Luca Di Carlo, Gao Wang, Van H. Do, Elia Mikhail, Robert H. Austin, Liyu Liu</dc:creator>
    </item>
    <item>
      <title>Automating the Practice of Science -- Opportunities, Challenges, and Implications</title>
      <link>https://arxiv.org/abs/2409.05890</link>
      <description>arXiv:2409.05890v1 Announce Type: cross 
Abstract: Automation transformed various aspects of our human civilization, revolutionizing industries and streamlining processes. In the domain of scientific inquiry, automated approaches emerged as powerful tools, holding promise for accelerating discovery, enhancing reproducibility, and overcoming the traditional impediments to scientific progress. This article evaluates the scope of automation within scientific practice and assesses recent approaches. Furthermore, it discusses different perspectives to the following questions: Where do the greatest opportunities lie for automation in scientific practice?; What are the current bottlenecks of automating scientific practice?; and What are significant ethical and practical consequences of automating scientific practice? By discussing the motivations behind automated science, analyzing the hurdles encountered, and examining its implications, this article invites researchers, policymakers, and stakeholders to navigate the rapidly evolving frontier of automated scientific practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05890v1</guid>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Musslick, Laura K. Bartlett, Suyog H. Chandramouli, Marina Dubova, Fernand Gobet, Thomas L. Griffiths, Jessica Hullman, Ross D. King, J. Nathan Kutz, Christopher G. Lucas, Suhas Mahesh, Franco Pestilli, Sabina J. Sloman, William R. Holmes</dc:creator>
    </item>
    <item>
      <title>Universal Workflow Language and Software Enables Geometric Learning and FAIR Scientific Protocol Reporting</title>
      <link>https://arxiv.org/abs/2409.05899</link>
      <description>arXiv:2409.05899v1 Announce Type: cross 
Abstract: The modern technological landscape has trended towards increased precision and greater digitization of information. However, the methods used to record and communicate scientific procedures have remained largely unchanged over the last century. Written text as the primary means for communicating scientific protocols poses notable limitations in human and machine information transfer. In this work, we present the Universal Workflow Language (UWL) and the open-source Universal Workflow Language interface (UWLi). UWL is a graph-based data architecture that can capture arbitrary scientific procedures through workflow representation of protocol steps and embedded procedure metadata. It is machine readable, discipline agnostic, and compatible with FAIR reporting standards. UWLi is an accompanying software package for building and manipulating UWL files into tabular and plain text representations in a controlled, detailed, and multilingual format. UWL transcription of protocols from three high-impact publications resulted in the identification of substantial deficiencies in the detail of the reported procedures. UWL transcription of these publications identified seventeen procedural ambiguities and thirty missing parameters for every one hundred words in published procedures. In addition to preventing and identifying procedural omission, UWL files were found to be compatible with geometric learning techniques for representing scientific protocols. In a surrogate function designed to represent an arbitrary multi-step experimental process, graph transformer networks were able to predict outcomes in approximately 6,000 fewer experiments than equivalent linear models. Implementation of UWL and UWLi into the scientific reporting process will result in higher reproducibility between both experimentalists and machines, thus proving an avenue to more effective modeling and control of complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05899v1</guid>
      <category>cs.DL</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert W. Epps, Amanda A. Volk, Robert R. White, Robert Tirawat, Rosemary C. Bramante, Joseph J. Berry</dc:creator>
    </item>
    <item>
      <title>Is methane the 'climate culprit'? Fixing the 'Broken Record' while unmasking the dangers of using imprecise, long-term GWP for methane to address the climate emergency</title>
      <link>https://arxiv.org/abs/2409.06212</link>
      <description>arXiv:2409.06212v1 Announce Type: cross 
Abstract: Methane (CH4) is a potent greenhouse gas (GHG) with a short atmospheric half-life (~8.4 years) and a high short-term impact on global warming, significantly higher than CO2 (Kleinberg, 2020; Balcombe et al., 2018). Traditional metrics such as the 100-year Global Warming Potential (GWP100) obscure methane's short-term, negative climatic effects, potentially leading to inadequate policy responses (Kleinberg, 2020). This letter examines the limitations of GWP100 in capturing methane's true climate impact, explores alternative metrics, and discusses the implications of underreporting methane emissions. We highlight the necessity of adopting a more immediate perspective on methane to accelerate climate emergency action, while noting the adverse effects of the rapid growth rate of methane emissions on reduction efforts. Additionally, we hope that in the immediate future, during COP29, policymakers will adopt actions that give appropriate attention to methane's short-term warming potential to dramatically reduce emissions and address the immediate climate crisis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06212v1</guid>
      <category>physics.ao-ph</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roger W. Bryenton, Farrukh A. Chishtie, Mujtaba Hassan, Tom Mommsen, Devyani Singh</dc:creator>
    </item>
    <item>
      <title>Fast nonparametric inference of network backbones for graph sparsification</title>
      <link>https://arxiv.org/abs/2409.06417</link>
      <description>arXiv:2409.06417v1 Announce Type: cross 
Abstract: A network backbone provides a useful sparse representation of a weighted network by keeping only its most important links, permitting a range of computational speedups and simplifying complex network visualizations. There are many possible criteria for a link to be considered important, and hence many methods have been developed for the task of network backboning for graph sparsification. These methods can be classified as global or local in nature depending on whether they evaluate the importance of an edge in the context of the whole network or an individual node neighborhood. A key limitation of existing network backboning methods is that they either artificially restrict the topology of the backbone to take a specific form (e.g. a tree) or they require the specification of a free parameter (e.g. a significance level) that determines the number of edges to keep in the backbone. Here we develop a completely nonparametric framework for inferring the backbone of a weighted network that overcomes these limitations by automatically selecting the optimal number of edges to retain in the backbone using the Minimum Description Length (MDL) principle from information theory. We develop two encoding schemes that serve as objective functions for global and local network backbones, as well as efficient optimization algorithms to identify the optimal backbones according to these objectives with runtime complexity log-linear in the number of edges. We show that the proposed framework is generalizable to any discrete weight distribution on the edges using a maximum a posteriori (MAP) estimation procedure with an asymptotically equivalent Bayesian generative model of the backbone. We compare the proposed method with existing methods in a range of tasks on real and synthetic networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06417v1</guid>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alec Kirkley</dc:creator>
    </item>
    <item>
      <title>An impossibility result for Markov Chain Monte Carlo sampling from micro-canonical bipartite graph ensembles</title>
      <link>https://arxiv.org/abs/2308.10838</link>
      <description>arXiv:2308.10838v3 Announce Type: replace-cross 
Abstract: Markov Chain Monte Carlo (MCMC) algorithms are commonly used to sample from graph ensembles. Two graphs are neighbors in the state space if one can be obtained from the other with only a few modifications, e.g., edge rewirings. For many common ensembles, e.g., those preserving the degree sequences of bipartite graphs, rewiring operations involving two edges are sufficient to create a fully-connected state space, and they can be performed efficiently. We show that, for ensembles of bipartite graphs with fixed degree sequences and number of butterflies (k2,2 bi-cliques), there is no universal constant c such that a rewiring of at most c edges at every step is sufficient for any such ensemble to be fully connected. Our proof relies on an explicit construction of a family of pairs of graphs with the same degree sequences and number of butterflies, with each pair indexed by a natural c, and such that any sequence of rewiring operations transforming one graph into the other must include at least one rewiring operation involving at least c edges. Whether rewiring these many edges is sufficient to guarantee the full connectivity of the state space of any such ensemble remains an open question. Our result implies the impossibility of developing efficient, graph-agnostic, MCMC algorithms for these ensembles, as the necessity to rewire an impractically large number of edges may hinder taking a step on the state space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.10838v3</guid>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevE.109.L053301</arxiv:DOI>
      <dc:creator>Giulia Preti, Gianmarco De Francisci Morales, Matteo Riondato</dc:creator>
    </item>
  </channel>
</rss>
