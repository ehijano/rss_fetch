<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 12 Nov 2024 05:00:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Layered Hill esimator for extreme data in clusters</title>
      <link>https://arxiv.org/abs/2411.05808</link>
      <description>arXiv:2411.05808v1 Announce Type: new 
Abstract: A new estimator is proposed for estimating the tail exponent of a heavy-tailed distribution. This estimator, referred to as the layered Hill estimator, is a generalization of the traditional Hill estimator, building upon a layered structure formed by clusters of extreme values. We argue that the layered Hill estimator provides a robust alternative to the traditional approach, exhibiting desirable asymptotic properties such as consistency and asymptotic normality for the tail exponent. Both theoretical analysis and simulation studies demonstrate that the layered Hill estimator shows significantly better and more robust performance, particularly when a portion of the extreme data is missing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05808v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taegyu Kang, Takashi Owada</dc:creator>
    </item>
    <item>
      <title>Shared-Endpoint Correlations and Hierarchy in Random Flows on Graphs</title>
      <link>https://arxiv.org/abs/2411.06314</link>
      <description>arXiv:2411.06314v1 Announce Type: new 
Abstract: We analyze the correlation between randomly chosen edge weights on neighboring edges in a directed graph. This shared-endpoint correlation controls the expected organization of randomly drawn edge flows when the flow on each edge is conditionally independent of the flows on other edges given its endpoints. To model different relationships between endpoints and flow, we draw edge weights in two stages. First, assign a random description to the vertices by sampling random attributes at each vertex. Then, sample a Gaussian process (GP) and evaluate it on the pair of endpoints connected by each edge. We model different relationships between endpoint attributes and flow by varying the kernel associated with the GP. We then relate the expected flow structure to the smoothness class containing functions generated by the GP. We compute the exact shared-endpoint correlation for the squared exponential kernel and provide accurate approximations for Mat\'ern kernels. In addition, we provide asymptotics in both smooth and rough limits and isolate three distinct domains distinguished by the regularity of the ensemble of sampled functions. Taken together, these results demonstrate a consistent effect; smoother functions relating attributes to flow produce more organized flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06314v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Richalnd, Alexander Strang</dc:creator>
    </item>
    <item>
      <title>Generalized Principal Component Analysis for Large-dimensional Matrix Factor Model</title>
      <link>https://arxiv.org/abs/2411.06423</link>
      <description>arXiv:2411.06423v1 Announce Type: new 
Abstract: Matrix factor models have been growing popular dimension reduction tools for large-dimensional matrix time series. However, the heteroscedasticity of the idiosyncratic components has barely received any attention. Starting from the pseudo likelihood function, this paper introduces a Generalized Principal Component Analysis (GPCA) method for matrix factor model which takes the heteroscedasticity into account. Theoretically, we first derive the asymptotic distribution of the GPCA estimators by assuming the separable covariance matrices are known in advance. We then propose adaptive thresholding estimators for the separable covariance matrices and show that this would not alter the asymptotic distribution of the GPCA estimators under certain regular sparsity conditions in the high-dimensional covariance matrix estimation literature. The GPCA estimators are shown to be more efficient than the state-of-the-art methods under certain heteroscedasticity conditions. Thorough numerical studies are conducted to demonstrate the superiority of our method over the existing approaches. Analysis of a financial portfolio dataset illustrates the empirical usefulness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06423v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yong He, Yujie Hou, Haixia Liu, Yalin Wang</dc:creator>
    </item>
    <item>
      <title>Algebraic and Statistical Properties of the Partially Regularized Ordinary Least Squares Interpolator</title>
      <link>https://arxiv.org/abs/2411.06593</link>
      <description>arXiv:2411.06593v1 Announce Type: new 
Abstract: Modern deep learning has revealed a surprising statistical phenomenon known as benign overfitting, with high-dimensional linear regression being a prominent example. This paper contributes to ongoing research on the ordinary least squares (OLS) interpolator, focusing on the partial regression setting, where only a subset of coefficients is implicitly regularized. On the algebraic front, we extend Cochran's formula and the leave-one-out residual formula for the partial regularization framework. On the stochastic front, we leverage our algebraic results to design several homoskedastic variance estimators under the Gauss-Markov model. These estimators serve as a basis for conducting statistical inference, albeit with slight conservatism in their performance. Through simulations, we study the finite-sample properties of these variance estimators across various generative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06593v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Letian Yang, Dennis Shen</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of trend for stochastic differential equations driven by multiplicative stochastic volatility</title>
      <link>https://arxiv.org/abs/2411.06865</link>
      <description>arXiv:2411.06865v1 Announce Type: new 
Abstract: We discuss nonparametric estimation of the trend coefficient in models governed by a stochastic differential equation driven by a multiplicative stochastic volatility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06865v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>B. L. S. Prakasa Rao</dc:creator>
    </item>
    <item>
      <title>Improved estimation of ordered parameter based on doubly type-II censored sample</title>
      <link>https://arxiv.org/abs/2411.06888</link>
      <description>arXiv:2411.06888v1 Announce Type: new 
Abstract: In the life testing experiment and reliability engineering doubly type-II censored scheme is an important sampling scheme. In the present commutation, we have considered estimating ordered scale parameters of two exponential distributions based on doubly type-II censored samples. For this estimation problem, we have considered a general scale invariant loss function. We have obtained several estimators using \cite{stein1964} techniques that improve upon the BAEE. Also we have obtained estimators which improve upon the restricted MLE. A class of improved estimators has been derived using Kubokawa's IERD approach. It is shown that the boundary estimator of this class is generalized Bayes. As an application, we have also obtained improved estimators with respect to three special loss functions, namely quadratic loss, entropy loss, and symmetric loss function. We have applied these results to special life testing sampling schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06888v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shrajal Bajpai, Lakshmi Kanta Patra</dc:creator>
    </item>
    <item>
      <title>Detecting relevant deviations from the white noise assumption for non-stationary time series</title>
      <link>https://arxiv.org/abs/2411.06909</link>
      <description>arXiv:2411.06909v1 Announce Type: new 
Abstract: We consider the problem of detecting deviations from a white noise assumption in time series. Our approach differs from the numerous methods proposed for this purpose with respect to two aspects. First, we allow for non-stationary time series. Second, we address the problem that a white noise test, for example checking the residuals of a model fit, is usually not performed because one believes in this hypothesis, but thinks that the white noise hypothesis may be approximately true, because a postulated models describes the unknown relation well. This reflects a meanwhile classical paradigm of Box(1976) that "all models are wrong but some are useful". We address this point of view by investigating if the maximum deviation of the local autocovariance functions from 0 exceeds a given threshold $\Delta$ that can either be specified by the user or chosen in a data dependent way.
  The formulation of the problem in this form raises several mathematical challenges, which do not appear when one is testing the classical white noise hypothesis. We use high dimensional Gaussian approximations for dependent data to furnish a bootstrap test, prove its validity and showcase its performance on both synthetic and real data, in particular we inspect log returns of stock prices and show that our approach reflects some observations of Fama(1970) regarding the efficient market hypothesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06909v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Bastian</dc:creator>
    </item>
    <item>
      <title>On strong posterior contraction rates for Besov-Laplace priors in the white noise model</title>
      <link>https://arxiv.org/abs/2411.06981</link>
      <description>arXiv:2411.06981v1 Announce Type: new 
Abstract: In this article, we investigate the problem of estimating a spatially inhomogeneous function and its derivatives in the white noise model using Besov-Laplace priors. We show that smoothness-matching priors attains minimax optimal posterior contraction rates, in strong Sobolev metrics, over the Besov spaces $B^\beta_{11}$, $\beta &gt; d/2$, closing a gap in the existing literature. Our strong posterior contraction rates also imply that the posterior distributions arising from Besov-Laplace priors with matching regularity enjoy a desirable plug-in property for derivative estimation, entailing that the push-forward measures under differential operators optimally recover the derivatives of the unknown regression function. The proof of our results relies on the novel approach to posterior contraction rates, based on Wasserstein distance, recently developed by Dolera, Favaro and Mainini (Probability Theory and Related Fields, 2024). We show how this approach allows to overcome some technical challenges that emerge in the frequentist analysis of smoothness-matching Besov-Laplace priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06981v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emanuele Dolera, Stefano Favaro, Matteo Giordano</dc:creator>
    </item>
    <item>
      <title>Estimation of the Adjusted Standard-deviatile for Extreme Risks</title>
      <link>https://arxiv.org/abs/2411.07203</link>
      <description>arXiv:2411.07203v1 Announce Type: new 
Abstract: In this paper, we modify the Bayes risk for the expectile, the so-called variantile risk measure, to better capture extreme risks. The modified risk measure is called the adjusted standard-deviatile. First, we derive the asymptotic expansions of the adjusted standard-deviatile. Next, based on the first-order asymptotic expansion, we propose two efficient estimation methods for the adjusted standard-deviatile at intermediate and extreme levels. By using techniques from extreme value theory, the asymptotic normality is proved for both estimators. Simulations and real data applications are conducted to examine the performance of the proposed estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07203v1</guid>
      <category>math.ST</category>
      <category>q-fin.RM</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoyu Chen, Tiantian Mao, Fan Yang</dc:creator>
    </item>
    <item>
      <title>Deep Nonparametric Conditional Independence Tests for Images</title>
      <link>https://arxiv.org/abs/2411.06140</link>
      <description>arXiv:2411.06140v1 Announce Type: cross 
Abstract: Conditional independence tests (CITs) test for conditional dependence between random variables. As existing CITs are limited in their applicability to complex, high-dimensional variables such as images, we introduce deep nonparametric CITs (DNCITs). The DNCITs combine embedding maps, which extract feature representations of high-dimensional variables, with nonparametric CITs applicable to these feature representations. For the embedding maps, we derive general properties on their parameter estimators to obtain valid DNCITs and show that these properties include embedding maps learned through (conditional) unsupervised or transfer learning. For the nonparametric CITs, appropriate tests are selected and adapted to be applicable to feature representations. Through simulations, we investigate the performance of the DNCITs for different embedding maps and nonparametric CITs under varying confounder dimensions and confounder relationships. We apply the DNCITs to brain MRI scans and behavioral traits, given confounders, of healthy individuals from the UK Biobank (UKB), confirming null results from a number of ambiguous personality neuroscience studies with a larger data set and with our more powerful tests. In addition, in a confounder control study, we apply the DNCITs to brain MRI scans and a confounder set to test for sufficient confounder control, leading to a potential reduction in the confounder dimension under improved confounder control compared to existing state-of-the-art confounder control studies for the UKB. Finally, we provide an R package implementing the DNCITs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06140v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Simnacher, Xiangnan Xu, Hani Park, Christoph Lippert, Sonja Greven</dc:creator>
    </item>
    <item>
      <title>When are dynamical systems learned from time series data statistically accurate?</title>
      <link>https://arxiv.org/abs/2411.06311</link>
      <description>arXiv:2411.06311v1 Announce Type: cross 
Abstract: Conventional notions of generalization often fail to describe the ability of learned models to capture meaningful information from dynamical data. A neural network that learns complex dynamics with a small test error may still fail to reproduce its \emph{physical} behavior, including associated statistical moments and Lyapunov exponents. To address this gap, we propose an ergodic theoretic approach to generalization of complex dynamical models learned from time series data. Our main contribution is to define and analyze generalization of a broad suite of neural representations of classes of ergodic systems, including chaotic systems, in a way that captures emulating underlying invariant, physical measures. Our results provide theoretical justification for why regression methods for generators of dynamical systems (Neural ODEs) fail to generalize, and why their statistical accuracy improves upon adding Jacobian information during training. We verify our results on a number of ergodic chaotic systems and neural network parameterizations, including MLPs, ResNets, Fourier Neural layers, and RNNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06311v1</guid>
      <category>cs.LG</category>
      <category>math-ph</category>
      <category>math.DS</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeongjin Park, Nicole Yang, Nisha Chandramoorthy</dc:creator>
    </item>
    <item>
      <title>BudgetIV: Optimal Partial Identification of Causal Effects with Mostly Invalid Instruments</title>
      <link>https://arxiv.org/abs/2411.06913</link>
      <description>arXiv:2411.06913v1 Announce Type: cross 
Abstract: Instrumental variables (IVs) are widely used to estimate causal effects in the presence of unobserved confounding between exposure and outcome. An IV must affect the outcome exclusively through the exposure and be unconfounded with the outcome. We present a framework for relaxing either or both of these strong assumptions with tuneable and interpretable budget constraints. Our algorithm returns a feasible set of causal effects that can be identified exactly given relevant covariance parameters. The feasible set may be disconnected but is a finite union of convex subsets. We discuss conditions under which this set is sharp, i.e., contains all and only effects consistent with the background assumptions and the joint distribution of observable variables. Our method applies to a wide class of semiparametric models, and we demonstrate how its ability to select specific subsets of instruments confers an advantage over convex relaxations in both linear and nonlinear settings. We also adapt our algorithm to form confidence sets that are asymptotically valid under a common statistical assumption from the Mendelian randomization literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06913v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordan Penn (King's College London), Lee M. Gunderson (University College London), Gecia Bravo-Hermsdorff (University College London), Ricardo Silva (University College London), David S. Watson (King's College London)</dc:creator>
    </item>
    <item>
      <title>Correction to: Multivariate CARMA processes, continuous-time state space models and complete regularity of the innovations of the sampled processes, Bernoulli 18, pp. 46-63, 2012</title>
      <link>https://arxiv.org/abs/2411.06935</link>
      <description>arXiv:2411.06935v1 Announce Type: cross 
Abstract: A serious flaw in the proof of the equivalence of continuous time state space models and MCARMA processes spotted in Fasen and Schenk (2024) is corrected. We point out that likewise an issue in the proof of Theorem 3.2 in Brockwell and Schlemm (2013) can be resolved and, hence, any MCARMA process and linear state space model has both a controller and an observer canonical representation. Equivalently, the transfer function has both a left and right matrix fraction representation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06935v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert Stelzer</dc:creator>
    </item>
    <item>
      <title>Universal Rank Inference via Residual Subsampling with Application to Large Networks</title>
      <link>https://arxiv.org/abs/1912.11583</link>
      <description>arXiv:1912.11583v3 Announce Type: replace 
Abstract: Determining the precise rank is an important problem in many large-scale applications with matrix data exploiting low-rank plus noise models. In this paper, we suggest a universal approach to rank inference via residual subsampling (RIRS) for testing and estimating rank in a wide family of models, including many popularly used network models such as the degree corrected mixed membership model as a special case. Our procedure constructs a test statistic via subsampling entries of the residual matrix after extracting the spiked components. The test statistic converges in distribution to the standard normal under the null hypothesis, and diverges to infinity with asymptotic probability one under the alternative hypothesis. The effectiveness of RIRS procedure is justified theoretically, utilizing the asymptotic expansions of eigenvectors and eigenvalues for large random matrices recently developed in [11] and [12]. The advantages of the newly suggested procedure are demonstrated through several simulation and real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:1912.11583v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiao Han, Qing Yang, Yingying Fan</dc:creator>
    </item>
    <item>
      <title>When Frictions are Fractional: Rough Noise in High-Frequency Data</title>
      <link>https://arxiv.org/abs/2106.16149</link>
      <description>arXiv:2106.16149v4 Announce Type: replace 
Abstract: The analysis of high-frequency financial data is often impeded by the presence of noise. This article is motivated by intraday return data in which market microstructure noise appears to be rough, that is, best captured by a continuous-time stochastic process that locally behaves as fractional Brownian motion. Assuming that the underlying efficient price process follows a continuous It\^o semimartingale, we derive consistent estimators and asymptotic confidence intervals for the roughness parameter of the noise and the integrated price and noise volatilities, in all cases where these quantities are identifiable. In addition to desirable features such as serial dependence of increments, compatibility between different sampling frequencies and diurnal effects, the rough noise model can further explain divergence rates in volatility signature plots that vary considerably over time and between assets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.16149v4</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carsten H. Chong, Thomas Delerue, Guoying Li</dc:creator>
    </item>
    <item>
      <title>Wasserstein-based Minimax Estimation of Dependence in Multivariate Regularly Varying Extremes</title>
      <link>https://arxiv.org/abs/2312.09862</link>
      <description>arXiv:2312.09862v2 Announce Type: replace 
Abstract: We present the first minimax risk bounds for estimators of the spectral measure in multivariate linear factor models, where observations are linear combinations of regularly varying latent factors. Non-asymptotic convergence rates are derived for the multivariate Peak-over-Threshold estimator in terms of the $p$-th order Wasserstein distance, and information-theoretic lower bounds for the minimax risks are established. The convergence rate of the estimator is shown to be minimax optimal under a class of Pareto-type models analogous to the standard class used in the setting of one-dimensional observations known as the Hall-Welsh class. When the estimator is minimax inefficient, a novel two-step estimator is introduced and demonstrated to attain the minimax lower bound. Our analysis bridges the gaps in understanding trade-offs between estimation bias and variance in multivariate extreme value theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.09862v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuhui Zhang, Jose Blanchet, Youssef Marzouk, Viet Anh Nguyen, Sven Wang</dc:creator>
    </item>
    <item>
      <title>Oja's Algorithm for Streaming Sparse PCA</title>
      <link>https://arxiv.org/abs/2402.07240</link>
      <description>arXiv:2402.07240v5 Announce Type: replace 
Abstract: Oja's algorithm for Streaming Principal Component Analysis (PCA) for $n$ data-points in a $d$ dimensional space achieves the same sin-squared error $O(r_{\mathsf{eff}}/n)$ as the offline algorithm in $O(d)$ space and $O(nd)$ time and a single pass through the datapoints. Here $r_{\mathsf{eff}}$ is the effective rank (ratio of the trace and the principal eigenvalue of the population covariance matrix $\Sigma$). Under this computational budget, we consider the problem of sparse PCA, where the principal eigenvector of $\Sigma$ is $s$-sparse, and $r_{\mathsf{eff}}$ can be large. In this setting, to our knowledge, \textit{there are no known single-pass algorithms} that achieve the minimax error bound in $O(d)$ space and $O(nd)$ time without either requiring strong initialization conditions or assuming further structure (e.g., spiked) of the covariance matrix. We show that a simple single-pass procedure that thresholds the output of Oja's algorithm (the Oja vector) can achieve the minimax error bound under some regularity conditions in $O(d)$ space and $O(nd)$ time. We present a nontrivial and novel analysis of the entries of the unnormalized Oja vector, which involves the projection of a product of independent random matrices on a random initial vector. This is completely different from previous analyses of Oja's algorithm and matrix products, which have been done when the $r_{\mathsf{eff}}$ is bounded.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07240v5</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Syamantak Kumar, Purnamrita Sarkar</dc:creator>
    </item>
    <item>
      <title>Self-Organizing State-Space Models with Artificial Dynamics</title>
      <link>https://arxiv.org/abs/2409.08928</link>
      <description>arXiv:2409.08928v4 Announce Type: replace 
Abstract: We consider a state-space model (SSM) parametrized by some parameter $\theta$ and aim at performing joint parameter and state inference. A popular idea to carry out this task is to replace $\theta$ by a Markov chain $(\theta_t)_{t\geq 0}$ and then to apply a filtering algorithm to the extended, or self-organizing SSM (SO-SSM). However, the practical implementation of this idea in a theoretically justified way has remained an open problem. In this paper we fill this gap by introducing constructions of $(\theta_t)_{t\geq 0}$ that ensure the validity of the SO-SSM for joint parameter and state inference. Notably, we show that such SO-SSMs can be defined even if $\|\mathrm{Var}(\theta_{t}|\theta_{t-1})\|\rightarrow 0$ slowly as $t\rightarrow\infty$. This result is important since these models can be efficiently approximated using a particle filter. While SO-SSMs have been introduced for online inference, the development of iterated filtering (IF) has shown that they can also serve for computing the maximum likelihood estimator of an SSM. We also derive constructions of $(\theta_t)_{t\geq 0}$ and theoretical guarantees tailored to these specific applications of SO-SSMs and introduce new IF algorithms. From a practical point of view, the algorithms we develop are simple to implement and only require minimal tuning to perform well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08928v4</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Chen, Mathieu Gerber, Christophe Andrieu, Randal Douc</dc:creator>
    </item>
    <item>
      <title>Valid Credible Ellipsoids for Linear Functionals by a Renormalized Bernstein-von Mises Theorem</title>
      <link>https://arxiv.org/abs/2409.10947</link>
      <description>arXiv:2409.10947v2 Announce Type: replace 
Abstract: We consider an infinite-dimensional Gaussian regression model, equipped with a high-dimensional Gaussian prior. We address the frequentist validity of posterior credible sets for a vector of linear functionals.
  We specify conditions for a 'renormalized' Bernstein-von Mises theorem (BvM), where the posterior, centered at its mean, and the posterior mean, centered at the ground truth, have the same normal approximation. This requires neither a solution to the information equation nor a $\sqrt{N}$-consistent estimator.
  We show that our renormalized BvM implies that a credible ellipsoid, specified by the mean and variance of the posterior, is an asymptotic confidence set. For a single linear functional, we identify a credible ellipsoid with a symmetric credible interval around the posterior mean. We bound the diameter.
  We check our conditions for Darcy's problem, where the information equation has no solution in natural settings. For the Schr\"odinger problem, we recover an efficient semi-parametric BvM from our renormalized BvM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10947v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gustav R{\o}mer</dc:creator>
    </item>
    <item>
      <title>High-frequency analysis of parabolic stochastic PDEs with multiplicative noise</title>
      <link>https://arxiv.org/abs/1908.04145</link>
      <description>arXiv:1908.04145v2 Announce Type: replace-cross 
Abstract: We consider the stochastic heat equation driven by a multiplicative Gaussian noise that is white in time and spatially homogeneous in space. Assuming that the spatial correlation function is given by a Riesz kernel of order $\alpha \in (0,1)$, we prove a central limit theorem for power variations and other related functionals of the solution. To our surprise, there is no asymptotic bias despite the low regularity of the noise coefficient in the multiplicative case. We trace this circumstance back to cancellation effects between error terms arising naturally in second-order limit theorems for power variations.</description>
      <guid isPermaLink="false">oai:arXiv.org:1908.04145v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carsten Chong</dc:creator>
    </item>
    <item>
      <title>Short-time expansion of characteristic functions in a rough volatility setting with applications</title>
      <link>https://arxiv.org/abs/2208.00830</link>
      <description>arXiv:2208.00830v2 Announce Type: replace-cross 
Abstract: We derive a higher-order asymptotic expansion of the conditional characteristic function of the increment of an It\^o semimartingale over a shrinking time interval. The spot characteristics of the It\^o semimartingale are allowed to have dynamics of general form. In particular, their paths can be rough, that is, exhibit local behavior like that of a fractional Brownian motion, while at the same time have jumps with arbitrary degree of activity. The expansion result shows the distinct roles played by the different features of the spot characteristics dynamics. As an application of our result, we construct a nonparametric estimator of the Hurst parameter of the diffusive volatility process from portfolios of short-dated options written on an underlying asset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.00830v2</guid>
      <category>q-fin.ST</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carsten H. Chong, Viktor Todorov</dc:creator>
    </item>
    <item>
      <title>Optimizing Noise for $f$-Differential Privacy via Anti-Concentration and Stochastic Dominance</title>
      <link>https://arxiv.org/abs/2308.08343</link>
      <description>arXiv:2308.08343v3 Announce Type: replace-cross 
Abstract: In this paper, we establish anti-concentration inequalities for additive noise mechanisms which achieve $f$-differential privacy ($f$-DP), a notion of privacy phrased in terms of a tradeoff function $f$ which limits the ability of an adversary to determine which individuals were in the database. We show that canonical noise distributions (CNDs), proposed by Awan and Vadhan (2023), match the anti-concentration bounds at half-integer values, indicating that their tail behavior is near-optimal. We also show that all CNDs are sub-exponential, regardless of the $f$-DP guarantee. In the case of log-concave CNDs, we show that they are the stochastically smallest noise compared to any other noise distributions with the same privacy guarantee. In terms of integer-valued noise, we propose a new notion of discrete CND and prove that a discrete CND always exists, can be constructed by rounding a continuous CND, and that the discrete CND is unique when designed for a statistic with sensitivity 1. We further show that the discrete CND at sensitivity 1 is stochastically smallest compared to other integer-valued noises. Our theoretical results shed light on the different types of privacy guarantees possible in the $f$-DP framework and can be incorporated in more complex mechanisms to optimize performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.08343v3</guid>
      <category>cs.CR</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordan Awan, Aishwarya Ramasethu</dc:creator>
    </item>
    <item>
      <title>Scalable simulation-based inference for implicitly defined models using a metamodel for Monte Carlo log-likelihood estimator</title>
      <link>https://arxiv.org/abs/2311.09446</link>
      <description>arXiv:2311.09446v2 Announce Type: replace-cross 
Abstract: Models implicitly defined through a random simulator of a process have become widely used in scientific and industrial applications in recent years. However, simulation-based inference methods for such implicit models, like approximate Bayesian computation (ABC), often scale poorly as data size increases. We develop a scalable inference method for implicitly defined models using a metamodel for the Monte Carlo log-likelihood estimator derived from simulations. This metamodel characterizes both statistical and simulation-based randomness in the distribution of the log-likelihood estimator across different parameter values. Our metamodel-based method quantifies uncertainty in parameter estimation in a principled manner, leveraging the local asymptotic normality of the mean function of the log-likelihood estimator. We apply this method to construct accurate confidence intervals for parameters of partially observed Markov process models where the Monte Carlo log-likelihood estimator is obtained using the bootstrap particle filter. We numerically demonstrate that our method enables accurate and highly scalable parameter inference across several examples, including a mechanistic compartment model for infectious diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.09446v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joonha Park</dc:creator>
    </item>
    <item>
      <title>Towards Efficient and Optimal Covariance-Adaptive Algorithms for Combinatorial Semi-Bandits</title>
      <link>https://arxiv.org/abs/2402.15171</link>
      <description>arXiv:2402.15171v3 Announce Type: replace-cross 
Abstract: We address the problem of stochastic combinatorial semi-bandits, where a player selects among $P$ actions from the power set of a set containing $d$ base items. Adaptivity to the problem's structure is essential in order to obtain optimal regret upper bounds. As estimating the coefficients of a covariance matrix can be manageable in practice, leveraging them should improve the regret. We design ``optimistic'' covariance-adaptive algorithms relying on online estimations of the covariance structure, called OLSUCBC and COSV (only the variances for the latter). They both yields improved gap-free regret. Although COSV can be slightly suboptimal, it improves on computational complexity by taking inspiration from Thompson Sampling approaches. It is the first sampling-based algorithm satisfying a $\sqrt{T}$ gap-free regret (up to poly-logs). We also show that in some cases, our approach efficiently leverages the semi-bandit feedback and outperforms bandit feedback approaches, not only in exponential regimes where $P\gg d$ but also when $P\leq d$, which is not covered by existing analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15171v3</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julien Zhou (Criteo, Inria), Pierre Gaillard (Inria), Thibaud Rahier (Criteo), Houssam Zenati (Inria), Julyan Arbel (Inria)</dc:creator>
    </item>
    <item>
      <title>Statistical Agnostic Regression: a machine learning method to validate regression models</title>
      <link>https://arxiv.org/abs/2402.15213</link>
      <description>arXiv:2402.15213v3 Announce Type: replace-cross 
Abstract: Regression analysis is a central topic in statistical modeling, aimed at estimating the relationships between a dependent variable, commonly referred to as the response variable, and one or more independent variables, i.e., explanatory variables. Linear regression is by far the most popular method for performing this task in various fields of research, such as data integration and predictive modeling when combining information from multiple sources. Classical methods for solving linear regression problems, such as Ordinary Least Squares (OLS), Ridge, or Lasso regressions, often form the foundation for more advanced machine learning (ML) techniques, which have been successfully applied, though without a formal definition of statistical significance. At most, permutation or analyses based on empirical measures (e.g., residuals or accuracy) have been conducted, leveraging the greater sensitivity of ML estimations for detection. In this paper, we introduce Statistical Agnostic Regression (SAR) for evaluating the statistical significance of ML-based linear regression models. This is achieved by analyzing concentration inequalities of the actual risk (expected loss) and considering the worst-case scenario. To this end, we define a threshold that ensures there is sufficient evidence, with a probability of at least $1-\eta$, to conclude the existence of a linear relationship in the population between the explanatory (feature) and the response (label) variables. Simulations demonstrate the ability of the proposed agnostic (non-parametric) test to provide an analysis of variance similar to the classical multivariate $F$-test for the slope parameter, without relying on the underlying assumptions of classical methods. Moreover, the residuals computed from this method represent a trade-off between those obtained from ML approaches and the classical OLS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15213v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan M Gorriz, J. Ramirez, F. Segovia, F. J. Martinez-Murcia, C. Jim\'enez-Mesa, J. Suckling</dc:creator>
    </item>
  </channel>
</rss>
