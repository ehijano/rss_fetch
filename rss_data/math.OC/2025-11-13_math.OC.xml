<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Nov 2025 02:35:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>An Optimal Solution is Not Enough: Alternative Solutions and Optimal Power Systems</title>
      <link>https://arxiv.org/abs/2511.08805</link>
      <description>arXiv:2511.08805v1 Announce Type: new 
Abstract: Power systems modeling and planning has long leveraged mathematical programming for its ability to provide optimality and feasibility guarantees. One feature that has been recognized in the optimization literature since the 1970s is the existence and meaning of multiple exact optimal and near-optimal solutions, which we call alternative solutions. In power systems modeling, the use of alternative solutions has been limited to energy system optimization modeling (ESOM) applications and modeling to generate alternative (MGA) techniques. We present three key results about alternative solutions for power systems modeling. First, we give a perspective, based on sublevel sets and projection, for characterizing alternative solutions as a facet of general optimization theory. Second, we include pointers to alternative solution generation methods and tools beyond MGA-style techniques. Third, we demonstrate the use cases for alternative solutions in power system modeling on the fundamental optimal power flow problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08805v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew Viens, J. Kyle Skolfield, William E. Hart, Michael Ferris</dc:creator>
    </item>
    <item>
      <title>Continuous-time Constrained Funnel Synthesis for Incrementally Quadratic Nonlinear Systems</title>
      <link>https://arxiv.org/abs/2511.08868</link>
      <description>arXiv:2511.08868v1 Announce Type: new 
Abstract: This paper presents a convex optimization-based framework for synthesizing time-varying controlled invariant funnels and associated feedback control around a given nominal trajectory for nonlinear systems subject to bounded disturbances. Nonlinearities are modeled using incremental quadratic constraints, including Lipschitz, L-smooth, and sector-bounded nonlinearities. Funnel invariance is ensured via a DLMI. Together with pointwise-in-time LMIs for state and input constraints, we formulate a continuous-time funnel synthesis problem. To solve it using numerical optimal control techniques, the DLMI is reformulated into a differential matrix equality (DME) and an LMI, where the DME acts as a funnel dynamics equation. We explore different formulations of these funnel dynamics. Continuous-time constraint satisfaction is addressed through two convex methods: one based on intermediate constraint-checking points, and another using a successive convexification method with subgradients to handle nondifferentiable maximum eigenvalue functions. Theoretical justification is provided for the existence of a measurable and integrable subgradient for the latter. The method is demonstrated on two numerical examples: the control of a unicycle and a 6-degree-of-freedom quadrotor for obstacle avoidance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08868v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Taewan Kim, Dayou Luo, Beh\c{c}et A\c{c}{\i}kme\c{s}e</dc:creator>
    </item>
    <item>
      <title>A 5D concept for space-time optimal control problems with application to simplified Carreau flow</title>
      <link>https://arxiv.org/abs/2511.09086</link>
      <description>arXiv:2511.09086v1 Announce Type: new 
Abstract: This work presents a 5D concept to optimizing non-Newtonian fluid flows through a simplified Carreau flow model. We solve the optimization problem by approximating the solution of the KKT System with fully space-time finite element methods instead of the more traditional time-stepping technique combined with spatial finite element discretization. Therein, the finite element method is formulated in 3D in space, 1D in time, and 1D in the optimization loop, yielding a 5D overall framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09086v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S. Beuchler, B. Endtmayer, U. Langer, A. Schafelner, T. Wick</dc:creator>
    </item>
    <item>
      <title>Deep Signature Approach for McKean-Vlasov FBSDEs in a Random Environment</title>
      <link>https://arxiv.org/abs/2511.09112</link>
      <description>arXiv:2511.09112v1 Announce Type: new 
Abstract: Mean-field games with common noise provide a powerful framework for modeling the collective behavior of large populations subject to shared randomness, such as systemic risk in finance or environmental shocks in economics. These problems can be reformulated as McKean-Vlasov forward-backward stochastic differential equations (MV-FBSDEs) in a random environment, where the coefficients depend on the conditional law of the state given the common noise. Existing numerical methods, however, are largely limited to cases where interactions depend only on expectations or low-order moments, and therefore cannot address the general setting of full distributional dependence.
  In this work, we introduce a deep learning-based algorithm for solving MV-FBSDEs with common noise and general mean-field interactions. Building on fictitious play, our method iteratively solves conditional FBSDEs with fixed distributions, where the conditional law is efficiently represented using signatures, and then updates the distribution through supervised learning. Deep neural networks are employed both to solve the conditional FBSDEs and to approximate the distribution-dependent coefficients, enabling scalability to high-dimensional problems. Under suitable assumptions, we establish convergence in terms of the fictitious play iterations, with error controlled by the supervised learning step. Numerical experiments, including a distribution-dependent mean-field game with common noise, demonstrate the effectiveness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09112v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruimeng Hu, Botao Jin, Mathieu Lauri\`ere, Jiacheng Zhang</dc:creator>
    </item>
    <item>
      <title>New numerical solutions to Newton's problem of least resistance via a convex hull approach</title>
      <link>https://arxiv.org/abs/2511.09177</link>
      <description>arXiv:2511.09177v1 Announce Type: new 
Abstract: We present a numerical method for the solution of Newton's problem of least resistance in the class of convex functions using a convex hull approach. We observe that the numerically computed solutions possess some symmetry. Further, their extremal points lie on several curves. By exploiting this conjectured structure, we are able to compute highly accurate solutions to Newton's problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09177v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Gerd Wachsmuth</dc:creator>
    </item>
    <item>
      <title>Scalable Mixed-Integer Optimization with Neural Constraints via Dual Decomposition</title>
      <link>https://arxiv.org/abs/2511.09186</link>
      <description>arXiv:2511.09186v1 Announce Type: new 
Abstract: Embedding deep neural networks (NNs) into mixed-integer programs (MIPs) is attractive for decision making with learned constraints, yet state-of-the-art monolithic linearisations blow up in size and quickly become intractable. In this paper, we introduce a novel dual-decomposition framework that relaxes the single coupling equality u=x with an augmented Lagrange multiplier and splits the problem into a vanilla MIP and a constrained NN block. Each part is tackled by the solver that suits it best-branch and cut for the MIP subproblem, first-order optimisation for the NN subproblem-so the model remains modular, the number of integer variables never grows with network depth, and the per-iteration cost scales only linearly with the NN size. On the public \textsc{SurrogateLIB} benchmark, our method proves \textbf{scalable}, \textbf{modular}, and \textbf{adaptable}: it runs \(120\times\) faster than an exact Big-M formulation on the largest test case; the NN sub-solver can be swapped from a log-barrier interior step to a projected-gradient routine with no code changes and identical objective value; and swapping the MLP for an LSTM backbone still completes the full optimisation in 47s without any bespoke adaptation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09186v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuli Zeng, Sijia Zhang, Feng Wu, Shaojie Tang, Xiang-Yang Li</dc:creator>
    </item>
    <item>
      <title>Robust Least-Squares Optimization for Data-Driven Predictive Control: A Geometric Approach</title>
      <link>https://arxiv.org/abs/2511.09242</link>
      <description>arXiv:2511.09242v1 Announce Type: new 
Abstract: The paper studies a geometrically robust least-squares problem that extends classical and norm-based robust formulations. Rather than minimizing residual error for fixed or perturbed data, we interpret least-squares as enforcing approximate subspace inclusion between measured and true data spaces. The uncertainty in this geometric relation is modeled as a metric ball on the Grassmannian manifold, leading to a min-max problem over Euclidean and manifold variables. The inner maximization admits a closed-form solution, enabling an efficient algorithm with a transparent geometric interpretation. Applied to robust finite-horizon linear-quadratic tracking in data-enabled predictive control, the method improves upon existing robust least-squares formulations, achieving stronger robustness and favorable scaling under small uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09242v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shreyas Bharadwaj, Bamdev Mishra, Cyrus Mostajeran, Alberto Padoan, Jeremy Coulson, Ravi N. Banavar</dc:creator>
    </item>
    <item>
      <title>Distributionally Robust Joint Planning of Coastal Distribution Network and PV-Storage-EV Stations</title>
      <link>https://arxiv.org/abs/2511.09321</link>
      <description>arXiv:2511.09321v1 Announce Type: new 
Abstract: The rapid integration of renewable energy resources, such as tidal and photovoltaic (PV) power, coupled with the growing deployment of electric vehicle (EV) charging infrastructure, necessitates coordinated planning for coastal urban distribution networks (DN). This paper presents a tri-layer distributionally robust optimization framework to jointly optimize the sitting of PV-storage-EV stations (PSES) and the configuration of coastal DNs, addressing uncertainties related to power load, PV generation, and EV charging demands. At the upper layer, optimal PSES siting and network topology decisions are made to minimize total investment and operational costs. The middle-layer formulation tackles worst case uncertainty scenarios via the optimal power flow model, utilizing ambiguity sets to capture correlated uncertainties. To handle non-convexities introduced by binary variables for energy storage systems, we propose and rigorously prove the exactness of a novel relaxation approach. At the lower layer, considering the dynamic pricing driven by tidal energy fluctuations, operational decisions-including electricity procurement and carbon emissions are optimized. An inexact column-and-constraint generation (i-CCG) algorithm is developed for efficient problem-solving. Numerical results from a realistic 47-node coastal DN in China illustrate that the proposed method effectively reduces costs and ensures robust, low-carbon planning under substantial uncertainties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09321v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenhao Gao, Yongheng Wang, Wei Chen, Xinwei Shen</dc:creator>
    </item>
    <item>
      <title>Experiment design for continuous-time systems using generalized filtering</title>
      <link>https://arxiv.org/abs/2511.09386</link>
      <description>arXiv:2511.09386v1 Announce Type: new 
Abstract: The goal of experiment design is to select the inputs of a dynamical system in such a way that the resulting data contain sufficient information for data-driven modeling and control. This paper investigates the problem of experiment design for continuous-time systems under piecewise constant input signals. To obviate the need for measuring time derivatives of (data) trajectories, we introduce a generalized filtering framework. Our main result is to establish conditions on the input and the filter functions under which the filtered data are informative for system identification, i.e., they satisfy a certain rank condition. The proposed framework encompasses several filter functions that have already appeared in the literature. Building on the proposed filtering framework, we develop an experiment design procedure where the input signal is designed online during system operation. This method is shown to be sample efficient, in the sense that it deals with the least possible number of filtered data samples for system identification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09386v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiwei Wang, Simone Baldi, Henk J. van Waarde</dc:creator>
    </item>
    <item>
      <title>Adversarially and Distributionally Robust Virtual Energy Storage Systems via the Scenario Approach</title>
      <link>https://arxiv.org/abs/2511.09427</link>
      <description>arXiv:2511.09427v1 Announce Type: new 
Abstract: We propose an optimization model where a parking lot manager (PLM) can aggregate parked EV batteries to provide virtual energy storage services that are provably robust under uncertain EV departures and state-of-charge caps. Our formulation yields a data-driven convex optimization problem where a prosumer community agrees on a contract with the PLM for the provision of storage services over a finite horizon. Leveraging recent results in the scenario approach, we certify out-of-sample constraint safety. Furthermore, we enable a tunable profit-risk trade-off through scenario relaxation and extend our model to account for robustness to adversarial perturbations and distributional shifts over Wasserstein-based ambiguity sets. All the approaches are accompanied by tight finite-sample certificates. Numerical studies demonstrate the out-of-sample and out-of-distribution constraint satisfaction of our proposed model compared to the developed theoretical guarantees, showing their effectiveness and potential in robust and efficient virtual energy services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09427v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgios Pantazis, Nicola Mignoni, Raffaele Carli, Mariagrazia Dotoli, Sergio Grammatico</dc:creator>
    </item>
    <item>
      <title>The fastest way through a traffic light</title>
      <link>https://arxiv.org/abs/2511.09530</link>
      <description>arXiv:2511.09530v1 Announce Type: new 
Abstract: We give a rigorous solution of an optimisation problem of minimizing the expected delay caused by encountering a red traffic light on a road journey. The problem incorporates simple constraints on maximum speed, acceleration and braking rates, and depends on the assumed distribution of the remaining time until the traffic light will turn green, after it is first noticed. We assume that this distribution has a bounded and non-increasing density, which is natural since this holds for the law of the excess time in any stationary renewal process. In two special cases, where this distribution is either Uniform or Exponential, we give a complete characterisation of all possible combinations of phases of maximum acceleration, maximum speed, maximum braking, following an Euler--Lagrange curve, and standing stationary at the traffic light, which can make up an optimal solution. The key technique is to write the problem in terms of a two-dimensional pressure integral, so that the problem becomes analogous to filling a tank with a given quantity of liquid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09530v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M\'arton Bal\'azs, Edward Crane, Alexander Tallis</dc:creator>
    </item>
    <item>
      <title>Recursive Binary Identification under Data Tampering and Non-Persistent Excitation with Application to Emission Control</title>
      <link>https://arxiv.org/abs/2511.08629</link>
      <description>arXiv:2511.08629v1 Announce Type: cross 
Abstract: This paper studies the problem of online parameter estimation for cyber-physical systems with binary outputs that may be subject to adversarial data tampering. Existing methods are primarily offline and unsuitable for real-time learning. To address this issue, we first develop a first-order gradient-based algorithm that updates parameter estimates recursively using incoming data. Considering that persistent excitation (PE) conditions are difficult to satisfy in feedback control scenarios, a second-order quasi-Newton algorithm is proposed to achieve faster convergence without requiring the PE condition. For both algorithms, corresponding versions are developed to handle known and unknown tampering strategies, and their parameter estimates are proven to converge almost surely over time. In particular, the second-order algorithm ensures convergence under a signal condition that matches the minimal excitation required by classical least-squares estimation in stochastic regression models. The second-order algorithm is also extended to an adaptive control framework, providing an explicit upper bound on the tracking error for binary-output FIR systems under unknown tampering. Three numerical simulations verify the theoretical results and show that the proposed methods are robust against data tampering. Finally, the approach is validated via a vehicle emission control problem, where it effectively improves the detection accuracy of excess-emission events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08629v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jian Guo, Lihong Pei, Wenchao Xue, Yanlong Zhao, Ji-Feng Zhang</dc:creator>
    </item>
    <item>
      <title>Inverse problems for time-fractional Schr\"odinger equations</title>
      <link>https://arxiv.org/abs/2511.08701</link>
      <description>arXiv:2511.08701v1 Announce Type: cross 
Abstract: We study some inverse problems for time-fractional Schr\"odinger equations involving the Caputo derivative of fractional order $\alpha \in (0,1)$. We prove refined uniqueness results from sets of positive Lebesgue measure for various problems by weakening the regularity of initial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08701v1</guid>
      <category>math.AP</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S. E. Chorfi, F. Et-tahri, L. Maniar, M. Yamamoto</dc:creator>
    </item>
    <item>
      <title>Topological Dynamics via Learned Hybrid Systems</title>
      <link>https://arxiv.org/abs/2511.08737</link>
      <description>arXiv:2511.08737v1 Announce Type: cross 
Abstract: The analysis of global dynamics, particularly the identification and characterization of attractors and their regions of attraction, is essential for complex nonlinear and hybrid systems. Combinatorial methods based on Conley's index theory have provided a rigorous framework for this analysis. However, the computation relies on rigorous outer approximations of the dynamics over a discretized state space, which is challenging to obtain from scattered trajectory data. We propose a methodology that integrates recent advances in switching system identification via convex optimization to bridge this gap between data and topological analysis. We leverage the identified switching system to construct combinatorial outer approximations. This paper outlines the integration of these methods and evaluates the efficacy of computing Morse graphs versus data-driven and statistical approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08737v1</guid>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bernardo Rivas, Kaito Iwasaki, William Kalies, Anthony Bloch, Maani Ghaffari</dc:creator>
    </item>
    <item>
      <title>New perturbation bounds for low rank approximation of matrices via contour analysis</title>
      <link>https://arxiv.org/abs/2511.08875</link>
      <description>arXiv:2511.08875v1 Announce Type: cross 
Abstract: Let $A$ be an $m \times n$ matrix with rank $r$ and spectral decomposition $A = \sum _{i=1}^r \sigma_i u_i v_i^\top, $ where $\sigma_i$ are its singular values, ordered decreasingly, and $u_i, v_i$ are the corresponding left and right singular vectors. For a parameter $1 \le p \le r$, $A_p := \sum_{i=1}^p \sigma_i u_i v_i^\top$ is the best rank $p$ approximation of $A$. In practice, one often chooses $p$ to be small, leading the commonly used phrase "low-rank approximation". Low-rank approximation plays a central role in data science because it can substantially reduce the dimensionality of the original data, the matrix $A$. For a large data matrix $A$, one typically computes a rank-$p$ approximation $A_p$ for a suitably chosen small $p$, stores $A_p$, and uses it as input for further computations. The reduced dimension of $A_p$ enables faster computations and significant data compression. In practice, noise is inevitable. We often have access only to noisy data $\tilde A = A + E$, where $E$ represents the noise. Consequently, the low-rank approximation used as input in many downstream tasks is $\tilde A_p$, the best rank $p$ approximation of $\tilde A$, rather than $A_p$. Therefore, it is natural and important to estimate the error $ \| \tilde A_p - A_p \|$. In this paper, we develop a new method (based on contour analysis) to bound $\| \tilde A_p - A_p \|$. We introduce new parameters that measure the skewness between the noise matrix $E$ and the singular vectors of $A$, and exploit these to obtain notable improvements in many popular settings. This method is of independent interest and has many further applications. We focus on the case where $A$ itself has relatively low rank. This assumption is frequently met in practice, and we think that this case deserves a separate, accessible treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08875v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>math.SP</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Phuc Tran, Van Vu</dc:creator>
    </item>
    <item>
      <title>An ICTM-RMSAV Framework for Bias-Field Aware Image Segmentation under Poisson and Multiplicative Noise</title>
      <link>https://arxiv.org/abs/2511.08988</link>
      <description>arXiv:2511.08988v1 Announce Type: cross 
Abstract: Image segmentation is a core task in image processing, yet many methods degrade when images are heavily corrupted by noise and exhibit intensity inhomogeneity. Within the iterative-convolution thresholding method (ICTM) framework, we propose a variational segmentation model that integrates denoising terms. Specifically, the denoising component consists of an I-divergence term and an adaptive total-variation (TV) regularizer, making the model well suited to images contaminated by Gamma--distributed multiplicative noise and Poisson noise. A spatially adaptive weight derived from a gray-level indicator guides diffusion differently across regions of varying intensity. To further address intensity inhomogeneity, we estimate a smoothly varying bias field, which improves segmentation accuracy. Regions are represented by characteristic functions, with contour length encoded accordingly. For efficient optimization, we couple ICTM with a relaxed modified scalar auxiliary variable (RMSAV) scheme. Extensive experiments on synthetic and real-world images with intensity inhomogeneity and diverse noise types show that the proposed model achieves superior accuracy and robustness compared with competing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08988v1</guid>
      <category>cs.CV</category>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Wang, Wenjun Yao, Fanghui Song, Zhichang Guo</dc:creator>
    </item>
    <item>
      <title>A Quantum Tunneling and Bio-Phototactic Driven Enhanced Dwarf Mongoose Optimizer for UAV Trajectory Planning and Engineering Problem</title>
      <link>https://arxiv.org/abs/2511.09020</link>
      <description>arXiv:2511.09020v1 Announce Type: cross 
Abstract: With the widespread adoption of unmanned aerial vehicles (UAV), effective path planning has become increasingly important. Although traditional search methods have been extensively applied, metaheuristic algorithms have gained popularity due to their efficiency and problem-specific heuristics. However, challenges such as premature convergence and lack of solution diversity still hinder their performance in complex scenarios. To address these issues, this paper proposes an Enhanced Multi-Strategy Dwarf Mongoose Optimization (EDMO) algorithm, tailored for three-dimensional UAV trajectory planning in dynamic and obstacle-rich environments. EDMO integrates three novel strategies: (1) a Dynamic Quantum Tunneling Optimization Strategy (DQTOS) to enable particles to probabilistically escape local optima; (2) a Bio-phototactic Dynamic Focusing Search Strategy (BDFSS) inspired by microbial phototaxis for adaptive local refinement; and (3) an Orthogonal Lens Opposition-Based Learning (OLOBL) strategy to enhance global exploration through structured dimensional recombination. EDMO is benchmarked on 39 standard test functions from CEC2017 and CEC2020, outperforming 14 advanced algorithms in convergence speed, robustness, and optimization accuracy. Furthermore, real-world validations on UAV three-dimensional path planning and three engineering design tasks confirm its practical applicability and effectiveness in field robotics missions requiring intelligent, adaptive, and time-efficient planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09020v1</guid>
      <category>cs.RO</category>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyang Yu, Haorui Yang, Kangning An, Xinjian Wei, Xiaoxuan Xu, Jing Xu</dc:creator>
    </item>
    <item>
      <title>Minimal Regret Walras Equilibria for Combinatorial Markets via Duality, Integrality, and Sensitivity Gaps</title>
      <link>https://arxiv.org/abs/2511.09021</link>
      <description>arXiv:2511.09021v2 Announce Type: cross 
Abstract: We consider combinatorial multi-item markets and propose the notion of a $\Delta$-regret Walras equilibrium, which is an allocation of items to players and a set of item prices that achieve the following goals: prices clear the market, the allocation is capacity-feasible, and the players' strategies lead to a total regret of $\Delta$. The regret is defined as the sum of individual player regrets measured by the utility gap with respect to the optimal item bundle given the prices. We derive necessary and sufficient conditions for the existence of $\Delta$-regret equilibria, where we establish a connection to the duality gap and the integrality gap of the social welfare problem. For the special case of monotone valuations, the derived necessary and sufficient optimality conditions coincide and lead to a complete characterization of achievable $\Delta$-regret equilibria. For general valuations, we establish an interesting connection to the area of sensitivity theory in linear optimization. We show that the sensitivity gap of the optimal-value function of two (configuration) linear programs with changed right-hand side can be used to establish a bound on the achievable regret. Finally, we use these general structural results to translate known approximation algorithms for the social welfare optimization problem into algorithms computing low-regret Walras equilibria. We also demonstrate how to derive strong lower bounds based on integrality and duality gaps but also based on NP-complexity theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09021v2</guid>
      <category>cs.GT</category>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alo\"is Duguet, Tobias Harks, Martin Schmidt, Julian Schwarz</dc:creator>
    </item>
    <item>
      <title>OR-R1: Automating Modeling and Solving of Operations Research Optimization Problem via Test-Time Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2511.09092</link>
      <description>arXiv:2511.09092v1 Announce Type: cross 
Abstract: Optimization modeling and solving are fundamental to the application of Operations Research (OR) in real-world decision making, yet the process of translating natural language problem descriptions into formal models and solver code remains highly expertise intensive. While recent advances in large language models (LLMs) have opened new opportunities for automation, the generalization ability and data efficiency of existing LLM-based methods are still limited, asmost require vast amounts of annotated or synthetic data, resulting in high costs and scalability barriers. In this work, we present OR-R1, a data-efficient training framework for automated optimization modeling and solving. OR-R1 first employs supervised fine-tuning (SFT) to help the model acquire the essential reasoning patterns for problem formulation and code generation from limited labeled data. In addition, it improves the capability and consistency through Test-Time Group Relative Policy Optimization (TGRPO). This two-stage design enables OR-R1 to leverage both scarce labeled and abundant unlabeled data for effective learning. Experiments show that OR-R1 achieves state-of-the-art performance with an average solving accuracy of $67.7\%$, using only $1/10$ the synthetic data required by prior methods such as ORLM, exceeding ORLM's solving accuracy by up to $4.2\%$. Remarkably, OR-R1 outperforms ORLM by over $2.4\%$ with just $100$ synthetic samples. Furthermore, TGRPO contributes an additional $3.1\%-6.4\%$ improvement in accuracy, significantly narrowing the gap between single-attempt (Pass@1) and multi-attempt (Pass@8) performance from $13\%$ to $7\%$. Extensive evaluations across diverse real-world benchmarks demonstrate that OR-R1 provides a robust, scalable, and cost-effective solution for automated OR optimization problem modeling and solving, lowering the expertise and data barriers for industrial OR applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09092v1</guid>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zezhen Ding, Zhen Tan, Jiheng Zhang, Tianlong Chen</dc:creator>
    </item>
    <item>
      <title>Quasi-Newton Compatible Actor-Critic for Deterministic Policies</title>
      <link>https://arxiv.org/abs/2511.09509</link>
      <description>arXiv:2511.09509v1 Announce Type: cross 
Abstract: In this paper, we propose a second-order deterministic actor-critic framework in reinforcement learning that extends the classical deterministic policy gradient method to exploit curvature information of the performance function. Building on the concept of compatible function approximation for the critic, we introduce a quadratic critic that simultaneously preserves the true policy gradient and an approximation of the performance Hessian. A least-squares temporal difference learning scheme is then developed to estimate the quadratic critic parameters efficiently. This construction enables a quasi-Newton actor update using information learned by the critic, yielding faster convergence compared to first-order methods. The proposed approach is general and applicable to any differentiable policy class. Numerical examples demonstrate that the method achieves improved convergence and performance over standard deterministic actor-critic baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09509v1</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arash Bahari Kordabad, Dean Brandner, Sebastien Gros, Sergio Lucia, Sadegh Soudjani</dc:creator>
    </item>
    <item>
      <title>The Nesterov-Spokoiny Acceleration Achieves Strict $o(1/k^2)$ Convergence</title>
      <link>https://arxiv.org/abs/2308.14314</link>
      <description>arXiv:2308.14314v5 Announce Type: replace 
Abstract: This paper studies the Nesterov-Spokoiny Acceleration (NSA), a variant of the accelerated gradient method by Nesterov and Spokoiny. For smooth convex optimization, NSA achieves a strict $o(1/k^2)$ convergence rate in function value and an $o(1/(k^3 \log k))$ rate in squared gradient norm, while ensuring monotonic descent of the objective. We further study a zeroth-order version of NSA that handles inexact gradients, and extends NSA to composite optimization problems, in each case establishing $o(1/k^2)$ convergence in function value. A continuous-time analysis reveals connections to high-resolution ODEs known to underlie acceleration phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.14314v5</guid>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weibin Peng, Yu Liu, Tianyu Wang</dc:creator>
    </item>
    <item>
      <title>Bandit Convex Optimisation</title>
      <link>https://arxiv.org/abs/2402.06535</link>
      <description>arXiv:2402.06535v5 Announce Type: replace 
Abstract: Bandit convex optimisation is a fundamental framework for studying zeroth-order convex optimisation. This book covers the many tools used for this problem, including cutting plane methods, interior point methods, continuous exponential weights, gradient descent and online Newton step. The nuances between the many assumptions and setups are explained. Although there is not much truly new here, some existing tools are applied in novel ways to obtain new algorithms. A few bounds are improved in minor ways.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06535v5</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tor Lattimore</dc:creator>
    </item>
    <item>
      <title>Proximal Oracles for Optimization and Sampling</title>
      <link>https://arxiv.org/abs/2404.02239</link>
      <description>arXiv:2404.02239v3 Announce Type: replace 
Abstract: We consider convex optimization with non-smooth objective function and log-concave sampling with non-smooth potential (negative log density). In particular, we study two specific settings where the convex objective/potential function is either H\"older smooth or in hybrid form as the finite sum of H\"older smooth components. To overcome the challenges caused by non-smoothness, our algorithms employ two powerful proximal frameworks in optimization and sampling: the proximal point framework for optimization and the alternating sampling framework (ASF) that uses Gibbs sampling on an augmented distribution. A key component of both optimization and sampling algorithms is the efficient implementation of the proximal map by the regularized cutting-plane method. We establish its iteration-complexity under both H\"older smoothness and hybrid settings using novel convergence analysis, yielding results that are new to the literature. We further propose an adaptive proximal bundle method for non-smooth optimization that employs an aggressive adaptive stepsize strategy, which adjusts stepsizes only when necessary and never rejects iterates. The proposed method is universal since it does not need any problem parameters as input. Additionally, we provide an exact implementation of a proximal sampling oracle, analogous to the proximal map in optimization, along with simple complexity analyses for both the H\"older smooth and hybrid cases, using a novel technique based on a modified Gaussian integral. Finally, we combine this proximal sampling oracle and ASF to obtain a Markov chain Monte Carlo method with non-asymptotic complexity bounds for sampling in H\"older smooth and hybrid settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02239v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaming Liang, Yongxin Chen</dc:creator>
    </item>
    <item>
      <title>Monotonicity of the jump set and jump amplitudes in one-dimensional TV denoising</title>
      <link>https://arxiv.org/abs/2502.11714</link>
      <description>arXiv:2502.11714v2 Announce Type: replace 
Abstract: We revisit the classical problem of denoising a one-dimensional scalar-valued function by minimizing the sum of an $L^2$ fidelity term and the total variation, scaled by a regularization parameter. This study focuses on proving that the jump set of solutions, corresponding to discontinuities or edges, as well as the amplitude of the jumps are nonincreasing as the regularization parameter increases. Compared with previous works, our results apply to a strictly larger class of input functions, extending beyond the traditional setting of functions of bounded variation to any input in $L^\infty$ with left and right approximate limits everywhere. The proof leverages competitor constructions and convexity properties of the taut string problem, a well-known equivalent formulation of the TV model. This monotonicity property reflects that the extent to which geometric and topological features of the original signal are preserved is consistent with the amount of smoothing desired when formulating the denoising method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11714v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Riccardo Cristoferi, Rita Ferreira, Irene Fonseca, Jos\'e A. Iglesias</dc:creator>
    </item>
    <item>
      <title>Bridging Model Reference Adaptive Control and Data Informativity</title>
      <link>https://arxiv.org/abs/2502.21091</link>
      <description>arXiv:2502.21091v2 Announce Type: replace 
Abstract: The goal of model reference adaptive control (MRAC) is to ensure that the trajectories of an unknown dynamical system track those of a given reference model. This is done by means of a feedback controller that adaptively changes its gains using data collected online from the closed-loop system. One of the approaches to solve the MRAC problem is to impose conditions on the data that guarantee convergence of the gains to a solution of the so-called matching equations. In the literature, various extensions of the concept of persistent excitation have been proposed in an effort to weaken the conditions on the data required for this convergence.Despite these efforts, it is not well-understood what are the weakest possible data requirements ensuring convergence of MRAC. In this paper, we propose a new framework to study the MRAC problem, using the concept of data informativity. Our main contribution is to provide \textit{necessary and sufficient} conditions for the asymptotic convergence of the adaptive gains to a solution of the matching equations. These necessary and sufficient conditions can be readily checked online as new data are generated by the closed-loop system. Our results reveal that existing excitation conditions impose stronger requirements on the collected data than required. Notably, the necessary and sufficient conditions provided in this paper are weaker than those for unique system identification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21091v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiwei Wang, Simone Baldi, Henk J. van Waarde</dc:creator>
    </item>
    <item>
      <title>Multilevel Bregman Proximal Gradient Descent</title>
      <link>https://arxiv.org/abs/2506.03950</link>
      <description>arXiv:2506.03950v3 Announce Type: replace 
Abstract: We present the Multilevel Bregman Proximal Gradient Descent (ML BPGD) method, a novel multilevel optimization framework tailored to constrained convex problems with relative Lipschitz smoothness. Our approach extends the classical multilevel optimization framework (MGOPT) to handle Bregman-based geometries and constrained domains. We provide a rigorous analysis of ML BPGD for multiple coarse levels and establish a global linear convergence rate. We demonstrate the effectiveness of ML BPGD in the context of image reconstruction, providing theoretical guarantees for the well-posedness of the multilevel framework and validating its performance through numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03950v3</guid>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yara Elshiaty, Stefania Petra</dc:creator>
    </item>
    <item>
      <title>Control-affine Schr\"odinger Bridge and Generalized Bohm Potential</title>
      <link>https://arxiv.org/abs/2508.08511</link>
      <description>arXiv:2508.08511v2 Announce Type: replace 
Abstract: The control-affine Schr\"odinger bridge concerns with a stochastic optimal control problem. Its solution is a controlled evolution of joint state probability density subject to a control-affine It\^o diffusion with a given deadline connecting a given pair of initial and terminal densities. In this work, we recast the necessary conditions of optimality for the control-affine Schr\"odinger bridge problem as a two point boundary value problem for a quantum mechanical Schr\"odinger PDE with complex potential. This complex-valued potential is a generalization of the real-valued Bohm potential in quantum mechanics. Our derived potential is akin to the optical potential in nuclear physics where the real part of the potential encodes elastic scattering (transmission of wave function), and the imaginary part encodes inelastic scattering (absorption of wave function). The key takeaway is that the process noise that drives the evolution of probability densities induces an absorbing medium in the evolution of wave function. These results make new connections between control theory and non-equilibrium statistical mechanics through the lens of quantum mechanics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08511v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.PR</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexis M. H. Teter, Abhishek Halder, Michael D. Schneider, Alexx S. Perloff, Jane Pratt, Conor M. Artman, Maria Demireva</dc:creator>
    </item>
    <item>
      <title>Predictive Control Strategies for Sustaining Innovation Adoption on Multilayer Social Networks</title>
      <link>https://arxiv.org/abs/2509.01457</link>
      <description>arXiv:2509.01457v2 Announce Type: replace 
Abstract: This paper studies an optimal control problem for an adoption-opinion model that couples innovation adoption with opinion formation on a multilayer network. Adoption spreads through social contagion and perceived benefits, while opinions evolve via social interactions and feedback from adoption levels. Individuals may abandon adoption due to dissatisfaction or external constraints, potentially hindering diffusion. We analyze system equilibria and their stability, identifying conditions under which adoption persists. We introduce a Model Predictive Control (MPC) strategy that dynamically adapts interventions to the predicted system evolution. Three types of control are compared: shaping opinions, acting on the adoption rate, and reducing dissatisfaction. Overall, MPC interventions outperform static constant control, achieving higher adoption at comparable or lower cost. These results highlight the potential of predictive, adaptive strategies to support sustainable behavior diffusion, offering policymakers scalable tools for effective interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01457v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martina Alutto, Qiulin Xu, Fabrizio Dabbene, Hideaki Ishii, Chiara Ravazzi</dc:creator>
    </item>
    <item>
      <title>On vehicle routing problems with stochastic demands -- Generic integer L-shaped formulations</title>
      <link>https://arxiv.org/abs/2510.04043</link>
      <description>arXiv:2510.04043v2 Announce Type: replace 
Abstract: We study a broad class of vehicle routing problems in which the cost of a route is allowed to be any nonnegative rational value computable in polynomial time in the input size. To address this class, we introduce a unifying framework that generalizes existing integer L-shaped (ILS) formulations developed for vehicle routing problems with stochastic demands (VRPSDs). This framework and subsequent analysis allow us to generalize previous ILS cuts and pinpoint which assumptions are needed to apply those generalizations to other problems. Using these tools, we develop the first algorithm for the VRPSD in the case where the demands are given by an empirical probability distribution of scenarios - a data-driven variant that tackles a significant challenge identified in the literature: dealing with correlations. Indeed, all previous ILS-based exact algorithms for the VRPSD assume either independence of customer demands or correlations through a single external factor. This shows the potential of this generic unifying framework to be applied to a multitude of different variants of the problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04043v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matheus J. Ota, Ricardo Fukasawa</dc:creator>
    </item>
    <item>
      <title>High-order Accumulative Regularization for Gradient Minimization in Convex Programming</title>
      <link>https://arxiv.org/abs/2511.03723</link>
      <description>arXiv:2511.03723v2 Announce Type: replace 
Abstract: This paper develops a unified high-order accumulative regularization (AR) framework for convex and uniformly convex gradient norm minimization. Existing high-order methods often exhibit a gap: the function-value residual decreases fast, while the gradient norm converges much slower. To close this gap, we introduce AR that systematically transforms the fast function-value residual convergence rate into a fast (matching) gradient norm convergence rate.
  Specifically, for composite convex problems, to compute an approximate solution such that the norm of its (sub)gradient does not exceed $\varepsilon,$ the proposed AR methods match the best corresponding convergence rate for the function-value residual. We further extend the framework to uniformly convex settings, establishing linear, superlinear, and sublinear convergence of the gradient norm under different lower curvature conditions. Moreover, we design parameter-free algorithms that require no input of problem parameters, e.g., the Lipschitz constant of the $p$-th-order gradient, the initial optimality gap and the uniform convexity parameter, and allow an inexact solution for each high-order step. To the best of our knowledge, no parameter-free methods can attain such a fast gradient norm convergence rate which matches that of the function-value residual in the convex case, and no such parameter-free methods for uniformly convex problems exist. These results substantially generalize existing parameter-free and inexact high-order methods and recover first-order algorithms as special cases, providing a unified approach for fast gradient minimization across a broad range of smoothness and curvature regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03723v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yao Ji, Guanghui Lan</dc:creator>
    </item>
    <item>
      <title>Arc travel time and path choice model estimation subsumed</title>
      <link>https://arxiv.org/abs/2210.14351</link>
      <description>arXiv:2210.14351v2 Announce Type: replace-cross 
Abstract: We address the problem of simultaneously estimating arc travel times in a network \emph{and} parameters of route choice models for strategic and tactical network planning purposes. Hitherto, these interdependent tasks have been approached separately in the literature on road traffic networks. We illustrate that ignoring this interdependence can lead to erroneous route choice model parameter estimates. We propose a method for maximum likelihood estimation to solve the simultaneous estimation problem that is applicable to any differentiable route choice model. Moreover, our approach allows to naturally mix observations at varying levels of granularity, including noisy or partial path data. Numerical results based on real taxi data from New York City show strong performance of our method, even in comparison to a benchmark method focused solely on arc travel time estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.14351v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sobhan Mohammadpour, Emma Frejinger</dc:creator>
    </item>
    <item>
      <title>Differentiable Extensions with Rounding Guarantees for Combinatorial Optimization over Permutations</title>
      <link>https://arxiv.org/abs/2411.10707</link>
      <description>arXiv:2411.10707v2 Announce Type: replace-cross 
Abstract: Continuously extending combinatorial optimization objectives is a powerful technique commonly applied to the optimization of set functions. However, few such methods exist for extending functions on permutations, despite the fact that many combinatorial optimization problems, such as the quadratic assignment problem (QAP) and the traveling salesperson problem (TSP), are inherently optimization over permutations. We present Birkhoff Extension (BE), an almost-everywhere-differentiable continuous polytime-computable extension of any real-valued function on permutations to doubly stochastic matrices. Key to this construction is our introduction of a continuous variant of the well-known Birkhoff decomposition. Our extension has several nice properties making it appealing for optimization problems. First, BE provides a rounding guarantee, namely any solution to the extension can be efficiently rounded to a permutation without increasing the function value. Furthermore, an approximate solution in the relaxed case will give rise to an approximate solution in the space of permutations. Second, using BE, any real-valued optimization objective on permutations can be extended to an almost-everywhere-differentiable objective function over the space of doubly stochastic matrices. This makes our BE amenable to not only gradient-descent based optimization, but also unsupervised neural combinatorial optimization where training often requires a differentiable loss. Third, based on the above properties, we present a simple optimization procedure which can be readily combined with existing optimization approaches to offer local improvements (i.e., the quality of the final solution is no worse than the initial solution). Finally, we also adapt our extension to optimization problems over a class of trees, such as Steiner tree and optimization-based hierarchical clustering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10707v2</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Robert R. Nerem, Zhishang Luo, Akbar Rafiey, Yusu Wang</dc:creator>
    </item>
    <item>
      <title>Prescribed-Time Newton Extremum Seeking using Delays and Time-Periodic Gains</title>
      <link>https://arxiv.org/abs/2502.05464</link>
      <description>arXiv:2502.05464v3 Announce Type: replace-cross 
Abstract: We study prescribed-time extremum seeking (PT-ES) for scalar maps in the presence of time delays. The PT-ES problem has been studied by Yilmaz and Krstic in 2023 using chirpy probing and time-varying gains that grow unbounded. To alleviate the gain singularity, in this paper we present an alternative approach, employing delays with bounded time-periodic gains, for achieving prescribed-time convergence to the extremum. Our results are not extensions or refinements of earlier works, but a new methodological direction --applicable even when the map has no delay. The main PT-ES algorithm compensates the map's delay and uses perturbation-based and the Newton (rather than gradient) approaches. With the help of averaging theorems in infinite dimension, specifically Retarded Functional Differential Equations (RFDEs), we conduct a prescribed-time convergence analysis on a suitable averaged target ES system, which contains the time-periodic gains of the map and feedback delays. We further extend our method to multivariable static maps and illustrate our results through numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05464v3</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Espitia, Jorge I. Poveda, Miroslav Krstic</dc:creator>
    </item>
    <item>
      <title>Uniform-in-Time Convergence Rates to a Nonlinear Markov Chain for Mean-Field Interacting Jump Processes</title>
      <link>https://arxiv.org/abs/2502.20262</link>
      <description>arXiv:2502.20262v2 Announce Type: replace-cross 
Abstract: We consider a system of $N$ particles interacting through their empirical distribution on a finite state space in continuous time. In the formal limit as $N\to\infty$, the system takes the form of a nonlinear (McKean--Vlasov) Markov chain. This paper rigorously establishes this limit. Specifically, under the assumption that the mean field system has a unique, exponentially stable stationary distribution, we show that the weak error between the empirical measures of the $N$-particle system and the law of the mean field system is of order $1/N$ uniformly in time. Our analysis makes use of a master equation for test functions evaluated along the measure flow of the mean field system, and we demonstrate that the solutions of this master equation are sufficiently regular. We then show that exponential stability of the mean field system is implied by exponential stability for solutions of the linearized Kolmogorov equation with a source term. Finally, we show that our results can be applied to the study of mean field games and give a new condition for the existence of a unique stationary distribution for a nonlinear Markov chain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20262v2</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Asaf Cohen, Ethan Huffman</dc:creator>
    </item>
    <item>
      <title>Integration Matters for Learning PDEs with Backwards SDEs</title>
      <link>https://arxiv.org/abs/2505.01078</link>
      <description>arXiv:2505.01078v2 Announce Type: replace-cross 
Abstract: Backward stochastic differential equation (BSDE)-based deep learning methods provide an alternative to Physics-Informed Neural Networks (PINNs) for solving high-dimensional partial differential equations (PDEs), offering potential algorithmic advantages in settings such as stochastic optimal control, where the PDEs of interest are tied to an underlying dynamical system. However, standard BSDE-based solvers have empirically been shown to underperform relative to PINNs in the literature. In this paper, we identify the root cause of this performance gap as a discretization bias introduced by the standard Euler-Maruyama (EM) integration scheme applied to one-step self-consistency BSDE losses, which shifts the optimization landscape off target. We find that this bias cannot be satisfactorily addressed through finer step-sizes or multi-step self-consistency losses. To properly handle this issue, we propose a Stratonovich-based BSDE formulation, which we implement with stochastic Heun integration. We show that our proposed approach completely eliminates the bias issues faced by EM integration. Furthermore, our empirical results show that our Heun-based BSDE method consistently outperforms EM-based variants and achieves competitive results with PINNs across multiple high-dimensional benchmarks. Our findings highlight the critical role of integration schemes in BSDE-based PDE solvers, an algorithmic detail that has received little attention thus far in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01078v2</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sungje Park, Stephen Tu</dc:creator>
    </item>
    <item>
      <title>The Integrality Gap of the Traveling Salesman Problem is $4/3$ if the LP Solution Has at Most $n+6$ Non-zero Components</title>
      <link>https://arxiv.org/abs/2507.07003</link>
      <description>arXiv:2507.07003v2 Announce Type: replace-cross 
Abstract: We address the classical Dantzig - Fulkerson - Johnson formulation of the symmetric metric Traveling Salesman Problem and study the integrality gap of its linear relaxation, namely the Subtour Elimination Problem (SEP). This integrality gap is conjectured to be 4/3. We prove that, when solving a problem on n nodes, if the optimal SEP solution has at most n + 6 non-zero components, then the conjecture is true. To establish this result, we devise a new methodology that combines theoretical analysis and computational verification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07003v2</guid>
      <category>cs.DM</category>
      <category>math.OC</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tullio Villa, Eleonora Vercesi, Janos Barta, Monaldo Mastrolilli</dc:creator>
    </item>
    <item>
      <title>Non-Asymptotic Optimization and Generalization Bounds for Stochastic Gauss-Newton in Overparameterized Models</title>
      <link>https://arxiv.org/abs/2511.03972</link>
      <description>arXiv:2511.03972v2 Announce Type: replace-cross 
Abstract: An important question in deep learning is how higher-order optimization methods affect generalization. In this work, we analyze a stochastic Gauss-Newton (SGN) method with Levenberg-Marquardt damping and mini-batch sampling for training overparameterized deep neural networks with smooth activations in a regression setting. Our theoretical contributions are twofold. First, we establish finite-time convergence bounds via a variable-metric analysis in parameter space, with explicit dependencies on the batch size, network width and depth. Second, we derive non-asymptotic generalization bounds for SGN using uniform stability in the overparameterized regime, characterizing the impact of curvature, batch size, and overparameterization on generalization performance. Our theoretical results identify a favorable generalization regime for SGN in which a larger minimum eigenvalue of the Gauss-Newton matrix along the optimization path yields tighter stability bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03972v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Semih Cayci</dc:creator>
    </item>
  </channel>
</rss>
