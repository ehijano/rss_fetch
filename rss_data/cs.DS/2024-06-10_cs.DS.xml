<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Jun 2024 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Optimized Deletion From an AVL Tree</title>
      <link>https://arxiv.org/abs/2406.05162</link>
      <description>arXiv:2406.05162v1 Announce Type: new 
Abstract: An AVL tree is a binary search tree that guarantees $ O\left( \log n \right ) $ search. The guarantee is obtained at the cost of rebalancing the AVL tree, potentially after every insertion or deletion. This article proposes a deletion algorithm that reduces rebalancing after deletion by 19 percent compared to previously reported deletion algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05162v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Russell A. Brown</dc:creator>
    </item>
    <item>
      <title>A Simple and Optimal Sublinear Algorithm for Mean Estimation</title>
      <link>https://arxiv.org/abs/2406.05254</link>
      <description>arXiv:2406.05254v1 Announce Type: new 
Abstract: We study the sublinear mean estimation problem. Specifically, we aim to output a point minimizing the sum of squared Euclidean distances. We show that a multiplicative $(1+\varepsilon)$ approximation can be found with probability $1-\delta$ using $O(\varepsilon^{-1}\log \delta^{-1})$ many independent random samples. We also provide a matching lower bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05254v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Beatrice Bertolotti, Matteo Russo, Chris Schwiegelshohn</dc:creator>
    </item>
    <item>
      <title>Optimal Preprocessing for Answering On-Line Product Queries</title>
      <link>https://arxiv.org/abs/2406.06321</link>
      <description>arXiv:2406.06321v1 Announce Type: new 
Abstract: We examine the amount of preprocessing needed for answering certain on-line queries as fast as possible. We start with the following basic problem. Suppose we are given a semigroup $(S,\circ )$. Let $s_1 ,\ldots, s_n$ be elements of $S$. We want to answer on-line queries of the form, ``What is the product $s_i \circ s_{i+1} \circ \cdots \circ s_{j-1} \circ s_j$?'' for any given $1\le i\le j\le n$. We show that a preprocessing of $\Theta(n \lambda (k,n))$ time and space is both necessary and sufficient to answer each such query in at most $k$ steps, for any fixed $k$. The function $\lambda (k,\cdot)$ is the inverse of a certain function at the $\lfloor {k/2}\rfloor$-th level of the primitive recursive hierarchy. In case linear preprocessing is desired, we show that one can answer each such query in $O( \alpha (n))$ steps and that this is best possible. The function $\alpha (n)$ is the inverse Ackermann function.
  We also consider the following extended problem. Let $T$ be a tree with an element of $S$ associated with each of its vertices. We want to answer on-line queries of the form, ``What is the product of the elements associated with the vertices along the path from $u$ to $v$?'' for any pair of vertices $u$ and $v$ in $T$. We derive results that are similar to the above, for the preprocessing needed for answering such queries.
  All our sequential preprocessing algorithms can be parallelized efficiently to give optimal parallel algorithms which run in $O(\log n)$ time on a CREW PRAM. These parallel algorithms are optimal in both running time and total number of operations.
  Our algorithms, especially for the semigroup of the real numbers with the minimum or maximum operations, have various applications in certain graph algorithms, in the utilization of communication networks and in Database retrieval.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06321v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noga Alon, Baruch Schieber</dc:creator>
    </item>
    <item>
      <title>Randomized Binary and Tree Search under Pressure</title>
      <link>https://arxiv.org/abs/2406.06468</link>
      <description>arXiv:2406.06468v1 Announce Type: new 
Abstract: We study a generalized binary search problem on the line and general trees. On the line (e.g., a sorted array), binary search finds a target node in $O(\log n)$ queries in the worst case, where $n$ is the number of nodes. In situations with limited budget or time, we might only be able to perform a few queries, possibly sub-logarithmic many. In this case, it is impossible to guarantee that the target will be found regardless of its position. Our main result is the construction of a randomized strategy that maximizes the minimum (over the target position) probability of finding the target. Such a strategy provides a natural solution where there is no apriori (stochastic) information of the target's position. As with regular binary search, we can find and run the strategy in $O(\log n)$ time (and using only $O(\log n)$ random bits). Our construction is obtained by reinterpreting the problem as a two-player (\textit{seeker} and \textit{hider}) zero-sum game and exploiting an underlying number theoretical structure.
  Furthermore, we generalize the setting to study a search game on trees. In this case, a query returns the edge's endpoint closest to the target. Again, when the number of queries is bounded by some given $k$, we quantify a \emph{the-less-queries-the-better} approach by defining a seeker's profit $p$ depending on the number of queries needed to locate the hider. For the linear programming formulation of the corresponding zero-sum game, we show that computing the best response for the hider (i.e., the separation problem of the underlying dual LP) can be done in time $O(n^2 2^{2k})$, where $n$ is the size of the tree. This result allows to compute a Nash equilibrium in polynomial time whenever $k=O(\log n)$. In contrast, computing the best response for the hider is NP-hard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06468v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agust\'in Caracci (Pontificia Universidad Cat\'olica de Chile, Institute for Mathematical and Computational Engineering), Christoph D\"urr (Sorbonne University, CNRS, LIP6), Jos\'e Verschae (Pontificia Universidad Cat\'olica de Chile, Institute for Mathematical and Computational Engineering)</dc:creator>
    </item>
    <item>
      <title>Improved Mechanisms and Prophet Inequalities for Graphical Dependencies</title>
      <link>https://arxiv.org/abs/2406.05077</link>
      <description>arXiv:2406.05077v1 Announce Type: cross 
Abstract: Over the past two decades, significant strides have been made in stochastic problems such as revenue-optimal auction design and prophet inequalities, traditionally modeled with $n$ independent random variables to represent the values of $n$ items. However, in many applications, this assumption of independence often diverges from reality. Given the strong impossibility results associated with arbitrary correlations, recent research has pivoted towards exploring these problems under models of mild dependency.
  In this work, we study the optimal auction and prophet inequalities problems within the framework of the popular graphical model of Markov Random Fields (MRFs), a choice motivated by its ability to capture complex dependency structures. Specifically, for the problem of selling $n$ items to a single buyer to maximize revenue, we show that the max of SRev and BRev is an $O(\Delta)$-approximation to the optimal revenue for subadditive buyers, where $\Delta$ is the maximum weighted degree of the underlying MRF. This is a generalization as well as an exponential improvement on the $\exp(O(\Delta))$-approximation results of Cai and Oikonomou (EC 2021) for additive and unit-demand buyers. We also obtain a similar exponential improvement for the prophet inequality problem, which is asymptotically optimal as we show a matching upper bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05077v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vasilis Livanos, Kalen Patton, Sahil Singla</dc:creator>
    </item>
    <item>
      <title>The Invertibility of Cellular Automata with Menory: Correcting Errors and New Conclusions</title>
      <link>https://arxiv.org/abs/2406.05642</link>
      <description>arXiv:2406.05642v1 Announce Type: cross 
Abstract: Cellular automata with memory (CAM) are widely used in fields such as image processing, pattern recognition, simulation, and cryptography. The invertibility of CAM is generally considered to be chaotic. Paper [Invertible behavior in elementary cellular automata with memory, Juan C. Seck-Tuoh-Mora et al., Information Sciences, 2012] presented necessary and sufficient conditions for the invertibility of elementary CAM, but it contains a critical error: it classifies identity CAM as non-invertible, whereas identity CAM is undoubtedly invertible. By integrating Amoroso's algorithm and cycle graphs, we provide the correct necessary and sufficient conditions for the invertibility of one-dimensional CAM. Additionally, we link CAM to a specific type of cellular automaton that is isomorphic to CAM, behaves identically, and has easily determinable invertibility. This makes it a promising alternative tool for CAM applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05642v1</guid>
      <category>nlin.CG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Wang, Xiang Deng, Chao Wang</dc:creator>
    </item>
    <item>
      <title>A Little Aggression Goes a Long Way</title>
      <link>https://arxiv.org/abs/2406.05742</link>
      <description>arXiv:2406.05742v1 Announce Type: cross 
Abstract: Aggression is a two-player game of troop placement and attack played on a map (modeled as a graph). Players take turns deploying troops on a territory (a vertex on the graph) until they run out. Once all troops are placed, players take turns attacking enemy territories. A territory can be attacked if it has $k$ troops and there are more than $k$ enemy troops on adjacent territories. At the end of the game, the player who controls the most territories wins. In the case of a tie, the player with more surviving troops wins. The first player to exhaust their troops in the placement phase leads the attack phase.
  We study the complexity of the game when the graph along with an assignment of troops and the sequence of attacks planned by the second player. Even in this restrained setting, we show that the problem of determining an optimal sequence of first player moves is NP-complete. We then analyze the game for when the input graph is a matching or a cycle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05742v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jyothi Krishnan, Neeldhara Misra, Saraswati Girish Nanoti</dc:creator>
    </item>
    <item>
      <title>MAGNOLIA: Matching Algorithms via GNNs for Online Value-to-go Approximation</title>
      <link>https://arxiv.org/abs/2406.05959</link>
      <description>arXiv:2406.05959v1 Announce Type: cross 
Abstract: Online Bayesian bipartite matching is a central problem in digital marketplaces and exchanges, including advertising, crowdsourcing, ridesharing, and kidney exchange. We introduce a graph neural network (GNN) approach that emulates the problem's combinatorially-complex optimal online algorithm, which selects actions (e.g., which nodes to match) by computing each action's value-to-go (VTG) -- the expected weight of the final matching if the algorithm takes that action, then acts optimally in the future. We train a GNN to estimate VTG and show empirically that this GNN returns high-weight matchings across a variety of tasks. Moreover, we identify a common family of graph distributions in spatial crowdsourcing applications, such as rideshare, under which VTG can be efficiently approximated by aggregating information within local neighborhoods in the graphs. This structure matches the local behavior of GNNs, providing theoretical justification for our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05959v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Hayderi, Amin Saberi, Ellen Vitercik, Anders Wikum</dc:creator>
    </item>
    <item>
      <title>The Limits of Interval-Regulated Price Discrimination</title>
      <link>https://arxiv.org/abs/2406.06023</link>
      <description>arXiv:2406.06023v1 Announce Type: cross 
Abstract: In this paper, we study third-degree price discrimination in a model first presented in Bergemann, Brooks, and Morris [2015]. Since such price discrimination might create market segments with vastly different posted prices, we consider regulating these prices, specifically, via restricting them to lie within an interval. Given a price interval, we consider segmentations of the market where a seller, who is oblivious to the existence of such regulation, still posts prices within the price interval. We show the following surprising result: For any market and price interval where such segmentation is feasible, there is always a different segmentation that optimally transfers all excess surplus to the consumers. In addition, we characterize the entire space of buyer and seller surplus that are achievable by such segmentation, including maximizing seller surplus, and simultaneously minimizing buyer and seller surplus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06023v1</guid>
      <category>econ.TH</category>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kamesh Munagala, Yiheng Shen, Renzhe Xu</dc:creator>
    </item>
    <item>
      <title>Testably Learning Polynomial Threshold Functions</title>
      <link>https://arxiv.org/abs/2406.06106</link>
      <description>arXiv:2406.06106v1 Announce Type: cross 
Abstract: Rubinfeld &amp; Vasilyan recently introduced the framework of testable learning as an extension of the classical agnostic model. It relaxes distributional assumptions which are difficult to verify by conditions that can be checked efficiently by a tester. The tester has to accept whenever the data truly satisfies the original assumptions, and the learner has to succeed whenever the tester accepts. We focus on the setting where the tester has to accept standard Gaussian data. There, it is known that basic concept classes such as halfspaces can be learned testably with the same time complexity as in the (distribution-specific) agnostic model. In this work, we ask whether there is a price to pay for testably learning more complex concept classes. In particular, we consider polynomial threshold functions (PTFs), which naturally generalize halfspaces. We show that PTFs of arbitrary constant degree can be testably learned up to excess error $\varepsilon &gt; 0$ in time $n^{\mathrm{poly}(1/\varepsilon)}$. This qualitatively matches the best known guarantees in the agnostic model. Our results build on a connection between testable learning and fooling. In particular, we show that distributions that approximately match at least $\mathrm{poly}(1/\varepsilon)$ moments of the standard Gaussian fool constant-degree PTFs (up to error $\varepsilon$). As a secondary result, we prove that a direct approach to show testable learning (without fooling), which was successfully used for halfspaces, cannot work for PTFs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06106v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Slot, Stefan Tiegel, Manuel Wiedmer</dc:creator>
    </item>
    <item>
      <title>A bijection for the evolution of $B$-trees</title>
      <link>https://arxiv.org/abs/2406.06359</link>
      <description>arXiv:2406.06359v1 Announce Type: cross 
Abstract: A $B$-tree is a type of search tree where every node (except possibly for the root) contains between $m$ and $2m$ keys for some positive integer $m$, and all leaves have the same distance to the root. We study sequences of $B$-trees that can arise from successively inserting keys, and in particular present a bijection between such sequences (which we call histories) and a special type of increasing trees. We describe the set of permutations for the keys that belong to a given history, and also show how to use this bijection to analyse statistics associated with $B$-trees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06359v1</guid>
      <category>math.CO</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Burghart, Stephan Wagner</dc:creator>
    </item>
    <item>
      <title>A Polynomial Decision for 3-SAT</title>
      <link>https://arxiv.org/abs/2208.12598</link>
      <description>arXiv:2208.12598v3 Announce Type: replace 
Abstract: We propose a polynomially bounded, in time and space, method to decide whether a given 3-SAT formula is satisfiable or not. The tools we use here are, in fact, very simple. We first decide satisfiability for a particular 3-SAT formula, called pivoted 3-SAT and, after a plain transformation, still keeping the polynomial boundaries, it is shown that 3-SAT formulas can be written as pivoted formulas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.12598v3</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Angela Weiss</dc:creator>
    </item>
    <item>
      <title>Random Wheeler Automata</title>
      <link>https://arxiv.org/abs/2307.07267</link>
      <description>arXiv:2307.07267v2 Announce Type: replace 
Abstract: Wheeler automata were introduced in 2017 as a tool to generalize existing indexing and compression techniques based on the Burrows-Wheeler transform. Intuitively, an automaton is said to be Wheeler if there exists a total order on its states reflecting the co-lexicographic order of the strings labeling the automaton's paths; this property makes it possible to represent the automaton's topology in a constant number of bits per transition, as well as efficiently solving pattern matching queries on its accepted regular language. After their introduction, Wheeler automata have been the subject of a prolific line of research, both from the algorithmic and language-theoretic points of view. A recurring issue faced in these studies is the lack of large datasets of Wheeler automata on which the developed algorithms and theories could be tested. One possible way to overcome this issue is to generate random Wheeler automata. Motivated by this observation, in this paper we initiate the theoretical study of random Wheeler automata, focusing on the deterministic case (Wheeler DFAs -- WDFAs). We start by extending the Erd\H{o}s-R\'enyi random graph model to WDFAs, and proceed by providing an algorithm generating uniform WDFAs according to this model. Our algorithm generates a uniform WDFA with $n$ states, $m$ transitions, and alphabet's cardinality $\sigma$ in $O(m)$ expected time ($O(m\log m)$ worst-case time w.h.p.) and constant working space for all alphabets of size $\sigma \le m/\ln m$. As a by-product, we also give formulas for the number of distinct WDFAs and obtain that $ n\sigma + (n - \sigma) \log \sigma$ bits are necessary and sufficient to encode a WDFA with $n$ states and alphabet of size $\sigma$, up to an additive $\Theta(n)$ term. We present an implementation of our algorithm and show that it is extremely fast in practice, with a throughput of over 8 million transitions per second.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.07267v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruben Becker, Davide Cenzato, Sung-Hwan Kim, Bojana Kodric, Riccardo Maso, Nicola Prezza</dc:creator>
    </item>
    <item>
      <title>An EPTAS for Cardinality Constrained Multiple Knapsack via Iterative Randomized Rounding</title>
      <link>https://arxiv.org/abs/2308.12622</link>
      <description>arXiv:2308.12622v2 Announce Type: replace 
Abstract: In [Math. Oper. Res., 2011], Fleischer et al. introduced a powerful technique for solving the generic class of separable assignment problems (SAP), in which a set of items of given values and weights needs to be packed into a set of bins subject to separable assignment constraints, so as to maximize the total value. The approach of Fleischer at al. relies on solving a configuration LP and sampling a configuration for each bin independently based on the LP solution. While there is a SAP variant for which this approach yields the best possible approximation ratio, for various special cases, there are discrepancies between the approximation ratios obtained using the above approach and the state-of-the-art approximations. This raises the following natural question: Can we do better by iteratively solving the configuration LP and sampling a few bins at a time?
  To assess the potential gain from iterative randomized rounding, we consider as a case study one interesting SAP variant, namely, Uniform Cardinality Constrained Multiple Knapsack, for which we answer this question affirmatively. The input is a set of items, each has a value and a weight, and a set of uniform capacity bins. The goal is to assign a subset of the items of maximum total value to the bins such that $(i)$ the capacity of any bin is not exceeded, and $(ii)$ the number of items assigned to each bin satisfies a given cardinality constraint. While the technique of Fleischer et al. yields a $\left(1-\frac{1}{e}\right)$-approximation for the problem, we show that iterative randomized rounding leads to an efficient polynomial time approximation scheme (EPTAS), thus essentially resolving the complexity status of the problem. Our analysis of iterative randomized rounding can be useful for solving other SAP variants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12622v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilan Doron-Arad, Ariel Kulik, Hadas Shachnai</dc:creator>
    </item>
    <item>
      <title>UltraLogLog: A Practical and More Space-Efficient Alternative to HyperLogLog for Approximate Distinct Counting</title>
      <link>https://arxiv.org/abs/2308.16862</link>
      <description>arXiv:2308.16862v5 Announce Type: replace 
Abstract: Since its invention HyperLogLog has become the standard algorithm for approximate distinct counting. Due to its space efficiency and suitability for distributed systems, it is widely used and also implemented in numerous databases. This work presents UltraLogLog, which shares the same practical properties as HyperLogLog. It is commutative, idempotent, mergeable, and has a fast guaranteed constant-time insert operation. At the same time, it requires 28% less space to encode the same amount of distinct count information, which can be extracted using the maximum likelihood method. Alternatively, a simpler and faster estimator is proposed, which still achieves a space reduction of 24%, but at an estimation speed comparable to that of HyperLogLog. In a non-distributed setting where martingale estimation can be used, UltraLogLog is able to reduce space by 17%. Moreover, its smaller entropy and its 8-bit registers lead to better compaction when using standard compression algorithms. All this is verified by experimental results that are in perfect agreement with the theoretical analysis which also outlines potential for even more space-efficient data structures. A production-ready Java implementation of UltraLogLog has been released as part of the open-source Hash4j library.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.16862v5</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.14778/3654621.3654632</arxiv:DOI>
      <dc:creator>Otmar Ertl</dc:creator>
    </item>
    <item>
      <title>Efficient Enumeration of Large Maximal k-Plexes</title>
      <link>https://arxiv.org/abs/2402.13008</link>
      <description>arXiv:2402.13008v3 Announce Type: replace 
Abstract: Finding cohesive subgraphs in a large graph has many important applications, such as community detection and biological network analysis. Clique is often a too strict cohesive structure since communities or biological modules rarely form as cliques for various reasons such as data noise. Therefore, $k$-plex is introduced as a popular clique relaxation, which is a graph where every vertex is adjacent to all but at most $k$ vertices. In this paper, we propose a fast branch-and-bound algorithm as well as its task-based parallel version to enumerate all maximal $k$-plexes with at least $q$ vertices. Our algorithm adopts an effective search space partitioning approach that provides a lower time complexity, a new pivot vertex selection method that reduces candidate vertex size, an effective upper-bounding technique to prune useless branches, and three novel pruning techniques by vertex pairs. Our parallel algorithm uses a timeout mechanism to eliminate straggler tasks, and maximizes cache locality while ensuring load balancing. Extensive experiments show that compared with the state-of-the-art algorithms, our sequential and parallel algorithms enumerate large maximal $k$-plexes with up to $5 \times$ and $18.9 \times$ speedup, respectively. Ablation results also demonstrate that our pruning techniques bring up to $7 \times$ speedup compared with our basic algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13008v3</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qihao Cheng, Da Yan, Tianhao Wu, Lyuheng Yuan, Ji Cheng, Zhongyi Huang, Yang Zhou</dc:creator>
    </item>
    <item>
      <title>Practical algorithms for Hierarchical overlap graphs</title>
      <link>https://arxiv.org/abs/2402.13920</link>
      <description>arXiv:2402.13920v2 Announce Type: replace 
Abstract: Genome assembly is a prominent problem studied in bioinformatics, which computes the source string using a set of its overlapping substrings. Classically, genome assembly uses assembly graphs built using this set of substrings to compute the source string efficiently, having a tradeoff between scalability and avoiding information loss. The scalable de Bruijn graphs come at the price of losing crucial overlap information. The complete overlap information is stored in overlap graphs using quadratic space. Hierarchical overlap graphs [IPL20] (HOG) overcome these limitations, avoiding information loss despite using linear space. After a series of suboptimal improvements, Khan and Park et al. simultaneously presented two optimal algorithms [CPM2021], where only the former was seemingly practical.
  We empirically analyze all the practical algorithms for computing HOG on real and random datasets, where the optimal algorithm [CPM2021] outperforms the previous algorithms as expected, though at the expense of extra memory. However, it uses non-intuitive approach and non-trivial data structures. We present arguably the most intuitive algorithm, using only elementary arrays, which is also optimal. Our algorithm empirically proves even better for both time and memory over all the algorithms, highlighting its significance in both theory and practice.
  We further explore the applications of hierarchical overlap graphs to solve various forms of suffix-prefix queries on a set of strings. Loukides et al. [CPM2023] recently presented state-of-the-art algorithms for these queries. However, these algorithms require complex black-box data structures and are seemingly impractical. Our algorithms, despite failing to match the state-of-the-art algorithms theoretically, answer different queries ranging from 0.01-100 milliseconds for a data set having around a billion characters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13920v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saumya Talera, Parth Bansal, Shabnam Khan, Shahbaz Khan</dc:creator>
    </item>
    <item>
      <title>Interval-Constrained Bipartite Matching over Time</title>
      <link>https://arxiv.org/abs/2402.18469</link>
      <description>arXiv:2402.18469v3 Announce Type: replace 
Abstract: Interval-constrained online bipartite matching problem frequently occurs in medical appointment scheduling: unit-time jobs representing patients arrive online and are assigned to a time slot within their given time interval. We consider a variant of this problem where reassignments are allowed and extend it by a notion of current time, which is decoupled from the job arrival events. As jobs appear, the current point in time gradually advances. Jobs that are assigned to the current time unit become processed, which fixes part of the matching and disables these jobs or slots for reassignments in future steps. We refer to these time-dependent restrictions on reassignments as the over-time property.
  We show that FirstFit with reassignments according to the shortest augmenting path rule is $\frac{2}{3}$-competitive with respect to the matching cardinality, and that the bound is tight. Interestingly, this bound holds even if the number of reassignments per job is bound by a constant. For the number of reassignments performed by the algorithm, we show that it is in $\Omega(n \log n)$ in the worst case, where $n$ is the number of patients or jobs on the online side. This result is in line with lower bounds for the number of reassignments in online bipartite matching with reassignments, and, similarly to this previous work, we also conjecture that this bound should be tight. Known upper bounds like the $O(n \log^2 n)$ for online bipartite matching with reassignments by Bernstein, Holm, and Rotenberg do not transfer directly: while our interval constraints simplify the problem, the over-time property restricts the set of possible reassignments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18469v3</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Abels, Mariia Anapolska</dc:creator>
    </item>
    <item>
      <title>Quantum algorithms for spectral sums</title>
      <link>https://arxiv.org/abs/2011.06475</link>
      <description>arXiv:2011.06475v2 Announce Type: replace-cross 
Abstract: We propose new quantum algorithms for estimating spectral sums of positive semi-definite (PSD) matrices. The spectral sum of an PSD matrix $A$, for a function $f$, is defined as $ \text{Tr}[f(A)] = \sum_j f(\lambda_j)$, where $\lambda_j$ are the eigenvalues of $A$. Typical examples of spectral sums are the von Neumann entropy, the trace of $A^{-1}$, the log-determinant, and the Schatten $p$-norm, where the latter does not require the matrix to be PSD. The current best classical randomized algorithms estimating these quantities have a runtime that is at least linearly in the number of nonzero entries of the matrix and quadratic in the estimation error. Assuming access to a block-encoding of a matrix, our algorithms are sub-linear in the matrix size, and depend at most quadratically on other parameters, like the condition number and the approximation error, and thus can compete with most of the randomized and distributed classical algorithms proposed in the literature, and polynomially improve the runtime of other quantum algorithms proposed for the same problems. We show how the algorithms and techniques used in this work can be applied to three problems in spectral graph theory: approximating the number of triangles, the effective resistance, and the number of spanning trees within a graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2011.06475v2</guid>
      <category>quant-ph</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alessandro Luongo, Changpeng Shao</dc:creator>
    </item>
    <item>
      <title>Fast Algorithms for Minimum Homology Basis</title>
      <link>https://arxiv.org/abs/2109.04567</link>
      <description>arXiv:2109.04567v2 Announce Type: replace-cross 
Abstract: We study the problem of finding a minimum homology basis, that is, a lightest set of cycles that generates the $1$-dimensional homology classes with $\mathbb{Z}_2$ coefficients in a given simplicial complex $K$. This problem has been extensively studied in the last few years. For general complexes, the current best deterministic algorithm, by Dey et al., runs in $O(N m^{\omega-1} + n m g)$ time, where $N$ denotes the total number of simplices in $K$, $m$ denotes the number of edges in $K$, $n$ denotes the number of vertices in $K$, $g$ denotes the rank of the $1$-homology group of $K$, and $\omega$ denotes the exponent of matrix multiplication. In this paper, we present three conceptually simple randomized algorithms that compute a minimum homology basis of a general simplicial complex $K$. The first algorithm runs in $\tilde{O}(m^\omega)$ time, the second algorithm runs in $O(N m^{\omega-1})$ time and the third algorithm runs in $\tilde{O}(N^2 g + N m g{^2} + m g{^3})$ time which is nearly quadratic time when $g=O(1)$. We also study the problem of finding a minimum cycle basis in an undirected graph $G$ with $n$ vertices and $m$ edges. The best known algorithm for this problem runs in $O(m^\omega)$ time. Our algorithm, which has a simpler high-level description, but is slightly more expensive, runs in $\tilde{O}(m^\omega)$ time.
  We also provide a practical implementation of computing the minimum homology basis for general weighted complexes. The implementation is broadly based on the algorithmic ideas described in this paper, differing in its use of practical subroutines. Of these subroutines, the more costly step makes use of a parallel implementation, thus potentially addressing the issue of scale. We compare results against the currently known state of the art implementation (ShortLoop).</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.04567v2</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.SoCG.2020.64</arxiv:DOI>
      <dc:creator>Amritendu Dhar, Vijay Natarajan, Abhishek Rathod</dc:creator>
    </item>
    <item>
      <title>Rethink Tree Traversal</title>
      <link>https://arxiv.org/abs/2209.04825</link>
      <description>arXiv:2209.04825v4 Announce Type: replace-cross 
Abstract: We will show how to implement binary decision tree traversal in the language of matrix computation. Our main contribution is to propose some equivalent algorithms of binary tree traversal based on a novel matrix representation of the hierarchical structure of the decision tree. Our key idea is to travel the binary decision tree by maximum inner product search. We not only implement decision tree methods without the recursive traverse but also delve into the partitioning nature of tree-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.04825v4</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jinxiong Zhang</dc:creator>
    </item>
    <item>
      <title>On Computationally Efficient Multi-Class Calibration</title>
      <link>https://arxiv.org/abs/2402.07821</link>
      <description>arXiv:2402.07821v2 Announce Type: replace-cross 
Abstract: Consider a multi-class labelling problem, where the labels can take values in $[k]$, and a predictor predicts a distribution over the labels. In this work, we study the following foundational question: Are there notions of multi-class calibration that give strong guarantees of meaningful predictions and can be achieved in time and sample complexities polynomial in $k$? Prior notions of calibration exhibit a tradeoff between computational efficiency and expressivity: they either suffer from having sample complexity exponential in $k$, or needing to solve computationally intractable problems, or give rather weak guarantees.
  Our main contribution is a notion of calibration that achieves all these desiderata: we formulate a robust notion of projected smooth calibration for multi-class predictions, and give new recalibration algorithms for efficiently calibrating predictors under this definition with complexity polynomial in $k$. Projected smooth calibration gives strong guarantees for all downstream decision makers who want to use the predictor for binary classification problems of the form: does the label belong to a subset $T \subseteq [k]$: e.g. is this an image of an animal? It ensures that the probabilities predicted by summing the probabilities assigned to labels in $T$ are close to some perfectly calibrated binary predictor for that task. We also show that natural strengthenings of our definition are computationally hard to achieve: they run into information theoretic barriers or computational intractability. Underlying both our upper and lower bounds is a tight connection that we prove between multi-class calibration and the well-studied problem of agnostic learning in the (standard) binary prediction setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07821v2</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parikshit Gopalan, Lunjia Hu, Guy N. Rothblum</dc:creator>
    </item>
    <item>
      <title>Amortized Analysis via Coalgebra</title>
      <link>https://arxiv.org/abs/2404.03641</link>
      <description>arXiv:2404.03641v2 Announce Type: replace-cross 
Abstract: Amortized analysis is a cost analysis technique for data structures in which cost is studied in aggregate, rather than considering the maximum cost of a single operation. Traditionally, amortized analysis has been phrased inductively, in terms of finite sequences of operations. Connecting to prior work on coalgebraic semantics for data structures, we develop the perspective that amortized analysis is naturally viewed coalgebraically in the category of algebras for a cost monad, where a morphism of coalgebras serves as a first-class generalization of potential function suitable for integrating cost and behavior. Using this simple definition, we consider amortization of other sample effects, non-commutative printing and randomization. To support imprecise amortized upper bounds, we adapt our discussion to the bicategorical setting, where a potential function is a colax morphism of coalgebras. We support parallel data structure usage patterns by using coalgebras for an endoprofunctor instead of an endofunctor, combining potential using a monoidal structure on the underlying category. Finally, we compose amortization arguments in the indexed category of coalgebras to implement one amortized data structure in terms of others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03641v2</guid>
      <category>cs.PL</category>
      <category>cs.DS</category>
      <category>math.CT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison Grodin, Robert Harper</dc:creator>
    </item>
    <item>
      <title>Active Learning with Simple Questions</title>
      <link>https://arxiv.org/abs/2405.07937</link>
      <description>arXiv:2405.07937v2 Announce Type: replace-cross 
Abstract: We consider an active learning setting where a learner is presented with a pool S of n unlabeled examples belonging to a domain X and asks queries to find the underlying labeling that agrees with a target concept h^* \in H.
  In contrast to traditional active learning that queries a single example for its label, we study more general region queries that allow the learner to pick a subset of the domain T \subset X and a target label y and ask a labeler whether h^*(x) = y for every example in the set T \cap S.
  Such more powerful queries allow us to bypass the limitations of traditional active learning and use significantly fewer rounds of interactions to learn but can potentially lead to a significantly more complex query language. Our main contribution is quantifying the trade-off between the number of queries and the complexity of the query language used by the learner.
  We measure the complexity of the region queries via the VC dimension of the family of regions. We show that given any hypothesis class H with VC dimension d, one can design a region query family Q with VC dimension O(d) such that for every set of n examples S \subset X and every h^* \in H, a learner can submit O(d log n) queries from Q to a labeler and perfectly label S. We show a matching lower bound by designing a hypothesis class H with VC dimension d and a dataset S \subset X of size n such that any learning algorithm using any query class with VC dimension less than O(d) must make poly(n) queries to label S perfectly.
  Finally, we focus on well-studied hypothesis classes including unions of intervals, high-dimensional boxes, and d-dimensional halfspaces, and obtain stronger results. In particular, we design learning algorithms that (i) are computationally efficient and (ii) work even when the queries are not answered based on the learner's pool of examples S but on some unknown superset L of S</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07937v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vasilis Kontonis, Mingchen Ma, Christos Tzamos</dc:creator>
    </item>
  </channel>
</rss>
