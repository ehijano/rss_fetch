<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 Aug 2024 04:01:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 02 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>When Audits and Recounts Distract from Election Integrity: The 2020 U.S. Presidential Election in Georgia</title>
      <link>https://arxiv.org/abs/2408.00055</link>
      <description>arXiv:2408.00055v1 Announce Type: new 
Abstract: Georgia was central to efforts to overturn the 2020 Presidential election, including a call from then-president Trump to Georgia Secretary of State Raffensperger asking Raffensperger to `find' 11,780 votes. Raffensperger has maintained that a `100% full-count risk-limiting audit' and a machine recount agreed with the initial machine-count results, which proved that the reported election results were accurate and that `no votes were flipped.' There is no indication of widespread fraud, but there is reason to distrust the election outcome: the two machine counts and the manual `audit' tallies disagree substantially, even about the number of ballots cast. Some ballots in Fulton County were included in the original count at least twice; some were included in the machine recount at least thrice. Audit results for some tally batches were omitted from the reported audit totals. The two machine counts and the audit were not probative of who won because of poor processes and controls: a lack of secure physical chain of custody, ballot accounting, pollbook reconciliation, and accounting for other election materials such as memory cards. Moreover, most voters voted with demonstrably untrustworthy ballot-marking devices, so even a perfect handcount or audit would not necessarily reveal who really won. True risk-limiting audits (RLAs) and rigorous recounts can limit the risk that an incorrect electoral outcome will be certified rather than being corrected. But no procedure can limit that risk without a trustworthy record of the vote. And even a properly conducted RLA of some contests in an election does not show that any other contests in that election were decided correctly. The 2020 U.S. Presidential election in Georgia illustrates unrecoverable errors that can render recounts and audits `security theater' that distract from the more serious problems rather than justifying trust.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00055v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Philip B. Stark</dc:creator>
    </item>
    <item>
      <title>Spatial Weather, Socio-Economic and Political Risks in Probabilistic Load Forecasting</title>
      <link>https://arxiv.org/abs/2408.00507</link>
      <description>arXiv:2408.00507v1 Announce Type: new 
Abstract: Accurate forecasts of the impact of spatial weather and pan-European socio-economic and political risks on hourly electricity demand for the mid-term horizon are crucial for strategic decision-making amidst the inherent uncertainty. Most importantly, these forecasts are essential for the operational management of power plants, ensuring supply security and grid stability, and in guiding energy trading and investment decisions. The primary challenge for this forecasting task lies in disentangling the multifaceted drivers of load, which include national deterministic (daily, weekly, annual, and holiday patterns) and national stochastic weather and autoregressive effects. Additionally, transnational stochastic socio-economic and political effects add further complexity, in particular, due to their non-stationarity. To address this challenge, we present an interpretable probabilistic mid-term forecasting model for the hourly load that captures, besides all deterministic effects, the various uncertainties in load. This model recognizes transnational dependencies across 24 European countries, with multivariate modeled socio-economic and political states and cross-country dependent forecasting. Built from interpretable Generalized Additive Models (GAMs), the model enables an analysis of the transmission of each incorporated effect to the hour-specific load. Our findings highlight the vulnerability of countries reliant on electric heating under extreme weather scenarios. This emphasizes the need for high-resolution forecasting of weather effects on pan-European electricity consumption especially in anticipation of widespread electric heating adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00507v1</guid>
      <category>stat.AP</category>
      <category>cs.CE</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>q-fin.RM</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Monika Zimmermann, Florian Ziel</dc:creator>
    </item>
    <item>
      <title>Bayesian reliability acceptance sampling plans under adaptive simple step stress partial accelerated life test</title>
      <link>https://arxiv.org/abs/2408.00734</link>
      <description>arXiv:2408.00734v1 Announce Type: new 
Abstract: In the traditional simple step-stress partial accelerated life test (SSSPALT), the items are put on normal operating conditions up to a certain time and after that the stress is increased to get the failure time information early. However, when the stress increases, an additional cost is incorporated that increases the cost of the life test. In this context, an adaptive SSSPALT is considered where the stress is increased after a certain time if the number of failures up to that point is less than a pre-specified number of failures. We consider determination of Bayesian reliability acceptance sampling plans (BSP) through adaptive SSSALT conducted under Type I censoring. The BSP under adaptive SSSPALT is called BSPAA. The Bayes decision function and Bayes risk are obtained for the general loss function. Optimal BSPAAs are obtained for the quadratic loss function by minimizing Bayes risk. An algorithm is provided for computation of optimum BSPAA. Comparisons between the proposed BSPAA and the conventional BSP through non-accelerated life test (CBSP) and conventional BSP through SSSPALT (CBSPA) are carried out.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00734v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rathin Das, Biswabrata Pradhan</dc:creator>
    </item>
    <item>
      <title>Multiway Alignment of Political Attitudes</title>
      <link>https://arxiv.org/abs/2408.00139</link>
      <description>arXiv:2408.00139v1 Announce Type: cross 
Abstract: The related concepts of partisan belief systems, issue alignment, and partisan sorting are central to our understanding of politics. These phenomena have been studied using measures of alignment between pairs of topics, or how much individuals' attitudes toward a topic reveal about their attitudes toward another topic. We introduce a higher-order measure that extends the assessment of alignment beyond pairs of topics by quantifying the amount of information individuals' opinions on one topic reveal about a set of topics simultaneously. Our multiway alignment measure indicates how much individuals' opinions on multiple topics align into a single ideological divide. Applying this approach to legislative voting behavior reveals that parliamentary systems typically exhibit similar multiway alignment characteristics, but can change in response to shifting intergroup dynamics. In American National Election Studies surveys, our approach reveals a growing significance of party identification together with a consistent rise in multiway alignment over time. Similarly, the growing multiway alignment among topical issues in Finnish online discussions suggests a trend towards a more ideologically driven political landscape. Our case studies demonstrate that the multiway alignment measure is a versatile tool for understanding societal polarization and partisan belief systems across diverse domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00139v1</guid>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Letizia Iannucci, Ali Faqeeh, Ali Salloum, Ted Hsuan Yun Chen, Mikko Kivel\"a</dc:creator>
    </item>
    <item>
      <title>Facilitating heterogeneous effect estimation via statistically efficient categorical modifiers</title>
      <link>https://arxiv.org/abs/2408.00618</link>
      <description>arXiv:2408.00618v1 Announce Type: cross 
Abstract: Categorical covariates such as race, sex, or group are ubiquitous in regression analysis. While main-only (or ANCOVA) linear models are predominant, cat-modified linear models that include categorical-continuous or categorical-categorical interactions are increasingly important and allow heterogeneous, group-specific effects. However, with standard approaches, the addition of cat-modifiers fundamentally alters the estimates and interpretations of the main effects, often inflates their standard errors, and introduces significant concerns about group (e.g., racial) biases. We advocate an alternative parametrization and estimation scheme using abundance-based constraints (ABCs). ABCs induce a model parametrization that is both interpretable and equitable. Crucially, we show that with ABCs, the addition of cat-modifiers 1) leaves main effect estimates unchanged and 2) enhances their statistical power, under reasonable conditions. Thus, analysts can, and arguably should include cat-modifiers in linear regression models to discover potential heterogeneous effects--without compromising estimation, inference, and interpretability for the main effects. Using simulated data, we verify these invariance properties for estimation and inference and showcase the capabilities of ABCs to increase statistical power. We apply these tools to study demographic heterogeneities among the effects of social and environmental factors on STEM educational outcomes for children in North Carolina. An R package lmabc is available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00618v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel R. Kowal</dc:creator>
    </item>
    <item>
      <title>Space-time extremes of severe US thunderstorm environments</title>
      <link>https://arxiv.org/abs/2201.05102</link>
      <description>arXiv:2201.05102v3 Announce Type: replace 
Abstract: Severe thunderstorms cause substantial economic and human losses in the United States. Simultaneous high values of convective available potential energy (CAPE) and storm relative helicity (SRH) are favorable to severe weather, and both they and the composite variable $\mathrm{PROD}=\sqrt{\mathrm{CAPE}} \times \mathrm{SRH}$ can be used as indicators of severe thunderstorm activity. Their extremal spatial dependence exhibits temporal non-stationarity due to seasonality and large-scale atmospheric signals such as El Ni\~no-Southern Oscillation (ENSO). In order to investigate this, we introduce a space-time model based on a max-stable, Brown--Resnick, field whose range depends on ENSO and on time through a tensor product spline. We also propose a max-stability test based on empirical likelihood and the bootstrap. The marginal and dependence parameters must be estimated separately owing to the complexity of the model, and we develop a bootstrap-based model selection criterion that accounts for the marginal uncertainty when choosing the dependence model. In the case study, the out-sample performance of our model is good. We find that extremes of PROD, CAPE and SRH are generally more localized in summer and, in some regions, less localized during El Ni\~no and La Ni\~na events, and give meteorological interpretations of these phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.05102v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Koh, Erwan Koch, Anthony C. Davison</dc:creator>
    </item>
    <item>
      <title>Using spatial extreme-value theory with machine learning to model and understand spatially compounding weather extremes</title>
      <link>https://arxiv.org/abs/2401.12195</link>
      <description>arXiv:2401.12195v3 Announce Type: replace 
Abstract: When extreme weather events affect large areas, their regional to sub-continental spatial scale is important for their impacts. We propose a novel machine learning (ML) framework that integrates spatial extreme-value theory to model weather extremes and to quantify probabilities associated with the occurrence, intensity, and spatial extent of these events. Our approach employs new loss functions adapted to extreme values, enabling our model to prioritize the tail rather than the bulk of the data distribution. Applied to a case study of Western European summertime heat extremes, we use daily 500-hPa geopotential height fields and local soil moisture as predictors to capture the complex interplay between local and remote physical processes. Our generative model reveals the importance of individual circulation features in determining different facets of heat extremes, thereby enriching our process understanding from a data-driven perspective. The occurrence, intensity, and spatial extent of heat extremes are sensitive to the relative position of upper-level ridges and troughs that are part of a large-scale wave pattern. Our approach is able to extrapolate beyond the range of the data to make risk-related probabilistic statements. It applies more generally to other weather extremes and offers an attractive alternative to traditional physical and ML-based techniques that focus less on the extremal aspects of weather data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12195v3</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Koh, Daniel Steinfeld, Olivia Martius</dc:creator>
    </item>
    <item>
      <title>Estimating the Number of Street Vendors in New York City</title>
      <link>https://arxiv.org/abs/2406.00527</link>
      <description>arXiv:2406.00527v2 Announce Type: replace 
Abstract: We estimate the number of street vendors in New York City. First, we summarize the process by which vendors receive licenses and permits to operate legally in New York City. Second, we describe a survey that was administered by the Street Vendor Project while distributing Coronavirus relief aid to vendors operating in New York City both with and without a license or permit. Third, we calculate the total number of vendors using ratio estimation, finding approximately 23,000 street vendors operate in New York City (20,500 mobile food vendors and 2,300 general merchandise vendors) with one third located in just six ZIP Codes (11368 (16%), 11372 (3%), and 11354 (3%) in North and West Queens and 10036 (5%), 10019 (4%), and 10001 (3%) in the Chelsea and Clinton neighborhoods of Manhattan). Fourth, we provide a theoretical justification of our estimates based on the theory of point processes. Finally, we evaluate the accuracy of the ratio estimator when the distribution of vendors is explained by a Poisson or Yule process, and we discuss several policy implications. In particular, our estimates suggest the American Community Survey misses the majority of New York City street vendors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00527v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Auerbach</dc:creator>
    </item>
    <item>
      <title>Confounder importance learning for treatment effect inference</title>
      <link>https://arxiv.org/abs/2110.00314</link>
      <description>arXiv:2110.00314v5 Announce Type: replace-cross 
Abstract: We address modelling and computational issues for multiple treatment effect inference under many potential confounders.
  Our main contribution is providing a trade-off between preventing the omission of relevant confounders, while not running into an over-selection of instruments that significantly inflates variance. We propose a novel empirical Bayes framework for Bayesian model averaging that learns from data the extent to which the inclusion of key covariates should be encouraged.
  Our framework sets a prior that asymptotically matches the true amount of confounding in the data, as measured by a novel confounding coefficient. A key challenge is computational. We develop fast algorithms, using an exact gradient of the marginal likelihood that has linear cost in the number of covariates, and a variational counterpart. Our framework uses widely-used ingredients and largely existing software, and it is implemented within the R package mombf. We illustrate our work with two applications. The first is the association between salary variation and discriminatory factors. The second, that has been debated in previous works, is the association between abortion policies and crime. Our approach provides insights that differ from previous analyses especially in situations with weaker treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.00314v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Miquel Torrens-i-Dinar\`es, Omiros Papaspiliopoulos, David Rossell</dc:creator>
    </item>
    <item>
      <title>Enhancing convolutional neural network generalizability via low-rank weight approximation</title>
      <link>https://arxiv.org/abs/2209.12715</link>
      <description>arXiv:2209.12715v2 Announce Type: replace-cross 
Abstract: Noise is ubiquitous during image acquisition. Sufficient denoising is often an important first step for image processing. In recent decades, deep neural networks (DNNs) have been widely used for image denoising. Most DNN-based image denoising methods require a large-scale dataset or focus on supervised settings, in which single/pairs of clean images or a set of noisy images are required. This poses a significant burden on the image acquisition process. Moreover, denoisers trained on datasets of limited scale may incur over-fitting. To mitigate these issues, we introduce a new self-supervised framework for image denoising based on the Tucker low-rank tensor approximation. With the proposed design, we are able to characterize our denoiser with fewer parameters and train it based on a single image, which considerably improves the model's generalizability and reduces the cost of data acquisition. Extensive experiments on both synthetic and real-world noisy images have been conducted. Empirical results show that our proposed method outperforms existing non-learning-based methods (e.g., low-pass filter, non-local mean), single-image unsupervised denoisers (e.g., DIP, NN+BM3D) evaluated on both in-sample and out-sample datasets. The proposed method even achieves comparable performances with some supervised methods (e.g., DnCNN).</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.12715v2</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyin Gao, Shu Yang, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Identified vaccine efficacy for binary post-infection outcomes under misclassification without monotonicity</title>
      <link>https://arxiv.org/abs/2211.16502</link>
      <description>arXiv:2211.16502v4 Announce Type: replace-cross 
Abstract: In order to meet regulatory approval, pharmaceutical companies often must demonstrate that new vaccines reduce the total risk of a post-infection outcome like transmission, symptomatic disease, severe illness, or death in randomized, placebo-controlled trials. Given that infection is a necessary precondition for a post-infection outcome, one can use principal stratification to partition the total causal effect of vaccination into two causal effects: vaccine efficacy against infection, and the principal effect of vaccine efficacy against a post-infection outcome in the patients that would be infected under both placebo and vaccination. Despite the importance of such principal effects to policymakers, these estimands are generally unidentifiable, even under strong assumptions that are rarely satisfied in real-world trials. We develop a novel method to nonparametrically point identify these principal effects while eliminating the monotonicity assumption and allowing for measurement error. Furthermore, our results allow for multiple treatments, and are general enough to be applicable outside of vaccine efficacy. Our method relies on the fact that many vaccine trials are run at geographically disparate health centers, and measure biologically-relevant categorical pretreatment covariates. We show that our method can be applied to a variety of clinical trial settings where vaccine efficacy against infection and a post-infection outcome can be jointly inferred. This can yield new insights from existing vaccine efficacy trial data and will aid researchers in designing new multi-arm clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.16502v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rob Trangucci, Yang Chen, Jon Zelner</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Continuous Multiple Time Point Interventions</title>
      <link>https://arxiv.org/abs/2305.06645</link>
      <description>arXiv:2305.06645v3 Announce Type: replace-cross 
Abstract: There are limited options to estimate the treatment effects of variables which are continuous and measured at multiple time points, particularly if the true dose-response curve should be estimated as closely as possible. However, these situations may be of relevance: in pharmacology, one may be interested in how outcomes of people living with -- and treated for -- HIV, such as viral failure, would vary for time-varying interventions such as different drug concentration trajectories. A challenge for doing causal inference with continuous interventions is that the positivity assumption is typically violated. To address positivity violations, we develop projection functions, which reweigh and redefine the estimand of interest based on functions of the conditional support for the respective interventions. With these functions, we obtain the desired dose-response curve in areas of enough support, and otherwise a meaningful estimand that does not require the positivity assumption. We develop $g$-computation type plug-in estimators for this case. Those are contrasted with g-computation estimators which are applied to continuous interventions without specifically addressing positivity violations, which we propose to be presented with diagnostics. The ideas are illustrated with longitudinal data from HIV positive children treated with an efavirenz-based regimen as part of the CHAPAS-3 trial, which enrolled children $&lt;13$ years in Zambia/Uganda. Simulations show in which situations a standard g-computation approach is appropriate, and in which it leads to bias and how the proposed weighted estimation approach then recovers the alternative estimand of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.06645v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Michael Schomaker, Helen McIlleron, Paolo Denti, Iv\'an D\'iaz</dc:creator>
    </item>
    <item>
      <title>From Counting Stations to City-Wide Estimates: Data-Driven Bicycle Volume Extrapolation</title>
      <link>https://arxiv.org/abs/2406.18454</link>
      <description>arXiv:2406.18454v2 Announce Type: replace-cross 
Abstract: Shifting to cycling in urban areas reduces greenhouse gas emissions and improves public health. Street-level bicycle volume information would aid cities in planning targeted infrastructure improvements to encourage cycling and provide civil society with evidence to advocate for cyclists' needs. Yet, the data currently available to cities and citizens often only comes from sparsely located counting stations. This paper extrapolates bicycle volume beyond these few locations to estimate bicycle volume for the entire city of Berlin. We predict daily and average annual daily street-level bicycle volumes using machine-learning techniques and various public data sources. These include app-based crowdsourced data, infrastructure, bike-sharing, motorized traffic, socioeconomic indicators, weather, and holiday data. Our analysis reveals that the best-performing model is XGBoost, and crowdsourced cycling and infrastructure data are most important for the prediction. We further simulate how collecting short-term counts at predicted locations improves performance. By providing ten days of such sample counts for each predicted location to the model, we are able to halve the error and greatly reduce the variability in performance among predicted locations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18454v2</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Silke K. Kaiser, Nadja Klein, Lynn H. Kaack</dc:creator>
    </item>
  </channel>
</rss>
