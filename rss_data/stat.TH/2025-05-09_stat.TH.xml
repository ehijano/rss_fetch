<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 May 2025 04:01:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Sparse Regularized Optimal Transport without Curse of Dimensionality</title>
      <link>https://arxiv.org/abs/2505.04721</link>
      <description>arXiv:2505.04721v1 Announce Type: new 
Abstract: Entropic optimal transport -- the optimal transport problem regularized by KL diver\-gence -- is highly successful in statistical applications. Thanks to the smoothness of the entropic coupling, its sample complexity avoids the curse of dimensionality suffered by unregularized optimal transport. The flip side of smoothness is overspreading: the entropic coupling always has full support, whereas the unregularized coupling that it approximates is usually sparse, even given by a map. Regularizing optimal transport by less-smooth $f$-divergences such as Tsallis divergence (i.e., $L^p$-regularization) is known to allow for sparse approximations, but is often thought to suffer from the curse of dimensionality as the couplings have limited differentiability and the dual is not strongly concave. We refute this conventional wisdom and show, for a broad family of divergences, that the key empirical quantities converge at the parametric rate, independently of the dimension. More precisely, we provide central limit theorems for the optimal cost, the optimal coupling, and the dual potentials induced by i.i.d.\ samples from the marginals. These results are obtained by a powerful yet elementary approach that is of broader interest for Z-estimation in function classes that are not Donsker.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04721v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Gonz\'alez-Sanz, Stephan Eckstein, Marcel Nutz</dc:creator>
    </item>
    <item>
      <title>The Poisson tensor completion non-parametric differential entropy estimator</title>
      <link>https://arxiv.org/abs/2505.04957</link>
      <description>arXiv:2505.04957v1 Announce Type: new 
Abstract: We introduce the Poisson tensor completion (PTC) estimator, a non-parametric differential entropy estimator. The PTC estimator leverages inter-sample relationships to compute a low-rank Poisson tensor decomposition of the frequency histogram. Our crucial observation is that the histogram bins are an instance of a space partitioning of counts and thus can be identified with a spatial Poisson process. The Poisson tensor decomposition leads to a completion of the intensity measure over all bins -- including those containing few to no samples -- and leads to our proposed PTC differential entropy estimator. A Poisson tensor decomposition models the underlying distribution of the count data and guarantees non-negative estimated values and so can be safely used directly in entropy estimation. We believe our estimator is the first tensor-based estimator that exploits the underlying spatial Poisson process related to the histogram explicitly when estimating the probability density with low-rank tensor decompositions or tensor completion. Furthermore, we demonstrate that our PTC estimator is a substantial improvement over standard histogram-based estimators for sub-Gaussian probability distributions because of the concentration of norm phenomenon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04957v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel M. Dunlavy, Richard B. Lehoucq, Carolyn D. Mayer, Arvind Prasadan</dc:creator>
    </item>
    <item>
      <title>Expectations of some ratio-type estimators under the gamma distribution</title>
      <link>https://arxiv.org/abs/2505.05080</link>
      <description>arXiv:2505.05080v1 Announce Type: new 
Abstract: We study the expectations of some ratio-type estimators under the gamma distribution. Expectations of ratio-type estimators are often difficult to compute due to the nature that they are constructed by combining two separate estimators. With the aid of Lukacs' Theorem and the gamma-beta (gamma-Dirichlet) relationship, we provide alternative proofs for the expected values of some common ratio-type estimators, including the sample Gini index, the sample Theil index, and the sample Atkinson index, under the gamma distribution. Our proofs using the distributional properties of the gamma distribution are much simpler than the existing ones. In addition, we also derive the expected value of the sample variance-to-mean ratio under the gamma distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05080v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jia-Han Shih</dc:creator>
    </item>
    <item>
      <title>Local linear Fr\'echet curve regression in manifolds</title>
      <link>https://arxiv.org/abs/2505.05168</link>
      <description>arXiv:2505.05168v1 Announce Type: new 
Abstract: Global Fr\'echet functional regression has been recently addressed from time correlated bivariate curve data evaluated in a manifold (see Torres et al. 2025). For this type of curve data sets, the present paper solves the problem of local linear approximation of the Fr\'echet conditional mean in an extrinsic and intrinsic way. The extrinsic local linear Fr\'echet functional regression predictor is obtained in the time varying tangent space by projection into an orthornormal basis of the ambient Hilbert space. The conditions assumed ensure the existence and uniqueness of this predictor, and its computation via exponential and logarithmic maps. A weighted Fr\'echet mean approach is adopted in the computation of an intrinsic local linear Fr\'echet functional regression predictor. The asymptotic optimality of this intrinsic local approximation is also proved. The performance of the empirical version of both, extrinsic and intrinsic functional predictors, and of a Nadaraya-Watson type Fr\'echet curve predictor is illustrated in the simulation study undertaken. The finite-sample size properties are also tested in a real-data application via cross-validation. Specifically, functional prediction of the magnetic vector field from the time-varying geocentric latitude and longitude of the satellite NASA's MAGSAT spacecraft is addressed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05168v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. D. Ruiz-Medina, A. Torres--Signes</dc:creator>
    </item>
    <item>
      <title>A Fourier-based inference method for learning interaction kernels in particle systems</title>
      <link>https://arxiv.org/abs/2505.05207</link>
      <description>arXiv:2505.05207v1 Announce Type: new 
Abstract: We consider the problem of inferring the interaction kernel of stochastic interacting particle systems from observations of a single particle. We adopt a semi-parametric approach and represent the interaction kernel in terms of a generalized Fourier series. The basis functions in this expansion are tailored to the problem at hand and are chosen to be orthogonal polynomials with respect to the invariant measure of the mean-field dynamics. The generalized Fourier coefficients are obtained as the solution of an appropriate linear system whose coefficients depend on the moments of the invariant measure, and which are approximated from the particle trajectory that we observe. We quantify the approximation error in the Lebesgue space weighted by the invariant measure and study the asymptotic properties of the estimator in the joint limit as the observation interval and the number of particles tend to infinity, i.e. the joint large time-mean field limit. We also explore the regime where an increasing number of generalized Fourier coefficients is needed to represent the interaction kernel. Our theoretical results are supported by extensive numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05207v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Grigorios A. Pavliotis, Andrea Zanoni</dc:creator>
    </item>
    <item>
      <title>Model Selection for Unit-root Time Series with Many Predictors</title>
      <link>https://arxiv.org/abs/2505.04884</link>
      <description>arXiv:2505.04884v1 Announce Type: cross 
Abstract: This paper studies model selection for general unit-root time series, including the case with many exogenous predictors. We propose FHTD, a new model selection algorithm that leverages forward stepwise regression (FSR), a high-dimensional information criterion (HDIC), a backward elimination method based on HDIC, and a data-driven thresholding (DDT) approach. Under some mild assumptions that allow for unknown locations and multiplicities of the characteristic roots on the unit circle of the time series and conditional heteroscedasticity in the predictors and errors, we establish the sure screening property of FSR and the selection consistency of FHTD. Central to our analysis are two key technical contributions, a new functional central limit theorem for multivariate linear processes and a uniform lower bound for the minimum eigenvalue of the sample covariance matrices, both of which are of independent interest. Simulation results corroborate the theoretical properties and show the superior performance of FHTD in model selection. We showcase the application of the proposed FHTD by modeling U.S. monthly housing starts and unemployment data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04884v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuo-Chieh Huang, Ching-Kang Ing, Ruey S. Tsay</dc:creator>
    </item>
    <item>
      <title>Precise gradient descent training dynamics for finite-width multi-layer neural networks</title>
      <link>https://arxiv.org/abs/2505.04898</link>
      <description>arXiv:2505.04898v1 Announce Type: cross 
Abstract: In this paper, we provide the first precise distributional characterization of gradient descent iterates for general multi-layer neural networks under the canonical single-index regression model, in the `finite-width proportional regime' where the sample size and feature dimension grow proportionally while the network width and depth remain bounded. Our non-asymptotic state evolution theory captures Gaussian fluctuations in first-layer weights and concentration in deeper-layer weights, and remains valid for non-Gaussian features.
  Our theory differs from existing neural tangent kernel (NTK), mean-field (MF) theories and tensor program (TP) in several key aspects. First, our theory operates in the finite-width regime whereas these existing theories are fundamentally infinite-width. Second, our theory allows weights to evolve from individual initializations beyond the lazy training regime, whereas NTK and MF are either frozen at or only weakly sensitive to initialization, and TP relies on special initialization schemes. Third, our theory characterizes both training and generalization errors for general multi-layer neural networks beyond the uniform convergence regime, whereas existing theories study generalization almost exclusively in two-layer settings.
  As a statistical application, we show that vanilla gradient descent can be augmented to yield consistent estimates of the generalization error at each iteration, which can be used to guide early stopping and hyperparameter tuning. As a further theoretical implication, we show that despite model misspecification, the model learned by gradient descent retains the structure of a single-index function with an effective signal determined by a linear combination of the true signal and the initialization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04898v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiyang Han, Masaaki Imaizumi</dc:creator>
    </item>
    <item>
      <title>A unified analysis of likelihood-based estimators in the Plackett--Luce model</title>
      <link>https://arxiv.org/abs/2306.02821</link>
      <description>arXiv:2306.02821v4 Announce Type: replace 
Abstract: The Plackett--Luce model has been extensively used for rank aggregation in social choice theory. A central statistical question in this model concerns estimating the utility vector that governs the model's likelihood. In this paper, we investigate the asymptotic theory of utility vector estimation by maximizing different types of likelihood, such as full, marginal, and quasi-likelihood. Starting from interpreting the estimating equations of these estimators to gain some initial insights, we analyze their asymptotic behavior as the number of compared objects increases. In particular, we establish both uniform consistency and asymptotic normality of these estimators and discuss the trade-off between statistical efficiency and computational complexity. For generality, our results are proven for deterministic graph sequences under appropriate graph topology conditions. These conditions are shown to be informative when applied to common sampling scenarios, such as nonuniform random hypergraph models and hypergraph stochastic block models. Numerical results are provided to support our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.02821v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruijian Han, Yiming Xu</dc:creator>
    </item>
    <item>
      <title>On robustness of Spectral R\'{e}nyi divergence</title>
      <link>https://arxiv.org/abs/2310.06902</link>
      <description>arXiv:2310.06902v3 Announce Type: replace 
Abstract: This paper studies a specific category of statistical divergences for spectral densities of time series: the spectral $\alpha$-R\'{e}nyi divergences, which includes the Itakura--Saito divergence as a subset. The aim of this paper is to highlight both information-theoretic and statistical properties of spectral $\alpha$-R\'{e}nyi divergences. We reveal the connection between the spectral $\alpha$-R\'{e}nyi divergence and the $\gamma$-divergence in robust statistics, and a variational representation of spectral $\alpha$-R\'{e}nyi divergence. Inspired by these results suggesting ``robustness'' of spectral $\alpha$-R\'{e}nyi divergence, we show that the minimum spectral R\'{e}nyi divergence estimate has a stable optimization path with respect to outliers in the frequency domain, unlike the minimum Itakura-Saito divergence estimator, and thus it delivers more stable estimate, reducing the need for intricate pre-processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06902v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tetsuya Takabatake, Keisuke Yano</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bayesian intensity estimation for covariate-driven inhomogeneous point processes</title>
      <link>https://arxiv.org/abs/2312.14073</link>
      <description>arXiv:2312.14073v4 Announce Type: replace 
Abstract: This work studies nonparametric Bayesian estimation of the intensity function of an inhomogeneous Poisson point process in the important case where the intensity depends on covariates, based on the observation of a single realisation of the point pattern over a large area. It is shown how the presence of covariates allows to borrow information from far away locations in the observation window, enabling consistent inference in the growing domain asymptotics. In particular, optimal posterior contraction rates under both global and point-wise loss functions are derived. The rates in global loss are obtained under conditions on the prior distribution resembling those in the well established theory of Bayesian nonparametrics, combined with concentration inequalities for functionals of stationary processes to control certain random covariate-dependent loss functions appearing in the analysis. The local rates are derived with an ad-hoc study that builds on recent advances in the theory of P\'olya tree priors, extended to the present multivariate setting with a novel construction that makes use of the random geometry induced by the covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14073v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Giordano, Alisa Kirichenko, Judith Rousseau</dc:creator>
    </item>
    <item>
      <title>Uncertainty quantification via cross-validation and its variants under algorithmic stability</title>
      <link>https://arxiv.org/abs/2312.14596</link>
      <description>arXiv:2312.14596v2 Announce Type: replace 
Abstract: Recently, there has been substantial interest in statistical guarantees for cross-validation (CV) methods of uncertainty quantification in statistical learning (cf. Barber et al. 2021a, Liang and Barber 2024, Steinberger and Leeb 2023). These guarantees should hold under minimal assumptions on the data generating process and conditional on the training data, because numerous predictions are usually computed based on one and the same training sample. We push this objective to the limit: We prove asymptotic conditional conservativeness of CV, that is, the probability of the actual coverage probability, conditional on the training data, undershooting its nominal level vanishes asymptotically, under minimal assumptions. In particular, we impose a stability condition, require that the prediction error is stochastically bounded, and show that neither condition can be dropped in general. By way of an asymptotic equivalence result, we also show that the closely related CV+ method of Barber et al. (2021a) provides exactly the same conditional statistical guarantees as CV in large samples, thereby extending the range of applicability of CV+ to the high-dimensional regime. We conclude that, in view of its marginal coverage guarantee, CV+ does indeed improve over simple CV. For our proofs we introduce a new concept called L\'evy gauge, which can be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14596v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolai Amann, Hannes Leeb, Lukas Steinberger</dc:creator>
    </item>
    <item>
      <title>Estimation of the long-run variance of nonlinear time series with an application to change point analysis</title>
      <link>https://arxiv.org/abs/2404.02643</link>
      <description>arXiv:2404.02643v2 Announce Type: replace 
Abstract: For a broad class of nonlinear time series known as Bernoulli shifts, we establish the asymptotic normality of the smoothed periodogram estimator of the long-run variance. This estimator uses only a narrow band of Fourier frequencies around the origin and so has been extensively used in local Whittle estimation. Existing asymptotic normality results apply only to linear time series, so our work substantially extends the scope of the applicability of the smoothed periodogram estimator. As an illustration, we apply it to a test of changes in mean against long-range dependence. A simulation study is also conducted to illustrate the performance of the test for nonlinear time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02643v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vaidotas Characiejus, Piotr Kokoszka, Xiangdong Meng</dc:creator>
    </item>
    <item>
      <title>Free Discontinuity Regression: With an Application to the Economic Effects of Internet Shutdowns</title>
      <link>https://arxiv.org/abs/2309.14630</link>
      <description>arXiv:2309.14630v3 Announce Type: replace-cross 
Abstract: Sharp, multidimensional changepoints-abrupt shifts in a regression surface whose locations and magnitudes are unknown-arise in settings as varied as gene-expression profiling, financial covariance breaks, climate-regime detection, and urban socioeconomic mapping. Despite their prevalence, there are no current approaches that jointly estimate the location and size of the discontinuity set in a one-shot approach with statistical guarantees. We therefore introduce Free Discontinuity Regression (FDR), a fully nonparametric estimator that simultaneously (i) smooths a regression surface, (ii) segments it into contiguous regions, and (iii) provably recovers the precise locations and sizes of its jumps. By extending a convex relaxation of the Mumford-Shah functional to random spatial sampling and correlated noise, FDR overcomes the fixed-grid and i.i.d. noise assumptions of classical image-segmentation approaches, thus enabling its application to real-world data of any dimension. This yields the first identification and uniform consistency results for multivariate jump surfaces: under mild SBV regularity, the estimated function, its discontinuity set, and all jump sizes converge to their true population counterparts. Hyperparameters are selected automatically from the data using Stein's Unbiased Risk Estimate, and large-scale simulations up to three dimensions validate the theoretical results and demonstrate good finite-sample performance. Applying FDR to an internet shutdown in India reveals a 25-35% reduction in economic activity around the estimated shutdown boundaries-much larger than previous estimates. By unifying smoothing, segmentation, and effect-size recovery in a general statistical setting, FDR turns free-discontinuity ideas into a practical tool with formal guarantees for modern multivariate data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.14630v3</guid>
      <category>econ.EM</category>
      <category>cs.CV</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Gunsilius, David Van Dijcke</dc:creator>
    </item>
    <item>
      <title>Guaranteed Recovery of Unambiguous Clusters</title>
      <link>https://arxiv.org/abs/2501.13093</link>
      <description>arXiv:2501.13093v3 Announce Type: replace-cross 
Abstract: Clustering is often a challenging problem because of the inherent ambiguity in what the "correct" clustering should be. Even when the number of clusters $K$ is known, this ambiguity often still exists, particularly when there is variation in density among different clusters, and clusters have multiple relatively separated regions of high density. In this paper we propose an information-theoretic characterization of when a $K$-clustering is ambiguous, and design an algorithm that recovers the clustering whenever it is unambiguous. This characterization formalizes the situation when two high density regions within a cluster are separable enough that they look more like two distinct clusters than two truly distinct clusters in the $K$-clustering. The algorithm first identifies $K$ partial clusters (or "seeds") using a density-based approach, and then adds unclustered points to the initial $K$ partial clusters in a greedy manner to form a complete clustering. We implement and test a version of the algorithm that is modified to effectively handle overlapping clusters, and observe that it requires little parameter selection and displays improved performance on many datasets compared to widely used algorithms for non-convex cluster recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13093v3</guid>
      <category>cs.IT</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kayvon Mazooji, Ilan Shomorony</dc:creator>
    </item>
    <item>
      <title>Design-Based Inference under Random Potential Outcomes via Riesz Representation</title>
      <link>https://arxiv.org/abs/2505.01324</link>
      <description>arXiv:2505.01324v3 Announce Type: replace-cross 
Abstract: We introduce a general framework for design-based causal inference that accommodates stochastic potential outcomes, thereby extending the classical Neyman-Rubin setup in which outcomes are treated as fixed. In our formulation, each unit's potential outcome is modelled as a function $\tilde{y}_i(z, \omega)$, where $\omega$ denotes latent randomness external to the treatment assignment. Building on recent work that connects design-based estimation with the Riesz representation theorem, we construct causal estimators by embedding potential outcomes in a Hilbert space and defining treatment effects as linear functionals. This allows us to derive unbiased and consistent estimators, even when potential outcomes exhibit random variation. The framework retains the key advantage of design-based analysis, namely, the use of a known randomisation scheme for identification, while enabling inference in settings with inherent stochasticity. We establish large-sample properties under local dependence, provide a variance estimator compatible with sparse dependency structures, and illustrate the method through a simulation. Our results unify design-based reasoning with random-outcome modelling, broadening the applicability of causal inference in complex experimental environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01324v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yukai Yang</dc:creator>
    </item>
  </channel>
</rss>
