<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Jun 2025 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Bayesian Nonparametric Approach for Semi-Competing Risks with Application to Cardiovascular Health</title>
      <link>https://arxiv.org/abs/2506.20860</link>
      <description>arXiv:2506.20860v1 Announce Type: new 
Abstract: We address causal estimation in semi-competing risks settings, where a non-terminal event may be precluded by one or more terminal events. We define a principal-stratification causal estimand for treatment effects on the non-terminal event, conditional on surviving past a specified landmark time. To estimate joint event-time distributions, we employ both vine-copula constructions and Bayesian nonparametric Enriched Dirichlet-process mixtures (EDPM), enabling inference under minimal parametric assumptions. We index our causal assumptions with sensitivity parameters. Posterior summaries via MCMC yield interpretable estimates with credible intervals. We illustrate the proposed method using data from a cardiovascular health study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20860v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karina Gelis-Cadena, Michael Daniels, Juned Siddique</dc:creator>
    </item>
    <item>
      <title>Disentangling network dependence among multiple variables</title>
      <link>https://arxiv.org/abs/2506.20974</link>
      <description>arXiv:2506.20974v1 Announce Type: new 
Abstract: When two variables depend on the same or similar underlying network, their shared network dependence structure can lead to spurious associations. While statistical associations between two variables sampled from interconnected subjects are a common inferential goal across various fields, little research has focused on how to disentangle shared dependence for valid statistical inference. We revisit two different approaches from distinct fields that may address shared network dependence: the pre-whitening approach, commonly used in time series analysis to remove the shared temporal dependence, and the network autocorrelation model, widely used in network analysis often to examine or account for autocorrelation of the outcome variable. We demonstrate how each approach implicitly entails assumptions about how a variable of interest propagates among nodes via network ties given the network structure. We further propose adaptations of existing pre-whitening methods to the network setting by explicitly reflecting underlying assumptions about "level of interaction" that induce network dependence, while accounting for its unique complexities. Our simulation studies demonstrate the effectiveness of the two approaches in reducing spurious associations due to shared network dependence when their respective assumptions hold. However, the results also show the sensitivity to assumption violations, underscoring the importance of correctly specifying the shared dependence structure based on available network information and prior knowledge about the interactions driving dependence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20974v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhejia Dong, Corwin Zigler, Youjin Lee</dc:creator>
    </item>
    <item>
      <title>Leveraging Relational Evidence: Population Size Estimation on Tree-Structured Data with the Weighted Multiplier Method</title>
      <link>https://arxiv.org/abs/2506.21020</link>
      <description>arXiv:2506.21020v1 Announce Type: new 
Abstract: Populations of interest are often hidden from data for a variety of reasons, though their magnitude remains important in determining resource allocation and appropriate policy. One popular approach to population size estimation, the multiplier method, is a back-calculation tool requiring only a marginal subpopulation size and an estimate of the proportion belonging to this subgroup. Another approach is to use Bayesian methods, which are inherently well-suited to incorporating multiple data sources. However, both methods have their drawbacks. A framework for applying the multiplier method which combines information from several known subpopulations has not yet been established; Bayesian models, though able to incorporate complex dependencies and various data sources, can be difficult for researchers in less technical fields to design and implement. Increasing data collection and linkage across diverse fields suggests accessible methods of estimating population size with synthesized data are needed. We propose an extension to the well-known multiplier method which is applicable to tree-structured data, where multiple subpopulations and corresponding proportions combine to generate a population size estimate via the minimum variance estimator. The methodology and resulting estimates are compared with those from a Bayesian hierarchical model, for both simulated and real world data. Subsequent analysis elucidates which data are key to estimation in each method, and examines robustness and feasibility of this new methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21020v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mallory J Flynn, Paul Gustafson</dc:creator>
    </item>
    <item>
      <title>Simultaneous estimation of the effective reproduction number and the time series of daily infections: Application to Covid-19</title>
      <link>https://arxiv.org/abs/2506.21027</link>
      <description>arXiv:2506.21027v1 Announce Type: new 
Abstract: The time varying effective reproduction number is an important parameter for communication and policy decisions during an epidemic. It is difficult to estimate because it depends on latent variables such as new infections and other characteristics of an epidemic which have to be inferred from available data. In this paper, we present new statistical methods for a popular model which defines the effective reproduction number based on self-exciting dynamics of new infections. Such a model is conceptually simple and less susceptible to misspecifications than more complicated multi-compartment models. In contrast to the state-of-the-art three-step estimation procedure of \citet{huisman2022estimation}, we present a coherent Bayesian method that approximates the joint posterior of daily new infections and reproduction numbers given the data using a novel Markov chain Monte Carlo (MCMC) algorithm. Comparing our method with that of \citet{huisman2022estimation}, both with daily confirmed cases from Switzerland in the Covid-19 epidemic and with simulated data, we find that our method is more accurate, especially near the beginning and end of the observation period.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21027v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hans R. K\"unsch, Fabio Sigrist</dc:creator>
    </item>
    <item>
      <title>Orthogonality conditions for convex regression</title>
      <link>https://arxiv.org/abs/2506.21110</link>
      <description>arXiv:2506.21110v1 Announce Type: new 
Abstract: Econometric identification generally relies on orthogonality conditions, which usually state that the random error term is uncorrelated with the explanatory variables. In convex regression, the orthogonality conditions for identification are unknown. Applying Lagrangian duality theory, we establish the sample orthogonality conditions for convex regression, including additive and multiplicative formulations of the regression model, with and without monotonicity and homogeneity constraints. We then propose a hybrid instrumental variable control function approach to mitigate the impact of potential endogeneity in convex regression. The superiority of the proposed approach is shown in a Monte Carlo study and examined in an empirical application to Chilean manufacturing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21110v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sheng Dai, Timo Kuosmanen, Xun Zhou</dc:creator>
    </item>
    <item>
      <title>Transformer-Based Spatial-Temporal Counterfactual Outcomes Estimation</title>
      <link>https://arxiv.org/abs/2506.21154</link>
      <description>arXiv:2506.21154v1 Announce Type: new 
Abstract: The real world naturally has dimensions of time and space. Therefore, estimating the counterfactual outcomes with spatial-temporal attributes is a crucial problem. However, previous methods are based on classical statistical models, which still have limitations in performance and generalization. This paper proposes a novel framework for estimating counterfactual outcomes with spatial-temporal attributes using the Transformer, exhibiting stronger estimation ability. Under mild assumptions, the proposed estimator within this framework is consistent and asymptotically normal. To validate the effectiveness of our approach, we conduct simulation experiments and real data experiments. Simulation experiments show that our estimator has a stronger estimation capability than baseline methods. Real data experiments provide a valuable conclusion to the causal effect of conflicts on forest loss in Colombia. The source code is available at https://github.com/lihe-maxsize/DeppSTCI_Release_Version-master.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21154v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>He Li, Haoang Chi, Mingyu Liu, Wanrong Huang, Liyang Xu, Wenjing Yang</dc:creator>
    </item>
    <item>
      <title>Evaluating Randomness Assumption: A Novel Graph Theoretic Approach</title>
      <link>https://arxiv.org/abs/2506.21157</link>
      <description>arXiv:2506.21157v1 Announce Type: new 
Abstract: Randomness or mutual independence is a fundamental assumption forming the basis of statistical inference across disciplines such as economics, finance, and management. Consequently, validating this assumption is essential for the reliable application of statistical methods. However, verifying randomness remains a challenge, as existing tests in the literature are often restricted to detecting specific types of data dependencies. In this paper, we propose a novel graph-theoretic approach to testing randomness using random interval graphs (RIGs). The key advantage of RIGs is that their properties are independent of the underlying distribution of the data, relying solely on the assumption of independence between observations. By using two key properties of RIGs-edge probability and vertex degree distribution-we develop two new randomness tests: the RIG-Edge Probability test and the RIG-Degree Distribution (RIG-DD) test. Through extensive simulations, we demonstrate that these tests can detect a broad range of dependencies, including complex phenomena such as conditional heteroskedasticity and chaotic behavior, beyond simple correlations. Furthermore, we show that the RIG-DD test outperforms most of the existing tests of randomness in the literature. We also provide real-world examples to illustrate the practical applicability of these tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21157v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shriya Gehlot, Arnab Kumar Laha</dc:creator>
    </item>
    <item>
      <title>Survival analysis under label shift</title>
      <link>https://arxiv.org/abs/2506.21190</link>
      <description>arXiv:2506.21190v1 Announce Type: new 
Abstract: Let P represent the source population with complete data, containing covariate $\mathbf{Z}$ and response $T$, and Q the target population, where only the covariate $\mathbf{Z}$ is available. We consider a setting with both label shift and label censoring. Label shift assumes that the marginal distribution of $T$ differs between $P$ and $Q$, while the conditional distribution of $\mathbf{Z}$ given $T$ remains the same. Label censoring refers to the case where the response $T$ in $P$ is subject to random censoring. Our goal is to leverage information from the label-shifted and label-censored source population $P$ to conduct statistical inference in the target population $Q$. We propose a parametric model for $T$ given $\mathbf{Z}$ in $Q$ and estimate the model parameters by maximizing an approximate likelihood. This allows for statistical inference in $Q$ and accommodates a range of classical survival models. Under the label shift assumption, the likelihood depends not only on the unknown parameters but also on the unknown distribution of $T$ in $P$ and $\mathbf{Z}$ in $Q$, which we estimate nonparametrically. The asymptotic properties of the estimator are rigorously established and the effectiveness of the method is demonstrated through simulations and a real data application. This work is the first to combine survival analysis with label shift, offering a new research direction in this emerging topic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21190v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxiang Zong, Yanyuan Ma, Ingrid Van Keilegom</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bayesian analysis for the Galton-Watson process</title>
      <link>https://arxiv.org/abs/2506.21304</link>
      <description>arXiv:2506.21304v1 Announce Type: new 
Abstract: The Galton-Watson process is a model for population growth which assumes that individuals reproduce independently according to the same offspring distribution. Inference usually focuses on the offspring average as it allows to classify the process with respect to extinction. We propose a fully non-parametric approach for Bayesian inference on the GW model using a Dirichlet Process prior. The prior naturally generalizes the Dirichlet conjugate prior distribution, and it allows learning the support of the offspring distribution from the data as well as taking into account possible overdispersion of the data. The performance of the proposed approach is compared with both frequentist and Bayesian procedures via simulation. In particular, we show that the use of a DP prior yields good classification performance with both complete and incomplete data. A real-world data example concerning COVID-19 data from Sardinia illustrates the use of the approach in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21304v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Massimo Cannas, Michele Guindani, Nicola Piras</dc:creator>
    </item>
    <item>
      <title>Bayesian Modeling for Aggregated Relational Data: A Unified Perspective</title>
      <link>https://arxiv.org/abs/2506.21353</link>
      <description>arXiv:2506.21353v1 Announce Type: new 
Abstract: Aggregated relational data is widely collected to study social network theory. It has been used to address a variety of key problems in fields such as sociology, public health and economics. ARD models enable researchers to estimate the size of hidden populations, estimate personal network sizes, understand global network structures and fit complex latent variable models to massive network data. Many of the successes of ARD models have been driven by the utilisation of Bayesian modeling, which provides a principled and flexible way to fit and interpret these models for real data. In this work we create a coherent collection of Bayesian implementations of existing models for ARD, within the state of the art Bayesian sampling language, Stan. Our implementations incorporate within-iteration rescaling procedures by default, eliminating the typical post-processing step and improving algorithm run time and convergence diagnostics. Bayesian modelling permits natural tools for model criticism and comparison, which is largely unexplored in the ARD setting. Using synthetic data, we demonstrate how well competing models recover true personal network sizes and subpopulation sizes and how well existing posterior predictive checks compare across a range of Bayesian ARD models. We implement and provide code to leverage Stan's modelling framework for leave-one-out cross-validation, which has not previously been examined for ARD models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21353v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Owen G. Ward, Anna L. Smith, Tian Zheng</dc:creator>
    </item>
    <item>
      <title>Analytic inference with two-way clustering</title>
      <link>https://arxiv.org/abs/2506.20749</link>
      <description>arXiv:2506.20749v1 Announce Type: cross 
Abstract: This paper studies analytic inference along two dimensions of clustering. In such setups, the commonly used approach has two drawbacks. First, the corresponding variance estimator is not necessarily positive. Second, inference is invalid in non-Gaussian regimes, namely when the estimator of the parameter of interest is not asymptotically Gaussian. We consider a simple fix that addresses both issues. In Gaussian regimes, the corresponding tests are asymptotically exact and equivalent to usual ones. Otherwise, the new tests are asymptotically conservative. We also establish their uniform validity over a certain class of data generating processes. Independently of our tests, we highlight potential issues with multiple testing and nonlinear estimators under two-way clustering. Finally, we compare our approach with existing ones through simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20749v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Laurent Davezies, Xavier D'Haultf{\oe}uille, Yannick Guyonvarch</dc:creator>
    </item>
    <item>
      <title>Gaussian Invariant Markov Chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2506.21511</link>
      <description>arXiv:2506.21511v1 Announce Type: cross 
Abstract: We develop sampling methods, which consist of Gaussian invariant versions of random walk Metropolis (RWM), Metropolis adjusted Langevin algorithm (MALA) and second order Hessian or Manifold MALA. Unlike standard RWM and MALA we show that Gaussian invariant sampling can lead to ergodic estimators with improved statistical efficiency. This is due to a remarkable property of Gaussian invariance that allows us to obtain exact analytical solutions to the Poisson equation for Gaussian targets. These solutions can be used to construct efficient and easy to use control variates for variance reduction of estimators under any intractable target. We demonstrate the new samplers and estimators in several examples, including high dimensional targets in latent Gaussian models where we compare against several advanced methods and obtain state-of-the-art results. We also provide theoretical results regarding geometric ergodicity, and an optimal scaling analysis that shows the dependence of the optimal acceptance rate on the Gaussianity of the target.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21511v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michalis K. Titsias, Angelos Alexopoulos, Siran Liu, Petros Dellaportas</dc:creator>
    </item>
    <item>
      <title>L-2 Regularized maximum likelihood for $\beta$-model in large and sparse networks</title>
      <link>https://arxiv.org/abs/2110.11856</link>
      <description>arXiv:2110.11856v5 Announce Type: replace 
Abstract: The $\beta$-model is a powerful tool for modeling large and sparse networks driven by degree heterogeneity, where many network models become infeasible due to computational challenge and network sparsity. However, existing estimation algorithms for $\beta$-model do not scale up. Also, theoretical understandings remain limited to dense networks. This paper brings several significant improvements over existing results to address the urgent needs of practice. We propose a new $\ell_2$-penalized MLE algorithm that can comfortably handle sparse networks of millions of nodes with much-improved memory parsimony. We establish the first rate-optimal error bounds and high-dimensional asymptotic normality results for $\beta$-models, under much weaker network sparsity assumptions than best existing results.
  Application of our method to large COVID-19 network data sets discovered meaningful results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.11856v5</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Meijia Shao, Yu Zhang, Qiuping Wang, Yuan Zhang, Jing Luo, Ting Yan</dc:creator>
    </item>
    <item>
      <title>Club Exco: clustering brain extreme communities from multi-channel EEG data</title>
      <link>https://arxiv.org/abs/2212.04338</link>
      <description>arXiv:2212.04338v2 Announce Type: replace 
Abstract: Current methods for clustering brain networks over time often rely on cross-dependence measures computed from the entire range of EEG signals, which can obscure information specific to extreme neural activity. To overcome this, we introduce Club Exco, a novel clustering method grounded in extreme value theory, designed to detect brain communities with co-occurring high-amplitude EEG events. By focusing on tail behavior, Club Exco isolates extreme-value synchrony across channels, offering new insights into seizure dynamics. We apply Club Exco to neonatal EEG recordings from 30 patients (13 seizure-free and 17 with clinically confirmed seizures). Our method identifies robust ``brain extreme communities'' and constructs Extreme Connectivity Persistence matrices that summarize how often channels exhibit synchronous extremes across time. Seizure patients exhibit more persistent and variable clustering among non-adjacent regions, suggesting seizure propagation, while non-seizure patients show more consistent clustering in anatomically adjacent regions. Compared to coherence-based methods (e.g., Hierarchical Cluster Coherence procedure), Club Exco captures distinct, seizure-associated connectivity patterns, especially in high-amplitude segments. These results highlight Club Exco's potential to characterize extreme neural events and inform clinical understanding of seizure localization and spread.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.04338v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matheus B. Guerrero, Paolo V. Redondo, Marco A. Pinto-Orellana, Beth A. Lopour, Hernando Ombao, Rapha\"el Huser</dc:creator>
    </item>
    <item>
      <title>Cluster-Randomized Trials with Cross-Cluster Interference</title>
      <link>https://arxiv.org/abs/2310.18836</link>
      <description>arXiv:2310.18836v4 Announce Type: replace 
Abstract: The literature on cluster-randomized trials typically allows for interference within but not across clusters. This may be implausible when units are irregularly distributed across space without well-separated communities, as clusters in such cases may not align with significant geographic, social, or economic divisions. This paper develops methods for reducing bias due to cross-cluster interference. We first propose an estimation strategy that excludes units not surrounded by clusters assigned to the same treatment arm. We show that this substantially reduces bias relative to conventional difference-in-means estimators without significant cost to variance. Second, we formally establish a bias-variance trade-off in the choice of clusters: constructing fewer, larger clusters reduces bias due to interference but increases variance. We provide a rule for choosing the number of clusters to balance the asymptotic orders of the bias and variance of our estimator. Finally, we consider unsupervised learning for cluster construction and provide theoretical guarantees for $k$-medoids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18836v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael P. Leung</dc:creator>
    </item>
    <item>
      <title>Tree-based variational inference for Poisson log-normal models</title>
      <link>https://arxiv.org/abs/2406.17361</link>
      <description>arXiv:2406.17361v4 Announce Type: replace 
Abstract: When studying ecosystems, hierarchical trees are often used to organize entities based on proximity criteria, such as the taxonomy in microbiology, social classes in geography, or product types in retail businesses, offering valuable insights into entity relationships. Despite their significance, current count-data models do not leverage this structured information. In particular, the widely used Poisson log-normal (PLN) model, known for its ability to model interactions between entities from count data, lacks the possibility to incorporate such hierarchical tree structures, limiting its applicability in domains characterized by such complexities. To address this matter, we introduce the PLN-Tree model as an extension of the PLN model, specifically designed for modeling hierarchical count data. By integrating structured variational inference techniques, we propose an adapted training procedure and establish identifiability results, enhancing both theoretical foundations and practical interpretability. Experiments on synthetic datasets and human gut microbiome data highlight generative improvements when using PLN-Tree, demonstrating the practical interest of knowledge graphs like the taxonomy in microbiome modeling. Additionally, we present a proof-of-concept implication of the identifiability results by illustrating the practical benefits of using identifiable features for classification tasks, showcasing the versatility of the framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17361v4</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandre Chaussard (LPSM), Anna Bonnet (LPSM), Elisabeth Gassiat (LMO), Sylvain Le Corff (LPSM)</dc:creator>
    </item>
    <item>
      <title>Flexible and Efficient Estimation of Causal Effects with Error-Prone Exposures: A Control Variates Approach for Measurement Error</title>
      <link>https://arxiv.org/abs/2410.12590</link>
      <description>arXiv:2410.12590v2 Announce Type: replace 
Abstract: Exposure measurement error is a ubiquitous but often overlooked challenge in causal inference with observational data. Existing methods accounting for exposure measurement error largely rely on restrictive parametric assumptions, while emerging data-adaptive estimation approaches allow for less restrictive assumptions but at the cost of flexibility, as they are typically tailored towards rigidly-defined statistical quantities. There remains a critical need for assumption-lean estimation methods that are both flexible and possess desirable theoretical properties across a variety of study designs. In this paper, we introduce a general framework for estimation of causal quantities in the presence of exposure measurement error, adapted from the control variates approach of Yang and Ding (2019). Our method can be implemented in various two-phase sampling study designs, where one obtains gold-standard exposure measurements for a small subset of the full study sample, called the validation data. The control variates framework leverages both the error-prone and error-free exposure measurements by augmenting an initial consistent estimator from the validation data with a variance reduction term formed from the full data. We show that our method inherits double-robustness properties under standard causal assumptions. Simulation studies show that our approach performs favorably compared to leading methods under various two-phase sampling schemes. We illustrate our method with observational electronic health record data on HIV outcomes from the Vanderbilt Comprehensive Care Clinic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12590v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keith Barnatchez, Rachel Nethery, Bryan E. Shepherd, Giovanni Parmigiani, Kevin P. Josey</dc:creator>
    </item>
    <item>
      <title>Is checking for sequential positivity violations getting you down? Try sPoRT!</title>
      <link>https://arxiv.org/abs/2412.10245</link>
      <description>arXiv:2412.10245v3 Announce Type: replace 
Abstract: Background: Sequential positivity is often a necessary assumption for drawing causal inferences, such as through marginal structural modeling. Unfortunately, verification of this assumption can be challenging because it usually relies on multiple parametric propensity score models, unlikely all correctly specified. Therefore, we propose a new algorithm, called sequential Positivity Regression Tree (sPoRT), to overcome this issue and identify the subgroups found to be violating this assumption, allowing for insights about the nature of the violations and potential solutions.
  Methods: We present different versions of sPoRT based on either stratifying or pooling over time under static or dynamic treatment strategies. This methodological development was motivated by a real-life application of the impact of the timing of initiation of HIV treatment with and without smoothing over time, which we also use to demonstrate the method.
  Results: The illustration of sPoRT demonstrates its easy use and the interpretability of the results for applied epidemiologists. Furthermore, an R notebook showing how to use sPoRT in practice is available at github.com/ArthurChatton/sPoRT-notebook.
  Conclusions: The sPoRT algorithm provides interpretable subgroups violating the sequential positivity violation, allowing patterns and trends in the confounders to be easily identified. We finally provided practical implications and recommendations when positivity violations are identified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10245v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arthur Chatton, Michael Schomaker, Miguel-Angel Luque-Fernandez, Robert W. Platt, Mireille E. Schnitzer</dc:creator>
    </item>
    <item>
      <title>PRECISE: PRivacy-loss-Efficient and Consistent Inference based on poSterior quantilEs</title>
      <link>https://arxiv.org/abs/2502.00192</link>
      <description>arXiv:2502.00192v2 Announce Type: replace 
Abstract: Differential Privacy (DP) is a mathematical framework for releasing information with formal privacy guarantees. While numerous DP procedures have been developed for statistical analysis and machine learning, valid statistical inference methods offering high utility under DP constraints remain limited. We formalize this gap by introducing the notion of valid Privacy-Preserving Interval Estimation (PPIE) and propose a new PPIE approach -- PRECISE -- to constructing privacy-preserving posterior intervals with the goal of offering a better privacy-utility tradeoff than existing DP inferential methods. PRECISE is a general-purpose and model-agnostic method that generates intervals using quantile estimates obtained from a sanitized posterior histogram with DP guarantees. We explicitly characterize the global sensitivity of the histogram formed from posterior samples for the parameter of interest, enabling its sanitization with formal DP guarantees. We also analyze the sources of error in the mean squared error (MSE) of the histogram-based private quantile estimator and prove its consistency for the true posterior quantiles as the sample size or privacy loss increases with along with its rate of convergence. We conduct extensive experiments to compare the utilities of PRECISE with common existing privacy-preserving inferential approaches across a wide range of inferential tasks, data types and sizes, DP types, and privacy loss levels. The results demonstrated a significant advantage of PRECISE with its nominal coverage and substantially narrower intervals than the existing methods, which are prone to either under-coverage or impractically wide intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00192v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruyu Zhou, Fang Liu</dc:creator>
    </item>
    <item>
      <title>No-prior Bayesian inference reIMagined: probabilistic approximations of inferential models</title>
      <link>https://arxiv.org/abs/2503.19748</link>
      <description>arXiv:2503.19748v2 Announce Type: replace 
Abstract: When prior information is lacking, the go-to strategy for probabilistic inference is to combine a "default prior" and the likelihood via Bayes's theorem. Objective Bayes, (generalized) fiducial inference, etc. fall under this umbrella. This construction is natural, but the corresponding posterior distributions generally only offer limited, approximately valid uncertainty quantification. The present paper takes a reimagined approach offering posterior distributions with stronger reliability properties. The proposed construction starts with an inferential model (IM), one that takes the mathematical form of a data-driven possibility measure and features exactly valid uncertainty quantification, and then returns a so-called inner probabilistic approximation thereof. This inner probabilistic approximation inherits many of the original IM's desirable properties, including credible sets with exact coverage and asymptotic efficiency. The approximation also agrees with the familiar Bayes/fiducial solution obtained in applications where the model has a group transformation structure. A Monte Carlo method for evaluating the probabilistic approximation is presented, along with numerical illustrations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19748v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin</dc:creator>
    </item>
    <item>
      <title>Easily Computed Marginal Likelihoods for Multivariate Mixture Models Using the THAMES Estimator</title>
      <link>https://arxiv.org/abs/2504.21812</link>
      <description>arXiv:2504.21812v2 Announce Type: replace 
Abstract: We present a new version of the truncated harmonic mean estimator (THAMES) for univariate or multivariate mixture models. The estimator computes the marginal likelihood from Markov chain Monte Carlo (MCMC) samples, is consistent, asymptotically normal and of finite variance. In addition, it is invariant to label switching, does not require posterior samples from hidden allocation vectors, and is easily approximated, even for an arbitrarily high number of components. Its computational efficiency is based on an asymptotically optimal ordering of the parameter space, which can in turn be used to provide useful visualisations. We test it in simulation settings where the true marginal likelihood is available analytically. It performs well against state-of-the-art competitors, even in multivariate settings with a high number of components. We demonstrate its utility for inference and model selection on univariate and multivariate data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21812v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Metodiev, Nicholas J. Irons, Marie Perrot-Dock\`es, Pierre Latouche, Adrian E. Raftery</dc:creator>
    </item>
    <item>
      <title>Lower-dimensional posterior density and cluster summaries for overparameterized Bayesian models</title>
      <link>https://arxiv.org/abs/2506.09850</link>
      <description>arXiv:2506.09850v2 Announce Type: replace 
Abstract: The usefulness of Bayesian models for density and cluster estimation is well established across multiple literatures. However, there is still a known tension between the use of simpler, more interpretable models and more flexible, complex ones. In this paper, we propose a novel method that integrates these two approaches by projecting the fit of a flexible, over-parameterized model onto a lower-dimensional parametric summary, which serves as a surrogate. This process increases interpretability while preserving most of the fit of the original model. Our approach involves three main steps. First, we fit the data using nonparametric or over-parameterized models. Second, we project the posterior predictive distribution of the original model onto a sequence of parametric summary estimates using a decision-theoretic approach. Finally, given the lower parametric dimension of the summary estimate that best approximates the original model learned in the second step, we construct uncertainty quantification for the summary by projecting the original full posterior distribution. We demonstrate the effectiveness of our method in summarizing a variety of nonparametric and overparameterized models, providing uncertainty quantification for both density and cluster summaries on synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09850v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Henrique Bolfarine, Hedibert F. Lopes, Carlos M. Carvalho</dc:creator>
    </item>
    <item>
      <title>An introduction to Causal Modelling</title>
      <link>https://arxiv.org/abs/2506.16486</link>
      <description>arXiv:2506.16486v2 Announce Type: replace 
Abstract: This tutorial provides a concise introduction to modern causal modeling by integrating potential outcomes and graphical methods. We motivate causal questions such as counterfactual reasoning under interventions and define binary treatments and potential outcomes. We discuss causal effect measures-including average treatment effects on the treated and on the untreated-and choices of effect scales for binary outcomes. We derive identification in randomized experiments under exchangeability and consistency, and extend to stratification and blocking designs. We present inverse probability weighting with propensity score estimation and robust inference via sandwich estimators. Finally, we introduce causal graphs, d-separation, the backdoor criterion, single-world intervention graphs, and structural equation models, showing how graphical and potential-outcome approaches complement each other. Emphasis is placed on clear notation, intuitive explanations, and practical examples for applied researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16486v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gauranga Kumar Baishya</dc:creator>
    </item>
    <item>
      <title>Likelihood ratio tests in random graph models with increasing dimensions</title>
      <link>https://arxiv.org/abs/2311.05806</link>
      <description>arXiv:2311.05806v3 Announce Type: replace-cross 
Abstract: We explore the Wilks phenomena in two random graph models: the $\beta$-model and the Bradley-Terry model. For two increasing dimensional null hypotheses, including a specified null $H_0: \beta_i=\beta_i^0$ for $i=1,\ldots, r$ and a homogenous null $H_0: \beta_1=\cdots=\beta_r$, we reveal high dimensional Wilks' phenomena that the normalized log-likelihood ratio statistic, $[2\{\ell(\widehat{\mathbf{\beta}}) - \ell(\widehat{\mathbf{\beta}}^0)\} - r]/(2r)^{1/2}$, converges in distribution to the standard normal distribution as $r$ goes to infinity. Here, $\ell( \mathbf{\beta})$ is the log-likelihood function on the model parameter $\mathbf{\beta}=(\beta_1, \ldots, \beta_n)^\top$, $\widehat{\mathbf{\beta}}$ is its maximum likelihood estimator (MLE) under the full parameter space, and $\widehat{\mathbf{\beta}}^0$ is the restricted MLE under the null parameter space. For the homogenous null with a fixed $r$, we establish Wilks-type theorems that $2\{\ell(\widehat{\mathbf{\beta}}) - \ell(\widehat{\mathbf{\beta}}^0)\}$ converges in distribution to a chi-square distribution with $r-1$ degrees of freedom, as the total number of parameters, $n$, goes to infinity. When testing the fixed dimensional specified null, we find that its asymptotic null distribution is a chi-square distribution in the $\beta$-model. However, unexpectedly, this is not true in the Bradley-Terry model. By developing several novel technical methods for asymptotic expansion, we explore Wilks type results in a principled manner; these principled methods should be applicable to a class of random graph models beyond the $\beta$-model and the Bradley-Terry model. Simulation studies and real network data applications further demonstrate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05806v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ting Yan, Yuanzhang Li, Jinfeng Xu, Yaning Yang, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>On the robustness of semi-discrete optimal transport</title>
      <link>https://arxiv.org/abs/2410.19596</link>
      <description>arXiv:2410.19596v2 Announce Type: replace-cross 
Abstract: We derive the breakdown point for solutions of semi-discrete optimal transport problems, which characterizes the robustness of the multivariate quantiles based on optimal transport proposed in \cite{GS}. We do so under very mild assumptions: the absolutely continuous reference measure is only assumed to have a support that is \textcolor{mygreen}{convex}, whereas the target measure is a general discrete measure on a finite number, $n$ say, of atoms. The breakdown point depends on the target measure only through its probability weights (hence not on the location of the atoms) and involves the geometry of the reference measure through the \cite{Tuk1975} concept of halfspace depth. Remarkably, depending on this geometry, the breakdown point of the optimal transport median can be strictly smaller than the breakdown point of the univariate median or the breakdown point of the spatial median, namely~$\lceil n/2\rceil /2$. In the context of robust location estimation, our results provide a subtle insight on how to perform multivariate trimming when constructing trimmed means based on optimal transport.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19596v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davy Paindaveine, Riccardo Passeggeri</dc:creator>
    </item>
    <item>
      <title>Gender disparities in rehospitalisations after coronary artery bypass grafting: evidence from a sparse functional causal mediation analysis of the MIMIC-IV data</title>
      <link>https://arxiv.org/abs/2410.22502</link>
      <description>arXiv:2410.22502v2 Announce Type: replace-cross 
Abstract: Hospital readmissions following coronary artery bypass grafting (CABG) not only impose a substantial cost burden on healthcare systems but also serve as a potential indicator of the quality of medical care. Previous studies of gender effects on complications after CABG surgery have consistently revealed that women tend to suffer worse outcomes. To better understand the causal pathway from gender to the number of rehospitalisations, we study the postoperative central venous pressure (CVP), recorded over the first 24 hours of patients' intensive care unit (ICU) stay after the CABG surgery, as sparse observations of a functional mediator. Confronted with time-varying CVP measurements and zero-inflated rehospitalisation counts within 60 days following discharge, we propose a parameter-simulating quasi-Bayesian Monte Carlo approximation method that accommodates a sparse functional mediator and a zero-inflated count outcome for causal mediation analysis. We find a causal relationship between the female gender and increased rehospitalisation counts after CABG, and that time-varying central venous pressure mediates this causal effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22502v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Henan Xu, Yeying Zhu, Donna L. Coffman</dc:creator>
    </item>
    <item>
      <title>Linear scaling causal discovery from high-dimensional time series by dynamical community detection</title>
      <link>https://arxiv.org/abs/2501.10886</link>
      <description>arXiv:2501.10886v2 Announce Type: replace-cross 
Abstract: Understanding which parts of a dynamical system cause each other is extremely relevant in fundamental and applied sciences. However, inferring causal links from observational data, namely without direct manipulations of the system, is still computationally challenging, especially if the data are high-dimensional. In this study we introduce a framework for constructing causal graphs from high-dimensional time series, whose computational cost scales linearly with the number of variables. The approach is based on the automatic identification of dynamical communities, groups of variables which mutually influence each other and can therefore be described as a single node in a causal graph. These communities are efficiently identified by optimizing the Information Imbalance, a statistical quantity that assigns a weight to each putative causal variable based on its information content relative to a target variable. The communities are then ordered starting from the fully autonomous ones, whose evolution is independent from all the others, to those that are progressively dependent on other communities, building in this manner a community causal graph. We demonstrate the computational efficiency and the accuracy of our approach on time-discrete and time-continuous dynamical systems including up to 80 variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10886v2</guid>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1103/kd73-93cg</arxiv:DOI>
      <dc:creator>Matteo Allione, Vittorio Del Tatto, Alessandro Laio</dc:creator>
    </item>
    <item>
      <title>New Bounds for Sparse Variational Gaussian Processes</title>
      <link>https://arxiv.org/abs/2502.08730</link>
      <description>arXiv:2502.08730v2 Announce Type: replace-cross 
Abstract: Sparse variational Gaussian processes (GPs) construct tractable posterior approximations to GP models. At the core of these methods is the assumption that the true posterior distribution over training function values ${\bf f}$ and inducing variables ${\bf u}$ is approximated by a variational distribution that incorporates the conditional GP prior $p({\bf f} | {\bf u})$ in its factorization. While this assumption is considered as fundamental, we show that for model training we can relax it through the use of a more general variational distribution $q({\bf f} | {\bf u})$ that depends on $N$ extra parameters, where $N$ is the number of training examples. In GP regression, we can analytically optimize the evidence lower bound over the extra parameters and express a tractable collapsed bound that is tighter than the previous bound. The new bound is also amenable to stochastic optimization and its implementation requires minor modifications to existing sparse GP code. Further, we also describe extensions to non-Gaussian likelihoods. On several datasets we demonstrate that our method can reduce bias when learning the hyperparameters and can lead to better predictive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08730v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michalis K. Titsias</dc:creator>
    </item>
  </channel>
</rss>
