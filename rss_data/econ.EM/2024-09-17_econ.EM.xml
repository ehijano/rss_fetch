<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Sep 2024 01:50:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Automatic Pricing and Replenishment Strategies for Vegetable Products Based on Data Analysis and Nonlinear Programming</title>
      <link>https://arxiv.org/abs/2409.09065</link>
      <description>arXiv:2409.09065v1 Announce Type: new 
Abstract: In the field of fresh produce retail, vegetables generally have a relatively limited shelf life, and their quality deteriorates with time. Most vegetable varieties, if not sold on the day of delivery, become difficult to sell the following day. Therefore, retailers usually perform daily quantitative replenishment based on historical sales data and demand conditions. Vegetable pricing typically uses a "cost-plus pricing" method, with retailers often discounting products affected by transportation loss and quality decline. In this context, reliable market demand analysis is crucial as it directly impacts replenishment and pricing decisions. Given the limited retail space, a rational sales mix becomes essential. This paper first uses data analysis and visualization techniques to examine the distribution patterns and interrelationships of vegetable sales quantities by category and individual item, based on provided data on vegetable types, sales records, wholesale prices, and recent loss rates. Next, it constructs a functional relationship between total sales volume and cost-plus pricing for vegetable categories, forecasts future wholesale prices using the ARIMA model, and establishes a sales profit function and constraints. A nonlinear programming model is then developed and solved to provide daily replenishment quantities and pricing strategies for each vegetable category for the upcoming week. Further, we optimize the profit function and constraints based on the actual sales conditions and requirements, providing replenishment quantities and pricing strategies for individual items on July 1 to maximize retail profit. Finally, to better formulate replenishment and pricing decisions for vegetable products, we discuss and forecast the data that retailers need to collect and analyses how the collected data can be applied to the above issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09065v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingpu Ma</dc:creator>
    </item>
    <item>
      <title>Unconditional Randomization Tests for Interference</title>
      <link>https://arxiv.org/abs/2409.09243</link>
      <description>arXiv:2409.09243v1 Announce Type: new 
Abstract: In social networks or spatial experiments, one unit's outcome often depends on another's treatment, a phenomenon called interference. Researchers are interested in not only the presence and magnitude of interference but also its pattern based on factors like distance, neighboring units, and connection strength. However, the non-random nature of these factors and complex correlations across units pose challenges for inference. This paper introduces the partial null randomization tests (PNRT) framework to address these issues. The proposed method is finite-sample valid and applicable with minimal network structure assumptions, utilizing randomization testing and pairwise comparisons. Unlike existing conditional randomization tests, PNRT avoids the need for conditioning events, making it more straightforward to implement. Simulations demonstrate the method's desirable power properties and its applicability to general interference scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09243v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liang Zhong</dc:creator>
    </item>
    <item>
      <title>Structural counterfactual analysis in macroeconomics: theory and inference</title>
      <link>https://arxiv.org/abs/2409.09577</link>
      <description>arXiv:2409.09577v1 Announce Type: new 
Abstract: We propose a structural model-free methodology to analyze two types of macroeconomic counterfactuals related to policy path deviation: hypothetical trajectory and policy intervention. Our model-free approach is built on a structural vector moving-average (SVMA) model that relies solely on the identification of policy shocks, thereby eliminating the need to specify an entire structural model. Analytical solutions are derived for the counterfactual parameters, and statistical inference for these parameter estimates is provided using the Delta method. By utilizing external instruments, we introduce a projection-based method for the identification, estimation, and inference of these parameters. This approach connects our counterfactual analysis with the Local Projection literature. A simulation-based approach with nonlinear model is provided to add in addressing Lucas' critique. The innovative model-free methodology is applied in three counterfactual studies on the U.S. monetary policy: (1) a historical scenario analysis for a hypothetical interest rate path in the post-pandemic era, (2) a future scenario analysis under either hawkish or dovish interest rate policy, and (3) an evaluation of the policy intervention effect of an oil price shock by zeroing out the systematic responses of the interest rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09577v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Endong Wang</dc:creator>
    </item>
    <item>
      <title>A Simple and Adaptive Confidence Interval when Nuisance Parameters Satisfy an Inequality</title>
      <link>https://arxiv.org/abs/2409.09962</link>
      <description>arXiv:2409.09962v1 Announce Type: new 
Abstract: Inequalities may appear in many models. They can be as simple as assuming a parameter is nonnegative, possibly a regression coefficient or a treatment effect. This paper focuses on the case that there is only one inequality and proposes a confidence interval that is particularly attractive, called the inequality-imposed confidence interval (IICI). The IICI is simple. It does not require simulations or tuning parameters. The IICI is adaptive. It reduces to the usual confidence interval (calculated by adding and subtracting the standard error times the $1 - \alpha/2$ standard normal quantile) when the inequality is sufficiently slack. When the inequality is sufficiently violated, the IICI reduces to an equality-imposed confidence interval (the usual confidence interval for the submodel where the inequality holds with equality). Also, the IICI is uniformly valid and has (weakly) shorter length than the usual confidence interval; it is never longer. The first empirical application considers a linear regression when a coefficient is known to be nonpositive. A second empirical application considers an instrumental variables regression when the endogeneity of a regressor is known to be nonnegative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09962v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregory Fletcher Cox</dc:creator>
    </item>
    <item>
      <title>Estimating Wage Disparities Using Foundation Models</title>
      <link>https://arxiv.org/abs/2409.09894</link>
      <description>arXiv:2409.09894v1 Announce Type: cross 
Abstract: One thread of empirical work in social science focuses on decomposing group differences in outcomes into unexplained components and components explained by observable factors. In this paper, we study gender wage decompositions, which require estimating the portion of the gender wage gap explained by career histories of workers. Classical methods for decomposing the wage gap employ simple predictive models of wages which condition on a small set of simple summaries of labor history. The problem is that these predictive models cannot take advantage of the full complexity of a worker's history, and the resulting decompositions thus suffer from omitted variable bias (OVB), where covariates that are correlated with both gender and wages are not included in the model. Here we explore an alternative methodology for wage gap decomposition that employs powerful foundation models, such as large language models, as the predictive engine. Foundation models excel at making accurate predictions from complex, high-dimensional inputs. We use a custom-built foundation model, designed to predict wages from full labor histories, to decompose the gender wage gap. We prove that the way such models are usually trained might still lead to OVB, but develop fine-tuning algorithms that empirically mitigate this issue. Our model captures a richer representation of career history than simple models and predicts wages more accurately. In detail, we first provide a novel set of conditions under which an estimator of the wage gap based on a fine-tuned foundation model is $\sqrt{n}$-consistent. Building on the theory, we then propose methods for fine-tuning foundation models that minimize OVB. Using data from the Panel Study of Income Dynamics, we find that history explains more of the gender wage gap than standard econometric models can measure, and we identify elements of history that are important for reducing OVB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09894v1</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keyon Vafa, Susan Athey, David M. Blei</dc:creator>
    </item>
    <item>
      <title>On LASSO Inference for High Dimensional Predictive Regression</title>
      <link>https://arxiv.org/abs/2409.10030</link>
      <description>arXiv:2409.10030v1 Announce Type: cross 
Abstract: LASSO introduces shrinkage bias into estimated coefficients, which can adversely affect the desirable asymptotic normality and invalidate the standard inferential procedure based on the $t$-statistic. The desparsified LASSO has emerged as a well-known remedy for this issue. In the context of high dimensional predictive regression, the desparsified LASSO faces an additional challenge: the Stambaugh bias arising from nonstationary regressors. To restore the standard inferential procedure, we propose a novel estimator called IVX-desparsified LASSO (XDlasso). XDlasso eliminates the shrinkage bias and the Stambaugh bias simultaneously and does not require prior knowledge about the identities of nonstationary and stationary regressors. We establish the asymptotic properties of XDlasso for hypothesis testing, and our theoretical findings are supported by Monte Carlo simulations. Applying our method to real-world applications from the FRED-MD database -- which includes a rich set of control variables -- we investigate two important empirical questions: (i) the predictability of the U.S. stock returns based on the earnings-price ratio, and (ii) the predictability of the U.S. inflation using the unemployment rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10030v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhan Gao, Ji Hyung Lee, Ziwei Mei, Zhentao Shi</dc:creator>
    </item>
    <item>
      <title>Why you should also use OLS estimation of tail exponents</title>
      <link>https://arxiv.org/abs/2409.10448</link>
      <description>arXiv:2409.10448v2 Announce Type: cross 
Abstract: Even though practitioners often estimate Pareto exponents running OLS rank-size regressions, the usual recommendation is to use the Hill MLE with a small-sample correction instead, due to its unbiasedness and efficiency. In this paper, we advocate that you should also apply OLS in empirical applications. On the one hand, we demonstrate that, with a small-sample correction, the OLS estimator is also unbiased. On the other hand, we show that the MLE assigns significantly greater weight to smaller observations. This suggests that the OLS estimator may outperform the MLE in cases where the distribution is (i) strictly Pareto but only in the upper tail or (ii) regularly varying rather than strictly Pareto. We substantiate our theoretical findings with Monte Carlo simulations and real-world applications, demonstrating the practical relevance of the OLS method in estimating tail exponents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10448v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thiago Trafane Oliveira Santos (Central Bank of Brazil, Bras\'ilia, Brazil. Department of %Economics, University of Brasilia, Brazil), Daniel Oliveira Cajueiro (Department of Economics, University of Brasilia, Brazil. National Institute of Science and Technology for Complex Systems)</dc:creator>
    </item>
    <item>
      <title>Theory of Low Frequency Contamination from Nonstationarity and Misspecification: Consequences for HAR Inference</title>
      <link>https://arxiv.org/abs/2103.01604</link>
      <description>arXiv:2103.01604v3 Announce Type: replace 
Abstract: We establish theoretical results about the low frequency contamination (i.e., long memory effects) induced by general nonstationarity for estimates such as the sample autocovariance and the periodogram, and deduce consequences for heteroskedasticity and autocorrelation robust (HAR) inference. We present explicit expressions for the asymptotic bias of these estimates. We distinguish cases where this contamination only occurs as a small-sample problem and cases where the contamination continues to hold asymptotically. We show theoretically that nonparametric smoothing over time is robust to low frequency contamination. Our results provide new insights on the debate between consistent versus inconsistent long-run variance (LRV) estimation. Existing LRV estimators tend to be in inflated when the data are nonstationary. This results in HAR tests that can be undersized and exhibit dramatic power losses. Our theory indicates that long bandwidths or fixed-b HAR tests suffer more from low frequency contamination relative to HAR tests based on HAC estimators, whereas recently introduced double kernel HAC estimators do not super from this problem. Finally, we present second-order Edgeworth expansions under nonstationarity about the distribution of HAC and DK-HAC estimators and about the corresponding t-test in the linear regression model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2103.01604v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alessandro Casini, Taosong Deng, Pierre Perron</dc:creator>
    </item>
    <item>
      <title>Empirical Welfare Maximization with Constraints</title>
      <link>https://arxiv.org/abs/2103.15298</link>
      <description>arXiv:2103.15298v2 Announce Type: replace 
Abstract: Empirical Welfare Maximization (EWM) is a framework that can be used to select welfare program eligibility policies based on data. This paper extends EWM by allowing for uncertainty in estimating the budget needed to implement the selected policy, in addition to its welfare. Due to the additional estimation error, I show there exist no rules that achieve the highest welfare possible while satisfying a budget constraint uniformly over a wide range of DGPs. This differs from the setting without a budget constraint where uniformity is achievable. I propose an alternative trade-off rule and illustrate it with Medicaid expansion, a setting with imperfect take-up and varying program costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2103.15298v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liyang Sun</dc:creator>
    </item>
    <item>
      <title>Counterfactual and Synthetic Control Method: Causal Inference with Instrumented Principal Component Analysis</title>
      <link>https://arxiv.org/abs/2408.09271</link>
      <description>arXiv:2408.09271v2 Announce Type: replace 
Abstract: In this paper, we propose a novel method for causal inference within the framework of counterfactual and synthetic control. Matching forward the generalized synthetic control method, our instrumented principal component analysis method instruments factor loadings with predictive covariates rather than including them as regressors. These instrumented factor loadings exhibit time-varying dynamics, offering a better economic interpretation. Covariates are instrumented through a transformation matrix, $\Gamma$, when we have a large number of covariates it can be easily reduced in accordance with a small number of latent factors helping us to effectively handle high-dimensional datasets and making the model parsimonious. Moreover, the novel way of handling covariates is less exposed to model misspecification and achieved better prediction accuracy. Our simulations show that this method is less biased in the presence of unobserved covariates compared to other mainstream approaches. In the empirical application, we use the proposed method to evaluate the effect of Brexit on foreign direct investment to the UK.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09271v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cong Wang</dc:creator>
    </item>
    <item>
      <title>Attention Overload</title>
      <link>https://arxiv.org/abs/2110.10650</link>
      <description>arXiv:2110.10650v4 Announce Type: replace-cross 
Abstract: We introduce an Attention Overload Model that captures the idea that alternatives compete for the decision maker's attention, and hence the attention that each alternative receives decreases as the choice problem becomes larger. Using this nonparametric restriction on the random attention formation, we show that a fruitful revealed preference theory can be developed and provide testable implications on the observed choice behavior that can be used to (point or partially) identify the decision maker's preference and attention frequency. We then enhance our attention overload model to accommodate heterogeneous preferences. Due to the nonparametric nature of our identifying assumption, we must discipline the amount of heterogeneity in the choice model: we propose the idea of List-based Attention Overload, where alternatives are presented to the decision makers as a list that correlates with both heterogeneous preferences and random attention. We show that preference and attention frequencies are (point or partially) identifiable under nonparametric assumptions on the list and attention formation mechanisms, even when the true underlying list is unknown to the researcher. Building on our identification results, for both preference and attention frequencies, we develop econometric methods for estimation and inference that are valid in settings with a large number of alternatives and choice problems, a distinctive feature of the economic environment we consider. We provide a software package in R implementing our empirical methods, and illustrate them in a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.10650v4</guid>
      <category>econ.TH</category>
      <category>econ.EM</category>
      <pubDate>Tue, 17 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Paul Cheung, Xinwei Ma, Yusufcan Masatlioglu</dc:creator>
    </item>
  </channel>
</rss>
