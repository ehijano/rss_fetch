<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 22 May 2025 01:51:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Ancestry-Adjusted Polygenic Risk Scores for Predicting Obesity Risk in the Indonesian Population</title>
      <link>https://arxiv.org/abs/2505.13503</link>
      <description>arXiv:2505.13503v1 Announce Type: new 
Abstract: Obesity prevalence in Indonesian adults increased from 10.5% in 2007 to 23.4% in 2023. Studies showed that genetic predisposition significantly influences obesity susceptibility. To aid this, polygenic risk scores (PRS) help aggregate the effects of numerous genetic variants to assess genetic risk. However, 91% of genome-wide association studies (GWAS) involve European populations, limiting their applicability to Indonesians due to genetic diversity. This study aims to develop and validate an ancestry adjusted PRS for obesity in the Indonesian population using principal component analysis (PCA) method constructed from the 1000 Genomes Project data and our own genomic data from approximately 2,800 Indonesians. We calculate PRS for obesity using all races, then determine the first four principal components using ancestry-informative SNPs and develop a linear regression model to predict PRS based on these principal components. The raw PRS is adjusted by subtracting the predicted score to obtain an ancestry adjusted PRS for the Indonesian population. Our results indicate that the ancestry-adjusted PRS improves obesity risk prediction. Compared to the unadjusted PRS, the adjusted score improved classification performance with a 5% increase in area under the ROC curve (AUC). This approach underscores the importance of population-specific adjustments in genetic risk assessments to enable more effective personalized healthcare and targeted intervention strategies for diverse populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13503v1</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jocelyn Verna Siswanto, Belinda Mutiara, Felicia Austin, Jonathan Susanto, Cathelyn Theophila Tan, Restu Unggul Kresnadi, Kezia Irene</dc:creator>
    </item>
    <item>
      <title>Finding Distributions that Differ, with False Discovery Rate Control</title>
      <link>https://arxiv.org/abs/2505.13769</link>
      <description>arXiv:2505.13769v1 Announce Type: new 
Abstract: We consider the problem of comparing a reference distribution with several other distributions. Given a sample from both the reference and the comparison groups, we aim to identify the comparison groups whose distributions differ from that of the reference group. Viewing this as a multiple testing problem, we introduce a methodology that provides exact, distribution-free control of the false discovery rate. To do so, we introduce the concept of batch conformal p-values and demonstrate that they satisfy positive regression dependence across the groups [Benjamini and Yekutieli, 2001], thereby enabling control of the false discovery rate through the Benjamini-Hochberg procedure. The proof of positive regression dependence introduces a novel technique for the inductive construction of rank vectors with almost sure dominance under exchangeability. We evaluate the performance of the proposed procedure through simulations, where, despite being distribution-free, in some cases they show performance comparable to methods with knowledge of the data-generating normal distribution; and further have more power than direct approaches based on conformal out-of-distribution detection. Further, we illustrate our methods on a Hepatitis C treatment dataset, where they can identify patient groups with large treatment effects; and on the Current Population Survey dataset, where they can identify sub-population with long work hours.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13769v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonghoon Lee, Edgar Dobriban, Eric Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>A Bayesian Sparse Kronecker Product Decomposition Framework for Tensor Predictors with Mixed-Type Responses</title>
      <link>https://arxiv.org/abs/2505.13821</link>
      <description>arXiv:2505.13821v1 Announce Type: new 
Abstract: Ultra-high-dimensional tensor predictors are increasingly common in neuroimaging and other biomedical studies, yet existing methods rarely integrate continuous, count, and binary responses in a single coherent model. We present a Bayesian Sparse Kronecker Product Decomposition (BSKPD) that represents each regression (or classification) coefficient tensor as a low-rank Kronecker product whose factors are endowed with element-wise Three-Parameter Beta-Normal shrinkage priors, yielding voxel-level sparsity and interpretability. Embedding Gaussian, Poisson, and Bernoulli outcomes in a unified exponential-family form, and combining the shrinkage priors with Polya-Gamma data augmentation, gives closed-form Gibbs updates that scale to full-resolution 3-D images. We prove posterior consistency and identifiability even when each tensor mode dimension grows subexponentially with the sample size, thereby extending high-dimensional Bayesian theory to mixed-type multivariate responses. Simulations and applications to ADNI and OASIS magnetic-resonance imaging datasets show that BSKPD delivers sharper signal recovery and lower predictive error than current low-rank or sparsity-only competitors while preserving scientific interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13821v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shao-Hsuan Wang, Hsin-Hsiung Huang</dc:creator>
    </item>
    <item>
      <title>Extension of Dynamic Network Biomarker using the propensity score method: Simulation of causal effects on variance and correlation coefficient</title>
      <link>https://arxiv.org/abs/2505.13846</link>
      <description>arXiv:2505.13846v1 Announce Type: new 
Abstract: In clinical biomarker studies, the Dynamic Network Biomarker (DNB) is sometimes used. DNB is a composite variable derived from the variance and the Pearson correlation coefficient of biological signals. When applying DNB to clinical data, it is important to account for confounding bias. However, little attention has been paid to statistical causal inference methods for variance and correlation coefficients. This study evaluates confounding adjustment using propensity score matching (PSM) through Monte Carlo simulations. Our results support the use of PSM to reduce bias and improve group comparisons when DNB is applied to clinical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13846v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Satoru Shinoda, Hideaki Kawaguchi</dc:creator>
    </item>
    <item>
      <title>Distributionally enhanced marginal sensitivity model and bounds</title>
      <link>https://arxiv.org/abs/2505.13868</link>
      <description>arXiv:2505.13868v1 Announce Type: new 
Abstract: For sensitivity analysis against unmeasured confounding, we build on the marginal sensitivity model (MSM) and propose a new model, deMSM, by incorporating a second constraint on the shift of potential outcome distributions caused by unmeasured confounders in addition to the constraint on the shift of treatment probabilities. We show that deMSM leads to interpretable sharp bounds of common causal parameters and tightens the corresponding MSM bounds. Moreover, the sharp bounds are symmetric in the two deMSM constraints, which facilitates practical applications. Lastly, we compare deMSM with other MSM-related models in both model constraints and sharp bounds, and reveal new interpretations for later models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13868v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yi Zhang, Wenfu Xu, Zhiqiang Tan</dc:creator>
    </item>
    <item>
      <title>Wavelet Canonical Coherence for Nonstationary Signals</title>
      <link>https://arxiv.org/abs/2505.14253</link>
      <description>arXiv:2505.14253v1 Announce Type: new 
Abstract: Understanding the evolving dependence between two clusters of multivariate signals is fundamental in neuroscience and other domains where sub-networks in a system interact dynamically over time. Despite the growing interest in multivariate time series analysis, existing methods for between-clusters dependence typically rely on the assumption of stationarity and lack the temporal resolution to capture transient, frequency-specific interactions. To overcome this limitation, we propose scale-specific wavelet canonical coherence (WaveCanCoh), a novel framework that extends canonical coherence analysis to the nonstationary setting by leveraging the multivariate locally stationary wavelet model. The proposed WaveCanCoh enables the estimation of time-varying canonical coherence between clusters, providing interpretable insight into scale-specific time-varying interactions between clusters. Through extensive simulation studies, we demonstrate that WaveCanCoh accurately recovers true coherence structures under both locally stationary and general nonstationary conditions. Application to local field potential (LFP) activity data recorded from the hippocampus reveals distinct dynamic coherence patterns between correct and incorrect memory-guided decisions, illustrating the capacity of the method to detect behaviorally relevant neural coordination. These results highlight WaveCanCoh as a flexible and principled tool for modeling complex cross-group dependencies in nonstationary multivariate systems. The code for WaveCanCoh is available at: https://github.com/mhaibo/WaveCanCoh.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14253v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haibo Wu, Marina I. Knight, Keiland W. Cooper, Norbert J. Fortin, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Quasi-Infinitely Divisible Distributions via Fourier Methods</title>
      <link>https://arxiv.org/abs/2505.14255</link>
      <description>arXiv:2505.14255v1 Announce Type: new 
Abstract: This study focuses on statistical inference for the class of quasi-infinitely divisible (QID) distributions, which was recently introduced by Lindner, Pan and Sato (2018). The paper presents a Fourier approach, based on the analogue of the L{\'e}vy-Khintchine theorem with a signed spectral measure. We prove that for some subclasses of QID distributions, the considered estimates have polynomial rates of convergence. This is a remarkable fact when compared to the logarithmic convergence rates of similar methods for infinitely divisible distributions, which cannot be improved in general. We demonstrate the numerical performance of the algorithm using simulated examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14255v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vladimir Panov, Anton Ryabchenko</dc:creator>
    </item>
    <item>
      <title>Exploration, Confirmation, and Replication in the Same Observational Study: A Two Team Cross-Screening Approach to Studying the Effect of Unwanted Pregnancy on Mothers' Later Life Outcomes</title>
      <link>https://arxiv.org/abs/2505.14480</link>
      <description>arXiv:2505.14480v1 Announce Type: new 
Abstract: The long term consequences of unwanted pregnancies carried to term on mothers have not been much explored. We use data from the Wisconsin Longitudinal Study (WLS) and propose a novel approach, namely two team cross-screening, to study the possible effects of unwanted pregnancies carried to term on various aspects of mothers' later-life mental health, physical health, economic well-being and life satisfaction. Our method, unlike existing approaches to observational studies, enables the investigators to perform exploratory data analysis, confirmatory data analysis and replication in the same study. This is a valuable property when there is only a single data set available with unique strengths to perform exploratory, confirmatory and replication analysis. In two team cross-screening, the investigators split themselves into two teams and the data is split as well according to a meaningful covariate. Each team then performs exploratory data analysis on its part of the data to design an analysis plan for the other part of the data. The complete freedom of the teams in designing the analysis has the potential to generate new unanticipated hypotheses in addition to a prefixed set of hypotheses. Moreover, only the hypotheses that looked promising in the data each team explored are forwarded for analysis (thus alleviating the multiple testing problem). These advantages are demonstrated in our study of the effects of unwanted pregnancies on mothers' later life outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14480v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samrat Roy, Marina Bogomolov, Ruth Heller, Amy M. Claridge, Tishra Beeson, Dylan S. Small</dc:creator>
    </item>
    <item>
      <title>Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference</title>
      <link>https://arxiv.org/abs/2505.13770</link>
      <description>arXiv:2505.13770v1 Announce Type: cross 
Abstract: Reliable causal inference is essential for making decisions in high-stakes areas like medicine, economics, and public policy. However, it remains unclear whether large language models (LLMs) can handle rigorous and trustworthy statistical causal inference. Current benchmarks usually involve simplified tasks. For example, these tasks might only ask LLMs to identify semantic causal relationships or draw conclusions directly from raw data. As a result, models may overlook important statistical pitfalls, such as Simpson's paradox or selection bias. This oversight limits the applicability of LLMs in the real world. To address these limitations, we propose CausalPitfalls, a comprehensive benchmark designed to rigorously evaluate the capability of LLMs in overcoming common causal inference pitfalls. Our benchmark features structured challenges across multiple difficulty levels, each paired with grading rubrics. This approach allows us to quantitatively measure both causal reasoning capabilities and the reliability of LLMs' responses. We evaluate models using two protocols: (1) direct prompting, which assesses intrinsic causal reasoning, and (2) code-assisted prompting, where models generate executable code for explicit statistical analysis. Additionally, we validate the effectiveness of this judge by comparing its scoring with assessments from human experts. Our results reveal significant limitations in current LLMs when performing statistical causal inference. The CausalPitfalls benchmark provides essential guidance and quantitative metrics to advance the development of trustworthy causal reasoning systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13770v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jin Du, Li Chen, Xun Xian, An Luo, Fangqiao Tian, Ganghua Wang, Charles Doss, Xiaotong Shen, Jie Ding</dc:creator>
    </item>
    <item>
      <title>High-dimensional Nonparametric Contextual Bandit Problem</title>
      <link>https://arxiv.org/abs/2505.14102</link>
      <description>arXiv:2505.14102v1 Announce Type: cross 
Abstract: We consider the kernelized contextual bandit problem with a large feature space. This problem involves $K$ arms, and the goal of the forecaster is to maximize the cumulative rewards through learning the relationship between the contexts and the rewards. It serves as a general framework for various decision-making scenarios, such as personalized online advertising and recommendation systems. Kernelized contextual bandits generalize the linear contextual bandit problem and offers a greater modeling flexibility. Existing methods, when applied to Gaussian kernels, yield a trivial bound of $O(T)$ when we consider $\Omega(\log T)$ feature dimensions. To address this, we introduce stochastic assumptions on the context distribution and show that no-regret learning is achievable even when the number of dimensions grows up to the number of samples. Furthermore, we analyze lenient regret, which allows a per-round regret of at most $\Delta &gt; 0$. We derive the rate of lenient regret in terms of $\Delta$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14102v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shogo Iwazaki, Junpei Komiyama, Masaaki Imaizumi</dc:creator>
    </item>
    <item>
      <title>Hybrid Bernstein Normalizing Flows for Flexible Multivariate Density Regression with Interpretable Marginals</title>
      <link>https://arxiv.org/abs/2505.14164</link>
      <description>arXiv:2505.14164v1 Announce Type: cross 
Abstract: Density regression models allow a comprehensive understanding of data by modeling the complete conditional probability distribution. While flexible estimation approaches such as normalizing flows (NF) work particularly well in multiple dimensions, interpreting the input-output relationship of such models is often difficult, due to the black-box character of deep learning models. In contrast, existing statistical methods for multivariate outcomes such as multivariate conditional transformation models (MCTM) are restricted in flexibility and are often not expressive enough to represent complex multivariate probability distributions. In this paper, we combine MCTM with state-of-the-art and autoregressive NF to leverage the transparency of MCTM for modeling interpretable feature effects on the marginal distributions in the first step and the flexibility of neural-network-based NF techniques to account for complex and non-linear relationships in the joint data distribution. We demonstrate our method's versatility in various numerical experiments and compare it with MCTM and other NF models on both simulated and real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14164v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcel Arpogaus, Thomas Kneib, Thomas Nagler, David R\"ugamer</dc:creator>
    </item>
    <item>
      <title>Mixing times of data-augmentation Gibbs samplers for high-dimensional probit regression</title>
      <link>https://arxiv.org/abs/2505.14343</link>
      <description>arXiv:2505.14343v1 Announce Type: cross 
Abstract: We investigate the convergence properties of popular data-augmentation samplers for Bayesian probit regression. Leveraging recent results on Gibbs samplers for log-concave targets, we provide simple and explicit non-asymptotic bounds on the associated mixing times (in Kullback-Leibler divergence). The bounds depend explicitly on the design matrix and the prior precision, while they hold uniformly over the vector of responses. We specialize the results for different regimes of statistical interest, when both the number of data points $n$ and parameters $p$ are large: in particular we identify scenarios where the mixing times remain bounded as $n,p\to\infty$, and ones where they do not. The results are shown to be tight (in the worst case with respect to the responses) and provide guidance on choices of prior distributions that provably lead to fast mixing. An empirical analysis based on coupling techniques suggests that the bounds are effective in predicting practically observed behaviours.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14343v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filippo Ascolani, Giacomo Zanella</dc:creator>
    </item>
    <item>
      <title>A Bayesian design for dual-agent dose optimization with targeted therapies</title>
      <link>https://arxiv.org/abs/2210.08894</link>
      <description>arXiv:2210.08894v2 Announce Type: replace 
Abstract: In this article, we propose a phase I-II design in two stages for the combination of molecularly targeted therapies. The design is motivated by a published case study that combines a MEK and a PIK3CA inhibitors; a setting in which higher dose levels do not necessarily translate into higher efficacy responses. The goal is therefore to identify dose combination(s) with a prespecified desirable risk-benefit trade-off. We propose a flexible cubic spline to model the marginal distribution of the efficacy response. In stage I, patients are allocated following the escalation with overdose control (EWOC) principle whereas, in stage II, we adaptively randomize patients to the available experimental dose combinations based on the continuously updated model parameters. A simulation study is presented to assess the design's performance under different scenarios, as well as to evaluate its sensitivity to the sample size and to model misspecification. Compared to a recently published dose finding algorithm for biologic drugs, our design is safer and more efficient at identifying optimal dose combinations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.08894v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jos\'e L. Jim\'enez, Mourad Tighiouart</dc:creator>
    </item>
    <item>
      <title>RISE: Two-Stage Rank-Based Identification of High-Dimensional Surrogate Markers Applied to Vaccinology</title>
      <link>https://arxiv.org/abs/2502.03030</link>
      <description>arXiv:2502.03030v2 Announce Type: replace 
Abstract: In vaccine trials with long-term participant follow-up, it is of great importance to identify surrogate markers that accurately infer long-term immune responses. These markers offer practical advantages such as providing early, indirect evidence of vaccine efficacy, and can accelerate vaccine development while identifying potential biomarkers. High-throughput technologies like RNA-sequencing have emerged as promising tools for understanding complex biological systems and informing new treatment strategies. However, these data are high-dimensional, presenting unique statistical challenges for existing surrogate marker identification methods. We introduce Rank-based Identification of high-dimensional SurrogatE Markers (RISE), a novel approach designed for small sample, high-dimensional settings typical in modern vaccine experiments. RISE employs a non-parametric univariate test to screen variables for promising candidates, followed by surrogate evaluation on independent data. Our simulation studies demonstrate RISE's desirable properties, including type one error rate control and empirical power under various conditions. Applying RISE to a clinical trial for inactivated influenza vaccination, we sought to identify genes whose expression could serve as a surrogate for the induced immune response. This analysis revealed a signature of genes appearing to function as a reasonable surrogate for the neutralising antibody response. Pathways related to innate antiviral signalling and interferon stimulation were strongly represented in this derived surrogate, providing a clear immunological interpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03030v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arthur Hughes, Layla Parast, Rodolphe Thi\'ebaut, Boris P. Hejblum</dc:creator>
    </item>
    <item>
      <title>biniLasso: Automated cut-point detection via sparse cumulative binarization</title>
      <link>https://arxiv.org/abs/2503.16687</link>
      <description>arXiv:2503.16687v2 Announce Type: replace 
Abstract: We present biniLasso and its sparse variant (sparse biniLasso), novel methods for prognostic analysis of high-dimensional survival data that enable detection of multiple cut-points per feature. Our approach leverages the Cox proportional hazards model with two key innovations: (1) a cumulative binarization scheme with $L_1$-penalized coefficients operating on context-dependent cut-point candidates, and (2) for sparse biniLasso, additional uniLasso regularization to enforce sparsity while preserving univariate coefficient patterns. These innovations yield substantially improved interpretability, computational efficiency (4-11x faster than existing approaches), and prediction performance. Through extensive simulations, we demonstrate superior performance in cut-point detection, particularly in high-dimensional settings. Application to three genomic cancer datasets from TCGA confirms the methods' practical utility, with both variants showing enhanced risk prediction accuracy compared to conventional techniques. The biniLasso framework thus provides a powerful, flexible tool for survival analysis in biomedical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16687v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdollah Safari, Hamed Halisaz, Peter Loewen</dc:creator>
    </item>
    <item>
      <title>Interpretable Deep Neural Network for Modeling Functional Surrogates</title>
      <link>https://arxiv.org/abs/2503.20528</link>
      <description>arXiv:2503.20528v3 Announce Type: replace 
Abstract: Developing surrogates for computer models has become increasingly important for addressing complex problems in science and engineering. This article introduces an artificial intelligent (AI) surrogate, referred to as the DeepSurrogate, for analyzing functional outputs with vector-valued inputs. The relationship between the functional output and vector-valued input is modeled as an infinite sequence of unknown functions, each representing the relationship at a specific location within the functional domain. These spatially indexed functions are expressed through a combination of basis functions and their corresponding coefficient functions, both of which are modeled using deep neural networks (DNN). The proposed framework accounts for spatial dependencies across locations, while capturing the relationship between the functional output and scalar predictors. It also integrates a Monte Carlo (MC) dropout strategy to quantify prediction uncertainty, enhancing explainability in the deep neural network architecture. The proposed method enables efficient inference on datasets with approximately 50,000 spatial locations and 20 simulations, achieving results in under 10 minutes using standard hardware. The approach is validated on extensive synthetic datasets and a large-scale simulation from the Sea Lake and Overland Surge from Hurricanes (SLOSH) simulator. An open-source Python package implementing the method is made available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20528v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yeseul Jeon, Rajarshi Guhaniyogi, Aaron Scheffler, Devin Francom, Donatella Pasqualini</dc:creator>
    </item>
    <item>
      <title>On Bessel's Correction: Unbiased Sample Variance, the "Bariance", and a Novel Runtime-Optimized Estimator</title>
      <link>https://arxiv.org/abs/2503.22333</link>
      <description>arXiv:2503.22333v4 Announce Type: replace 
Abstract: Bessel's correction adjusts the denominator in the sample variance formula from n to n-1 to produce an unbiased estimator for the population variance. This paper includes rigorous derivations, geometric interpretations, and visualizations. It then introduces the concept of "bariance", an alternative pairwise distances based intuition of sample dispersion without an arithmetic mean. Finally, we address practical concerns raised in Rosenthal's article (Rosenthal, 2015) advocating the use of n-based estimates from a more holistic MSE-based viewpoint for pedagogical reasons and in certain practical contexts. Finally, the empirical part using simulation reveals that the run-time of estimating population variance can be shortened when using an algebraically optimized "bariance" approach to estimate an unbiased population sample variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22333v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felix Reichel</dc:creator>
    </item>
    <item>
      <title>Regularized Fingerprinting with Linearly Optimal Weight Matrix in Detection and Attribution of Climate Change</title>
      <link>https://arxiv.org/abs/2505.04070</link>
      <description>arXiv:2505.04070v2 Announce Type: replace 
Abstract: Climate change detection and attribution play a central role in establishing the causal influence of human activities on global warming. The dominant framework, optimal fingerprinting, is a linear errors-in-variables model in which each covariate is subject to measurement error with covariance proportional to that of the regression error. The reliability of such analyses depends critically on accurate inference of the regression coefficients. The optimal weight matrix for estimating these coefficients is the precision matrix of the regression error, which is typically unknown and must be estimated from climate model simulations. However, existing regularized optimal fingerprinting approaches often yield underestimated uncertainties and overly narrow confidence intervals that fail to attain nominal coverage, thereby compromising the reliability of analysis. In this paper, we first propose consistent variance estimators for the regression coefficients within the class of linear shrinkage weight matrices, addressing undercoverage in conventional methods. Building on this, we derive a linearly optimal weight matrix that directly minimizes the asymptotic variances of the estimated scaling factors. Numerical studies confirm improved empirical coverage and shorter interval lengths. When applied to annual mean temperature data, the proposed method produces narrower, more reliable intervals and provides new insights into detection and attribution across different regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04070v2</guid>
      <category>stat.ME</category>
      <category>physics.ao-ph</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoran Li, Yan Li</dc:creator>
    </item>
    <item>
      <title>Statistically Significant Linear Regression Coefficients Solely Driven By Outliers In Finite-sample Inference</title>
      <link>https://arxiv.org/abs/2505.10738</link>
      <description>arXiv:2505.10738v2 Announce Type: replace 
Abstract: In this paper, we investigate the impact of outliers on the statistical significance of coefficients in linear regression. We demonstrate, through numerical simulation using R, that a single outlier can cause an otherwise insignificant coefficient to appear statistically significant. We compare this with robust Huber regression, which reduces the effects of outliers. Afterwards, we approximate the influence of a single outlier on estimated regression coefficients and discuss common diagnostic statistics to detect influential observations in regression (e.g., studentized residuals). Furthermore, we relate this issue to the optional normality assumption in simple linear regression [14], required for exact finite-sample inference but asymptotically justified for large n by the Central Limit Theorem (CLT). We also address the general dangers of relying solely on p-values without performing adequate regression diagnostics. Finally, we provide a brief overview of regression methods and discuss how they relate to the assumptions of the Gauss-Markov theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10738v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Felix Reichel</dc:creator>
    </item>
    <item>
      <title>Model-X Change-Point Detection of Conditional Distribution</title>
      <link>https://arxiv.org/abs/2505.12023</link>
      <description>arXiv:2505.12023v2 Announce Type: replace 
Abstract: The dynamic nature of many real-world systems can lead to temporal outcome model shifts, causing a deterioration in model accuracy and reliability over time. This requires change-point detection on the outcome models to guide model retraining and adjustments. However, inferring the change point of conditional models is more prone to loss of validity or power than classic detection problems for marginal distributions. This is due to both the temporal covariate shift and the complexity of the outcome model. To address these challenges, we propose a novel model-X Conditional Random Testing (CRT) method computationally enhanced with latent mixture model (LMM) distillation for simultaneous change-point detection and localization of the conditional outcome model. Built upon the model-X framework, our approach can effectively adjust for the potential bias caused by the temporal covariate shift and allow the flexible use of general machine learning methods for outcome modeling. It preserves good validity against complex or erroneous outcome models, even with imperfect knowledge of the temporal covariate shift learned from some auxiliary unlabeled data. Moreover, the incorporation of LMM distillation significantly reduces the computational burden of the CRT by eliminating the need for repeated complex model refitting in its resampling procedure and preserves the statistical validity and power well. Theoretical validity of the proposed method is justified. Extensive simulation studies and a real-world example demonstrate the statistical effectiveness and computational scalability of our method as well as its significant improvements over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12023v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiwen Huang, Yan Dong, Mengying Yan, Ziye Tian, Chuan Hong, Doudou Zhou, Molei Liu</dc:creator>
    </item>
    <item>
      <title>Adaptive Inference through Bayesian and Inverse Bayesian Inference with Symmetry-Bias in Nonstationary Environments</title>
      <link>https://arxiv.org/abs/2505.12796</link>
      <description>arXiv:2505.12796v2 Announce Type: replace 
Abstract: This study introduces a novel inference framework, designated as Bayesian and inverse Bayesian (BIB) inference, which concurrently performs both conventional and inverse Bayesian updates by integrating symmetry bias into Bayesian inference. The effectiveness of the model was evaluated through a sequential estimation task involving observations sampled from a Gaussian distribution with a stochastically time-varying mean. Conventional Bayesian inference entails a fundamental trade-off between adaptability to abrupt environmental shifts and estimation accuracy during stable intervals. The BIB framework addresses this limitation by dynamically modulating the learning rate through inverse Bayesian updates, thereby enhancing adaptive flexibility. The BIB model generated spontaneous bursts in the learning rate during sudden environmental transitions, transiently entering a high-sensitivity state to accommodate incoming data. This intermittent burst-relaxation pattern functions as a dynamic mechanism that balances adaptability and accuracy. Further analysis of burst interval distributions demonstrated that the BIB model consistently produced power-law distributions under diverse conditions. Such robust scaling behavior, absent in conventional Bayesian inference, appears to emerge from a self-regulatory mechanism driven by inverse Bayesian updates. These results present a novel computational perspective on scale-free phenomena in natural systems and offer implications for designing adaptive inference systems in nonstationary environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12796v2</guid>
      <category>stat.ME</category>
      <category>cs.MA</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuji Shinohara, Daiki Morita, Hayato Hirai, Ryosuke Kuribayashi, Nobuhito Manome, Toru Moriyama, Yoshihiro Nakajima, Yukio-Pegio Gunji, Ung-il Chung</dc:creator>
    </item>
    <item>
      <title>Sequential Kernelized Independence Testing</title>
      <link>https://arxiv.org/abs/2212.07383</link>
      <description>arXiv:2212.07383v4 Announce Type: replace-cross 
Abstract: Independence testing is a classical statistical problem that has been extensively studied in the batch setting when one fixes the sample size before collecting data. However, practitioners often prefer procedures that adapt to the complexity of a problem at hand instead of setting sample size in advance. Ideally, such procedures should (a) stop earlier on easy tasks (and later on harder tasks), hence making better use of available resources, and (b) continuously monitor the data and efficiently incorporate statistical evidence after collecting new data, while controlling the false alarm rate. Classical batch tests are not tailored for streaming data: valid inference after data peeking requires correcting for multiple testing which results in low power. Following the principle of testing by betting, we design sequential kernelized independence tests that overcome such shortcomings. We exemplify our broad framework using bets inspired by kernelized dependence measures, e.g., the Hilbert-Schmidt independence criterion. Our test is also valid under non-i.i.d., time-varying settings. We demonstrate the power of our approaches on both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.07383v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aleksandr Podkopaev, Patrick Bl\"obaum, Shiva Prasad Kasiviswanathan, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>A new approach to locally adaptive polynomial regression</title>
      <link>https://arxiv.org/abs/2412.19802</link>
      <description>arXiv:2412.19802v2 Announce Type: replace-cross 
Abstract: Adaptive bandwidth selection is a fundamental challenge in nonparametric regression. This paper introduces a new bandwidth selection procedure inspired by the optimality criteria for $\ell_0$-penalized regression. Although similar in spirit to Lepski's method and its variants in selecting the largest interval satisfying an admissibility criterion, our approach stems from a distinct philosophy, utilizing criteria based on $\ell_2$-norms of interval projections rather than explicit point and variance estimates. We obtain non-asymptotic risk bounds for the local polynomial regression methods based on our bandwidth selection procedure which adapt (near-)optimally to the local H\"{o}lder exponent of the underlying regression function simultaneously at all points in its domain. Furthermore, we show that there is a single ideal choice of a global tuning parameter in each case under which the above-mentioned local adaptivity holds. The optimal risks of our methods derive from the properties of solutions to a new ``bandwidth selection equation'' which is of independent interest. We believe that the principles underlying our approach provide a new perspective to the classical yet ever relevant problem of locally adaptive nonparametric regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19802v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sabyasachi Chatterjee, Subhajit Goswami, Soumendu Sundar Mukherjee</dc:creator>
    </item>
    <item>
      <title>Does Unsupervised Domain Adaptation Improve the Robustness of Amortized Bayesian Inference? A Systematic Evaluation</title>
      <link>https://arxiv.org/abs/2502.04949</link>
      <description>arXiv:2502.04949v2 Announce Type: replace-cross 
Abstract: Neural networks are fragile when confronted with data that significantly deviates from their training distribution. This is true in particular for simulation-based inference methods, such as neural amortized Bayesian inference (ABI), where models trained on simulated data are deployed on noisy real-world observations. Recent robust approaches employ unsupervised domain adaptation (UDA) to match the embedding spaces of simulated and observed data. However, the lack of comprehensive evaluations across different domain mismatches raises concerns about the reliability in high-stakes applications. We address this gap by systematically testing UDA approaches across a wide range of misspecification scenarios in silico and practice. We demonstrate that aligning summary spaces between domains effectively mitigates the impact of unmodeled phenomena or noise. However, the same alignment mechanism can lead to failures under prior misspecifications - a critical finding with practical consequences. Our results underscore the need for careful consideration of misspecification types when using UDA to increase the robustness of ABI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04949v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 21 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lasse Elsem\"uller, Valentin Pratz, Mischa von Krause, Andreas Voss, Paul-Christian B\"urkner, Stefan T. Radev</dc:creator>
    </item>
  </channel>
</rss>
