<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 14 May 2025 01:37:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Geometry of Learning -- L2 Phase Transitions in Deep and Shallow Neural Networks</title>
      <link>https://arxiv.org/abs/2505.06597</link>
      <description>arXiv:2505.06597v1 Announce Type: cross 
Abstract: When neural networks (NNs) are subject to L2 regularization, increasing the regularization strength beyond a certain threshold pushes the model into an under-parameterization regime. This transition manifests as a first-order phase transition in single-hidden-layer NNs and a second-order phase transition in NNs with two or more hidden layers. This paper establishes a unified framework for such transitions by integrating the Ricci curvature of the loss landscape with regularizer-driven deep learning. First, we show that a curvature change-point separates the model-accuracy regimes in the onset of learning and that it is identical to the critical point of the phase transition driven by regularization. Second, we show that for more complex data sets additional phase transitions exist between model accuracies, and that they are again identical to curvature change points in the error landscape. Third, by studying the MNIST data set using a Variational Autoencoder, we demonstrate that the curvature change points identify phase transitions in model accuracy outside the L2 setting. Our framework also offers practical insights for optimizing model performance across various architectures and datasets. By linking geometric features of the error landscape to observable phase transitions, our work paves the way for more informed regularization strategies and potentially new methods for probing the intrinsic structure of neural networks beyond the L2 context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06597v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ibrahim Talha Ersoy, Karoline Wiesner</dc:creator>
    </item>
    <item>
      <title>Compression, Regularity, Randomness and Emergent Structure: Rethinking Physical Complexity in the Data-Driven Era</title>
      <link>https://arxiv.org/abs/2505.07222</link>
      <description>arXiv:2505.07222v1 Announce Type: cross 
Abstract: Complexity science offers a wide range of measures for quantifying unpredictability, structure, and information. Yet, a systematic conceptual organization of these measures is still missing.
  We present a unified framework that locates statistical, algorithmic, and dynamical measures along three axes (regularity, randomness, and complexity) and situates them in a common conceptual space. We map statistical, algorithmic, and dynamical measures into this conceptual space, discussing their computational accessibility and approximability.
  This taxonomy reveals the deep challenges posed by uncomputability and highlights the emergence of modern data-driven methods (including autoencoders, latent dynamical models, symbolic regression, and physics-informed neural networks) as pragmatic approximations to classical complexity ideals. Latent spaces emerge as operational arenas where regularity extraction, noise management, and structured compression converge, bridging theoretical foundations with practical modeling in high-dimensional systems.
  We close by outlining implications for physics-informed AI and AI-guided discovery in complex physical systems, arguing that classical questions of complexity remain central to next-generation scientific modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07222v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>physics.bio-ph</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nima Dehghani</dc:creator>
    </item>
    <item>
      <title>Equilibrium Propagation for Learning in Lagrangian Dynamical Systems</title>
      <link>https://arxiv.org/abs/2505.07363</link>
      <description>arXiv:2505.07363v2 Announce Type: cross 
Abstract: We propose a method for training dynamical systems governed by Lagrangian mechanics using Equilibrium Propagation. Our approach extends Equilibrium Propagation -- initially developed for energy-based models -- to dynamical trajectories by leveraging the principle of action extremization. Training is achieved by gently nudging trajectories toward desired targets and measuring how the variables conjugate to the parameters to be trained respond. This method is particularly suited to systems with periodic boundary conditions or fixed initial and final states, enabling efficient parameter updates without requiring explicit backpropagation through time. In the case of periodic boundary conditions, this approach yields the semiclassical limit of Quantum Equilibrium Propagation. Applications to systems with dissipation are also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07363v2</guid>
      <category>nlin.CD</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Serge Massar</dc:creator>
    </item>
    <item>
      <title>Experimental verification of the optimal fingerprint method for detecting climate change</title>
      <link>https://arxiv.org/abs/2406.11879</link>
      <description>arXiv:2406.11879v2 Announce Type: replace-cross 
Abstract: The optimal fingerprint method serves as a potent approach for detecting and attributing climate change. However, its experimental validation encounters challenges due to the intricate nature of climate systems. Here, we experimentally examine the optimal fingerprint method simulated by a precisely controlled magnetic resonance system of spins. The spin dynamic under an applied deterministic driving field and a noise field is utilized to emulate the complex climate system with external forcing and internal variability. Our experimental results affirm the theoretical prediction regarding the existence of an optimal detection direction which maximizes the signal-to-noise ratio, thereby validating the optimal fingerprint method. This work offers direct empirical verification of the optimal fingerprint method, crucial for comprehending climate change and its societal impacts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11879v2</guid>
      <category>physics.ao-ph</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinbo Hu, Hong Yuan, Letian Chen, Nan Zhao, C. P. Sun</dc:creator>
    </item>
    <item>
      <title>SymbolFit: Automatic Parametric Modeling with Symbolic Regression</title>
      <link>https://arxiv.org/abs/2411.09851</link>
      <description>arXiv:2411.09851v4 Announce Type: replace-cross 
Abstract: We introduce SymbolFit, a framework that automates parametric modeling by using symbolic regression to perform a machine-search for functions that fit the data while simultaneously providing uncertainty estimates in a single run. Traditionally, constructing a parametric model to accurately describe binned data has been a manual and iterative process, requiring an adequate functional form to be determined before the fit can be performed. The main challenge arises when the appropriate functional forms cannot be derived from first principles, especially when there is no underlying true closed-form function for the distribution. In this work, we develop a framework that automates and streamlines the process by utilizing symbolic regression, a machine learning technique that explores a vast space of candidate functions without requiring a predefined functional form because the functional form itself is treated as a trainable parameter, making the process far more efficient and effortless than traditional regression methods. We demonstrate the framework in high-energy physics experiments at the CERN Large Hadron Collider (LHC) using five real proton-proton collision datasets from new physics searches, including background modeling in resonance searches for high-mass dijet, trijet, paired-dijet, diphoton, and dimuon events. We show that our framework can flexibly and efficiently generate a wide range of candidate functions that fit a nontrivial distribution well using a simple fit configuration that varies only by random seed, and that the same fit configuration, which defines a vast function space, can also be applied to distributions of different shapes, whereas achieving a comparable result with traditional methods would have required extensive manual effort.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09851v4</guid>
      <category>hep-ex</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ho Fung Tsoi, Dylan Rankin, Cecile Caillol, Miles Cranmer, Sridhara Dasu, Javier Duarte, Philip Harris, Elliot Lipeles, Vladimir Loncar</dc:creator>
    </item>
    <item>
      <title>Sparsity covariance: a source of uncertainty when estimating correlation functions with a discrete sample of observations in the sky</title>
      <link>https://arxiv.org/abs/2502.18327</link>
      <description>arXiv:2502.18327v2 Announce Type: replace-cross 
Abstract: Cosmological observables rely heavily on summary statistics such as two-point correlation functions. In many practical cases (e.g. the weak-lensing cosmic shear), those correlation functions are estimated from a finite, discrete sample of measurements that are randomly distributed in the sky. The result then inevitably depends on the sample at hand, regardless of any experimental noise. This sample dependence is a source of uncertainty for cosmological observables which I call sparsity covariance. This article proposes a mathematical definition and a generic method to compute sparsity covariance. It is then applied to the concrete case of cosmic shear, showing that sparsity covariance mostly enhances shape noise, whose amplitude is determined by the apparent ellipticity of galaxies rather than their intrinsic ellipticity. In general, sparsity covariance is non-negligible when the signal-to-noise ratio of individual measurements in the sample is comparable to, or larger than, unity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18327v2</guid>
      <category>astro-ph.CO</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.33232/001c.138039</arxiv:DOI>
      <arxiv:journal_reference>The Open Journal of Astrophysics Vol. 8 (May) 2025</arxiv:journal_reference>
      <dc:creator>Pierre Fleury</dc:creator>
    </item>
  </channel>
</rss>
