<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 May 2025 04:00:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Towards Adaptive Self-Normalized Importance Samplers</title>
      <link>https://arxiv.org/abs/2505.00372</link>
      <description>arXiv:2505.00372v1 Announce Type: new 
Abstract: The self-normalized importance sampling (SNIS) estimator is a Monte Carlo estimator widely used to approximate expectations in statistical signal processing and machine learning.
  The efficiency of SNIS depends on the choice of proposal, but selecting a good proposal is typically unfeasible. In particular, most of the existing adaptive IS (AIS) literature overlooks the optimal SNIS proposal.
  In this paper, we introduce an AIS framework that uses MCMC to approximate the optimal SNIS proposal within an iterative scheme. This is, to the best of our knowledge, the first AIS framework targeting specifically the SNIS optimal proposal. We find a close connection with adaptive schemes used in ratio importance sampling (RIS), which also brings a new perspective and paves the way for combining techniques from AIS and adaptive RIS. We outline possible extensions, connections with existing MCMC-driven AIS algorithms, theoretical directions, and demonstrate performance in numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00372v1</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicola Branchini, V\'ictor Elvira</dc:creator>
    </item>
    <item>
      <title>Local Quasi-Exponential Growth Models: Kernel Differential Equation Regression and Sparse Data</title>
      <link>https://arxiv.org/abs/2505.00231</link>
      <description>arXiv:2505.00231v1 Announce Type: cross 
Abstract: Local polynomial regression struggles with several challenges when dealing with sparse data. The difficulty in capturing local features of the underlying function can lead to a potential misrepresentation of the true relationship. Additionally, with limited data points in local neighborhoods, the variance of estimators can increase significantly. Local polynomial regression also requires a substantial amount of data to produce good models, making it less efficient for sparse datasets. This paper employs a differential equation-constrained regression approach, introduced by \citet{ding2014estimation}, for local quasi-exponential growth models. By incorporating first-order differential equations, this method extends the sparse design capacity of local polynomial regression while reducing bias and variance. We discuss the asymptotic biases and variances of kernel estimators using first-degree Taylor polynomials. Model comparisons are conducted using mouse tumor growth data, along with simulation studies under various scenarios that simulate quasi-exponential growth with different noise levels and growth rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00231v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chunlei Ge, W. John Braun</dc:creator>
    </item>
    <item>
      <title>Optimal Vector Compressed Sensing Using James Stein Shrinkage</title>
      <link>https://arxiv.org/abs/2505.00326</link>
      <description>arXiv:2505.00326v1 Announce Type: cross 
Abstract: The trend in modern science and technology is to take vector measurements rather than scalars, ruthlessly scaling to ever higher dimensional vectors. For about two decades now, traditional scalar Compressed Sensing has been synonymous with a Convex Optimization based procedure called Basis Pursuit. In the vector recovery case, the natural tendency is to return to a straightforward vector extension of Basis Pursuit, also based on Convex Optimization. However, Convex Optimization is provably suboptimal, particularly when $B$ is large. In this paper, we propose SteinSense, a lightweight iterative algorithm, which is provably optimal when $B$ is large. It does not have any tuning parameter, does not need any training data, requires zero knowledge of sparsity, is embarrassingly simple to implement, and all of this makes it easily scalable to high vector dimensions. We conduct a massive volume of both real and synthetic experiments that confirm the efficacy of SteinSense, and also provide theoretical justification based on ideas from Approximate Message Passing. Fascinatingly, we discover that SteinSense is quite robust, delivering the same quality of performance on real data, and even under substantial departures from conditions under which existing theory holds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00326v1</guid>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Apratim Dey, David Donoho</dc:creator>
    </item>
    <item>
      <title>Robust Parameter Estimation in Dynamical Systems by Stochastic Differential Equations</title>
      <link>https://arxiv.org/abs/2505.00491</link>
      <description>arXiv:2505.00491v1 Announce Type: cross 
Abstract: Ordinary and stochastic differential equations (ODEs and SDEs) are widely used to model continuous-time processes across various scientific fields. While ODEs offer interpretability and simplicity, SDEs incorporate randomness, providing robustness to noise and model misspecifications. Recent research highlights the statistical advantages of SDEs, such as improved parameter identifiability and stability under perturbations. This paper investigates the robustness of parameter estimation in SDEs versus ODEs under three types of model misspecifications: unrecognized noise sources, external perturbations, and simplified models. Furthermore, the effect of missing data is explored. Through simulations and an analysis of Danish COVID-19 data, we demonstrate that SDEs yield more stable and reliable parameter estimates, making them a strong alternative to traditional ODE modeling in the presence of uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00491v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qingchuan Sun, Susanne Ditlevsen</dc:creator>
    </item>
    <item>
      <title>Pre-Training Estimators for Structural Models: Application to Consumer Search</title>
      <link>https://arxiv.org/abs/2505.00526</link>
      <description>arXiv:2505.00526v1 Announce Type: cross 
Abstract: We explore pretraining estimators for structural econometric models. The estimator is "pretrained" in the sense that the bulk of the computational cost and researcher effort occur during the construction of the estimator. Subsequent applications of the estimator to different datasets require little computational cost or researcher effort. The estimation leverages a neural net to recognize the structural model's parameter from data patterns. As an initial trial, this paper builds a pretrained estimator for a sequential search model that is known to be difficult to estimate. We evaluate the pretrained estimator on 14 real datasets. The estimation takes seconds to run and shows high accuracy. We provide the estimator at pnnehome.github.io. More generally, pretrained, off-the-shelf estimators can make structural models more accessible to researchers and practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00526v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanhao 'Max' Wei, Zhenling Jiang</dc:creator>
    </item>
    <item>
      <title>Improving the convergence of Markov chains via permutations and projections</title>
      <link>https://arxiv.org/abs/2411.08295</link>
      <description>arXiv:2411.08295v2 Announce Type: replace-cross 
Abstract: This paper aims at improving the convergence to equilibrium of finite ergodic Markov chains via permutations and projections. First, we prove that a specific mixture of permuted Markov chains arises naturally as a projection under the KL divergence or the squared-Frobenius norm. We then compare various mixing properties of the mixture with other competing Markov chain samplers and demonstrate that it enjoys improved convergence. This geometric perspective motivates us to propose samplers based on alternating projections to combine different permutations and to analyze their rate of convergence. We give necessary, and under some additional assumptions also sufficient, conditions for the projection to achieve stationarity in the limit in terms of the trace of the transition matrix. We proceed to discuss tuning strategies of the projection samplers when these permutations are viewed as parameters. Along the way, we reveal connections between the mixture and a Markov chain Sylvester's equation as well as assignment problems, and highlight how these can be used to understand and improve Markov chain mixing. We provide two examples as illustrations. In the first example, the projection sampler (with a suitable choice of the permutation) improves upon Metropolis-Hastings in a discrete bimodal distribution with a reduced relaxation time from exponential to polynomial in the system size, while in the second example, the mixture of permuted Markov chain yields a mixing time that is logarithmic in system size (with high probability under random permutation), compared to a linear mixing time in the Diaconis-Holmes-Neal sampler. Finally, we provide numerical experiments on statistical physics models to illustrate the improved mixing performance of the proposed projection samplers over standard Metropolis-Hastings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08295v2</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <pubDate>Fri, 02 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael C. H. Choi, Max Hird, Youjia Wang</dc:creator>
    </item>
  </channel>
</rss>
