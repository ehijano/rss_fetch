<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 Oct 2025 13:01:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Bayesian power spectral density estimation for LISA noise based on P-splines with a parametric boost</title>
      <link>https://arxiv.org/abs/2510.00533</link>
      <description>arXiv:2510.00533v1 Announce Type: new 
Abstract: Flexible and efficient noise characterization is crucial for the precise estimation of gravitational wave parameters. We introduce a fast and accurate Bayesian method for estimating the power spectral density (PSD) of long, stationary time series tailored specifically for LISA data analysis. Our approach models the PSD as a geometric mean of a parametric and a nonparametric component, combining the computational efficiency of parametric models with the flexibility to capture deviations from theoretical expectations. The nonparametric component is expressed by a mixture of penalized B-splines. Adaptive, data-driven knot placement performed once during initialization eliminates computationally expensive reversible-jump Markov Chain Monte Carlo, while hierarchical roughness penalty priors prevent overfitting. This design yields stable, flexible PSD estimates with runtimes of minutes instead of hours. Validation on simulated autoregressive AR(4) data demonstrates estimator consistency. It shows that well-matched parametric components reduce the integrated absolute error compared to an uninformative baseline, requiring fewer spline knots to achieve comparable accuracy. Applied to a year of simulated LISA $X$-channel noise, our method achieves relative integrated absolute errors of $\mathcal{O}(10^{-2})$ with computation times less than three minutes, which makes it suitable for iterative analysis pipelines and multi-year mission datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00533v1</guid>
      <category>stat.CO</category>
      <category>physics.comp-ph</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nazeela Aimen, Patricio Maturana-Russel, Avi Vajpeyi, Nelson Christensen, Renate Meyer</dc:creator>
    </item>
    <item>
      <title>Sequential Bayesian Inference of the GTN Damage Model Using Multimodal Experimental Data</title>
      <link>https://arxiv.org/abs/2510.01016</link>
      <description>arXiv:2510.01016v1 Announce Type: new 
Abstract: Reliable parameter identification in ductile damage models remains challenging because the salient physics of damage progression are localized to small regions in material responses, and their signatures are often diluted in specimen-level measurements. Here, we propose a sequential Bayesian Inference (BI) framework for the calibration of the Gurson-Tvergaard-Needleman (GTN) model using multimodal experimental data (i.e., the specimen-level force-displacement (F-D) measurements and the spatially resolved digital image correlation (DIC) strain fields). This calibration approach builds on a previously developed two-step BI framework that first establishes a low-computational-cost emulator for a physics-based simulator (here, a finite element model incorporating the GTN material model) and then uses the experimental data to sample posteriors for the material model parameters using the Transitional Markov Chain Monte Carlo (T-MCMC). A central challenge to the successful application of this BI framework to the present problem arises from the high-dimensional representations needed to capture the salient features embedded in the F-D curves and the DIC fields. In this paper, it is demonstrated that Principal Component Analysis (PCA) provides low-dimensional representations that make it possible to apply the BI framework to the problem. Most importantly, it is shown that the sequence in which the BI is applied has a dramatic influence on the results obtained. Specifically, it is observed that applying BI first on F-D curves and subsequently on the DIC fields produces improved estimates of the GTN parameters. Possible causes for these observations are discussed in this paper, using AA6111 aluminum alloy as a case study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01016v1</guid>
      <category>stat.CO</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Ali Seyed Mahmoud, Dominic Renner, Ali Khosravani, Surya R. Kalidindi</dc:creator>
    </item>
    <item>
      <title>Zero variance self-normalized importance sampling via estimating equations</title>
      <link>https://arxiv.org/abs/2510.00389</link>
      <description>arXiv:2510.00389v1 Announce Type: cross 
Abstract: In ordinary importance sampling with a nonnegative integrand there exists an importance sampling strategy with zero variance. Practical sampling strategies are often based on approximating that optimal solution, potentially approaching zero variance. There is a positivisation extension of that method to handle integrands that take both positive and negative values. Self-normalized importance sampling uses a ratio estimate, for which the optimal sampler does not have zero variance and so zero variance cannot even be approached in practice. Strategies that separately estimate the numerator and denominator of that ratio can approach zero variance. This paper develops another zero variance solution for self-normalized importance sampling. The first step is to write the desired expectation as the zero of an estimating equation using Fieller's technique. Then we apply the positivisation strategy to the estimating equation. This paper give conditions for existence and uniqueness of the sample solution to the estimating equation. Then it give conditions for consistency and asymptotic normality and an expression for the asymptotic variance. The sample size multiplied by the variance of the asymptotic formula becomes arbitrarily close to zero for certain sampling strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00389v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Art B. Owen</dc:creator>
    </item>
    <item>
      <title>Approximation of differential entropy in Bayesian optimal experimental design</title>
      <link>https://arxiv.org/abs/2510.00734</link>
      <description>arXiv:2510.00734v1 Announce Type: cross 
Abstract: Bayesian optimal experimental design provides a principled framework for selecting experimental settings that maximize obtained information. In this work, we focus on estimating the expected information gain in the setting where the differential entropy of the likelihood is either independent of the design or can be evaluated explicitly. This reduces the problem to maximum entropy estimation, alleviating several challenges inherent in expected information gain computation.
  Our study is motivated by large-scale inference problems, such as inverse problems, where the computational cost is dominated by expensive likelihood evaluations. We propose a computational approach in which the evidence density is approximated by a Monte Carlo or quasi-Monte Carlo surrogate, while the differential entropy is evaluated using standard methods without additional likelihood evaluations. We prove that this strategy achieves convergence rates that are comparable to, or better than, state-of-the-art methods for full expected information gain estimation, particularly when the cost of entropy evaluation is negligible. Moreover, our approach relies only on mild smoothness of the forward map and avoids stronger technical assumptions required in earlier work. We also present numerical experiments, which confirm our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00734v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chuntao Chen, Tapio Helin, Nuutti Hyv\"onen, Yuya Suzuki</dc:creator>
    </item>
    <item>
      <title>Parallel-in-time quantum simulation via Page and Wootters quantum time</title>
      <link>https://arxiv.org/abs/2308.12944</link>
      <description>arXiv:2308.12944v3 Announce Type: replace-cross 
Abstract: In the past few decades, researchers have created a veritable zoo of quantum algorithms by drawing inspiration from classical computing, information theory, and even from physical phenomena. Here we present quantum algorithms for parallel-in-time simulations that are inspired by the Page and Wootters formalism. In this framework, and thus in our algorithms, the classical time-variable of quantum mechanics is promoted to the quantum realm by introducing a Hilbert space of ``clock'' qubits which are then entangled with the ``system'' qubits. We show that our algorithms can compute temporal properties over $N$ different times of many-body systems by only using $\log(N)$ clock qubits. As such, we achieve an exponential trade-off between time and spatial complexities. In addition, we rigorously prove that the entanglement created between the system qubits and the clock qubits has operational meaning, as it encodes valuable information about the system's dynamics. We also provide a circuit depth estimation of all the protocols, showing a running time advantage in computation times over traditional sequential-in-time algorithms. In particular, for the case when the dynamics are determined by the Aubry--Andre model, we present a hybrid method for which our algorithms have a depth that only scales as $\mathcal{O}(\log(N)n)$. As a by-product, we can relate the previous schemes to the problem of equilibration of an isolated quantum system, thus indicating that our framework enables a new dimension for studying dynamical properties of many-body systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12944v3</guid>
      <category>quant-ph</category>
      <category>stat.CO</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/wpnf-4nnn</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. Research 7, 033294 (2025)</arxiv:journal_reference>
      <dc:creator>N. L. Diaz, Paolo Braccia, Martin Larocca, J. M. Matera, R. Rossignoli, M. Cerezo</dc:creator>
    </item>
  </channel>
</rss>
