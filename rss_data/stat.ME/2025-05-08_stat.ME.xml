<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 May 2025 01:43:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Learning Interactions Between Continuous Treatments and Covariates with a Semiparametric Model</title>
      <link>https://arxiv.org/abs/2505.03893</link>
      <description>arXiv:2505.03893v1 Announce Type: new 
Abstract: Estimating the impact of continuous treatment variables (e.g., dosage amount) on binary outcomes presents significant challenges in modeling and estimation because many existing approaches make strong assumptions that do not hold for certain continuous treatment variables. For instance, traditional logistic regression makes strong linearity assumptions that do not hold for continuous treatment variables like time of initiation. In this work, we propose a semiparametric regression framework that decomposes effects into two interpretable components: a prognostic score that captures baseline outcome risk based on a combination of clinical, genetic, and sociodemographic features, and a treatment-interaction score that flexibly models the optimal treatment level via a nonparametric link function. By connecting these two parametric scores with Nadaraya-Watson regression, our approach is both interpretable and flexible. The potential of our approach is demonstrated through numerical simulations that show empirical estimation convergence. We conclude by applying our approach to a real-world case study using the International Warfarin Pharmacogenomics Consortium (IWPC) dataset to show our approach's clinical utility by deriving personalized warfarin dosing recommendations that integrate both genetic and clinical data, providing insights towards enhancing patient safety and therapeutic efficacy in anticoagulation therapy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03893v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Muyan Jiang, Yunkai Zhang, Anil Aswani</dc:creator>
    </item>
    <item>
      <title>ROSE: Randomized Optimal Selection Design for Dose Optimization</title>
      <link>https://arxiv.org/abs/2505.03898</link>
      <description>arXiv:2505.03898v1 Announce Type: new 
Abstract: The U.S. Food and Drug Administration (FDA) launched Project Optimus to shift the objective of dose selection from the maximum tolerated dose to the optimal biological dose (OBD), optimizing the benefit-risk tradeoff. One approach recommended by the FDA's guidance is to conduct randomized trials comparing multiple doses. In this paper, using the selection design framework (Simon et al., 1985), we propose a randomized optimal selection (ROSE) design, which minimizes sample size while ensuring the probability of correct selection of the OBD at prespecified accuracy levels. The ROSE design is simple to implement, involving a straightforward comparison of the difference in response rates between two dose arms against a predetermined decision boundary. We further consider a two-stage ROSE design that allows for early selection of the OBD at the interim when there is sufficient evidence, further reducing the sample size. Simulation studies demonstrate that the ROSE design exhibits desirable operating characteristics in correctly identifying the OBD. A sample size of 15 to 40 patients per dosage arm typically results in a percentage of correct selection of the optimal dose ranging from 60% to 70%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03898v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuqi Wang, Ying Yuan, Suyu Liu</dc:creator>
    </item>
    <item>
      <title>Causal Inference in Counterbalanced Within-Subjects Designs</title>
      <link>https://arxiv.org/abs/2505.03937</link>
      <description>arXiv:2505.03937v1 Announce Type: new 
Abstract: Experimental designs are fundamental for estimating causal effects. In some fields, within-subjects designs, which expose participants to both control and treatment at different time periods, are used to address practical and logistical concerns. Counterbalancing, a common technique in within-subjects designs, aims to remove carryover effects by randomizing treatment sequences. Despite its appeal, counterbalancing relies on the assumption that carryover effects are symmetric and cancel out, which is often unverifiable a priori. In this paper, we formalize the challenges of counterbalanced within-subjects designs using the potential outcomes framework. We introduce sequential exchangeability as an additional identification assumption necessary for valid causal inference in these designs. To address identification concerns, we propose diagnostic checks, the use of washout periods, and covariate adjustments, and alternative experimental designs to counterbalanced within-subjects design. Our findings demonstrate the limitations of counterbalancing and provide guidance on when and how within-subjects designs can be appropriately used for causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03937v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Justin Ho, Jonathan Min</dc:creator>
    </item>
    <item>
      <title>Survey Experience and Nonresponse in an Online Probability Panel: A Survival Analysis</title>
      <link>https://arxiv.org/abs/2505.03971</link>
      <description>arXiv:2505.03971v1 Announce Type: new 
Abstract: We fit discrete time survival models to data from an online probability panel, where the outcome is the respondent first nonresponse to a survey invitation, following at least one previous survey completion. This approach has the advantage of utilising information about survey experiences over multiple survey waves, while accommodating the unbalanced data structure typical of OPPs, where the number, timing and content of survey invitations varies widely between panel members. We show that the nature and quality of previous survey experience has a strong influence on the propensity to respond to the next survey invitation. Longer surveys, reporting a survey as less enjoyable, a phone interview, and more days since the last survey invitation are found to be important predictors of nonresponse. We also find strong effects of personality on response propensity across survey invitations. Our findings have important implications for survey designers wishing to minimise nonresponse and attrition from OPPs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03971v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Katya Kostadintcheva, Jouni Kuha, Patrick Sturgis</dc:creator>
    </item>
    <item>
      <title>Batch Sequential Experimental Design for Calibration of Stochastic Simulation Models</title>
      <link>https://arxiv.org/abs/2505.03990</link>
      <description>arXiv:2505.03990v1 Announce Type: new 
Abstract: Calibration of expensive simulation models involves an emulator based on simulation outputs generated across various parameter settings to replace the actual model. Noisy outputs of stochastic simulation models require many simulation evaluations to understand the complex input-output relationship effectively. Sequential design with an intelligent data collection strategy can improve the efficiency of the calibration process. The growth of parallel computing environments can further enhance calibration efficiency by enabling simultaneous evaluation of the simulation model at a batch of parameters within a sequential design. This article proposes novel criteria that determine if a new batch of simulation evaluations should be assigned to existing parameter locations or unexplored ones to minimize the uncertainty of posterior prediction. Analysis of several simulated models and real-data experiments from epidemiology demonstrates that the proposed approach results in improved posterior predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03990v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\"Ozge S\"urer</dc:creator>
    </item>
    <item>
      <title>Estimating the Joint Distribution of Two Binary Variables with Marginal Statistics</title>
      <link>https://arxiv.org/abs/2505.03995</link>
      <description>arXiv:2505.03995v1 Announce Type: new 
Abstract: Clinical trial simulation (CTS) is critical in new drug development, providing insight into safety and efficacy while guiding trial design. Achieving realistic outcomes in CTS requires an accurately estimated joint distribution of the underlying variables. However, privacy concerns and data availability issues often restrict researchers to marginal summary-level data of each variable, making it challenging to estimate the joint distribution due to the lack of access to individual-level data or relational summaries between variables. We propose a novel approach based on the method of maximum likelihood that estimates the joint distribution of two binary variables using only marginal summary data. By leveraging numerical optimization and accommodating varying sample sizes across studies, our method preserves privacy while bypassing the need for granular or relational data. Through an extensive simulation study covering a diverse range of scenarios and an application to a real-world dataset, we demonstrate the accuracy, robustness, and practicality of our method. This method enhances the generation of realistic simulated data, thereby improving decision-making processes in drug development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03995v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Longwen Shang, Min Tsao, Xuekui Zhang</dc:creator>
    </item>
    <item>
      <title>Robust Variable Selection in High-dimensional Nonparametric Additive Model</title>
      <link>https://arxiv.org/abs/2505.04009</link>
      <description>arXiv:2505.04009v1 Announce Type: new 
Abstract: Additive models belong to the class of structured nonparametric regression models that do not suffer from the curse of dimensionality. Finding the additive components that are nonzero when the true model is assumed to be sparse is an important problem, and it is well studied in the literature. The majority of the existing methods focused on using the $L_2$ loss function, which is sensitive to outliers in the data. We propose a new variable selection method for additive models that is robust to outliers in the data. The proposed method employs a nonconcave penalty for variable selection and considers the framework of B-splines and density power divergence loss function for estimation. The loss function produces an M-estimator that down weights the effect outliers. Our asymptotic results are derived under the sub-Weibull assumption, which allows the error distribution to have an exponentially heavy tail. Under regularity conditions, we show that the proposed method achieves the optimal convergence rate. In addition, our results include the convergence rates for sub-Gaussian and sub-Exponential distributions as special cases. We numerically validate our theoretical findings using simulations and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04009v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suneel Babu Chatla, Abhijit Mandal</dc:creator>
    </item>
    <item>
      <title>Regularized Fingerprinting with Linearly Optimal Weight Matrix in Detection and Attribution of Climate Change</title>
      <link>https://arxiv.org/abs/2505.04070</link>
      <description>arXiv:2505.04070v1 Announce Type: new 
Abstract: Climate change detection and attribution plays a central role in establishing the causal influence of human activities on global warming. The most widely used framework, optimal fingerprinting, is a linear regression model with errors-in-variables (EIV), in which each covariate is subject to measurement error whose covariance matrix is the same as that of the regression error up to a known scale. The reliability of such detection and attribution analyses critically depends on accurate inference of the regression coefficients. The optimal weight matrix in estimating the regression coefficient is the precision matrix of the regression error, which is typically unknown and has to be estimated from climate model simulations with appropriate regularization. However, the estimators from the prevailing method, regularized optimal fingerprinting, are not optimal as believed, owing to the unreliable estimation of the optimal weight matrix, and their uncertainties are underestimated, leading to too narrow confidence intervals to match the nominal confidence levels. In this paper, we propose consistent estimates of the variances of regression coefficients for weight matrices within the class of linear shrinkage estimators. Building on this result, we derive a linearly optimal weight matrix that directly minimizes the asymptotic variances of the estimated scaling factors within the fingerprinting framework. Numerical studies confirm that the proposed method yields confidence intervals with empirical coverage rates close to the nominal level, while also achieving shorter average interval lengths. In applications to the detection and attribution analyses of annual mean near-surface air temperature at the global, continental, and subcontinental scales during 1951--2020, the proposed method produced shorter confidence intervals than the existing approaches in most of the analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04070v1</guid>
      <category>stat.ME</category>
      <category>physics.ao-ph</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoran Li, Yan Li</dc:creator>
    </item>
    <item>
      <title>Time-lagged marginal expected shortfall</title>
      <link>https://arxiv.org/abs/2505.04243</link>
      <description>arXiv:2505.04243v1 Announce Type: new 
Abstract: Marginal expected shortfall (MES) is an important measure when assessing and quantifying the contribution of the financial institution to a systemic crisis. In this paper, we propose time-lagged marginal expected shortfall (TMES) as a dynamic extension of the MES, accounting for time lags in assessing systemic risks. A natural estimator for the TMES is proposed, and its asymptotic properties are studied. To address challenges in constructing confidence intervals for the TMES in practice, we apply the stationary bootstrap method to generate confidence bands for the TMES estimator. Extensive simulation studies were conducted to investigate the asymptotic properties of empirical and bootstrapped TMES. Two practical applications of TMES, supported by real data analyses, effectively demonstrate its ability to account for time lags in risk assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04243v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiajun Liu, Xuannan Liu, Yuwei Zhao</dc:creator>
    </item>
    <item>
      <title>Conformal Survival Bands for Risk Screening under Right-Censoring</title>
      <link>https://arxiv.org/abs/2505.04568</link>
      <description>arXiv:2505.04568v1 Announce Type: new 
Abstract: We propose a method to quantify uncertainty around individual survival distribution estimates using right-censored data, compatible with any survival model. Unlike classical confidence intervals, the survival bands produced by this method offer predictive rather than population-level inference, making them useful for personalized risk screening. For example, in a low-risk screening scenario, they can be applied to flag patients whose survival band at 12 months lies entirely above 50\%, while ensuring that at least half of flagged individuals will survive past that time on average. Our approach builds on recent advances in conformal inference and integrates ideas from inverse probability of censoring weighting and multiple testing with false discovery rate control. We provide asymptotic guarantees and show promising performance in finite samples with both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04568v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Sesia, Vladimir Svetnik</dc:creator>
    </item>
    <item>
      <title>Likelihood-Free Adaptive Bayesian Inference via Nonparametric Distribution Matching</title>
      <link>https://arxiv.org/abs/2505.04603</link>
      <description>arXiv:2505.04603v1 Announce Type: new 
Abstract: When the likelihood is analytically unavailable and computationally intractable, approximate Bayesian computation (ABC) has emerged as a widely used methodology for approximate posterior inference; however, it suffers from severe computational inefficiency in high-dimensional settings or under diffuse priors. To overcome these limitations, we propose Adaptive Bayesian Inference (ABI), a framework that bypasses traditional data-space discrepancies and instead compares distributions directly in posterior space through nonparametric distribution matching. By leveraging a novel Marginally-augmented Sliced Wasserstein (MSW) distance on posterior measures and exploiting its quantile representation, ABI transforms the challenging problem of measuring divergence between posterior distributions into a tractable sequence of one-dimensional conditional quantile regression tasks. Moreover, we introduce a new adaptive rejection sampling scheme that iteratively refines the posterior approximation by updating the proposal distribution via generative density estimation. Theoretically, we establish parametric convergence rates for the trimmed MSW distance and prove that the ABI posterior converges to the true posterior as the tolerance threshold vanishes. Through extensive empirical evaluation, we demonstrate that ABI significantly outperforms data-based Wasserstein ABC, summary-based ABC, and state-of-the-art likelihood-free simulators, especially in high-dimensional or dependent observation regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04603v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenhui Sophia Lu, Wing Hung Wong</dc:creator>
    </item>
    <item>
      <title>On the sensitivity of different ensemble filters to the type of assimilated observation networks</title>
      <link>https://arxiv.org/abs/2505.04541</link>
      <description>arXiv:2505.04541v1 Announce Type: cross 
Abstract: Recent advances in data assimilation (DA) have focused on developing more flexible approaches that can better accommodate nonlinearities in models and observations. However, it remains unclear how the performance of these advanced methods depends on the observation network characteristics. In this study, we present initial experiments with the surface quasi-geostrophic model, in which we compare a recently developed AI-based ensemble filter with the standard Local Ensemble Transform Kalman Filter (LETKF). Our results show that the analysis solutions respond differently to the number, spatial distribution, and nonlinear fraction of assimilated observations. We also find notable changes in the multiscale characteristics of the analysis errors. Given that standard DA techniques will be eventually replaced by more advanced methods, we hope this study sets the ground for future efforts to reassess the value of Earth observation systems in the context of newly emerging algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04541v1</guid>
      <category>physics.ao-ph</category>
      <category>nlin.CD</category>
      <category>stat.ME</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zixiang Xiong, Siming Liang, Feng Bao, Guannan Zhang, Hristo G. Chipilski</dc:creator>
    </item>
    <item>
      <title>Particle Gibbs without the Gibbs bit</title>
      <link>https://arxiv.org/abs/2505.04611</link>
      <description>arXiv:2505.04611v2 Announce Type: cross 
Abstract: Exact parameter and trajectory inference in state-space models is typically achieved by one of two methods: particle marginal Metropolis-Hastings (PMMH) or particle Gibbs (PGibbs). PMMH is a pseudo-marginal algorithm which jointly proposes a new trajectory and parameter, and accepts or rejects both at once. PGibbs instead alternates between sampling from the trajectory, using an algorithm known as conditional sequential Monte Carlo (CSMC) and the parameter in a Hastings-within-Gibbs fashion. While particle independent Metropolis Hastings (PIMH), the parameter-free version of PMMH, is known to be statistically worse than CSMC, PGibbs can induce a slow mixing if the parameter and the state trajectory are very correlated. This has made PMMH the method of choice for many practitioners, despite theory and experiments favouring CSMC over PIMH for the parameter-free problem. In this article, we describe a formulation of PGibbs which bypasses the Gibbs step, essentially marginalizing over the trajectory distribution in a fashion similar to PMMH. This is achieved by considering the implementation of a CSMC algortihm for the state-space model integrated over the joint distribution of the current parameter and the parameter proposal. We illustrate the benefits of method on a simple example known to be challenging for PMMH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04611v2</guid>
      <category>stat.CO</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrien Corenflos</dc:creator>
    </item>
    <item>
      <title>A Computationally Efficient Approach to False Discovery Rate Control and Power Maximisation via Randomisation and Mirror Statistic</title>
      <link>https://arxiv.org/abs/2401.12697</link>
      <description>arXiv:2401.12697v2 Announce Type: replace 
Abstract: Simultaneously performing variable selection and inference in high-dimensional regression models is an open challenge in statistics and machine learning. The increasing availability of vast amounts of variables requires the adoption of specific statistical procedures to accurately select the most important predictors in a high-dimensional space, while controlling the false discovery rate (FDR) associated with the variable selection procedure. In this paper, we propose the joint adoption of the Mirror Statistic approach to FDR control, coupled with outcome randomisation to maximise the statistical power of the variable selection procedure, measured through the true positive rate. Through extensive simulations, we show how our proposed strategy allows us to combine the benefits of the two techniques. The Mirror Statistic is a flexible method to control FDR, which only requires mild model assumptions, but requires two sets of independent regression coefficient estimates, usually obtained after splitting the original dataset. Outcome randomisation is an alternative to data splitting that allows to generate two independent outcomes, which can then be used to estimate the coefficients that go into the construction of the Mirror Statistic. The combination of these two approaches provides increased testing power in a number of scenarios, such as highly correlated covariates and high percentages of active variables. Moreover, it is scalable to very high-dimensional problems, since the algorithm has a low memory footprint and only requires a single run on the full dataset, as opposed to iterative alternatives such as multiple data splitting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12697v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1177/09622802251329768</arxiv:DOI>
      <arxiv:journal_reference>Statistical Methods in Medical Research. 2025;0(0)</arxiv:journal_reference>
      <dc:creator>Marco Molinari, Magne Thoresen</dc:creator>
    </item>
    <item>
      <title>Dynamic Bayesian Networks with Conditional Dynamics in Edge Addition and Deletion</title>
      <link>https://arxiv.org/abs/2409.08965</link>
      <description>arXiv:2409.08965v2 Announce Type: replace 
Abstract: This study presents a dynamic Bayesian network framework that facilitates intuitive gradual edge changes. We use two conditional dynamics to model the edge addition and deletion, and edge selection separately. Unlike previous research that uses a mixture network approach, which restricts the number of possible edge changes, or structural priors to induce gradual changes, which can lead to unclear network evolution, our model induces more frequent and intuitive edge change dynamics. We employ Markov chain Monte Carlo (MCMC) sampling to estimate the model structures and parameters and demonstrate the model's effectiveness in a portfolio selection application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08965v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lupe S. H. Chan, Amanda M. Y. Chu, Mike K. P. So</dc:creator>
    </item>
    <item>
      <title>Enhancing Statistical Validity and Power in Hybrid Controlled Trials: A Randomization Inference Approach with Conformal Selective Borrowing</title>
      <link>https://arxiv.org/abs/2410.11713</link>
      <description>arXiv:2410.11713v3 Announce Type: replace 
Abstract: External controls from historical trials or observational data can augment randomized controlled trials when large-scale randomization is impractical or unethical, such as in drug evaluation for rare diseases. However, non-randomized external controls can introduce biases, and existing Bayesian and frequentist methods may inflate the type I error rate, particularly in small-sample trials where external data borrowing is most critical. To address these challenges, we propose a randomization inference framework that ensures finite-sample exact and model-free type I error rate control, adhering to the "analyze as you randomize" principle to safeguard against hidden biases. Recognizing that biased external controls reduce the power of randomization tests, we leverage conformal inference to develop an individualized test-then-pool procedure that selectively borrows comparable external controls to improve power. Our approach incorporates selection uncertainty into randomization tests, providing valid post-selection inference. Additionally, we propose an adaptive procedure to optimize the selection threshold by minimizing the mean squared error across a class of estimators encompassing both no-borrowing and full-borrowing approaches. The proposed methods are supported by non-asymptotic theoretical analysis, validated through simulations, and applied to a randomized lung cancer trial that integrates external controls from the National Cancer Database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11713v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ke Zhu, Shu Yang, Xiaofei Wang</dc:creator>
    </item>
    <item>
      <title>Improved subsample-and-aggregate via the private modified winsorized mean</title>
      <link>https://arxiv.org/abs/2501.14095</link>
      <description>arXiv:2501.14095v3 Announce Type: replace 
Abstract: We develop a univariate, differentially private mean estimator, called the private modified winsorized mean, designed to be used as the aggregator in subsample-and-aggregate. We demonstrate, via real data analysis, that common differentially private multivariate mean estimators may not perform well as the aggregator, even in large datasets, motivating our developments.We show that the modified winsorized mean is minimax optimal for several, large classes of distributions, even under adversarial contamination. We also demonstrate that, empirically, the private modified winsorized mean performs well compared to other private mean estimates. We consider the modified winsorized mean as the aggregator in subsample-and-aggregate, deriving a finite sample deviations bound for a subsample-and-aggregate estimate generated with the new aggregator. This result yields two important insights: (i) the optimal choice of subsamples depends on the bias of the estimator computed on the subsamples, and (ii) the rate of convergence of the subsample-and-aggregate estimator depends on the robustness of the estimator computed on the subsamples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14095v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kelly Ramsay, Dylan Spicker</dc:creator>
    </item>
    <item>
      <title>Compatibility of Missing Data Handling Methods across the Stages of Producing Clinical Prediction Models</title>
      <link>https://arxiv.org/abs/2504.06799</link>
      <description>arXiv:2504.06799v2 Announce Type: replace 
Abstract: Missing data is a challenge when developing, validating and deploying clinical prediction models (CPMs). Traditionally, decisions concerning missing data handling during CPM development and validation havent accounted for whether missingness is allowed at deployment. We hypothesised that the missing data approach used during model development should optimise model performance upon deployment, whilst the approach used during model validation should yield unbiased predictive performance estimates upon deployment; we term this compatibility. We aimed to determine which combinations of missing data handling methods across the CPM life cycle are compatible. We considered scenarios where CPMs are intended to be deployed with missing data allowed or not, and we evaluated the impact of that choice on earlier modelling decisions. Through a simulation study and an empirical analysis of thoracic surgery data, we compared CPMs developed and validated using combinations of complete case analysis, mean imputation, single regression imputation, multiple imputation, and pattern sub-modelling. If planning to deploy a CPM without allowing missing data, then development and validation should use multiple imputation when required. Where missingness is allowed at deployment, the same imputation method must be used during development and validation. Commonly used combinations of missing data handling methods result in biased predictive performance estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06799v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonia Tsvetanova, Matthew Sperrin, David A. Jenkins, Niels Peek, Iain Buchan, Stephanie Hyland, Marcus Taylor, Angela Wood, Richard D. Riley, Glen P. Martin</dc:creator>
    </item>
    <item>
      <title>Functional Partial Least-Squares: Adaptive Estimation and Inference</title>
      <link>https://arxiv.org/abs/2402.11134</link>
      <description>arXiv:2402.11134v2 Announce Type: replace-cross 
Abstract: We study the functional linear regression model with a scalar response and a Hilbert space-valued predictor, a canonical example of an ill-posed inverse problem. We show that the functional partial least squares (PLS) estimator attains nearly minimax-optimal convergence rates over a class of ellipsoids and propose an adaptive early stopping procedure for selecting the number of PLS components. In addition, we develop new test that can detect local alternatives converging at the parametric rate which can be inverted to construct confidence sets. Simulation results demonstrate that the estimator performs favorably relative to several existing methods and the proposed test exhibits good power properties. We apply our methodology to evaluate the nonlinear effects of temperature on corn and soybean yields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11134v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrii Babii, Marine Carrasco, Idriss Tsafack</dc:creator>
    </item>
    <item>
      <title>Double Cross-fit Doubly Robust Estimators: Beyond Series Regression</title>
      <link>https://arxiv.org/abs/2403.15175</link>
      <description>arXiv:2403.15175v3 Announce Type: replace-cross 
Abstract: Doubly robust estimators with cross-fitting have gained popularity in causal inference due to their favorable structure-agnostic error guarantees. However, when additional structure, such as H\"{o}lder smoothness, is available then more accurate "double cross-fit doubly robust" (DCDR) estimators can be constructed by splitting the training data and undersmoothing nuisance function estimators on independent samples. We study a DCDR estimator of the Expected Conditional Covariance, a functional of interest in causal inference and conditional independence testing. We first provide a structure-agnostic error analysis for the DCDR estimator with no assumptions on the nuisance functions or their estimators. Then, assuming the nuisance functions are H\"{o}lder smooth, but without assuming knowledge of the true smoothness level or the covariate density, we establish that DCDR estimators with several linear smoothers are $\sqrt{n}$-consistent and asymptotically normal under minimal conditions and achieve fast convergence rates in the non-$\sqrt{n}$ regime. When the covariate density and smoothnesses are known, we propose a minimax rate-optimal DCDR estimator based on undersmoothed kernel regression. Moreover, we show an undersmoothed DCDR estimator satisfies a slower-than-$\sqrt{n}$ central limit theorem, and that inference is possible even in the non-$\sqrt{n}$ regime. Finally, we support our theoretical results with simulations, providing intuition for double cross-fitting and undersmoothing, demonstrating where our estimator achieves $\sqrt{n}$-consistency while the usual "single cross-fit" estimator fails, and illustrating asymptotic normality for the undersmoothed DCDR estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15175v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alec McClean, Sivaraman Balakrishnan, Edward H. Kennedy, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Cross-sectional Dependence in Idiosyncratic Volatility</title>
      <link>https://arxiv.org/abs/2408.13437</link>
      <description>arXiv:2408.13437v2 Announce Type: replace-cross 
Abstract: This paper introduces an econometric framework for analyzing cross-sectional dependence in the idiosyncratic volatilities of assets using high frequency data. We first consider the estimation of standard measures of dependence in the idiosyncratic volatilities such as covariances and correlations. Naive estimators of these measures are biased due to the use of the error-laden estimates of idiosyncratic volatilities. We provide bias-corrected estimators and the relevant asymptotic theory. Next, we introduce an idiosyncratic volatility factor model, in which we decompose the variation in idiosyncratic volatilities into two parts: the variation related to the systematic factors such as the market volatility, and the residual variation. Again, naive estimators of the decomposition are biased, and we provide bias-corrected estimators. We also provide the asymptotic theory that allows us to test whether the residual (non-systematic) components of the idiosyncratic volatilities exhibit cross-sectional dependence. We apply our methodology to the S&amp;P 100 index constituents, and document strong cross-sectional dependence in their idiosyncratic volatilities. We consider two different sets of idiosyncratic volatility factors, and find that neither can fully account for the cross-sectional dependence in idiosyncratic volatilities. For each model, we map out the network of dependencies in residual (non-systematic) idiosyncratic volatilities across all stocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13437v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jeconom.2025.106003</arxiv:DOI>
      <dc:creator>Ilze Kalnina, Kokouvi Tewou</dc:creator>
    </item>
    <item>
      <title>Non-linear dependence and Granger causality: A vine copula approach</title>
      <link>https://arxiv.org/abs/2409.15070</link>
      <description>arXiv:2409.15070v2 Announce Type: replace-cross 
Abstract: Inspired by Jang et al. (2022), we propose a Granger causality-in-the-mean test for bivariate $k-$Markov stationary processes based on a recently introduced class of non-linear models, i.e., vine copula models. By means of a simulation study, we show that the proposed test improves on the statistical properties of the original test in Jang et al. (2022), and also of other previous methods, constituting an excellent tool for testing Granger causality in the presence of non-linear dependence structures. Finally, we apply our test to study the pairwise relationships between energy consumption, GDP and investment in the U.S. and, notably, we find that Granger-causality runs two ways between GDP and energy consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15070v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Roberto Fuentes-Mart\'inez, Irene Crimaldi, Armando Rungi</dc:creator>
    </item>
    <item>
      <title>Stochastic Subspace via Probabilistic Principal Component Analysis for Characterizing Model Error</title>
      <link>https://arxiv.org/abs/2504.19963</link>
      <description>arXiv:2504.19963v2 Announce Type: replace-cross 
Abstract: This paper proposes a probabilistic model of subspaces based on the probabilistic principal component analysis (PCA). Given a sample of vectors in the embedding space -- commonly known as a snapshot matrix -- this method uses quantities derived from the probabilistic PCA to construct distributions of the sample matrix, as well as the principal subspaces. It is applicable to projection-based reduced-order modeling methods, such as proper orthogonal decomposition and related model reduction methods. The stochastic subspace thus constructed can be used, for example, to characterize model-form uncertainty in computational mechanics. The proposed method has multiple desirable properties: (1) it is naturally justified by the probabilistic PCA and has analytic forms for the induced random matrix models; (2) it satisfies linear constraints, such as boundary conditions of all kinds, by default; (3) it has only one hyperparameter, which significantly simplifies training; and (4) its algorithm is very easy to implement. We demonstrate the performance of the proposed method via several numerical examples in computational mechanics and structural dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19963v2</guid>
      <category>cs.CE</category>
      <category>math.ST</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 08 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akash Yadav, Ruda Zhang</dc:creator>
    </item>
  </channel>
</rss>
