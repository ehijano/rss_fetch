<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Jun 2025 04:01:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Fairly Wired: Towards Leximin-Optimal Division of Electricity</title>
      <link>https://arxiv.org/abs/2506.02193</link>
      <description>arXiv:2506.02193v1 Announce Type: new 
Abstract: In many parts of the world - particularly in developing countries - the demand for electricity exceeds the available supply. In such cases, it is impossible to provide electricity to all households simultaneously. This raises a fundamental question: how should electricity be allocated fairly? In this paper, we explore this question through the lens of egalitarianism - a principle that emphasizes equality by prioritizing the welfare of the worst-off households. One natural rule that aligns with this principle is to maximize the egalitarian welfare - the smallest utility across all households. We show that computing such an allocation is NP-hard, even under strong simplifying assumptions. Leximin is a stronger fairness notion that generalizes the egalitarian welfare: it also requires to maximize the smallest utility, but then, subject to that, the second-smallest, then the third, and so on. The hardness results extends directly to leximin as well. Despite this, we present a Fully Polynomial-Time Approximation Scheme (FPTAS) for leximin in the special case where the network connectivity graph is a tree. This means that we can efficiently approximate leximin - and, in particular, the egalitarian welfare - to any desired level of accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02193v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>cs.MA</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eden Hartman, Dinesh Kumar Baghel, Erel Segal-Halevi</dc:creator>
    </item>
    <item>
      <title>Stochastically Dominant Peer Prediction</title>
      <link>https://arxiv.org/abs/2506.02259</link>
      <description>arXiv:2506.02259v1 Announce Type: new 
Abstract: Eliciting reliable human feedback is essential for many machine learning tasks, such as learning from noisy labels and aligning AI systems with human preferences. Peer prediction mechanisms incentivize truthful reporting without ground truth verification by scoring agents based on correlations with peers. Traditional mechanisms, which ensure that truth-telling maximizes the expected scores in equilibrium, can elicit honest information while assuming agents' utilities are linear functions of their scores. However, in practice, non-linear payment rules are usually preferred, or agents' utilities are inherently non-linear.
  We propose stochastically dominant truthfulness (SD-truthfulness) as a stronger guarantee: the score distribution of truth-telling stochastically dominates all other strategies, incentivizing truthful reporting for a wide range of monotone utility functions. Our first observation is that no existing peer prediction mechanism naturally satisfies this criterion without strong assumptions. A simple solution -- rounding scores into binary lotteries -- can enforce SD-truthfulness, but often degrades sensitivity, a key property related to fairness and statistical efficiency. We demonstrate how a more careful application of rounding can better preserve sensitivity. Furthermore, we introduce a new enforced agreement (EA) mechanism that is theoretically guaranteed to be SD-truthful in binary-signal settings under mild assumptions, and empirically achieves the highest sensitivity among all known SD-truthful mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02259v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yichi Zhang, Shengwei Xu, David Pennock, Grant Schoenebeck</dc:creator>
    </item>
    <item>
      <title>Learning Optimal Posted Prices for a Unit-Demand Buyer</title>
      <link>https://arxiv.org/abs/2506.02284</link>
      <description>arXiv:2506.02284v1 Announce Type: new 
Abstract: We study the problem of learning the optimal item pricing for a unit-demand buyer with independent item values, and the learner has query access to the buyer's value distributions. We consider two common query models in the literature: the sample access model where the learner can obtain a sample of each item value, and the pricing query model where the learner can set a price for an item and obtain a binary signal on whether the sampled value of the item is greater than our proposed price. In this work, we give nearly tight sample complexity and pricing query complexity of the unit-demand pricing problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02284v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifeng Teng, Yifan Wang</dc:creator>
    </item>
    <item>
      <title>A Transformer-Based Neural Network for Optimal Deterministic-Allocation and Anonymous Joint Auction Design</title>
      <link>https://arxiv.org/abs/2506.02435</link>
      <description>arXiv:2506.02435v1 Announce Type: new 
Abstract: With the advancement of machine learning, an increasing number of studies are employing automated mechanism design (AMD) methods for optimal auction design. However, all previous AMD architectures designed to generate optimal mechanisms that satisfy near dominant strategy incentive compatibility (DSIC) fail to achieve deterministic allocation, and some also lack anonymity, thereby impacting the efficiency and fairness of advertising allocation. This has resulted in a notable discrepancy between the previous AMD architectures for generating near-DSIC optimal mechanisms and the demands of real-world advertising scenarios. In this paper, we prove that in all online advertising scenarios, previous non-deterministic allocation methods lead to the non-existence of feasible solutions, resulting in a gap between the rounded solution and the optimal solution. Furthermore, we propose JTransNet, a transformer-based neural network architecture, designed for optimal deterministic-allocation and anonymous joint auction design. Although the deterministic allocation module in JTransNet is designed for the latest joint auction scenarios, it can be applied to other non-deterministic AMD architectures with minor modifications. Additionally, our offline and online data experiments demonstrate that, in joint auction scenarios, JTransNet significantly outperforms baseline methods in terms of platform revenue, resulting in a substantial increase in platform earnings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02435v1</guid>
      <category>cs.GT</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhen Zhang, Luowen Liu, Wanzhi Zhang, Zitian Guo, Kun Huang, Qi Qi, Qiang Liu, Xingxing Wang</dc:creator>
    </item>
    <item>
      <title>Branch-and-Cut for Mixed-Integer Generalized Nash Equilibrium Problems</title>
      <link>https://arxiv.org/abs/2506.02520</link>
      <description>arXiv:2506.02520v1 Announce Type: new 
Abstract: Generalized Nash equilibrium problems with mixed-integer variables form an important class of games in which each player solves a mixed-integer optimization problem with respect to her own variables and the strategy space of each player depends on the strategies chosen by the rival players. In this work, we introduce a branch-and-cut algorithm to compute exact pure Nash equilibria for different classes of such mixed-integer games. The main idea is to reformulate the equilibrium problem as a suitable bilevel problem based on the Nikaido--Isoda function of the game. The proposed branch-and-cut method is applicable to generalized Nash equilibrium problems under quite mild assumptions. Depending on the specific setting, we use tailored equilibrium or intersection cuts. The latter are well-known in mixed-integer linear optimization and we adapt them to the game setting. We prove finite termination and correctness of the algorithm and present some first numerical results for two different types of knapsack games and another game based on capacitated flow problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02520v1</guid>
      <category>cs.GT</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alo\"is Duguet, Tobias Harks, Martin Schmidt, Julian Schwarz</dc:creator>
    </item>
    <item>
      <title>Computational adversarial risk analysis for general security games</title>
      <link>https://arxiv.org/abs/2506.02603</link>
      <description>arXiv:2506.02603v1 Announce Type: new 
Abstract: This paper provides an efficient computational scheme to handle general security games from an adversarial risk analysis perspective. Two cases in relation to single-stage and multi-stage simultaneous defend-attack games motivate our approach to general setups which uses bi-agent influence diagrams as underlying problem structure and augmented probability simulation as core computational methodology. Theoretical convergence and numerical, modeling, and implementation issues are thoroughly discussed. A disinformation war case study illustrates the relevance of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02603v1</guid>
      <category>cs.GT</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jose Manuel Camacho, Roi Naveiro, David Rios Insua</dc:creator>
    </item>
    <item>
      <title>The power of mediators: Price of anarchy and stability in Bayesian games with submodular social welfare</title>
      <link>https://arxiv.org/abs/2506.02655</link>
      <description>arXiv:2506.02655v1 Announce Type: new 
Abstract: This paper investigates the role of mediators in Bayesian games by examining their impact on social welfare through the price of anarchy (PoA) and price of stability (PoS). Mediators can communicate with players to guide them toward equilibria of varying quality, and different communication protocols lead to a variety of equilibrium concepts collectively known as Bayes (coarse) correlated equilibria. To analyze these equilibrium concepts, we consider a general class of Bayesian games with submodular social welfare, which naturally extends valid utility games and their variant, basic utility games. These frameworks, introduced by Vetta (2002), have been developed to analyze the social welfare guarantees of equilibria in games such as competitive facility location, influence maximization, and other resource allocation problems.
  We provide upper and lower bounds on the PoA and PoS for a broad class of Bayes (coarse) correlated equilibria. Central to our analysis is the strategy representability gap, which measures the multiplicative gap between the optimal social welfare achievable with and without knowledge of other players' types. For monotone submodular social welfare functions, we show that this gap is $1-1/\mathrm{e}$ for independent priors and $\Theta(1/\sqrt{n})$ for correlated priors, where $n$ is the number of players. These bounds directly lead to upper and lower bounds on the PoA and PoS for various equilibrium concepts, while we also derive improved bounds for specific concepts by developing smoothness arguments. Notably, we identify a fundamental gap in the PoA and PoS across different classes of Bayes correlated equilibria, highlighting essential distinctions among these concepts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02655v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaito Fujii</dc:creator>
    </item>
    <item>
      <title>Proportional Response Dynamics in Gross Substitutes Markets</title>
      <link>https://arxiv.org/abs/2506.02852</link>
      <description>arXiv:2506.02852v1 Announce Type: new 
Abstract: Proportional response is a well-established distributed algorithm which has been shown to converge to competitive equilibria in both Fisher and Arrow-Debreu markets, for various sub-families of homogeneous utilities, including linear and constant elasticity of substitution utilities. We propose a natural generalization of proportional response for gross substitutes utilities, and prove that it converges to competitive equilibria in Fisher markets. This is the first convergence result of a proportional response style dynamics in Fisher markets for utilities beyond the homogeneous utilities covered by the Eisenberg-Gale convex program. We show an empirical convergence rate of $O(1/T)$ for the prices. Furthermore, we show that the allocations of a lazy version of the generalized proportional response dynamics converge to competitive equilibria in Arrow-Debreu markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02852v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yun Kuen Cheung, Richard Cole, Yixin Tao</dc:creator>
    </item>
    <item>
      <title>Dynamic Fee for Reducing Impermanent Loss in Decentralized Exchanges</title>
      <link>https://arxiv.org/abs/2506.03001</link>
      <description>arXiv:2506.03001v1 Announce Type: new 
Abstract: Decentralized exchanges (DEXs) are crucial to decentralized finance (DeFi) as they enable trading without intermediaries. However, they face challenges like impermanent loss (IL), where liquidity providers (LPs) see their assets' value change unfavorably within a liquidity pool compared to outside it. To tackle these issues, we propose dynamic fee mechanisms over traditional fixed-fee structures used in automated market makers (AMM). Our solution includes asymmetric fees via block-adaptive, deal-adaptive, and the "ideal but unattainable" oracle-based fee algorithm, utilizing all data available to arbitrageurs to mitigate IL. We developed a simulation-based framework to compare these fee algorithms systematically. This framework replicates trading on a DEX, considering both informed and uninformed users and a psychological relative loss factor. Results show that adaptive algorithms outperform fixed-fee baselines in reducing IL while maintaining trading activity among uninformed users. Additionally, insights from oracle-based performance underscore the potential of dynamic fee strategies to lower IL, boost LP profitability, and enhance overall market efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03001v1</guid>
      <category>cs.GT</category>
      <category>cs.DC</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Irina Lebedeva, Dmitrii Umnov, Yury Yanovich, Ignat Melnikov, George Ovchinnikov</dc:creator>
    </item>
    <item>
      <title>Designing Algorithmic Delegates: The Role of Indistinguishability in Human-AI Handoff</title>
      <link>https://arxiv.org/abs/2506.03102</link>
      <description>arXiv:2506.03102v1 Announce Type: new 
Abstract: As AI technologies improve, people are increasingly willing to delegate tasks to AI agents. In many cases, the human decision-maker chooses whether to delegate to an AI agent based on properties of the specific instance of the decision-making problem they are facing. Since humans typically lack full awareness of all the factors relevant to this choice for a given decision-making instance, they perform a kind of categorization by treating indistinguishable instances -- those that have the same observable features -- as the same. In this paper, we define the problem of designing the optimal algorithmic delegate in the presence of categories. This is an important dimension in the design of algorithms to work with humans, since we show that the optimal delegate can be an arbitrarily better teammate than the optimal standalone algorithmic agent. The solution to this optimal delegation problem is not obvious: we discover that this problem is fundamentally combinatorial, and illustrate the complex relationship between the optimal design and the properties of the decision-making task even in simple settings. Indeed, we show that finding the optimal delegate is computationally hard in general. However, we are able to find efficient algorithms for producing the optimal delegate in several broad cases of the problem, including when the optimal action may be decomposed into functions of features observed by the human and the algorithm. Finally, we run computational experiments to simulate a designer updating an algorithmic delegate over time to be optimized for when it is actually adopted by users, and show that while this process does not recover the optimal delegate in general, the resulting delegate often performs quite well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03102v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sophie Greenwood, Karen Levy, Solon Barocas, Hoda Heidari, Jon Kleinberg</dc:creator>
    </item>
    <item>
      <title>An Isotonic Mechanism for Overlapping Ownership</title>
      <link>https://arxiv.org/abs/2306.11154</link>
      <description>arXiv:2306.11154v3 Announce Type: replace 
Abstract: Motivated by the problem of improving peer review at large scientific conferences, this paper studies how to elicit self-evaluations to improve review scores in a natural many-to-many owner-item (e.g., author-paper) situation with overlapping ownership. We design a simple, efficient and truthful mechanism to elicit self-evaluations from item owners that can be used to calibrate their noisy review scores in the existing evaluation process (e.g., papers' review scores from peers).
  Our approach starts by partitioning the owner-item relation structure into disjoint blocks, each sharing a common set of co-owners. We then elicit the ranking of items from each owner and employ isotonic regression to produce adjusted item scores, aligning with both the reported rankings and raw item review scores. We prove that truth-telling by all owners is a payoff dominant Nash equilibrium for any valid partition of the overlapping ownership sets under natural conditions. Moreover, the truthfulness depends on eliciting rankings independently within each block, making block partition optimization crucial for improving statistical efficiency. Despite being computationally intractable in general, we develop a nearly linear-time greedy algorithm that provably finds a performant block partition with appealing robust approximation guarantees. Extensive experiments on both synthetic data and real-world conference review data demonstrate the effectiveness of our mechanism in a pressing real-world problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11154v3</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jibang Wu, Haifeng Xu, Yifan Guo, Weijie Su</dc:creator>
    </item>
    <item>
      <title>Grace Period is All You Need: Individual Fairness without Revenue Loss in Revenue Management</title>
      <link>https://arxiv.org/abs/2402.08533</link>
      <description>arXiv:2402.08533v3 Announce Type: replace 
Abstract: Imagine you and a friend purchase identical items at a store, yet only your friend received a discount. Would your friend's discount make you feel unfairly treated by the store? And would you be less willing to purchase from that store again in the future? Based on a large-scale online survey that we ran on Prolific, it turns out that the answers to the above questions are positive. Therefore, when allocating resources to different customers, sellers should consider both the total reward and individual fairness. Motivated by these findings, in this work we propose a notion of individual fairness in online revenue management and an algorithmic module (called ``Grace Period'') that can be embedded in traditional revenue management algorithms and guarantee individual fairness. Specifically, we show how to embed the Grace Period in five common revenue management algorithms including Deterministic Linear Programming with Probabilistic Assignment, Resolving Deterministic Linear Programming with Probabilistic Assignment, Static Bid Price Control, Booking Limit, and Nesting, thus covering both stochastic and adversarial customer arrival settings. Embedding the Grace Period does not incur additional regret for any of these algorithms. This finding indicates that, in an asymptotic regime, there is no tradeoff between a seller maximizing their revenue and guaranteeing that each customer feels fairly treated. The core intuition behind the Grace Period is that independent randomized decisions for each customer often lead to unfair outcomes. However, we cannot eliminate the randomness, as it plays a crucial role in maximizing profit. The Grace Period addresses this by shifting randomness away from individual decisions and applying it instead to the total number of customers receiving a particular decision. This approach preserves revenue potential while mitigating fairness issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08533v3</guid>
      <category>cs.GT</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Jaillet, Chara Podimata, Zijie Zhou</dc:creator>
    </item>
    <item>
      <title>Flexible Demand Manipulation</title>
      <link>https://arxiv.org/abs/2410.24191</link>
      <description>arXiv:2410.24191v2 Announce Type: replace-cross 
Abstract: We develop a simple framework to analyze how targeted persuasive advertising shapes market power and welfare. A designer flexibly manipulates the demand curve by influencing individual valuations at a cost. A monopolist prices against this manipulated demand curve. We fully characterize the form of optimal advertising plans under ex-ante and ex-post welfare measures. Flexibility per se is powerful, and can substantially harm or benefit consumers vis-a-vis uniform advertising. We discuss implications for regulation, intermediation, and the joint design of manipulation and information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.24191v2</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Dai, Andrew Koh</dc:creator>
    </item>
    <item>
      <title>PAC Learning with Improvements</title>
      <link>https://arxiv.org/abs/2503.03184</link>
      <description>arXiv:2503.03184v2 Announce Type: replace-cross 
Abstract: One of the most basic lower bounds in machine learning is that in nearly any nontrivial setting, it takes $\textit{at least}$ $1/\epsilon$ samples to learn to error $\epsilon$ (and more, if the classifier being learned is complex). However, suppose that data points are agents who have the ability to improve by a small amount if doing so will allow them to receive a (desired) positive classification. In that case, we may actually be able to achieve $\textit{zero}$ error by just being "close enough". For example, imagine a hiring test used to measure an agent's skill at some job such that for some threshold $\theta$, agents who score above $\theta$ will be successful and those who score below $\theta$ will not (i.e., learning a threshold on the line). Suppose also that by putting in effort, agents can improve their skill level by some small amount $r$. In that case, if we learn an approximation $\hat{\theta}$ of $\theta$ such that $\theta \leq \hat{\theta} \leq \theta + r$ and use it for hiring, we can actually achieve error zero, in the sense that (a) any agent classified as positive is truly qualified, and (b) any agent who truly is qualified can be classified as positive by putting in effort. Thus, the ability for agents to improve has the potential to allow for a goal one could not hope to achieve in standard models, namely zero error.
  In this paper, we explore this phenomenon more broadly, giving general results and examining under what conditions the ability of agents to improve can allow for a reduction in the sample complexity of learning, or alternatively, can make learning harder. We also examine both theoretically and empirically what kinds of improvement-aware algorithms can take into account agents who have the ability to improve to a limited extent when it is in their interest to do so.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03184v2</guid>
      <category>stat.ML</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Idan Attias, Avrim Blum, Keziah Naggita, Donya Saless, Dravyansh Sharma, Matthew Walter</dc:creator>
    </item>
  </channel>
</rss>
