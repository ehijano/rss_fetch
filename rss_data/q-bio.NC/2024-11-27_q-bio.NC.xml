<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 27 Nov 2024 05:00:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Energy Consumption Optimization, Response Time Differences and Indicators in Cortical Working Memory Revealed by Nonequilibrium</title>
      <link>https://arxiv.org/abs/2411.17206</link>
      <description>arXiv:2411.17206v1 Announce Type: new 
Abstract: The neocortex, a complex system driving multi-region interactions, remains a core puzzle in neuroscience. Despite quantitative insights across brain scales, understanding the mechanisms underlying neural activities is challenging. Advances from Hopfield networks to large-scale cortical models have deepened neural network theory, yet these models often fall short of capturing global brain functions. In large-scale cortical networks, an intriguing hierarchy of timescales reflects diverse information processing speeds across spatial regions. As a non-equilibrium system, the brain incurs significant energy costs, with long-distance connectivity suggesting an evolutionary spatial organization. To explore these complexities, we introduce a nonequilibrium landscape flux approach to analyze cortical networks. This allows us to quantify potential landscapes and principal transition paths, uncovering dynamical characteristics across timescales. We examine whether temporal hierarchies correlate with stimuli distribution and how hierarchical networks exhibit differential responses. Furthermore, our analysis quantifies the thermodynamic cost of sustaining cognition, highlighting a link to network connectivity. These findings provide insights into energy consumption during cognitive processes and emphasize the spatial benefits for working memory tasks. Experimental validation is challenging due to evolutionary variability, making our theoretical approach valuable for quantifying complex dynamics. By assessing time irreversibility and critical slowdown, we gain predictive insights into network bifurcations and state transitions, offering practical tools for identifying cortical state changes. These results advance our understanding of cortical dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17206v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaochen Wang, Yuxuan Wu, Feng Zhang, Jin Wang</dc:creator>
    </item>
    <item>
      <title>Storing overlapping associative memories on latent manifolds in low-rank spiking networks</title>
      <link>https://arxiv.org/abs/2411.17485</link>
      <description>arXiv:2411.17485v1 Announce Type: new 
Abstract: Associative memory architectures such as the Hopfield network have long been important conceptual and theoretical models for neuroscience and artificial intelligence. However, translating these abstract models into spiking neural networks has been surprisingly difficult. Indeed, much previous work has been restricted to storing a small number of primarily non-overlapping memories in large networks, thereby limiting their scalability. Here, we revisit the associative memory problem in light of recent advances in understanding spike-based computation. Using a recently-established geometric framework, we show that the spiking activity for a large class of all-inhibitory networks is situated on a low-dimensional, convex, and piecewise-linear manifold, with dynamics that move along the manifold. We then map the associative memory problem onto these dynamics, and demonstrate how the vertices of a hypercubic manifold can be used to store stable, overlapping activity patterns with a direct correspondence to the original Hopfield model. We propose several learning rules, and demonstrate a linear scaling of the storage capacity with the number of neurons, as well as robust pattern completion abilities. Overall, this work serves as a case study to demonstrate the effectiveness of using a geometrical perspective to design dynamics on neural manifolds, with implications for neuroscience and machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17485v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William F. Podlaski, Christian K. Machens</dc:creator>
    </item>
    <item>
      <title>Quantifying information stored in synaptic connections rather than in firing patterns of neural networks</title>
      <link>https://arxiv.org/abs/2411.17692</link>
      <description>arXiv:2411.17692v1 Announce Type: new 
Abstract: A cornerstone of our understanding of both biological and artificial neural networks is that they store information in the strengths of connections among the constituent neurons. However, in contrast to the well-established theory for quantifying information encoded by the firing patterns of neural networks, little is known about quantifying information encoded by its synaptic connections. Here, we develop a theoretical framework using continuous Hopfield networks as an exemplar for associative neural networks, and data that follow mixtures of broadly applicable multivariate log-normal distributions. Specifically, we analytically derive the Shannon mutual information between the data and singletons, pairs, triplets, quadruplets, and arbitrary n-tuples of synaptic connections within the network. Our framework corroborates well-established insights about storage capacity of, and distributed coding by, neural firing patterns. Strikingly, it discovers synergistic interactions among synapses, revealing that the information encoded jointly by all the synapses exceeds the 'sum of its parts'. Taken together, this study introduces an interpretable framework for quantitatively understanding information storage in neural networks, one that illustrates the duality of synaptic connectivity and neural population activity in learning and memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17692v1</guid>
      <category>q-bio.NC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>physics.bio-ph</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xinhao Fan, Shreesh P Mysore</dc:creator>
    </item>
    <item>
      <title>Getting aligned on representational alignment</title>
      <link>https://arxiv.org/abs/2310.13018</link>
      <description>arXiv:2310.13018v3 Announce Type: replace 
Abstract: Biological and artificial information processing systems form representations of the world that they can use to categorize, reason, plan, navigate, and make decisions. How can we measure the similarity between the representations formed by these diverse systems? Do similarities in representations then translate into similar behavior? If so, then how can a system's representations be modified to better match those of another system? These questions pertaining to the study of representational alignment are at the heart of some of the most promising research areas in contemporary cognitive science, neuroscience, and machine learning. In this Perspective, we survey the exciting recent developments in representational alignment research in the fields of cognitive science, neuroscience, and machine learning. Despite their overlapping interests, there is limited knowledge transfer between these fields, so work in one field ends up duplicated in another, and useful innovations are not shared effectively. To improve communication, we propose a unifying framework that can serve as a common language for research on representational alignment, and map several streams of existing work across fields within our framework. We also lay out open problems in representational alignment where progress can benefit all three of these fields. We hope that this paper will catalyze cross-disciplinary collaboration and accelerate progress for all communities studying and developing information processing systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13018v3</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilia Sucholutsky, Lukas Muttenthaler, Adrian Weller, Andi Peng, Andreea Bobu, Been Kim, Bradley C. Love, Christopher J. Cueva, Erin Grant, Iris Groen, Jascha Achterberg, Joshua B. Tenenbaum, Katherine M. Collins, Katherine L. Hermann, Kerem Oktar, Klaus Greff, Martin N. Hebart, Nathan Cloos, Nikolaus Kriegeskorte, Nori Jacoby, Qiuyi Zhang, Raja Marjieh, Robert Geirhos, Sherol Chen, Simon Kornblith, Sunayana Rane, Talia Konkle, Thomas P. O'Connell, Thomas Unterthiner, Andrew K. Lampinen, Klaus-Robert M\"uller, Mariya Toneva, Thomas L. Griffiths</dc:creator>
    </item>
    <item>
      <title>Exploring Behavior-Relevant and Disentangled Neural Dynamics with Generative Diffusion Models</title>
      <link>https://arxiv.org/abs/2410.09614</link>
      <description>arXiv:2410.09614v3 Announce Type: replace 
Abstract: Understanding the neural basis of behavior is a fundamental goal in neuroscience. Current research in large-scale neuro-behavioral data analysis often relies on decoding models, which quantify behavioral information in neural data but lack details on behavior encoding. This raises an intriguing scientific question: ``how can we enable in-depth exploration of neural representations in behavioral tasks, revealing interpretable neural dynamics associated with behaviors''. However, addressing this issue is challenging due to the varied behavioral encoding across different brain regions and mixed selectivity at the population level. To tackle this limitation, our approach, named ``BeNeDiff'', first identifies a fine-grained and disentangled neural subspace using a behavior-informed latent variable model. It then employs state-of-the-art generative diffusion models to synthesize behavior videos that interpret the neural dynamics of each latent factor. We validate the method on multi-session datasets containing widefield calcium imaging recordings across the dorsal cortex. Through guiding the diffusion model to activate individual latent factors, we verify that the neural dynamics of latent factors in the disentangled neural subspace provide interpretable quantifications of the behaviors of interest. At the same time, the neural subspace in BeNeDiff demonstrates high disentanglement and neural reconstruction quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09614v3</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yule Wang, Chengrui Li, Weihan Li, Anqi Wu</dc:creator>
    </item>
    <item>
      <title>Comment on Lubineau et al. (2023) 'Does word flickering improve reading? Negative evidence from four experiments using low and high frequencies'</title>
      <link>https://arxiv.org/abs/2411.00019</link>
      <description>arXiv:2411.00019v2 Announce Type: replace-cross 
Abstract: In 2023, Lubineau et al. published an article [1] detailing several experiments carried out with dyslexic readers. These authors attempted to measure the change in reading performance under different reading conditions using flickering devices. Beyond the low-frequency systems which have nevertheless shown their interest for some cases, we restrict here our response to the high-frequency systems, i.e. electronically controlled glasses (Lexilens) and lamps, designed and built upon recent work by Le Floch and Ropars [2]. Lubineau et al. found no significant change in reading performance at either low or high frequencies and concluded that these devices provide no or minor benefits. Unfortunately, experimental misunderstandings and some methodological issues invalidate namely the main conclusion concerning the high frequency systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00019v2</guid>
      <category>physics.soc-ph</category>
      <category>physics.bio-ph</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Michael Kodochian</dc:creator>
    </item>
  </channel>
</rss>
