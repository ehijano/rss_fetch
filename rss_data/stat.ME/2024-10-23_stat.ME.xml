<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Oct 2024 04:00:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Causal Data Fusion for Panel Data without Pre-Intervention Period</title>
      <link>https://arxiv.org/abs/2410.16391</link>
      <description>arXiv:2410.16391v1 Announce Type: new 
Abstract: Traditional panel data causal inference frameworks, such as difference-in-differences and synthetic control methods, rely on pre-intervention data to estimate counterfactuals, which may not be available in real-world settings when interventions are implemented in response to sudden events. In this paper, we introduce two data fusion methods for causal inference from panel data in scenarios where pre-intervention data is unavailable. These methods leverage auxiliary reference domains with related panel data to estimate causal effects in the target domain, overcoming the limitations imposed by the absence of pre-intervention data. We show the efficacy of these methods by obtaining converging bounds on the absolute bias as well as through simulations, showing their robustness in a variety of panel data settings. Our findings provide a framework for applying causal inference in urgent and data-constrained environments, such as public health crises or epidemiological shocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16391v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zou Yang, Seung Hee Lee, Julia R. K\"ohler, AmirEmad Ghassami</dc:creator>
    </item>
    <item>
      <title>Finite-Sample and Distribution-Free Fair Classification: Optimal Trade-off Between Excess Risk and Fairness, and the Cost of Group-Blindness</title>
      <link>https://arxiv.org/abs/2410.16477</link>
      <description>arXiv:2410.16477v1 Announce Type: new 
Abstract: Algorithmic fairness in machine learning has recently garnered significant attention. However, two pressing challenges remain: (1) The fairness guarantees of existing fair classification methods often rely on specific data distribution assumptions and large sample sizes, which can lead to fairness violations when the sample size is moderate-a common situation in practice. (2) Due to legal and societal considerations, using sensitive group attributes during decision-making (referred to as the group-blind setting) may not always be feasible.
  In this work, we quantify the impact of enforcing algorithmic fairness and group-blindness in binary classification under group fairness constraints. Specifically, we propose a unified framework for fair classification that provides distribution-free and finite-sample fairness guarantees with controlled excess risk. This framework is applicable to various group fairness notions in both group-aware and group-blind scenarios. Furthermore, we establish a minimax lower bound on the excess risk, showing the minimax optimality of our proposed algorithm up to logarithmic factors. Through extensive simulation studies and real data analysis, we further demonstrate the superior performance of our algorithm compared to existing methods, and provide empirical support for our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16477v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaotian Hou, Linjun Zhang</dc:creator>
    </item>
    <item>
      <title>A Dynamic Spatiotemporal and Network ARCH Model with Common Factors</title>
      <link>https://arxiv.org/abs/2410.16526</link>
      <description>arXiv:2410.16526v1 Announce Type: new 
Abstract: We introduce a dynamic spatiotemporal volatility model that extends traditional approaches by incorporating spatial, temporal, and spatiotemporal spillover effects, along with volatility-specific observed and latent factors. The model offers a more general network interpretation, making it applicable for studying various types of network spillovers. The primary innovation lies in incorporating volatility-specific latent factors into the dynamic spatiotemporal volatility model. Using Bayesian estimation via the Markov Chain Monte Carlo (MCMC) method, the model offers a robust framework for analyzing the spatial, temporal, and spatiotemporal effects of a log-squared outcome variable on its volatility. We recommend using the deviance information criterion (DIC) and a regularized Bayesian MCMC method to select the number of relevant factors in the model. The model's flexibility is demonstrated through two applications: a spatiotemporal model applied to the U.S. housing market and another applied to financial stock market networks, both highlighting the model's ability to capture varying degrees of interconnectedness. In both applications, we find strong spatial/network interactions with relatively stronger spillover effects in the stock market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16526v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Osman Do\u{g}an, Raffaele Mattera, Philipp Otto, S\"uleyman Ta\c{s}p{\i}nar</dc:creator>
    </item>
    <item>
      <title>High-dimensional Grouped-regression using Bayesian Sparse Projection-posterior</title>
      <link>https://arxiv.org/abs/2410.16577</link>
      <description>arXiv:2410.16577v1 Announce Type: new 
Abstract: We consider a novel Bayesian approach to estimation, uncertainty quantification, and variable selection for a high-dimensional linear regression model under sparsity. The number of predictors can be nearly exponentially large relative to the sample size. We put a conjugate normal prior initially disregarding sparsity, but for making an inference, instead of the original multivariate normal posterior, we use the posterior distribution induced by a map transforming the vector of regression coefficients to a sparse vector obtained by minimizing the sum of squares of deviations plus a suitably scaled $\ell_1$-penalty on the vector. We show that the resulting sparse projection-posterior distribution contracts around the true value of the parameter at the optimal rate adapted to the sparsity of the vector. We show that the true sparsity structure gets a large sparse projection-posterior probability. We further show that an appropriately recentred credible ball has the correct asymptotic frequentist coverage. Finally, we describe how the computational burden can be distributed to many machines, each dealing with only a small fraction of the whole dataset. We conduct a comprehensive simulation study under a variety of settings and found that the proposed method performs well for finite sample sizes. We also apply the method to several real datasets, including the ADNI data, and compare its performance with the state-of-the-art methods. We implemented the method in the \texttt{R} package called \texttt{sparseProj}, and all computations have been carried out using this package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16577v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Samhita Pal, Subhashis Ghoshal</dc:creator>
    </item>
    <item>
      <title>Assessing and improving reliability of neighbor embedding methods: a map-continuity perspective</title>
      <link>https://arxiv.org/abs/2410.16608</link>
      <description>arXiv:2410.16608v1 Announce Type: new 
Abstract: Visualizing high-dimensional data is an important routine for understanding biomedical data and interpreting deep learning models. Neighbor embedding methods, such as t-SNE, UMAP, and LargeVis, among others, are a family of popular visualization methods which reduce high-dimensional data to two dimensions. However, recent studies suggest that these methods often produce visual artifacts, potentially leading to incorrect scientific conclusions. Recognizing that the current limitation stems from a lack of data-independent notions of embedding maps, we introduce a novel conceptual and computational framework, LOO-map, that learns the embedding maps based on a classical statistical idea known as the leave-one-out. LOO-map extends the embedding over a discrete set of input points to the entire input space, enabling a systematic assessment of map continuity, and thus the reliability of the visualizations. We find for many neighbor embedding methods, their embedding maps can be intrinsically discontinuous. The discontinuity induces two types of observed map distortion: ``overconfidence-inducing discontinuity," which exaggerates cluster separation, and ``fracture-inducing discontinuity," which creates spurious local structures. Building upon LOO-map, we propose two diagnostic point-wise scores -- perturbation score and singularity score -- to address these limitations. These scores can help identify unreliable embedding points, detect out-of-distribution data, and guide hyperparameter selection. Our approach is flexible and works as a wrapper around many neighbor embedding algorithms. We test our methods across multiple real-world datasets from computer vision and single-cell omics to demonstrate their effectiveness in enhancing the interpretability and accuracy of visualizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16608v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhexuan Liu, Rong Ma, Yiqiao Zhong</dc:creator>
    </item>
    <item>
      <title>Enhancing Computational Efficiency in High-Dimensional Bayesian Analysis: Applications to Cancer Genomics</title>
      <link>https://arxiv.org/abs/2410.16627</link>
      <description>arXiv:2410.16627v1 Announce Type: new 
Abstract: In this study, we present a comprehensive evaluation of the Two-Block Gibbs (2BG) sampler as a robust alternative to the traditional Three-Block Gibbs (3BG) sampler in Bayesian shrinkage models. Through extensive simulation studies, we demonstrate that the 2BG sampler exhibits superior computational efficiency and faster convergence rates, particularly in high-dimensional settings where the ratio of predictors to samples is large. We apply these findings to real-world data from the NCI-60 cancer cell panel, leveraging gene expression data to predict protein expression levels. Our analysis incorporates feature selection, identifying key genes that influence protein expression while shedding light on the underlying genetic mechanisms in cancer cells. The results indicate that the 2BG sampler not only produces more effective samples than the 3BG counterpart but also significantly reduces computational costs, thereby enhancing the applicability of Bayesian methods in high-dimensional data analysis. This contribution extends the understanding of shrinkage techniques in statistical modeling and offers valuable insights for cancer genomics research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16627v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Osafo Agyare</dc:creator>
    </item>
    <item>
      <title>Parsimonious Dynamic Mode Decomposition: A Robust and Automated Approach for Optimally Sparse Mode Selection in Complex Systems</title>
      <link>https://arxiv.org/abs/2410.16656</link>
      <description>arXiv:2410.16656v1 Announce Type: new 
Abstract: This paper introduces the Parsimonious Dynamic Mode Decomposition (parsDMD), a novel algorithm designed to automatically select an optimally sparse subset of dynamic modes for both spatiotemporal and purely temporal data. By incorporating time-delay embedding and leveraging Orthogonal Matching Pursuit (OMP), parsDMD ensures robustness against noise and effectively handles complex, nonlinear dynamics. The algorithm is validated on a diverse range of datasets, including standing wave signals, identifying hidden dynamics, fluid dynamics simulations (flow past a cylinder and transonic buffet), and atmospheric sea-surface temperature (SST) data. ParsDMD addresses a significant limitation of the traditional sparsity-promoting DMD (spDMD), which requires manual tuning of sparsity parameters through a rigorous trial-and-error process to balance between single-mode and all-mode solutions. In contrast, parsDMD autonomously determines the optimally sparse subset of modes without user intervention, while maintaining minimal computational complexity. Comparative analyses demonstrate that parsDMD consistently outperforms spDMD by providing more accurate mode identification and effective reconstruction in noisy environments. These advantages render parsDMD an effective tool for real-time diagnostics, forecasting, and reduced-order model construction across various disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16656v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.DS</category>
      <category>physics.data-an</category>
      <category>physics.flu-dyn</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arpan Das, Pier Marzocca, Oleg Levinski</dc:creator>
    </item>
    <item>
      <title>A class of modular and flexible covariate-based covariance functions for nonstationary spatial modeling</title>
      <link>https://arxiv.org/abs/2410.16716</link>
      <description>arXiv:2410.16716v1 Announce Type: new 
Abstract: The assumptions of stationarity and isotropy often stated over spatial processes have not aged well during the last two decades, partly explained by the combination of computational developments and the increasing availability of high-resolution spatial data. While a plethora of approaches have been developed to relax these assumptions, it is often a costly tradeoff between flexibility and a diversity of computational challenges. In this paper, we present a class of covariance functions that relies on fixed, observable spatial information that provides a convenient tradeoff while offering an extra layer of numerical and visual representation of the flexible spatial dependencies. This model allows for separate parametric structures for different sources of nonstationarity, such as marginal standard deviation, geometric anisotropy, and smoothness. It simplifies to a Mat\'ern covariance function in its basic form and is adaptable for large datasets, enhancing flexibility and computational efficiency. We analyze the capabilities of the presented model through simulation studies and an application to Swiss precipitation data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16716v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico Blasi, Reinhard Furrer</dc:creator>
    </item>
    <item>
      <title>Robust Variable Selection for High-dimensional Regression with Missing Data and Measurement Errors</title>
      <link>https://arxiv.org/abs/2410.16722</link>
      <description>arXiv:2410.16722v1 Announce Type: new 
Abstract: In our paper, we focus on robust variable selection for missing data and measurement error.Missing data and measurement errors can lead to confusing data distribution.We propose an exponential loss function with tuning parameter to apply to Missing and measurement errors data.By adjusting the parameter,the loss function can be better and more robust under various different data distributions.We use inverse probability weighting and additivity error models to address missing data and measurement errors. Also, we find that the Atan punishment method works better.We used Monte Carlo simulations to assess the validity of robust variable selection and validated our findings with the breast cancer dataset</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16722v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenhao Zhang, Yunquan Song</dc:creator>
    </item>
    <item>
      <title>Simplified vine copula models: state of science and affairs</title>
      <link>https://arxiv.org/abs/2410.16806</link>
      <description>arXiv:2410.16806v1 Announce Type: new 
Abstract: Vine copula models have become highly popular practical tools for modeling multivariate dependencies. To maintain traceability, a commonly employed simplifying assumption is that conditional copulas remain unchanged by the conditioning variables. This assumption has sparked a somewhat polarizing debate within our community. The fact that much of this dispute occurs outside the public record has placed the field in an unfortunate position, impeding scientific progress. In this article, I will review what we know about the flexibility and limitations of simplified vine copula models, explore the broader implications, and offer my own, hopefully reconciling, perspective on the issue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16806v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Nagler</dc:creator>
    </item>
    <item>
      <title>Second-order characteristics for spatial point processes with graph-valued marks</title>
      <link>https://arxiv.org/abs/2410.16903</link>
      <description>arXiv:2410.16903v1 Announce Type: new 
Abstract: The immense progress in data collection and storage capacities have yielded rather complex, challenging spatial event-type data, where each event location is augmented by a non-simple mark. Despite the growing interest in analysing such complex event patterns, the methodology for such analysis is not embedded well in the literature. In particular, the literature lacks statistical methods to analyse marks which are characterised by an inherent relational structure, i.e.\ where the mark is graph-valued. Motivated by epidermal nerve fibre data, we introduce different mark summary characteristics, which investigate the average variation or association between pairs of graph-valued marks, and apply some of the methods to the nerve data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16903v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias Eckardt, Farnaz Ghorbanpour, Aila S\"arkk\"a</dc:creator>
    </item>
    <item>
      <title>Mesoscale two-sample testing for network data</title>
      <link>https://arxiv.org/abs/2410.17046</link>
      <description>arXiv:2410.17046v1 Announce Type: new 
Abstract: Networks arise naturally in many scientific fields as a representation of pairwise connections. Statistical network analysis has most often considered a single large network, but it is common in a number of applications, for example, neuroimaging, to observe multiple networks on a shared node set. When these networks are grouped by case-control status or another categorical covariate, the classical statistical question of two-sample comparison arises. In this work, we address the problem of testing for statistically significant differences in a given arbitrary subset of connections. This general framework allows an analyst to focus on a single node, a specific region of interest, or compare whole networks. Our ability to conduct "mesoscale" testing on a meaningful group of edges is particularly relevant for applications such as neuroimaging and distinguishes our approach from prior work, which tends to focus either on a single node or the whole network. In this mesoscale setting, we develop statistically sound projection-based tests for two-sample comparison in both weighted and binary edge networks. Our approach can leverage all available network information, and learn informative projections which improve testing power when low-dimensional latent network structure is present.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17046v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter W. MacDonald, Elizaveta Levina, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>A general framework for probabilistic model uncertainty</title>
      <link>https://arxiv.org/abs/2410.17108</link>
      <description>arXiv:2410.17108v1 Announce Type: new 
Abstract: Existing approaches to model uncertainty typically either compare models using a quantitative model selection criterion or evaluate posterior model probabilities having set a prior. In this paper, we propose an alternative strategy which views missing observations as the source of model uncertainty, where the true model would be identified with the complete data. To quantify model uncertainty, it is then necessary to provide a probability distribution for the missing observations conditional on what has been observed. This can be set sequentially using one-step-ahead predictive densities, which recursively sample from the best model according to some consistent model selection criterion. Repeated predictive sampling of the missing data, to give a complete dataset and hence a best model each time, provides our measure of model uncertainty. This approach bypasses the need for subjective prior specification or integration over parameter spaces, addressing issues with standard methods such as the Bayes factor. Predictive resampling also suggests an alternative view of hypothesis testing as a decision problem based on a population statistic, where we directly index the probabilities of competing models. In addition to hypothesis testing, we provide illustrations from density estimation and variable selection, demonstrating our approach on a range of standard problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17108v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vik Shirvaikar, Stephen G. Walker, Chris Holmes</dc:creator>
    </item>
    <item>
      <title>Functional Clustering of Discount Functions for Behavioral Investor Profiling</title>
      <link>https://arxiv.org/abs/2410.16307</link>
      <description>arXiv:2410.16307v1 Announce Type: cross 
Abstract: Classical finance models are based on the premise that investors act rationally and utilize all available information when making portfolio decisions. However, these models often fail to capture the anomalies observed in intertemporal choices and decision-making under uncertainty, particularly when accounting for individual differences in preferences and consumption patterns. Such limitations hinder traditional finance theory's ability to address key questions like: How do personal preferences shape investment choices? What drives investor behaviour? And how do individuals select their portfolios? One prominent contribution is Pompian's model of four Behavioral Investor Types (BITs), which links behavioural finance studies with Keirsey's temperament theory, highlighting the role of personality in financial decision-making. Yet, traditional parametric models struggle to capture how these distinct temperaments influence intertemporal decisions, such as how individuals evaluate trade-offs between present and future outcomes. To address this gap, the present study employs Functional Data Analysis (FDA) to specifically investigate temporal discounting behaviours revealing nuanced patterns in how different temperaments perceive and manage uncertainty over time. Our findings show heterogeneity within each temperament, suggesting that investor profiles are far more diverse than previously thought. This refined classification provides deeper insights into the role of temperament in shaping intertemporal financial decisions, offering practical implications for financial advisors to better tailor strategies to individual risk preferences and decision-making styles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16307v1</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annamaria Porreca, Viviana Ventre, Roberta Martino, Salvador Cruz Rambaud, Fabrizio Maturo</dc:creator>
    </item>
    <item>
      <title>Data Augmentation of Multivariate Sensor Time Series using Autoregressive Models and Application to Failure Prognostics</title>
      <link>https://arxiv.org/abs/2410.16419</link>
      <description>arXiv:2410.16419v1 Announce Type: cross 
Abstract: This work presents a novel data augmentation solution for non-stationary multivariate time series and its application to failure prognostics. The method extends previous work from the authors which is based on time-varying autoregressive processes. It can be employed to extract key information from a limited number of samples and generate new synthetic samples in a way that potentially improves the performance of PHM solutions. This is especially valuable in situations of data scarcity which are very usual in PHM, especially for failure prognostics. The proposed approach is tested based on the CMAPSS dataset, commonly employed for prognostics experiments and benchmarks. An AutoML approach from PHM literature is employed for automating the design of the prognostics solution. The empirical evaluation provides evidence that the proposed method can substantially improve the performance of PHM solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16419v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Douglas Baptista de Souza, Bruno Paes Leao</dc:creator>
    </item>
    <item>
      <title>Efficient Neural Network Training via Subset Pretraining</title>
      <link>https://arxiv.org/abs/2410.16523</link>
      <description>arXiv:2410.16523v1 Announce Type: cross 
Abstract: In training neural networks, it is common practice to use partial gradients computed over batches, mostly very small subsets of the training set. This approach is motivated by the argument that such a partial gradient is close to the true one, with precision growing only with the square root of the batch size. A theoretical justification is with the help of stochastic approximation theory. However, the conditions for the validity of this theory are not satisfied in the usual learning rate schedules. Batch processing is also difficult to combine with efficient second-order optimization methods. This proposal is based on another hypothesis: the loss minimum of the training set can be expected to be well-approximated by the minima of its subsets. Such subset minima can be computed in a fraction of the time necessary for optimizing over the whole training set. This hypothesis has been tested with the help of the MNIST, CIFAR-10, and CIFAR-100 image classification benchmarks, optionally extended by training data augmentation. The experiments have confirmed that results equivalent to conventional training can be reached. In summary, even small subsets are representative if the overdetermination ratio for the given model parameter set sufficiently exceeds unity. The computing expense can be reduced to a tenth or less.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16523v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Sp\"orer, Bernhard Bermeitinger, Tomas Hrycej, Niklas Limacher, Siegfried Handschuh</dc:creator>
    </item>
    <item>
      <title>On the breakdown point of transport-based quantiles</title>
      <link>https://arxiv.org/abs/2410.16554</link>
      <description>arXiv:2410.16554v1 Announce Type: cross 
Abstract: Recent work has used optimal transport ideas to generalize the notion of (center-outward) quantiles to dimension $d\geq 2$. We study the robustness properties of these transport-based quantiles by deriving their breakdown point, roughly, the smallest amount of contamination required to make these quantiles take arbitrarily aberrant values. We prove that the transport median defined in Chernozhukov et al.~(2017) and Hallin et al.~(2021) has breakdown point of $1/2$. Moreover, a point in the transport depth contour of order $\tau\in [0,1/2]$ has breakdown point of $\tau$. This shows that the multivariate transport depth shares the same breakdown properties as its univariate counterpart. Our proof relies on a general argument connecting the breakdown point of transport maps evaluated at a point to the Tukey depth of that point in the reference measure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16554v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Avella-Medina, Alberto Gonz\'alez-Sanz</dc:creator>
    </item>
    <item>
      <title>HDNRA: An R package for HDLSS location testing with normal-reference approaches</title>
      <link>https://arxiv.org/abs/2410.16702</link>
      <description>arXiv:2410.16702v1 Announce Type: cross 
Abstract: The challenge of location testing for high-dimensional data in statistical inference is notable. Existing literature suggests various methods, many of which impose strong regularity conditions on underlying covariance matrices to ensure asymptotic normal distribution of test statistics, leading to difficulties in size control. To address this, a recent set of tests employing the normal-reference approach has been proposed. Moreover, the availability of tests for high-dimensional location testing in R packages implemented in C++ is limited. This paper introduces the latest methods utilizing normal-reference approaches to test the equality of mean vectors in high-dimensional samples with potentially different covariance matrices. We present an R package named HDNRA to illustrate the implementation of these tests, extending beyond the two-sample problem to encompass general linear hypothesis testing (GLHT). The package offers easy and user-friendly access to these tests, with its core implemented in C++ using Rcpp, OpenMP and RcppArmadillo for efficient execution. Theoretical properties of these normal-reference tests are revisited, and examples based on real datasets using different tests are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16702v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengfei Wang, Tianming Zhu, Jin-Ting Zhang</dc:creator>
    </item>
    <item>
      <title>Scaling of Piecewise Deterministic Monte Carlo for Anisotropic Targets</title>
      <link>https://arxiv.org/abs/2305.00694</link>
      <description>arXiv:2305.00694v2 Announce Type: replace 
Abstract: Piecewise deterministic Markov processes (PDMPs) are a type of continuous-time Markov process that combine deterministic flows with jumps. Recently, PDMPs have garnered attention within the Monte Carlo community as a potential alternative to traditional Markov chain Monte Carlo (MCMC) methods. The Zig-Zag sampler and the Bouncy Particle Sampler are commonly used examples of the PDMP methodology which have also yielded impressive theoretical properties, but little is known about their robustness to extreme dependence or anisotropy of the target density. It turns out that PDMPs may suffer from poor mixing due to anisotropy and this paper investigates this effect in detail in the stylised but important Gaussian case. To this end, we employ a multi-scale analysis framework in this paper. Our results show that when the Gaussian target distribution has two scales, of order $1$ and $\epsilon$, the computational cost of the Bouncy Particle Sampler is of order $\epsilon^{-1}$, and the computational cost of the Zig-Zag sampler is $\epsilon^{-2}$. In comparison, the cost of the traditional MCMC methods such as RWM is of order $\epsilon^{-2}$, at least when the dimensionality of the small component is more than $1$. Therefore, there is a robustness advantage to using PDMPs in this context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.00694v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joris Bierkens, Kengo Kamatani, Gareth O. Roberts</dc:creator>
    </item>
    <item>
      <title>Reducing Symbiosis Bias Through Better A/B Tests of Recommendation Algorithms</title>
      <link>https://arxiv.org/abs/2309.07107</link>
      <description>arXiv:2309.07107v3 Announce Type: replace 
Abstract: It is increasingly common in digital environments to use A/B tests to compare the performance of recommendation algorithms. However, such experiments often violate the stable unit treatment value assumption (SUTVA), particularly SUTVA's "no hidden treatments" assumption, due to the shared data between algorithms being compared. This results in a novel form of bias, which we term "symbiosis bias," where the performance of each algorithm is influenced by the training data generated by its competitor. In this paper, we investigate three experimental designs--cluster-randomized, data-diverted, and user-corpus co-diverted experiments--aimed at mitigating symbiosis bias. We present a theoretical model of symbiosis bias and simulate the impact of each design in dynamic recommendation environments. Our results show that while each design reduces symbiosis bias to some extent, they also introduce new challenges, such as reduced training data in data-diverted experiments. We further validate the existence of symbiosis bias using data from a large-scale A/B test conducted on a global recommender system, demonstrating that symbiosis bias affects treatment effect estimates in the field. Our findings provide actionable insights for researchers and practitioners seeking to design experiments that accurately capture algorithmic performance without bias in treatment effect estimates introduced by shared data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.07107v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jennifer Brennan, Yahu Cong, Yiwei Yu, Lina Lin, Yajun Peng, Changping Meng, Ningren Han, Jean Pouget-Abadie, David Holtz</dc:creator>
    </item>
    <item>
      <title>Regularized estimation of Monge-Kantorovich quantiles for spherical data</title>
      <link>https://arxiv.org/abs/2407.02085</link>
      <description>arXiv:2407.02085v3 Announce Type: replace 
Abstract: Tools from optimal transport (OT) theory have recently been used to define a notion of quantile function for directional data. In practice, regularization is mandatory for applications that require out-of-sample estimates. To this end, we introduce a regularized estimator built from entropic optimal transport, by extending the definition of the entropic map to the spherical setting. We propose a stochastic algorithm to directly solve a continuous OT problem between the uniform distribution and a target distribution, by expanding Kantorovich potentials in the basis of spherical harmonics. In addition, we define the directional Monge-Kantorovich depth, a companion concept for OT-based quantiles. We show that it benefits from desirable properties related to Liu-Zuo-Serfling axioms for the statistical analysis of directional data. Building on our regularized estimators, we illustrate the benefits of our methodology for data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02085v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bernard Bercu, J\'er\'emie Bigot, Gauthier Thurin</dc:creator>
    </item>
    <item>
      <title>On metric choice in dimension reduction for Fr\'echet regression</title>
      <link>https://arxiv.org/abs/2410.01783</link>
      <description>arXiv:2410.01783v2 Announce Type: replace 
Abstract: Fr\'echet regression is becoming a mainstay in modern data analysis for analyzing non-traditional data types belonging to general metric spaces. This novel regression method is especially useful in the analysis of complex health data such as continuous monitoring and imaging data. Fr\'echet regression utilizes the pairwise distances between the random objects, which makes the choice of metric crucial in the estimation. In this paper, existing dimension reduction methods for Fr\'echet regression are reviewed, and the effect of metric choice on the estimation of the dimension reduction subspace is explored for the regression between random responses and Euclidean predictors. Extensive numerical studies illustrate how different metrics affect the central and central mean space estimators. Two real applications involving analysis of brain connectivity networks of subjects with and without Parkinson's disease and an analysis of the distributions of glycaemia based on continuous glucose monitoring data are provided, to demonstrate how metric choice can influence findings in real applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01783v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdul-Nasah Soale, Congli Ma, Siyu Chen, Obed Koomson</dc:creator>
    </item>
    <item>
      <title>Bias correction of quadratic spectral estimators</title>
      <link>https://arxiv.org/abs/2410.12386</link>
      <description>arXiv:2410.12386v2 Announce Type: replace 
Abstract: The three cardinal, statistically consistent, families of non-parametric estimators to the power spectral density of a time series are lag-window, multitaper and Welch estimators. However, when estimating power spectral densities from a finite sample each can be subject to non-ignorable bias. Astfalck et al. (2024) developed a method that offers significant bias reduction for finite samples for Welch's estimator, which this article extends to the larger family of quadratic estimators, thus offering similar theory for bias correction of lag-window and multitaper estimators as well as combinations thereof. Importantly, this theory may be used in conjunction with any and all tapers and lag-sequences designed for bias reduction, and so should be seen as an extension to valuable work in these fields, rather than a supplanting methodology. The order of computation is larger than O(n log n) typical in spectral analyses, but not insurmountable in practice. Simulation studies support the theory with comparisons across variations of quadratic estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12386v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lachlan Astfalck, Adam Sykulski, Edward Cripps</dc:creator>
    </item>
    <item>
      <title>Community Detection Guarantees Using Embeddings Learned by Node2Vec</title>
      <link>https://arxiv.org/abs/2310.17712</link>
      <description>arXiv:2310.17712v3 Announce Type: replace-cross 
Abstract: Embedding the nodes of a large network into an Euclidean space is a common objective in modern machine learning, with a variety of tools available. These embeddings can then be used as features for tasks such as community detection/node clustering or link prediction, where they achieve state of the art performance. With the exception of spectral clustering methods, there is little theoretical understanding for commonly used approaches to learning embeddings. In this work we examine the theoretical properties of the embeddings learned by node2vec. Our main result shows that the use of $k$-means clustering on the embedding vectors produced by node2vec gives weakly consistent community recovery for the nodes in (degree corrected) stochastic block models. We also discuss the use of these embeddings for node and link prediction tasks. We demonstrate this result empirically, and examine how this relates to other embedding tools for network data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17712v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Davison, S. Carlyle Morgan, Owen G. Ward</dc:creator>
    </item>
    <item>
      <title>Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework</title>
      <link>https://arxiv.org/abs/2311.18460</link>
      <description>arXiv:2311.18460v3 Announce Type: replace-cross 
Abstract: Fairness for machine learning predictions is widely required in practice for legal, ethical, and societal reasons. Existing work typically focuses on settings without unobserved confounding, even though unobserved confounding can lead to severe violations of causal fairness and, thus, unfair predictions. In this work, we analyze the sensitivity of causal fairness to unobserved confounding. Our contributions are three-fold. First, we derive bounds for causal fairness metrics under different sources of unobserved confounding. This enables practitioners to examine the sensitivity of their machine learning models to unobserved confounding in fairness-critical applications. Second, we propose a novel neural framework for learning fair predictions, which allows us to offer worst-case guarantees of the extent to which causal fairness can be violated due to unobserved confounding. Third, we demonstrate the effectiveness of our framework in a series of experiments, including a real-world case study about predicting prison sentences. To the best of our knowledge, ours is the first work to study causal fairness under unobserved confounding. To this end, our work is of direct practical value as a refutation strategy to ensure the fairness of predictions in high-stakes applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18460v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <pubDate>Wed, 23 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Published as a conference paper at ICLR 2024</arxiv:journal_reference>
      <dc:creator>Maresa Schr\"oder, Dennis Frauen, Stefan Feuerriegel</dc:creator>
    </item>
  </channel>
</rss>
