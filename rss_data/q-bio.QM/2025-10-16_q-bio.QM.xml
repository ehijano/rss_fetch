<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Oct 2025 04:01:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Physics-Informed autoencoder for DSC-MRI Perfusion post-processing: application to glioma grading</title>
      <link>https://arxiv.org/abs/2510.13886</link>
      <description>arXiv:2510.13886v1 Announce Type: new 
Abstract: DSC-MRI perfusion is a medical imaging technique for diagnosing and prognosing brain tumors and strokes. Its analysis relies on mathematical deconvolution, but noise or motion artifacts in a clinical environment can disrupt this process, leading to incorrect estimate of perfusion parameters. Although deep learning approaches have shown promising results, their calibration typically rely on third-party deconvolution algorithms to generate reference outputs and are bound to reproduce their limitations.
  To adress this problem, we propose a physics-informed autoencoder that leverages an analytical model to decode the perfusion parameters and guide the learning of the encoding network. This autoencoder is trained in a self-supervised fashion without any third-party software and its performance is evaluated on a database with glioma patients. Our method shows reliable results for glioma grading in accordance with other well-known deconvolution algorithms despite a lower computation time. It also achieved competitive performance even in the presence of high noise which is critical in a medical environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13886v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierre Fayolle, Alexandre B\^one, No\"elie Debs, Mathieu Naudin, Pascal Bourdon, Remy Guillevin, David Helbert</dc:creator>
    </item>
    <item>
      <title>GenCellAgent: Generalizable, Training-Free Cellular Image Segmentation via Large Language Model Agents</title>
      <link>https://arxiv.org/abs/2510.13896</link>
      <description>arXiv:2510.13896v1 Announce Type: new 
Abstract: Cellular image segmentation is essential for quantitative biology yet remains difficult due to heterogeneous modalities, morphological variability, and limited annotations. We present GenCellAgent, a training-free multi-agent framework that orchestrates specialist segmenters and generalist vision-language models via a planner-executor-evaluator loop (choose tool $\rightarrow$ run $\rightarrow$ quality-check) with long-term memory. The system (i) automatically routes images to the best tool, (ii) adapts on the fly using a few reference images when imaging conditions differ from what a tool expects, (iii) supports text-guided segmentation of organelles not covered by existing models, and (iv) commits expert edits to memory, enabling self-evolution and personalized workflows. Across four cell-segmentation benchmarks, this routing yields a 15.7\% mean accuracy gain over state-of-the-art baselines. On endoplasmic reticulum and mitochondria from new datasets, GenCellAgent improves average IoU by 37.6\% over specialist models. It also segments novel objects such as the Golgi apparatus via iterative text-guided refinement, with light human correction further boosting performance. Together, these capabilities provide a practical path to robust, adaptable cellular image segmentation without retraining, while reducing annotation burden and matching user preferences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13896v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.MA</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xi Yu, Yang Yang, Qun Liu, Yonghua Du, Sean McSweeney, Yuewei Lin</dc:creator>
    </item>
    <item>
      <title>Dual-attention ResNet outperforms transformers in HER2 prediction on DCE-MRI</title>
      <link>https://arxiv.org/abs/2510.13897</link>
      <description>arXiv:2510.13897v1 Announce Type: new 
Abstract: Breast cancer is the most diagnosed cancer in women, with HER2 status critically guiding treatment decisions. Noninvasive prediction of HER2 status from dynamic contrast-enhanced MRI (DCE-MRI) could streamline diagnostics and reduce reliance on biopsy. However, preprocessing high-dynamic-range DCE-MRI into standardized 8-bit RGB format for pretrained neural networks is nontrivial, and normalization strategy significantly affects model performance. We benchmarked intensity normalization strategies using a Triple-Head Dual-Attention ResNet that processes RGB-fused temporal sequences from three DCE phases. Trained on a multicenter cohort (n=1,149) from the I-SPY trials and externally validated on BreastDCEDL_AMBL (n=43 lesions), our model outperformed transformer-based architectures, achieving 0.75 accuracy and 0.74 AUC on I-SPY test data. N4 bias field correction slightly degraded performance. Without fine-tuning, external validation yielded 0.66 AUC, demonstrating cross-institutional generalizability. These findings highlight the effectiveness of dual-attention mechanisms in capturing transferable spatiotemporal features for HER2 stratification, advancing reproducible deep learning biomarkers in breast cancer imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13897v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naomi Fridman, Anat Goldstein</dc:creator>
    </item>
    <item>
      <title>OralGPT: A Two-Stage Vision-Language Model for Oral Mucosal Disease Diagnosis and Description</title>
      <link>https://arxiv.org/abs/2510.13911</link>
      <description>arXiv:2510.13911v1 Announce Type: new 
Abstract: Oral mucosal diseases such as leukoplakia, oral lichen planus, and recurrent
  aphthous ulcers exhibit diverse and overlapping visual features,
  making diagnosis challenging for non-specialists. While vision-language
  models (VLMs) have shown promise in medical image interpretation,
  their application in oral healthcare remains underexplored due to
  the lack of large-scale, well-annotated datasets. In this work, we present
  \textbf{OralGPT}, the first domain-specific two-stage vision-language
  framework designed for oral mucosal disease diagnosis and captioning.
  In Stage 1, OralGPT learns visual representations and disease-related
  concepts from classification labels. In Stage 2, it enhances its language
  generation ability using long-form expert-authored captions. To
  overcome the annotation bottleneck, we propose a novel similarity-guided
  data augmentation strategy that propagates descriptive knowledge from
  expert-labeled images to weakly labeled ones. We also construct the
  first benchmark dataset for oral mucosal diseases, integrating multi-source
  image data with both structured and unstructured textual annotations.
  Experimental results on four common oral conditions demonstrate that
  OralGPT achieves competitive diagnostic performance while generating
  fluent, clinically meaningful image descriptions. This study
  provides a foundation for language-assisted diagnostic tools in oral
  healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13911v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jia Zhang, Bodong Du, Yitong Miao, Dongwei Sun, Xiangyong Cao</dc:creator>
    </item>
    <item>
      <title>SUND: simulation using nonlinear dynamic models - a toolbox for simulating multi-level, time-dynamic systems in a modular way</title>
      <link>https://arxiv.org/abs/2510.13932</link>
      <description>arXiv:2510.13932v1 Announce Type: new 
Abstract: When modeling complex, hierarchical, and time-dynamic systems, such as biological systems, good computational tools are essential. Current tools, while powerful, often lack comprehensive frameworks for modular model composition, hierarchical system building, and time-dependent input handling, particularly within the Python ecosystem. We present SUND (Simulation Using Nonlinear Dynamic models), a Python toolbox designed to address these challenges. SUND provides a unified framework for defining, combining, and simulating multi-level time-dynamic systems. The toolbox enables users to define models with interconnectable inputs and outputs, facilitating the construction of complex systems from simpler, reusable components. It supports time-dependent functions and piecewise constant inputs, enabling intuitive simulation of various experimental conditions such as multiple dosing schemes. We demonstrate the toolbox's capabilities through simulation of a multi-level human glucose-insulin system model, showcasing its flexibility in handling multiple temporal scales, and levels of biological detail. SUND is open-source, easily extensible, and available at PyPI (https://pypi.org/project/sund/) and at Gitlab (https://gitlab.liu.se/ISBgroup/projects/sund/).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13932v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Henrik Pod\'eus (Department of Biomedical Engineering, Link\"oping University, Link\"oping, Sweden), Gustav Magnusson (Center for Medical Image Science and Visualization), Sasan Keshmiri (Department of Biomedical Engineering, Link\"oping University, Link\"oping, Sweden), Kajsa Tunedal (Department of Biomedical Engineering, Link\"oping University, Link\"oping, Sweden), Nicolas Sundqvist (Department of Biomedical Engineering, Link\"oping University, Link\"oping, Sweden), William L\"ovfors (Department of Biomedical Engineering, Link\"oping University, Link\"oping, Sweden), Gunnar Cedersund (Department of Biomedical Engineering, Link\"oping University, Link\"oping, Sweden)</dc:creator>
    </item>
    <item>
      <title>cubic: CUDA-accelerated 3D Bioimage Computing</title>
      <link>https://arxiv.org/abs/2510.14143</link>
      <description>arXiv:2510.14143v1 Announce Type: cross 
Abstract: Quantitative analysis of multidimensional biological images is useful for understanding complex cellular phenotypes and accelerating advances in biomedical research. As modern microscopy generates ever-larger 2D and 3D datasets, existing computational approaches are increasingly limited by their scalability, efficiency, and integration with modern scientific computing workflows. Existing bioimage analysis tools often lack application programmable interfaces (APIs), do not support graphics processing unit (GPU) acceleration, lack broad 3D image processing capabilities, and/or have poor interoperability for compute-heavy workflows. Here, we introduce cubic, an open-source Python library that addresses these challenges by augmenting widely used SciPy and scikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM. cubic's API is device-agnostic and dispatches operations to GPU when data reside on the device and otherwise executes on CPU, seamlessly accelerating a broad range of image processing routines. This approach enables GPU acceleration of existing bioimage analysis workflows, from preprocessing to segmentation and feature extraction for 2D and 3D data. We evaluate cubic both by benchmarking individual operations and by reproducing existing deconvolution and segmentation pipelines, achieving substantial speedups while maintaining algorithmic fidelity. These advances establish a robust foundation for scalable, reproducible bioimage analysis that integrates with the broader Python scientific computing ecosystem, including other GPU-accelerated methods, enabling both interactive exploration and automated high-throughput analysis workflows. cubic is openly available at https://github$.$com/alxndrkalinin/cubic</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14143v1</guid>
      <category>cs.CV</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alexandr A. Kalinin, Anne E. Carpenter, Shantanu Singh, Matthew J. O'Meara</dc:creator>
    </item>
    <item>
      <title>Using Information Geometry to Characterize Higher-Order Interactions in EEG</title>
      <link>https://arxiv.org/abs/2510.14188</link>
      <description>arXiv:2510.14188v1 Announce Type: cross 
Abstract: In neuroscience, methods from information geometry (IG) have been successfully applied in the modelling of binary vectors from spike train data, using the orthogonal decomposition of the Kullback-Leibler divergence and mutual information to isolate different orders of interaction between neurons. While spike train data is well-approximated with a binary model, here we apply these IG methods to data from electroencephalography (EEG), a continuous signal requiring appropriate discretization strategies. We developed and compared three different binarization methods and used them to identify third-order interactions in an experiment involving imagined motor movements. The statistical significance of these interactions was assessed using phase-randomized surrogate data that eliminated higher-order dependencies while preserving the spectral characteristics of the original signals. We validated our approach by implementing known second- and third-order dependencies in a forward model and quantified information attenuation at different steps of the analysis. This revealed that the greatest loss in information occurred when going from the idealized binary case to enforcing these dependencies using oscillatory signals. When applied to the real EEG dataset, our analysis detected statistically significant third-order interactions during the task condition despite the relatively sparse data (45 trials per condition). This work demonstrates that IG methods can successfully extract genuine higher-order dependencies from continuous neural recordings when paired with appropriate binarization schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14188v1</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Albers, Paul Marriott, Masami Tatsuno</dc:creator>
    </item>
    <item>
      <title>Viral population dynamics at the cellular level, considering the replication cycle</title>
      <link>https://arxiv.org/abs/2510.14481</link>
      <description>arXiv:2510.14481v1 Announce Type: cross 
Abstract: Viruses are microscopic infectious agents that require a host cell for replication. Viral replication occurs in several stages, and the completion time for each stage varies due to differences in the cellular environment. Thus, the time to complete each stage in viral replication is a random variable. However, no analytic expression exists for the viral population at the cellular level when the completion time for each process constituting viral replication is a random variable. This paper presents a simplified model of viral replication, treating each stage as a renewal process with independently and identically distributed completion times. Using the proposed model, we derive an analytical formula for viral populations at the cellular level, based on viewing viral replication as a birth-death process. The mean viral count is expressed via probability density functions representing the completion time for each step in the replication process. This work validates the results with stochastic simulations. This study provides a new quantitative framework for understanding viral infection dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14481v1</guid>
      <category>q-bio.PE</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Seong Jun Park</dc:creator>
    </item>
    <item>
      <title>EasyNER: A Customizable Easy-to-Use Pipeline for Deep Learning- and Dictionary-based Named Entity Recognition from Medical and Life Science Text</title>
      <link>https://arxiv.org/abs/2304.07805</link>
      <description>arXiv:2304.07805v3 Announce Type: replace 
Abstract: Background Medical and life science research generates millions of publications, and it is a great challenge for researchers to utilize this information in full since its scale and complexity greatly surpasses human reading capabilities. Automated text mining can help extract and connect information spread across this large body of literature, but this technology is not easily accessible to life scientists.
  Methods and Results Here, we developed an easy-to-use end-to-end pipeline for deep learning- and dictionary-based named entity recognition (NER) of typical entities found in medical and life science research articles, including diseases, cells, chemicals, genes/proteins, species and others. The pipeline can access and process large medical research article collections (PubMed, CORD-19) or raw text and incorporates a series of deep learning models fine-tuned on the HUNER corpora collection. In addition, the pipeline can perform dictionary-based NER related to COVID-19 and other medical topics. Users can also load their own NER models and dictionaries to include additional entities. The output consists of publication-ready ranked lists and graphs of detected entities and files containing the annotated texts. In addition, we provide two accessory scripts which allow processing of files in PubTator format and rapid inspection of the results for specific entities of interest. As model use cases, the pipeline was deployed on two collections of autophagy-related abstracts from PubMed and on the CORD19 dataset, a collection of 764 398 research article abstracts related to COVID-19.
  Conclusions The NER pipeline we present is applicable in a variety of medical research settings and makes customizable text mining accessible to life scientists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.07805v3</guid>
      <category>q-bio.QM</category>
      <category>cs.CL</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rafsan Ahmed, Petter Berntsson, Alexander Skafte, Salma Kazemi Rashed, Marcus Klang, Adam Barvesten, Ola Olde, William Lindholm, Antton Lamarca Arrizabalaga, Pierre Nugues, Sonja Aits</dc:creator>
    </item>
    <item>
      <title>Protenix-Mini+: efficient structure prediction model with scalable pairformer</title>
      <link>https://arxiv.org/abs/2510.12842</link>
      <description>arXiv:2510.12842v2 Announce Type: replace 
Abstract: Lightweight inference is critical for biomolecular structure prediction and downstream tasks, enabling efficient real-world deployment and inference-time scaling for large-scale applications. While AF3 and its variants (e.g., Protenix, Chai-1) have advanced structure prediction results, they suffer from critical limitations: high inference latency and cubic time complexity with respect to token count, both of which restrict scalability for large biomolecular complexes. To address the core challenge of balancing model efficiency and prediction accuracy, we introduce three key innovations: (1) compressing non-scalable operations to mitigate cubic time complexity, (2) removing redundant blocks across modules to reduce unnecessary overhead, and (3) adopting a few-step sampler for the atom diffusion module to accelerate inference. Building on these design principles, we develop Protenix-Mini+, a highly lightweight and scalable variant of the Protenix model. Within an acceptable range of performance degradation, it substantially improves computational efficiency. For example, in the case of low-homology single-chain proteins, Protenix-Mini+ experiences an intra-protein LDDT drop of approximately 3% relative to the full Protenix model -- an acceptable performance trade-off given its substantially 90%+ improved computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12842v2</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bo Qiang, Chengyue Gong, Xinshi Chen, Yuxuan Zhang, Wenzhi Xiao</dc:creator>
    </item>
    <item>
      <title>Omni-QALAS: Optimized Multiparametric Imaging for Simultaneous T1, T2 and Myelin Water Mapping</title>
      <link>https://arxiv.org/abs/2510.13118</link>
      <description>arXiv:2510.13118v2 Announce Type: replace 
Abstract: Purpose: To improve the accuracy of multiparametric estimation, including myelin water fraction (MWF) quantification, and reduce scan time in 3D-QALAS by optimizing sequence parameters, using a self-supervised multilayer perceptron network. Methods: We jointly optimize flip angles, T2 preparation durations, and sequence gaps for T1 recovery using a self-supervised MLP trained to minimize a Cramer-Rao bound-based loss function, with explicit constraints on total scan time. The optimization targets white matter, gray matter, and myelin water tissues, and its performance was validated through simulation, phantom, and in vivo experiments. Results: Building on our previously proposed MWF-QALAS method for simultaneous MWF, T1, and T2 mapping, the optimized sequence reduces the number of readouts from six to five and achieves a scan time nearly one minute shorter, while also yielding higher T1 and T2 accuracy and improved MWF maps. This sequence enables simultaneous multiparametric quantification, including MWF, at 1 mm isotropic resolution within 3 minutes and 30 seconds. Conclusion: This study demonstrated that optimizing sequence parameters using a self-supervised MLP network improved T1, T2 and MWF estimation accuracy, while reducing scan time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13118v2</guid>
      <category>q-bio.QM</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shizhuo Li, Unay Dorken Gallastegi, Shohei Fujita, Yuting Chen, Pengcheng Xu, Yangsean Choi, Borjan Gagoski, Huihui Ye, Huafeng Liu, Berkin Bilgic, Yohan Jun</dc:creator>
    </item>
    <item>
      <title>TABSurfer: a Hybrid Deep Learning Architecture for Subcortical Segmentation</title>
      <link>https://arxiv.org/abs/2312.08267</link>
      <description>arXiv:2312.08267v2 Announce Type: replace-cross 
Abstract: Subcortical segmentation remains challenging despite its important applications in quantitative structural analysis of brain MRI scans. The most accurate method, manual segmentation, is highly labor intensive, so automated tools like FreeSurfer have been adopted to handle this task. However, these traditional pipelines are slow and inefficient for processing large datasets. In this study, we propose TABSurfer, a novel 3D patch-based CNN-Transformer hybrid deep learning model designed for superior subcortical segmentation compared to existing state-of-the-art tools. To evaluate, we first demonstrate TABSurfer's consistent performance across various T1w MRI datasets with significantly shorter processing times compared to FreeSurfer. Then, we validate against manual segmentations, where TABSurfer outperforms FreeSurfer based on the manual ground truth. In each test, we also establish TABSurfer's advantage over a leading deep learning benchmark, FastSurferVINN. Together, these studies highlight TABSurfer's utility as a powerful tool for fully automated subcortical segmentation with high fidelity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.08267v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aaron Cao, Vishwanatha M. Rao, Kejia Liu, Xinrui Liu, Andrew F. Laine, Jia Guo</dc:creator>
    </item>
    <item>
      <title>VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular Design</title>
      <link>https://arxiv.org/abs/2506.23339</link>
      <description>arXiv:2506.23339v2 Announce Type: replace-cross 
Abstract: Large Language Models demonstrate substantial promise for advancing scientific discovery, yet their deployment in disciplines demanding factual precision and specialized domain constraints presents significant challenges. Within molecular design for pharmaceutical development, these models can propose innovative molecular modifications but frequently generate chemically infeasible structures. We introduce VALID-Mol, a comprehensive framework that integrates chemical validation with LLM-driven molecular design, achieving an improvement in valid chemical structure generation from 3% to 83%. Our methodology synthesizes systematic prompt optimization, automated chemical verification, and domain-adapted fine-tuning to ensure dependable generation of synthesizable molecules with enhanced properties. Our contribution extends beyond implementation details to provide a transferable methodology for scientifically-constrained LLM applications with measurable reliability enhancements. Computational analyses indicate our framework generates promising synthesis candidates with up to 17-fold predicted improvements in target binding affinity while preserving synthetic feasibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23339v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>physics.chem-ph</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator> Malikussaid, Hilal Hudan Nuha, Isman Kurniawan</dc:creator>
    </item>
    <item>
      <title>Oligonucleotide selective detection by levitated optomechanics</title>
      <link>https://arxiv.org/abs/2507.17940</link>
      <description>arXiv:2507.17940v3 Announce Type: replace-cross 
Abstract: This study examines the detection of oligonucleotide-specific signals in sensitive optomechanical experiments. Silica nanoparticles were functionalized using ZnCl$_2$ and 25-mers of single-stranded deoxyadenosine and deoxythymidine monophosphate which were optically trapped by a 1550 nm wavelength laser in vacuum. In the optical trap, silica nanoparticles behave as harmonic oscillators, and their oscillation frequency and amplitude can be precisely detected by optical interferometry. The data was compared across particle types, revealing differences in frequency, width and amplitude of peaks with respect to motion of the silica nanoparticles which can be explained by a theoretical model. Data obtained from this platform was analyzed by fitting Lorentzian curves to the spectra. Dimensionality reduction detected differences between the functionalized and non-functionalized silica nanoparticles. Random forest modeling provided further evidence that the fitted data were different between the groups. Transmission electron microscopy was carried out, but did not reveal any visual differences between the particle types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17940v3</guid>
      <category>physics.optics</category>
      <category>q-bio.QM</category>
      <category>quant-ph</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timothy Wilson, Owen J. L. Rackham, Hendrik Ulbricht</dc:creator>
    </item>
    <item>
      <title>A predictive modular approach to constraint satisfaction under uncertainty - with application to glycosylation in continuous monoclonal antibody biosimilar production</title>
      <link>https://arxiv.org/abs/2508.16803</link>
      <description>arXiv:2508.16803v2 Announce Type: replace-cross 
Abstract: The paper proposes a modular-based approach to constraint handling in process optimization and control. This is partly motivated by the recent interest in learning-based methods, e.g., within bioproduction, for which constraint handling under uncertainty is a challenge. The proposed constraint handler, called predictive filter, is combined with an adaptive constraint margin and a constraint violation cost monitor to minimize the cost of violating soft constraints due to model uncertainty and disturbances. The module can be combined with any controller and is based on minimally modifying the controller output, in a least squares sense, such that constraints are satisfied within the considered horizon. The proposed method is computationally efficient and suitable for real-time applications. The effectiveness of the method is illustrated through a realistic simulation case study of glycosylation constraint satisfaction in continuous monoclonal antibody biosimilar production using Chinese hamster ovary cells, for which the metabolic network model consists of 23 extracellular metabolites and 126 reactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16803v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Wang, Xiao Chen, Hubert Schwarz, V\'eronique Chotteau, Elling W. Jacobsen</dc:creator>
    </item>
  </channel>
</rss>
