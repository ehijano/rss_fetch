<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 Aug 2024 04:00:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 20 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Jet Tagging with More-Interaction Particle Transformer</title>
      <link>https://arxiv.org/abs/2407.08682</link>
      <description>arXiv:2407.08682v2 Announce Type: cross 
Abstract: In this study, we introduce the More-Interaction Particle Transformer (MIParT), a novel deep learning neural network designed for jet tagging. This framework incorporates our own design, the More-Interaction Attention (MIA) mechanism, which increases the dimensionality of particle interaction embeddings. We tested MIParT using the top tagging and quark-gluon datasets. Our results show that MIParT not only matches the accuracy and AUC of LorentzNet and a series of Lorentz-equivariant methods, but also significantly outperforms the ParT model in background rejection. Specifically, it improves background rejection by approximately 25% at a 30% signal efficiency on the top tagging dataset and by 3% on the quark-gluon dataset. Additionally, MIParT requires only 30% of the parameters and 53% of the computational complexity needed by ParT, proving that high performance can be achieved with reduced model complexity. For very large datasets, we double the dimension of particle embeddings, referring to this variant as MIParT-Large (MIParT-L). We find that MIParT-L can further capitalize on the knowledge from large datasets. From a model pre-trained on the 100M JetClass dataset, the background rejection performance of the fine-tuned MIParT-L improved by 39% on the top tagging dataset and by 6% on the quark-gluon dataset, surpassing that of the fine-tuned ParT. Specifically, the background rejection of fine-tuned MIParT-L improved by an additional 2% compared to the fine-tuned ParT. The results suggest that MIParT has the potential to advance efficiency benchmarks for jet tagging and event identification in particle physics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08682v2</guid>
      <category>hep-ph</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifan Wu, Kun Wang, Congqiao Li, Huilin Qu, Jingya Zhu</dc:creator>
    </item>
    <item>
      <title>Approximate Differentiable Likelihoods for Astroparticle Physics Experiments</title>
      <link>https://arxiv.org/abs/2408.09057</link>
      <description>arXiv:2408.09057v1 Announce Type: cross 
Abstract: Traditionally, inference in liquid xenon direct detection dark matter experiments has used estimators of event energy or density estimation of simulated data. Such methods have drawbacks compared to the computation of explicit likelihoods, such as an inability to conduct statistical inference in high-dimensional parameter spaces, or a failure to make use of all available information. In this work, we implement a continuous approximation of an event simulator model within a probabilistic programming framework, allowing for the application of high performance gradient-based inference methods such as the No-U-Turn Sampler. We demonstrate an improvement in inference results, with percent-level decreases in measurement uncertainties. Finally, in the case where some observables can be measured using multiple independent channels, such a method also enables the incorporation of additional information seamlessly, allowing for full use of the available information to be made.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09057v1</guid>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juehang Qin, Christopher Tunnell</dc:creator>
    </item>
    <item>
      <title>Large-Scale Pretraining and Finetuning for Efficient Jet Classification in Particle Physics</title>
      <link>https://arxiv.org/abs/2408.09343</link>
      <description>arXiv:2408.09343v1 Announce Type: cross 
Abstract: This study introduces an innovative approach to analyzing unlabeled data in high-energy physics (HEP) through the application of self-supervised learning (SSL). Faced with the increasing computational cost of producing high-quality labeled simulation samples at the CERN LHC, we propose leveraging large volumes of unlabeled data to overcome the limitations of supervised learning methods, which heavily rely on detailed labeled simulations. By pretraining models on these vast, mostly untapped datasets, we aim to learn generic representations that can be finetuned with smaller quantities of labeled data. Our methodology employs contrastive learning with augmentations on jet datasets to teach the model to recognize common representations of jets, addressing the unique challenges of LHC physics. Building on the groundwork laid by previous studies, our work demonstrates the critical ability of SSL to utilize large-scale unlabeled data effectively. We showcase the scalability and effectiveness of our models by gradually increasing the size of the pretraining dataset and assessing the resultant performance enhancements. Our results, obtained from experiments on two datasets -- JetClass, representing unlabeled data, and Top Tagging, serving as labeled simulation data -- show significant improvements in data efficiency, computational efficiency, and overall performance. These findings suggest that SSL can greatly enhance the adaptability of ML models to the HEP domain. This work opens new avenues for the use of unlabeled data in HEP and contributes to a better understanding the potential of SSL for scientific discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09343v1</guid>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihan Zhao, Farouk Mokhtar, Raghav Kansal, Haoyang Li, Javier Duarte</dc:creator>
    </item>
    <item>
      <title>Euler Characteristic Surfaces: A Stable Multiscale Topological Summary of Time Series Data</title>
      <link>https://arxiv.org/abs/2408.09400</link>
      <description>arXiv:2408.09400v1 Announce Type: cross 
Abstract: We present Euler Characteristic Surfaces as a multiscale spatiotemporal topological summary of time series data encapsulating the topology of the system at different time instants and length scales. Euler Characteristic Surfaces with an appropriate metric is used to quantify stability and locate critical changes in a dynamical system with respect to variations in a parameter, while being substantially computationally cheaper than available alternate methods such as persistent homology. The stability of the construction is demonstrated by a quantitative comparison bound with persistent homology, and a quantitative stability bound under small changes in time is established. The proposed construction is used to analyze two different kinds of simulated disordered flow situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09400v1</guid>
      <category>cond-mat.other</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anamika Roy, Atish J. Mitra, Tapati Dutta</dc:creator>
    </item>
    <item>
      <title>Towards a Field Based Bayesian Evidence Inference from Nested Sampling Data</title>
      <link>https://arxiv.org/abs/2408.09889</link>
      <description>arXiv:2408.09889v1 Announce Type: cross 
Abstract: Nested sampling (NS) is a stochastic method for computing the log-evidence of a Bayesian problem. It relies on stochastic estimates of prior volumes enclosed by likelihood contours, which limits the accuracy of the log-evidence calculation. We propose to transform the prior volume estimation into a Bayesian inference problem, which allows us to incorporate a smoothness assumption for likelihood-prior volume relations. As a result, we aim to increase the accuracy of the volume estimates and thus improve the overall log-evidence calculation using NS. The method presented works as a post-processing step for NS and provides posterior samples of the likelihood-prior-volume relation, from which the log-evidence can be calculated. We demonstrate an implementation of the algorithm and compare its results with plain NS on two synthetic datasets for which the underlying evidence is known. We find a significant improvement in accuracy for runs with less than one hundred active samples in NS, but are prone to numerical problems beyond this point.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09889v1</guid>
      <category>physics.comp-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Margret Westerkamp, Jakob Roth, Philipp Frank, Will Handley, Torsten En{\ss}lin</dc:creator>
    </item>
    <item>
      <title>KAN 2.0: Kolmogorov-Arnold Networks Meet Science</title>
      <link>https://arxiv.org/abs/2408.10205</link>
      <description>arXiv:2408.10205v1 Announce Type: cross 
Abstract: A major challenge of AI + Science lies in their inherent incompatibility: today's AI is primarily based on connectionism, while science depends on symbolism. To bridge the two worlds, we propose a framework to seamlessly synergize Kolmogorov-Arnold Networks (KANs) and science. The framework highlights KANs' usage for three aspects of scientific discovery: identifying relevant features, revealing modular structures, and discovering symbolic formulas. The synergy is bidirectional: science to KAN (incorporating scientific knowledge into KANs), and KAN to science (extracting scientific insights from KANs). We highlight major new functionalities in the pykan package: (1) MultKAN: KANs with multiplication nodes. (2) kanpiler: a KAN compiler that compiles symbolic formulas into KANs. (3) tree converter: convert KANs (or any neural networks) to tree graphs. Based on these tools, we demonstrate KANs' capability to discover various types of physical laws, including conserved quantities, Lagrangians, symmetries, and constitutive laws.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10205v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziming Liu, Pingchuan Ma, Yixuan Wang, Wojciech Matusik, Max Tegmark</dc:creator>
    </item>
  </channel>
</rss>
