<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 11 Apr 2025 04:00:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Effective treatment allocation strategies under partial interference</title>
      <link>https://arxiv.org/abs/2504.07305</link>
      <description>arXiv:2504.07305v1 Announce Type: new 
Abstract: Interference occurs when the potential outcomes of a unit depend on the treatment of others. Interference can be highly heterogeneous, where treating certain individuals might have a larger effect on the population's overall outcome. A better understanding of how covariates explain this heterogeneity may lead to more effective interventions. In the presence of clusters of units, we assume that interference occurs within clusters but not across them. We define novel causal estimands under hypothetical, stochastic treatment allocation strategies that fix the marginal treatment probability in a cluster and vary how the treatment probability depends on covariates, such as a unit's network position and characteristics. We illustrate how these causal estimands can shed light on the heterogeneity of interference and on the network and covariate profile of influential individuals. For experimental settings, we develop standardized weighting estimators for our novel estimands and derive their asymptotic distribution. We design an inferential procedure for testing the null hypothesis of interference homogeneity with respect to covariates. We validate the performance of the estimator and inferential procedure through simulations.We then apply the novel estimators to a clustered experiment in China to identify the important characteristics that drive heterogeneity in the effect of providing information sessions on insurance uptake.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07305v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samantha G Dean, Georgia Papadogeorgou, Laura Forastiere</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Large-Scale Classification: Error Rate Control and Optimality</title>
      <link>https://arxiv.org/abs/2504.07321</link>
      <description>arXiv:2504.07321v1 Announce Type: new 
Abstract: Classification is a fundamental task in supervised learning, while achieving valid misclassification rate control remains challenging due to possibly the limited predictive capability of the classifiers or the intrinsic complexity of the classification task. In this article, we address large-scale multi-class classification problems with general error rate guarantees to enhance algorithmic trustworthiness. To this end, we first introduce a notion of group-wise classification, which unifies the common class-wise and overall classifications as special cases. We then develop a unified algorithmic framework for the general group-wise classification that consists of three steps: Pre-classification, Selective $p$-value construction, and large-scale Post-classification decisions (PSP). Theoretically, PSP is distribution-free and provides valid finite-sample guarantees for controlling general group-wise false decision rates at target levels. To show the power of PSP, we demonstrate that the step of post-classification decisions never degrades the power of pre-classification, provided that pre-classification has been sufficiently powerful to meet the target error levels. Additionally, we further establish general power optimality theories for PSP from both non-asymptotic and asymptotic perspectives. Numerical results in both simulations and real data analysis validate the performance of the proposed PSP approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07321v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinrui Sun, Yin Xia</dc:creator>
    </item>
    <item>
      <title>What is the price of approximation? The saddlepoint approximation to a likelihood function</title>
      <link>https://arxiv.org/abs/2504.07324</link>
      <description>arXiv:2504.07324v1 Announce Type: new 
Abstract: The saddlepoint approximation to the likelihood, and its corresponding maximum likelihood estimate (MLE), offer an alternative estimation method when the true likelihood is intractable or computationally expensive. However, maximizing this approximated likelihood instead of the true likelihood inevitably comes at a price: a discrepancy between the MLE derived from the saddlepoint approximation and the true MLE. In previous studies, the size of this discrepancy has been investigated via simulation, or by engaging with the true likelihood despite its computational difficulties. Here, we introduce an explicit and computable approximation formula for the discrepancy, through which the adequacy of the saddlepoint-based MLE can be directly assessed. We present examples demonstrating the accuracy of this formula in specific cases where the true likelihood can be calculated. Additionally, we present asymptotic results that capture the behaviour of the discrepancy in a suitable limiting framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07324v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Godrick Oketch, Rachel M. Fewster, Jesse Goodman</dc:creator>
    </item>
    <item>
      <title>Estimand framework development for eGFR slope estimation and comparative analyses across various estimation methods</title>
      <link>https://arxiv.org/abs/2504.07411</link>
      <description>arXiv:2504.07411v1 Announce Type: new 
Abstract: Chronic kidney disease (CKD) is a global health challenge characterized by progressive kidney function decline, often culminating in end-stage kidney disease (ESKD) and increased mortality. To address the limitations such as the extended trial follow-up necessitated by the low incidence of kidney composite endpoint, the eGFR slope -- a surrogate endpoint reflecting the trajectory of kidney function decline -- has gained prominence for its predictive power and regulatory support. Despite its advantages, the lack of a standardized framework for eGFR slope estimand and estimation complicates consistent interpretation and cross-trial comparisons. Existing methods, including simple linear regression and mixed-effects models, vary in their underlying assumptions, creating a need for a formalized approach to align estimation methods with trial objectives. This manuscript proposes an estimand framework tailored to eGFR slope-based analyses in CKD RCTs, ensuring clarity in defining "what to estimate" and enhancing the comparability of results. Through simulation studies and real-world data applications, we evaluate the performance of various commonly applied estimation techniques under distinct scenarios. By recommending a clear characterization for eGFR slope estimand and providing considerations for estimation approaches, this work aims to improve the reliability and interpretability of CKD trial results, advancing therapeutic development and clinical decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07411v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tuo Wang, Yu Du</dc:creator>
    </item>
    <item>
      <title>Regression for Left-Truncated and Right-Censored Data: A Semiparametric Sieve Likelihood Approach</title>
      <link>https://arxiv.org/abs/2504.07413</link>
      <description>arXiv:2504.07413v1 Announce Type: new 
Abstract: Cohort studies of the onset of a disease often encounter left-truncation on the event time of interest in addition to right-censoring due to variable enrollment times of study participants. Analysis of such event time data can be biased if left-truncation is not handled properly. We propose a semiparametric sieve likelihood approach for fitting a linear regression model to data where the response variable is subject to both left-truncation and right-censoring. We show that the estimators of regression coefficients are consistent, asymptotically normal and semiparametrically efficient. Extensive simulation studies show the effectiveness of the method across a wide variety of error distributions. We further illustrate the method by analyzing a dataset from The 90+ Study for aging and dementia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07413v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Spencer Matthews, Bin Nan</dc:creator>
    </item>
    <item>
      <title>Conditional Data Synthesis Augmentation</title>
      <link>https://arxiv.org/abs/2504.07426</link>
      <description>arXiv:2504.07426v1 Announce Type: new 
Abstract: Reliable machine learning and statistical analysis rely on diverse, well-distributed training data. However, real-world datasets are often limited in size and exhibit underrepresentation across key subpopulations, leading to biased predictions and reduced performance, particularly in supervised tasks such as classification. To address these challenges, we propose Conditional Data Synthesis Augmentation (CoDSA), a novel framework that leverages generative models, such as diffusion models, to synthesize high-fidelity data for improving model performance across multimodal domains including tabular, textual, and image data. CoDSA generates synthetic samples that faithfully capture the conditional distributions of the original data, with a focus on under-sampled or high-interest regions. Through transfer learning, CoDSA fine-tunes pre-trained generative models to enhance the realism of synthetic data and increase sample density in sparse areas. This process preserves inter-modal relationships, mitigates data imbalance, improves domain adaptation, and boosts generalization. We also introduce a theoretical framework that quantifies the statistical accuracy improvements enabled by CoDSA as a function of synthetic sample volume and targeted region allocation, providing formal guarantees of its effectiveness. Extensive experiments demonstrate that CoDSA consistently outperforms non-adaptive augmentation strategies and state-of-the-art baselines in both supervised and unsupervised settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07426v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Tian, Xiaotong Shen</dc:creator>
    </item>
    <item>
      <title>Detecting changes in space-varying parameters of local Poisson point processes</title>
      <link>https://arxiv.org/abs/2504.07915</link>
      <description>arXiv:2504.07915v1 Announce Type: new 
Abstract: Recent advances in local models for point processes have highlighted the need for flexible methodologies to account for the spatial heterogeneity of external covariates influencing process intensity. In this work, we introduce tessellated spatial regression, a novel framework that extends segmented regression models to spatial point processes, with the aim of detecting abrupt changes in the effect of external covariates onto the process intensity.
  Our approach consists of two main steps. First, we apply a spatial segmentation algorithm to geographically weighted regression estimates, generating different tessellations that partition the study area into regions where model parameters can be assumed constant. Next, we fit log-linear Poisson models in which covariates interact with the tessellations, enabling region-specific parameter estimation and classical inferential procedures, such as hypothesis testing on regression coefficients.
  Unlike geographically weighted regression, our approach allows for discrete changes in regression coefficients, making it possible to capture abrupt spatial variations in the effect of real-valued spatial covariates. Furthermore, the method naturally addresses the problem of locating and quantifying the number of detected spatial changes.
  We validate our methodology through simulation studies and applications to two examples where a model with region-wise parameters seems appropriate and to an environmental dataset of earthquake occurrences in Greece.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07915v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Nicoletta D'Angelo</dc:creator>
    </item>
    <item>
      <title>Characteristic function-based tests for spatial randomness</title>
      <link>https://arxiv.org/abs/2504.07946</link>
      <description>arXiv:2504.07946v1 Announce Type: new 
Abstract: We introduce a new type of test for complete spatial randomness that applies to mapped point patterns in a rectangle or a cube of any dimension. This is the first test of its kind to be based on characteristic functions and utilizes a weighted L2-distance between the empirical and uniform characteristic functions. It is simple to calculate and does not require adjusting for edge effects. An efficient algorithm is developed to find the asymptotic null distribution of the test statistic under the Cauchy weight function. In a simulation, our test shows varying sensitivity to different levels of spatial interaction depending on the scale parameter of the Cauchy weight function. Tests with different parameter values can be combined to create a Bonferroni-corrected omnibus test, which is almost always more powerful than the popular L-test and the Clark-Evans test for detecting heterogeneous and aggregated alternatives, although less powerful than the L-test for detecting regular alternatives. The simplicity of empirical characteristic function makes it straightforward to extend our test to non-rectangular or sparsely sampled point patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07946v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Zeng, Dale L. Zimmerman</dc:creator>
    </item>
    <item>
      <title>NFL Draft Modelling: Loss Functional Analysis</title>
      <link>https://arxiv.org/abs/2504.07291</link>
      <description>arXiv:2504.07291v1 Announce Type: cross 
Abstract: In the NFL draft, teams must strategically balance immediate player impact against long-term value, presenting a complex optimization challenge for draft capital management. This paper introduces a framework for evaluating the fairness and efficiency of draft pick trades using norm-based loss functions. Draft pick valuations are modelled by the Weibull distribution. Utilizing these valuation techniques, the research identifies key trade-offs between aggressive, immediate-impact strategies and conservative, risk-averse approaches. Ultimately, this framework serves as a valuable analytical tool for assessing NFL draft trade fairness and value distribution, aiding team decision-makers and enriching insights within the sports analytics community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07291v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tanmay Grandhisiri</dc:creator>
    </item>
    <item>
      <title>Geological Inference from Textual Data using Word Embeddings</title>
      <link>https://arxiv.org/abs/2504.07490</link>
      <description>arXiv:2504.07490v1 Announce Type: cross 
Abstract: This research explores the use of Natural Language Processing (NLP) techniques to locate geological resources, with a specific focus on industrial minerals. By using word embeddings trained with the GloVe model, we extract semantic relationships between target keywords and a corpus of geological texts. The text is filtered to retain only words with geographical significance, such as city names, which are then ranked by their cosine similarity to the target keyword. Dimensional reduction techniques, including Principal Component Analysis (PCA), Autoencoder, Variational Autoencoder (VAE), and VAE with Long Short-Term Memory (VAE-LSTM), are applied to enhance feature extraction and improve the accuracy of semantic relations.
  For benchmarking, we calculate the proximity between the ten cities most semantically related to the target keyword and identified mine locations using the haversine equation. The results demonstrate that combining NLP with dimensional reduction techniques provides meaningful insights into the spatial distribution of natural resources. Although the result shows to be in the same region as the supposed location, the accuracy has room for improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07490v1</guid>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nanmanas Linphrachaya, Irving G\'omez-M\'endez, Adil Siripatana</dc:creator>
    </item>
    <item>
      <title>nimblewomble: An R package for Bayesian Wombling with nimble</title>
      <link>https://arxiv.org/abs/2504.07673</link>
      <description>arXiv:2504.07673v1 Announce Type: cross 
Abstract: This exposition presents nimblewomble, a software package to perform wombling, or boundary analysis, using the nimble Bayesian hierarchical modeling language in the R statistical computing environment. Wombling is used widely to track regions of rapid change within the spatial reference domain. Specific functions in the package implement Gaussian process models for point-referenced spatial data followed by predictive inference on rates of change over curves using line integrals. We demonstrate model based Bayesian inference using posterior distributions featuring simple analytic forms while offering uncertainty quantification over curves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07673v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aritra Halder, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Relaxing the Markov Requirements on Reinforcement Learning Under Weak Partial Ignorability</title>
      <link>https://arxiv.org/abs/2504.07722</link>
      <description>arXiv:2504.07722v1 Announce Type: cross 
Abstract: Incomplete data, confounding effects, and violations of the Markov property are interrelated problems which are ubiquitous in Reinforcement Learning applications. We introduce the concept of ``partial ignorabilty" and leverage it to establish a novel convergence theorem for adaptive Reinforcement Learning. This theoretical result relaxes the Markov assumption on the stochastic process underlying conventional $Q$-learning, deploying a generalized form of the Robbins-Monro stochastic approximation theorem to establish optimality. This result has clear downstream implications for most active subfields of Reinforcement Learning, with clear paths for extension to the field of Causal Inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07722v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>MaryLena Bleile</dc:creator>
    </item>
    <item>
      <title>A Recursive Partitioning Approach for Dynamic Discrete Choice Modeling in High Dimensional Settings</title>
      <link>https://arxiv.org/abs/2208.01476</link>
      <description>arXiv:2208.01476v2 Announce Type: replace 
Abstract: Dynamic discrete choice models are widely employed to answer substantive and policy questions in settings where individuals' current choices have future implications. However, estimation of these models is often computationally intensive and/or infeasible in high-dimensional settings. Indeed, even specifying the structure for how the utilities/state transitions enter the agent's decision is challenging in high-dimensional settings when we have no guiding theory. In this paper, we present a semi-parametric formulation of dynamic discrete choice models that incorporates a high-dimensional set of state variables, in addition to the standard variables used in a parametric utility function. The high-dimensional variable can include all the variables that are not the main variables of interest but may potentially affect people's choices and must be included in the estimation procedure, i.e., control variables. We present a data-driven recursive partitioning algorithm that reduces the dimensionality of the high-dimensional state space by taking the variation in choices and state transition into account. Researchers can then use the method of their choice to estimate the problem using the discretized state space from the first stage. Our approach can reduce the estimation bias and make estimation feasible at the same time. We present Monte Carlo simulations to demonstrate the performance of our method compared to standard estimation methods where we ignore the high-dimensional explanatory variable set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.01476v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ebrahim Barzegary, Hema Yoganarasimhan</dc:creator>
    </item>
    <item>
      <title>Estimation of High-Dimensional Markov-Switching VAR Models with an Approximate EM Algorithm</title>
      <link>https://arxiv.org/abs/2210.07456</link>
      <description>arXiv:2210.07456v2 Announce Type: replace 
Abstract: Regime shifts in high-dimensional time series arise naturally in many applications, from neuroimaging to finance. This problem has received considerable attention in low-dimensional settings, with both Bayesian and frequentist methods used extensively for parameter estimation. The EM algorithm is a particularly popular strategy for parameter estimation in low-dimensional settings, although the statistical properties of the resulting estimates have not been well understood. Furthermore, its extension to high-dimensional time series has proved challenging. To overcome these challenges, in this paper we propose an approximate EM algorithm for Markov-switching VAR models that leads to efficient computation and also facilitates the investigation of asymptotic properties of the resulting parameter estimates. We establish the consistency of the proposed EM algorithm in high dimensions and investigate its performance via simulation studies. We also demonstrate the algorithm by analyzing a brain electroencephalography (EEG) dataset recorded on a patient experiencing epileptic seizure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.07456v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiudi Li, Abolfazl Safikhani, Ali Shojaie</dc:creator>
    </item>
    <item>
      <title>Tree-Regularized Bayesian Latent Class Analysis for Improving Weakly Separated Dietary Pattern Subtyping in Small-Sized Subpopulations</title>
      <link>https://arxiv.org/abs/2306.04700</link>
      <description>arXiv:2306.04700v2 Announce Type: replace 
Abstract: Dietary patterns synthesize multiple related diet components, which can be used by nutrition researchers to examine diet-disease relationships. Latent class models (LCMs) have been used to derive dietary patterns from dietary intake assessment, where each class profile represents the probabilities of exposure to a set of diet components. However, LCM-derived dietary patterns can exhibit strong similarities, or weak separation, resulting in numerical and inferential instabilities that challenge scientific interpretation. This issue is exacerbated in small-sized subpopulations. To address these issues, we provide a simple solution that empowers LCMs to improve dietary pattern estimation. We develop a tree-regularized Bayesian LCM that shares statistical strength between dietary patterns to make better estimates using limited data. This is achieved via a Dirichlet diffusion tree process that specifies a prior distribution for the unknown tree over classes. Dietary patterns that share proximity to one another in the tree are shrunk towards ancestral dietary patterns a priori, with the degree of shrinkage varying across pre-specified food groups. Using dietary intake data from the Hispanic Community Health Study/Study of Latinos, we apply the proposed approach to a sample of 496 US adults of South American ethnic background to identify and compare dietary patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.04700v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengbing Li, Briana Stephenson, Zhenke Wu</dc:creator>
    </item>
    <item>
      <title>A smoothed-Bayesian approach to frequency recovery from sketched data</title>
      <link>https://arxiv.org/abs/2309.15408</link>
      <description>arXiv:2309.15408v4 Announce Type: replace 
Abstract: We provide a novel statistical perspective on a classical problem at the intersection of computer science and information theory: recovering the empirical frequency of a symbol in a large discrete dataset using only a compressed representation, or sketch, obtained via random hashing. Departing from traditional algorithmic approaches, recent works have proposed Bayesian nonparametric (BNP) methods that can provide more informative frequency estimates by leveraging modeling assumptions about the distribution of the sketched data. In this paper, we propose a smoothed-Bayesian method, inspired by existing BNP approaches but designed in a frequentist framework to overcome the computational limitations of the BNP approaches when dealing with large-scale data from realistic distributions, including those with power-law tail behaviors. For sketches obtained with a single hash function, our approach is supported by rigorous frequentist properties, including unbiasedness and optimality under a squared error loss function within an intuitive class of linear estimators. For sketches with multiple hash functions, we introduce an approach based on multi-view learning to construct computationally efficient frequency estimators. We validate our method on synthetic and real data, comparing its performance to that of existing alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15408v4</guid>
      <category>stat.ME</category>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Beraha, Stefano Favaro, Matteo Sesia</dc:creator>
    </item>
    <item>
      <title>An energy-based model approach to rare event probability estimation</title>
      <link>https://arxiv.org/abs/2310.04082</link>
      <description>arXiv:2310.04082v2 Announce Type: replace 
Abstract: The estimation of rare event probabilities plays a pivotal role in diverse fields. Our aim is to determine the probability of a hazard or system failure occurring when a quantity of interest exceeds a critical value. In our approach, the distribution of the quantity of interest is represented by an energy density, characterized by a free energy function. To efficiently estimate the free energy, a bias potential is introduced. Using concepts from energy-based models (EBM), this bias potential is optimized such that the corresponding probability density function approximates a pre-defined distribution targeting the failure region of interest. Given the optimal bias potential, the free energy function and the rare event probability of interest can be determined. The approach is applicable not just in traditional rare event settings where the variable upon which the quantity of interest relies has a known distribution, but also in inversion settings where the variable follows a posterior distribution. By combining the EBM approach with a Stein discrepancy-based stopping criterion, we aim for a balanced accuracy-efficiency trade-off. Furthermore, we explore both parametric and non-parametric approaches for the bias potential, with the latter eliminating the need for choosing a particular parameterization, but depending strongly on the accuracy of the kernel density estimate used in the optimization process. Through three illustrative test cases encompassing both traditional and inversion settings, we show that the proposed EBM approach, when properly configured, (i) allows stable and efficient estimation of rare event probabilities and (ii) compares favorably against subset sampling approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.04082v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>SIAM/ASA Journal on Uncertainty Quantification, Vol. 13, Iss. 2 (2025)</arxiv:journal_reference>
      <dc:creator>Lea Friedli, David Ginsbourger, Arnaud Doucet, Niklas Linde</dc:creator>
    </item>
    <item>
      <title>A new way to evaluate G-Wishart normalising constants via Fourier analysis</title>
      <link>https://arxiv.org/abs/2404.06803</link>
      <description>arXiv:2404.06803v2 Announce Type: replace 
Abstract: The G-Wishart distribution is an essential component for the Bayesian analysis of Gaussian graphical models as the conjugate prior for the precision matrix. Evaluating the marginal likelihood of such models usually requires computing high-dimensional integrals to determine the G-Wishart normalising constant. Closed-form results are known for decomposable or chordal graphs, while an explicit representation as a formal series expansion has been derived recently for general graphs. The nested infinite sums, however, do not lend themselves to computation, remaining of limited practical value. Borrowing techniques from random matrix theory and Fourier analysis, we provide novel exact results well suited to the numerical evaluation of the normalising constant for classes of graphs beyond chordal graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06803v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ching Wong, Giusi Moffa, Jack Kuipers</dc:creator>
    </item>
    <item>
      <title>Model Uncertainty in Latent Gaussian Models with Univariate Link Function</title>
      <link>https://arxiv.org/abs/2406.17318</link>
      <description>arXiv:2406.17318v2 Announce Type: replace 
Abstract: We consider a class of latent Gaussian models with a univariate link function (ULLGMs). These are based on standard likelihood specifications (such as Poisson, Binomial, Bernoulli, Erlang, etc.) but incorporate a latent normal linear regression framework on a transformation of a key scalar parameter. We allow for model uncertainty regarding the covariates included in the regression. The ULLGM class typically accommodates extra dispersion in the data and has clear advantages for deriving theoretical properties and designing computational procedures. We formally characterize posterior existence under a convenient and popular improper prior and show that ULLGMs inherit the consistency properties from the latent Gaussian model. We propose a simple and general Markov chain Monte Carlo algorithm for Bayesian model averaging in ULLGMs. Simulation results suggest that the framework provides accurate results that are robust to some degree of misspecification. The methodology is successfully applied to measles vaccination coverage data from Ethiopia and to data on bilateral migration flows between OECD countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17318v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark F. J. Steel, Gregor Zens</dc:creator>
    </item>
    <item>
      <title>Causal generalized linear models via Pearson risk invariance</title>
      <link>https://arxiv.org/abs/2407.16786</link>
      <description>arXiv:2407.16786v2 Announce Type: replace 
Abstract: Prediction invariance of causal models under heterogeneous settings has been exploited by a number of recent methods for causal discovery, typically focussing on recovering the causal parents of a target variable of interest. Existing methods require observational data from a number of sufficiently different environments, which is rarely available. In this paper, we consider a structural equation model where the target variable is described by a generalized linear model conditional on its parents. Besides having finite moments, no modelling assumptions are made on the conditional distributions of the other variables in the system, and nonlinear effects on the target variable can naturally be accommodated by a generalized additive structure. Under this setting, we characterize the causal model uniquely by means of two key properties: the Pearson risk invariant under the causal model and, conditional on the causal parents, the causal parameters maximize the expected likelihood. These two properties form the basis of a computational strategy for searching the causal model among all possible models. A stepwise greedy search is proposed for systems with a large number of variables. Crucially, for generalized linear models with a known dispersion parameter, such as Poisson and logistic regression, the causal model can be identified from a single data environment. The method is implemented in the R package causalreg.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16786v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alice Polinelli, Veronica Vinciotti, Ernst C. Wit</dc:creator>
    </item>
    <item>
      <title>Stabilized Inverse Probability Weighting via Isotonic Calibration</title>
      <link>https://arxiv.org/abs/2411.06342</link>
      <description>arXiv:2411.06342v3 Announce Type: replace 
Abstract: Inverse weighting with an estimated propensity score is widely used by estimation methods in causal inference to adjust for confounding bias. However, directly inverting propensity score estimates can lead to instability, bias, and excessive variability due to large inverse weights, especially when treatment overlap is limited. In this work, we propose a post-hoc calibration algorithm for inverse propensity weights that generates well-calibrated, stabilized weights from user-supplied, cross-fitted propensity score estimates. Our approach employs a variant of isotonic regression with a loss function specifically tailored to the inverse propensity weights. Through theoretical analysis and empirical studies, we demonstrate that isotonic calibration improves the performance of doubly robust estimators of the average treatment effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06342v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars van der Laan, Ziming Lin, Marco Carone, Alex Luedtke</dc:creator>
    </item>
    <item>
      <title>Design of Bayesian Clinical Trials with Clustered Data and Multiple Endpoints</title>
      <link>https://arxiv.org/abs/2501.13218</link>
      <description>arXiv:2501.13218v2 Announce Type: replace 
Abstract: In the design of clinical trials, it is essential to assess the design operating characteristics (i.e., the probabilities of making correct decisions). Common practice for the evaluation of operating characteristics in Bayesian clinical trials relies on estimating the sampling distribution of posterior summaries via Monte Carlo simulation. It is computationally intensive to repeat this estimation process for each design configuration considered, particularly for clustered data that are analyzed using complex, high-dimensional models. In this paper, we propose an efficient method to assess operating characteristics and determine sample sizes for Bayesian trials with clustered data and multiple endpoints. We prove theoretical results that enable posterior probabilities to be modeled as a function of the sample size. Using these functions, we assess operating characteristics at a range of sample sizes given simulations conducted at only two sample sizes. These theoretical results are also leveraged to quantify the impact of simulation variability on our sample size recommendations. The applicability of our methodology is illustrated using a current cluster-randomized Bayesian adaptive clinical trial with multiple endpoints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13218v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luke Hagar, Shirin Golchi</dc:creator>
    </item>
    <item>
      <title>A retake on the analysis of scores truncated by terminal events</title>
      <link>https://arxiv.org/abs/2502.03942</link>
      <description>arXiv:2502.03942v4 Announce Type: replace 
Abstract: Analysis of data from randomized controlled trials in vulnerable populations requires special attention when assessing treatment effect by a score measuring, e.g., disease stage or activity together with onset of prevalent terminal events. In reality, it is impossible to disentangle a disease score from the terminal event, since the score is not clinically meaningful after this event. In this work, we propose to assess treatment interventions simultaneously on disease score and the terminal event. Our proposal is based on a natural data-generating mechanism respecting that a disease score does not exist beyond the terminal event. We use modern semi-parametric statistical methods to provide robust and efficient estimation of the risk of terminal event and expected disease score conditional on no terminal event at a pre-specified landmark time. We also use the simultaneous asymptotic behavior of our estimators to develop a powerful closed testing procedure for confirmatory assessment of treatment effect on both onset of terminal event and level of disease score. A simulation study mimicking a large-scale outcome trial in chronic kidney patients as well as an analysis of that trial is provided to assess performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03942v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Klaus K\"ahler Holst, Andreas Nordland, Julie Funch Furberg, Lars Holm Damgaard, Christian Bressen Pipper</dc:creator>
    </item>
    <item>
      <title>Graphical Transformation Models</title>
      <link>https://arxiv.org/abs/2503.17845</link>
      <description>arXiv:2503.17845v2 Announce Type: replace 
Abstract: Graphical Transformation Models (GTMs) are introduced as a novel approach to effectively model multivariate data with intricate marginals and complex dependency structures non-parametrically, while maintaining interpretability through the identification of varying conditional independencies. GTMs extend multivariate transformation models by replacing the Gaussian copula with a custom-designed multivariate transformation, offering two major advantages. Firstly, GTMs can capture more complex interdependencies using penalized splines, which also provide an efficient regularization scheme. Secondly, we demonstrate how to approximately regularize GTMs using a lasso penalty towards pairwise conditional independencies, akin to Gaussian graphical models. The model's robustness and effectiveness are validated through simulations, showcasing its ability to accurately learn parametric vine copulas and identify conditional independencies. Additionally, the model is applied to a benchmark astrophysics dataset, where the GTM demonstrates favorable performance compared to non-parametric vine copulas in learning complex multivariate distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17845v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Herp, Johannes Brachem, Michael Altenbuchinger, Thomas Kneib</dc:creator>
    </item>
    <item>
      <title>Conformal Anomaly Detection for Functional Data with Elastic Distance Metrics</title>
      <link>https://arxiv.org/abs/2504.01172</link>
      <description>arXiv:2504.01172v2 Announce Type: replace 
Abstract: This paper considers the problem of outlier detection in functional data analysis focusing particularly on the more difficult case of shape outliers. We present an inductive conformal anomaly detection method based on elastic functional distance metrics. This method is evaluated and compared to similar conformal anomaly detection methods for functional data using simulation experiments. The method is also used in the analysis of two real exemplar data sets that show its utility in practical applications. The results demonstrate the efficacy of the proposed method for detecting both magnitude and shape outliers in two distinct outlier detection scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01172v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason Adams, Brandon Berman, Joshua Michalenko, J. Derek Tucker</dc:creator>
    </item>
    <item>
      <title>Assessment of the quality of a prediction</title>
      <link>https://arxiv.org/abs/2404.15764</link>
      <description>arXiv:2404.15764v5 Announce Type: replace-cross 
Abstract: Shannon defined the mutual information between two variables. We illustrate why the true mutual information between a variable and the predictions made by a prediction algorithm is not a suitable measure of prediction quality, but the apparent Shannon mutual information (ASI) is; indeed it is the unique prediction quality measure with either of two very different lists of desirable properties, as previously shown by de Finetti and other authors. However, estimating the uncertainty of the ASI is a difficult problem, because of long and non-symmetric heavy tails to the distribution of the individual values of $j(x,y)=\log\frac{Q_y(x)}{P(x)}$ We propose a Bayesian modelling method for the distribution of $j(x,y)$, from the posterior distribution of which the uncertainty in the ASI can be inferred. This method is based on Dirichlet-based mixtures of skew-Student distributions. We illustrate its use on data from a Bayesian model for prediction of the recurrence time of prostate cancer. We believe that this approach is generally appropriate for most problems, where it is infeasible to derive the explicit distribution of the samples of $j(x,y)$, though the precise modelling parameters may need adjustment to suit particular cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15764v5</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Roger Sewell</dc:creator>
    </item>
  </channel>
</rss>
