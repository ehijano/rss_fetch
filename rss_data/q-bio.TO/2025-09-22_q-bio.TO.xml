<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.TO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.TO</link>
    <description>q-bio.TO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.TO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Sep 2025 04:09:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A study on Deep Convolutional Neural Networks, transfer learning, and Mnet model for Cervical Cancer Detection</title>
      <link>https://arxiv.org/abs/2509.16250</link>
      <description>arXiv:2509.16250v1 Announce Type: new 
Abstract: Early and accurate detection through Pap smear analysis is critical to improving patient outcomes and reducing mortality of Cervical cancer. State-of-the-art (SOTA) Convolutional Neural Networks (CNNs) require substantial computational resources, extended training time, and large datasets. In this study, a lightweight CNN model, S-Net (Simple Net), is developed specifically for cervical cancer detection and classification using Pap smear images to address these limitations. Alongside S-Net, six SOTA CNNs were evaluated using transfer learning, including multi-path (DenseNet201, ResNet152), depth-based (Serasnet152), width-based multi-connection (Xception), depth-wise separable convolutions (MobileNetV2), and spatial exploitation-based (VGG19). All models, including S-Net, achieved comparable accuracy, with S-Net reaching 99.99%. However, S-Net significantly outperforms the SOTA CNNs in terms of computational efficiency and inference time, making it a more practical choice for real-time and resource-constrained applications. A major limitation in CNN-based medical diagnosis remains the lack of transparency in the decision-making process. To address this, Explainable AI (XAI) techniques, such as SHAP, LIME, and Grad-CAM, were employed to visualize and interpret the key image regions influencing model predictions. The novelty of this study lies in the development of a highly accurate yet computationally lightweight model (S-Net) caPable of rapid inference while maintaining interpretability through XAI integration. Furthermore, this work analyzes the behavior of SOTA CNNs, investigates the effects of negative transfer learning on Pap smear images, and examines pixel intensity patterns in correctly and incorrectly classified samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16250v1</guid>
      <category>q-bio.TO</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saifuddin Sagor, Md Taimur Ahad, Faruk Ahmed, Rokonozzaman Ayon, Sanzida Parvin</dc:creator>
    </item>
    <item>
      <title>R-Net: A Reliable and Resource-Efficient CNN for Colorectal Cancer Detection with XAI Integration</title>
      <link>https://arxiv.org/abs/2509.16251</link>
      <description>arXiv:2509.16251v1 Announce Type: new 
Abstract: State-of-the-art (SOTA) Convolutional Neural Networks (CNNs) are criticized for their extensive computational power, long training times, and large datasets. To overcome this limitation, we propose a reasonable network (R-Net), a lightweight CNN only to detect and classify colorectal cancer (CRC) using the Enteroscope Biopsy Histopathological Hematoxylin and Eosin Image Dataset (EBHI). Furthermore, six SOTA CNNs, including Multipath-based CNNs (DenseNet121, ResNet50), Depth-based CNNs (InceptionV3), width-based multi-connection CNNs (Xception), depth-wise separable convolutions (MobileNetV2), spatial exploitation-based CNNs (VGG16), Transfer learning, and two ensemble models are also tested on the same dataset. The ensemble models are a multipath-depth-width combination (DenseNet121-InceptionV3-Xception) and a multipath-depth-spatial combination (ResNet18-InceptionV3-VGG16). However, the proposed R-Net lightweight achieved 99.37% accuracy, outperforming MobileNet (95.83%) and ResNet50 (96.94%). Most importantly, to understand the decision-making of R-Net, Explainable AI such as SHAP, LIME, and Grad-CAM are integrated to visualize which parts of the EBHI image contribute to the detection and classification process of R-Net. The main novelty of this research lies in building a reliable, lightweight CNN R-Net that requires fewer computing resources yet maintains strong prediction results. SOTA CNNs, transfer learning, and ensemble models also extend our knowledge on CRC classification and detection. XAI functionality and the impact of pixel intensity on correct and incorrect classification images are also some novelties in CRC detection and classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16251v1</guid>
      <category>q-bio.TO</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rokonozzaman Ayon, Md Taimur Ahad, Bo Song, Yan Li</dc:creator>
    </item>
    <item>
      <title>Imaging Modalities-Based Classification for Lung Cancer Detection</title>
      <link>https://arxiv.org/abs/2509.16254</link>
      <description>arXiv:2509.16254v1 Announce Type: new 
Abstract: Lung cancer continues to be the predominant cause of cancer-related mortality globally. This review analyzes various approaches, including advanced image processing methods, focusing on their efficacy in interpreting CT scans, chest radiographs, and biological markers. Notably, we identify critical gaps in the previous surveys, including the need for robust models that can generalize across diverse populations and imaging modalities. This comprehensive synthesis aims to serve as a foundational resource for researchers and clinicians, guiding future efforts toward more accurate and efficient lung cancer detection. Key findings reveal that 3D CNN architectures integrated with CT scans achieve the most superior performances, yet challenges such as high false positives, dataset variability, and computational complexity persist across modalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16254v1</guid>
      <category>q-bio.TO</category>
      <category>cs.AI</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sajim Ahmed, Muhammad Zain Chaudhary, Muhammad Zohaib Chaudhary, Mahmoud Abbass, Ahmed Sherif, Mohammad Mahbubur Rahman Khan Mamun</dc:creator>
    </item>
    <item>
      <title>RootletSeg: Deep learning method for spinal rootlets segmentation across MRI contrasts</title>
      <link>https://arxiv.org/abs/2509.16255</link>
      <description>arXiv:2509.16255v1 Announce Type: new 
Abstract: Purpose: To develop a deep learning method for the automatic segmentation of spinal nerve rootlets on various MRI scans. Material and Methods: This retrospective study included MRI scans from two open-access and one private dataset, consisting of 3D isotropic 3T TSE T2-weighted (T2w) and 7T MP2RAGE (T1-weighted [T1w] INV1 and INV2, and UNIT1) MRI scans. A deep learning model, RootletSeg, was developed to segment C2-T1 dorsal and ventral spinal rootlets. Training was performed on 76 scans and testing on 17 scans. The Dice score was used to compare the model performance with an existing open-source method. Spinal levels derived from RootletSeg segmentations were compared with vertebral levels defined by intervertebral discs using Bland-Altman analysis. Results: The RootletSeg model developed on 93 MRI scans from 50 healthy adults (mean age, 28.70 years $\pm$ 6.53 [SD]; 28 [56%] males, 22 [44%] females) achieved a mean $\pm$ SD Dice score of 0.67 $\pm$ 0.09 for T1w-INV2, 0.65 $\pm$ 0.11 for UNIT1, 0.64 $\pm$ 0.08 for T2w, and 0.62 $\pm$ 0.10 for T1w-INV1 contrasts. Spinal-vertebral level correspondence showed a progressively increasing rostrocaudal shift, with Bland-Altman bias ranging from 0.00 to 8.15 mm (median difference between level midpoints). Conclusion: RootletSeg accurately segmented C2-T1 spinal rootlets across MRI contrasts, enabling the determination of spinal levels directly from MRI scans. The method is open-source and can be used for a variety of downstream analyses, including lesion classification, neuromodulation therapy, and functional MRI group analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16255v1</guid>
      <category>q-bio.TO</category>
      <category>eess.IV</category>
      <category>physics.med-ph</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katerina Krejci, Jiri Chmelik, Sandrine B\'edard, Falk Eippert, Ulrike Horn, Virginie Callot, Julien Cohen-Adad, Jan Valosek</dc:creator>
    </item>
    <item>
      <title>The Role of High-Performance GPU Resources in Large Language Model Based Radiology Imaging Diagnosis</title>
      <link>https://arxiv.org/abs/2509.16328</link>
      <description>arXiv:2509.16328v1 Announce Type: new 
Abstract: Large-language models (LLMs) are rapidly being applied to radiology, enabling automated image interpretation and report generation tasks. Their deployment in clinical practice requires both high diagnostic accuracy and low inference latency, which in turn demands powerful hardware. High-performance graphical processing units (GPUs) provide the necessary compute and memory throughput to run large LLMs on imaging data. We review modern GPU architectures (e.g. NVIDIA A100/H100, AMD Instinct MI250X/MI300) and key performance metrics of floating-point throughput, memory bandwidth, VRAM capacity. We show how these hardware capabilities affect radiology tasks: for example, generating reports or detecting findings on CheXpert and MIMIC-CXR images is computationally intensive and benefits from GPU parallelism and tensor-core acceleration. Empirical studies indicate that using appropriate GPU resources can reduce inference time and improve throughput. We discuss practical challenges including privacy, deployment, cost, power and optimization strategies: mixed-precision, quantization, compression, and multi-GPU scaling. Finally, we anticipate that next-generation features (8-bit tensor cores, enhanced interconnect) will further enable on-premise and federated radiology AI. Advancing GPU infrastructure is essential for safe, efficient LLM-based radiology diagnostics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16328v1</guid>
      <category>q-bio.TO</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jyun-Ping Kao</dc:creator>
    </item>
    <item>
      <title>Magnetically Guided Endothelial BioBots: A Next-Generation Strategy for Treating Complex Cerebral Aneurysms</title>
      <link>https://arxiv.org/abs/2509.17854</link>
      <description>arXiv:2509.17854v1 Announce Type: new 
Abstract: Cerebral aneurysms affect three to five percent of the population, and rupture remains a major cause of stroke-related death and disability. Current therapies, surgical clipping, endovascular coiling, and flow diversion, have improved outcomes but each carries limitations. Clipping is invasive and often unsuitable for deep or posterior lesions. Coiling is prone to recurrence from compaction or incomplete occlusion, particularly in wide-neck or fusiform aneurysms. Flow diverters offer improved durability but rely on rigid metallic scaffolds that may malappose in tortuous vessels, compromise branch arteries, delay endothelialization, and necessitate long-term dual antiplatelet therapy. These shortcomings highlight a gap in current management: devices primarily provide mechanical occlusion but fail to conform to complex geometries or reliably promote rapid, complete endothelialization. As a result, aneurysm necks may remain exposed to persistent flow, delayed healing, and thrombosis.
  To address this, we propose magnetically guided endothelial BioBots as a next-generation therapeutic strategy. BioBots are biodegradable hydrogel carriers embedded with magnetic nanoparticles and coated with primed endothelial progenitor cells. Delivered through microcatheters and guided by external electromagnetic fields, they can assemble across aneurysm defects. Once localized, they form a conformal, geometry-adaptive endothelial patch that provides immediate antithrombotic protection and, as the hydrogel degrades, leaves behind a stable, functional endothelial lining. By integrating microrobotic navigation with regenerative vascular biology, BioBots may overcome the central limitations of current devices and enable safer, more durable treatment for complex aneurysms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17854v1</guid>
      <category>q-bio.TO</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Duong Le (Department of Biomedical Engineering, University of Massachusetts Amherst, Amherst, MA)</dc:creator>
    </item>
    <item>
      <title>Cycling and tensed cells interpenetrated by non-cycling and compressed cells form a critical epithelial reticulum</title>
      <link>https://arxiv.org/abs/2509.16661</link>
      <description>arXiv:2509.16661v1 Announce Type: cross 
Abstract: With the completion of development and wound repair, as the epithelium approaches homeostasis, cell proliferation is reduced to a minimum. In parallel, cellular motion transitions from a migratory unjammed state to a quiescent jammed state. This quiescent state is commonly regarded as devoid of large-scale regional variations in cell-cycle re-entry and cellular mechanics. To the contrary, here we report that during late maturation there arises a heretofore unanticipated epithelial reticulum that is supracellular and spans multiple scales of length. This reticulum evolves dynamically and comprises two interpenetrating networks: large regions of cycling and mechanically tensed cells, embedded with islands of non-cycling and mechanically compressed cells. The islands of compressed cells emerge and grow in cell numbers, with gradual jamming and with reduced cellular rearrangements. We show how island growth is both reversible, by provoking unjamming, and detainable, by cell cycle arrest treatment. Moreover, the distribution of island sizes was found to conform to a power-law, thus leading us to employ a computational model of percolating critical networks. Together, the observations indicate that the newly discovered epithelial reticulum self-organizes close to but just shy of criticality - thus avoiding mergers of compressed cell islands. This quasi-criticality reframes epithelial homeostasis as a dynamic regional balance of forces and proliferation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16661v1</guid>
      <category>q-bio.CB</category>
      <category>cond-mat.soft</category>
      <category>nlin.AO</category>
      <category>physics.bio-ph</category>
      <category>q-bio.TO</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liav Daraf, Yael Lavi, Areej Saleem, Daniel Sevilla Sanchez, Yuri Feldman, Lior Atia</dc:creator>
    </item>
    <item>
      <title>Medical priority fusion: achieving dual optimization of sensitivity and interpretability in nipt anomaly detection</title>
      <link>https://arxiv.org/abs/2509.17924</link>
      <description>arXiv:2509.17924v1 Announce Type: cross 
Abstract: Clinical machine learning faces a critical dilemma in high-stakes medical applications: algorithms achieving optimal diagnostic performance typically sacrifice the interpretability essential for physician decision-making, while interpretable methods compromise sensitivity in complex scenarios. This paradox becomes particularly acute in non-invasive prenatal testing (NIPT), where missed chromosomal abnormalities carry profound clinical consequences yet regulatory frameworks mandate explainable AI systems. We introduce Medical Priority Fusion (MPF), a constrained multi-objective optimization framework that resolves this fundamental trade-off by systematically integrating Naive Bayes probabilistic reasoning with Decision Tree rule-based logic through mathematically-principled weighted fusion under explicit medical constraints. Rigorous validation on 1,687 real-world NIPT samples characterized by extreme class imbalance (43.4:1 normal-to-abnormal ratio) employed stratified 5-fold cross-validation with comprehensive ablation studies and statistical hypothesis testing using McNemar's paired comparisons. MPF achieved simultaneous optimization of dual objectives: 89.3% sensitivity (95% CI: 83.9-94.7%) with 80% interpretability score, significantly outperforming individual algorithms (McNemar's test, p &lt; 0.001). The optimal fusion configuration achieved Grade A clinical deployment criteria with large effect size (d = 1.24), establishing the first clinically-deployable solution that maintains both diagnostic accuracy and decision transparency essential for prenatal care. This work demonstrates that medical-constrained algorithm fusion can resolve the interpretability-performance trade-off, providing a mathematical framework for developing high-stakes medical decision support systems that meet both clinical efficacy and explainability requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17924v1</guid>
      <category>cs.LG</category>
      <category>q-bio.TO</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiuqi Ge, Zhibo Yao, Yaosong Du</dc:creator>
    </item>
    <item>
      <title>Towards deep-learning based detection and quantification of intestinal metaplasia on digitized gastric biopsies: a multi-expert comparative study</title>
      <link>https://arxiv.org/abs/2509.06991</link>
      <description>arXiv:2509.06991v2 Announce Type: replace 
Abstract: Current gastric cancer (GCa) risk systems are prone to errors since they evaluate a visual estimation of intestinal metaplasia percentages in histopathology images of gastric mucosa to assign a risk. This study presents an automated method to detect and quantify intestinal metaplasia using deep convolutional neural networks as well as a comparative analysis with visual estimations of three experienced pathologists. Gastric samples were collected from two different cohorts: 149 asymptomatic volunteers from a region with a high prevalence of GCa in Colombia and 56 patients from a tertiary hospital. Deep learning models were trained to classify intestinal metaplasia, and predictions were used to estimate the percentage of intestinal metaplasia and assign the OLGIM risk score. Results were compared with independent blinded metaplastic assessments performed by three experienced pathologists. The best-performing deep learning architecture classified intestinal metaplasia with F1-Score of 0.80 +- 0.01 and AUC of 0.91 +- 0.01. Among pathologists, inter-observer agreement by a Fleiss's Kappa score ranged from 0.20 to 0.48. In comparison, agreement between the pathologists and the best-performing model ranged from 0.12 to 0.35. Deep learning models show potential to reliably detect and quantify the percentage of intestinal metaplasia, achieving high classification performance. Visual estimation of intestinal metaplasia remains highly dependent on individual expertise, resulting in inter-observer variability. Deep learning models provide consistent estimates that could help reduce this subjectivity in risk stratification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06991v2</guid>
      <category>q-bio.TO</category>
      <category>cs.CE</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fabian Cano, Mauricio Caviedes, Andres Siabatto, Jesus Villarreal, Jose Quijano, \'Alvaro Bedoya-Urresta, Marino Coral Bedoya, Yomaira Yepez Caicedo, Angel Cruz-Roa, Fabio A. Gonz\'alez, Satish E. Viswanath, Eduardo Romero</dc:creator>
    </item>
    <item>
      <title>MAE-SAM2: Mask Autoencoder-Enhanced SAM2 for Clinical Retinal Vascular Leakage Segmentation</title>
      <link>https://arxiv.org/abs/2509.10554</link>
      <description>arXiv:2509.10554v2 Announce Type: replace 
Abstract: We propose MAE-SAM2, a novel foundation model for retinal vascular leakage segmentation on fluorescein angiography images. Due to the small size and dense distribution of the leakage areas, along with the limited availability of labeled clinical data, this presents a significant challenge for segmentation tasks. Our approach integrates a Self-Supervised learning (SSL) strategy, Masked Autoencoder (MAE), with SAM2. In our implementation, we explore different loss functions and conclude a task-specific combined loss. Extensive experiments and ablation studies demonstrate that MAE-SAM2 outperforms several state-of-the-art models, achieving the highest Dice score and Intersection-over-Union (IoU). Compared to the original SAM2, our model achieves a $5\%$ performance improvement, highlighting the promise of foundation models with self-supervised pretraining in clinical imaging tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10554v2</guid>
      <category>q-bio.TO</category>
      <category>eess.IV</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Xing, Irmak Karaca, Samira Badrloo, Quan Dong Nguyen, Mahadevan Subramaniam</dc:creator>
    </item>
  </channel>
</rss>
