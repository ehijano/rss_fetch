<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.NA</link>
    <description>math.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Jul 2024 01:33:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Hybrid Algorithm for Computing a Partial Singular Value Decomposition Satisfying a Given Threshold</title>
      <link>https://arxiv.org/abs/2407.06306</link>
      <description>arXiv:2407.06306v1 Announce Type: new 
Abstract: In this paper, we describe a new hybrid algorithm for computing all singular triplets above a given threshold and provide its implementation in MATLAB/Octave and R. The high performance of our codes and ease at which they can be used, either independently or within a larger numerical scheme, are illustrated through several numerical examples with applications to matrix completion and image compression. Well-documented MATLAB and R codes are provided for public use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06306v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Baglama, Jonathan A. Ch\'avez Casillas, Vasilije Perovi\'c</dc:creator>
    </item>
    <item>
      <title>Higher Order Multidimensional Slope Limiters with Local Maximum Principles</title>
      <link>https://arxiv.org/abs/2407.06437</link>
      <description>arXiv:2407.06437v1 Announce Type: new 
Abstract: Higher-order numerical methods are used to find accurate numerical solutions to hyperbolic partial differential equations and equations of transport type. Limiting is required to either converge to the correct type of solution or to adhere to physically motivated local maximum principles. Less restrictive limiting procedures are required so as to not severely decrease the accuracy.
  In this paper, we develop an existing slope limiter framework, to achieve different local boundedness principles for higher-order schemes on unstructured meshes. Quadrature points contributing to numerical fluxes can be limited based on face defined maximum principles, and the resulting cell mean at the next timestep can satisfy a cell mean maximum principle but with less limiting. We demonstrate the practical application of the introduced framework to a second-order finite volume scheme as well as a fourth-order finite volume scheme, in the context of the advection equation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06437v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.flu-dyn</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James Woodfield</dc:creator>
    </item>
    <item>
      <title>A Constrained Least-Squares Ghost Sample Points (CLS-GSP) Method for Differential Operators on Point Clouds</title>
      <link>https://arxiv.org/abs/2407.06467</link>
      <description>arXiv:2407.06467v1 Announce Type: new 
Abstract: We introduce a novel meshless method called the Constrained Least-Squares Ghost Sample Points (CLS-GSP) method for solving partial differential equations on irregular domains or manifolds represented by randomly generated sample points. Our approach involves two key innovations. Firstly, we locally reconstruct the underlying function using a linear combination of radial basis functions centered at a set of carefully chosen \textit{ghost sample points} that are independent of the point cloud samples. Secondly, unlike conventional least-squares methods, which minimize the sum of squared differences from all sample points, we regularize the local reconstruction by imposing a hard constraint to ensure that the least-squares approximation precisely passes through the center. This simple yet effective constraint significantly enhances the diagonal dominance and conditioning of the resulting differential matrix. We provide analytical proofs demonstrating that our method consistently estimates the exact Laplacian. Additionally, we present various numerical examples showcasing the effectiveness of our proposed approach in solving the Laplace/Poisson equation and related eigenvalue problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06467v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ningchen Ying, Kwunlun Chu, Shingyu Leung</dc:creator>
    </item>
    <item>
      <title>Estimates on the stability constant for the truncated Fourier transform</title>
      <link>https://arxiv.org/abs/2407.06656</link>
      <description>arXiv:2407.06656v1 Announce Type: new 
Abstract: In this paper we are interested in the inverse problem of recovering a compact supported function from its truncated Fourier transform. We derive new Lipschitz stability estimates for the inversion in terms of the truncation parameter. The obtained results show that the Lipschitz constant is of order one when the truncation parameter is larger than the spatial frequency of the function, and it grows exponentially when the truncation parameter tends to zero. Finally, we present some numerical examples of reconstruction of a compactly supported function from its noisy truncated Fourier transform. The numerical illustrations validate our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06656v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mirza Karamehmedovi\'c, Martin S{\ae}bye Car{\o}e, Faouzi Triki</dc:creator>
    </item>
    <item>
      <title>PDEformer-1: A Foundation Model for One-Dimensional Partial Differential Equations</title>
      <link>https://arxiv.org/abs/2407.06664</link>
      <description>arXiv:2407.06664v1 Announce Type: new 
Abstract: This paper introduces PDEformer-1, a versatile neural solver capable of simultaneously addressing various partial differential equations (PDEs). With the PDE represented as a computational graph, we facilitate the seamless integration of symbolic and numeric information inherent in a PDE. A graph Transformer and an implicit neural representation (INR) are employed subsequently to generate mesh-free predicted solutions. We generated a dataset with up to three million samples involving diverse one-dimensional PDEs to pretrain our model. Compared with baseline models trained specifically on benchmark datasets, our pretrained model achieves comparable accuracy via zero-shot inference, and the advantage expands after finetuning. For PDEs new or unseen in the pretraining stage, our model can adapt quickly by finetuning on a relatively small set of examples from the target equation. Additionally, PDEformer-1 demonstrates promising results in the inverse problem of PDE scalar coefficient recovery and coefficient field recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06664v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhanhong Ye, Xiang Huang, Leheng Chen, Zining Liu, Bingyang Wu, Hongsheng Liu, Zidong Wang, Bin Dong</dc:creator>
    </item>
    <item>
      <title>Almost-sure quasi-optimal approximation in reproducing kernel Hilbert spaces</title>
      <link>https://arxiv.org/abs/2407.06674</link>
      <description>arXiv:2407.06674v1 Announce Type: new 
Abstract: This manuscript addresses the problem of approximating an unknown function from point evaluations. When obtaining these point evaluations is costly, minimising the required sample size becomes crucial, and it is unreasonable to reserve a sufficiently large test sample for estimating the approximation accuracy. Therefore, an approximation with a certified quasi-optimality factor is required. This article shows that such an approximation can be obtained when the sought function lies in a \emph{reproducing kernel Hilbert space} (RKHS) and is to be approximated in a finite-dimensional linear subspace. However, selecting the sample points to minimise the quasi-optimality factor requires optimising over an infinite set of points and computing exact inner products in RKHS, which is often infeasible in practice. Extending results from optimal sampling for $L^2$ approximation, the present manuscript proves that random points, drawn independently from the Christoffel sampling distribution associated with $\mcal{V}_d$, can yield a controllable quasi-optimality factor with high probability. Inspired by this result, a novel sampling scheme, coined subspace-informed volume sampling, is introduced and evaluated in numerical experiments, where it outperforms classical i.i.d.\ Christoffel sampling and continuous volume sampling. To reduce the size of such a random sample, an additional greedy subsampling scheme with provable suboptimality bounds is introduced. Our presentation is of independent interest to the community researching the \emph{parametrised background data weak} (PBDW) method, as it offers a simpler interpretation of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06674v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Trunschke, Anthony Nouy</dc:creator>
    </item>
    <item>
      <title>Multilevel quadrature formulae for the optimal control of random PDEs</title>
      <link>https://arxiv.org/abs/2407.06678</link>
      <description>arXiv:2407.06678v1 Announce Type: new 
Abstract: This manuscript presents a framework for using multilevel quadrature formulae to compute the solution of optimal control problems constrained by random partial differential equations. Our approach consists in solving a sequence of optimal control problems discretized with different levels of accuracy of the physical and probability discretizations. The final approximation of the control is then obtained in a postprocessing step, by suitably combining the adjoint variables computed on the different levels. We present a convergence analysis for an unconstrained linear quadratic problem, and detail our framework for the specific case of a Multilevel Monte Carlo quadrature formula. Numerical experiments confirm the better computational complexity of our MLMC approach compared to a standard Monte Carlo sample average approximation, even beyond the theoretical assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06678v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabio Nobile, Tommaso Vanzan</dc:creator>
    </item>
    <item>
      <title>Preasymptotic error estimates of EEM and CIP-EEM for the time-harmonic Maxwell equations with large wave number</title>
      <link>https://arxiv.org/abs/2407.06784</link>
      <description>arXiv:2407.06784v1 Announce Type: new 
Abstract: Preasymptotic error estimates are derived for the linear edge element method (EEM) and the linear $\boldsymbol{H}(\boldsymbol{\mathrm{curl}})$-conforming interior penalty edge element method (CIP-EEM) for the time-harmonic Maxwell equations with large wave number. It is shown that under the mesh condition that $\kappa^3 h^2$ is sufficiently small, the errors of the solutions to both methods are bounded by $\mathcal{O} (\kappa h + \kappa^3 h^2 )$ in the energy norm and $\mathcal{O} (\kappa h^2 + \kappa^2 h^2 )$ in the $\boldsymbol{L}^2$ norm, where $\kappa$ is the wave number and $h$ is the mesh size. Numerical tests are provided to verify our theoretical results and to illustrate the potential of CIP-EEM in significantly reducing the pollution effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06784v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuaishuai Lu, Haijun Wu</dc:creator>
    </item>
    <item>
      <title>A Locking-free modified conforming FEM for planar elasticity</title>
      <link>https://arxiv.org/abs/2407.06831</link>
      <description>arXiv:2407.06831v1 Announce Type: new 
Abstract: Due to the divergence-instability, low-order conforming finite element methods (FEMs) for nearly incompressible elasticity equations suffer from the so-called locking phenomenon as the Lam\'e parameter $\lambda\to\infty$ and consequently the material becomes more and more incompressible. For the piecewise linear case, the error in the $L^2$-norm of the standard Galerkin conforming FEM is bounded by $C_\lambda h^2$. However, $C_\lambda \to \infty$ as $\lambda \to \infty$, resulting in poor accuracy for practical values of $h$ if $\lambda$ is sufficiently large. In this short paper, we show that for 2D problems the locking phenomenon can be controlled by replacing $\lambda$ with $\lambda^\alpha$ in the stiffness matrix, for a certain choice of $\alpha=\alpha_*(h,\lambda)$ in the range $0&lt;\alpha\le 1$. We prove that for this optimal choice of $\alpha$, the error in the $L^2$-norm is bounded by $Ch$ where $C$ does not depend on $\lambda$. Numerical experiments confirm the expected convergence behaviour and show that, for practical meshes, our locking-free method is more accurate than the standard method if the material is nearly incompressible. Our analysis also shows that the error in the $H^1$-norm is bounded by $Ch^{1/2}$, but our numerical experiments suggest that this bound is not sharp.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06831v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>K. Mustapha, W. McLean, J. Dick, Q. T. Le Gia</dc:creator>
    </item>
    <item>
      <title>Efficient nonlocal linear image denoising: Bilevel optimization with Nonequispaced Fast Fourier Transform and matrix-free preconditioning</title>
      <link>https://arxiv.org/abs/2407.06834</link>
      <description>arXiv:2407.06834v1 Announce Type: new 
Abstract: We present a new approach for nonlocal image denoising, based around the application of an unnormalized extended Gaussian ANOVA kernel within a bilevel optimization algorithm. A critical bottleneck when solving such problems for finely-resolved images is the solution of huge-scale, dense linear systems arising from the minimization of an energy term. We tackle this using a Krylov subspace approach, with a Nonequispaced Fast Fourier Transform utilized to approximate matrix-vector products in a matrix-free manner. We accelerate the algorithm using a novel change of basis approach to account for the (known) smallest eigenvalue-eigenvector pair of the matrices involved, coupled with a simple but frequently very effective diagonal preconditioning approach. We present a number of theoretical results concerning the eigenvalues and predicted convergence behavior, and a range of numerical experiments which validate our solvers and use them to tackle parameter learning problems. These demonstrate that very large problems may be effectively and rapidly denoised with very low storage requirements on a computer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06834v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andr\'es Miniguano-Trujillo, John W. Pearson, Benjamin D. Goddard</dc:creator>
    </item>
    <item>
      <title>Convergence of the Semi-Discrete WaveHoltz Iteration</title>
      <link>https://arxiv.org/abs/2407.06929</link>
      <description>arXiv:2407.06929v1 Announce Type: new 
Abstract: In this paper we prove that for stable semi-discretizations of the wave equation for the WaveHoltz iteration is guaranteed to converge to an approximate solution of the corresponding frequency domain problem, if it exists. We show that for certain classes of frequency domain problems, the WaveHoltz iteration without acceleration converges in $O({\omega})$ iterations with the constant factor depending logarithmically on the desired tolerance. We conjecture that the Helmholtz problem in open domains with no trapping waves is one such class of problems and we provide numerical examples in one and two dimensions using finite differences and discontinuous Galerkin discretizations which demonstrate these converge results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06929v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amit Rotem, Olof Runborg, Daniel Appel\"o</dc:creator>
    </item>
    <item>
      <title>Approximation of rogue waves using Malmquist-Takenaka functions</title>
      <link>https://arxiv.org/abs/2407.04013</link>
      <description>arXiv:2407.04013v1 Announce Type: cross 
Abstract: Rogue waves are fascinating large amplitude coherent structures that abruptly appear and then disappear soon after. In certain partial differential equations these waves are modeled by rational solutions. In this work we discuss approximating rogue wave solutions in a basis of orthogonal functions known as the Malmquist-Takenaka (MT) functions. This family of rational functions can be directly mapped to a modified Fourier series, allowing the fast Fourier transform computation of the spectral MT coefficients. Spectral differentiation matrices are derived. The approximation of the various rogue wave solutions in the nonlinear Schrodinger (NLS) equation is explored. The unstable nature of the NLS equation on a constant background and its effect on destabilizing and generating rogue waves is studied. Perturbing the constant solution with certain localized functions is found to generate rogue wave-like structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04013v1</guid>
      <category>nlin.PS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Justin T. Cole, Troy I. Johnson</dc:creator>
    </item>
    <item>
      <title>Limits and Powers of Koopman Learning</title>
      <link>https://arxiv.org/abs/2407.06312</link>
      <description>arXiv:2407.06312v1 Announce Type: cross 
Abstract: Dynamical systems provide a comprehensive way to study complex and changing behaviors across various sciences. Many modern systems are too complicated to analyze directly or we do not have access to models, driving significant interest in learning methods. Koopman operators have emerged as a dominant approach because they allow the study of nonlinear dynamics using linear techniques by solving an infinite-dimensional spectral problem. However, current algorithms face challenges such as lack of convergence, hindering practical progress. This paper addresses a fundamental open question: \textit{When can we robustly learn the spectral properties of Koopman operators from trajectory data of dynamical systems, and when can we not?} Understanding these boundaries is crucial for analysis, applications, and designing algorithms. We establish a foundational approach that combines computational analysis and ergodic theory, revealing the first fundamental barriers -- universal for any algorithm -- associated with system geometry and complexity, regardless of data quality and quantity. For instance, we demonstrate well-behaved smooth dynamical systems on tori where non-trivial eigenfunctions of the Koopman operator cannot be determined by any sequence of (even randomized) algorithms, even with unlimited training data. Additionally, we identify when learning is possible and introduce optimal algorithms with verification that overcome issues in standard methods. These results pave the way for a sharp classification theory of data-driven dynamical systems based on how many limits are needed to solve a problem. These limits characterize all previous methods, presenting a unified view. Our framework systematically determines when and how Koopman spectral properties can be learned.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06312v1</guid>
      <category>math.DS</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>math.SP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew J. Colbrook, Igor Mezi\'c, Alexei Stepanenko</dc:creator>
    </item>
    <item>
      <title>A third-order finite difference weighted essentially non-oscillatory scheme with shallow neural network</title>
      <link>https://arxiv.org/abs/2407.06333</link>
      <description>arXiv:2407.06333v2 Announce Type: cross 
Abstract: In this paper, we introduce the finite difference weighted essentially non-oscillatory (WENO) scheme based on the neural network for hyperbolic conservation laws. We employ the supervised learning and design two loss functions, one with the mean squared error and the other with the mean squared logarithmic error, where the WENO3-JS weights are computed as the labels. Each loss function consists of two components where the first component compares the difference between the weights from the neural network and WENO3-JS weights, while the second component matches the output weights of the neural network and the linear weights. The former of the loss function enforces the neural network to follow the WENO properties, implying that there is no need for the post-processing layer. Additionally the latter leads to better performance around discontinuities. As a neural network structure, we choose the shallow neural network (SNN) for computational efficiency with the Delta layer consisting of the normalized undivided differences. These constructed WENO3-SNN schemes show the outperformed results in one-dimensional examples and improved behavior in two-dimensional examples, compared with the simulations from WENO3-JS and WENO3-Z.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06333v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>cs.NE</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kwanghyuk Park, Xinjuan Chen, Dongjin Lee, Jiaxi Gu, Jae-Hun Jung</dc:creator>
    </item>
    <item>
      <title>System stabilization with policy optimization on unstable latent manifolds</title>
      <link>https://arxiv.org/abs/2407.06418</link>
      <description>arXiv:2407.06418v1 Announce Type: cross 
Abstract: Stability is a basic requirement when studying the behavior of dynamical systems. However, stabilizing dynamical systems via reinforcement learning is challenging because only little data can be collected over short time horizons before instabilities are triggered and data become meaningless. This work introduces a reinforcement learning approach that is formulated over latent manifolds of unstable dynamics so that stabilizing policies can be trained from few data samples. The unstable manifolds are minimal in the sense that they contain the lowest dimensional dynamics that are necessary for learning policies that guarantee stabilization. This is in stark contrast to generic latent manifolds that aim to approximate all -- stable and unstable -- system dynamics and thus are higher dimensional and often require higher amounts of data. Experiments demonstrate that the proposed approach stabilizes even complex physical systems from few data samples for which other methods that operate either directly in the system state space or on generic latent manifolds fail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06418v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Steffen W. R. Werner, Benjamin Peherstorfer</dc:creator>
    </item>
    <item>
      <title>Convergence rates for Poisson learning to a Poisson equation with measure data</title>
      <link>https://arxiv.org/abs/2407.06783</link>
      <description>arXiv:2407.06783v1 Announce Type: cross 
Abstract: In this paper we prove discrete to continuum convergence rates for Poisson Learning, a graph-based semi-supervised learning algorithm that is based on solving the graph Poisson equation with a source term consisting of a linear combination of Dirac deltas located at labeled points and carrying label information. The corresponding continuum equation is a Poisson equation with measure data in a Euclidean domain $\Omega \subset \mathbb{R}^d$. The singular nature of these equations is challenging and requires an approach with several distinct parts: (1) We prove quantitative error estimates when convolving the measure data of a Poisson equation with (approximately) radial function supported on balls. (2) We use quantitative variational techniques to prove discrete to continuum convergence rates on random geometric graphs with bandwidth $\varepsilon&gt;0$ for bounded source terms. (3) We show how to regularize the graph Poisson equation via mollification with the graph heat kernel, and we study fine asymptotics of the heat kernel on random geometric graphs. Combining these three pillars we obtain $L^1$ convergence rates that scale, up to logarithmic factors, like $O(\varepsilon^{\frac{1}{d+2}})$ for general data distributions, and $O(\varepsilon^{\frac{2-\sigma}{d+4}})$ for uniformly distributed data, where $\sigma&gt;0$. These rates are valid with high probability if $\varepsilon\gg\left({\log n}/{n}\right)^q$ where $n$ denotes the number of vertices of the graph and $q \approx \frac{1}{3d}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06783v1</guid>
      <category>math.AP</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leon Bungert, Jeff Calder, Max Mihailescu, Kodjo Houssou, Amber Yuan</dc:creator>
    </item>
    <item>
      <title>A scalable multidimensional fully implicit solver for Hall magnetohydrodynamics</title>
      <link>https://arxiv.org/abs/2407.07031</link>
      <description>arXiv:2407.07031v1 Announce Type: cross 
Abstract: We propose an optimally performant fully implicit algorithm for the Hall magnetohydrodynamics (HMHD) equations based on multigrid-preconditioned Jacobian-free Newton-Krylov methods. HMHD is a challenging system to solve numerically because it supports stiff fast dispersive waves. The preconditioner is formulated using an operator-split approximate block factorization (Schur complement), informed by physics insight. We use a vector-potential formulation (instead of a magnetic field one) to allow a clean segregation of the problematic $\nabla \times \nabla \times$ operator in the electron Ohm's law subsystem. This segregation allows the formulation of an effective damped block-Jacobi smoother for multigrid. We demonstrate by analysis that our proposed block-Jacobi iteration is convergent and has the smoothing property. The resulting HMHD solver is verified linearly with wave propagation examples, and nonlinearly with the GEM challenge reconnection problem by comparison against another HMHD code. We demonstrate the excellent algorithmic and parallel performance of the algorithm up to 16384 MPI tasks in two dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07031v1</guid>
      <category>physics.plasm-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Luis Chacon</dc:creator>
    </item>
    <item>
      <title>Laplace-based strategies for Bayesian optimal experimental design with nuisance uncertainty</title>
      <link>https://arxiv.org/abs/2310.10783</link>
      <description>arXiv:2310.10783v2 Announce Type: replace 
Abstract: Finding the optimal design of experiments in the Bayesian setting typically requires estimation and optimization of the expected information gain functional. This functional consists of one outer and one inner integral, separated by the logarithm function applied to the inner integral. When the mathematical model of the experiment contains uncertainty about the parameters of interest and nuisance uncertainty, (i.e., uncertainty about parameters that affect the model but are not themselves of interest to the experimenter), two inner integrals must be estimated. Thus, the already considerable computational effort required to determine good approximations of the expected information gain is increased further. The Laplace approximation has been applied successfully in the context of experimental design in various ways, and we propose two novel estimators featuring the Laplace approximation to alleviate the computational burden of both inner integrals considerably. The first estimator applies Laplace's method followed by a Laplace approximation, introducing a bias. The second estimator uses two Laplace approximations as importance sampling measures for Monte Carlo approximations of the inner integrals. Both estimators use Monte Carlo approximation for the remaining outer integral estimation. We provide four numerical examples demonstrating the applicability and effectiveness of our proposed estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.10783v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arved Bartuska, Luis Espath, Ra\'ul Tempone</dc:creator>
    </item>
    <item>
      <title>$hp$-optimal convergence of the original DG method for linear hyperbolic problems on special simplicial meshes</title>
      <link>https://arxiv.org/abs/2310.13564</link>
      <description>arXiv:2310.13564v4 Announce Type: replace 
Abstract: We prove hp-optimal error estimates for the original DG method when approximating solutions to first-order hyperbolic problems with constant convection fields in the L2 and DG norms. The main theoretical tools used in the analysis are novel hp-optimal approximation properties of the special projector introduced in [Cockburn, Dong, Guzman, SINUM, 2008]. We assess the theoretical findings on some test cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13564v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhaonan Dong, Lorenzo Mascotto</dc:creator>
    </item>
    <item>
      <title>A fast and efficient numerical method for computing the stress concentration between closely located stiff inclusions of general shapes</title>
      <link>https://arxiv.org/abs/2312.00630</link>
      <description>arXiv:2312.00630v2 Announce Type: replace 
Abstract: When two stiff inclusions are closely located, the gradient of the solution to the Lam\'{e} system, in other words the stress, may become arbitrarily large as the distance between two inclusions tends to zero. To compute the gradient of the solution in the narrow region, extremely fine meshes are required. It is a challenging problem to numerically compute the stress near the narrow region between two inclusions of general shapes as their distance goes to zero. A recent study [15] has shown that the major singularity of the gradient can be extracted in an explicit way for two general shaped inclusions. Thus the complexity of the computation can be greatly reduced by removing the singular term and it suffices to compute the residual term only using regular meshes. The goal of this paper is to numerically compute the stress concentration in a fast and efficient way. In this paper, we compute the value of the stress concentration factor, which is the normalized magnitude of the stress concentration, for general shaped domain as the distance between two inclusions tends to zero. We also compute the solution for two closely located inclusions of general shapes and show the convergence of the solution. Only regular meshes are used in our numerical computation and the results clearly show that the characterization of the singular term method can be efficiently used for computation of the stress concentration between two closely located inclusions of general shapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.00630v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaofei Li, Shengqi Lin, Haojie Wang</dc:creator>
    </item>
    <item>
      <title>Singular Layer Physics-Informed Neural Network Method for Convection-Dominated Boundary Layer Problems in Two Dimensions</title>
      <link>https://arxiv.org/abs/2312.03295</link>
      <description>arXiv:2312.03295v2 Announce Type: replace 
Abstract: This research explores neural network-based numerical approximation of two-dimensional convection-dominated singularly perturbed problems on square, circular, and elliptic domains. Singularly perturbed boundary value problems pose significant challenges due to sharp boundary layers in their solutions. Additionally, the characteristic points of these domains give rise to degenerate boundary layer problems. The stiffness of these problems, caused by sharp singular layers, can lead to substantial computational errors if not properly addressed. Conventional neural network-based approaches often fail to capture these sharp transitions accurately, highlighting a critical flaw in machine learning methods. To address these issues, we conduct a thorough boundary layer analysis to enhance our understanding of sharp transitions within the boundary layers, guiding the application of numerical methods. Specifically, we employ physics-informed neural networks (PINNs) to better handle these boundary layer problems. However, PINNs may struggle with rapidly varying singularly perturbed solutions in small domain regions, leading to inaccurate or unstable results. To overcome this limitation, we introduce a semi-analytic method that augments PINNs with singular layers or corrector functions. Our numerical experiments demonstrate significant improvements in both accuracy and stability, showcasing the effectiveness of our proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.03295v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gung-Min Gie, Youngjoon Hong, Chang-Yeol Jung, Dongseok Lee</dc:creator>
    </item>
    <item>
      <title>Efficient simulation of mixed boundary value problems and conformal mappings</title>
      <link>https://arxiv.org/abs/2312.15382</link>
      <description>arXiv:2312.15382v2 Announce Type: replace 
Abstract: In this paper, we present a stochastic method for the simulation of Laplace's equation with a mixed boundary condition in planar domains that are polygonal or bounded by circular arcs. We call this method the Reflected Walk-on-Spheres algorithm. The method combines a traditional Walk-on-Spheres algorithm with use of reflections at the Neumann boundaries. We apply our algorithm to simulate numerical conformal mappings from certain quadrilaterals to the corresponding canonical domains, and to compute their conformal moduli. Finally, we give examples of the method on three dimensional polyhedral domains, and use it to simulate the heat flow on an L-shaped insulated polyhedron.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15382v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.CV</category>
      <category>math.PR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiansheng Han, Antti Rasila, Tommi Sottinen</dc:creator>
    </item>
    <item>
      <title>Mixed-Precision Paterson--Stockmeyer Method for Evaluating Polynomials of Matrices</title>
      <link>https://arxiv.org/abs/2312.17396</link>
      <description>arXiv:2312.17396v2 Announce Type: replace 
Abstract: The Paterson--Stockmeyer method is an evaluation scheme for matrix polynomials with scalar coefficients that arise in many state-of-the-art algorithms based on polynomial or rational approximation, for example, those for computing transcendental matrix functions. We derive a mixed-precision version of the Paterson--Stockmeyer method that is particularly useful for evaluating matrix polynomials with scalar coefficients of decaying magnitude. The new method is mainly of interest in the arbitrary precision arithmetic, and it is particularly attractive for high-precision computations. The key idea is to perform computations on data of small magnitude in low precision, and rounding error analysis is provided for the use of lower-than-working precisions. We focus on the evaluation of the Taylor approximants of the matrix exponential and show the applicability of our method to the existing scaling and squaring algorithms. We also demonstrate through experiments the general applicability of our method to other problems, such as computing the polynomials from the Pad\'e approximant of the matrix exponential and the Taylor approximant of the matrix cosine. Numerical experiments show our mixed-precision Paterson--Stockmeyer algorithms can be more efficient than its fixed-precision counterpart while delivering the same level of accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17396v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaobo Liu</dc:creator>
    </item>
    <item>
      <title>Two-level overlapping Schwarz preconditioners with universal coarse spaces for $2m$th-order elliptic problems</title>
      <link>https://arxiv.org/abs/2403.18970</link>
      <description>arXiv:2403.18970v2 Announce Type: replace 
Abstract: We propose a novel universal construction of two-level overlapping Schwarz preconditioners for $2m$th-order elliptic boundary value problems, where $m$ is a positive integer. The word "universal" here signifies that the coarse space construction can be applied to any finite element discretization for any $m$ that satisfies some common assumptions. We present numerical results for conforming, nonconforming, and discontinuous Galerkin-type finite element discretizations for high-order problems to demonstrate the scalability of the proposed two-level overlapping Schwarz preconditioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18970v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jongho Park</dc:creator>
    </item>
    <item>
      <title>Singular Value and Frame Decomposition-based Reconstruction for Atmospheric Tomography</title>
      <link>https://arxiv.org/abs/2405.01079</link>
      <description>arXiv:2405.01079v2 Announce Type: replace 
Abstract: Atmospheric tomography, the problem of reconstructing atmospheric turbulence profiles from wavefront sensor measurements, is an integral part of many adaptive optics systems used for enhancing the image quality of ground-based telescopes. Singular-value and frame decompositions of the underlying atmospheric tomography operator can reveal useful analytical information on this inverse problem, as well as serve as the basis of efficient numerical reconstruction algorithms. In this paper, we extend existing singular value decompositions to more realistic Sobolev settings including weighted inner products, and derive an explicit representation of a frame-based (approximate) solution operator. These investigations form the basis of efficient numerical solution methods, which we analyze via numerical simulations for the challenging, real-world Adaptive Optics system of the Extremely Large Telescope using the entirely MATLAB-based simulation tool MOST.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01079v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Weissinger, Simon Hubmer, Bernadett Stadler, Ronny Ramlau</dc:creator>
    </item>
    <item>
      <title>SOC-MartNet: A Martingale Neural Network for the Hamilton-Jacobi-Bellman Equation without Explicit inf H in Stochastic Optimal Controls</title>
      <link>https://arxiv.org/abs/2405.03169</link>
      <description>arXiv:2405.03169v2 Announce Type: replace 
Abstract: In this work, we propose a martingale based neural network, SOC-MartNet, for solving high-dimensional Hamilton-Jacobi-Bellman (HJB) equations where no explicit expression is needed for the Hamiltonian $\inf_{u \in U} H(t,x,u, z,p)$, and stochastic optimal control problems with controls on both drift and volatility.
  We reformulate the HJB equations for the value function into a stochastic neural network learning process,
  i.e., training a control network and a value network such that the associated Hamiltonian process is minimized and the cost process becomes a martingale, yielding the value function.
  To enforce the martingale property for the cost process, we employ an adversarial network and construct a loss function based on the projection property of conditional expectations.
  Then, the control/value networks and the adversarial network are trained adversarially, such that the cost process is driven towards a martingale and the minimum principle is satisfied for the control.
  Numerical results show that the proposed SOC-MartNet is effective and efficient for solving HJB-type equations and SOCP with a dimension up to $1000$ in a small number (less than 150) of training epochs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03169v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Cai, Shuixin Fang, Tao Zhou</dc:creator>
    </item>
    <item>
      <title>A Guide to Stochastic Optimisation for Large-Scale Inverse Problems</title>
      <link>https://arxiv.org/abs/2406.06342</link>
      <description>arXiv:2406.06342v2 Announce Type: replace 
Abstract: Stochastic optimisation algorithms are the de facto standard for machine learning with large amounts of data. Handling only a subset of available data in each optimisation step dramatically reduces the per-iteration computational costs, while still ensuring significant progress towards the solution. Driven by the need to solve large-scale optimisation problems as efficiently as possible, the last decade has witnessed an explosion of research in this area. Leveraging the parallels between machine learning and inverse problems has allowed harnessing the power of this research wave for solving inverse problems. In this survey, we provide a comprehensive account of the state-of-the-art in stochastic optimisation from the viewpoint of inverse problems. We present algorithms with diverse modalities of problem randomisation and discuss the roles of variance reduction, acceleration, higher-order methods, and other algorithmic modifications, and compare theoretical results with practical behaviour. We focus on the potential and the challenges for stochastic optimisation that are unique to inverse imaging problems and are not commonly encountered in machine learning. We conclude the survey with illustrative examples from imaging problems to examine the advantages and disadvantages that this new generation of algorithms bring to the field of inverse problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06342v2</guid>
      <category>math.NA</category>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias J. Ehrhardt, Zeljko Kereta, Jingwei Liang, Junqi Tang</dc:creator>
    </item>
    <item>
      <title>ROM inversion of monostatic data lifted to full MIMO</title>
      <link>https://arxiv.org/abs/2407.00822</link>
      <description>arXiv:2407.00822v3 Announce Type: replace 
Abstract: The Lippmann--Schwinger--Lanczos (LSL) algorithm has recently been shown to provide an efficient tool for imaging and direct inversion of synthetic aperture radar data in multi-scattering environments [17], where the data set is limited to the monostatic, a.k.a. single input/single output (SISO) measurements. The approach is based on constructing data-driven estimates of internal fields via a reduced-order model (ROM) framework and then plugging them into the Lippmann-Schwinger integral equation. However, the approximations of the internal solutions may have more error due to missing the off diagonal elements of the multiple input/multiple output (MIMO) matrix valued transfer function. This, in turn, may result in multiple echoes in the image. Here we present a ROM-based data completion algorithm to mitigate this problem. First, we apply the LSL algorithm to the SISO data as in [17] to obtain approximate reconstructions as well as the estimate of internal field. Next, we use these estimates to calculate a forward Lippmann-Schwinger integral to populate the missing off-diagonal data (the lifting step). Finally, to update the reconstructions, we solve the Lippmann-Schwinger equation using the original SISO data, where the internal fields are constructed from the lifted MIMO data. The steps of obtaining the approximate reconstructions and internal fields and populating the missing MIMO data entries can be repeated for complex models to improve the images even further. Efficiency of the proposed approach is demonstrated on 2D and 2.5D numerical examples, where we see reconstructions are improved substantially.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00822v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>V. Druskin, S. Moskow, M. Zaslavsky</dc:creator>
    </item>
    <item>
      <title>On posterior consistency of data assimilation with Gaussian process priors: the 2D Navier-Stokes equations</title>
      <link>https://arxiv.org/abs/2307.08136</link>
      <description>arXiv:2307.08136v3 Announce Type: replace-cross 
Abstract: We consider a non-linear Bayesian data assimilation model for the periodic two-dimensional Navier-Stokes equations with initial condition modelled by a Gaussian process prior. We show that if the system is updated with sufficiently many discrete noisy measurements of the velocity field, then the posterior distribution eventually concentrates near the ground truth solution of the time evolution equation, and in particular that the initial condition is recovered consistently by the posterior mean vector field. We further show that the convergence rate can in general not be faster than inverse logarithmic in sample size, but describe specific conditions on the initial conditions when faster rates are possible. In the proofs we provide an explicit quantitative estimate for backward uniqueness of solutions of the two-dimensional Navier-Stokes equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.08136v3</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Richard Nickl, Edriss S. Titi</dc:creator>
    </item>
    <item>
      <title>A hybrid tau-leap for simulating chemical kinetics with applications to parameter estimation</title>
      <link>https://arxiv.org/abs/2401.09097</link>
      <description>arXiv:2401.09097v3 Announce Type: replace-cross 
Abstract: We consider the problem of efficiently simulating stochastic models of chemical kinetics. The Gillespie Stochastic Simulation algorithm (SSA) is often used to simulate these models, however, in many scenarios of interest, the computational cost quickly becomes prohibitive. This is further exasperated in the Bayesian inference context when estimating parameters of chemical models, as the intractability of the likelihood requires multiple simulations of the underlying system. To deal with issues of computational complexity in this paper, we propose a novel hybrid $\tau$-leap algorithm for simulating well-mixed chemical systems. In particular, the algorithm uses $\tau$-leap when appropriate (high population densities), and SSA when necessary (low population densities, when discrete effects become non-negligible). In the intermediate regime, a combination of the two methods, which leverages the properties of the underlying Poisson formulation, is employed. As illustrated through a number of numerical experiments the hybrid $\tau$ offers significant computational savings when compared to SSA without however sacrificing the overall accuracy. This feature is particularly welcomed in the Bayesian inference context, as it allows for parameter estimation of stochastic chemical kinetics at reduced computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09097v3</guid>
      <category>q-bio.MN</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Trigo Trindade, Konstantinos C. Zygalakis</dc:creator>
    </item>
    <item>
      <title>Robustness and Exploration of Variational and Machine Learning Approaches to Inverse Problems: An Overview</title>
      <link>https://arxiv.org/abs/2402.12072</link>
      <description>arXiv:2402.12072v2 Announce Type: replace-cross 
Abstract: This paper provides an overview of current approaches for solving inverse problems in imaging using variational methods and machine learning. A special focus lies on point estimators and their robustness against adversarial perturbations. In this context results of numerical experiments for a one-dimensional toy problem are provided, showing the robustness of different approaches and empirically verifying theoretical guarantees. Another focus of this review is the exploration of the subspace of data-consistent solutions through explicit guidance to satisfy specific semantic or textural properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12072v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Auras, Kanchana Vaishnavi Gandikota, Hannah Droege, Michael Moeller</dc:creator>
    </item>
  </channel>
</rss>
