<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 23 Jan 2025 05:01:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Predicting high dengue incidence in municipalities of Brazil using path signatures</title>
      <link>https://arxiv.org/abs/2501.12395</link>
      <description>arXiv:2501.12395v1 Announce Type: new 
Abstract: Predicting whether to expect a high incidence of infectious diseases is critical for health surveillance. In the epidemiology of dengue, environmental conditions can significantly impact the transmission of the virus. Utilizing epidemiological indicators alongside environmental variables can enhance predictions of dengue incidence risk. This study analyzed a dataset of weekly case numbers, temperature, and humidity across Brazilian municipalities to forecast the risk of high dengue incidence using data from 2014 to 2023. The framework involved constructing path signatures and applying lasso regression for binary outcomes. Sensitivity reached 75%, while specificity was extremely high, ranging from 75% to 100%. The best performance was observed with information gathered after 35 weeks of observations using data augmentation via embedding techniques. The use of path signatures effectively captures the stream of information given by epidemiological and climate variables that influence dengue transmission. This framework could be instrumental in optimizing resources to predict high dengue risk in municipalities in Brazil and other countries after learning these country patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12395v1</guid>
      <category>q-bio.QM</category>
      <category>q-bio.PE</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel A. M. Villela</dc:creator>
    </item>
    <item>
      <title>Interpolation pour l'augmentation de donnees : Application \`a la gestion des adventices de la canne a sucre a la Reunion</title>
      <link>https://arxiv.org/abs/2501.12400</link>
      <description>arXiv:2501.12400v1 Announce Type: new 
Abstract: Data augmentation is a crucial step in the development of robust supervised learning models, especially when dealing with limited datasets. This study explores interpolation techniques for the augmentation of geo-referenced data, with the aim of predicting the presence of Commelina benghalensis L. in sugarcane plots in La R\'eunion. Given the spatial nature of the data and the high cost of data collection, we evaluated two interpolation approaches: Gaussian processes (GPs) with different kernels and kriging with various variograms. The objectives of this work are threefold: (i) to identify which interpolation methods offer the best predictive performance for various regression algorithms, (ii) to analyze the evolution of performance as a function of the number of observations added, and (iii) to assess the spatial consistency of augmented datasets. The results show that GP-based methods, in particular with combined kernels (GP-COMB), significantly improve the performance of regression algorithms while requiring less additional data. Although kriging shows slightly lower performance, it is distinguished by a more homogeneous spatial coverage, a potential advantage in certain contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12400v1</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Frederick Fabre Ferber, Dominique Gay, Jean-Christophe Soulie, Jean Diatta, Odalric-Ambrym Maillard</dc:creator>
    </item>
    <item>
      <title>A Bayesian Modelling Framework with Model Comparison for Epidemics with Super-Spreading</title>
      <link>https://arxiv.org/abs/2501.12768</link>
      <description>arXiv:2501.12768v1 Announce Type: new 
Abstract: The transmission dynamics of an epidemic are rarely homogeneous. Super-spreading events and super-spreading individuals are two types of heterogeneous transmissibility. Inference of super-spreading is commonly carried out on secondary case data, the expected distribution of which is known as the offspring distribution. However, this data is seldom available. Here we introduce a multi-model framework fit to incidence time-series, data that is much more readily available. The framework consists of five discrete-time, stochastic, branching-process models of epidemics spread through a susceptible population. The framework includes a baseline model of homogeneous transmission, a unimodal and a bimodal model for super-spreading events, as well as a unimodal and a bimodal model for super-spreading individuals. Bayesian statistics is used to infer model parameters using Markov Chain Monte-Carlo. Model comparison is conducted by computing Bayes factors, with importance sampling used to estimate the marginal likelihood of each model. This estimator is selected for its consistency and lower variance compared to alternatives. Application to simulated data from each model identifies the correct model for the majority of simulations and accurately infers the true parameters, such as the basic reproduction number. We also apply our methods to incidence data from the 2003 SARS outbreak and the Covid-19 pandemic. Model selection consistently identifies the same model and mechanism for a given disease, even when using different time series. Our estimates are consistent with previous studies based on secondary case data. Quantifying the contribution of super-spreading to disease transmission has important implications for infectious disease management and control. Our modelling framework is disease-agnostic and implemented as an R package, with potential to be a valuable tool for public health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12768v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hannah Craddock, Simon EF Spencer, Xavier Didelot</dc:creator>
    </item>
    <item>
      <title>Comparative Analysis of Hand-Crafted and Machine-Driven Histopathological Features for Prostate Cancer Classification and Segmentation</title>
      <link>https://arxiv.org/abs/2501.12415</link>
      <description>arXiv:2501.12415v1 Announce Type: cross 
Abstract: Histopathological image analysis is a reliable method for prostate cancer identification. In this paper, we present a comparative analysis of two approaches for segmenting glandular structures in prostate images to automate Gleason grading. The first approach utilizes a hand-crafted learning technique, combining Gray Level Co-Occurrence Matrix (GLCM) and Local Binary Pattern (LBP) texture descriptors to highlight spatial dependencies and minimize information loss at the pixel level. For machine driven feature extraction, we employ a U-Net convolutional neural network to perform semantic segmentation of prostate gland stroma tissue. Support vector machine-based learning of hand-crafted features achieves impressive classification accuracies of 99.0% and 95.1% for GLCM and LBP, respectively, while the U-Net-based machine-driven features attain 94% accuracy. Furthermore, a comparative analysis demonstrates superior segmentation quality for histopathological grades 1, 2, 3, and 4 using the U-Net approach, as assessed by Jaccard and Dice metrics. This work underscores the utility of machine-driven features in clinical applications that rely on automated pixel-level segmentation in prostate tissue images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12415v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.18178/joig.12.4.437-449</arxiv:DOI>
      <arxiv:journal_reference>Journal of Image and Graphics, Vol. 12, No. 4, pp. 437-449, 2024</arxiv:journal_reference>
      <dc:creator>Feda Bolus Al Baqain, Omar Sultan Al-Kadi</dc:creator>
    </item>
    <item>
      <title>Tackling Small Sample Survival Analysis via Transfer Learning: A Study of Colorectal Cancer Prognosis</title>
      <link>https://arxiv.org/abs/2501.12421</link>
      <description>arXiv:2501.12421v1 Announce Type: cross 
Abstract: Survival prognosis is crucial for medical informatics. Practitioners often confront small-sized clinical data, especially cancer patient cases, which can be insufficient to induce useful patterns for survival predictions. This study deals with small sample survival analysis by leveraging transfer learning, a useful machine learning technique that can enhance the target analysis with related knowledge pre-learned from other data. We propose and develop various transfer learning methods designed for common survival models. For parametric models such as DeepSurv, Cox-CC (Cox-based neural networks), and DeepHit (end-to-end deep learning model), we apply standard transfer learning techniques like pretraining and fine-tuning. For non-parametric models such as Random Survival Forest, we propose a new transfer survival forest (TSF) model that transfers tree structures from source tasks and fine-tunes them with target data. We evaluated the transfer learning methods on colorectal cancer (CRC) prognosis. The source data are 27,379 SEER CRC stage I patients, and the target data are 728 CRC stage I patients from the West China Hospital. When enhanced by transfer learning, Cox-CC's $C^{td}$ value was boosted from 0.7868 to 0.8111, DeepHit's from 0.8085 to 0.8135, DeepSurv's from 0.7722 to 0.8043, and RSF's from 0.7940 to 0.8297 (the highest performance). All models trained with data as small as 50 demonstrated even more significant improvement. Conclusions: Therefore, the current survival models used for cancer prognosis can be enhanced and improved by properly designed transfer learning techniques. The source code used in this study is available at https://github.com/YonghaoZhao722/TSF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12421v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yonghao Zhao, Changtao Li, Chi Shu, Qingbin Wu, Hong Li, Chuan Xu, Tianrui Li, Ziqiang Wang, Zhipeng Luo, Yazhou He</dc:creator>
    </item>
    <item>
      <title>Multi-stage intermediate fusion for multimodal learning to classify non-small cell lung cancer subtypes from CT and PET</title>
      <link>https://arxiv.org/abs/2501.12425</link>
      <description>arXiv:2501.12425v1 Announce Type: cross 
Abstract: Accurate classification of histological subtypes of non-small cell lung cancer (NSCLC) is essential in the era of precision medicine, yet current invasive techniques are not always feasible and may lead to clinical complications. This study presents a multi-stage intermediate fusion approach to classify NSCLC subtypes from CT and PET images. Our method integrates the two modalities at different stages of feature extraction, using voxel-wise fusion to exploit complementary information across varying abstraction levels while preserving spatial correlations. We compare our method against unimodal approaches using only CT or PET images to demonstrate the benefits of modality fusion, and further benchmark it against early and late fusion techniques to highlight the advantages of intermediate fusion during feature extraction. Additionally, we compare our model with the only existing intermediate fusion method for histological subtype classification using PET/CT images. Our results demonstrate that the proposed method outperforms all alternatives across key metrics, with an accuracy and AUC equal to 0.724 and 0.681, respectively. This non-invasive approach has the potential to significantly improve diagnostic accuracy, facilitate more informed treatment decisions, and advance personalized care in lung cancer management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12425v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fatih Aksu, Fabrizia Gelardi, Arturo Chiti, Paolo Soda</dc:creator>
    </item>
    <item>
      <title>Comorbid anxiety predicts lower odds of depression improvement during smartphone-delivered psychotherapy</title>
      <link>https://arxiv.org/abs/2409.11183</link>
      <description>arXiv:2409.11183v2 Announce Type: replace 
Abstract: Comorbid anxiety disorders are common among patients with major depressive disorder (MDD), and numerous studies have identified an association between comorbid anxiety and resistance to pharmacological depression treatment. However, the impact of anxiety on the effectiveness of non-pharmacological interventions for MDD is not as well understood. In this study, we applied machine learning techniques to predict treatment responses in a large-scale clinical trial (n=493) of individuals with MDD, who were recruited online and randomly assigned to one of three smartphone-based interventions. Our analysis reveals that a baseline GAD-7 questionnaire score in the moderate to severe range (&gt;10) predicts reduced probability of recovery from MDD. Our findings suggest that depressed individuals with comorbid anxiety face lower odds of substantial improvement in the context of smartphone-based therapeutic interventions for depression. Our work highlights a methodology that can identify simple, clinically useful "rules of thumb" for treatment response prediction using interpretable machine learning models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11183v2</guid>
      <category>q-bio.QM</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Morgan B. Talbot, Jessica M. Lipschitz, Omar Costilla-Reyes</dc:creator>
    </item>
    <item>
      <title>DBgDel: Database-Enhanced Gene Deletion Framework for Growth-Coupled Production in Genome-Scale Metabolic Models</title>
      <link>https://arxiv.org/abs/2411.08077</link>
      <description>arXiv:2411.08077v2 Announce Type: replace 
Abstract: When simulating metabolite productions with genome-scale constraint-based metabolic models, gene deletion strategies are necessary to achieve growth-coupled production, which means cell growth and target metabolite production occur simultaneously. Since obtaining gene deletion strategies for large genome-scale models suffers from significant computational time, it is necessary to develop methods to mitigate this computational burden. In this study, we introduce a novel framework for computing gene deletion strategies. The proposed framework first mines related databases to extract prior information about gene deletions for growth-coupled production. It then integrates the extracted information with downstream algorithms to narrow down the algorithmic search space, resulting in highly efficient calculations on genome-scale models. Computational experiment results demonstrated that our framework can compute stoichiometrically feasible gene deletion strategies for numerous target metabolites, showcasing a noteworthy improvement in computational efficiency. Specifically, our framework achieves an average 6.1-fold acceleration in computational speed compared to existing methods while maintaining a respectable success rate. The source code of DBgDel with examples are available on https://github.com/MetNetComp/DBgDel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08077v2</guid>
      <category>q-bio.QM</category>
      <category>cs.DB</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziwei Yang, Takeyuki Tamura</dc:creator>
    </item>
  </channel>
</rss>
