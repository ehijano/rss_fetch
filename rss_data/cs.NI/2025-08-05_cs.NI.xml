<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 06 Aug 2025 01:34:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Deep Reinforcement Learning-Based TCP Congestion Control Algorithm: Design, Simulation, and Evaluation</title>
      <link>https://arxiv.org/abs/2508.01047</link>
      <description>arXiv:2508.01047v1 Announce Type: new 
Abstract: This paper presents a novel TCP congestion control algorithm based on Deep Reinforcement Learning. The proposed approach utilizes Deep Q-Networks to optimize the congestion window (cWnd) by observing key network parameters and taking real-time actions. The algorithm is trained and evaluated within the NS-3 network simulator using the OpenGym interface. The results demonstrate significant improvements over traditional TCP New Reno in terms of latency and throughput, with better adaptability to changing network conditions. This study emphasizes the potential of reinforcement learning techniques for solving complex congestion control problems in modern networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01047v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Efe A\u{g}lamazlar, Emirhan Eken, Harun Batur Ge\c{c}ici</dc:creator>
    </item>
    <item>
      <title>Connectivity Management in Satellite-Aided Vehicular Networks with Multi-Head Attention-Based State Estimation</title>
      <link>https://arxiv.org/abs/2508.01060</link>
      <description>arXiv:2508.01060v1 Announce Type: new 
Abstract: Managing connectivity in integrated satellite-terrestrial vehicular networks is critical for 6G, yet is challenged by dynamic conditions and partial observability. This letter introduces the Multi-Agent Actor-Critic with Satellite-Aided Multi-head self-attention (MAAC-SAM), a novel multi-agent reinforcement learning framework that enables vehicles to autonomously manage connectivity across Vehicle-to-Satellite (V2S), Vehicle-to-Infrastructure (V2I), and Vehicle-to-Vehicle (V2V) links. Our key innovation is the integration of a multi-head attention mechanism, which allows for robust state estimation even with fluctuating and limited information sharing among vehicles. The framework further leverages self-imitation learning (SIL) and fingerprinting to improve learning efficiency and real-time decisions. Simulation results, based on realistic SUMO traffic models and 3GPP-compliant configurations, demonstrate that MAAC-SAM outperforms state-of-the-art terrestrial and satellite-assisted baselines by up to 14% in transmission utility and maintains high estimation accuracy across varying vehicle densities and sharing levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01060v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ibrahim Althamary, Chen-Fu Chou, Chih-Wei Huang</dc:creator>
    </item>
    <item>
      <title>Improving performance of content-centric networks via decentralized coded caching for multi-level popularity and access</title>
      <link>https://arxiv.org/abs/2508.01298</link>
      <description>arXiv:2508.01298v1 Announce Type: new 
Abstract: Content-Centric Networking (CCN) offers a novel architectural paradigm that seeks to address the inherent limitations of the prevailing Internet Protocol (IP)-based networking model. In contrast to the host-centric communication approach of IP networks, CCN prioritizes content by enabling direct addressing and routing based on content identifiers. The potential performance improvements of CCN can be further amplified through optimized management of coded data storage and transmission strategies. Decentralized Coded Caching (DCC) emerges as a promising technique that harnesses the collective caching power of distributed network elements. By strategically pre-positioning frequently accessed content closer to potential consumers during periods of low network utilization, DCC has the potential to mitigate content transfer rates during peak traffic periods. This paper proposes a series of fundamental modifications to the CCN architecture by integrating DCC. The proposed framework incorporates differentiated coding strategies tailored to user access privileges, thereby eliminating the overhead associated with queue-based searching. Additionally, the framework facilitates recoding of uncoded data encountered along the content delivery path. These combined methodologies demonstrably enhance network throughput, elevate cache hit ratios, and consequently, reduce content delivery latency compared to conventional CCN implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01298v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10586-025-05256-6</arxiv:DOI>
      <dc:creator>Azadeh Sadat Miraftab, Ahmadreza Montazerolghaem, Behrad Mahboobi</dc:creator>
    </item>
    <item>
      <title>M3LLM: Model Context Protocol-aided Mixture of Vision Experts For Multimodal LLMs in Networks</title>
      <link>https://arxiv.org/abs/2508.01805</link>
      <description>arXiv:2508.01805v1 Announce Type: new 
Abstract: Current Multimodal Large Language Models (MLLMs) rely on centralized architectures and often suffer from poor alignment between the input task and their fixed visual encoding modules, which limits performance on diverse and dynamic visual tasks. With the increasing deployment of resource-efficient models on edge devices in wireless networks, a new opportunity emerges to dynamically use distributed vision experts for improved MLLM inference quality. To enable this, we propose M3LLM, where the Model Context Protocol (MCP) coordinates a mixture of vision experts to achieve distributed MLLMs. Specifically, MCP is an open protocol that structures the input task context into interpretable representations, enabling wireless network-aware coordination between the central model backbone and edge-hosted vision experts. Based on the MCP representation, M3LLM formulates vision expert routing as a joint optimization problem that balances task-expert semantic compatibility and channel performance. To solve the resulting gradient conflicts, we develop a dual-stream Soft Actor-Critic (SAC) algorithm with decoupled reward signals and introduce an Adaptive Stability Enhancement Module (ASEM) based on hierarchical Bayesian modeling to ensure effective routing. Experiments show that M3LLM improves task accuracy, reduces communication cost, and enhances expert routing adaptability under dynamic wireless network conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01805v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yongjie Zeng, Hongyang Du</dc:creator>
    </item>
    <item>
      <title>Revenue Optimization in Wireless Video Caching Networks: A Privacy-Preserving Two-Stage Solution</title>
      <link>https://arxiv.org/abs/2508.01898</link>
      <description>arXiv:2508.01898v1 Announce Type: new 
Abstract: Video caching can significantly improve delivery efficiency and enhance quality of video streaming, which constitutes the majority of wireless communication traffic. Due to limited cache size, caching strategies must be designed to adapt to and dynamic user demand in order to maximize system revenue. The system revenue depends on the benefits of delivering the requested videos and costs for (a) transporting the files to the users and (b) cache replacement. Since the cache content at any point in time impacts the replacement costs in the future, demand predictions over multiple cache placement slots become an important prerequisite for efficient cache planning. Motivated by this, we introduce a novel two-stage privacy-preserving solution for revenue optimization in wireless video caching networks. First, we train a Transformer using privacy-preserving federated learning (FL) to predict multi-slot future demands. Given that prediction results are never entirely accurate, especially for longer horizons, we further combine global content popularity with per-user prediction results to estimate the content demand distribution. Then, in the second stage, we leverage these estimation results to find caching strategies that maximize the long-term system revenue. This latter problem takes on the form of a multi-stage knapsack problem, which we then transform to a integer linear program. Our extensive simulation results demonstrate that (i) our FL solution delivers nearly identical performance to that of the ideal centralized solution and outperforms other existing caching methods, and (ii) our novel revenue optimization approach provides deeper system performance insights than traditional cache hit ratio (CHR)-based optimization approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01898v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yijing Zhang, Md-Ferdous Pervej, Andreas F. Molisch</dc:creator>
    </item>
    <item>
      <title>Convolutions are Competitive with Transformers for Encrypted Traffic Classification with Pre-training</title>
      <link>https://arxiv.org/abs/2508.02001</link>
      <description>arXiv:2508.02001v1 Announce Type: new 
Abstract: Encrypted traffic classification is vital for modern network management and security. To reduce reliance on handcrafted features and labeled data, recent methods focus on learning generic representations through pre-training on large-scale unlabeled data. However, current pre-trained models face two limitations originating from the adopted Transformer architecture: (1) Limited model efficiency due to the self-attention mechanism with quadratic complexity; (2) Unstable traffic scalability to longer byte sequences, as the explicit positional encodings fail to generalize to input lengths not seen during pre-training. In this paper, we investigate whether convolutions, with linear complexity and implicit positional encoding, are competitive with Transformers in encrypted traffic classification with pre-training. We first conduct a systematic comparison, and observe that convolutions achieve higher efficiency and scalability, with lower classification performance. To address this trade-off, we propose NetConv, a novel pre-trained convolution model for encrypted traffic classification. NetConv employs stacked traffic convolution layers, which enhance the ability to capture localized byte-sequence patterns through window-wise byte scoring and sequence-wise byte gating. We design a continuous byte masking pre-training task to help NetConv learn protocol-specific patterns. Experimental results on four tasks demonstrate that NetConv improves average classification performance by 6.88% and model throughput by 7.41X over existing pre-trained models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02001v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chungang Lin, Weiyao Zhang, Tianyu Zuo, Chao Zha, Yilong Jiang, Ruiqi Meng, Haitong Luo, Xuying Meng, Yujun Zhang</dc:creator>
    </item>
    <item>
      <title>PRIME: Plasticity-Robust Incremental Model for Encrypted Traffic Classification in Dynamic Network Environments</title>
      <link>https://arxiv.org/abs/2508.02031</link>
      <description>arXiv:2508.02031v1 Announce Type: new 
Abstract: With the continuous development of network environments and technologies, ensuring cyber security and governance is increasingly challenging. Network traffic classification(ETC) can analyzes attributes such as application categories and malicious intent, supporting network management services like QoS optimization, intrusion detection, and targeted billing. As the prevalence of traffic encryption increases, deep learning models are relied upon for content-agnostic analysis of packet sequences. However, the emergence of new services and attack variants often leads to incremental tasks for ETC models. To ensure model effectiveness, incremental learning techniques are essential; however, recent studies indicate that neural networks experience declining plasticity as tasks increase. We identified plasticity issues in existing incremental learning methods across diverse traffic samples and proposed the PRIME framework. By observing the effective rank of model parameters and the proportion of inactive neurons, the PRIME architecture can appropriately increase the parameter scale when the model's plasticity deteriorates. Experiments show that in multiple encrypted traffic datasets and different category increment scenarios, the PRIME architecture performs significantly better than other incremental learning algorithms with minimal increase in parameter scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02031v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tian Qin, Guang Cheng, Zihan Chen, Yuyang Zhou</dc:creator>
    </item>
    <item>
      <title>Distillation-Enhanced Clustering Acceleration for Encrypted Traffic Classification</title>
      <link>https://arxiv.org/abs/2508.02282</link>
      <description>arXiv:2508.02282v1 Announce Type: new 
Abstract: Traffic classification plays a significant role in network service management. The advancement of deep learning has established pretrained models as a robust approach for this task. However, contemporary encrypted traffic classification systems face dual limitations. Firstly, pretrained models typically exhibit large-scale architectures, where their extensive parameterization results in slow inference speeds and high computational latency. Secondly, reliance on labeled data for fine-tuning restricts these models to predefined supervised classes, creating a bottleneck when novel traffic types emerge in the evolving Internet landscape. To address these challenges, we propose NetClus, a novel framework integrating pretrained models with distillation-enhanced clustering acceleration. During fine-tuning, NetClus first introduces a cluster-friendly loss to jointly reshape the latent space for both classification and clustering. With the fine-tuned model, it distills the model into a lightweight Feed-Forward Neural Network model to retain semantics. During inference, NetClus performs heuristic merge with near-linear runtime, and valid the cluster purity with newly proposed metrics ASI to identify emergent traffic types while expediting classification. Benchmarked against existing pretrained methods, NetClus achieves up to 6.2x acceleration while maintaining classification degradation below 1%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02282v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyue Huang, Chungang Lin, Weiyao Zhang, Xuying Meng, Yujun Zhang</dc:creator>
    </item>
    <item>
      <title>On Effectiveness of Graph Neural Network Architectures for Network Digital Twins (NDTs)</title>
      <link>https://arxiv.org/abs/2508.02373</link>
      <description>arXiv:2508.02373v1 Announce Type: new 
Abstract: Future networks, such as 6G, will need to support a vast and diverse range of interconnected devices and applications, each with its own set of requirements. While traditional network management approaches will suffice, an automated solutions are becoming a must. However, network automation frameworks are prone to errors, and often they employ ML-based techniques that require training to learn how the network can be optimized. In this sense, network digital twins are a useful tool that allows for the simulation, testing, and training of AI models without affecting the real-world networks and users. This paper presents an AI-based Network Digital Twin (AI-NDT) that leverages a multi-layered knowledge graph architecture and graph neural networks to predict network metrics that directly affect the quality of experience of users. An evaluation of the four most prominent Graph Neural Networks (GNN) architectures was conducted to assess their effectiveness in developing network digital twins. We trained the digital twin on publicly available measurement data from RIPE Atlas, therefore obtaining results close to what is expected in real-world applications. The results show that among the four architectures evaluated, GraphTransformer presents the best performance. However, other architectures might fit better in scenarios where shorter training time is important, while also delivering acceptable results. The results of this work are indicative of what might become common practice for proactive network management, offering a scalable and accurate solution aligned with the requirements of the next-generation networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02373v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iulisloi Zacarias, Oussama Ben Taarit, Admela Jukan</dc:creator>
    </item>
    <item>
      <title>ASINT: Learning AS-to-Organization Mapping from Internet Metadata</title>
      <link>https://arxiv.org/abs/2508.02571</link>
      <description>arXiv:2508.02571v1 Announce Type: new 
Abstract: Accurately mapping Autonomous Systems (ASNs) to their owning or operating organizations underpins Internet measurement research and security applications. Yet existing approaches commonly rely solely on WHOIS or PeeringDB, missing important relationships (e.g., cross-regional aliases, parent-child ownership) and failing to unify organizations scattered across different RIR identifiers. We introduce ASINT, an end-to-end pipeline that fuses bulk registry data with unstructured Web sources, then employs retrieval-augmented generation (RAG) to guide large language model (LLM) inference. Through a multi-stage procedure, ASINT merges ASNs into "organization families," capturing nuanced ties beyond the scope of simpler heuristics.
  ASINT maps 111,470 ASNs to 81,233 organization families; compared to both AS2ORG+ and AS-Sibling, ASINT identifies more cross-regional groupings (e.g., operator aliases, rebrands) that other datasets overlook. Moreover, our refined mappings enhance multiple security and measurement tasks: ASINT exposes 27.5% more intra-organizational RPKI misconfigurations, cuts false-positive hijack alarms by 9.4%, and lowers erroneous IP leasing inferences by 5.9%.
  Finally, ASINT supports periodic updates and cost-sensitive LLM selection, demonstrating that broader Web evidence can provide a more accurate, evolving view of the Internet's organizational structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02571v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yongzhe Xu, Weitong Li, Eeshan Umrani, Taejoong Chung</dc:creator>
    </item>
    <item>
      <title>eBPF-Based Real-Time DDoS Mitigation for IoT Edge Devices</title>
      <link>https://arxiv.org/abs/2508.00851</link>
      <description>arXiv:2508.00851v1 Announce Type: cross 
Abstract: The rapid expansion of the Internet of Things (IoT) has intensified security challenges, notably from Distributed Denial of Service (DDoS) attacks launched by compromised, resource-constrained devices. Traditional defenses are often ill-suited for the IoT paradigm, creating a need for lightweight, high-performance, edge-based solutions. This paper presents the design, implementation, and evaluation of an IoT security framework that leverages the extended Berkeley Packet Filter (eBPF) and the eXpress Data Path (XDP) for in-kernel mitigation of DDoS attacks. The system uses a rate-based detection algorithm to identify and block malicious traffic at the earliest stage of the network stack. The framework is evaluated using both Docker-based simulations and real-world deployment on a Raspberry Pi 4, showing over 97% mitigation effectiveness under a 100 Mbps flood. Legitimate traffic remains unaffected, and system stability is preserved even under attack. These results confirm that eBPF/XDP provides a viable and highly efficient solution for hardening IoT edge devices against volumetric network attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00851v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdurrahman Tolay</dc:creator>
    </item>
    <item>
      <title>Service Discovery-Based Hybrid Network Middleware for Efficient Communication in Distributed Robotic Systems</title>
      <link>https://arxiv.org/abs/2508.00947</link>
      <description>arXiv:2508.00947v1 Announce Type: cross 
Abstract: Robotic middleware is fundamental to ensuring reliable communication among system components and is crucial for intelligent robotics, autonomous vehicles, and smart manufacturing. However, existing robotic middleware often struggles to meet the diverse communication demands, optimize data transmission efficiency, and maintain scheduling determinism between Orin computing units in large-scale L4 autonomous vehicle deployments. This paper presents RIMAOS2C, a service discovery-based hybrid network communication middleware designed to tackle these challenges. By leveraging multi-level service discovery multicast, RIMAOS2C supports a wide variety of communication modes, including multiple cross-chip Ethernet protocols and PCIe communication capabilities. Its core mechanism, the Message Bridge, optimizes data flow forwarding and employs shared memory for centralized message distribution, reducing message redundancy and minimizing transmission delay uncertainty. Tested on L4 vehicles and Jetson Orin domain controllers, RIMAOS2C leverages TCP-based ZeroMQ to overcome the large-message transmission bottleneck in native CyberRT. In scenarios with two cross-chip subscribers, it eliminates message redundancy and improves large-data transmission efficiency by 36 to 40 percent while reducing callback latency variation by 42 to 906 percent. This research advances the communication capabilities of robotic operating systems and proposes a novel approach to optimizing communication in distributed computing architectures for autonomous driving.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00947v1</guid>
      <category>cs.RO</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiyao Sang, Yinggang Ling</dc:creator>
    </item>
    <item>
      <title>VWAttacker: A Systematic Security Testing Framework for Voice over WiFi User Equipments</title>
      <link>https://arxiv.org/abs/2508.01469</link>
      <description>arXiv:2508.01469v1 Announce Type: cross 
Abstract: We present VWAttacker, the first systematic testing framework for analyzing the security of Voice over WiFi (VoWiFi) User Equipment (UE) implementations. VWAttacker includes a complete VoWiFi network testbed that communicates with Commercial-Off-The-Shelf (COTS) UEs based on a simple interface to test the behavior of diverse VoWiFi UE implementations; uses property-guided adversarial testing to uncover security issues in different UEs systematically. To reduce manual effort in extracting and testing properties, we introduce an LLM-based, semi-automatic, and scalable approach for property extraction and testcase (TC) generation. These TCs are systematically mutated by two domain-specific transformations. Furthermore, we introduce two deterministic oracles to detect property violations automatically. Coupled with these techniques, VWAttacker extracts 63 properties from 11 specifications, evaluates 1,116 testcases, and detects 13 issues in 21 UEs. The issues range from enforcing a DH shared secret to 0 to supporting weak algorithms. These issues result in attacks that expose the victim UE's identity or establish weak channels, thus severely hampering the security of cellular networks. We responsibly disclose the findings to all the related vendors. At the time of writing, one of the vulnerabilities has been acknowledged by MediaTek with high severity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01469v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Imtiaz Karim, Hyunwoo Lee, Hassan Asghar, Kazi Samin Mubasshir, Seulgi Han, Mashroor Hasan Bhuiyan, Elisa Bertino</dc:creator>
    </item>
    <item>
      <title>Diffusion Models for Future Networks and Communications: A Comprehensive Survey</title>
      <link>https://arxiv.org/abs/2508.01586</link>
      <description>arXiv:2508.01586v1 Announce Type: cross 
Abstract: The rise of Generative AI (GenAI) in recent years has catalyzed transformative advances in wireless communications and networks. Among the members of the GenAI family, Diffusion Models (DMs) have risen to prominence as a powerful option, capable of handling complex, high-dimensional data distribution, as well as consistent, noise-robust performance. In this survey, we aim to provide a comprehensive overview of the theoretical foundations and practical applications of DMs across future communication systems. We first provide an extensive tutorial of DMs and demonstrate how they can be applied to enhance optimizers, reinforcement learning and incentive mechanisms, which are popular approaches for problems in wireless networks. Then, we review and discuss the DM-based methods proposed for emerging issues in future networks and communications, including channel modeling and estimation, signal detection and data reconstruction, integrated sensing and communication, resource management in edge computing networks, semantic communications and other notable issues. We conclude the survey with highlighting technical limitations of DMs and their applications, as well as discussing future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01586v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nguyen Cong Luong, Nguyen Duc Hai, Duc Van Le, Huy T. Nguyen, Thai-Hoc Vu, Thien Huynh-The, Ruichen Zhang, Nguyen Duc Duy Anh, Dusit Niyato, Marco Di Renzo, Dong In Kim, Quoc-Viet Pham</dc:creator>
    </item>
    <item>
      <title>Hard-Earned Lessons in Access Control at Scale: Enforcing Identity and Policy Across Trust Boundaries with Reverse Proxies and mTLS</title>
      <link>https://arxiv.org/abs/2508.01863</link>
      <description>arXiv:2508.01863v1 Announce Type: cross 
Abstract: In today's enterprise environment, traditional access methods such as Virtual Private Networks (VPNs) and application-specific Single Sign-On (SSO) often fall short when it comes to securely scaling access for a distributed and dynamic workforce. This paper presents our experience implementing a modern, Zero Trust-aligned architecture that leverages a reverse proxy integrated with Mutual TLS (mTLS) and centralized SSO, along with the key challenges we encountered and lessons learned during its deployment and scaling. This multidimensional solution involves both per-device and per-user authentication, centralized enforcement of security policies, and comprehensive observability, hence enabling organizations to deliver secure and seamless access to their internal applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01863v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <category>cs.SE</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjay Singh, Mitendra Mahto</dc:creator>
    </item>
    <item>
      <title>Balancing Information Accuracy and Response Timeliness in Networked LLMs</title>
      <link>https://arxiv.org/abs/2508.02209</link>
      <description>arXiv:2508.02209v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) have transformed many fields including scientific discovery, content generation, biomedical text mining, and educational technology. However, the substantial requirements for training data, computational resources, and energy consumption pose significant challenges for their practical deployment. A promising alternative is to leverage smaller, specialized language models and aggregate their outputs to improve overall response quality. In this work, we investigate a networked LLM system composed of multiple users, a central task processor, and clusters of topic-specialized LLMs. Each user submits categorical binary (true/false) queries, which are routed by the task processor to a selected cluster of $m$ LLMs. After gathering individual responses, the processor returns a final aggregated answer to the user. We characterize both the information accuracy and response timeliness in this setting, and formulate a joint optimization problem to balance these two competing objectives. Our extensive simulations demonstrate that the aggregated responses consistently achieve higher accuracy than those of individual LLMs. Notably, this improvement is more significant when the participating LLMs exhibit similar standalone performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02209v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yigit Turkmen, Baturalp Buyukates, Melih Bastopcu</dc:creator>
    </item>
    <item>
      <title>Fully Decentralised Consensus for Extreme-scale Blockchain</title>
      <link>https://arxiv.org/abs/2508.02595</link>
      <description>arXiv:2508.02595v1 Announce Type: cross 
Abstract: Blockchain is a decentralised, immutable ledger technology that has been widely adopted in many sectors for various applications such as cryptocurrencies, smart contracts and supply chain management. Distributed consensus is a fundamental component of blockchain, which is required to ensure trust, security, and integrity of the data stored and the transactions processed in the blockchain. Various consensus algorithms have been developed, each affected from certain issues such as node failures, high resource consumption, collusion, etc. This work introduces a fully decentralised consensus protocol, Blockchain Epidemic Consensus Protocol (BECP), suitable for very large and extreme-scale blockchain systems. The proposed approach leverages the benefits of epidemic protocols, such as no reliance on a fixed set of validators or leaders, probabilistic guarantees of convergence, efficient use of network resources, and tolerance to node and network failures. A comparative experimental analysis has been carried out with traditional protocols including PAXOS, RAFT, and Practical Byzantine Fault Tolerance (PBFT), as well as a relatively more recent protocol such as Avalanche, which is specifically designed for very large-scale systems. The results illustrate how BECP outperforms them in terms of throughput, scalability and consensus latency. BECP achieves an average of 1.196 times higher throughput in terms of consensus on items and 4.775 times better average consensus latency. Furthermore, BECP significantly reduces the number of messages compared to Avalanche. These results demonstrate the effectiveness and efficiency of fully decentralised consensus for blockchain technology based on epidemic protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02595v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siamak Abdi, Giuseppe Di Fatta, Atta Badii, Giancarlo Fortino</dc:creator>
    </item>
    <item>
      <title>RC-Gossip: Information Freshness in Clustered Networks with Rate-Changing Gossip</title>
      <link>https://arxiv.org/abs/2508.02657</link>
      <description>arXiv:2508.02657v1 Announce Type: cross 
Abstract: A clustered gossip network is considered in which a source updates its information over time, and end-nodes, organized in clusters through clusterheads, are keeping track of it. The goal for the nodes is to remain as fresh as possible, i.e., have the same information as the source, which we assess by the long-term average binary freshness metric. We introduce a smart mechanism of information dissemination which we coin rate-changing gossip (RC-Gossip). Its main idea is that gossiping is directed towards nodes that need it the most, and hence the rate of gossiping changes based on the number of fresh nodes in the network at a given time. While Stochastic Hybrid System (SHS) analysis has been the norm in studying freshness of gossip networks, we present an equivalent way to analyze freshness using a renewal-reward-based approach. Using that, we show that RC-gossip significantly increases freshness of nodes in different clustered networks, with optimal cluster sizes, compared to traditional gossiping techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02657v1</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Irtiza Hasan, Ahmed Arafa</dc:creator>
    </item>
    <item>
      <title>Dynamic Optimization of Video Streaming Quality Using Network Digital Twin Technology</title>
      <link>https://arxiv.org/abs/2407.00513</link>
      <description>arXiv:2407.00513v2 Announce Type: replace 
Abstract: This paper introduces a novel dynamic optimization framework for video streaming that leverages Network Digital Twin (NDT) technology to address the challenges posed by fluctuating wireless network conditions. Traditional adaptive streaming methods often struggle with rapid changes in network bandwidth, latency, and packet loss, leading to suboptimal user experiences characterized by frequent buffering and reduced video quality. Our proposed framework integrates a sophisticated NDT that models the wireless network in real-time and employs predictive analytics to forecast near-future network states. Utilizing machine learning techniques, specifically Random Forest and Neural Networks, the NDT predicts bandwidth availability, latency trends, and potential packet losses before they impact video transmission. Based on these predictions, our adaptive streaming algorithm dynamically adjusts video bitrates, resolution, and buffering strategies, thus ensuring an uninterrupted and high-quality viewing experience. Experimental validations demonstrate that our approach significantly enhances the Quality of Experience (QoE) by reducing buffering times by up to 50\% and improving resolution in varied network conditions compared to conventional streaming methods. This paper underscores the potential of integrating digital twin technology into multimedia transmission, paving the way for more resilient and user-centric video streaming solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00513v2</guid>
      <category>cs.NI</category>
      <category>cs.GR</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zurh Farus, Betty Searcy, Tina Nassisid, Kevin Muhammad</dc:creator>
    </item>
    <item>
      <title>Efficient Resource Management in Multicast Short Video Streaming Systems</title>
      <link>https://arxiv.org/abs/2407.00552</link>
      <description>arXiv:2407.00552v2 Announce Type: replace 
Abstract: The surge in popularity of short-form video content, particularly through platforms like TikTok and Instagram, has led to an exponential increase in data traffic, presenting significant challenges in network resource management. Traditional unicast streaming methods, while straightforward, are inefficient in scenarios where videos need to be delivered to a large number of users simultaneously. Multicast streaming, which sends a single stream to multiple users, can drastically reduce the required bandwidth, yet it introduces complexities in resource allocation, especially in wireless environments where bandwidth is limited and user demands are heterogeneous. This paper introduces a novel multicast resource management framework tailored for the efficient distribution of short-form video content. The proposed framework dynamically optimizes resource allocation to enhance Quality of Service (QoS) and Quality of Experience (QoE) for multiple users, balancing the trade-offs between cost, efficiency, and user satisfaction. We implement a series of optimization algorithms that account for diverse network conditions and user requirements, ensuring optimal service delivery across varying network topologies. Experimental results demonstrate that our framework can effectively reduce bandwidth usage and decrease video startup delay compared to traditional multicast approaches, significantly improving overall user satisfaction. This study not only advances the understanding of multicast streaming dynamics but also provides practical insights into scalable and efficient video distribution strategies in congested network environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00552v2</guid>
      <category>cs.NI</category>
      <category>cs.GR</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Betty Searcy, Zurh Farus, Bronny Bush, Kevin Muhammad, Zubair Clinton</dc:creator>
    </item>
    <item>
      <title>Adaptive Video Streaming over 6G Networks: Buffer Control and User Behavior Analysis</title>
      <link>https://arxiv.org/abs/2407.05436</link>
      <description>arXiv:2407.05436v2 Announce Type: replace 
Abstract: This paper delves into the synergistic potential of adaptive video streaming over emerging 6G wireless networks, emphasizing innovative buffer control techniques and detailed analysis of user viewing behaviors. As 6G technology heralds a new era with significantly enhanced capabilities including higher bandwidths, lower latencies, and increased connection densities, it is poised to fundamentally transform video streaming services. This study explores the integration of these technological advancements to optimize video streaming processes, ensuring seamless service delivery and superior Quality of Experience (QoE) for users. We propose novel buffer management strategies that leverage the ultra-reliable and low-latency communication features of 6G networks to mitigate issues related to video streaming such as rebuffering and quality fluctuations. Additionally, we examine how insights into viewing behaviors can inform adaptive streaming algorithms, allowing for real-time adjustments that align with user preferences and viewing conditions. The implications of our findings are demonstrated through rigorous simulation studies, which validate the effectiveness of our proposed solutions across diverse scenarios. This research not only highlights the challenges faced in deploying adaptive streaming solutions over 6G but also outlines future directions for research and development in this fast-evolving field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05436v2</guid>
      <category>cs.NI</category>
      <category>cs.GR</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kevin Nassisid, Teef David, Kassi Muhammad</dc:creator>
    </item>
    <item>
      <title>Enhancing Vehicular Networks with Generative AI: Opportunities and Challenges</title>
      <link>https://arxiv.org/abs/2407.11020</link>
      <description>arXiv:2407.11020v2 Announce Type: replace 
Abstract: In the burgeoning field of intelligent transportation systems, the integration of Generative Artificial Intelligence (AI) into vehicular networks presents a transformative potential for the automotive industry. This paper explores the innovative applications of generative AI in enhancing communication protocols, optimizing traffic management, and bolstering security frameworks within vehicular networks. By examining current technologies and recent advancements, we identify key challenges such as scalability, real-time data processing, and security vulnerabilities that come with AI integration. Additionally, we propose novel applications and methodologies that leverage generative AI to simulate complex network scenarios, generate adaptive communication schemes, and enhance predictive capabilities for traffic conditions. This study not only reviews the state of the art but also highlights significant opportunities where generative AI can lead to groundbreaking improvements in vehicular network efficiency and safety. Through this comprehensive exploration, our findings aim to guide future research directions and foster a deeper understanding of generative AI's role in the next generation of vehicular technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11020v2</guid>
      <category>cs.NI</category>
      <category>cs.GR</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Teef David, Kassi Muhammad, Kevin Nassisid, Bronny Farus</dc:creator>
    </item>
    <item>
      <title>MixNet: A Runtime Reconfigurable Optical-Electrical Fabric for Distributed Mixture-of-Experts Training</title>
      <link>https://arxiv.org/abs/2501.03905</link>
      <description>arXiv:2501.03905v3 Announce Type: replace 
Abstract: Mixture-of-Expert (MoE) models outperform conventional models by selectively activating different subnets, named \emph{experts}, on a per-token basis. This gated computation generates dynamic communications that cannot be determined beforehand, challenging the existing GPU interconnects that remain \emph{static} during the distributed training process. In this paper, we advocate for a first-of-its-kind system, called mFabric, that unlocks topology reconfiguration \emph{during} distributed MoE training. Towards this vision, we first perform a production measurement study and show that the MoE dynamic communication pattern has \emph{strong locality}, alleviating the requirement of global reconfiguration. Based on this, we design and implement a \emph{regionally reconfigurable high-bandwidth domain} on top of existing electrical interconnects using optical circuit switching (OCS), achieving scalability while maintaining rapid adaptability. We have built a fully functional mFabric prototype with commodity hardware and a customized collective communication runtime that trains state-of-the-art MoE models with \emph{in-training} topology reconfiguration across 32 A100 GPUs. Large-scale packet-level simulations show that mFabric delivers comparable performance as the non-blocking fat-tree fabric while boosting the training cost efficiency (e.g., performance per dollar) of four representative MoE models by 1.2$\times$--1.5$\times$ and 1.9$\times$--2.3$\times$ at 100 Gbps and 400 Gbps link bandwidths, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03905v3</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xudong Liao, Yijun Sun, Han Tian, Xinchen Wan, Yilun Jin, Zilong Wang, Zhenghang Ren, Xinyang Huang, Wenxue Li, Kin Fai Tse, Zhizhen Zhong, Guyue Liu, Ying Zhang, Xiaofeng Ye, Yiming Zhang, Kai Chen</dc:creator>
    </item>
    <item>
      <title>6G Cellular Networks: Mapping the Landscape for the IMT-2030 Framework</title>
      <link>https://arxiv.org/abs/2501.14552</link>
      <description>arXiv:2501.14552v2 Announce Type: replace 
Abstract: The IMT-2030 framework provides the vision and conceptual foundation for the next-generation of mobile broadband systems, colloquially known as Sixth-Generation (6G) cellular networks. Academic circles, industry players, and Standard Developing Organizations (SDOs) are already engaged in early standardization discussions for the system, providing key insights for future technical specifications. In this context, a structured thematic synthesis aligned with IMT-2030 is essential to inform the discussions and assist collaboration among 6G stakeholders -- including scholars, professionals, regulators, and SDO officials. This review intends to offer a concise yet informative synthesis of well-established 6G literature, viewed through the IMT-2030 lens, for both specialists and generalists engaged in shaping future standards and advancing 6G research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14552v2</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ekram Hossain, Angelo Vera-Rivera</dc:creator>
    </item>
    <item>
      <title>The Forest Behind the Tree: Revealing Hidden Smart Home Communication Patterns</title>
      <link>https://arxiv.org/abs/2502.08535</link>
      <description>arXiv:2502.08535v4 Announce Type: replace 
Abstract: The widespread use of Smart Home devices has attracted significant research interest in understanding their behavior within home networks. Unlike general-purpose computers, these devices exhibit relatively simple and predictable network activity patterns. However, previous studies have primarily focused on normal network conditions, overlooking potential hidden patterns that emerge under challenging conditions. Discovering these hidden flows is crucial for assessing device robustness. This paper addresses this gap by presenting a framework that systematically and automatically reveals these hidden communication patterns. By actively disturbing communication and blocking observed traffic, the framework generates comprehensive profiles structured as behavior trees, uncovering flows that are missed by more shallow methods. This approach was applied to ten real-world devices, identifying 254 unique flows, with over 27% only discovered through this new method. These insights enhance our understanding of device robustness and can be leveraged to improve the accuracy of network security measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08535v4</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fran\c{c}ois De Keersmaeker, R\'emi Van Boxem, Cristel Pelsser, Ramin Sadre</dc:creator>
    </item>
    <item>
      <title>Intent-Based Network for RAN Management with Large Language Models</title>
      <link>https://arxiv.org/abs/2507.14230</link>
      <description>arXiv:2507.14230v2 Announce Type: replace 
Abstract: Advanced intelligent automation becomes an important feature to deal with the increased complexity in managing wireless networks. This paper proposes a novel automation approach of intent-based network for Radio Access Networks (RANs) management by leveraging Large Language Models (LLMs). The proposed method enhances intent translation, autonomously interpreting high-level objectives, reasoning over complex network states, and generating precise configurations of the RAN by integrating LLMs within an agentic architecture. We propose a structured prompt engineering technique and demonstrate that the network can automatically improve its energy efficiency by dynamically optimizing critical RAN parameters through a closed-loop mechanism. It showcases the potential to enable robust resource management in RAN by adapting strategies based on real-time feedback via LLM-orchestrated agentic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14230v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fransiscus Asisi Bimo, Maria Amparo Canaveras Galdon, Chun-Kai Lai, Ray-Guang Cheng, Edwin K. P. Chong</dc:creator>
    </item>
    <item>
      <title>Integrating Generative AI with Network Digital Twins for Enhanced Network Operations</title>
      <link>https://arxiv.org/abs/2406.17112</link>
      <description>arXiv:2406.17112v2 Announce Type: replace-cross 
Abstract: As telecommunications networks become increasingly complex, the integration of advanced technologies such as network digital twins and generative artificial intelligence (AI) emerges as a pivotal solution to enhance network operations and resilience. This paper explores the synergy between network digital twins, which provide a dynamic virtual representation of physical networks, and generative AI, particularly focusing on Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). We propose a novel architectural framework that incorporates these technologies to significantly improve predictive maintenance, network scenario simulation, and real-time data-driven decision-making. Through extensive simulations, we demonstrate how generative AI can enhance the accuracy and operational efficiency of network digital twins, effectively handling real-world complexities such as unpredictable traffic loads and network failures. The findings suggest that this integration not only boosts the capability of digital twins in scenario forecasting and anomaly detection but also facilitates a more adaptive and intelligent network management system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17112v2</guid>
      <category>cs.LG</category>
      <category>cs.GR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kassi Muhammad, Teef David, Giulia Nassisid, Tina Farus</dc:creator>
    </item>
    <item>
      <title>User Digital Twin-Driven Video Streaming for Customized Preferences and Adaptive Transcoding</title>
      <link>https://arxiv.org/abs/2407.09766</link>
      <description>arXiv:2407.09766v2 Announce Type: replace-cross 
Abstract: In the rapidly evolving field of multimedia services, video streaming has become increasingly prevalent, demanding innovative solutions to enhance user experience and system efficiency. This paper introduces a novel approach that integrates user digital twins-a dynamic digital representation of a user's preferences and behaviors-with traditional video streaming systems. We explore the potential of this integration to dynamically adjust video preferences and optimize transcoding processes according to real-time data. The methodology leverages advanced machine learning algorithms to continuously update the user's digital twin, which in turn informs the transcoding service to adapt video parameters for optimal quality and minimal buffering. Experimental results show that our approach not only improves the personalization of content delivery but also significantly enhances the overall efficiency of video streaming services by reducing bandwidth usage and improving video playback quality. The implications of such advancements suggest a shift towards more adaptive, user-centric multimedia services, potentially transforming how video content is consumed and delivered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09766v2</guid>
      <category>cs.MM</category>
      <category>cs.NI</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stephen Jimmy, Kalkidan Berhane, Kevin Muhammad</dc:creator>
    </item>
    <item>
      <title>Handoff Design in User-Centric Cell-Free Massive MIMO Networks Using DRL</title>
      <link>https://arxiv.org/abs/2507.20966</link>
      <description>arXiv:2507.20966v2 Announce Type: replace-cross 
Abstract: In the user-centric cell-free massive MIMO (UC-mMIMO) network scheme, user mobility necessitates updating the set of serving access points to maintain the user-centric clustering. Such updates are typically performed through handoff (HO) operations; however, frequent HOs lead to overheads associated with the allocation and release of resources. This paper presents a deep reinforcement learning (DRL)-based solution to predict and manage these connections for mobile users. Our solution employs the Soft Actor-Critic algorithm, with continuous action space representation, to train a deep neural network to serve as the HO policy. We present a novel proposition for a reward function that integrates a HO penalty in order to balance the attainable rate and the associated overhead related to HOs. We develop two variants of our system; the first one uses mobility direction-assisted (DA) observations that are based on the user movement pattern, while the second one uses history-assisted (HA) observations that are based on the history of the large-scale fading (LSF). Simulation results show that our DRL-based continuous action space approach is more scalable than discrete space counterpart, and that our derived HO policy automatically learns to gather HOs in specific time slots to minimize the overhead of initiating HOs. Our solution can also operate in real time with a response time less than 0.4 ms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20966v2</guid>
      <category>cs.IT</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hussein A. Ammar, Raviraj Adve, Shahram Shahbazpanahi, Gary Boudreau, Israfil Bahceci</dc:creator>
    </item>
  </channel>
</rss>
