<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Jul 2025 04:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Iran's Stealth Internet Blackout: A New Model of Censorship</title>
      <link>https://arxiv.org/abs/2507.14183</link>
      <description>arXiv:2507.14183v1 Announce Type: new 
Abstract: In mid-2025, Iran experienced a novel, stealthy Internet shutdown that preserved global routing presence while isolating domestic users through deep packet inspection, aggressive throttling, and selective protocol blocking. This paper analyzes active network measurements such as DNS poisoning, HTTP injection, TLS interception, and protocol whitelisting, traced to a centralized border gateway. We quantify an approximate 707 percent rise in VPN demand and describe the multi-layered censorship infrastructure, highlighting implications for circumvention and digital rights monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14183v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arash Aryapour</dc:creator>
    </item>
    <item>
      <title>A Disentangled Representation Learning Framework for Low-altitude Network Coverage Prediction</title>
      <link>https://arxiv.org/abs/2507.14186</link>
      <description>arXiv:2507.14186v1 Announce Type: new 
Abstract: The expansion of the low-altitude economy has underscored the significance of Low-Altitude Network Coverage (LANC) prediction for designing aerial corridors. While accurate LANC forecasting hinges on the antenna beam patterns of Base Stations (BSs), these patterns are typically proprietary and not readily accessible. Operational parameters of BSs, which inherently contain beam information, offer an opportunity for data-driven low-altitude coverage prediction. However, collecting extensive low-altitude road test data is cost-prohibitive, often yielding only sparse samples per BS. This scarcity results in two primary challenges: imbalanced feature sampling due to limited variability in high-dimensional operational parameters against the backdrop of substantial changes in low-dimensional sampling locations, and diminished generalizability stemming from insufficient data samples. To overcome these obstacles, we introduce a dual strategy comprising expert knowledge-based feature compression and disentangled representation learning. The former reduces feature space complexity by leveraging communications expertise, while the latter enhances model generalizability through the integration of propagation models and distinct subnetworks that capture and aggregate the semantic representations of latent features. Experimental evaluation confirms the efficacy of our framework, yielding a 7% reduction in error compared to the best baseline algorithm. Real-network validations further attest to its reliability, achieving practical prediction accuracy with MAE errors at the 5dB level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14186v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaojie Li, Zhijie Cai, Nan Qi, Chao Dong, Guangxu Zhu, Haixia Ma, Qihui Wu, Shi Jin</dc:creator>
    </item>
    <item>
      <title>From Cell Towers to Satellites: A 2040 Blueprint for Urban-Grade Direct-to-Device Mobile Networks</title>
      <link>https://arxiv.org/abs/2507.14188</link>
      <description>arXiv:2507.14188v1 Announce Type: new 
Abstract: In 2023, satellite and mobile networks crossed a historic threshold: standard smartphones, using unmodified 3GPP protocols, connected directly to low Earth orbit (LEO) satellites. This first wave of direct-to-device (D2D) demonstrations validated the physical feasibility of satellite-based mobile access. However, these systems remain fallback-grade--rural-only, bandwidth-limited, and fully dependent on Earth-based mobile cores for identity, session, and policy control. This paper asks a more ambitious question: Can a complete mobile network, including radio access, core functions, traffic routing, and content delivery, operate entirely from orbit? And can it deliver sustained, urban-grade service in the world's densest cities? We present the first end-to-end system architecture for a fully orbital telco, integrating electronically steered phased arrays with 1000-beam capacity, space-based deployment of 5G core functions (UPF, AMF), and inter-satellite laser mesh backhaul. We analyze spectral efficiency, beam capacity, and link budgets under dense urban conditions, accounting for path loss, Doppler, and multipath. Simulations show that rooftop and line-of-sight users can sustain 64-QAM throughput, while street-level access is feasible with relay or assisted beam modes. The paper outlines the remaining constraints, power, thermal dissipation, compute radiation hardening, and regulatory models, and demonstrates that these are engineering bottlenecks, not physical limits. Finally, we propose a staged 15-year roadmap from today's fallback D2D systems to autonomous orbital overlays delivering 50-100 Mbps to handhelds in megacities, with zero reliance on terrestrial infrastructure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14188v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sebastian Barros Elgueta</dc:creator>
    </item>
    <item>
      <title>On Splitting Lightweight Semantic Image Segmentation for Wireless Communications</title>
      <link>https://arxiv.org/abs/2507.14199</link>
      <description>arXiv:2507.14199v1 Announce Type: new 
Abstract: Semantic communication represents a promising technique towards reducing communication costs, especially when dealing with image segmentation, but it still lacks a balance between computational efficiency and bandwidth requirements while maintaining high image segmentation accuracy, particularly in resource-limited environments and changing channel conditions. On the other hand, the more complex and larger semantic image segmentation models become, the more stressed the devices are when processing data. This paper proposes a novel approach to implementing semantic communication based on splitting the semantic image segmentation process between a resource constrained transmitter and the receiver. This allows saving bandwidth by reducing the transmitted data while maintaining the accuracy of the semantic image segmentation. Additionally, it reduces the computational requirements at the resource constrained transmitter compared to doing all the semantic image segmentation in the transmitter. The proposed approach is evaluated by means of simulation-based experiments in terms of different metrics such as computational resource usage, required bit rate and segmentation accuracy. The results when comparing the proposal with the full semantic image segmentation in the transmitter show that up to 72% of the bit rate was reduced in the transmission process. In addition, the computational load of the transmitter is reduced by more than 19%. This reflects the interest of this technique for its application in communication systems, particularly in the upcoming 6G systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14199v1</guid>
      <category>cs.NI</category>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ebrahim Abu-Helalah, Jordi Serra, Jordi Perez-Romero</dc:creator>
    </item>
    <item>
      <title>A Fault-Tolerant Architecture for Urban and Rural Digital Connectivity: Synergizing SDWMN, Direct-to-Mobile Broadcasting, and Hybrid Cloud Streaming</title>
      <link>https://arxiv.org/abs/2507.14205</link>
      <description>arXiv:2507.14205v1 Announce Type: new 
Abstract: We propose an integrated architecture combining Software-Defined Wireless Mesh Networks (SDWMN), Direct-to-Mobile (D2M) broadcasting, and Kafka-based hybrid cloud streaming to improve wireless network performance in both urban and rural settings. The approach addresses urban congestion and rural digital exclusion through traffic offloading, enhanced fault tolerance, and equitable resource allocation. We model urban congestion $\rho_u = \lambda_t / \mu_c$ and rural coverage deficit $\delta_r = 1 - C_r / C_{req}$, and aim to minimize global performance loss $GPL = w_1 \cdot \rho_u + w_2 \cdot \delta_r + w_3 \cdot T_{rec}$, where $T_{rec}$ is recovery time. Experiments in Bangkok, Mumbai, and rural Finland demonstrate latency reduction over 32%, bandwidth offloading of 40%, rural coverage gain of 28%, and fairness index rising from 0.78 to 0.91. The system achieves recovery under 10 s using SDWMN and Kafka. We recommend optimal spectrum allocation $\alpha_s$, targeted subsidies, and device mandates to promote adoption. This scalable, fault-tolerant design supports equitable digital transformation and suggests directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14205v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pavel Malinovskiy</dc:creator>
    </item>
    <item>
      <title>White paper: Towards Human-centric and Sustainable 6G Services -- the fortiss Research Perspective</title>
      <link>https://arxiv.org/abs/2507.14209</link>
      <description>arXiv:2507.14209v1 Announce Type: new 
Abstract: As a leading research institute in software-intensive systems, fortiss is actively shaping the vision of Sixth Generation Mobile Communication (6G). Our mission is to ensure that 6G technologies go beyond technical advancements and are aligned with societal needs. fortiss plays a key role in 6G initiatives worldwide, including contributions to standardization bodies and collaborative Research and Development programs. We focus on software-defined, AI-enabled, and sustainable communication services that prioritize human values and long-term impact. 6G will redefine digital connectivity through cognitive intelligence, decentralized orchestration, and sustainability-oriented architectures. As expectations rise for ultra-reliable low-latency communication (URLLC) and personalized digital services, 6G must outperform prior generations. It will rely on AI-native networking, Edge-Cloud resource orchestration, and energy-aware data frameworks, ensuring both technical performance and societal relevance. This white paper presents the fortiss vision for a human-centric, sustainable, and AI-integrated 6G network. It outlines key research domains such as semantic communication, green orchestration, and distributed AI, all linked to societal and technological challenges. The white paper is aimed at researchers, industry experts, policymakers, and developers. It articulates the strategic direction and contributions of fortiss to 6G, emphasizing responsible innovation and interdisciplinary collaboration toward a meaningful 2030 vision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14209v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rute C. Sofia, Hao Shen, Yuanting Liu, Severin Kacianka, Holger Pfeifer</dc:creator>
    </item>
    <item>
      <title>PRATA: A Framework to Enable Predictive QoS in Vehicular Networks via Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2507.14211</link>
      <description>arXiv:2507.14211v1 Announce Type: new 
Abstract: Predictive Quality of Service (PQoS) makes it possible to anticipate QoS changes, e.g., in wireless networks, and trigger appropriate countermeasures to avoid performance degradation. Hence, PQoS is extremely useful for automotive applications such as teleoperated driving, which poses strict constraints in terms of latency and reliability. A promising tool for PQoS is given by Reinforcement Learning (RL), a methodology that enables the design of decision-making strategies for stochastic optimization. In this manuscript, we present PRATA, a new simulation framework to enable PRedictive QoS based on AI for Teleoperated driving Applications. PRATA consists of a modular pipeline that includes (i) an end-to-end protocol stack to simulate the 5G Radio Access Network (RAN), (ii) a tool for generating automotive data, and (iii) an Artificial Intelligence (AI) unit to optimize PQoS decisions. To prove its utility, we use PRATA to design an RL unit, named RAN-AI, to optimize the segmentation level of teleoperated driving data in the event of resource saturation or channel degradation. Hence, we show that the RAN-AI entity efficiently balances the trade-off between QoS and Quality of Experience (QoE) that characterize teleoperated driving applications, almost doubling the system performance compared to baseline approaches. In addition, by varying the learning settings of the RAN-AI entity, we investigate the impact of the state space and the relative cost of acquiring network data that are necessary for the implementation of RL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14211v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Federico Mason, Tommaso Zugno, Matteo Drago, Marco Giordani, Mate Boban, Michele Zorzi</dc:creator>
    </item>
    <item>
      <title>Intent-Based Network for RAN Management with Large Language Models</title>
      <link>https://arxiv.org/abs/2507.14230</link>
      <description>arXiv:2507.14230v1 Announce Type: new 
Abstract: Advanced intelligent automation becomes an important feature to deal with the increased complexity in managing wireless networks. This paper proposes a novel automation approach of intent-based network for Radio Access Networks (RANs) management by leveraging Large Language Models (LLMs). The proposed method enhances intent translation, autonomously interpreting high-level objectives, reasoning over complex network states, and generating precise configurations of the RAN by integrating LLMs within an agentic architecture. We propose a structured prompt engineering technique and demonstrate that the network can automatically improve its energy efficiency by dynamically optimizing critical RAN parameters through a closed-loop mechanism. It showcases the potential to enable robust resource management in RAN by adapting strategies based on real-time feedback via LLM-orchestrated agentic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14230v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fransiscus Asisi Bimo, Maria Amparo Canaveras Galdon, Chun-Kai Lai, Ray-Guang Cheng, Edwin K. P. Chong</dc:creator>
    </item>
    <item>
      <title>Feasibility of Energy Neutral Wildlife Tracking using Multi-Source Energy Harvesting</title>
      <link>https://arxiv.org/abs/2507.14234</link>
      <description>arXiv:2507.14234v1 Announce Type: new 
Abstract: Long-term wildlife tracking is crucial for biodiversity monitoring, but energy limitations pose challenges, especially for animal tags, where replacing batteries is impractical and stressful for the animal due to the need to locate, possibly sedate, and handle it. Energy harvesting offers a sustainable alternative, yet most existing systems rely on a single energy source and infrastructure-limited communication technologies. This paper presents an energy-neutral system that combines solar and kinetic energy harvesting to enable the tracking and monitoring of wild animals. Harvesting from multiple sources increases the total available energy. Uniquely, the kinetic harvester also serves as a motion proxy by sampling harvested current, enabling activity monitoring without dedicated sensors. Our approach also ensures compatibility with existing cellular infrastructure, using Narrowband Internet of Things (NB-IoT). We present a simulation framework that models energy harvesting, storage, and consumption at the component level. An energy-aware scheduler coordinates task execution based on real-time energy availability. We evaluate performance under realistically varying conditions, comparing task frequencies and capacitor sizes. Results show that our approach maintains energy-neutral operation while significantly increasing data yield and reliability compared to single-source systems, with the ability to consistently sample GPS location data and kinetic harvesting data every two minutes while transmitting these results over NB-IoT every hour. These findings demonstrate the potential for maintenance-free, environmentally friendly tracking in remote habitats, enabling more effective and scalable wildlife monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14234v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samer Nasser, Henrique Duarte Moura, Dragan Subotic, Ritesh Kumar Singh, Maarten Weyn, Jeroen Famaey</dc:creator>
    </item>
    <item>
      <title>Beyond DNS: Unlocking the Internet of AI Agents via the NANDA Index and Verified AgentFacts</title>
      <link>https://arxiv.org/abs/2507.14263</link>
      <description>arXiv:2507.14263v1 Announce Type: new 
Abstract: The Internet is poised to host billions to trillions of autonomous AI agents that negotiate, delegate, and migrate in milliseconds and workloads that will strain DNS-centred identity and discovery. In this paper, we describe the NANDA index architecture, which we envision as a means for discoverability, identifiability and authentication in the internet of AI agents. We present an architecture where a minimal lean index resolves to dynamic, cryptographically verifiable AgentFacts that supports multi-endpoint routing, load balancing, privacy-preserving access, and credentialed capability assertions. Our architecture design delivers five concrete guarantees: (1) A quilt-like index proposal that supports both NANDA-native agents as well as third party agents being discoverable via the index, (2) rapid global resolution for newly spawned AI agents, (3) sub-second revocation and key rotation, (4) schema-validated capability assertions, and (5) privacy-preserving discovery across organisational boundaries via verifiable, least-disclosure queries. We formalize the AgentFacts schema, specify a CRDT-based update protocol, and prototype adaptive resolvers. The result is a lightweight, horizontally scalable foundation that unlocks secure, trust-aware collaboration for the next generation of the Internet of AI agents, without abandoning existing web infrastructure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14263v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.MA</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ramesh Raskar, Pradyumna Chari, John Zinky, Mahesh Lambe, Jared James Grogan, Sichao Wang, Rajesh Ranjan, Rekha Singhal, Shailja Gupta, Robert Lincourt, Raghu Bala, Aditi Joshi, Abhishek Singh, Ayush Chopra, Dimitris Stripelis, Bhuwan B, Sumit Kumar, Maria Gorskikh</dc:creator>
    </item>
    <item>
      <title>NetIntent: Leveraging Large Language Models for End-to-End Intent-Based SDN Automation</title>
      <link>https://arxiv.org/abs/2507.14398</link>
      <description>arXiv:2507.14398v1 Announce Type: new 
Abstract: Intent-Based Networking (IBN) often leverages the programmability of Software-Defined Networking (SDN) to simplify network management. However, significant challenges remain in automating the entire pipeline, from user-specified high-level intents to device-specific low-level configurations. Existing solutions often rely on rigid, rule-based translators and fixed APIs, limiting extensibility and adaptability. By contrast, recent advances in large language models (LLMs) offer a promising pathway that leverages natural language understanding and flexible reasoning. However, it is unclear to what extent LLMs can perform IBN tasks. To address this, we introduce IBNBench, a first-of-its-kind benchmarking suite comprising four novel datasets: Intent2Flow-ODL, Intent2Flow-ONOS, FlowConflict-ODL, and FlowConflict-ONOS. These datasets are specifically designed for evaluating LLMs performance in intent translation and conflict detection tasks within the industry-grade SDN controllers ODL and ONOS. Our results provide the first comprehensive comparison of 33 open-source LLMs on IBNBench and related datasets, revealing a wide range of performance outcomes. However, while these results demonstrate the potential of LLMs for isolated IBN tasks, integrating LLMs into a fully autonomous IBN pipeline remains unexplored. Thus, our second contribution is NetIntent, a unified and adaptable framework that leverages LLMs to automate the full IBN lifecycle, including translation, activation, and assurance within SDN systems. NetIntent orchestrates both LLM and non-LLM agents, supporting dynamic re-prompting and contextual feedback to robustly execute user-defined intents with minimal human intervention. Our implementation of NetIntent across both ODL and ONOS SDN controllers achieves a consistent and adaptive end-to-end IBN realization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14398v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>IEEE Open Journal of the Communications Society, 2025</arxiv:journal_reference>
      <dc:creator>Md. Kamrul Hossain, Walid Aljoby</dc:creator>
    </item>
    <item>
      <title>Dora: A Controller Provisioning Strategy in Hierarchical Domain-based Satellite Networks</title>
      <link>https://arxiv.org/abs/2507.14512</link>
      <description>arXiv:2507.14512v1 Announce Type: new 
Abstract: The rapid proliferation of satellite constellations in Space-Air-Ground Integrated Networks (SAGIN) presents significant challenges for network management. Conventional flat network architectures struggle with synchronization and data transmission across massive distributed nodes. In response, hierarchical domain-based satellite network architectures have emerged as a scalable solution, highlighting the critical importance of controller provisioning strategies. However, existing network management architectures and traditional search-based algorithms fail to generate efficient controller provisioning solutions due to limited computational resources in satellites and strict time constraints. To address these challenges, we propose a three-layer domain-based architecture that enhances both scalability and adaptability. Furthermore, we introduce Dora, a reinforcement learning-based controller provisioning strategy designed to optimize network performance while minimizing computational overhead. Our comprehensive experimental evaluation demonstrates that Dora significantly outperforms state-of-the-art benchmarks, achieving 10% improvement in controller provisioning quality while requiring only 1/30 to 1/90 of the computation time compared to traditional algorithms. These results underscore the potential of reinforcement learning approaches for efficient satellite network management in next-generation SAGIN deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14512v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiyuan Peng, Qi Zhang, Yue Gao, Kun Qiu</dc:creator>
    </item>
    <item>
      <title>UAV-Enabled Wireless-Powered Underground Communication Networks: A Novel Time Allocation Approach</title>
      <link>https://arxiv.org/abs/2507.14627</link>
      <description>arXiv:2507.14627v1 Announce Type: new 
Abstract: Wireless-powered underground communication networks (WPUCNs), which allow underground devices (UDs) to harvest energy from wireless signals for battery-free communication, offer a promising solution for sustainable underground monitoring. However, the severe wireless signal attenuation in challenging underground environments and the costly acquisition of channel state information (CSI) make large-scale WPUCNs economically infeasible in practice. To address this challenge, we introduce flexible unmanned aerial vehicles (UAVs) into WPUCNs, leading to UAV-enabled WPUCN systems. In this system, a UAV is first charged by a terrestrial hybrid access point (HAP), then flies to the monitoring area to wirelessly charge UDs. Afterwards, the UAV collects data from the UDs and finally returns to the HAP for data offloading. Based on the proposed UAV-enabled WPUCN system, we first propose its energy consumption model and a hybrid wireless energy transfer (WET) approach (i.e., UDs can harvest energy from both the HAP and the UAV) relying on full-CSI and CSI-free multi-antenna beamforming. Then, we formulate and address a time allocation problem to minimize the energy consumption of UAV, while ensuring that the throughput requirements of all UDs are met and all sensor data is offloaded. Through simulations of a realistic farming scenario, we demonstrate that the proposed hybrid WET approach outperforms other WET approaches, with performance gains influenced by the number of antennas, communication distance, number of UDs, and underground conditions. Additionally, under the optimized time allocation, we found that the proposed hybrid WET approach based on a CSI-free multi-antenna scheme achieves the lowest UAV's energy consumption among all WET mechanisms, thereby enabling sustainable underground monitoring in WPUCNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14627v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaiqiang Lin, Yijie Mao, Onel Luis Alcaraz L\'opez, Mohamed-Slim Alouini</dc:creator>
    </item>
    <item>
      <title>Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks: A Survey on Generative Approaches</title>
      <link>https://arxiv.org/abs/2507.14633</link>
      <description>arXiv:2507.14633v1 Announce Type: new 
Abstract: The development of satellite-augmented low-altitude economy and terrestrial networks (SLAETNs) demands intelligent and autonomous systems that can operate reliably across heterogeneous, dynamic, and mission-critical environments. To address these challenges, this survey focuses on enabling agentic artificial intelligence (AI), that is, artificial agents capable of perceiving, reasoning, and acting, through generative AI (GAI) and large language models (LLMs). We begin by introducing the architecture and characteristics of SLAETNs, and analyzing the challenges that arise in integrating satellite, aerial, and terrestrial components. Then, we present a model-driven foundation by systematically reviewing five major categories of generative models: variational autoencoders (VAEs), generative adversarial networks (GANs), generative diffusion models (GDMs), transformer-based models (TBMs), and LLMs. Moreover, we provide a comparative analysis to highlight their generative mechanisms, capabilities, and deployment trade-offs within SLAETNs. Building on this foundation, we examine how these models empower agentic functions across three domains: communication enhancement, security and privacy protection, and intelligent satellite tasks. Finally, we outline key future directions for building scalable, adaptive, and trustworthy generative agents in SLAETNs. This survey aims to provide a unified understanding and actionable reference for advancing agentic AI in next-generation integrated networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14633v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaozheng Gao, Yichen Wang, Bosen Liu, Xiao Zhou, Ruichen Zhang, Jiacheng Wang, Dusit Niyato, Dong In Kim, Abbas Jamalipour, Chau Yuen, Jianping An, Kai Yang</dc:creator>
    </item>
    <item>
      <title>Data-Plane Telemetry to Mitigate Long-Distance BGP Hijacks</title>
      <link>https://arxiv.org/abs/2507.14842</link>
      <description>arXiv:2507.14842v1 Announce Type: new 
Abstract: Poor security of Internet routing enables adversaries to divert user data through unintended infrastructures (hijack). Of particular concern -- and the focus of this paper -- are cases where attackers reroute domestic traffic through foreign countries, exposing it to surveillance, bypassing legal privacy protections, and posing national security threats. Efforts to detect and mitigate such attacks have focused primarily on the control plane while data-plane signals remain largely overlooked. In particular, change in propagation delay caused by rerouting offers a promising signal: the change is unavoidable and the increased propagation delay is directly observable from the affected networks. In this paper, we explore the practicality of using delay variations for hijack detection, addressing two key questions: (1) What coverage can this provide, given its heavy dependence on the geolocations of the sender, receiver, and adversary? and (2) Can an always-on latency-based detection system be deployed without disrupting normal network operations? We observe that for 86% of victim-attacker country pairs in the world, mid-attack delays exceed pre-attack delays by at least 25% in real deployments, making delay-based hijack detection promising. To demonstrate practicality, we design HiDe, which reliably detects delay surges from long-distance hijacks at line rate. We measure HiDe's accuracy and false-positive rate on real-world data and validate it with ethically conducted hijacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14842v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Satadal Sengupta, Hyojoon Kim, Daniel Jubas, Maria Apostolaki, Jennifer Rexford</dc:creator>
    </item>
    <item>
      <title>Tidal-Like Concept Drift in RIS-Covered Buildings: When Programmable Wireless Environments Meet Human Behaviors</title>
      <link>https://arxiv.org/abs/2507.14876</link>
      <description>arXiv:2507.14876v1 Announce Type: new 
Abstract: Indoor mobile networks handle the majority of data traffic, with their performance limited by building materials and structures. However, building designs have historically not prioritized wireless performance. Prior to the advent of reconfigurable intelligent surfaces (RIS), the industry passively adapted to wireless propagation challenges within buildings. Inspired by RIS's successes in outdoor networks, we propose embedding RIS into building structures to manipulate and enhance building wireless performance comprehensively. Nonetheless, the ubiquitous mobility of users introduces complex dynamics to the channels of RIS-covered buildings. A deep understanding of indoor human behavior patterns is essential for achieving wireless-friendly building design. This article is the first to systematically examine the tidal evolution phenomena emerging in the channels of RIS-covered buildings driven by complex human behaviors. We demonstrate that a universal channel model is unattainable and focus on analyzing the challenges faced by advanced deep learning-based prediction and control strategies, including high-order Markov dependencies, concept drift, and generalization issues caused by human-induced disturbances. Possible solutions for orchestrating the coexistence of RIS-covered buildings and crowd mobility are also laid out.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14876v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zi-Yang Wu, Muhammad Ismail, Jiliang Zhang, Jie Zhang</dc:creator>
    </item>
    <item>
      <title>FENIX: Enabling In-Network DNN Inference with FPGA-Enhanced Programmable Switches</title>
      <link>https://arxiv.org/abs/2507.14891</link>
      <description>arXiv:2507.14891v1 Announce Type: new 
Abstract: Machine learning (ML) is increasingly used in network data planes for advanced traffic analysis. However, existing solutions (such as FlowLens, N3IC, and BoS) still struggle to simultaneously achieve low latency, high throughput, and high accuracy. To address these challenges, we present FENIX, a hybrid in-network ML system that performs feature extraction on programmable switch ASICs and deep neural network inference on FPGAs. FENIX introduces a Data Engine that leverages a probabilistic token bucket algorithm to control the sending rate of feature streams, effectively addressing the throughput gap between programmable switch ASICs and FPGAs. In addition, FENIX designs a Model Engine to enable high-accuracy deep neural network inference in the network, overcoming the difficulty of deploying complex models on resource-constrained switch chips. We implement FENIX on a programmable switch platform that integrates a Tofino ASIC and a ZU19EG FPGA directly and evaluate it on real-world network traffic datasets. Our results show that FENIX achieves microsecond-level inference latency and multi-terabit throughput with low hardware overhead, and delivers over 95\% accuracy on mainstream network traffic classification tasks, outperforming SOTA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14891v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiangyu Gao (Tsinghua University), Tong Li (Renmin University of China), Yinchao Zhang (Tsinghua University), Ziqiang Wang (Southeast University), Xiangsheng Zeng (Huazhong University of Science and Technology), Su Yao (Tsinghua University), Ke Xu (Tsinghua University)</dc:creator>
    </item>
    <item>
      <title>Quantum Machine Learning for Secure Cooperative Multi-Layer Edge AI with Proportional Fairness</title>
      <link>https://arxiv.org/abs/2507.15145</link>
      <description>arXiv:2507.15145v1 Announce Type: new 
Abstract: This paper proposes a communication-efficient, event-triggered inference framework for cooperative edge AI systems comprising multiple user devices and edge servers. Building upon dual-threshold early-exit strategies for rare-event detection, the proposed approach extends classical single-device inference to a distributed, multi-device setting while incorporating proportional fairness constraints across users. A joint optimization framework is formulated to maximize classification utility under communication, energy, and fairness constraints. To solve the resulting problem efficiently, we exploit the monotonicity of the utility function with respect to the confidence thresholds and apply alternating optimization with Benders decomposition. Experimental results show that the proposed framework significantly enhances system-wide performance and fairness in resource allocation compared to single-device baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15145v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thai T. Vu, John Le</dc:creator>
    </item>
    <item>
      <title>User Head Movement-Predictive XR in Immersive H2M Collaborations over Future Enterprise Networks</title>
      <link>https://arxiv.org/abs/2507.15254</link>
      <description>arXiv:2507.15254v1 Announce Type: new 
Abstract: The evolution towards future generation of mobile systems and fixed wireless networks is primarily driven by the urgency to support high-bandwidth and low-latency services across various vertical sectors. This endeavor is fueled by smartphones as well as technologies like industrial internet of things, extended reality (XR), and human-to-machine (H2M) collaborations for fostering industrial and social revolutions like Industry 4.0/5.0 and Society 5.0. To ensure an ideal immersive experience and avoid cyber-sickness for users in all the aforementioned usage scenarios, it is typically challenging to synchronize XR content from a remote machine to a human collaborator according to their head movements across a large geographic span in real-time over communication networks. Thus, we propose a novel H2M collaboration scheme where the human's head movements are predicted ahead with highly accurate models like bidirectional long short-term memory networks to orient the machine's camera in advance. We validate that XR frame size varies in accordance with the human's head movements and predict the corresponding bandwidth requirements from the machine's camera to propose a human-machine coordinated dynamic bandwidth allocation (HMC-DBA) scheme. Through extensive simulations, we show that end-to-end latency and jitter requirements of XR frames are satisfied with much lower bandwidth consumption over enterprise networks like Fiber-To-The-Room-Business. Furthermore, we show that better efficiency in network resource utilization is achieved by employing our proposed HMC-DBA over state-of-the-art schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15254v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/JIOT.2025.3587064</arxiv:DOI>
      <dc:creator>Sourav Mondal, Elaine Wong</dc:creator>
    </item>
    <item>
      <title>Low-Power and Accurate IoT Monitoring Under Radio Resource Constraint</title>
      <link>https://arxiv.org/abs/2507.15338</link>
      <description>arXiv:2507.15338v1 Announce Type: new 
Abstract: This paper investigates how to achieve both low-power operations of sensor nodes and accurate state estimation using Kalman filter for internet of things (IoT) monitoring employing wireless sensor networks under radio resource constraint. We consider two policies used by the base station to collect observations from the sensor nodes: (i) an oblivious policy, based on statistics of the observations, and (ii) a decentralized policy, based on autonomous decision of each sensor based on its instantaneous observation. This work introduces a wake-up receiver and wake-up signaling to both policies to improve the energy efficiency of the sensor nodes. The decentralized policy designed with random access prioritizes transmissions of instantaneous observations that are highly likely to contribute to the improvement of state estimation. Our numerical results show that the decentralized policy improves the accuracy of the estimation in comparison to the oblivious policy under the constraint on the radio resource and consumed energy when the correlation between the processes observed by the sensor nodes is low. We also clarify the degree of correlation in which the superiority of two policies changes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15338v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takaho Shimokasa, Hiroyuki Yomo, Federico Chiariotti, Junya Shiraishi, Petar Popovski</dc:creator>
    </item>
    <item>
      <title>Enhancements to P4TG: Histogram-Based RTT Monitoring in the Data Plane</title>
      <link>https://arxiv.org/abs/2507.15382</link>
      <description>arXiv:2507.15382v1 Announce Type: new 
Abstract: Modern traffic generators are essential tools for evaluating the performance of network environments. P4TG is a P4-based traffic generator implemented for Intel Tofino switches that offers high-speed packet generation with fine-grained measurement capabilities. However, P4TG samples time-based metrics such as the round-trip time (RTT) in the data plane and collects them at the controller. This leads to a reduced accuracy. In this paper, we introduce a histogram-based RTT measurement feature for P4TG. It enables accurate analysis at line rate without sampling. Generally, histogram bins are modeled as ranges, and values are matched to a bin. Efficient packet matching in hardware is typically achieved using ternary content addressable memory (TCAM). However, representing range matching rules in TCAM poses a challenge. Therefore, we implemented a range-to-prefix conversion algorithm that models range matching with multiple ternary entries. This paper describes the data plane implementation and runtime configuration of RTT histograms in P4TG. Further, we discuss the efficiency of the ternary decomposition. Our evaluation demonstrates the applicability of the histogram-based RTT analysis by comparing the measured values with a configured theoretical distribution of RTTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15382v1</guid>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabian Ihle, Etienne Zink, Michael Menth</dc:creator>
    </item>
    <item>
      <title>Stack Management for MPLS Network Actions: Integration of Nodes with Limited Hardware Capabilities</title>
      <link>https://arxiv.org/abs/2507.15391</link>
      <description>arXiv:2507.15391v1 Announce Type: new 
Abstract: The MPLS Network Actions (MNA) framework enhances MPLS forwarding with a generalized encoding for manifold extensions such as network slicing and in-situ OAM (IOAM). Network actions in MNA are encoded in Label Stack Entries (LSEs) and are added to the MPLS stack. Routers have a physical limit on the number of LSEs they can read, called the readable label depth (RLD). With MNA, routers must be able to process a minimum number of LSEs which requires a relatively large RLD. In this paper, we perform a hardware analysis of an MNA implementation and identify the reason for a large RLD requirement in the MNA protocol design. Based on this, we present a mechanism that reduces the required RLD for MNA nodes by restructuring the MPLS stack during forwarding. We then introduce the novel stack management network action that enables the proposed mechanism as well as its integration in networks with MNA-incapable nodes. The feasibility of the mechanism on programmable hardware is verified by providing a P4-based implementation. Further, the effects on the required RLD, ECMP, and packet overhead are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15391v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3744200.3744779</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 2025 Applied Networking Research Workshop</arxiv:journal_reference>
      <dc:creator>Fabian Ihle, Michael Menth</dc:creator>
    </item>
    <item>
      <title>Assessing the Benefits of Ground Vehicles as Moving Urban Base Stations</title>
      <link>https://arxiv.org/abs/2507.15423</link>
      <description>arXiv:2507.15423v1 Announce Type: new 
Abstract: In the evolution towards 6G user-centric networking, the moving network (MN) paradigm can play an important role. In a MN, some small cell base stations (BS) are installed on top of vehicles, and enable a more dynamic, flexible and sustainable, network operation. By "following" the users movements and adapting dynamically to their requests, the MN paradigm enables a more efficient utilization of network resources, mitigating the need for dense small cell BS deployments at the cost of an increase in resource utilization due to wireless backhauling. This aspect is at least partly compensated by the shorter distance between users and BS, which allows for lower power and Line-of-Sight communications. While the MN paradigm has been investigated for some time, to date, it is still unclear in which conditions the advantages of MN outweigh the additional resource costs. In this paper, we propose a stochastic geometry framework for the characterization of the potential benefits of the MN paradigm as part of an HetNet in urban settings. Our approach allows the estimation of user-perceived performance, accounting for wireless backhaul connectivity as well as base station resource scheduling. We formulate an optimization problem for determining the resource-optimal network configurations and BS scheduling which minimize the overall amount of deployed BSs in a QoS-aware manner, and the minimum vehicular flow between different urban districts required to support them, and we propose an efficient stochastic heuristic to solve it. Our numerical assessment suggests that the MN paradigm, coupled with appropriate dynamic network management strategies, significantly reduces the amount of deployed network infrastructure while guaranteeing the target QoS perceived by users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15423v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Laura Finarelli, Falko Dressler, Marco Ajmone Marsan, Gianluca Rizzo</dc:creator>
    </item>
    <item>
      <title>SENSOR: A Cost-Efficient Open-Source Flow Monitoring Platform</title>
      <link>https://arxiv.org/abs/2507.15659</link>
      <description>arXiv:2507.15659v1 Announce Type: new 
Abstract: This paper presents a cost-effective and distributed flow monitoring platform for collecting unsampled IPFIX data exclusively using open-source tools, which is implemented at the University of T\"ubingen. An overview of all tools is given and their use is explained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15659v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel Paradzik, Benjamin Steinert, Heinrich Abele, Michael Menth</dc:creator>
    </item>
    <item>
      <title>Vehicular Cloud Computing: A cost-effective alternative to Edge Computing in 5G networks</title>
      <link>https://arxiv.org/abs/2507.15670</link>
      <description>arXiv:2507.15670v1 Announce Type: new 
Abstract: Edge Computing (EC) is a computational paradigm that involves deploying resources such as CPUs and GPUs near end-users, enabling low-latency applications like augmented reality and real-time gaming. However, deploying and maintaining a vast network of EC nodes is costly, which can explain its limited deployment today. A new paradigm called Vehicular Cloud Computing (VCC) has emerged and inspired interest among researchers and industry. VCC opportunistically utilizes existing and idle vehicular computational resources for external task offloading. This work is the first to systematically address the following question: Can VCC replace EC for low-latency applications? Answering this question is highly relevant for Network Operators (NOs), as VCC could eliminate costs associated with EC given that it requires no infrastructural investment. Despite its potential, no systematic study has yet explored the conditions under which VCC can effectively support low-latency applications without relying on EC. This work aims to fill that gap. Extensive simulations allow for assessing the crucial scenario factors that determine when this EC-to-VCC substitution is feasible. Considered factors are load, vehicles mobility and density, and availability. Potential for substitution is assessed based on multiple criteria, such as latency, task completion success, and cost. Vehicle mobility is simulated in SUMO, and communication in NS3 5G-LENA. The findings show that VCC can effectively replace EC for low-latency applications, except in extreme cases when the EC is still required (latency &lt; 16 ms).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15670v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.comnet.2025.111365</arxiv:DOI>
      <arxiv:journal_reference>Computer Network, Elsevier, 2025 August</arxiv:journal_reference>
      <dc:creator>Rosario Patan\`e, Nadjib Achir, Andrea Araldo, Lila Boukhatem</dc:creator>
    </item>
    <item>
      <title>Point Cloud Streaming with Latency-Driven Implicit Adaptation using MoQ</title>
      <link>https://arxiv.org/abs/2507.15673</link>
      <description>arXiv:2507.15673v1 Announce Type: cross 
Abstract: Point clouds are a promising video representation for next-generation multimedia experiences in virtual and augmented reality. Point clouds are notoriously high-bitrate, however, which limits the feasibility of live streaming systems. Prior methods have adopted traditional HTTP-based protocols for point cloud streaming, but they rely on explicit client-side adaptation to maintain low latency under congestion. In this work, we leverage the delivery timeout feature within the Media Over QUIC protocol to perform implicit server-side adaptation based on an application's latency target. Through experimentation with several publisher and network configurations, we demonstrate that our system unlocks a unique trade-off on a per-client basis: applications with lower latency requirements will receive lower-quality video, while applications with more relaxed latency requirements will receive higher-quality video.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15673v1</guid>
      <category>cs.MM</category>
      <category>cs.NI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Freeman, Michael Rudolph, Amr Rizk</dc:creator>
    </item>
    <item>
      <title>Federated Split Learning with Improved Communication and Storage Efficiency</title>
      <link>https://arxiv.org/abs/2507.15816</link>
      <description>arXiv:2507.15816v1 Announce Type: cross 
Abstract: Federated learning (FL) is one of the popular distributed machine learning (ML) solutions but incurs significant communication and computation costs at edge devices. Federated split learning (FSL) can train sub-models in parallel and reduce the computational burden of edge devices by splitting the model architecture. However, it still requires a high communication overhead due to transmitting the smashed data and gradients between clients and the server in every global round. Furthermore, the server must maintain separate partial models for every client, leading to a significant storage requirement. To address these challenges, this paper proposes a novel communication and storage efficient federated split learning method, termed CSE-FSL, which utilizes an auxiliary network to locally update the weights of the clients while keeping a single model at the server, hence avoiding frequent transmissions of gradients from the server and greatly reducing the storage requirement of the server. Additionally, a new model update method of transmitting the smashed data in selected epochs can reduce the amount of smashed data sent from the clients. We provide a theoretical analysis of CSE-FSL, rigorously guaranteeing its convergence under non-convex loss functions. The extensive experimental results further indicate that CSE-FSL achieves a significant communication reduction over existing FSL solutions using real-world FL tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15816v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujia Mu, Cong Shen</dc:creator>
    </item>
    <item>
      <title>The Capacity of Semantic Private Information Retrieval with Colluding Servers</title>
      <link>https://arxiv.org/abs/2507.15818</link>
      <description>arXiv:2507.15818v1 Announce Type: cross 
Abstract: We study the problem of semantic private information retrieval (Sem-PIR) with $T$ colluding servers (Sem-TPIR), i.e., servers that collectively share user queries. In Sem-TPIR, the message sizes are different, and message retrieval probabilities by any user are not uniform. This is a generalization of the classical PIR problem where the message sizes are equal and message retrieval probabilities are identical. The earlier work on Sem-PIR considered the case of no collusions, i.e., the collusion parameter of $T=1$. In this paper, we consider the general problem for arbitrary $T &lt; N$. We find an upper bound on the retrieval rate and design a scheme that achieves this rate, i.e., we derive the exact capacity of Sem-TPIR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15818v1</guid>
      <category>cs.IT</category>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohamed Nomeir, Alptug Aytekin, Sennur Ulukus</dc:creator>
    </item>
    <item>
      <title>Binary VPN Traffic Detection Using Wavelet Features and Machine Learning</title>
      <link>https://arxiv.org/abs/2502.13804</link>
      <description>arXiv:2502.13804v2 Announce Type: replace 
Abstract: Encrypted traffic classification faces growing challenges as encryption renders traditional deep packet inspection ineffective. This study addresses binary VPN detection, distinguishing VPN-encrypted from non-VPN traffic using wavelet transform-based features across multiple machine learning models. Unlike previous studies focused on application-level classification within encrypted traffic, we specifically evaluate the fundamental task of VPN identification regardless of application type. We analyze the impact of wavelet decomposition levels and dataset filtering on classification performance across significantly imbalanced data, where filtering reduces some traffic categories by up to 95%. Our results demonstrate that Random Forest (RF) achieves superior performance with an F1-score of 99%, maintaining robust accuracy even after significant dataset filtering. Neural Networks (NN) show comparable effectiveness with an F1-score of 98% when trained on wavelet level 12, while Support Vector Machines (SVM) exhibit notable sensitivity to dataset reduction, with F1-scores dropping from 90% to 85% after filtering. Comparing wavelet decomposition at levels 5 and 12, we observe improved classification performance at level 12, particularly for variable traffic types, though the marginal gains may not justify the additional computational overhead. These findings establish RF as the most reliable model for VPN traffic classification while highlighting key performance tradeoffs in feature extraction and preprocessing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13804v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yasameen Sajid Razooqi, Adrian Pekar</dc:creator>
    </item>
    <item>
      <title>Age of Information in Unreliable Tandem Queues</title>
      <link>https://arxiv.org/abs/2506.09245</link>
      <description>arXiv:2506.09245v2 Announce Type: replace 
Abstract: Stringent demands for timely information delivery, driven by the widespread adoption of real-time applications and the Internet of Things, have established the age of information (AoI) as a critical metric for quantifying data freshness. Existing AoI models often assume multi-hop communication networks with fully reliable nodes, which may not accurately capture scenarios involving node transmission failures. This paper presents an analytical framework for two configurations of tandem queue systems, where status updates generated by a single sensor are relayed to a destination monitor through unreliable intermediate nodes. Using the probability generating function, we first derive the sojourn time distribution for an infinite-buffer M/M/1 tandem system with two unreliable nodes. We then extend our analysis to an M/G/1 tandem system with an arbitrary number of unreliable nodes, employing the supplementary variable technique while assuming that only the first node has an infinite buffer. Numerical results demonstrate the impact of key system parameters on the average AoI in unreliable tandem queues with Markovian and non-Markovian service times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09245v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muthukrishnan Senthilkumar, Aresh Dadlani, Hina Tabassum</dc:creator>
    </item>
    <item>
      <title>Model-Based Diagnosis: Automating End-to-End Diagnosis of Network Failures</title>
      <link>https://arxiv.org/abs/2506.23083</link>
      <description>arXiv:2506.23083v2 Announce Type: replace 
Abstract: Fast diagnosis and repair of enterprise network failures is critically important since disruptions cause major business impacts. Prior works focused on diagnosis primitives or procedures limited to a subset of the problem, such as only data plane or only control plane faults. This paper proposes a new paradigm, model-based network diagnosis, that provides a systematic way to derive automated procedures for identifying the root cause of network failures, based on reports of end-to-end user-level symptoms. The diagnosis procedures are systematically derived from a model of packet forwarding and routing, covering hardware, firmware, and software faults in both the data plane and distributed control plane. These automated procedures replace and dramatically accelerate diagnosis by an experienced human operator. Model-based diagnosis is inspired by, leverages, and is complementary to recent work on network verification. We have built NetDx, a proof-of-concept implementation of model-based network diagnosis. We deployed NetDx on a new emulator of networks consisting of P4 switches with distributed routing software. We validated the robustness and coverage of NetDx with an automated fault injection campaign, in which 100% of faults were diagnosed correctly. Furthermore, on a data set of 33 faults from a large cloud provider that are within the domain targeted by NetDx, 30 are efficiently diagnosed in seconds instead of hours.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23083v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Changrong Wu, Yiyao Yu, Myungjin Lee, Jayanth Srinivasa, Ennan Zhai, George Varghese, Yuval Tamir</dc:creator>
    </item>
    <item>
      <title>5G Traffic Prediction with Time Series Analysis</title>
      <link>https://arxiv.org/abs/2110.03781</link>
      <description>arXiv:2110.03781v2 Announce Type: replace-cross 
Abstract: In today's day and age, a mobile phone has become a basic requirement needed for anyone to thrive. With the cellular traffic demand increasing so dramatically, it is now necessary to accurately predict the user traffic in cellular networks, so as to improve the performance in terms of resource allocation and utilisation. Since traffic learning and prediction is a classical and appealing field, which still yields many meaningful results, there has been an increasing interest in leveraging Machine Learning tools to analyse the total traffic served in a given region, to optimise the operation of the network. With the help of this project, we seek to exploit the traffic history by using it to predict the nature and occurrence of future traffic. Furthermore, we classify the traffic into particular application types, to increase our understanding of the nature of the traffic. By leveraging the power of machine learning and identifying its usefulness in the field of cellular networks we try to achieve three main objectives - classification of the application generating the traffic, prediction of packet arrival intensity and burst occurrence. The design of the prediction and classification system is done using Long Short Term Memory (LSTM) model. The LSTM predictor developed in this experiment would return the number of uplink packets and also estimate the probability of burst occurrence in the specified future time interval. For the purpose of classification, the regression layer in our LSTM prediction model is replaced by a softmax classifier which is used to classify the application generating the cellular traffic into one of the four applications including surfing, video calling, voice calling, and video streaming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.03781v2</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikhil Nayak, Rujula Singh R, Rameshwar Garg, Varun Danda, Chandana Kiran, Kaustuv Saha</dc:creator>
    </item>
  </channel>
</rss>
