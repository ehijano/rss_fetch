<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Dec 2025 03:45:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Minimax spectral estimation of weighted Laplace operators</title>
      <link>https://arxiv.org/abs/2511.22694</link>
      <description>arXiv:2511.22694v1 Announce Type: new 
Abstract: Given $n$ i.i.d. observations, we study the problem of estimating the spectrum of weighted Laplace operators of the form $\Delta_f=\Delta + \alpha \nabla \log f\cdot \nabla$, where $f$ is a positive probability density on a known compact $d$-dimensional manifold without boundary and $\alpha\in \mathbb{R}$ is a hyperparameter. These operators arise as continuum limits of graph Laplacian matrices and provide valuable geometric information on the underlying data distribution. We establish the exact minimax rates of estimation for this problem, by exhibiting two different rates of convergence for eigenfunctions and eigenvalues. When $f$ belongs to a H\"older-Zygmund class $\mathscr{C}^s$ of regularity $s\geqslant 2$, the eigenfunctions can be estimated with respect to the $\mathrm{L}^q$-norm ($q\geqslant 1$) via plug-in methods at the minimax rate $n^{-\frac{s+1}{2s+d}}$ for $d\geqslant 3$ (with different rates for $d\leqslant 2$). Moreover, eigenvalues can be estimated at the minimax rate $n^{-\frac{4s}{4s+d}}+n^{-\frac 12}$. In the regime $s&gt;\frac d4$, we further show that asymptotically efficient estimators exist.
  We also present a general framework for estimating nonlinear functionals over H\"older-Zygmund spaces, with potential applications to a broad class of statistical problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22694v1</guid>
      <category>math.ST</category>
      <category>math.SP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yann Chaubet, Vincent Divol</dc:creator>
    </item>
    <item>
      <title>What the Jeffreys-Lindley Paradox Really Is: Correcting a Persistent Misconception</title>
      <link>https://arxiv.org/abs/2511.22816</link>
      <description>arXiv:2511.22816v1 Announce Type: new 
Abstract: The Jeffreys-Lindley paradox stands as the most profound divergence between frequentist and Bayesian approaches to hypothesis testing. Yet despite more than six decades of discussion, this paradox remains frequently misunderstood--even in the pages of leading statistical journals. In a 1993 paper published in Statistica Sinica, Robert characterized the Jeffreys-Lindley paradox as "the fact that a point null hypothesis will always be accepted when the variance of a conjugate prior goes to infinity." This characterization, however, describes a different phenomenon entirely-what we term Bartlett's Anomaly-rather than the Jeffreys-Lindley paradox as originally formulated. The paradox, as presented by Lindley (1957), concerns what happens as sample size increases without bound while holding the significance level fixed, not what happens as prior variance diverges. This distinction is not merely terminological: the two phenomena have different mathematical structures, different implications, and require different solutions. The present paper aims to clarify this confusion, demonstrating through Lindley's own equations that he was concerned exclusively with sample size asymptotics. We show that even Jeffreys himself underestimated the practical frequency of the paradox. Finally, we argue that the only genuine resolution lies in abandoning point null hypotheses in favor of interval nulls, a paradigm shift that eliminates the paradox and restores harmony between Bayesian and frequentist inference. Submitted to Statistica Sinica.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22816v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Miodrag M. Lovric</dc:creator>
    </item>
    <item>
      <title>Functional Gaussian Fields on Hyperspheres with their Equivalent Gaussian Measures</title>
      <link>https://arxiv.org/abs/2511.23008</link>
      <description>arXiv:2511.23008v1 Announce Type: new 
Abstract: We develop a general framework for isotropic functional Gaussian fields on the $d$-dimensional sphere $\mathbb{S}^{d}$, where the field takes values in a separable Hilbert space $\mathcal{H}$. We establish an operator-valued extension of Schoenberg's theorem and show that the covariance structure of such fields admits a representation through a sequence of trace-class $d$-Schoenberg operators, yielding an explicit spectral decomposition of the covariance operator on $L^{2}(\mathbb{S}^{d};\mathcal{H})$. We derive a functional version of the Feldman-H'ajek criterion and prove that equivalence of the Gaussian measures induced by two Hilbert-valued spherical fields is determined by a Hilbert summability condition involving Schoenberg functional sequences, extending classical results for scalar and vector fields to the infinite-dimensional setting. We further show how equivalence of all scalar projections is contained within, and dominated by, the functional criterion. The theory is illustrated through two models: (i) a multiquadratic bivariate family on $\mathbb{S}^{d}$, where the equivalence region has a closed-form description in terms of cross-correlation and geodesic decay parameters, and (ii) an infinite-dimensional Legendre-Mat'ern construction, where operator-valued spectra yield identifiability conditions on smoothness and scale. These examples show how operator-valued Schoenberg coefficients govern both geometry and measure-theoretic behavior of functional spherical fields. Overall, the results provide a unified spectral framework for Gaussian measures on $L^{2}(\mathbb{S}^{d};\mathcal{H})$, bridging harmonic analysis, operator theory, and stochastic geometry on manifolds, and offering tools for functional data analysis, spatial statistics, and kernel methods on spherical domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23008v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessia Caponera, Vinicius Ferreira, Emilio Porcu</dc:creator>
    </item>
    <item>
      <title>Detecting Periodicity of a General Stationary Time Series via AR(2)-Model Fitting</title>
      <link>https://arxiv.org/abs/2511.23094</link>
      <description>arXiv:2511.23094v1 Announce Type: new 
Abstract: Estimating the periodicity of a stationary time series via fitting a second order stationary autoregressive (AR(2)) model has been initiated by the seminal paper of Yule(1927).. We investigate properties of this procedure when applied to a general stationary processes possessing a spectral density with a dominant peak at some frequency $\lambda_0\in(0,\pi)$. We show that if the peak of the spectral density is sharp enough (in a way to be specified) then the AR(2) model, which best (in mean square sense) approximates the underlying process, correctly identifies the frequency $\lambda_0$. To investigate consistency properties of the AR(2) based estimator of $\lambda_0$, a near to pole framework is adopted. Triangular arrays of stationary stochastic processes are considered that possess a spectral density the peak of which at $\lambda_0$ becomes more pronounced as the sample size $n$ of the observed time series increases to infinity. It is shown in this set up, that the AR(2) based estimator achieves a rate of convergence which is larger than the parametric $n^{-1/2}$ rate and which can be arbitrarily close to $ n^{-2/3}$, the best rate that can be achieved by this estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23094v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jens-Peter Kreiss, Panagiotis Maouris, Efstathios Paparoditis</dc:creator>
    </item>
    <item>
      <title>The dynamic of a tax on land value : concepts, models and impact scenario</title>
      <link>https://arxiv.org/abs/2511.21766</link>
      <description>arXiv:2511.21766v1 Announce Type: cross 
Abstract: This paper develops a spatial-dynamic framework to analyze the theoretical and quantitative effects of a Land Value Tax (LVT) on urban land markets, capital accumulation, and spatial redistribution. Building upon the Georgist distinction between produced value and unearned rent, the model departs from the static equilibrium tradition by introducing an explicit diffusion process for land values and a local investment dynamic governed by profitability thresholds. Land value $V (x, y, t)$ and built capital $K(x, y, t)$evolve over a two-dimensional urban domain according to coupled nonlinear partial differential equations, incorporating local productivity $A(x, y)$, centrality effects $\mu(x, y)$, depreciation $\delta$, and fiscal pressure $\tau$ . Analytical characterization of the steady states reveals a transcritical bifurcation in the parameter $\tau$ , separating inactive (low-investment) and active (self-sustaining) spatial regimes. The equilibrium pair $(V ^*, K^*)$ is shown to exist only when the effective decay rate $\alpha = r + \tau - \mu(x, y)$ exceeds a profitability threshold $\theta = \kappa + \delta / I_0$, and becomes locally unstable beyond this boundary. The introduction of diffusion, $D_V \Delta V$, stabilizes spatial dynamics and generates continuous gradients of land value and capital intensity, mitigating speculative clustering while preserving productive incentives. Numerical simulations confirm these analytical properties and display the emergence of spatially heterogeneous steady states driven by urban centrality and local productivity. The model also quantifies key aggregate outcomes, including dynamic tax revenues, adjusted capital-to-land ratios, and net present values under spatial heterogeneity and temporal discounting. Sensitivity analyses demonstrate that the main qualitative mechanisms-critical activation, spatial recomposition, and bifurcation structure-remain robust under alternative spatial profiles $(A, \mu)$, discretization schemes, and moderate differentiation of the tax rate $\tau (x, y)$. From an economic perspective, the results clarify the dual nature of the LVT: while it erodes unproductive rents and speculative land holding, its dynamic incidence on built capital depends on local profitability and financing constraints. The taxation parameter $\tau$ thus acts as a control variable in a nonlinear spatial system, shaping transitions between rent-driven and production-driven equilibria. Within a critical range around $\tau_c$, the LVT functions as an efficient spatial reallocation operator-reducing inequality in land values and investment density without impairing aggregate productivity. Beyond this range, excessive taxation induces systemic contraction and investment stagnation. Overall, this research bridges static urban tax theory with dynamic spatial economics by formalizing how a land-based fiscal instrument can reshape the geography of value creation through endogenous diffusion and nonlinear feedback. The framework provides a foundation for future extensions involving stochastic shocks, adaptive policy feedbacks, or endogenous public investment, offering a unified quantitative perspective on the dynamic efficiency and spatial equity of land value taxation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21766v1</guid>
      <category>econ.GN</category>
      <category>math.ST</category>
      <category>q-fin.EC</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Spring-Ragain (HEIP)</dc:creator>
    </item>
    <item>
      <title>Convergence of a Sequential Monte Carlo algorithm towards multimodal distributions on Rd</title>
      <link>https://arxiv.org/abs/2511.22564</link>
      <description>arXiv:2511.22564v2 Announce Type: cross 
Abstract: In an earlier joint work, we studied a sequential Monte Carlo algorithm to sample from the Gibbs measure supported on torus with a non-convex energy function at a low temperature, where we proved that the time complexity of the algorithm is polynomial in the inverse temperature. However, the analysis in that torus setting relied crucially on compactness and does not directly extend to unbounded domains. This work introduces a new approach that resolves this issue and establishes a similar result for sampling from Gibbs measures supported on Rd.
  In particular, our main result shows that when the energy function is double-well with equal depth, the time complexity scales as seventh power of the inverse temperature, and quadratically in both the inverse allowed absolute error and probability error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22564v2</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiyu Han</dc:creator>
    </item>
    <item>
      <title>The Bayesian optimal two-stage design for clinical phase II trials based on Bayes factors</title>
      <link>https://arxiv.org/abs/2511.23144</link>
      <description>arXiv:2511.23144v1 Announce Type: cross 
Abstract: Sequential trial design is an important statistical approach to increase the efficiency of clinical trials. Bayesian sequential trial design relies primarily on conducting a Monte Carlo simulation under the hypotheses of interest and investigating the resulting design characteristics via Monte Carlo estimates. This approach has several drawbacks, namely that replicating the calibration of a Bayesian design requires repeating a possibly complex Monte Carlo simulation. Furthermore, Monte Carlo standard errors are required to judge the reliability of the simulation. All of this is due to a lack of closed-form or numerical approaches to calibrate a Bayesian design which uses Bayes factors. In this paper, we propose the Bayesian optimal two-stage design for clinical phase II trials based on Bayes factors. The optimal two-stage Bayes factor design is a sequential clinical trial design that is built on the idea of trinomial tree branching, a method we propose to correct the resulting design characteristics for introducing a single interim analysis. We build upon this idea to invent a calibration algorithm which yields the optimal Bayesian design that minimizes the expected sample size under the null hypothesis. Examples show that our design recovers Simon's two-stage optimal design as a special case, improves upon non-sequential Bayesian design based on Bayes factors, and can be calibrated quickly, as it makes use only of standard numerical techniques instead of time-consuming Monte Carlo simulations. Furthermore, the design allows to ensure a minimum probability on compelling evidence in favour of the null hypothesis, which is not possible with other designs. As the idea of trinomial tree branching is neither dependent on the endpoint, nor on the use of Bayes factors, the design can therefore be generalized to other settings, too.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23144v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Riko Kelter, Samuel Pawel</dc:creator>
    </item>
    <item>
      <title>Asymptotic Theory and Phase Transitions for Variable Importance in Quantile Regression Forests</title>
      <link>https://arxiv.org/abs/2511.23212</link>
      <description>arXiv:2511.23212v1 Announce Type: cross 
Abstract: Quantile Regression Forests (QRF) are widely used for non-parametric conditional quantile estimation, yet statistical inference for variable importance measures remains challenging due to the non-smoothness of the loss function and the complex bias-variance trade-off. In this paper, we develop a asymptotic theory for variable importance defined as the difference in pinball loss risks. We first establish the asymptotic normality of the QRF estimator by handling the non-differentiable pinball loss via Knight's identity. Second, we uncover a "phase transition" phenomenon governed by the subsampling rate $\beta$ (where $s \asymp n^{\beta}$). We prove that in the bias-dominated regime ($\beta \ge 1/2$), which corresponds to large subsample sizes typically favored in practice to maximize predictive accuracy, standard inference breaks down as the estimator converges to a deterministic bias constant rather than a zero-mean normal distribution. Finally, we derive the explicit analytic form of this asymptotic bias and discuss the theoretical feasibility of restoring valid inference via analytic bias correction. Our results highlight a fundamental trade-off between predictive performance and inferential validity, providing a theoretical foundation for understanding the intrinsic limitations of random forest inference in high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23212v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoshige Nakamura, Hiroshi Shiraishi</dc:creator>
    </item>
    <item>
      <title>Convergence rates of self-repellent random walks, their local time and Event Chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2511.23453</link>
      <description>arXiv:2511.23453v1 Announce Type: cross 
Abstract: We study the rate of convergence to equilibrium of the self-repellent random walk and its local time process on the discrete circle $\mathbb{Z}_n$. While the self-repellent random walk alone is non-Markovian since the jump rates depend on its history via its local time, jointly considering the evolution of the local time profile and the position yields a piecewise deterministic, non-reversible Markov process. We show that this joint process can be interpreted as a second-order lift of a reversible diffusion process, the discrete stochastic heat equation with Gaussian invariant measure. In particular, we obtain a lower bound on the relaxation time of order $\Omega(n^{3/2})$. Using a flow Poincar\'e inequality, we prove an upper bound for a slightly modified dynamics of order $O(n^2)$, matching recent conjectures in the physics literature. Furthermore, since the self-repellent random walk and its local time process coincide with the Event Chain Monte Carlo algorithm for the harmonic chain, a non-reversible MCMC method, we demonstrate that the relaxation time bound confirms the recent empirical observation that Event Chain Monte Carlo algorithms can outperform traditional MCMC methods such as Hamiltonian Monte Carlo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23453v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Eberle, Francis L\"orler</dc:creator>
    </item>
    <item>
      <title>Information-Theoretic Limits and Strong Consistency on Binary Non-uniform Hypergraph Stochastic Block Models</title>
      <link>https://arxiv.org/abs/2306.06845</link>
      <description>arXiv:2306.06845v3 Announce Type: replace 
Abstract: We investigate the unsupervised node classification problem on random hypergraphs under the non-uniform Hypergraph Stochastic Block Model (HSBM) with two equal-sized communities. In this model, edges appear independently with probabilities depending only on the labels of their vertices. We identify the threshold for strong consistency, expressed in terms of the Generalized Hellinger distance. Below this threshold, strong consistency is impossible, and we derive the Information-Theoretic (IT) lower bound on the expected mismatch ratio. Above the threshold, the parameter space is typically divided into two disjoint regions. When only the aggregated adjacency matrices are accessible, while one-stage algorithms accomplish strong consistency with high probability in the region far from the threshold, they fail in the region closer to the threshold. We propose a new refinement algorithm which, in conjunction with the initial estimation, provably achieves strong consistency throughout the entire region above the threshold, and attains the IT lower bound when below the threshold, proving its optimality. This novel refinement algorithm applies the power iteration method to a weighted adjacency matrix, where the weights are determined by hyperedge sizes and the initial label estimate. Unlike the constant degree regime where a subset selection of uniform layers is necessary to enhance clustering accuracy, in the scenario with diverging degrees, each uniform layer contributes non-negatively to clustering accuracy. Therefore, aggregating information across all uniform layers yields better performance than using any single layer alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.06845v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hai-Xiao Wang</dc:creator>
    </item>
    <item>
      <title>Robust Regression under Adversarial Contamination: Theory and Algorithms for the Welsch Estimator</title>
      <link>https://arxiv.org/abs/2412.19183</link>
      <description>arXiv:2412.19183v4 Announce Type: replace 
Abstract: Convex and penalized robust regression methods often suffer from a persistent bias induced by large outliers, limiting their effectiveness in adversarial or heavy-tailed settings. In this work, we study a smooth redescending non-convex M-estimator, specifically the Welsch estimator, and show that it can eliminate this bias whenever it is statistically identifiable. We focus on high-dimensional linear regression under adversarial contamination, where a fraction of samples may be corrupted by an adversary with full knowledge of the data and underlying model. A central technical contribution of this paper is a practical algorithm that provably finds a statistically valid solution to this non-convex problem. We show that the Welsch objective remains locally convex within a well-characterized basin of attraction, and our algorithm is guaranteed to converge into this region and recover the desired estimator. We establish three main guarantees: (a) non-asymptotic minimax-optimal deviation bounds under contamination, (b) improved unbiasedness in the presence of large outliers, and (c) asymptotic normality, yielding statistical efficiency as the sample size grows. Finally, we support our theoretical findings with comprehensive experiments on synthetic and real datasets, demonstrating the estimator's superior robustness, efficiency, and effectiveness in mitigating outlier-induced bias relative to state-of-the-art robust regression methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19183v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilyes Hammouda, Mohamed Ndaoud, Abd-Krim Seghouane</dc:creator>
    </item>
    <item>
      <title>Totally Concave Regression</title>
      <link>https://arxiv.org/abs/2501.04360</link>
      <description>arXiv:2501.04360v3 Announce Type: replace 
Abstract: Shape constraints in nonparametric regression provide a powerful framework for estimating regression functions under realistic assumptions without tuning parameters. However, most existing methods$\unicode{x2013}$except additive models$\unicode{x2013}$impose too weak restrictions, often leading to overfitting in high dimensions. Conversely, additive models can be too rigid, failing to capture covariate interactions. This paper introduces a novel multivariate shape-constrained regression approach based on total concavity, originally studied by T. Popoviciu. Our method allows interactions while mitigating the curse of dimensionality, with convergence rates that depend only logarithmically on the number of covariates. We characterize and compute the least squares estimator over totally concave functions, derive theoretical guarantees, and demonstrate its practical effectiveness through empirical studies on real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04360v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dohyeong Ki, Adityanand Guntuboyina</dc:creator>
    </item>
    <item>
      <title>Importance sampling for Sobol' indices estimation</title>
      <link>https://arxiv.org/abs/2507.05958</link>
      <description>arXiv:2507.05958v2 Announce Type: replace 
Abstract: We propose a new importance sampling framework for the estimation and analysis of Sobol' indices. We focus on the estimation of the conditional second-moment quantity underlying these indices, which is the most challenging term to estimate. We show that this quantity, originally defined under a reference input distribution, can be estimated from samples drawn under auxiliary distributions by reweighting the model outputs. We derive the optimal sampling distribution that minimises the asymptotic variance of efficient estimators and demonstrate its impact on estimation. Beyond variance reduction, the framework also supports distributional sensitivity analysis through reverse importance sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05958v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Haythem Boucharif, J\'er\^ome Morio, Paul Rochet</dc:creator>
    </item>
    <item>
      <title>Assessing continuous common-shock risk through matrix distributions</title>
      <link>https://arxiv.org/abs/2507.15637</link>
      <description>arXiv:2507.15637v2 Announce Type: replace 
Abstract: We introduce a class of continuous-time bivariate phase-type distributions for modeling dependencies from common shocks. The construction uses continuous-time Markov processes that evolve identically until an internal common-shock event, after which they diverge into independent processes. We derive and analyze key risk measures for this new class, including joint cumulative distribution functions, dependence measures, and conditional risk measures. Theoretical results establish analytically tractable properties of the model. For parameter estimation, we employ efficient gradient-based methods. Applications to both simulated and real-world data illustrate the ability to capture common-shock dependencies effectively. Our analysis also demonstrates that common-shock continuous phase-type distributions may capture dependencies that extend beyond those explicitly triggered by common shocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15637v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bladt, Oscar Peralta, Jorge Yslas</dc:creator>
    </item>
    <item>
      <title>Finite- and Large- Sample Inference for Model and Coefficients in High-dimensional Linear Regression with Repro Samples</title>
      <link>https://arxiv.org/abs/2209.09299</link>
      <description>arXiv:2209.09299v4 Announce Type: replace-cross 
Abstract: In this paper, we present a novel and effective inference approach to conduct both finite- and large-sample inference for high-dimensional linear regression models. This approach is developed under the so-called repro samples framework, in which we conduct statistical inference by creating and studying the behavior of artificial samples that are obtained by mimicking the sampling mechanism of the data. We construct confidence sets for (a) the true model corresponding to the nonzero coefficients, (b) a single or any collection of regression coefficients, and (c) both the model and regression coefficients jointly. To facilitate the constructions of these confidence sets and overcome computational difficulties of searching all possible models, we use an innovative Fisher inversion technique to construct a model candidate set that includes the true sparse model with the probability close to 1 for models with both Gaussian and non-Gaussian errors. The proposed approach fills in two major gaps in the high-dimensional regression literature: (1) lack of effective approaches to addressing model selection uncertainty and providing valid inference for the underlying true model; (2) lack of effective inference approaches to guaranteeing finite-sample performance. We provide both finite-sample and asymptotic results to theoretically guarantee the performance of the proposed methods. In addition, our numerical results demonstrate that the proposed methods are valid and achieve better coverage with smaller confidence sets than the current state-of-the-art approaches, such as debiasing and bootstrap approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.09299v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peng Wang, Min-Ge Xie, Linjun Zhang</dc:creator>
    </item>
    <item>
      <title>Theoretical guarantees for lifted samplers</title>
      <link>https://arxiv.org/abs/2405.15952</link>
      <description>arXiv:2405.15952v2 Announce Type: replace-cross 
Abstract: Lifted samplers form a class of Markov chain Monte Carlo methods which has drawn a lot attention in recent years due to superior performance in challenging Bayesian applications. A canonical example of such sampler is the one that is derived from a random walk Metropolis algorithm for a totally-ordered state space such as the integers or the real numbers. The lifted sampler is derived by splitting into two the proposal distribution: one part in the increasing direction, and the other part in the decreasing direction. It keeps following a direction, until a rejection occurs, upon which it flips the direction. In terms of asymptotic variances, it outperforms the random walk Metropolis algorithm, regardless of the target distribution, at no additional computational cost. Other studies show, however, that beyond this simple case, lifted samplers do not always outperform their Metropolis counterparts. In this paper, we leverage the celebrated work of Tierney (1998) to provide an analysis in a general framework encompassing a broad class of lifted samplers. Our finding is that, essentially, the asymptotic variances cannot increase by a factor of more than 2, regardless of the target distribution, the way the directions are induced, and the type of algorithm from which the lifted sampler is derived (be it a Metropolis--Hastings algorithm, a reversible jump algorithm, etc.). This result indicates that, while there is potentially a lot to gain from lifting a sampler, there is not much to lose.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15952v2</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philippe Gagnon, Florian Maire</dc:creator>
    </item>
    <item>
      <title>Randomized Block Coordinate DC Programming</title>
      <link>https://arxiv.org/abs/2411.11664</link>
      <description>arXiv:2411.11664v3 Announce Type: replace-cross 
Abstract: We introduce an extension of the Difference of Convex Algorithm (DCA) in the form of a randomized block coordinate approach for problems with separable structure. For $n$ coordinate-blocks and $k$ iterations, our main result proves a non-asymptotic convergence rate of $O(n/k)$ in expectation, with respect to a stationarity measure based on a Forward-Backward envelope. Furthermore, leveraging the connection between DCA and Expectation Maximization (EM), we propose a randomized block coordinate EM algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11664v3</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hoomaan Maskan, Paniz Halvachi, Suvrit Sra, Alp Yurtsever</dc:creator>
    </item>
    <item>
      <title>One-Shot Learning for k-SAT</title>
      <link>https://arxiv.org/abs/2502.07135</link>
      <description>arXiv:2502.07135v3 Announce Type: replace-cross 
Abstract: Consider a $k$-SAT formula $\Phi$ where every variable appears at most $d$ times. Let $\sigma$ be a satisfying assignment, sampled proportionally to $e^{\beta m(\sigma)}$ where $m(\sigma)$ is the number of true variables and $\beta$ is a real parameter. Given $\Phi$ and $\sigma$, can we efficiently learn $\beta$?
  This problem falls into a recent line of work about single-sample (``one-shot'') learning of Markov random fields. Our $k$-SAT setting was recently studied by Galanis, Kalavasis, Kandiros (SODA24). They showed that single-sample learning is possible when roughly $d\leq 2^{k/6.45}$ and impossible when $d\geq (k+1) 2^{k-1}$. In addition to the gap in~$d$, their impossibility result left open the question of whether the feasibility threshold for one-shot learning is dictated by the satisfiability threshold for bounded-degree $k$-SAT formulas.
  Our main contribution is to answer this question negatively. We show that one-shot learning for $k$-SAT is infeasible well below the satisfiability threshold; in fact, we obtain impossibility results for degrees $d$ as low as $k^2$ when $\beta$ is sufficiently large, and bootstrap this to small values of $\beta$ when $d$ scales exponentially with $k$, via a probabilistic construction. On the positive side, we simplify the analysis of the learning algorithm, obtaining significantly stronger bounds on $d$ in terms of $\beta$. For the uniform case $\beta\rightarrow 0$, we show that learning is possible under the condition $d\lesssim 2^{k/2}$. This is (up to constant factors) all the way to the sampling threshold -- it is known that sampling a uniformly-distributed satisfying assignment is NP-hard for $d\gtrsim 2^{k/2}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07135v3</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreas Galanis, Leslie Ann Goldberg, Xusheng Zhang</dc:creator>
    </item>
    <item>
      <title>A Generalized Tangent Approximation based Variational Inference Framework for Strongly Super-Gaussian Likelihoods</title>
      <link>https://arxiv.org/abs/2504.05431</link>
      <description>arXiv:2504.05431v2 Announce Type: replace-cross 
Abstract: Variational inference, as an alternative to Markov chain Monte Carlo sampling, has played a transformative role in enabling scalable computation for complex Bayesian models. Nevertheless, existing approaches often depend on either rigid model-specific formulations or stochastic black-box optimization routines. Tangent approximation is a principled class of structured variational methods that exploits the geometry of the underlying probability model. However, its utility has largely been confined to logistic regression and related modeling regimes. In this article, we propose a novel variational framework based on tangent transformation for a broad class of probability models characterized by strongly super-Gaussian likelihoods. Our method leverages convex duality to construct tangent minorants of the log-likelihood, thereby inducing conjugacy with Gaussian priors over model parameters in an otherwise intractable setup. Under mild assumptions on the data-generating mechanism, we establish algorithmic convergence guarantees, a contribution that stands in contrast to the limited theoretical assurances typically available for black-box variational methods. Additionally, we derive near-minimax optimal bounds for the variational risk. Superior performance of our proposed methodology is illustrated on simulated and real-data scenarios that challenge state-of-the-art variational algorithms in terms of scalability and their ability to consistently capture complex underlying data structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05431v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Somjit Roy, Pritam Dey, Debdeep Pati, Bani K. Mallick</dc:creator>
    </item>
    <item>
      <title>Polyhedral Aspects of Maxoids</title>
      <link>https://arxiv.org/abs/2504.21068</link>
      <description>arXiv:2504.21068v2 Announce Type: replace-cross 
Abstract: The conditional independence (CI) relation of a distribution in a max-linear Bayesian network depends on its weight matrix through the $C^\ast$-separation criterion. These CI~models, which we call maxoids, are compositional graphoids which are in general not representable by Gaussian random variables. We prove that every maxoid can be obtained from a transitively closed weighted DAG and show that the stratification of generic weight matrices by their maxoids yields a polyhedral~fan. We also use this connection to polyhedral geometry to develop an algorithm for solving the conditional independence implication problem for maxoids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21068v2</guid>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tobias Boege, Kamillo Ferry, Benjamin Hollering, Francesco Nowell</dc:creator>
    </item>
    <item>
      <title>Property Elicitation on Imprecise Probabilities</title>
      <link>https://arxiv.org/abs/2507.05857</link>
      <description>arXiv:2507.05857v2 Announce Type: replace-cross 
Abstract: Property elicitation studies which attributes of a probability distribution can be determined by minimizing a risk. We investigate a generalization of property elicitation to imprecise probabilities (IP). This investigation is motivated by distributionally robust optimization and multi-distribution learning. Both those frameworks replace the minimization of a single risk over a (precise) probability by a maximin risk minimization over a set of probabilities -- i.e. an IP. We show what can be learned in those multi-distribution setups by providing necessary and sufficient conditions for the elicitability of an IP-property. Central to these conditions is the observation made in related literature that the elicited IP-property is the corresponding classical property of the probability in the IP with the maximum Bayes risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05857v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>James Bailie, Rabanus Derr</dc:creator>
    </item>
    <item>
      <title>Beyond Scores: Proximal Diffusion Models</title>
      <link>https://arxiv.org/abs/2507.08956</link>
      <description>arXiv:2507.08956v2 Announce Type: replace-cross 
Abstract: Diffusion models have quickly become some of the most popular and powerful generative models for high-dimensional data. The key insight that enabled their development was the realization that access to the score -- the gradient of the log-density at different noise levels -- allows for sampling from data distributions by solving a reverse-time stochastic differential equation (SDE) via forward discretization, and that popular denoisers allow for unbiased estimators of this score. In this paper, we demonstrate that an alternative, backward discretization of these SDEs, using proximal maps in place of the score, leads to theoretical and practical benefits. We leverage recent results in proximal matching to learn proximal operators of the log-density and, with them, develop Proximal Diffusion Models (ProxDM). Theoretically, we prove that $\widetilde{O}(d/\sqrt{\varepsilon})$ steps suffice for the resulting discretization to generate an $\varepsilon$-accurate distribution w.r.t. the KL divergence. Empirically, we show that two variants of ProxDM achieve significantly faster convergence within just a few sampling steps compared to conventional score-matching methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08956v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenghan Fang, Mateo D\'iaz, Sam Buchanan, Jeremias Sulam</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bayesian Calibration of Computer Models</title>
      <link>https://arxiv.org/abs/2509.22597</link>
      <description>arXiv:2509.22597v3 Announce Type: replace-cross 
Abstract: Calibration of computer models is a key step in making inferences, predictions, and decisions for complex science and engineering systems. We formulate and analyze a nonparametric Bayesian methodology for computer model calibration. This paper presents a number of key results including; establishment of a unique nonparametric Bayesian posterior corresponding to a chosen prior with an explicit formula for the corresponding conditional density; a maximum entropy property of the posterior corresponding to the uniform prior; the almost everywhere continuity of the density of the nonparametric posterior; and a comprehensive convergence and asymptotic analysis of an estimator based on a form of importance sampling. We illustrate the problem and results using several examples, including a simple experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22597v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haiyi Shi, Lei Yang, Jiarui Chi, Troy Butler, Haonan Wang, Derek Bingham, Don Estep</dc:creator>
    </item>
  </channel>
</rss>
