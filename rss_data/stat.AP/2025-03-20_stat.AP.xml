<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Mar 2025 01:55:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A proposal of smooth interpolation to optimal transport for restoring biased data for algorithmic fairness</title>
      <link>https://arxiv.org/abs/2503.15119</link>
      <description>arXiv:2503.15119v1 Announce Type: new 
Abstract: The so-called algorithmic bias is a hot topic in the decision making process based on Artificial Intelligence, especially when demographics, such as gender, age or ethnic origin, come into play. Frequently, the problem is not only in the algorithm itself, but also in the biased data feeding the algorithm, which is just the reflection of the societal bias. Thus, this `food' given to the algorithm have to be repaired in order to produce unbiased results for all. As a simple, but frequent case, two different subgroups will be considered, the privileged and the unprivileged groups. Assuming that results should not depend on such characteristic dividing the data, the rest of attributes in each group have to be moved (transported) so that its underlying distribution can be considered similar. For doing this, optimal transport (OT) theory is used to effectively transport the values of the features, excluding the sensitive variable, to the so-called {\it Wasserstein barycenter} of the two distributions conditional to each group. An efficient procedure based on the {\it auction algorithm} is adapted for doing so. The transportation is made for the data at hand. If new data arrive then the OT problem has to be solved for the new set gathering previous and incoming data, which is rather inefficient. Alternatively, an implementation of a smooth interpolation procedure called \textit{Extended Total Repair (ExTR)} is proposed, which is one of the main contributions of the paper. The methodology is applied with satisfactory results to simulated biased data as well as to a real case for a German credit dataset used for risk assessment prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15119v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena M. De Diego, Paula Gordaliza, Jes\'us Lopez-Fidalgo</dc:creator>
    </item>
    <item>
      <title>Making Online Polls More Accurate: Statistical Methods Explained</title>
      <link>https://arxiv.org/abs/2503.15395</link>
      <description>arXiv:2503.15395v1 Announce Type: new 
Abstract: Online data has the potential to transform how researchers and companies produce election forecasts. Social media surveys, online panels and even comments scraped from the internet can offer valuable insights into political preferences. However, such data is often affected by significant selection bias, as online respondents may not be representative of the overall population. At the same time, traditional data collection methods are becoming increasingly cost-prohibitive. In this scenario, scientists need instruments to be able to draw the most accurate estimate possible from samples drawn online. This paper provides an introduction to key statistical methods for mitigating bias and improving inference in such cases, with a focus on electoral polling. Specifically, it presents the main statistical techniques, categorized into weighting, modeling and other approaches. It also offers practical recommendations for drawing estimates with measures of uncertainty. Designed for both researchers and industry practitioners, this introduction takes a hands-on approach, with code available for implementing the main methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15395v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Arletti, Maria Letizia Tanturri, Omar Paccagnella</dc:creator>
    </item>
    <item>
      <title>A Precision Trial Case Study for Heterogeneous Treatment Effects in Obstructive Sleep Apnea</title>
      <link>https://arxiv.org/abs/2503.15455</link>
      <description>arXiv:2503.15455v1 Announce Type: new 
Abstract: Precision medicine tailors treatments to individual patient characteristics, which is especially valuable for conditions like obstructive sleep apnea (OSA), where treatment responses vary widely. Traditional trials often overlook subgroup differences, leading to suboptimal recommendations. Current approaches rely on pre-specified thresholds with inherent uncertainty, assuming these thresholds are correct-a flawed assumption. This case study compares pre-specified thresholds to two advanced Bayesian methods: the established FK-BMA method and its novel variant, FK. The FK approach retains the flexibility of free-knot splines but omits variable selection, providing stable, interpretable models. Using biomarker data from large studies, this design identifies subgroups dynamically, allowing early trial termination or enrollment adjustments. Simulations in this specific context show FK improves precision, efficiency, and subgroup detection, offering practical benefits over FK-BMA and advancing precision medicine for OSA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15455v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lara Maleyeff, Shirin Golchi, Erica E. M. Moodie, R. John Kimoff</dc:creator>
    </item>
    <item>
      <title>Machine learning algorithms to predict stroke in China based on causal inference of time series analysis</title>
      <link>https://arxiv.org/abs/2503.14512</link>
      <description>arXiv:2503.14512v1 Announce Type: cross 
Abstract: Participants: This study employed a combination of Vector Autoregression (VAR) model and Graph Neural Networks (GNN) to systematically construct dynamic causal inference. Multiple classic classification algorithms were compared, including Random Forest, Logistic Regression, XGBoost, Support Vector Machine (SVM), K-Nearest Neighbor (KNN), Gradient Boosting, and Multi Layer Perceptron (MLP). The SMOTE algorithm was used to undersample a small number of samples and employed Stratified K-fold Cross Validation. Results: This study included a total of 11,789 participants, including 6,334 females (53.73%) and 5,455 males (46.27%), with an average age of 65 years. Introduction of dynamic causal inference features has significantly improved the performance of almost all models. The area under the ROC curve of each model ranged from 0.78 to 0.83, indicating significant difference (P &lt; 0.01). Among all the models, the Gradient Boosting model demonstrated the highest performance and stability. Model explanation and feature importance analysis generated model interpretation that illustrated significant contributors associated with risks of stroke. Conclusions and Relevance: This study proposes a stroke risk prediction method that combines dynamic causal inference with machine learning models, significantly improving prediction accuracy and revealing key health factors that affect stroke. The research results indicate that dynamic causal inference features have important value in predicting stroke risk, especially in capturing the impact of changes in health status over time on stroke risk. By further optimizing the model and introducing more variables, this study provides theoretical basis and practical guidance for future stroke prevention and intervention strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14512v1</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qizhi Zheng, Ayang Zhao, Xinzhu Wang, Yanhong Bai, Zikun Wang, Xiuying Wang, Xianzhang Zeng, Guanghui Dong</dc:creator>
    </item>
    <item>
      <title>Spline refinement with differentiable rendering</title>
      <link>https://arxiv.org/abs/2503.14525</link>
      <description>arXiv:2503.14525v1 Announce Type: cross 
Abstract: Detecting slender, overlapping structures remains a challenge in computational microscopy. While recent coordinate-based approaches improve detection, they often produce less accurate splines than pixel-based methods. We introduce a training-free differentiable rendering approach to spline refinement, achieving both high reliability and sub-pixel accuracy. Our method improves spline quality, enhances robustness to distribution shifts, and shrinks the gap between synthetic and real-world data. Being fully unsupervised, the method is a drop-in replacement for the popular active contour model for spline refinement. Evaluated on C. elegans nematodes, a popular model organism for drug discovery and biomedical research, we demonstrate that our approach combines the strengths of both coordinate- and pixel-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14525v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Frans Zdyb, Albert Alonso, Julius B. Kirkegaard</dc:creator>
    </item>
    <item>
      <title>AI-driven Uncertainty Quantification &amp; Multi-Physics Approach to Evaluate Cladding Materials in a Microreactor</title>
      <link>https://arxiv.org/abs/2503.14679</link>
      <description>arXiv:2503.14679v1 Announce Type: cross 
Abstract: The pursuit of enhanced nuclear safety has spurred the development of accident-tolerant cladding (ATC) materials for light water reactors (LWRs). This study investigates the potential of repurposing these ATCs in advanced reactor designs, aiming to expedite material development and reduce costs. The research employs a multi-physics approach, encompassing neutronics, heat transfer, thermodynamics, and structural mechanics, to evaluate four candidate materials (Haynes 230, Zircaloy-4, FeCrAl, and SiC-SiC) within the context of a high-temperature, sodium-cooled microreactor, exemplified by the Kilopower design. While neutronic simulations revealed negligible power profile variations among the materials, finite element analyses highlighted the superior thermal stability of SiC-SiC and the favorable stress resistance of Haynes 230. The high-temperature environment significantly impacted material performance, particularly for Zircaloy-4 and FeCrAl, while SiC-SiC's inherent properties limited its ability to withstand stress loads. Additionally, AI-driven uncertainty quantification and sensitivity analysis were conducted to assess the influence of material property variations on maximum hoop stress. The findings underscore the need for further research into high-temperature material properties to facilitate broader applicability of existing materials to advanced reactors. Haynes 230 is identified as the most promising candidate based on the evaluated criteria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14679v1</guid>
      <category>physics.ins-det</category>
      <category>cond-mat.mtrl-sci</category>
      <category>stat.AP</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Foutch, Kazuma Kobayashi, Ayodeji Alajo, Dinesh Kumar, Syed Bahauddin Alam</dc:creator>
    </item>
    <item>
      <title>PSInference: A Package to Draw Inference for Released Plug-in Sampling Single Synthetic Dataset</title>
      <link>https://arxiv.org/abs/2503.14711</link>
      <description>arXiv:2503.14711v1 Announce Type: cross 
Abstract: The development and generation of synthetic data are becoming increasingly vital in the field of statistical disclosure control. The PSInference package provides tools to perform exact inferential analysis on singly imputed synthetic data generated through Plug-in Sampling assuming that the original dataset follows a multivariate normal distribution. Includes functions to test the synthetic data's covariance structure, covering aspects like generalized variance, sphericity, independence between subsets of variables, and regression of one set of variables on another. This package addresses the gap in the existing software by providing exact inferential methods suitable for cases where only a single synthetic dataset is released.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14711v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ricardo Moura, Mina Norouzirad, Vitor Augusto, Miguel Fonseca</dc:creator>
    </item>
    <item>
      <title>Bayesian hierarchical non-stationary hybrid modeling for threshold estimation in peak over threshold approach</title>
      <link>https://arxiv.org/abs/2503.14839</link>
      <description>arXiv:2503.14839v1 Announce Type: cross 
Abstract: Extreme value theory (EVT) has been utilized to estimate crash risk from traffic conflicts with the peak over threshold approach. However, it's challenging to determine a suitable threshold to distinguish extreme conflicts in an objective way. The subjective and arbitrary selection of the threshold in the peak over threshold approach can result in biased estimation outcomes. This study proposes a Bayesian hierarchical hybrid modeling (BHHM) framework for the threshold estimation in the peak over threshold approach. Specifically, BHHM is based on a piecewise function to model the general conflicts with specific distribution while model the extreme conflicts with generalized Pareto distribution (GPD). The Bayesian hierarchical structure is used to combine traffic conflicts from different sites, incorporating covariates and site-specific unobserved heterogeneity. Five non-stationary BHHM models, including Normal-GPD, Cauchy-GPD, Logistic-GPD, Gamma-GPD, and Lognormal-GPD models, were developed and compared. Traditional graphical diagnostic and quantile regression approaches were also used for comparison. Traffic conflicts collected from three signalized intersections in the city of Surrey, British Columbia were used for the study. The results show that the proposed BHHM approach could estimate the threshold parameter objectively. The Lognormal-GPD model is superior to the other four BHHM models in terms of crash estimation accuracy and model fit. The crash estimates using the threshold determined by the BHHM outperform those estimated based on the graphical diagnostic and quantile regression approaches, indicating the superiority of the proposed threshold determination approach. The findings of this study contribute to enhancing the existing EVT methods for providing a threshold determination approach as well as producing reliable crash estimations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14839v1</guid>
      <category>stat.OT</category>
      <category>stat.AP</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Quansheng Yue, Yanyong Guo, Tarek Sayed, Lai Zheng, Hao Lyu, Pan Liu</dc:creator>
    </item>
    <item>
      <title>Optimal Data Splitting for Holdout Cross-Validation in Large Covariance Matrix Estimation</title>
      <link>https://arxiv.org/abs/2503.15186</link>
      <description>arXiv:2503.15186v1 Announce Type: cross 
Abstract: Cross-validation is a statistical tool that can be used to improve large covariance matrix estimation. Although its efficiency is observed in practical applications, the theoretical reasons behind it remain largely intuitive, with formal proofs currently lacking. To carry on analytical analysis, we focus on the holdout method, a single iteration of cross-validation, rather than the traditional $k$-fold approach. We derive a closed-form expression for the estimation error when the population matrix follows a white inverse Wishart distribution, and we observe the optimal train-test split scales as the square root of the matrix dimension. For general population matrices, we connected the error to the variance of eigenvalues distribution, but approximations are necessary. Interestingly, in the high-dimensional asymptotic regime, both the holdout and $k$-fold cross-validation methods converge to the optimal estimator when the train-test ratio scales with the square root of the matrix dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15186v1</guid>
      <category>math.ST</category>
      <category>q-fin.PM</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lamia Lamrani, Christian Bongiorno, Marc Potters</dc:creator>
    </item>
    <item>
      <title>Fast Two-photon Microscopy by Neuroimaging with Oblong Random Acquisition (NORA)</title>
      <link>https://arxiv.org/abs/2503.15487</link>
      <description>arXiv:2503.15487v1 Announce Type: cross 
Abstract: Advances in neural imaging have enabled neuroscience to study how the joint activity of large neural populations conspire to produce perception, behavior and cognition. Despite many advances in optical methods, there exists a fundamental tradeoff between imaging speed, field of view, and resolution that limits the scope of neural imaging, especially for the raster-scanning multi-photon imaging needed for imaging deeper into the brain. One approach to overcoming this trade-off is in computational imaging: the co-development of optics and algorithms where the optics are designed to encode the target images into fewer measurements that are faster to acquire, and the algorithms compensate by inverting the optical image coding process to recover a larger or higher resolution image. We present here one such approach for raster-scanning two-photon imaging: Neuroimaging with Oblong Random Acquisition (NORA). NORA quickly acquires each frame in a microscopic video by subsampling only a fraction of the fast scanning lines, ignoring large portions of each frame. NORA mitigates the information loss by extending the point-spread function in the slow-scan direction to integrate the fluorescence of neighboring lines into a single set of measurements. By imaging different, randomly selected, lines at each frame, NORA diversifies the information collected across frames and enables video-level reconstruction. Rather than reconstruct the video frame-by-frame using image-level recovery, NORA recovers full video sequences through a nuclear-norm minimization (i.e., matrix completion) on the pixels-by-time matrix. We simulated NORA imaging using the Neural Anatomy and Optical Microscopy (NAOMi) biophysical simulation suite. Using these simulations we demonstrate that NORA imaging can accurately recover 400 um X 400 um fields of view at subsampling rates up to 20X, despite realistic noise and motion conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15487v1</guid>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <category>physics.optics</category>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Esther Whang, Skyler Thomas, Ji Yi, Adam S. Charles</dc:creator>
    </item>
    <item>
      <title>Information-theoretic evaluation of covariate distributions models</title>
      <link>https://arxiv.org/abs/2406.10611</link>
      <description>arXiv:2406.10611v2 Announce Type: replace 
Abstract: Statistical modelling of covariate distributions allows to generate virtual populations or to impute missing values in a covariate dataset. Covariate distributions typically have non-Gaussian margins and show nonlinear correlation structures, which simple multivariate Gaussian distributions fail to represent. Prominent non-Gaussian frameworks for covariate distribution modelling are copula-based models and models based on multiple imputation by chained equations (MICE). While both frameworks have already found applications in the life sciences, a systematic investigation of their goodness-of-fit to the theoretical underlying distribution, indicating strengths and weaknesses under different conditions, is still lacking. To bridge this gap, we thoroughly evaluated covariate distribution models in terms of Kullback-Leibler divergence (KL-D), a scale-invariant information-theoretic goodness-of-fit criterion for distributions. Methodologically, we proposed a new approach to construct confidence intervals for KL-D by combining nearest neighbour-based KL-D estimators with subsampling-based uncertainty quantification. In relevant data sets of different sizes and dimensionalities with both continuous and discrete covariates, non-Gaussian models showed consistent improvements in KL-D, compared to simpler Gaussian or scale transform approximations. KL-D estimates were also robust to the inclusion of latent variables and large fractions of missing values. While good generalization behaviour to new data could be seen in copula-based models, MICE shows a trend for overfitting and its performance should always be evaluated on separate test data. Parametric copula models and MICE were found to scale much better with the dataset dimension than nonparametric copula models. These findings corroborate the potential of non-Gaussian models for modelling realistic life science covariate distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10611v2</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10928-025-09968-5</arxiv:DOI>
      <dc:creator>Niklas Hartung, Aleksandra Khatova</dc:creator>
    </item>
    <item>
      <title>A Metric-based Principal Curve Approach for Learning One-dimensional Manifold</title>
      <link>https://arxiv.org/abs/2405.12390</link>
      <description>arXiv:2405.12390v4 Announce Type: replace-cross 
Abstract: Principal curve is a well-known statistical method oriented in manifold learning using concepts from differential geometry. In this paper, we propose a novel metric-based principal curve (MPC) method that learns one-dimensional manifold of spatial data. Synthetic datasets Real applications using MNIST dataset show that our method can learn the one-dimensional manifold well in terms of the shape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12390v4</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eliuvish Cuicizion</dc:creator>
    </item>
  </channel>
</rss>
