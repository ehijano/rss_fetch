<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 May 2024 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Generative AI: The power of the new education</title>
      <link>https://arxiv.org/abs/2405.13487</link>
      <description>arXiv:2405.13487v1 Announce Type: new 
Abstract: The effective integration of generative artificial intelligence in education is a fundamental aspect to prepare future generations. This study proposes an accelerated learning methodology in artificial intelligence, focused on its generative capacity, as a way to achieve this goal. It recognizes the challenge of getting teachers to engage with new technologies and adapt their methods in all subjects, not just those related to AI. This methodology not only promotes interest in science, technology, engineering and mathematics, but also facilitates student understanding of the ethical uses and risks associated with AI. Students' perceptions of generative AI are examined, addressing their emotions towards its evolution, evaluation of its ethical implications, and everyday use of AI tools. In addition, AI applications commonly used by students and their integration into other disciplines are investigated. The study aims to provide educators with a deeper understanding of students' perceptions of AI and its relevance in society and in their future career paths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13487v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sergio Altares-L\'opez, Jos\'e M. Bengochea-Guevara, Carlos Ranz, H\'ector Montes, Angela Ribeiro</dc:creator>
    </item>
    <item>
      <title>From the evolution of public data ecosystems to the evolving horizons of the forward-looking intelligent public data ecosystem empowered by emerging technologies</title>
      <link>https://arxiv.org/abs/2405.13606</link>
      <description>arXiv:2405.13606v1 Announce Type: new 
Abstract: Public data ecosystems (PDEs) represent complex socio-technical systems crucial for optimizing data use in the public sector and outside it. Recognizing their multifaceted nature, previous research pro-posed a six-generation Evolutionary Model of Public Data Ecosystems (EMPDE). Designed as a result of a systematic literature review on the topic spanning three decade, this model, while theoretically robust, necessitates empirical validation to enhance its practical applicability. This study addresses this gap by validating the theoretical model through a real-life examination in five European countries - Latvia, Serbia, Czech Republic, Spain, and Poland. This empirical validation provides insights into PDEs dynamics and variations of implementations across contexts, particularly focusing on the 6th generation of forward-looking PDE generation named "Intelligent Public Data Generation" that represents a paradigm shift driven by emerging technologies such as cloud computing, Artificial Intelligence, Natural Language Processing tools, Generative AI, and Large Language Models (LLM) with potential to contribute to both automation and augmentation of business processes within these ecosystems. By transcending their traditional status as a mere component, evolving into both an actor and a stakeholder simultaneously, these technologies catalyze innovation and progress, enhancing PDE management strategies to align with societal, regulatory, and technical imperatives in the digital era.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13606v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>In: Janssen, M, J. Crompvoets, J. Ramon Gil-Garcia, H. Lee, I Lindgren, A Nikiforova, G. Viale Pereira. Electronic Government. EGOV 2024. Lecture Notes in Computer Science, Springer, Cham</arxiv:journal_reference>
      <dc:creator>Anastasija Nikiforova, Martin Lnenicka, Petar Mili\'c, Mariusz Luterek, Manuel Pedro Rodr\'iguez Bol\'ivar</dc:creator>
    </item>
    <item>
      <title>A framework for expected capability sets</title>
      <link>https://arxiv.org/abs/2405.13647</link>
      <description>arXiv:2405.13647v1 Announce Type: new 
Abstract: This paper addresses decision-aiding problems that involve multiple objectives and uncertain states of the world. Inspired by the capability approach, we focus on cases where a policy maker chooses an act that, combined with a state of the world, leads to a set of choices for citizens. While no preferential information is available to construct importance parameters for the criteria, we can obtain likelihoods for the different states. To effectively support decision-aiding in this context, we propose two procedures that merge the potential set of choices for each state of the world taking into account their respective likelihoods. Our procedures satisfy several fundamental and desirable properties that characterize the outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13647v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Fayard, David R\'ios Insua, Alexis Tsouki\`as</dc:creator>
    </item>
    <item>
      <title>Towards A Comprehensive Assessment of AI's Environmental Impact</title>
      <link>https://arxiv.org/abs/2405.14004</link>
      <description>arXiv:2405.14004v1 Announce Type: new 
Abstract: Artificial Intelligence, machine learning (AI/ML) has allowed exploring solutions for a variety of environmental and climate questions ranging from natural disasters, greenhouse gas emission, monitoring biodiversity, agriculture, to weather and climate modeling, enabling progress towards climate change mitigation. However, the intersection of AI/ML and environment is not always positive. The recent surge of interest in ML, made possible by processing very large volumes of data, fueled by access to massive compute power, has sparked a trend towards large-scale adoption of AI/ML. This interest places tremendous pressure on natural resources, that are often overlooked and under-reported. There is a need for a framework that monitors the environmental impact and degradation from AI/ML throughout its lifecycle for informing policymakers, stakeholders to adequately implement standards and policies and track the policy outcome over time. For these policies to be effective, AI's environmental impact needs to be monitored in a spatially-disaggregated, timely manner across the globe at the key activity sites. This study proposes a methodology to track environmental variables relating to the multifaceted impact of AI around datacenters using openly available energy data and globally acquired satellite observations. We present a case study around Northern Virginia, United States that hosts a growing number of datacenters and observe changes in multiple satellite-based environmental metrics. We then discuss the steps to expand this methodology for comprehensive assessment of AI's environmental impact across the planet. We also identify data gaps and formulate recommendations for improving the understanding and monitoring AI-induced changes to the environment and climate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14004v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Srija Chakraborty</dc:creator>
    </item>
    <item>
      <title>Attitudes Towards Migration in a COVID-19 Context: Testing a Behavioral Immune System Hypothesis with Twitter Data</title>
      <link>https://arxiv.org/abs/2405.14043</link>
      <description>arXiv:2405.14043v1 Announce Type: new 
Abstract: The COVID-19 outbreak implied many changes in the daily life of most of the world's population for a long time, prompting severe restrictions on sociality. The Behavioral Immune System (BIS) suggests that when facing pathogens, a psychological mechanism would be activated that, among other things, would generate an increase in prejudice and discrimination towards marginalized groups, including immigrants. This study aimed to test if people tend to enhance their rejection of minorities and foreign groups under the threat of contagious diseases, using the users' attitudes towards migrants in Twitter data from Chile, for pre-pandemic and pandemic contexts. Our results only partially support the BIS hypothesis, since threatened users increased their tweet production in the pandemic period, compared to empathetic users, but the latter grew in number and also increased the reach of their tweets between the two periods. We also found differences in the use of language between these types of users. Alternative explanations for these results may be context-dependent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14043v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yerka Freire-Vidal, Gabriela Fajardo, Carlos Rodr\'iguez-Sickert, Eduardo Graells-Garrido, Jos\'e Antonio Mu\~noz-Reyes, Oriana Figueroa</dc:creator>
    </item>
    <item>
      <title>Desirable Characteristics for AI Teaching Assistants in Programming Education</title>
      <link>https://arxiv.org/abs/2405.14178</link>
      <description>arXiv:2405.14178v1 Announce Type: new 
Abstract: Providing timely and personalized feedback to large numbers of students is a long-standing challenge in programming courses. Relying on human teaching assistants (TAs) has been extensively studied, revealing a number of potential shortcomings. These include inequitable access for students with low confidence when needing support, as well as situations where TAs provide direct solutions without helping students to develop their own problem-solving skills. With the advent of powerful large language models (LLMs), digital teaching assistants configured for programming contexts have emerged as an appealing and scalable way to provide instant, equitable, round-the-clock support. Although digital TAs can provide a variety of help for programming tasks, from high-level problem solving advice to direct solution generation, the effectiveness of such tools depends on their ability to promote meaningful learning experiences. If students find the guardrails implemented in digital TAs too constraining, or if other expectations are not met, they may seek assistance in ways that do not help them learn. Thus, it is essential to identify the features that students believe make digital teaching assistants valuable. We deployed an LLM-powered digital assistant in an introductory programming course and collected student feedback ($n=813$) on the characteristics of the tool they perceived to be most important. Our results highlight that students value such tools for their ability to provide instant, engaging support, particularly during peak times such as before assessment deadlines. They also expressed a strong preference for features that enable them to retain autonomy in their learning journey, such as scaffolding that helps to guide them through problem-solving steps rather than simply being shown direct solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14178v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3649217.3653574</arxiv:DOI>
      <dc:creator>Paul Denny, Stephen MacNeil, Jaromir Savelka, Leo Porter, Andrew Luxton-Reilly</dc:creator>
    </item>
    <item>
      <title>SmartCS: Enabling the Creation of ML-Powered Computer Vision Mobile Apps for Citizen Science Applications without Coding</title>
      <link>https://arxiv.org/abs/2405.14323</link>
      <description>arXiv:2405.14323v1 Announce Type: new 
Abstract: It is undeniable that citizen science contributes to the advancement of various fields of study. There are now software tools that facilitate the development of citizen science apps. However, apps developed with these tools rely on individual human skills to correctly collect useful data. Machine learning (ML)-aided apps provide on-field guidance to citizen scientists on data collection tasks. However, these apps rely on server-side ML support, and therefore need a reliable internet connection. Furthermore, the development of citizen science apps with ML support requires a significant investment of time and money. For some projects, this barrier may preclude the use of citizen science effectively. We present a platform that democratizes citizen science by making it accessible to a much broader audience of both researchers and participants. The SmartCS platform allows one to create citizen science apps with ML support quickly and without coding skills. Apps developed using SmartCS have client-side ML support, making them usable in the field, even when there is no internet connection. The client-side ML helps educate users to better recognize the subjects, thereby enabling high-quality data collection. We present several citizen science apps created using SmartCS, some of which were conceived and created by high school students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14323v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fahim Hasan Khan, Akila de Silva, Gregory Dusek, James Davis, Alex Pang</dc:creator>
    </item>
    <item>
      <title>Expert exploranation for communicating scientific methods -- A case study in conflict research</title>
      <link>https://arxiv.org/abs/2405.14345</link>
      <description>arXiv:2405.14345v1 Announce Type: new 
Abstract: Science communication aims at making key research insights accessible to the broad public. If explanatory and exploratory visualization techniques are combined to do so, the approach is also referred to as exploranation. In this context, the audience is usually not required to have domain expertise. However, we show that exploranation can not only support the communication between researchers and a broad audience, but also between researchers directly. With the goal of communicating an existing method for conducting causal inference on spatio-temporal conflict event data, we investigated how to perform exploranation for experts, i.e., expert exploranation. Based on application scenarios of the inference method, we developed three versions of an interactive visual story to explain the method to conflict researchers. We abstracted the corresponding design process and evaluated the stories both with experts who were unfamiliar with the explained method and experts who were already familiar with it. The positive and extensive feedback from the evaluation shows that expert exploranation is a promising direction for visual storytelling, as it can help to improve scientific outreach, methodological understanding, and accessibility for researchers new to a field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14345v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cag.2024.103937</arxiv:DOI>
      <arxiv:journal_reference>Computers &amp; Graphics 120: 103937 (2024)</arxiv:journal_reference>
      <dc:creator>Benedikt Mayer, Karsten Donnay, Kai Lawonn, Bernhard Preim, Monique Meuschke</dc:creator>
    </item>
    <item>
      <title>Push and Pull: A Framework for Measuring Attentional Agency</title>
      <link>https://arxiv.org/abs/2405.14614</link>
      <description>arXiv:2405.14614v1 Announce Type: new 
Abstract: We propose a framework for measuring attentional agency - the ability to allocate one's attention according to personal desires, goals, and intentions - on digital platforms. Platforms extend people's limited powers of attention by extrapolating their preferences to large collections of previously unconsidered informational objects. However, platforms typically also allow people to influence one another's attention. We introduce a formal framework for measuring how much a given platform empowers people to both pull information into their own attentional field and push information into the attentional fields of others. We also use these definitions to shed light on the implications of generative foundation models, which enable users to bypass the implicit "attentional bargain" that underlies embedded advertising and other methods for capturing economic value from informational goods. We conclude with a set of policy strategies that can be used to understand and reshape the distribution of attentional agency online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14614v1</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.IR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zachary Wojtowicz, Shrey Jain, Nicholas Vincent</dc:creator>
    </item>
    <item>
      <title>Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View</title>
      <link>https://arxiv.org/abs/2405.14744</link>
      <description>arXiv:2405.14744v1 Announce Type: new 
Abstract: Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM agents remains under-explored. As LLM Agents are increasingly employed in intricate social environments, a pressing and natural question emerges: Can LLM Agents leverage hallucinations to mirror human cognitive biases, thus exhibiting irrational social intelligence? In this paper, we probe the irrational behavior among contemporary LLM agents by melding practical social science experiments with theoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM Agents framework that utilizes hallucination properties to assess and enhance LLM Agents' social intelligence through cognitive biases. Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities, and highlighting the significance of hallucination properties. Additionally, CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14744v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuan Liu, Jie Zhang, Song Guo, Haoyang Shang, Chengxu Yang, Quanyan Zhu</dc:creator>
    </item>
    <item>
      <title>As an AI Language Model, "Yes I Would Recommend Calling the Police'': Norm Inconsistency in LLM Decision-Making</title>
      <link>https://arxiv.org/abs/2405.14812</link>
      <description>arXiv:2405.14812v1 Announce Type: new 
Abstract: We investigate the phenomenon of norm inconsistency: where LLMs apply different norms in similar situations. Specifically, we focus on the high-risk application of deciding whether to call the police in Amazon Ring home surveillance videos. We evaluate the decisions of three state-of-the-art LLMs -- GPT-4, Gemini 1.0, and Claude 3 Sonnet -- in relation to the activities portrayed in the videos, the subjects' skin-tone and gender, and the characteristics of the neighborhoods where the videos were recorded. Our analysis reveals significant norm inconsistencies: (1) a discordance between the recommendation to call the police and the actual presence of criminal activity, and (2) biases influenced by the racial demographics of the neighborhoods. These results highlight the arbitrariness of model decisions in the surveillance context and the limitations of current bias detection and mitigation strategies in normative decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14812v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shomik Jain, D Calacci, Ashia Wilson</dc:creator>
    </item>
    <item>
      <title>Large Language Models Can Infer Personality from Free-Form User Interactions</title>
      <link>https://arxiv.org/abs/2405.13052</link>
      <description>arXiv:2405.13052v1 Announce Type: cross 
Abstract: This study investigates the capacity of Large Language Models (LLMs) to infer the Big Five personality traits from free-form user interactions. The results demonstrate that a chatbot powered by GPT-4 can infer personality with moderate accuracy, outperforming previous approaches drawing inferences from static text content. The accuracy of inferences varied across different conversational settings. Performance was highest when the chatbot was prompted to elicit personality-relevant information from users (mean r=.443, range=[.245, .640]), followed by a condition placing greater emphasis on naturalistic interaction (mean r=.218, range=[.066, .373]). Notably, the direct focus on personality assessment did not result in a less positive user experience, with participants reporting the interactions to be equally natural, pleasant, engaging, and humanlike across both conditions. A chatbot mimicking ChatGPT's default behavior of acting as a helpful assistant led to markedly inferior personality inferences and lower user experience ratings but still captured psychologically meaningful information for some of the personality traits (mean r=.117, range=[-.004, .209]). Preliminary analyses suggest that the accuracy of personality inferences varies only marginally across different socio-demographic subgroups. Our results highlight the potential of LLMs for psychological profiling based on conversational interactions. We discuss practical implications and ethical challenges associated with these findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13052v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heinrich Peters, Moran Cerf, Sandra C. Matz</dc:creator>
    </item>
    <item>
      <title>The AI Community Building the Future? A Quantitative Analysis of Development Activity on Hugging Face Hub</title>
      <link>https://arxiv.org/abs/2405.13058</link>
      <description>arXiv:2405.13058v1 Announce Type: cross 
Abstract: Open source developers have emerged as key actors in the political economy of artificial intelligence (AI), with open model development being recognised as an alternative to closed-source AI development. However, we still have a limited understanding of collaborative practices in open source AI. This paper responds to this gap with a three-part quantitative analysis of development activity on the Hugging Face (HF) Hub, a popular platform for building, sharing, and demonstrating models. First, we find that various types of activity across 348,181 model, 65,761 dataset, and 156,642 space repositories exhibit right-skewed distributions. Activity is extremely imbalanced between repositories; for example, over 70% of models have 0 downloads, while 1% account for 99% of downloads. Second, we analyse a snapshot of the social network structure of collaboration on models, finding that the community has a core-periphery structure, with a core of prolific developers and a majority of isolate developers (89%). Upon removing isolates, collaboration is characterised by high reciprocity regardless of developers' network positions. Third, we examine model adoption through the lens of model usage in spaces, finding that a minority of models, developed by a handful of companies, are widely used on the HF Hub. Overall, we find that various types of activity on the HF Hub are characterised by Pareto distributions, congruent with prior observations about OSS development patterns on platforms like GitHub. We conclude with a discussion of the implications of the findings and recommendations for (open source) AI researchers, developers, and policymakers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13058v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cailean Osborne, Jennifer Ding, Hannah Rose Kirk</dc:creator>
    </item>
    <item>
      <title>Exploring Teachers' Perception of Artificial Intelligence: The Socio-emotional Deficiency as Opportunities and Challenges in Human-AI Complementarity in K-12 Education</title>
      <link>https://arxiv.org/abs/2405.13065</link>
      <description>arXiv:2405.13065v1 Announce Type: cross 
Abstract: In schools, teachers play a multitude of roles, serving as educators, counselors, decision-makers, and members of the school community. With recent advances in artificial intelligence (AI), there is increasing discussion about how AI can assist, complement, and collaborate with teachers. To pave the way for better teacher-AI complementary relationships in schools, our study aims to expand the discourse on teacher-AI complementarity by seeking educators' perspectives on the potential strengths and limitations of AI across a spectrum of responsibilities. Through a mixed method using a survey with 100 elementary school teachers in South Korea and in-depth interviews with 12 teachers, our findings indicate that teachers anticipate AI's potential to complement human teachers by automating administrative tasks and enhancing personalized learning through advanced intelligence. Interestingly, the deficit of AI's socio-emotional capabilities has been perceived as both challenges and opportunities. Overall, our study demonstrates the nuanced perception of teachers and different levels of expectations over their roles, challenging the need for decisions about AI adoption tailored to educators' preferences and concerns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13065v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soon-young Oh, Yongsu Ahn</dc:creator>
    </item>
    <item>
      <title>FairLENS: Assessing Fairness in Law Enforcement Speech Recognition</title>
      <link>https://arxiv.org/abs/2405.13166</link>
      <description>arXiv:2405.13166v1 Announce Type: cross 
Abstract: Automatic speech recognition (ASR) techniques have become powerful tools, enhancing efficiency in law enforcement scenarios. To ensure fairness for demographic groups in different acoustic environments, ASR engines must be tested across a variety of speakers in realistic settings. However, describing the fairness discrepancies between models with confidence remains a challenge. Meanwhile, most public ASR datasets are insufficient to perform a satisfying fairness evaluation. To address the limitations, we built FairLENS - a systematic fairness evaluation framework. We propose a novel and adaptable evaluation method to examine the fairness disparity between different models. We also collected a fairness evaluation dataset covering multiple scenarios and demographic dimensions. Leveraging this framework, we conducted fairness assessments on 1 open-source and 11 commercially available state-of-the-art ASR models. Our results reveal that certain models exhibit more biases than others, serving as a fairness guideline for users to make informed choices when selecting ASR models for a given real-world scenario. We further explored model biases towards specific demographic groups and observed that shifts in the acoustic domain can lead to the emergence of new biases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13166v1</guid>
      <category>eess.AS</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yicheng Wang, Mark Cusick, Mohamed Laila, Kate Puech, Zhengping Ji, Xia Hu, Michael Wilson, Noah Spitzer-Williams, Michael Wheeler, Yasser Ibrahim</dc:creator>
    </item>
    <item>
      <title>A Multilingual Similarity Dataset for News Article Frame</title>
      <link>https://arxiv.org/abs/2405.13272</link>
      <description>arXiv:2405.13272v1 Announce Type: cross 
Abstract: Understanding the writing frame of news articles is vital for addressing social issues, and thus has attracted notable attention in the fields of communication studies. Yet, assessing such news article frames remains a challenge due to the absence of a concrete and unified standard dataset that considers the comprehensive nuances within news content.
  To address this gap, we introduce an extended version of a large labeled news article dataset with 16,687 new labeled pairs. Leveraging the pairwise comparison of news articles, our method frees the work of manual identification of frame classes in traditional news frame analysis studies. Overall we introduce the most extensive cross-lingual news article similarity dataset available to date with 26,555 labeled news article pairs across 10 languages. Each data point has been meticulously annotated according to a codebook detailing eight critical aspects of news content, under a human-in-the-loop framework. Application examples demonstrate its potential in unearthing country communities within global news coverage, exposing media bias among news outlets, and quantifying the factors related to news creation. We envision that this news similarity dataset will broaden our understanding of the media ecosystem in terms of news coverage of events and perspectives across countries, locations, languages, and other social constructs. By doing so, it can catalyze advancements in social science research and applied methodologies, thereby exerting a profound impact on our society.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13272v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Chen, Mattia Samory, Scott Hale, David Jurgens, Przemyslaw A. Grabowicz</dc:creator>
    </item>
    <item>
      <title>What is a typical signalized intersection in a city? A pipeline for intersection data imputation from OpenStreetMap</title>
      <link>https://arxiv.org/abs/2405.13480</link>
      <description>arXiv:2405.13480v1 Announce Type: cross 
Abstract: Signalized intersections, arguably the most complicated type of traffic scenario, are essential to urban mobility systems. With recent advancements in intelligent transportation technologies, signalized intersections have great prospects for making transportation greener, safer, and faster. Several studies have been conducted focusing on intersection-level control and optimization. However, arbitrarily structured signalized intersections that are often used do not represent the ground-truth distribution, and there is no standardized way that exists to extract information about real-world signalized intersections. As the largest open-source map in the world, OpenStreetMap (OSM) has been used by many transportation researchers for a variety of studies, including intersection-level research such as adaptive traffic signal control and eco-driving. However, the quality of OSM data has been a serious concern.
  In this paper, we propose a pipeline for effectively extracting information about signalized intersections from OSM and constructing a comprehensive dataset. We thoroughly discuss challenges related to this task and we propose our solution for each challenge. We also use Salt Lake City as an example to demonstrate the performance of our methods. The pipeline has been published as an open-source Python library so everyone can freely download and use it to facilitate their research. Hopefully, this paper can serve as a starting point that inspires more efforts to build a standardized and systematic data pipeline for various types of transportation problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13480v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ao Qu, Anirudh Valiveru, Catherine Tang, Vindula Jayawardana, Baptiste Freydt, Cathy Wu</dc:creator>
    </item>
    <item>
      <title>What Do Privacy Advertisements Communicate to Consumers?</title>
      <link>https://arxiv.org/abs/2405.13857</link>
      <description>arXiv:2405.13857v1 Announce Type: cross 
Abstract: When companies release marketing materials aimed at promoting their privacy practices or highlighting specific privacy features, what do they actually communicate to consumers? In this paper, we explore the impact of privacy marketing materials on: (1) consumers' attitude towards the organizations providing the campaigns, (2) overall privacy awareness, and (3) the actionability of suggested privacy advice. To this end, we investigated the impact of four privacy advertising videos and one privacy game published by five different technology companies. We conducted 24 semi-structured interviews with participants randomly assigned to view one or two of the videos or play the game. Our findings suggest that awareness of privacy features can contribute to positive perceptions of a company or its products. The ads we tested were more successful in communicating the advertised privacy features than the game we tested. We observed that advertising a single privacy feature using a single metaphor in a short ad increased awareness of the advertised feature. The game failed to communicate privacy features or motivate study participants to use the features. Our results also suggest that privacy campaigns can be useful for raising awareness about privacy features and improving brand image, but may not be the most effective way to teach viewers how to use privacy features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13857v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoxin Shen, Eman Alashwali, Lorrie Faith Cranor</dc:creator>
    </item>
    <item>
      <title>Fairness Hub Technical Briefs: Definition and Detection of Distribution Shift</title>
      <link>https://arxiv.org/abs/2405.14186</link>
      <description>arXiv:2405.14186v1 Announce Type: cross 
Abstract: Distribution shift is a common situation in machine learning tasks, where the data used for training a model is different from the data the model is applied to in the real world. This issue arises across multiple technical settings: from standard prediction tasks, to time-series forecasting, and to more recent applications of large language models (LLMs). This mismatch can lead to performance reductions, and can be related to a multiplicity of factors: sampling issues and non-representative data, changes in the environment or policies, or the emergence of previously unseen scenarios. This brief focuses on the definition and detection of distribution shifts in educational settings. We focus on standard prediction problems, where the task is to learn a model that takes in a series of input (predictors) $X=(x_1,x_2,...,x_m)$ and produces an output $Y=f(X)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14186v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nicolas Acevedo, Carmen Cortez, Chris Brooks, Rene Kizilcec, Renzhe Yu</dc:creator>
    </item>
    <item>
      <title>Explainable Few-shot Knowledge Tracing</title>
      <link>https://arxiv.org/abs/2405.14391</link>
      <description>arXiv:2405.14391v1 Announce Type: cross 
Abstract: Knowledge tracing (KT), aiming to mine students' mastery of knowledge by their exercise records and predict their performance on future test questions, is a critical task in educational assessment. While researchers achieved tremendous success with the rapid development of deep learning techniques, current knowledge tracing tasks fall into the cracks from real-world teaching scenarios. Relying heavily on extensive student data and solely predicting numerical performances differs from the settings where teachers assess students' knowledge state from limited practices and provide explanatory feedback. To fill this gap, we explore a new task formulation: Explainable Few-shot Knowledge Tracing. By leveraging the powerful reasoning and generation abilities of large language models (LLMs), we then propose a cognition-guided framework that can track the student knowledge from a few student records while providing natural language explanations. Experimental results from three widely used datasets show that LLMs can perform comparable or superior to competitive deep knowledge tracing methods. We also discuss potential directions and call for future improvements in relevant topics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14391v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoxuan Li, Jifan Yu, Yuanxin Ouyang, Zhuang Liu, Wenge Rong, Juanzi Li, Zhang Xiong</dc:creator>
    </item>
    <item>
      <title>Qualifying and Quantifying the Benefits of Mindfulness Practices for IT Workers</title>
      <link>https://arxiv.org/abs/2405.14393</link>
      <description>arXiv:2405.14393v1 Announce Type: cross 
Abstract: The well-being and productivity of IT workers are crucial for both individual success and the overall prosperity of the organisations they serve. This study proposes mindfulness to alleviate stress and improve mental well-being for IT workers. During an 8-week program, IT workers learn about mindfulness, coupled with breathing practices. This study investigates the potential effects of these practices by analysing participants' reflections through thematic analysis and daily well-being ratings. The analysis showcased an increase in mental well-being and perceived productivity. It also indicated a change in the participants' perception, which showed increased self-awareness. The study recommends continuing the program in the industry to see its impact on work outputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14393v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cristina Martinez Montes, Fredrik Sj\"ogren, Adam Klevfors, Birgit Penzenstadler</dc:creator>
    </item>
    <item>
      <title>Endowing Interpretability for Neural Cognitive Diagnosis by Efficient Kolmogorov-Arnold Networks</title>
      <link>https://arxiv.org/abs/2405.14399</link>
      <description>arXiv:2405.14399v1 Announce Type: cross 
Abstract: In the realm of intelligent education, cognitive diagnosis plays a crucial role in subsequent recommendation tasks attributed to the revealed students' proficiency in knowledge concepts. Although neural network-based neural cognitive diagnosis models (CDMs) have exhibited significantly better performance than traditional models, neural cognitive diagnosis is criticized for the poor model interpretability due to the multi-layer perception (MLP) employed, even with the monotonicity assumption. Therefore, this paper proposes to empower the interpretability of neural cognitive diagnosis models through efficient kolmogorov-arnold networks (KANs), named KAN2CD, where KANs are designed to enhance interpretability in two manners. Specifically, in the first manner, KANs are directly used to replace the used MLPs in existing neural CDMs; while in the second manner, the student embedding, exercise embedding, and concept embedding are directly processed by several KANs, and then their outputs are further combined and learned in a unified KAN to get final predictions. To overcome the problem of training KANs slowly, we modify the implementation of original KANs to accelerate the training. Experiments on four real-world datasets show that the proposed KA2NCD exhibits better performance than traditional CDMs, and the proposed KA2NCD still has a bit of performance leading even over the existing neural CDMs. More importantly, the learned structures of KANs enable the proposed KA2NCD to hold as good interpretability as traditional CDMs, which is superior to existing neural CDMs. Besides, the training cost of the proposed KA2NCD is competitive to existing models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14399v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shangshang Yang, Linrui Qin, Xiaoshan Yu</dc:creator>
    </item>
    <item>
      <title>Unraveling overoptimism and publication bias in ML-driven science</title>
      <link>https://arxiv.org/abs/2405.14422</link>
      <description>arXiv:2405.14422v1 Announce Type: cross 
Abstract: Machine Learning (ML) is increasingly used across many disciplines with impressive reported results across many domain areas. However, recent studies suggest that the published performance of ML models are often overoptimistic and not reflective of true accuracy were these models to be deployed. Validity concerns are underscored by findings of a concerning inverse relationship between sample size and reported accuracy in published ML models across several domains. This is in contrast with the theory of learning curves in ML, where we expect accuracy to improve or stay the same with increasing sample size. This paper investigates the factors contributing to overoptimistic accuracy reports in ML-based science, focusing on data leakage and publication bias. Our study introduces a novel stochastic model for observed accuracy, integrating parametric learning curves and the above biases. We then construct an estimator based on this model that corrects for these biases in observed data. Theoretical and empirical results demonstrate that this framework can estimate the underlying learning curve that gives rise to the observed overoptimistic results, thereby providing more realistic performance assessments of ML performance from a collection of published results. We apply the model to various meta-analyses in the digital health literature, including neuroimaging-based and speech-based classifications of several neurological conditions. Our results indicate prevalent overoptimism across these fields and we estimate the inherent limits of ML-based prediction in each domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14422v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pouria Saidi, Gautam Dasarathy, Visar Berisha</dc:creator>
    </item>
    <item>
      <title>Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models</title>
      <link>https://arxiv.org/abs/2405.14555</link>
      <description>arXiv:2405.14555v1 Announce Type: cross 
Abstract: Research on Large Language Models (LLMs) has often neglected subtle biases that, although less apparent, can significantly influence the models' outputs toward particular social narratives. This study addresses two such biases within LLMs: \textit{representative bias}, which denotes a tendency of LLMs to generate outputs that mirror the experiences of certain identity groups, and \textit{affinity bias}, reflecting the models' evaluative preferences for specific narratives or viewpoints. We introduce two novel metrics to measure these biases: the Representative Bias Score (RBS) and the Affinity Bias Score (ABS), and present the Creativity-Oriented Generation Suite (CoGS), a collection of open-ended tasks such as short story writing and poetry composition, designed with customized rubrics to detect these subtle biases. Our analysis uncovers marked representative biases in prominent LLMs, with a preference for identities associated with being white, straight, and men. Furthermore, our investigation of affinity bias reveals distinctive evaluative patterns within each model, akin to `bias fingerprints'. This trend is also seen in human evaluators, highlighting a complex interplay between human and machine bias perceptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14555v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhishek Kumar, Sarfaroz Yunusov, Ali Emami</dc:creator>
    </item>
    <item>
      <title>Implicit Personalization in Language Models: A Systematic Study</title>
      <link>https://arxiv.org/abs/2405.14808</link>
      <description>arXiv:2405.14808v1 Announce Type: cross 
Abstract: Implicit Personalization (IP) is a phenomenon of language models inferring a user's background from the implicit cues in the input prompts and tailoring the response based on this inference. While previous work has touched upon various instances of this problem, there lacks a unified framework to study this behavior. This work systematically studies IP through a rigorous mathematical formulation, a multi-perspective moral reasoning framework, and a set of case studies. Our theoretical foundation for IP relies on a structural causal model and introduces a novel method, indirect intervention, to estimate the causal effect of a mediator variable that cannot be directly intervened upon. Beyond the technical approach, we also introduce a set of moral reasoning principles based on three schools of moral philosophy to study when IP may or may not be ethically appropriate. Equipped with both mathematical and ethical insights, we present three diverse case studies illustrating the varied nature of the IP problem and offer recommendations for future research. Our code and data are at https://github.com/jiarui-liu/IP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14808v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zhijing Jin, Nils Heil, Jiarui Liu, Shehzaad Dhuliawala, Yahang Qi, Bernhard Sch\"olkopf, Rada Mihalcea, Mrinmaya Sachan</dc:creator>
    </item>
    <item>
      <title>Managing extreme AI risks amid rapid progress</title>
      <link>https://arxiv.org/abs/2310.17688</link>
      <description>arXiv:2310.17688v3 Announce Type: replace 
Abstract: Artificial Intelligence (AI) is progressing rapidly, and companies are shifting their focus to developing generalist AI systems that can autonomously act and pursue goals. Increases in capabilities and autonomy may soon massively amplify AI's impact, with risks that include large-scale social harms, malicious uses, and an irreversible loss of human control over autonomous AI systems. Although researchers have warned of extreme risks from AI, there is a lack of consensus about how exactly such risks arise, and how to manage them. Society's response, despite promising first steps, is incommensurate with the possibility of rapid, transformative progress that is expected by many experts. AI safety research is lagging. Present governance initiatives lack the mechanisms and institutions to prevent misuse and recklessness, and barely address autonomous systems. In this short consensus paper, we describe extreme risks from upcoming, advanced AI systems. Drawing on lessons learned from other safety-critical technologies, we then outline a comprehensive plan combining technical research and development with proactive, adaptive governance mechanisms for a more commensurate preparation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17688v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1126/science.adn0117</arxiv:DOI>
      <dc:creator>Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Trevor Darrell, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Hadfield, Jeff Clune, Tegan Maharaj, Frank Hutter, At{\i}l{\i}m G\"une\c{s} Baydin, Sheila McIlraith, Qiqi Gao, Ashwin Acharya, David Krueger, Anca Dragan, Philip Torr, Stuart Russell, Daniel Kahneman, Jan Brauner, S\"oren Mindermann</dc:creator>
    </item>
    <item>
      <title>Decoding Social Sentiment in DAO: A Comparative Analysis of Blockchain Governance Communities</title>
      <link>https://arxiv.org/abs/2311.14676</link>
      <description>arXiv:2311.14676v2 Announce Type: replace 
Abstract: Blockchain technology is leading a revolutionary transformation across diverse industries, with effective governance being critical for the success and sustainability of blockchain projects. Community forums, pivotal in engaging decentralized autonomous organizations (DAOs), significantly impact blockchain governance decisions. Concurrently, Natural Language Processing (NLP), particularly sentiment analysis, provides powerful insights from textual data. While prior research has explored the potential of NLP tools in social media sentiment analysis, there is a gap in understanding the sentiment landscape of blockchain governance communities. The evolving discourse and sentiment dynamics on the forums of top DAOs remain largely unknown. This paper delves deep into the evolving discourse and sentiment dynamics on the public forums of leading DeFi projects: Aave, Uniswap, Curve DAO, Yearn.finance, Merit Circle, and Balancer, focusing primarily on discussions related to governance issues. Our study shows that participants in decentralized communities generally express positive sentiments during Discord discussions. Furthermore, there is a potential interaction between discussion intensity and sentiment dynamics; higher discussion volume may contribute to a more stable sentiment from code analysis. The insights gained from this study are valuable for decision-makers in blockchain governance, underscoring the pivotal role of sentiment analysis in interpreting community emotions and its evolving impact on the landscape of blockchain governance. This research significantly contributes to the interdisciplinary exploration of the intersection of blockchain and society, specifically emphasizing the decentralized blockchain governance ecosystem. We provide our data and code for replicability as open access on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.14676v2</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.31219/osf.io/bq6tu</arxiv:DOI>
      <dc:creator>Yutong Quan, Xintong Wu, Wanlin Deng, Luyao Zhang</dc:creator>
    </item>
    <item>
      <title>Identifying and Improving Disability Bias in GPT-Based Resume Screening</title>
      <link>https://arxiv.org/abs/2402.01732</link>
      <description>arXiv:2402.01732v2 Announce Type: replace 
Abstract: As Generative AI rises in adoption, its use has expanded to include domains such as hiring and recruiting. However, without examining the potential of bias, this may negatively impact marginalized populations, including people with disabilities. To address this important concern, we present a resume audit study, in which we ask ChatGPT (specifically, GPT-4) to rank a resume against the same resume enhanced with an additional leadership award, scholarship, panel presentation, and membership that are disability related. We find that GPT-4 exhibits prejudice towards these enhanced CVs. Further, we show that this prejudice can be quantifiably reduced by training a custom GPTs on principles of DEI and disability justice. Our study also includes a unique qualitative analysis of the types of direct and indirect ableism GPT-4 uses to justify its biased decisions and suggest directions for additional bias mitigation work. Additionally, since these justifications are presumably drawn from training data containing real-world biased statements made by humans, our analysis suggests additional avenues for understanding and addressing human bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01732v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3630106.3658933</arxiv:DOI>
      <dc:creator>Kate Glazko, Yusuf Mohammed, Ben Kosa, Venkatesh Potluri, Jennifer Mankoff</dc:creator>
    </item>
    <item>
      <title>Measuring Implicit Bias in Explicitly Unbiased Large Language Models</title>
      <link>https://arxiv.org/abs/2402.04105</link>
      <description>arXiv:2402.04105v2 Announce Type: replace 
Abstract: Large language models (LLMs) can pass explicit social bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases. Measuring such implicit biases can be a challenge: as LLMs become increasingly proprietary, it may not be possible to access their embeddings and apply existing bias measures; furthermore, implicit biases are primarily a concern if they affect the actual decisions that these systems make. We address both challenges by introducing two new measures of bias: LLM Implicit Bias, a prompt-based method for revealing implicit bias; and LLM Decision Bias, a strategy to detect subtle discrimination in decision-making tasks. Both measures are based on psychological research: LLM Implicit Bias adapts the Implicit Association Test, widely used to study the automatic associations between concepts held in human minds; and LLM Decision Bias operationalizes psychological results indicating that relative evaluations between two candidates, not absolute evaluations assessing each independently, are more diagnostic of implicit biases. Using these measures, we found pervasive stereotype biases mirroring those in society in 8 value-aligned models across 4 social categories (race, gender, religion, health) in 21 stereotypes (such as race and criminality, race and weapons, gender and science, age and negativity). Our prompt-based LLM Implicit Bias measure correlates with existing language model embedding-based bias methods, but better predicts downstream behaviors measured by LLM Decision Bias. These new prompt-based measures draw from psychology's long history of research into measuring stereotype biases based on purely observable behavior; they expose nuanced biases in proprietary value-aligned LLMs that appear unbiased according to standard benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04105v2</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuechunzi Bai, Angelina Wang, Ilia Sucholutsky, Thomas L. Griffiths</dc:creator>
    </item>
    <item>
      <title>Open-Source Assessments of AI Capabilities: The Proliferation of AI Analysis Tools, Replicating Competitor Models, and the Zhousidun Dataset</title>
      <link>https://arxiv.org/abs/2405.12167</link>
      <description>arXiv:2405.12167v2 Announce Type: replace 
Abstract: The integration of artificial intelligence (AI) into military capabilities has become a norm for major military power across the globe. Understanding how these AI models operate is essential for maintaining strategic advantages and ensuring security. This paper demonstrates an open-source methodology for analyzing military AI models through a detailed examination of the Zhousidun dataset, a Chinese-originated dataset that exhaustively labels critical components on American and Allied destroyers. By demonstrating the replication of a state-of-the-art computer vision model on this dataset, we illustrate how open-source tools can be leveraged to assess and understand key military AI capabilities. This methodology offers a robust framework for evaluating the performance and potential of AI-enabled military capabilities, thus enhancing the accuracy and reliability of strategic assessments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12167v2</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ritwik Gupta, Leah Walker, Eli Glickman, Raine Koizumi, Sarthak Bhatnagar, Andrew W. Reddie</dc:creator>
    </item>
    <item>
      <title>Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation</title>
      <link>https://arxiv.org/abs/2009.09435</link>
      <description>arXiv:2009.09435v4 Announce Type: replace-cross 
Abstract: Bolukbasi et al. (2016) presents one of the first gender bias mitigation techniques for word representations. Their method takes pre-trained word representations as input and attempts to isolate a linear subspace that captures most of the gender bias in the representations. As judged by an analogical evaluation task, their method virtually eliminates gender bias in the representations. However, an implicit and untested assumption of their method is that the bias subspace is actually linear. In this work, we generalize their method to a kernelized, nonlinear version. We take inspiration from kernel principal component analysis and derive a nonlinear bias isolation technique. We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in word representations and analyze empirically whether the bias subspace is actually linear. Our analysis shows that gender bias is in fact well captured by a linear subspace, justifying the assumption of Bolukbasi et al. (2016).</description>
      <guid isPermaLink="false">oai:arXiv.org:2009.09435v4</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</arxiv:journal_reference>
      <dc:creator>Francisco Vargas, Ryan Cotterell</dc:creator>
    </item>
    <item>
      <title>Towards Realistic Mechanisms That Incentivize Federated Participation and Contribution</title>
      <link>https://arxiv.org/abs/2310.13681</link>
      <description>arXiv:2310.13681v3 Announce Type: replace-cross 
Abstract: Edge device participation in federating learning (FL) is typically studied through the lens of device-server communication (e.g., device dropout) and assumes an undying desire from edge devices to participate in FL. As a result, current FL frameworks are flawed when implemented in realistic settings, with many encountering the free-rider dilemma. In a step to push FL towards realistic settings, we propose RealFM: the first federated mechanism that (1) realistically models device utility, (2) incentivizes data contribution and device participation, (3) provably removes the free-rider dilemma, and (4) relaxes assumptions on data homogeneity and data sharing. Compared to previous FL mechanisms, RealFM allows for a non-linear relationship between model accuracy and utility, which improves the utility gained by the server and participating devices. On real-world data, RealFM improves device and server utility, as well as data contribution, by over 3 and 4 magnitudes respectively compared to baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13681v3</guid>
      <category>cs.GT</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>econ.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Bornstein, Amrit Singh Bedi, Anit Kumar Sahu, Furqan Khan, Furong Huang</dc:creator>
    </item>
    <item>
      <title>Large Language Models Help Reveal Unhealthy Diet and Body Concerns in Online Eating Disorders Communities</title>
      <link>https://arxiv.org/abs/2401.09647</link>
      <description>arXiv:2401.09647v2 Announce Type: replace-cross 
Abstract: Eating disorders (ED), a severe mental health condition with high rates of mortality and morbidity, affect millions of people globally, especially adolescents. The proliferation of online communities that promote and normalize ED has been linked to this public health crisis. However, identifying harmful communities is challenging due to the use of coded language and other obfuscations. To address this challenge, we propose a novel framework to surface implicit attitudes of online communities by adapting large language models (LLMs) to the language of the community. We describe an alignment method and evaluate results along multiple dimensions of semantics and affect. We then use the community-aligned LLM to respond to psychometric questionnaires designed to identify ED in individuals. We demonstrate that LLMs can effectively adopt community-specific perspectives and reveal significant variations in eating disorder risks in different online communities. These findings highlight the utility of LLMs to reveal implicit attitudes and collective mindsets of communities, offering new tools for mitigating harmful content on social media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09647v2</guid>
      <category>cs.SI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Minh Duc Chu, Zihao He, Rebecca Dorn, Kristina Lerman</dc:creator>
    </item>
    <item>
      <title>Causal Perception</title>
      <link>https://arxiv.org/abs/2401.13408</link>
      <description>arXiv:2401.13408v2 Announce Type: replace-cross 
Abstract: Perception occurs when two individuals interpret the same information differently. Despite being a known phenomenon with implications for bias in decision-making, as individual experience determines interpretation, perception remains largely overlooked in machine learning (ML) research. Modern decision flows, whether partially or fully automated, involve human experts interacting with ML applications. How might we then, e.g., account for two experts that interpret differently a deferred instance or an explanation from a ML model? To account for perception, we first need to formulate it. In this work, we define perception under causal reasoning using structural causal models (SCM). Our framework formalizes individual experience as additional causal knowledge that comes with and is used by a human expert (read, decision maker). We present two kinds of causal perception, unfaithful and inconsistent, based on the SCM properties of faithfulness and consistency. Further, we motivate the importance of perception within fairness problems. We illustrate our framework through a series of decision flow examples involving ML applications and human experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13408v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jose M. Alvarez, Salvatore Ruggieri</dc:creator>
    </item>
    <item>
      <title>Effect of recommending users and opinions on the network connectivity and idea generation process</title>
      <link>https://arxiv.org/abs/2401.16504</link>
      <description>arXiv:2401.16504v2 Announce Type: replace-cross 
Abstract: The growing reliance on online services underscores the crucial role of recommendation systems, especially on social media platforms seeking increased user engagement. This study investigates how recommendation systems influence the impact of personal behavioral traits on social network dynamics. It explores the interplay between homophily, users' openness to novel ideas, and recommendation-driven exposure to new opinions. Additionally, the research examines the impact of recommendation systems on the diversity of newly generated ideas, shedding light on the challenges and opportunities in designing effective systems that balance the exploration of new ideas with the risk of reinforcing biases or filtering valuable, unconventional concepts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16504v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.22191/nejcs/vol6/iss1/1</arxiv:DOI>
      <arxiv:journal_reference>NEJCS, 2024, Vol. 6 : No. 1</arxiv:journal_reference>
      <dc:creator>Sriniwas Pandey, Hiroki Sayama</dc:creator>
    </item>
    <item>
      <title>Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation</title>
      <link>https://arxiv.org/abs/2402.14744</link>
      <description>arXiv:2402.14744v2 Announce Type: replace-cross 
Abstract: This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and effective personal mobility generation. LLMs overcome the limitations of previous models by effectively processing semantic data and offering versatility in modeling various tasks. Our approach addresses three research questions: aligning LLMs with real-world urban mobility data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. We evaluate our LLM agent framework and compare it with state-of-the-art personal mobility generation approaches, demonstrating the effectiveness of our approach and its potential applications in urban mobility. Overall, this study marks the pioneering work of designing an LLM agent framework for activity generation based on real-world human activity data, offering a promising tool for urban mobility analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14744v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Wang, Renhe Jiang, Chuang Yang, Zengqing Wu, Makoto Onizuka, Ryosuke Shibasaki, Noboru Koshizuka, Chuan Xiao</dc:creator>
    </item>
    <item>
      <title>Prediction-Powered Ranking of Large Language Models</title>
      <link>https://arxiv.org/abs/2402.17826</link>
      <description>arXiv:2402.17826v2 Announce Type: replace-cross 
Abstract: Large language models are often ranked according to their level of alignment with human preferences -- a model is better than other models if its outputs are more frequently preferred by humans. One of the popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs. However, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a common practice to gather pairwise comparisons by a strong large language model -- a model strongly aligned with human preferences. Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings. In this work, we develop a statistical framework to bridge this gap. Given a (small) set of pairwise comparisons by humans and a large set of pairwise comparisons by a model, our framework provides a rank-set -- a set of possible ranking positions -- for each of the models under comparison. Moreover, it guarantees that, with a probability greater than or equal to a user-specified value, the rank-sets cover the true ranking consistent with the distribution of human pairwise preferences asymptotically. Using pairwise comparisons made by humans in the LMSYS Chatbot Arena platform and pairwise comparisons made by three strong large language models, we empirically demonstrate the effectivity of our framework and show that the rank-sets constructed using only pairwise comparisons by the strong large language models are often inconsistent with (the distribution of) human pairwise preferences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17826v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivi Chatzi, Eleni Straitouri, Suhas Thejaswi, Manuel Gomez Rodriguez</dc:creator>
    </item>
    <item>
      <title>Embodied LLM Agents Learn to Cooperate in Organized Teams</title>
      <link>https://arxiv.org/abs/2403.12482</link>
      <description>arXiv:2403.12482v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs to propose enhanced organizational prompts, via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12482v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xudong Guo, Kaixuan Huang, Jiale Liu, Wenhui Fan, Natalia V\'elez, Qingyun Wu, Huazheng Wang, Thomas L. Griffiths, Mengdi Wang</dc:creator>
    </item>
    <item>
      <title>Fair Mixed Effects Support Vector Machine</title>
      <link>https://arxiv.org/abs/2405.06433</link>
      <description>arXiv:2405.06433v2 Announce Type: replace-cross 
Abstract: To ensure unbiased and ethical automated predictions, fairness must be a core principle in machine learning applications. Fairness in machine learning aims to mitigate biases present in the training data and model imperfections that could lead to discriminatory outcomes. This is achieved by preventing the model from making decisions based on sensitive characteristics like ethnicity or sexual orientation. A fundamental assumption in machine learning is the independence of observations. However, this assumption often does not hold true for data describing social phenomena, where data points are often clustered based. Hence, if the machine learning models do not account for the cluster correlations, the results may be biased. Especially high is the bias in cases where the cluster assignment is correlated to the variable of interest. We present a fair mixed effects support vector machine algorithm that can handle both problems simultaneously. With a reproducible simulation study we demonstrate the impact of clustered data on the quality of fair machine learning predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06433v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jo\~ao Vitor Pamplona, Jan Pablo Burgard</dc:creator>
    </item>
    <item>
      <title>A Farewell to Harms: Risk Management for Medical Devices via the Riskman Ontology &amp; Shapes</title>
      <link>https://arxiv.org/abs/2405.09875</link>
      <description>arXiv:2405.09875v2 Announce Type: replace-cross 
Abstract: We introduce the Riskman ontology &amp; shapes for representing and analysing information about risk management for medical devices. Risk management is concerned with taking necessary precautions so a medical device does not cause harms for users or the environment. To date, risk management documentation is submitted to notified bodies (for certification) in the form of semi-structured natural language text. We propose to use classes from the Riskman ontology to logically model risk management documentation and to use the included SHACL constraints to check for syntactic completeness and conformity to relevant standards. In particular, the ontology is modelled after ISO 14971 and the recently published VDE Spec 90025. Our proposed methodology has the potential to save many person-hours for both manufacturers (when creating risk management documentation) as well as notified bodies (when assessing submitted applications for certification), and thus offers considerable benefits for healthcare and, by extension, society as a whole.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09875v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Piotr Gorczyca, D\"orthe Arndt, Martin Diller, Pascal Kettmann, Stephan Mennicke, Hannes Strass</dc:creator>
    </item>
    <item>
      <title>Combining Twitter and Mobile Phone Data to Observe Border-Rush: The Turkish-European Border Opening</title>
      <link>https://arxiv.org/abs/2405.12642</link>
      <description>arXiv:2405.12642v2 Announce Type: replace-cross 
Abstract: Following Turkey's 2020 decision to revoke border controls, many individuals journeyed towards the Greek, Bulgarian, and Turkish borders. However, the lack of verifiable statistics on irregular migration and discrepancies between media reports and actual migration patterns require further exploration. The objective of this study is to bridge this knowledge gap by harnessing novel data sources, specifically mobile phone and Twitter data, to construct estimators of cross-border mobility and to cultivate a qualitative comprehension of the unfolding events. By employing a migration diplomacy framework, we analyse emergent mobility patterns at the border. Our findings demonstrate the potential of mobile phone data for quantitative metrics and Twitter data for qualitative understanding. We underscore the ethical implications of leveraging Big Data, particularly considering the vulnerability of the population under study. This underscores the imperative for exhaustive research into the socio-political facets of human mobility, with the aim of discerning the potentialities, limitations, and risks inherent in these data sources and their integration. This scholarly endeavour contributes to a more nuanced understanding of migration dynamics and paves the way for the formulation of regulations that preclude misuse and oppressive surveillance, thereby ensuring a more accurate representation of migration realities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12642v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Arcila Calder\'on, Bilge\c{c}a\u{g} Aydo\u{g}du, Tuba Bircan, B\"unyamin G\"und\"uz, Onur \"Ones, Albert Ali Salah, Alina S\^irbu</dc:creator>
    </item>
  </channel>
</rss>
