<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Oct 2024 02:06:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On The MCMC Performance In Bernoulli Group Testing And The Random Max-Set Problem</title>
      <link>https://arxiv.org/abs/2410.09231</link>
      <description>arXiv:2410.09231v1 Announce Type: new 
Abstract: The group testing problem is a canonical inference task where one seeks to identify $k$ infected individuals out of a population of $n$ people, based on the outcomes of $m$ group tests. Of particular interest is the case of Bernoulli group testing (BGT), where each individual participates in each test independently and with a fixed probability. BGT is known to be an ``information-theoretically'' optimal design, as there exists a decoder that can identify with high probability as $n$ grows the infected individuals using $m^*=\log_2 \binom{n}{k}$ BGT tests, which is the minimum required number of tests among \emph{all} group testing designs.
  An important open question in the field is if a polynomial-time decoder exists for BGT which succeeds also with $m^*$ samples. In a recent paper (Iliopoulos, Zadik COLT '21) some evidence was presented (but no proof) that a simple low-temperature MCMC method could succeed. The evidence was based on a first-moment (or ``annealed'') analysis of the landscape, as well as simulations that show the MCMC success for $n \approx 1000s$.
  In this work, we prove that, despite the intriguing success in simulations for small $n$, the class of MCMC methods proposed in previous work for BGT with $m^*$ samples takes super-polynomial-in-$n$ time to identify the infected individuals, when $k=n^{\alpha}$ for $\alpha \in (0,1)$ small enough. Towards obtaining our results, we establish the tight max-satisfiability thresholds of the random $k$-set cover problem, a result of potentially independent interest in the study of random constraint satisfaction problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09231v1</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maxwell Lovig, Ilias Zadik</dc:creator>
    </item>
    <item>
      <title>Minimax rates of convergence for nonparametric regression under adversarial attacks</title>
      <link>https://arxiv.org/abs/2410.09402</link>
      <description>arXiv:2410.09402v1 Announce Type: new 
Abstract: Recent research shows the susceptibility of machine learning models to adversarial attacks, wherein minor but maliciously chosen perturbations of the input can significantly degrade model performance. In this paper, we theoretically analyse the limits of robustness against such adversarial attacks in a nonparametric regression setting, by examining the minimax rates of convergence in an adversarial sup-norm. Our work reveals that the minimax rate under adversarial attacks in the input is the same as sum of two terms: one represents the minimax rate in the standard setting without adversarial attacks, and the other reflects the maximum deviation of the true regression function value within the target function class when subjected to the input perturbations. The optimal rates under the adversarial setup can be achieved by a plug-in procedure constructed from a minimax optimal estimator in the corresponding standard setting. Two specific examples are given to illustrate the established minimax results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09402v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingfu Peng, Yuhong Yang</dc:creator>
    </item>
    <item>
      <title>Knockoffs for exchangeable categorical covariates</title>
      <link>https://arxiv.org/abs/2410.09835</link>
      <description>arXiv:2410.09835v1 Announce Type: new 
Abstract: Let $X=(X_1,\ldots,X_p)$ be a $p$-variate random vector and $F$ a fixed finite set. In a number of applications, mainly in genetics, it turns out that $X_i\in F$ for each $i=1,\ldots,p$. Despite the latter fact, to obtain a knockoff $\widetilde{X}$ (in the sense of \cite{CFJL18}), $X$ is usually modeled as an absolutely continuous random vector. While comprehensible from the point of view of applications, this approximate procedure does not make sense theoretically, since $X$ is supported by the finite set $F^p$. In this paper, explicit formulae for the joint distribution of $(X,\widetilde{X})$ are provided when $P(X\in F^p)=1$ and $X$ is exchangeable or partially exchangeable. In fact, when $X_i\in F$ for all $i$, there seem to be various reasons for assuming $X$ exchangeable or partially exchangeable. The robustness of $\widetilde{X}$, with respect to the de Finetti's measure $\pi$ of $X$, is investigated as well. Let $\mathcal{L}_\pi(\widetilde{X}\mid X=x)$ denote the conditional distribution of $\widetilde{X}$, given $X=x$, when the de Finetti's measure is $\pi$. It is shown that $$\norm{\mathcal{L}_{\pi_1}(\widetilde{X}\mid X=x)-\mathcal{L}_{\pi_2}(\widetilde{X}\mid X=x)}\le c(x)\,\norm{\pi_1-\pi_2}$$ where $\norm{\cdot}$ is total variation distance and $c(x)$ a suitable constant. Finally, a numerical experiment is performed. Overall, the knockoffs of this paper outperform the alternatives (i.e., the knockoffs obtained by giving $X$ an absolutely continuous distribution) as regards the false discovery rate but are slightly weaker in terms of power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09835v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Emanuela Dreassi, Luca Pratelli, Pietro Rigo</dc:creator>
    </item>
    <item>
      <title>Bivariate dynamic conditional failure extropy</title>
      <link>https://arxiv.org/abs/2410.09882</link>
      <description>arXiv:2410.09882v2 Announce Type: new 
Abstract: Nair and Sathar (2020) introduced a new metric for uncertainty known as dynamic failure extropy, focusing on the analysis of past lifetimes. In this study, we extend this concept to a bivariate context, exploring various properties associated with the proposed bivariate measure. We show that bivariate conditional failure extropy can uniquely determine the joint distribution function. Additionally, we derive characterizations for certain bivariate lifetime models using this measure. A new stochastic ordering, based on bivariate conditional failure extropy, is also proposed, along with some established bounds. We further develop an estimator for the bivariate conditional failure extropy using a smoothed kernel and empirical approach. The performance of the proposed estimator is evaluated through simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09882v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aman Pandey, Chanchal Kundu</dc:creator>
    </item>
    <item>
      <title>Testing for unspecified periodicities in binary time series</title>
      <link>https://arxiv.org/abs/2410.10203</link>
      <description>arXiv:2410.10203v1 Announce Type: new 
Abstract: Given independent random variables $Y_1, \ldots, Y_n$ with $Y_i \in \{0,1\}$ we test the hypothesis whether the underlying success probabilities $p_i$ are constant or whether they are periodic with an unspecified period length of $r \ge 2$. The test relies on an auxiliary integer $d$ which can be chosen arbitrarily, using which a new time series of length $d$ is constructed. For this new time series, the test statistic is derived according to the classical $g$ test by Fisher. Under the null hypothesis of a constant success probability it is shown that the test keeps the level asymptotically, while it has power for most alternatives, i.e. typically in the case of $r \ge 3$ and where $r$ and $d$ have common divisors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10203v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Finn Schmidtke, Mathias Vetter</dc:creator>
    </item>
    <item>
      <title>Kinetic interacting particle system: parameter estimation from complete and partial discrete observations</title>
      <link>https://arxiv.org/abs/2410.10226</link>
      <description>arXiv:2410.10226v1 Announce Type: new 
Abstract: In this paper, we study the estimation of drift and diffusion coefficients in a two dimensional system of N interacting particles modeled by a degenerate stochastic differential equation. We consider both complete and partial observation cases over a fixed time horizon [0, T] and propose novel contrast functions for parameter estimation. In the partial observation scenario, we tackle the challenge posed by unobserved velocities by introducing a surrogate process based on the increments of the observed positions. This requires a modified contrast function to account for the correlation between successive increments. Our analysis demonstrates that, despite the loss of Markovianity due to the velocity approximation in the partial observation case, the estimators converge to a Gaussian distribution (with a correction factor in the partial observation case). The proofs are based on Ito like bounds and an adaptation of the Euler scheme. Additionally, we provide insights into H\"ormander's condition, which helps establish hypoellipticity in our model within the framework of stochastic calculus of variations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10226v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Amorino, Vytaut\.e Pilipauskait\.e</dc:creator>
    </item>
    <item>
      <title>A non-asymptotic upper bound in prediction for the PLS estimator</title>
      <link>https://arxiv.org/abs/2410.10237</link>
      <description>arXiv:2410.10237v1 Announce Type: new 
Abstract: We investigate the theoretical performances of the Partial Least Square (PLS) algorithm in a high dimensional context. We provide upper bounds on the risk in prediction for the statistical linear model when considering the PLS estimator. Our bounds are non-asymptotic and are expressed in terms of the number of observations, the noise level, the properties of the design matrix, and the number of considered PLS components. In particular, we exhibit some scenarios where the variability of the PLS may explode and prove that we can get round of these situations by introducing a Ridge regularization step. These theoretical findings are illustrated by some numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10237v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Castelli (ICJ, PSPM), Ir\`ene Gannaz (G-SCOP\_GROG, G-SCOP), Cl\'ement Marteau (ICJ, PSPM)</dc:creator>
    </item>
    <item>
      <title>Convergence rates for estimating multivariate scale mixtures of uniform densities</title>
      <link>https://arxiv.org/abs/2410.10251</link>
      <description>arXiv:2410.10251v1 Announce Type: new 
Abstract: The Grenander estimator is a well-studied procedure for univariate nonparametric density estimation. It is usually defined as the Maximum Likelihood Estimator (MLE) over the class of all non-increasing densities on the positive real line. It can also be seen as the MLE over the class of all scale mixtures of uniform densities. Using the latter viewpoint, Pavlides and Wellner~\cite{pavlides2012nonparametric} proposed a multivariate extension of the Grenander estimator as the nonparametric MLE over the class of all multivariate scale mixtures of uniform densities. We prove that this multivariate estimator achieves the univariate cube root rate of convergence with only a logarithmic multiplicative factor that depends on the dimension. The usual curse of dimensionality is therefore avoided to some extent for this multivariate estimator. This result positively resolves a conjecture of Pavlides and Wellner~\cite{pavlides2012nonparametric} under an additional lower bound assumption. Our proof proceeds via a general accuracy result for the Hellinger accuracy of MLEs over convex classes of densities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10251v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arlene K. H. Kim, Gil Kur, Adityanand Guntuboyina</dc:creator>
    </item>
    <item>
      <title>Statistical inference of partially linear time-varying coefficients spatial autoregressive panel data model</title>
      <link>https://arxiv.org/abs/2410.10647</link>
      <description>arXiv:2410.10647v1 Announce Type: new 
Abstract: This paper investigates a partially linear spatial autoregressive panel data model that incorporates fixed effects, constant and time-varying regression coefficients, and a time-varying spatial lag coefficient. A two-stage least squares estimation method based on profile local linear dummy variables (2SLS-PLLDV) is proposed to estimate both constant and time-varying coefficients without the need for first differencing. The asymptotic properties of the estimator are derived under certain conditions. Furthermore, a residual-based goodness-of-fit test is constructed for the model, and a residual-based bootstrap method is used to obtain p-values. Simulation studies show the good performance of the proposed method in various scenarios. The Chinese provincial carbon emission data set is analyzed for illustration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10647v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lingling Tian, Chuanhua Wei, Mixia Wu</dc:creator>
    </item>
    <item>
      <title>Vecchia Gaussian Processes: Probabilistic Properties, Minimax Rates and Methodological Developments</title>
      <link>https://arxiv.org/abs/2410.10649</link>
      <description>arXiv:2410.10649v1 Announce Type: new 
Abstract: Gaussian Processes (GPs) are widely used to model dependency in spatial statistics and machine learning, yet the exact computation suffers an intractable time complexity of $O(n^3)$. Vecchia approximation allows scalable Bayesian inference of GPs in $O(n)$ time by introducing sparsity in the spatial dependency structure that is characterized by a directed acyclic graph (DAG). Despite the popularity in practice, it is still unclear how to choose the DAG structure and there are still no theoretical guarantees in nonparametric settings. In this paper, we systematically study the Vecchia GPs as standalone stochastic processes and uncover important probabilistic properties and statistical results in methodology and theory. For probabilistic properties, we prove that the conditional distributions of the Mat\'{e}rn GPs, as well as the Vecchia approximations of the Mat\'{e}rn GPs, can be characterized by polynomials. This allows us to prove a series of results regarding the small ball probabilities and RKHSs of Vecchia GPs. For statistical methodology, we provide a principled guideline to choose parent sets as norming sets with fixed cardinality and provide detailed algorithms following such guidelines. For statistical theory, we prove posterior contraction rates for applying Vecchia GPs to regression problems, where minimax optimality is achieved by optimally tuned GPs via either oracle rescaling or hierarchical Bayesian methods. Our theory and methodology are demonstrated with numerical studies, where we also provide efficient implementation of our methods in C++ with R interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10649v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Botond Szabo, Yichen Zhu</dc:creator>
    </item>
    <item>
      <title>Fast Convergence of $\Phi$-Divergence Along the Unadjusted Langevin Algorithm and Proximal Sampler</title>
      <link>https://arxiv.org/abs/2410.10699</link>
      <description>arXiv:2410.10699v1 Announce Type: new 
Abstract: We study the mixing time of two popular discrete time Markov chains in continuous space, the unadjusted Langevin algorithm and the proximal sampler, which are discretizations of the Langevin dynamics. We extend mixing time analyses for these Markov chains to hold in $\Phi$-divergence. We show that any $\Phi$-divergence arising from a twice-differentiable strictly convex function $\Phi$ converges to $0$ exponentially fast along these Markov chains, under the assumption that their stationary distributions satisfies the corresponding $\Phi$-Sobolev inequality. Our rates of convergence are tight and include as special cases popular mixing time regimes, namely the mixing in chi-squared divergence under a Poincar\'e inequality, and the mixing in relative entropy under a log-Sobolev inequality. Our results follow by bounding the contraction coefficients arising in the appropriate strong data processing inequalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10699v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siddharth Mitra, Andre Wibisono</dc:creator>
    </item>
    <item>
      <title>Estimation beyond Missing (Completely) at Random</title>
      <link>https://arxiv.org/abs/2410.10704</link>
      <description>arXiv:2410.10704v1 Announce Type: new 
Abstract: We study the effects of missingness on the estimation of population parameters. Moving beyond restrictive missing completely at random (MCAR) assumptions, we first formulate a missing data analogue of Huber's arbitrary $\epsilon$-contamination model. For mean estimation with respect to squared Euclidean error loss, we show that the minimax quantiles decompose as a sum of the corresponding minimax quantiles under a heterogeneous, MCAR assumption, and a robust error term, depending on $\epsilon$, that reflects the additional error incurred by departure from MCAR.
  We next introduce natural classes of realisable $\epsilon$-contamination models, where an MCAR version of a base distribution $P$ is contaminated by an arbitrary missing not at random (MNAR) version of $P$. These classes are rich enough to capture various notions of biased sampling and sensitivity conditions, yet we show that they enjoy improved minimax performance relative to our earlier arbitrary contamination classes for both parametric and nonparametric classes of base distributions. For instance, with a univariate Gaussian base distribution, consistent mean estimation over realisable $\epsilon$-contamination classes is possible even when $\epsilon$ and the proportion of missingness converge (slowly) to 1. Finally, we extend our results to the setting of departures from missing at random (MAR) in normal linear regression with a realisable missing response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10704v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyi Ma, Kabir A. Verchand, Thomas B. Berrett, Tengyao Wang, Richard J. Samworth</dc:creator>
    </item>
    <item>
      <title>Anytime-Valid Continuous-Time Confidence Processes for Inhomogeneous Poisson Processes</title>
      <link>https://arxiv.org/abs/2410.09282</link>
      <description>arXiv:2410.09282v1 Announce Type: cross 
Abstract: Motivated by monitoring the arrival of incoming adverse events such as customer support calls or crash reports from users exposed to an experimental product change, we consider sequential hypothesis testing of continuous-time inhomogeneous Poisson point processes. Specifically, we provide an interval-valued confidence process $C^\alpha(t)$ over continuous time $t$ for the cumulative arrival rate $\Lambda(t) = \int_0^t \lambda(s) \mathrm{d}s$ with a continuous-time anytime-valid coverage guarantee $\mathbb{P}[\Lambda(t) \in C^\alpha(t) \, \forall t &gt;0] \geq 1-\alpha$. We extend our results to compare two independent arrival processes by constructing multivariate confidence processes and a closed-form $e$-process for testing the equality of rates with a time-uniform Type-I error guarantee at a nominal $\alpha$. We characterize the asymptotic growth rate of the proposed $e$-process under the alternative and show that it has power 1 when the average rates of the two Poisson process differ in the limit. We also observe a complementary relationship between our multivariate confidence process and the universal inference $e$-process for testing composite null hypotheses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09282v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Lindon, Nathan Kallus</dc:creator>
    </item>
    <item>
      <title>Joint identifiability of ancestral sequence, phylogeny and mutation rates under the TKF91 model</title>
      <link>https://arxiv.org/abs/2410.09620</link>
      <description>arXiv:2410.09620v1 Announce Type: cross 
Abstract: We consider the problem of identifying jointly the ancestral sequence, the phylogeny and the parameters in models of DNA sequence evolution with insertion and deletion (indel). Under the classical TKF91 model of sequence evolution, we obtained explicit formulas for the root sequence, the pairwise distances of leaf sequences, as well as the scaled rates of indel and substitution in terms of the distribution of the leaf sequences of an arbitrary phylogeny. These explicit formulas not only strengthen existing invertibility results and work for phylogeny that are not necessarily ultrametric, but also lead to new estimators with less assumption compared with the existing literature. Our simulation study demonstrates that these estimators are statistically consistent as the number of independent samples tends to infinity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09620v1</guid>
      <category>q-bio.PE</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Xue, Brandon Legried, Wai-Tong Louis Fan</dc:creator>
    </item>
    <item>
      <title>On Goodhart's law, with an application to value alignment</title>
      <link>https://arxiv.org/abs/2410.09638</link>
      <description>arXiv:2410.09638v1 Announce Type: cross 
Abstract: ``When a measure becomes a target, it ceases to be a good measure'', this adage is known as {\it Goodhart's law}. In this paper, we investigate formally this law and prove that it critically depends on the tail distribution of the discrepancy between the true goal and the measure that is optimized. Discrepancies with long-tail distributions favor a Goodhart's law, that is, the optimization of the measure can have a counter-productive effect on the goal.
  We provide a formal setting to assess Goodhart's law by studying the asymptotic behavior of the correlation between the goal and the measure, as the measure is optimized. Moreover, we introduce a distinction between a {\it weak} Goodhart's law, when over-optimizing the metric is useless for the true goal, and a {\it strong} Goodhart's law, when over-optimizing the metric is harmful for the true goal. A distinction which we prove to depend on the tail distribution.
  We stress the implications of this result to large-scale decision making and policies that are (and have to be) based on metrics, and propose numerous research directions to better assess the safety of such policies in general, and to the particularly concerning case where these policies are automated with algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09638v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>El-Mahdi El-Mhamdi, L\^e-Nguy\^en Hoang</dc:creator>
    </item>
    <item>
      <title>MARS via LASSO</title>
      <link>https://arxiv.org/abs/2111.11694</link>
      <description>arXiv:2111.11694v5 Announce Type: replace 
Abstract: Multivariate adaptive regression splines (MARS) is a popular method for nonparametric regression introduced by Friedman in 1991. MARS fits simple nonlinear and non-additive functions to regression data. We propose and study a natural lasso variant of the MARS method. Our method is based on least squares estimation over a convex class of functions obtained by considering infinite-dimensional linear combinations of functions in the MARS basis and imposing a variation based complexity constraint. Our estimator can be computed via finite-dimensional convex optimization, although it is defined as a solution to an infinite-dimensional optimization problem. Under a few standard design assumptions, we prove that our estimator achieves a rate of convergence that depends only logarithmically on dimension and thus avoids the usual curse of dimensionality to some extent. We also show that our method is naturally connected to nonparametric estimation techniques based on smoothness constraints. We implement our method with a cross-validation scheme for the selection of the involved tuning parameter and compare it to the usual MARS method in various simulation and real data settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.11694v5</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/24-AOS2384</arxiv:DOI>
      <arxiv:journal_reference>Ann. Statist. 52 (3) 1102 - 1126, June 2024</arxiv:journal_reference>
      <dc:creator>Dohyeong Ki, Billy Fang, Adityanand Guntuboyina</dc:creator>
    </item>
    <item>
      <title>Testing Independence of Infinite Dimensional Random Elements: A Sup-norm Approach</title>
      <link>https://arxiv.org/abs/2301.00375</link>
      <description>arXiv:2301.00375v4 Announce Type: replace 
Abstract: In this article, we study the test for independence of two random elements $X$ and $Y$ lying in an infinite dimensional space ${\cal{H}}$ (specifically, a real separable Hilbert space equipped with the inner product $\langle ., .\rangle_{\cal{H}}$). In the course of this study, a measure of association is proposed based on the sup-norm difference between the joint probability density function of the bivariate random vector $(\langle l_{1}, X \rangle_{\cal{H}}, \langle l_{2}, Y \rangle_{\cal{H}})$ and the product of marginal probability density functions of the random variables $\langle l_{1}, X \rangle_{\cal{H}}$ and $\langle l_{2}, Y \rangle_{\cal{H}}$, where $l_{1}\in{\cal{H}}$ and $l_{2}\in{\cal{H}}$ are two arbitrary elements. It is established that the proposed measure of association equals zero if and only if the random elements are independent. In order to carry out the test whether $X$ and $Y$ are independent or not, the sample version of the proposed measure of association is considered as the test statistic after appropriate normalization, and the asymptotic distributions of the test statistic under the null and the local alternatives are derived. The performance of the new test is investigated for simulated data sets and the practicability of the test is shown for three real data sets related to climatology, biological science and chemical science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.00375v4</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suprio Bhar (Indian Institute of Technology Kanpur), Subhra Sankar Dhar (Indian Institute of Technology Kanpur)</dc:creator>
    </item>
    <item>
      <title>Deflated HeteroPCA: Overcoming the curse of ill-conditioning in heteroskedastic PCA</title>
      <link>https://arxiv.org/abs/2303.06198</link>
      <description>arXiv:2303.06198v2 Announce Type: replace 
Abstract: This paper is concerned with estimating the column subspace of a low-rank matrix $\boldsymbol{X}^\star \in \mathbb{R}^{n_1\times n_2}$ from contaminated data. How to obtain optimal statistical accuracy while accommodating the widest range of signal-to-noise ratios (SNRs) becomes particularly challenging in the presence of heteroskedastic noise and unbalanced dimensionality (i.e., $n_2\gg n_1$). While the state-of-the-art algorithm $\textsf{HeteroPCA}$ emerges as a powerful solution for solving this problem, it suffers from "the curse of ill-conditioning," namely, its performance degrades as the condition number of $\boldsymbol{X}^\star$ grows. In order to overcome this critical issue without compromising the range of allowable SNRs, we propose a novel algorithm, called $\textsf{Deflated-HeteroPCA}$, that achieves near-optimal and condition-number-free theoretical guarantees in terms of both $\ell_2$ and $\ell_{2,\infty}$ statistical accuracy. The proposed algorithm divides the spectrum of $\boldsymbol{X}^\star$ into well-conditioned and mutually well-separated subblocks, and applies $\textsf{HeteroPCA}$ to conquer each subblock successively. Further, an application of our algorithm and theory to two canonical examples -- the factor model and tensor PCA -- leads to remarkable improvement for each application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.06198v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchen Zhou, Yuxin Chen</dc:creator>
    </item>
    <item>
      <title>Existence of solutions to the nonlinear equations characterizing the precise error of M-estimators</title>
      <link>https://arxiv.org/abs/2312.13254</link>
      <description>arXiv:2312.13254v3 Announce Type: replace 
Abstract: Major progress has been made in the previous decade to characterize the asymptotic behavior of regularized M-estimators in high-dimensional regression problems in the proportional asymptotic regime where the sample size $n$ and the number of features $p$ are increasing simultaneously such that $n/p\to \delta \in(0,\infty)$, using powerful tools such as Approximate Message Passing or the Convex Gaussian Min-Max Theorem (CGMT). The asymptotic error and behavior of the regularized M-estimator is then typically described by a system of nonlinear equations with a few scalar unknowns, and the solution to this system precisely characterizes the asymptotic error. Application of the CGMT and related machinery requires the existence and uniqueness of a solution to this low-dimensional system of equations or to a related scalar convex minimization problem.
  This paper resolves the question of existence and uniqueness of solution to this low-dimensional system for the case of linear models with independent additive noise, when both the data-fitting loss function and regularizer are separable and convex. Such existence result was previously known under strong convexity or for specific estimators such as the Lasso. The main idea behind this existence result is inspired by an argument developed by Montanari et al. [2023] and Celentano et al. [2023] in different contexts: By constructing an ad-hoc convex minimization problem in an infinite dimensional Hilbert space, the existence of the Lagrange multiplier for this optimization problem makes it possible to construct explicit solutions to the low-dimensional system of interest.
  The conditions under which we derive this existence result exactly correspond to the side of the phase transition where perfect recovery $\hat{x} = x_0$ fails, so that these conditions are optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13254v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre C. Bellec, Takuya Koriyama</dc:creator>
    </item>
    <item>
      <title>The Minimax Rate of HSIC Estimation for Translation-Invariant Kernels</title>
      <link>https://arxiv.org/abs/2403.07735</link>
      <description>arXiv:2403.07735v2 Announce Type: replace 
Abstract: Kernel techniques are among the most influential approaches in data science and statistics. Under mild conditions, the reproducing kernel Hilbert space associated to a kernel is capable of encoding the independence of $M\ge 2$ random variables. Probably the most widespread independence measure relying on kernels is the so-called Hilbert-Schmidt independence criterion (HSIC; also referred to as distance covariance in the statistics literature). Despite various existing HSIC estimators designed since its introduction close to two decades ago, the fundamental question of the rate at which HSIC can be estimated is still open. In this work, we prove that the minimax optimal rate of HSIC estimation on $\mathbb R^d$ for Borel measures containing the Gaussians with continuous bounded translation-invariant characteristic kernels is $\mathcal O\!\left(n^{-1/2}\right)$. Specifically, our result implies the optimality in the minimax sense of many of the most-frequently used estimators (including the U-statistic, the V-statistic, and the Nystr\"om-based one) on $\mathbb R^d$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07735v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Kalinke, Zoltan Szabo</dc:creator>
    </item>
    <item>
      <title>Bayesian Nonparametric Inference in McKean-Vlasov models</title>
      <link>https://arxiv.org/abs/2404.16742</link>
      <description>arXiv:2404.16742v2 Announce Type: replace 
Abstract: We consider nonparametric statistical inference on a periodic interaction potential $W$ from noisy discrete space-time measurements of solutions $\rho=\rho_W$ of the nonlinear McKean-Vlasov equation, describing the probability density of the mean field limit of an interacting particle system. We show how Gaussian process priors assigned to $W$ give rise to posterior mean estimators that exhibit fast convergence rates for the implied estimated densities $\bar \rho$ towards $\rho_W$. We further show that if the initial condition $\phi$ is not too smooth and satisfies a standard deconvolvability condition, then one can consistently infer Sobolev-regular potentials $W$ at convergence rates $N^{-\theta}$ for appropriate $\theta&gt;0$, where $N$ is the number of measurements. The exponent $\theta$ can be taken to approach $1/2$ as the regularity of $W$ increases corresponding to `near-parametric' models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16742v2</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richard Nickl, Grigorios A. Pavliotis, Kolyan Ray</dc:creator>
    </item>
    <item>
      <title>Asymptotic Theory for Estimation of the Husler-Reiss Distribution via Block Maxima Method</title>
      <link>https://arxiv.org/abs/2405.15649</link>
      <description>arXiv:2405.15649v3 Announce Type: replace 
Abstract: The H\"usler-Reiss distribution describes the limit of the pointwise maxima of a bivariate normal distribution. This distribution is defined by a single parameter, $\lambda$. We provide asymptotic theory for maximum likelihood estimation of $\lambda$ under a block maxima approach. Our work assumes independent and identically distributed bivariate normal random variables, grouped into blocks where the block size and number of blocks increase simultaneously. With these assumptions our results provide conditions for the asymptotic normality of the Maximum Likelihood Estimator (MLE). We characterize the bias of the MLE, provide conditions under which this bias is asymptotically negligible, and discuss how to choose the block size to minimize a bias-variance trade-off. The proofs are an extension of previous results for choosing the block size in the estimation of univariate extreme value distributions (Dombry and Ferreria 2019), providing a potential basis for extensions to multivariate cases where both the marginal and dependence parameters are unknown. The proofs rely on the Argmax Theorem applied to a localized loglikelihood function, combined with a Lindeberg-Feller Central Limit Theorem argument to establish asymptotic normality. Possible applications of the method include composite likelihood estimation in Brown-Resnick processes, where it is known that the bivariate distributions are of H\"usler-Reiss form.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15649v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hank Flury, Jan Hannig, Richard Smith</dc:creator>
    </item>
    <item>
      <title>Dimension-free uniform concentration bound for logistic regression</title>
      <link>https://arxiv.org/abs/2405.18055</link>
      <description>arXiv:2405.18055v5 Announce Type: replace 
Abstract: We provide a novel dimension-free uniform concentration bound for the empirical risk function of constrained logistic regression. Our bound yields a milder sufficient condition for a uniform law of large numbers than conditions derived by the Rademacher complexity argument and McDiarmid's inequality. The derivation is based on the PAC-Bayes approach with second-order expansion and Rademacher-complexity-based bounds for the residual term of the expansion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18055v5</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shogo Nakakita</dc:creator>
    </item>
    <item>
      <title>Anomaly Detection based on Markov Data: A Statistical Depth Approach</title>
      <link>https://arxiv.org/abs/2406.16759</link>
      <description>arXiv:2406.16759v3 Announce Type: replace 
Abstract: The main purpose of this article to extend the notion of statistical depth to the case of sample paths of a Markov chain. Initially introduced to define a center-outward ordering of points in the support of a multivariate distribution, depth functions permit to generalize the notions of quantiles and (signed) ranks for observations in $\mathbb{R}^d$ with $d&gt;1$, as well as statistical procedures based on such quantities. In this paper, overcoming the lack of natural order on the torus composed of all possible trajectories of finite length, we develop a general theoretical framework for evaluating the depth of a Markov sample path and recovering it statistically from an estimate of its transition probability with (non-) asymptotic guarantees. We also detail some of its numerous applications, focusing particularly on anomaly detection, a key task in various fields involving the analysis of (supposedly) Markov time-series (\textit{e.g.} health monitoring of complex infrastructures, security). Beyond the description of the methodology promoted and the statistical analysis carried out to guarantee its validity, numerical experiments are displayed, providing strong empirical evidence of the relevance of the novel concept we introduce here to quantify the degree of abnormality of Markov path sequences of variable length.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16759v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Fern\'andez, Stephan Cl\'emen\c{c}on</dc:creator>
    </item>
    <item>
      <title>Transition of $\alpha$-mixing in Random Iterations with Applications in Queuing Theory</title>
      <link>https://arxiv.org/abs/2410.05056</link>
      <description>arXiv:2410.05056v2 Announce Type: replace 
Abstract: Nonlinear time series models with exogenous regressors are essential in econometrics, queuing theory, and machine learning, though their statistical analysis remains incomplete. Key results, such as the law of large numbers and the functional central limit theorem, are known for weakly dependent variables. We demonstrate the transfer of mixing properties from the exogenous regressor to the response via coupling arguments. Additionally, we study Markov chains in random environments with drift and minorization conditions, even under non-stationary environments with favorable mixing properties, and apply this framework to single-server queuing models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05056v2</guid>
      <category>math.ST</category>
      <category>cs.AI</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Attila Lovas</dc:creator>
    </item>
    <item>
      <title>Optimal Scoring Rule Design under Partial Knowledge</title>
      <link>https://arxiv.org/abs/2107.07420</link>
      <description>arXiv:2107.07420v3 Announce Type: replace-cross 
Abstract: This paper studies the design of optimal proper scoring rules when the principal has partial knowledge of an agent's signal distribution. Recent work characterizes the proper scoring rules that maximize the increase of an agent's payoff when the agent chooses to access a costly signal to refine a posterior belief from her prior prediction, under the assumption that the agent's signal distribution is fully known to the principal. In our setting, the principal only knows about a set of distributions where the agent's signal distribution belongs. We formulate the scoring rule design problem as a max-min optimization that maximizes the worst-case increase in payoff across the set of distributions.
  We propose an efficient algorithm to compute an optimal scoring rule when the set of distributions is finite, and devise a fully polynomial-time approximation scheme that accommodates various infinite sets of distributions. We further remark that widely used scoring rules, such as the quadratic and log rules, as well as previously identified optimal scoring rules under full knowledge, can be far from optimal in our partial knowledge settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.07420v3</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiling Chen, Fang-Yi Yu</dc:creator>
    </item>
    <item>
      <title>Multivariate Tie-breaker Designs</title>
      <link>https://arxiv.org/abs/2202.10030</link>
      <description>arXiv:2202.10030v5 Announce Type: replace-cross 
Abstract: In a tie-breaker design (TBD), subjects with high values of a running variable are given some (usually desirable) treatment, subjects with low values are not, and subjects in the middle are randomized. TBDs are intermediate between regression discontinuity designs (RDDs) and randomized controlled trials (RCTs). TBDs allow a tradeoff between the resource allocation efficiency of an RDD and the statistical efficiency of an RCT. We study a model where the expected response is one multivariate regression for treated subjects and another for control subjects. We propose a prospective D-optimality, analogous to Bayesian optimal design, to understand design tradeoffs without reference to a specific data set. For given covariates, we show how to use convex optimization to choose treatment probabilities that optimize this criterion. We can incorporate a variety of constraints motivated by economic and ethical considerations. In our model, D-optimality for the treatment effect coincides with D-optimality for the whole regression, and, without constraints, an RCT is globally optimal. We show that a monotonicity constraint favoring more deserving subjects induces sparsity in the number of distinct treatment probabilities. We apply the convex optimization solution to a semi-synthetic example involving triage data from the MIMIC-IV-ED database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.10030v5</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim P. Morrison, Art B. Owen</dc:creator>
    </item>
    <item>
      <title>Parameter Estimation in Nonlinear Multivariate Stochastic Differential Equations Based on Splitting Schemes</title>
      <link>https://arxiv.org/abs/2211.11884</link>
      <description>arXiv:2211.11884v3 Announce Type: replace-cross 
Abstract: The likelihood functions for discretely observed nonlinear continuous-time models based on stochastic differential equations are not available except for a few cases. Various parameter estimation techniques have been proposed, each with advantages, disadvantages, and limitations depending on the application. Most applications still use the Euler-Maruyama discretization, despite many proofs of its bias. More sophisticated methods, such as Kessler's Gaussian approximation, Ozaki's Local Linearization, A\"it-Sahalia's Hermite expansions, or MCMC methods, might be complex to implement, do not scale well with increasing model dimension, or can be numerically unstable. We propose two efficient and easy-to-implement likelihood-based estimators based on the Lie-Trotter (LT) and the Strang (S) splitting schemes. We prove that S has $L^p$ convergence rate of order 1, a property already known for LT. We show that the estimators are consistent and asymptotically efficient under the less restrictive one-sided Lipschitz assumption. A numerical study on the 3-dimensional stochastic Lorenz system complements our theoretical findings. The simulation shows that the S estimator performs the best when measured on precision and computational speed compared to the state-of-the-art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.11884v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/24-AOS2371</arxiv:DOI>
      <arxiv:journal_reference>The Annals of Statistics (2024), Vol. 52, No. 2, 842 - 867</arxiv:journal_reference>
      <dc:creator>Predrag Pilipovic, Adeline Samson, Susanne Ditlevsen</dc:creator>
    </item>
    <item>
      <title>From isotonic to Lipschitz regression: a new interpolative perspective on shape-restricted estimation</title>
      <link>https://arxiv.org/abs/2307.05732</link>
      <description>arXiv:2307.05732v4 Announce Type: replace-cross 
Abstract: This manuscript seeks to bridge two seemingly disjoint paradigms of nonparametric regression: estimation based on smoothness assumptions and shape constraints. The proposed approach is motivated by a conceptually simple observation: Every Lipschitz function is a sum of monotonic and linear functions. This principle is further generalized to the higher-order monotonicity and multivariate covariates. A family of estimators is proposed based on a sample-splitting procedure, which inherits desirable methodological, theoretical, and computational properties of shape-restricted estimators. The theoretical analysis provides convergence guarantees of the estimator under heteroscedastic and heavy-tailed errors, as well as adaptivity properties to the unknown complexity of the true regression function. The generality of the proposed decomposition framework is demonstrated through new approximation results, and extensive numerical studies validate the theoretical properties of the proposed estimation framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.05732v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenta Takatsu, Tianyu Zhang, Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>The Local Landscape of Phase Retrieval Under Limited Samples</title>
      <link>https://arxiv.org/abs/2311.15221</link>
      <description>arXiv:2311.15221v2 Announce Type: replace-cross 
Abstract: In this paper, we present a fine-grained analysis of the local landscape of phase retrieval under the regime of limited samples. Specifically, we aim to ascertain the minimal sample size required to guarantee a benign local landscape surrounding global minima in high dimensions. Let $n$ and $d$ denote the sample size and input dimension, respectively. We first explore the local convexity and establish that when $n=o(d\log d)$, for almost every fixed point in the local ball, the Hessian matrix has negative eigenvalues, provided $d$ is sufficiently large. % Consequently, the local landscape is highly non-convex. We next consider the one-point convexity and show that, as long as $n=\omega(d)$, with high probability, the landscape is one-point strongly convex in the local annulus: $\{w\in\mathbb{R}^d: o_d(1)\leqslant \|w-w^*\|\leqslant c\}$, where $w^*$ is the ground truth and $c$ is an absolute constant. This implies that gradient descent, initialized from any point in this domain, can converge to an $o_d(1)$-loss solution exponentially fast. Furthermore, we show that when $n=o(d\log d)$, there is a radius of $\widetilde\Theta\left(\sqrt{1/d}\right)$ such that one-point convexity breaks down in the corresponding smaller local ball. This indicates an impossibility of establishing a convergence to the exact $w^*$ for gradient descent under limited samples by relying solely on one-point convexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15221v2</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaizhao Liu, Zihao Wang, Lei Wu</dc:creator>
    </item>
    <item>
      <title>Convergence of the Adapted Smoothed Empirical Measures</title>
      <link>https://arxiv.org/abs/2401.14883</link>
      <description>arXiv:2401.14883v2 Announce Type: replace-cross 
Abstract: The adapted Wasserstein distance controls the calibration errors of optimal values in various stochastic optimization problems, pricing and hedging problems, optimal stopping problems, etc. However, statistical aspects of the adapted Wasserstein distance are bottlenecked by the failure of empirical measures to converge under this distance. Kernel smoothing and adapted projection have been introduced to construct converging substitutes of empirical measures, known respectively as smoothed empirical measures and adapted empirical measures. However, both approaches have limitations. Specifically, smoothed empirical measures lack comprehensive convergence results, whereas adapted empirical measures in practical applications lead to fewer distinct samples compared to standard empirical measures.
  In this work, we address both of the aforementioned issues. First, we develop comprehensive convergence results of smoothed empirical measures. We then introduce a smoothed version for adapted empirical measures, which provide as many distinct samples as desired. We refer them as adapted smoothed empirical measures and establish their convergence in mean, deviation, and almost sure convergence. The convergence estimation incorporates two results: the empirical analysis of the smoothed adapted Wasserstein distance and its bandwidth effects. Both results are novel and their proof techniques could be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14883v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Songyan Hou</dc:creator>
    </item>
    <item>
      <title>Matrix denoising: Bayes-optimal estimators via low-degree polynomials</title>
      <link>https://arxiv.org/abs/2402.16719</link>
      <description>arXiv:2402.16719v2 Announce Type: replace-cross 
Abstract: We consider the additive version of the matrix denoising problem, where a random symmetric matrix $S$ of size $n$ has to be inferred from the observation of $Y=S+Z$, with $Z$ an independent random matrix modeling a noise. For prior distributions of $S$ and $Z$ that are invariant under conjugation by orthogonal matrices we determine, using results from first and second order free probability theory, the Bayes-optimal (in terms of the mean square error) polynomial estimators of degree at most $D$, asymptotically in $n$, and show that as $D$ increases they converge towards the estimator introduced by Bun, Allez, Bouchaud and Potters in [IEEE Transactions on Information Theory 62, 7475 (2016)]. We conjecture that this optimality holds beyond strictly orthogonally invariant priors, and provide partial evidences of this universality phenomenon when $S$ is an arbitrary Wishart matrix and $Z$ is drawn from the Gaussian Orthogonal Ensemble, a case motivated by the related extensive rank matrix factorization problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16719v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guilhem Semerjian</dc:creator>
    </item>
    <item>
      <title>Random Multi-Type Spanning Forests for Synchronization on Sparse Graphs</title>
      <link>https://arxiv.org/abs/2403.19300</link>
      <description>arXiv:2403.19300v2 Announce Type: replace-cross 
Abstract: Random diffusions are a popular tool in Monte-Carlo estimations, with well established algorithms such as Walk-on-Spheres (WoS) going back several decades. In this work, we introduce diffusion estimators for the problems of angular synchronization and smoothing on graphs, in the presence of a rotation associated to each edge. Unlike classical WoS algorithms that are point-wise estimators, our diffusion estimators allow for global estimations by propagating along the branches of random spanning subgraphs called multi-type spanning forests. Building upon efficient samplers based on variants of Wilson's algorithm, we show that our estimators outperform standard numerical-linear-algebra solvers in challenging instances, depending on the topology and density of the graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19300v2</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugo Jaquard, Pierre-Olivier Amblard, Simon Barthelm\'e, Nicolas Tremblay</dc:creator>
    </item>
    <item>
      <title>New matrix perturbation bounds via contour expansion</title>
      <link>https://arxiv.org/abs/2409.20207</link>
      <description>arXiv:2409.20207v2 Announce Type: replace-cross 
Abstract: Matrix perturbation bounds (such as Weyl and Davis-Kahan) are frequently used in many branches of mathematics. Most of the classical results in this area are optimal, in the worst case analysis. However, in modern applications, both the ground and the nose matrices frequently have extra structural properties. For instance, it is often assumed that the ground matrix is essentially low rank, and the nose matrix is random or pseudo-random. We aim to rebuild a part of perturbation theory, adapting to these modern assumptions. We will do this using a contour expansion argument, which enables us to exploit the skewness among the leading eigenvectors of the ground and the noise matrix (which is significant when the two are uncorrelated) to our advantage. In the current paper, we focus on the perturbation of eigenspaces. This helps us to introduce the arguments in the cleanest way, avoiding the more technical consideration of the general case. In applications, this case is also one of the most useful. More general results appear in a subsequent paper. Our method has led to several improvements, which have direct applications in central problems. Among others, we derive a sharp result for perturbation of a low rank matrix with random perturbation, answering an open question in this area. Next, we derive new results concerning the spike model, an important model in statistics, bridging two different directions of current research. Finally, we use our results on the perturbation of eigenspaces to derive new results concerning eigenvalues of deterministic and random matrices. In particular, we obtain new results concerning the outliers in the deformed Wigner model and the least singular value of random matrices with non-zero mean.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20207v2</guid>
      <category>math.SP</category>
      <category>math.CO</category>
      <category>math.FA</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Phuc Tran, Van Vu</dc:creator>
    </item>
  </channel>
</rss>
