<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 22 Jul 2024 10:54:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 22 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Building Population-Informed Priors for Bayesian Inference Using Data-Consistent Stochastic Inversion</title>
      <link>https://arxiv.org/abs/2407.13814</link>
      <description>arXiv:2407.13814v1 Announce Type: new 
Abstract: Bayesian inference provides a powerful tool for leveraging observational data to inform model predictions and uncertainties. However, when such data is limited, Bayesian inference may not adequately constrain uncertainty without the use of highly informative priors. Common approaches for constructing informative priors typically rely on either assumptions or knowledge of the underlying physics, which may not be available in all scenarios. In this work, we consider the scenario where data are available on a population of assets/individuals, which occurs in many problem domains such as biomedical or digital twin applications, and leverage this population-level data to systematically constrain the Bayesian prior and subsequently improve individualized inferences. The approach proposed in this paper is based upon a recently developed technique known as data-consistent inversion (DCI) for constructing a pullback probability measure. Succinctly, we utilize DCI to build population-informed priors for subsequent Bayesian inference on individuals. While the approach is general and applies to nonlinear maps and arbitrary priors, we prove that for linear inverse problems with Gaussian priors, the population-informed prior produces an increase in the information gain as measured by the determinant and trace of the inverse posterior covariance. We also demonstrate that the Kullback-Leibler divergence often improves with high probability. Numerical results, including linear-Gaussian examples and one inspired by digital twins for additively manufactured assets, indicate that there is significant value in using these population-informed priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13814v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rebekah D. White, John D. Jakeman, Tim Wildey, Troy Butler</dc:creator>
    </item>
    <item>
      <title>Projection-pursuit Bayesian regression for symmetric matrix predictors</title>
      <link>https://arxiv.org/abs/2407.13865</link>
      <description>arXiv:2407.13865v1 Announce Type: new 
Abstract: This paper develops a novel Bayesian approach for nonlinear regression with symmetric matrix predictors, often used to encode connectivity of different nodes. Unlike methods that vectorize matrices as predictors that result in a large number of model parameters and unstable estimation, we propose a Bayesian multi-index regression method, resulting in a projection-pursuit-type estimator that leverages the structure of matrix-valued predictors. We establish the model identifiability conditions and impose a sparsity-inducing prior on the projection directions for sparse sampling to prevent overfitting and enhance interpretability of the parameter estimates. Posterior inference is conducted through Bayesian backfitting. The performance of the proposed method is evaluated through simulation studies and a case study investigating the relationship between brain connectivity features and cognitive scores.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13865v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaomeng Ju, Hyung G. Park, Thaddeus Tarpey</dc:creator>
    </item>
    <item>
      <title>In defense of MAR over latent ignorability (or latent MAR) for outcome missingness in studying principal causal effects: a causal graph view</title>
      <link>https://arxiv.org/abs/2407.13904</link>
      <description>arXiv:2407.13904v1 Announce Type: new 
Abstract: This paper concerns outcome missingness in principal stratification analysis. We revisit a common assumption known as latent ignorability or latent missing-at-random (LMAR), often considered a relaxation of missing-at-random (MAR). LMAR posits that the outcome is independent of its missingness if one conditions on principal stratum (which is partially unobservable) in addition to observed variables. The literature has focused on methods assuming LMAR (usually supplemented with a more specific assumption about the missingness), without considering the theoretical plausibility and necessity of LMAR. In this paper, we devise a way to represent principal stratum in causal graphs, and use causal graphs to examine this assumption. We find that LMAR is harder to satisfy than MAR, and for the purpose of breaking the dependence between the outcome and its missingness, no benefit is gained from conditioning on principal stratum on top of conditioning on observed variables. This finding has an important implication: MAR should be preferred over LMAR. This is convenient because MAR is easier to handle and (unlike LMAR) if MAR is assumed no additional assumption is needed. We thus turn to focus on the plausibility of MAR and its implications, with a view to facilitate appropriate use of this assumption. We clarify conditions on the causal structure and on auxiliary variables (if available) that need to hold for MAR to hold, and we use MAR to recover effect identification under two dominant identification assumptions (exclusion restriction and principal ignorability). We briefly comment on cases where MAR does not hold. In terms of broader connections, most of the MAR findings are also relevant to classic instrumental variable analysis that targets the local average treatment effect; and the LMAR finding suggests general caution with assumptions that condition on principal stratum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13904v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Trang Quynh Nguyen</dc:creator>
    </item>
    <item>
      <title>Flexible max-stable processes for fast and efficient inference</title>
      <link>https://arxiv.org/abs/2407.13958</link>
      <description>arXiv:2407.13958v1 Announce Type: new 
Abstract: Max-stable processes serve as the fundamental distributional family in extreme value theory. However, likelihood-based inference methods for max-stable processes still heavily rely on composite likelihoods, rendering them intractable in high dimensions due to their intractable densities. In this paper, we introduce a fast and efficient inference method for max-stable processes based on their angular densities for a class of max-stable processes whose angular densities do not put mass on the boundary space of the simplex, which can be used to construct r-Pareto processes. We demonstrate the efficiency of the proposed method through two new max-stable processes, the truncated extremal-t process and the skewed Brown-Resnick process. The proposed method is shown to be computationally efficient and can be applied to large datasets. Furthermore, the skewed Brown-Resnick process contains the popular Brown-Resnick model as a special case and possesses nonstationary extremal dependence structures. We showcase the new max-stable processes on simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13958v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peng Zhong, Scott A. Sisson, Boris Beranger</dc:creator>
    </item>
    <item>
      <title>Dimension-reduced Reconstruction Map Learning for Parameter Estimation in Likelihood-Free Inference Problems</title>
      <link>https://arxiv.org/abs/2407.13971</link>
      <description>arXiv:2407.13971v1 Announce Type: new 
Abstract: Many application areas rely on models that can be readily simulated but lack a closed-form likelihood, or an accurate approximation under arbitrary parameter values. Existing parameter estimation approaches in this setting are generally approximate. Recent work on using neural network models to reconstruct the mapping from the data space to the parameters from a set of synthetic parameter-data pairs suffers from the curse of dimensionality, resulting in inaccurate estimation as the data size grows. We propose a dimension-reduced approach to likelihood-free estimation which combines the ideas of reconstruction map estimation with dimension-reduction approaches based on subject-specific knowledge. We examine the properties of reconstruction map estimation with and without dimension reduction and explore the trade-off between approximation error due to information loss from reducing the data dimension and approximation error. Numerical examples show that the proposed approach compares favorably with reconstruction map estimation, approximate Bayesian computation, and synthetic likelihood estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13971v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rui Zhang, Oksana A. Chkrebtii, Dongbin Xiu</dc:creator>
    </item>
    <item>
      <title>Byzantine-tolerant distributed learning of finite mixture models</title>
      <link>https://arxiv.org/abs/2407.13980</link>
      <description>arXiv:2407.13980v1 Announce Type: new 
Abstract: This paper proposes two split-and-conquer (SC) learning estimators for finite mixture models that are tolerant to Byzantine failures. In SC learning, individual machines obtain local estimates, which are then transmitted to a central server for aggregation. During this communication, the server may receive malicious or incorrect information from some local machines, a scenario known as Byzantine failures. While SC learning approaches have been devised to mitigate Byzantine failures in statistical models with Euclidean parameters, developing Byzantine-tolerant methods for finite mixture models with non-Euclidean parameters requires a distinct strategy. Our proposed distance-based methods are hyperparameter tuning free, unlike existing methods, and are resilient to Byzantine failures while achieving high statistical efficiency. We validate the effectiveness of our methods both theoretically and empirically via experiments on simulated and real data from machine learning applications for digit recognition. The code for the experiment can be found at https://github.com/SarahQiong/RobustSCGMM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13980v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiong Zhang, Jiahua Chen</dc:creator>
    </item>
    <item>
      <title>Derandomized Truncated D-vine Copula Knockoffs with e-values to control the false discovery rate</title>
      <link>https://arxiv.org/abs/2407.14002</link>
      <description>arXiv:2407.14002v1 Announce Type: new 
Abstract: The Model-X knockoffs is a practical methodology for variable selection, which stands out from other selection strategies since it allows for the control of the false discovery rate (FDR), relying on finite-sample guarantees. In this article, we propose a Truncated D-vine Copula Knockoffs (TDCK) algorithm for sampling approximate knockoffs from complex multivariate distributions. Our algorithm enhances and improves features of previous attempts to sample knockoffs under the multivariate setting, with the three main contributions being: 1) the truncation of the D-vine copula, which reduces the dependence between the original variables and their corresponding knockoffs, improving the statistical power; 2) the employment of a straightforward non-parametric formulation for marginal transformations, eliminating the need for a specific parametric family or a kernel density estimator; 3) the use of the "rvinecopulib'' R package offers better flexibility than the existing fitting vine copula knockoff methods. To eliminate the randomness in distinct realizations resulting in different sets of selected variables, we wrap the TDCK method with an existing derandomizing procedure for knockoffs, leading to a Derandomized Truncated D-vine Copula Knockoffs with e-values (DTDCKe) procedure. We demonstrate the robustness of the DTDCKe procedure under various scenarios with extensive simulation studies. We further illustrate its efficacy using a gene expression dataset, showing it achieves a more reliable gene selection than other competing methods, when the findings are compared with those of a meta-analysis. The results indicate that our Truncated D-vine copula approach is robust and has superior power, representing an appealing approach for variable selection in different multivariate applications, particularly in gene expression analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14002v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alejandro Rom\'an V\'asquez, Jos\'e Ulises M\'arquez Urbina, Graciela Gonz\'alez Far\'ias, Gabriel Escarela</dc:creator>
    </item>
    <item>
      <title>Causal Inference with Complex Treatments: A Survey</title>
      <link>https://arxiv.org/abs/2407.14022</link>
      <description>arXiv:2407.14022v1 Announce Type: new 
Abstract: Causal inference plays an important role in explanatory analysis and decision making across various fields like statistics, marketing, health care, and education. Its main task is to estimate treatment effects and make intervention policies. Traditionally, most of the previous works typically focus on the binary treatment setting that there is only one treatment for a unit to adopt or not. However, in practice, the treatment can be much more complex, encompassing multi-valued, continuous, or bundle options. In this paper, we refer to these as complex treatments and systematically and comprehensively review the causal inference methods for addressing them. First, we formally revisit the problem definition, the basic assumptions, and their possible variations under specific conditions. Second, we sequentially review the related methods for multi-valued, continuous, and bundled treatment settings. In each situation, we tentatively divide the methods into two categories: those conforming to the unconfoundedness assumption and those violating it. Subsequently, we discuss the available datasets and open-source codes. Finally, we provide a brief summary of these works and suggest potential directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14022v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingrong Wang, Haoxuan Li, Minqin Zhu, Anpeng Wu, Ruoxuan Xiong, Fei Wu, Kun Kuang</dc:creator>
    </item>
    <item>
      <title>Incertus.jl -- The Julia Lego Blocks for Randomized Clinical Trial Designs</title>
      <link>https://arxiv.org/abs/2407.14248</link>
      <description>arXiv:2407.14248v1 Announce Type: new 
Abstract: In this paper, we present Insertus.jl, the Julia package that can help the user generate a randomization sequence of a given length for a multi-arm trial with a pre-specified target allocation ratio and assess the operating characteristics of the chosen randomization method through Monte Carlo simulations. The developed package is computationally efficient, and it can be invoked in R. Furthermore, the package is open-ended -- it can flexibly accommodate new randomization procedures and evaluate their statistical properties via simulation. It may be also helpful for validating other randomization methods for which software is not readily available. In summary, Insertus.jl can be used as ``Lego Blocks'' to construct a fit-for-purpose randomization procedure for a given clinical trial design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14248v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yevgen Ryeznik, Oleksandr Sverdlov</dc:creator>
    </item>
    <item>
      <title>A Bayesian joint model of multiple longitudinal and categorical outcomes with application to multiple myeloma using permutation-based variable importance</title>
      <link>https://arxiv.org/abs/2407.14311</link>
      <description>arXiv:2407.14311v1 Announce Type: new 
Abstract: Joint models have proven to be an effective approach for uncovering potentially hidden connections between various types of outcomes, mainly continuous, time-to-event, and binary. Typically, longitudinal continuous outcomes are characterized by linear mixed-effects models, survival outcomes are described by proportional hazards models, and the link between outcomes are captured by shared random effects. Other modeling variations include generalized linear mixed-effects models for longitudinal data and logistic regression when a binary outcome is present, rather than time until an event of interest. However, in a clinical research setting, one might be interested in modeling the physician's chosen treatment based on the patient's medical history in order to identify prognostic factors. In this situation, there are often multiple treatment options, requiring the use of a multiclass classification approach. Inspired by this context, we develop a Bayesian joint model for longitudinal and categorical data. In particular, our motivation comes from a multiple myeloma study, in which biomarkers display nonlinear trajectories that are well captured through bi-exponential submodels, where patient-level information is shared with the categorical submodel. We also present a variable importance strategy for ranking prognostic factors. We apply our proposal and a competing model to the multiple myeloma data, compare the variable importance and inferential results for both models, and illustrate patient-level interpretations using our joint model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14311v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danilo Alvares, Jessica K. Barrett, Fran\c{c}ois Mercier, Jochen Schulze, Sean Yiu, Felipe Castro, Spyros Roumpanis, Yajing Zhu</dc:creator>
    </item>
    <item>
      <title>Measuring and testing tail equivalence</title>
      <link>https://arxiv.org/abs/2407.14349</link>
      <description>arXiv:2407.14349v1 Announce Type: new 
Abstract: We call two copulas tail equivalent if their first-order approximations in the tail coincide. As a special case, a copula is called tail symmetric if it is tail equivalent to the associated survival copula. We propose a novel measure and statistical test for tail equivalence. The proposed measure takes the value of zero if and only if the two copulas share a pair of tail order and tail order parameter in common. Moreover, taking the nature of these tail quantities into account, we design the proposed measure so that it takes a large value when tail orders are different, and a small value when tail order parameters are non-identical. We derive asymptotic properties of the proposed measure, and then propose a novel statistical test for tail equivalence. Performance of the proposed test is demonstrated in a series of simulation studies and empirical analyses of financial stock returns in the periods of the world financial crisis and the COVID-19 recession. Our empirical analysis reveals non-identical tail behaviors in different pairs of stocks, different parts of tails, and the two periods of recessions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14349v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takaaki Koike, Shogo Kato, Toshinao Yoshiba</dc:creator>
    </item>
    <item>
      <title>Modified BART for Learning Heterogeneous Effects in Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2407.14365</link>
      <description>arXiv:2407.14365v1 Announce Type: new 
Abstract: This paper introduces BART-RDD, a sum-of-trees regression model built around a novel regression tree prior, which incorporates the special covariate structure of regression discontinuity designs. Specifically, the tree splitting process is constrained to ensure overlap within a narrow band surrounding the running variable cutoff value, where the treatment effect is identified. It is shown that unmodified BART-based models estimate RDD treatment effects poorly, while our modified model accurately recovers treatment effects at the cutoff. Specifically, BART-RDD is perhaps the first RDD method that effectively learns conditional average treatment effects. The new method is investigated in thorough simulation studies as well as an empirical application looking at the effect of academic probation on student performance in subsequent terms (Lindo et al., 2010).</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14365v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rafael Alcantara, Meijia Wang, P. Richard Hahn, Hedibert Lopes</dc:creator>
    </item>
    <item>
      <title>tidychangepoint: a unified framework for analyzing changepoint detection in univariate time series</title>
      <link>https://arxiv.org/abs/2407.14369</link>
      <description>arXiv:2407.14369v1 Announce Type: new 
Abstract: We present tidychangepoint, a new R package for changepoint detection analysis. tidychangepoint leverages existing packages like changepoint, GA, tsibble, and broom to provide tidyverse-compliant tools for segmenting univariate time series using various changepoint detection algorithms. In addition, tidychangepoint also provides model-fitting procedures for commonly-used parametric models, tools for computing various penalized objective functions, and graphical diagnostic displays. tidychangepoint wraps both deterministic algorithms like PELT, and also flexible, randomized, genetic algorithms that can be used with any compliant model-fitting function and any penalized objective function. By bringing all of these disparate tools together in a cohesive fashion, tidychangepoint facilitates comparative analysis of changepoint detection algorithms and models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14369v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin S. Baumer, Biviana Marcela Suarez Sierra</dc:creator>
    </item>
    <item>
      <title>Time Series Generative Learning with Application to Brain Imaging Analysis</title>
      <link>https://arxiv.org/abs/2407.14003</link>
      <description>arXiv:2407.14003v1 Announce Type: cross 
Abstract: This paper focuses on the analysis of sequential image data, particularly brain imaging data such as MRI, fMRI, CT, with the motivation of understanding the brain aging process and neurodegenerative diseases. To achieve this goal, we investigate image generation in a time series context. Specifically, we formulate a min-max problem derived from the $f$-divergence between neighboring pairs to learn a time series generator in a nonparametric manner. The generator enables us to generate future images by transforming prior lag-k observations and a random vector from a reference distribution. With a deep neural network learned generator, we prove that the joint distribution of the generated sequence converges to the latent truth under a Markov and a conditional invariance condition. Furthermore, we extend our generation mechanism to a panel data scenario to accommodate multiple samples. The effectiveness of our mechanism is evaluated by generating real brain MRI sequences from the Alzheimer's Disease Neuroimaging Initiative. These generated image sequences can be used as data augmentation to enhance the performance of further downstream tasks, such as Alzheimer's disease detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14003v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenghao Li, Sanyou Wu, Long Feng</dc:creator>
    </item>
    <item>
      <title>Regression Adjustment for Estimating Distributional Treatment Effects in Randomized Controlled Trials</title>
      <link>https://arxiv.org/abs/2407.14074</link>
      <description>arXiv:2407.14074v1 Announce Type: cross 
Abstract: In this paper, we address the issue of estimating and inferring the distributional treatment effects in randomized experiments. The distributional treatment effect provides a more comprehensive understanding of treatment effects by characterizing heterogeneous effects across individual units, as opposed to relying solely on the average treatment effect. To enhance the precision of distributional treatment effect estimation, we propose a regression adjustment method that utilizes the distributional regression and pre-treatment information. Our method is designed to be free from restrictive distributional assumptions. We establish theoretical efficiency gains and develop a practical, statistically sound inferential framework. Through extensive simulation studies and empirical applications, we illustrate the substantial advantages of our method, equipping researchers with a powerful tool for capturing the full spectrum of treatment effects in experimental research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14074v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tatsushi Oka, Shota Yasui, Yuta Hayakawa, Undral Byambadalai</dc:creator>
    </item>
    <item>
      <title>From Small Scales to Large Scales: Distance-to-Measure Density based Geometric Analysis of Complex Data</title>
      <link>https://arxiv.org/abs/2205.07689</link>
      <description>arXiv:2205.07689v3 Announce Type: replace 
Abstract: How can we tell complex point clouds with different small scale characteristics apart, while disregarding global features? Can we find a suitable transformation of such data in a way that allows to discriminate between differences in this sense with statistical guarantees? In this paper, we consider the analysis and classification of complex point clouds as they are obtained, e.g., via single molecule localization microscopy. We focus on the task of identifying differences between noisy point clouds based on small scale characteristics, while disregarding large scale information such as overall size. We propose an approach based on a transformation of the data via the so-called Distance-to-Measure (DTM) function, a transformation which is based on the average of nearest neighbor distances. For each data set, we estimate the probability density of average local distances of all data points and use the estimated densities for classification. While the applicability is immediate and the practical performance of the proposed methodology is very good, the theoretical study of the density estimators is quite challenging, as they are based on i.i.d. observations that have been obtained via a complicated transformation. In fact, the transformed data are stochastically dependent in a non-local way that is not captured by commonly considered dependence measures. Nonetheless, we show that the asymptotic behaviour of the density estimator is driven by a kernel density estimator of certain i.i.d. random variables by using theoretical properties of U-statistics, which allows to handle the dependencies via a Hoeffding decomposition. We show via a numerical study and in an application to simulated single molecule localization microscopy data of chromatin fibers that unsupervised classification tasks based on estimated DTM-densities achieve excellent separation results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.07689v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katharina Proksch, Christoph Alexander Weitkamp, Thomas Staudt, Beno\^it Lelandais, Christophe Zimmer</dc:creator>
    </item>
    <item>
      <title>Spline-Based Multi-State Models for Analyzing Disease Progression</title>
      <link>https://arxiv.org/abs/2312.05345</link>
      <description>arXiv:2312.05345v4 Announce Type: replace 
Abstract: Motivated by disease progression-related studies, we propose an estimation method for fitting general non-homogeneous multi-state Markov models. The proposal can handle many types of multi-state processes, with several states and various combinations of observation schemes (e.g., intermittent, exactly observed, censored), and allows for the transition intensities to be flexibly modelled through additive (spline-based) predictors. The algorithm is based on a computationally efficient and stable penalized maximum likelihood estimation approach which exploits the information provided by the analytical Hessian matrix of the model log-likelihood. The proposed modeling framework is employed in case studies that aim at modeling the onset of cardiac allograft vasculopathy, and cognitive decline due to aging, where novel patterns are uncovered. To support applicability and reproducibility, all developed tools are implemented in the R package flexmsm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05345v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessia Eletti, Giampiero Marra, Rosalba Radice</dc:creator>
    </item>
    <item>
      <title>A Robust Bayesian Method for Building Polygenic Risk Scores using Projected Summary Statistics and Bridge Prior</title>
      <link>https://arxiv.org/abs/2401.15014</link>
      <description>arXiv:2401.15014v2 Announce Type: replace 
Abstract: Polygenic risk scores (PRS) developed from genome-wide association studies (GWAS) are of increasing interest for clinical and research applications. Bayesian methods have been popular for building PRS because of their natural ability to regularize models and incorporate external information. In this article, we present new theoretical results, methods, and extensive numerical studies to advance Bayesian methods for PRS applications. We identify a potential risk, under a common Bayesian PRS framework, of posterior impropriety when integrating the required GWAS summary-statistics and linkage disequilibrium (LD) data from two distinct sources. As a principled remedy to this problem, we propose a projection of the summary statistics data that ensures compatibility between the two sources and in turn a proper behavior of the posterior. We further introduce a new PRS method, with accompanying software package, under the less-explored Bayesian bridge prior to more flexibly model varying sparsity levels in effect size distributions. We extensively benchmark it against alternative Bayesian methods using both synthetic and real datasets, quantifying the impact of both prior specification and LD estimation strategy. Our proposed PRS-Bridge, equipped with the projection technique and flexible prior, demonstrates the most consistent and generally superior performance across a variety of scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15014v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuzheng Dun, Nilanjan Chatterjee, Jin Jin, Akihiko Nishimura</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for High-dimensional Time Series by Latent Process Modeling</title>
      <link>https://arxiv.org/abs/2403.04915</link>
      <description>arXiv:2403.04915v2 Announce Type: replace 
Abstract: Time series data arising in many applications nowadays are high-dimensional. A large number of parameters describe features of these time series. We propose a novel approach to modeling a high-dimensional time series through several independent univariate time series, which are then orthogonally rotated and sparsely linearly transformed. With this approach, any specified intrinsic relations among component time series given by a graphical structure can be maintained at all time snapshots. We call the resulting process an Orthogonally-rotated Univariate Time series (OUT). Key structural properties of time series such as stationarity and causality can be easily accommodated in the OUT model. For Bayesian inference, we put suitable prior distributions on the spectral densities of the independent latent times series, the orthogonal rotation matrix, and the common precision matrix of the component times series at every time point. A likelihood is constructed using the Whittle approximation for univariate latent time series. An efficient Markov Chain Monte Carlo (MCMC) algorithm is developed for posterior computation. We study the convergence of the pseudo-posterior distribution based on the Whittle likelihood for the model's parameters upon developing a new general posterior convergence theorem for pseudo-posteriors. We find that the posterior contraction rate for independent observations essentially prevails in the OUT model under very mild conditions on the temporal dependence described in terms of the smoothness of the corresponding spectral densities. Through a simulation study, we compare the accuracy of estimating the parameters and identifying the graphical structure with other approaches. We apply the proposed methodology to analyze a dataset on different industrial components of the US gross domestic product between 2010 and 2019 and predict future observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04915v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arkaprava Roy, Anindya Roy, Subhashis Ghosal</dc:creator>
    </item>
    <item>
      <title>Exploring the difficulty of estimating win probability: a simulation study</title>
      <link>https://arxiv.org/abs/2406.16171</link>
      <description>arXiv:2406.16171v2 Announce Type: replace 
Abstract: Estimating win probability is one of the classic modeling tasks of sports analytics. Many widely used win probability estimators are statistical win probability models, which fit the relationship between a binary win/loss outcome variable and certain game-state variables using data-driven regression or machine learning approaches. To illustrate just how difficult it is to accurately fit a statistical win probability model from noisy and highly correlated observational data, in this paper we conduct a simulation study. We create a simplified random walk version of football in which true win probability at each game-state is known, and we see how well a model recovers it. We find that the dependence structure of observational play-by-play data substantially inflates the bias and variance of estimators and lowers the effective sample size. This makes it essential to quantify uncertainty in win probability estimates, but typical bootstrapped confidence intervals are too narrow and don't achieve nominal coverage. Hence, we introduce a novel method, the fractional bootstrap, to calibrate these intervals to achieve adequate coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16171v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan S. Brill, Abraham J. Wyner</dc:creator>
    </item>
    <item>
      <title>Algorithms for Non-Negative Matrix Factorization on Noisy Data With Negative Values</title>
      <link>https://arxiv.org/abs/2311.04855</link>
      <description>arXiv:2311.04855v3 Announce Type: replace-cross 
Abstract: Non-negative matrix factorization (NMF) is a dimensionality reduction technique that has shown promise for analyzing noisy data, especially astronomical data. For these datasets, the observed data may contain negative values due to noise even when the true underlying physical signal is strictly positive. Prior NMF work has not treated negative data in a statistically consistent manner, which becomes problematic for low signal-to-noise data with many negative values. In this paper we present two algorithms, Shift-NMF and Nearly-NMF, that can handle both the noisiness of the input data and also any introduced negativity. Both of these algorithms use the negative data space without clipping, and correctly recover non-negative signals without any introduced positive offset that occurs when clipping negative data. We demonstrate this numerically on both simple and more realistic examples, and prove that both algorithms have monotonically decreasing update rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04855v3</guid>
      <category>astro-ph.IM</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dylan Green, Stephen Bailey</dc:creator>
    </item>
    <item>
      <title>Modeling Long Sequences in Bladder Cancer Recurrence: A Comparative Evaluation of LSTM,Transformer,and Mamba</title>
      <link>https://arxiv.org/abs/2405.18518</link>
      <description>arXiv:2405.18518v2 Announce Type: replace-cross 
Abstract: Traditional survival analysis methods often struggle with complex time-dependent data,failing to capture and interpret dynamic characteristics adequately.This study aims to evaluate the performance of three long-sequence models,LSTM,Transformer,and Mamba,in analyzing recurrence event data and integrating them with the Cox proportional hazards model.This study integrates the advantages of deep learning models for handling long-sequence data with the Cox proportional hazards model to enhance the performance in analyzing recurrent events with dynamic time information.Additionally,this study compares the ability of different models to extract and utilize features from time-dependent clinical recurrence data.The LSTM-Cox model outperformed both the Transformer-Cox and Mamba-Cox models in prediction accuracy and model fit,achieving a Concordance index of up to 0.90 on the test set.Significant predictors of bladder cancer recurrence,such as treatment stop time,maximum tumor size at recurrence and recurrence frequency,were identified.The LSTM-Cox model aligned well with clinical outcomes,effectively distinguishing between high-risk and low-risk patient groups.This study demonstrates that the LSTM-Cox model is a robust and efficient method for recurrent data analysis and feature extraction,surpassing newer models like Transformer and Mamba.It offers a practical approach for integrating deep learning technologies into clinical risk prediction systems,thereby improving patient management and treatment outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18518v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Runquan Zhang, Jiawen Jiang, Xiaoping Shi</dc:creator>
    </item>
  </channel>
</rss>
