<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.IV updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.IV</link>
    <description>eess.IV updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.IV" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 21 May 2024 04:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 21 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Classification of colorectal primer carcinoma from normal colon with mid-infrared spectra</title>
      <link>https://arxiv.org/abs/2405.10950</link>
      <description>arXiv:2405.10950v1 Announce Type: new 
Abstract: In this project, we used formalin-fixed paraffin-embedded (FFPE) tissue samples to measure thousands of spectra per tissue core with Fourier transform mid-infrared spectroscopy using an FT-IR imaging system. These cores varied between normal colon (NC) and colorectal primer carcinoma (CRC) tissues. We created a database to manage all the multivariate data obtained from the measurements. Then, we applied classifier algorithms to identify the tissue based on its yielded spectra. For classification, we used the random forest, a support vector machine, XGBoost, and linear discriminant analysis methods, as well as three deep neural networks. We compared two data manipulation techniques using these models and then applied filtering. In the end, we compared model performances via the sum of ranking differences (SRD).</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10950v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>q-bio.TO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/cem.3542</arxiv:DOI>
      <dc:creator>B. Borkovits, E. Kontsek, A. Pesti, P. Gordon, S. Gergely, I. Csabai, A. Kiss, P. Pollner</dc:creator>
    </item>
    <item>
      <title>A reproducible pipeline for extracting representative signals from wire cuts</title>
      <link>https://arxiv.org/abs/2405.11012</link>
      <description>arXiv:2405.11012v1 Announce Type: new 
Abstract: We propose a reproducible pipeline for extracting representative signals from 2D topographic scans of the tips of cut wires. The process fully addresses many potential problems in the quality of wire cuts, including edge effects, extreme values, trends, missing values, angles, and warping. The resulting signals can be further used in source determination, which plays an important role in forensic examinations. With commonly used measurements such as the cross-correlation function, the procedure controls the false positive rate and false negative rate to the desirable values as the manual extraction pipeline but outperforms it with robustness and objectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11012v1</guid>
      <category>eess.IV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuhang Lin, Heike Hofmann</dc:creator>
    </item>
    <item>
      <title>Ptychographic non-line-of-sight imaging for depth-resolved visualization of hidden objects</title>
      <link>https://arxiv.org/abs/2405.11115</link>
      <description>arXiv:2405.11115v1 Announce Type: new 
Abstract: Non-line-of-sight (NLOS) imaging enables the visualization of objects hidden from direct view, with applications in surveillance, remote sensing, and light detection and ranging. Here, we introduce a NLOS imaging technique termed ptychographic NLOS (pNLOS), which leverages coded ptychography for depth-resolved imaging of obscured objects. Our approach involves scanning a laser spot on a wall to illuminate the hidden objects in an obscured region. The reflected wavefields from these objects then travel back to the wall, get modulated by the wall's complex-valued profile, and the resulting diffraction patterns are captured by a camera. By modulating the object wavefields, the wall surface serves the role of the coded layer as in coded ptychography. As we scan the laser spot to different positions, the reflected object wavefields on the wall translate accordingly, with the shifts varying for objects at different depths. This translational diversity enables the acquisition of a set of modulated diffraction patterns referred to as a ptychogram. By processing the ptychogram, we recover both the objects at different depths and the modulation profile of the wall surface. Experimental results demonstrate high-resolution, high-fidelity imaging of hidden objects, showcasing the potential of pNLOS for depth-aware vision beyond the direct line of sight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11115v1</guid>
      <category>eess.IV</category>
      <category>physics.optics</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pengming Song, Qianhao Zhao, Ruihai Wang, Ninghe Liu, Yingqi Qiang, Tianbo Wang, Xincheng Zhang, Yi Zhang, Liangcai Cao, Guoan Zheng</dc:creator>
    </item>
    <item>
      <title>XCAT-2.0: A Comprehensive Library of Personalized Digital Twins Derived from CT Scans</title>
      <link>https://arxiv.org/abs/2405.11133</link>
      <description>arXiv:2405.11133v1 Announce Type: new 
Abstract: Virtual Imaging Trials (VIT) offer a cost-effective and scalable approach for evaluating medical imaging technologies. Computational phantoms, which mimic real patient anatomy and physiology, play a central role in VIT. However, the current libraries of computational phantoms face limitations, particularly in terms of sample size and diversity. Insufficient representation of the population hampers accurate assessment of imaging technologies across different patient groups. Traditionally, phantoms were created by manual segmentation, which is a laborious and time-consuming task, impeding the expansion of phantom libraries. This study presents a framework for realistic computational phantom modeling using a suite of four deep learning segmentation models, followed by three forms of automated organ segmentation quality control. Over 2500 computational phantoms with up to 140 structures illustrating a sophisticated approach to detailed anatomical modeling are released. Phantoms are available in both voxelized and surface mesh formats. The framework is aggregated with an in-house CT scanner simulator to produce realistic CT images. The framework can potentially advance virtual imaging trials, facilitating comprehensive and reliable evaluations of medical imaging technologies. Phantoms may be requested at https://cvit.duke.edu/resources/, code, model weights, and sample CT images are available at https://xcat-2.github.io.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11133v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lavsen Dahal, Mobina Ghojoghnejad, Dhrubajyoti Ghosh, Yubraj Bhandari, David Kim, Fong Chi Ho, Fakrul Islam Tushar, Ehsan Abadi, Ehsan Samei, Joseph Lo, Paul Segars</dc:creator>
    </item>
    <item>
      <title>Diffusion Model Driven Test-Time Image Adaptation for Robust Skin Lesion Classification</title>
      <link>https://arxiv.org/abs/2405.11289</link>
      <description>arXiv:2405.11289v1 Announce Type: new 
Abstract: Deep learning-based diagnostic systems have demonstrated potential in skin disease diagnosis. However, their performance can easily degrade on test domains due to distribution shifts caused by input-level corruptions, such as imaging equipment variability, brightness changes, and image blur. This will reduce the reliability of model deployment in real-world scenarios. Most existing solutions focus on adapting the source model through retraining on different target domains. Although effective, this retraining process is sensitive to the amount of data and the hyperparameter configuration for optimization. In this paper, we propose a test-time image adaptation method to enhance the accuracy of the model on test data by simultaneously updating and predicting test images. We modify the target test images by projecting them back to the source domain using a diffusion model. Specifically, we design a structure guidance module that adds refinement operations through low-pass filtering during reverse sampling, regularizing the diffusion to preserve structural information. Additionally, we introduce a self-ensembling scheme automatically adjusts the reliance on adapted and unadapted inputs, enhancing adaptation robustness by rejecting inappropriate generative modeling results. To facilitate this study, we constructed the ISIC2019-C and Dermnet-C corruption robustness evaluation benchmarks. Extensive experiments on the proposed benchmarks demonstrate that our method makes the classifier more robust across various corruptions, architectures, and data regimes. Our datasets and code will be available at \url{https://github.com/minghu0830/Skin-TTA_Diffusion}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11289v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ming Hu, Siyuan Yan, Peng Xia, Feilong Tang, Wenxue Li, Peibo Duan, Lin Zhang, Zongyuan Ge</dc:creator>
    </item>
    <item>
      <title>Medical Image Analysis for Detection, Treatment and Planning of Disease using Artificial Intelligence Approaches</title>
      <link>https://arxiv.org/abs/2405.11295</link>
      <description>arXiv:2405.11295v1 Announce Type: new 
Abstract: X-ray is one of the prevalent image modalities for the detection and diagnosis of the human body. X-ray provides an actual anatomical structure of an organ present with disease or absence of disease. Segmentation of disease in chest X-ray images is essential for the diagnosis and treatment. In this paper, a framework for the segmentation of X-ray images using artificial intelligence techniques has been discussed. Here data has been pre-processed and cleaned followed by segmentation using SegNet and Residual Net approaches to X-ray images. Finally, segmentation has been evaluated using well known metrics like Loss, Dice Coefficient, Jaccard Coefficient, Precision, Recall, Binary Accuracy, and Validation Accuracy. The experimental results reveal that the proposed approach performs better in all respect of well-known parameters with 16 batch size and 50 epochs. The value of validation accuracy, precision, and recall of SegNet and Residual Unet models are 0.9815, 0.9699, 0.9574, and 0.9901, 0.9864, 0.9750 respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11295v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.MM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.10057577</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Microsystems and IoT, Vol. 1, Issue 5, pp.278- 287, 2023</arxiv:journal_reference>
      <dc:creator>Nand Lal Yadav, Satyendra Singh, Rajesh Kumar, Sudhakar Singh</dc:creator>
    </item>
    <item>
      <title>Liver Fat Quantification Network with Body Shape</title>
      <link>https://arxiv.org/abs/2405.11386</link>
      <description>arXiv:2405.11386v1 Announce Type: new 
Abstract: It is clinically important to detect liver fat content as it is related to cardiac complications and cardiovascular disease mortality. However, existing methods are associated with high cost and/or medical complications (e.g., liver biopsy, medical imaging technology) or only roughly estimate the grades of steatosis. In this paper, we propose a deep neural network to accurately estimate liver fat percentage using only body shapes. The proposed framework is composed of a flexible baseline regression network and a lightweight attention module. The attention module is trained to generate discriminative and diverse features, thus significantly improving performance. To validate our proposed method, we perform extensive tests on medical datasets. The experimental results validate our method and prove the efficacy of designing neural networks to predict liver fat using only body shape. Since body shapes can be acquired using inexpensive and readily available optical scanners, the proposed method promised to make accurate assessment of hepatic steatosis more accessible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11386v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiyue Wang, Wu Xue, Xiaoke Zhang, Fang Jin, James Hahn</dc:creator>
    </item>
    <item>
      <title>Verification technology for finger vein biometric</title>
      <link>https://arxiv.org/abs/2405.11540</link>
      <description>arXiv:2405.11540v1 Announce Type: new 
Abstract: Finger vein biometrics is an approach to identifying individuals based on the unique patterns of blood vessels in their fingers, and the technology is advanced in image capture and processing techniques, which is leading to more efficient, accurate, and reliable systems. This article focuses on a verification system that compares the matrices of an efficient finger vein verification system on different databases to test its strength and efficiency. Contrast Limited Adaptive Histogram Equalization (CLAHE) has been examined as an image enhancement and processing method to improve contrast and render details in an image easier to detect. A random forest classifier is deployed with a comparison between two pretrained systems, VGG16 and ResNet50, which are types of convolutional neural networks. VGG-16 and ResNet-50 models are implemented on three different datasets, and fine-tuning these models enabled the harnessing of their powerful capabilities and achieving superior performance on the specific image classification task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11540v1</guid>
      <category>eess.IV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Kumi Kyeremeh, M. Abdul-Al, R. Qahwaji, R. A. Abd-Alhameed</dc:creator>
    </item>
    <item>
      <title>AI-Assisted Diagnosis for Covid-19 CXR Screening: From Data Collection to Clinical Validation</title>
      <link>https://arxiv.org/abs/2405.11598</link>
      <description>arXiv:2405.11598v1 Announce Type: new 
Abstract: In this paper, we present the major results from the Covid Radiographic imaging System based on AI (Co.R.S.A.) project, which took place in Italy. This project aims to develop a state-of-the-art AI-based system for diagnosing Covid-19 pneumonia from Chest X-ray (CXR) images. The contributions of this work are manyfold: the release of the public CORDA dataset, a deep learning pipeline for Covid-19 detection, and the clinical validation of the developed solution by expert radiologists. The proposed detection model is based on a two-step approach that, paired with state-of-the-art debiasing, provides reliable results. Most importantly, our investigation includes the actual usage of the diagnosis aid tool by radiologists, allowing us to assess the real benefits in terms of accuracy and time efficiency. Project homepage: https://corsa.di.unito.it/</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11598v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlo Alberto Barbano, Riccardo Renzulli, Marco Grosso, Domenico Basile, Marco Busso, Marco Grangetto</dc:creator>
    </item>
    <item>
      <title>Wireless vs. Traditional Ultrasound Assessed Knee Cartilage Outcomes Utilizing Automated Gain and Normalization Techniques</title>
      <link>https://arxiv.org/abs/2405.12172</link>
      <description>arXiv:2405.12172v1 Announce Type: new 
Abstract: Advancements in wireless ultrasound technology allow for point of care cartilage imaging, yet validation against traditional ultrasound units remains to be established for knee cartilage outcomes. Therefore, the purpose of our study was to establish the agreement of articular cartilage thickness and echo-intensity measures between traditional and wireless ultrasound units utilizing automatic-gain and normalization techniques. We used traditional and wireless ultrasound to assess the femoral cartilage via transverse suprapatellar scans with the knee in maximum flexion in 71 female NCAA Division I athletes (age: 20.0$\pm$1.3 years, height: 171.7$\pm$8.7 cm, mass: 69.4$\pm$11.0 kg). Wireless ultrasound images (auto-gain and standard gain) were compared to traditional ultrasound images (standard gain) before and after normalization. Ultrasound image pixel values were algebraically scaled to normalize imaging parameter differences between units. Mean thickness and echo-intensity of the global and sub-regions of interest were measured for unnormalized and normalized images. Intraclass correlation coefficients ($ICC_{2,k}$) for absolute agreement, standard error of the measurement, and minimum detectable difference were calculated between the traditional and wireless ultrasound units across both gain parameters and normalization. Cartilage thickness demonstrated good to excellent agreement for all regions ($ICC_{2,k} = 0.83 {\text -} 0.95$) regardless of gain and normalization. However, mean echo-intensity demonstrated poor to moderate agreement in all regions regardless of gain and normalization ($ICC_{2,k} = 0.23 {\text -} 0.68 $). While there was a high level of agreement between units when assessing cartilage thickness, further research in ultrasound beam forming may lead to improvements in agreement of cartilage echo-intensity between ultrasound units.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12172v1</guid>
      <category>eess.IV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arjun Parmar, Corey D Grozier, Robert Dima, Jessica E Tolzman, Ilker Hacihaliloglu, Kenneth L Cameron, Ryan Fajardo, Matthew S Harkey</dc:creator>
    </item>
    <item>
      <title>Discrete approximations of Gaussian smoothing and Gaussian derivatives</title>
      <link>https://arxiv.org/abs/2311.11317</link>
      <description>arXiv:2311.11317v7 Announce Type: cross 
Abstract: This paper develops an in-depth treatment concerning the problem of approximating the Gaussian smoothing and Gaussian derivative computations in scale-space theory for application on discrete data. With close connections to previous axiomatic treatments of continuous and discrete scale-space theory, we consider three main ways discretizing these scale-space operations in terms of explicit discrete convolutions, based on either (i) sampling the Gaussian kernels and the Gaussian derivative kernels, (ii) locally integrating the Gaussian kernels and the Gaussian derivative kernels over each pixel support region and (iii) basing the scale-space analysis on the discrete analogue of the Gaussian kernel, and then computing derivative approximations by applying small-support central difference operators to the spatially smoothed image data.
  We study the properties of these three main discretization methods both theoretically and experimentally, and characterize their performance by quantitative measures, including the results they give rise to with respect to the task of scale selection, investigated for four different use cases, and with emphasis on the behaviour at fine scales. The results show that the sampled Gaussian kernels and derivatives as well as the integrated Gaussian kernels and derivatives perform very poorly at very fine scales. At very fine scales, the discrete analogue of the Gaussian kernel with its corresponding discrete derivative approximations performs substantially better. The sampled Gaussian kernel and the sampled Gaussian derivatives do, on the other hand, lead to numerically very good approximations of the corresponding continuous results, when the scale parameter is sufficiently large, in the experiments presented in the paper, when the scale parameter is greater than a value of about 1, in units of the grid spacing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11317v7</guid>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tony Lindeberg</dc:creator>
    </item>
    <item>
      <title>Approximation properties relative to continuous scale space for hybrid discretizations of Gaussian derivative operators</title>
      <link>https://arxiv.org/abs/2405.05095</link>
      <description>arXiv:2405.05095v2 Announce Type: cross 
Abstract: This paper presents an analysis of properties of two hybrid discretization methods for Gaussian derivatives, based on convolutions with either the normalized sampled Gaussian kernel or the integrated Gaussian kernel followed by central differences. The motivation for studying these discretization methods is that in situations when multiple spatial derivatives of different order are needed at the same scale level, they can be computed significantly more efficiently compared to more direct derivative approximations based on explicit convolutions with either sampled Gaussian kernels or integrated Gaussian kernels.
  While these computational benefits do also hold for the genuinely discrete approach for computing discrete analogues of Gaussian derivatives, based on convolution with the discrete analogue of the Gaussian kernel followed by central differences, the underlying mathematical primitives for the discrete analogue of the Gaussian kernel, in terms of modified Bessel functions of integer order, may not be available in certain frameworks for image processing, such as when performing deep learning based on scale-parameterized filters in terms of Gaussian derivatives, with learning of the scale levels.
  In this paper, we present a characterization of the properties of these hybrid discretization methods, in terms of quantitative performance measures concerning the amount of spatial smoothing that they imply, as well as the relative consistency of scale estimates obtained from scale-invariant feature detectors with automatic scale selection, with an emphasis on the behaviour for very small values of the scale parameter, which may differ significantly from corresponding results obtained from the fully continuous scale-space theory, as well as between different types of discretization methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05095v2</guid>
      <category>math.NA</category>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tony Lindeberg</dc:creator>
    </item>
    <item>
      <title>Surgical-LVLM: Learning to Adapt Large Vision-Language Model for Grounded Visual Question Answering in Robotic Surgery</title>
      <link>https://arxiv.org/abs/2405.10948</link>
      <description>arXiv:2405.10948v1 Announce Type: cross 
Abstract: Recent advancements in Surgical Visual Question Answering (Surgical-VQA) and related region grounding have shown great promise for robotic and medical applications, addressing the critical need for automated methods in personalized surgical mentorship. However, existing models primarily provide simple structured answers and struggle with complex scenarios due to their limited capability in recognizing long-range dependencies and aligning multimodal information. In this paper, we introduce Surgical-LVLM, a novel personalized large vision-language model tailored for complex surgical scenarios. Leveraging the pre-trained large vision-language model and specialized Visual Perception LoRA (VP-LoRA) blocks, our model excels in understanding complex visual-language tasks within surgical contexts. In addressing the visual grounding task, we propose the Token-Interaction (TIT) module, which strengthens the interaction between the grounding module and the language responses of the Large Visual Language Model (LVLM) after projecting them into the latent space. We demonstrate the effectiveness of Surgical-LVLM on several benchmarks, including EndoVis-17-VQLA, EndoVis-18-VQLA, and a newly introduced EndoVis Conversations dataset, which sets new performance standards. Our work contributes to advancing the field of automated surgical mentorship by providing a context-aware solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10948v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <category>eess.IV</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guankun Wang, Long Bai, Wan Jun Nah, Jie Wang, Zhaoxi Zhang, Zhen Chen, Jinlin Wu, Mobarakol Islam, Hongbin Liu, Hongliang Ren</dc:creator>
    </item>
    <item>
      <title>Unsupervised Image Prior via Prompt Learning and CLIP Semantic Guidance for Low-Light Image Enhancement</title>
      <link>https://arxiv.org/abs/2405.11478</link>
      <description>arXiv:2405.11478v1 Announce Type: cross 
Abstract: Currently, low-light conditions present a significant challenge for machine cognition. In this paper, rather than optimizing models by assuming that human and machine cognition are correlated, we use zero-reference low-light enhancement to improve the performance of downstream task models. We propose to improve the zero-reference low-light enhancement method by leveraging the rich visual-linguistic CLIP prior without any need for paired or unpaired normal-light data, which is laborious and difficult to collect. We propose a simple but effective strategy to learn prompts that help guide the enhancement method and experimentally show that the prompts learned without any need for normal-light data improve image contrast, reduce over-enhancement, and reduce noise over-amplification. Next, we propose to reuse the CLIP model for semantic guidance via zero-shot open vocabulary classification to optimize low-light enhancement for task-based performance rather than human visual perception. We conduct extensive experimental results showing that the proposed method leads to consistent improvements across various datasets regarding task-based performance and compare our method against state-of-the-art methods, showing favorable results across various low-light datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11478v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Igor Morawski, Kai He, Shusil Dangi, Winston H. Hsu</dc:creator>
    </item>
    <item>
      <title>Automated Coastline Extraction Using Edge Detection Algorithms</title>
      <link>https://arxiv.org/abs/2405.11494</link>
      <description>arXiv:2405.11494v1 Announce Type: cross 
Abstract: We analyse the effectiveness of edge detection algorithms for the purpose of automatically extracting coastlines from satellite images. Four algorithms - Canny, Sobel, Scharr and Prewitt are compared visually and using metrics. With an average SSIM of 0.8, Canny detected edges that were closest to the reference edges. However, the algorithm had difficulty distinguishing noisy edges, e.g. due to development, from coastline edges. In addition, histogram equalization and Gaussian blur were shown to improve the effectiveness of the edge detection algorithms by up to 1.5 and 1.6 times respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11494v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/IGARSS52108.2023.10282621</arxiv:DOI>
      <dc:creator>Conor O'Sullivan, Seamus Coveney, Xavier Monteys, Soumyabrata Dev</dc:creator>
    </item>
    <item>
      <title>Nickel and Diming Your GAN: A Dual-Method Approach to Enhancing GAN Efficiency via Knowledge Distillation</title>
      <link>https://arxiv.org/abs/2405.11614</link>
      <description>arXiv:2405.11614v1 Announce Type: cross 
Abstract: In this paper, we address the challenge of compressing generative adversarial networks (GANs) for deployment in resource-constrained environments by proposing two novel methodologies: Distribution Matching for Efficient compression (DiME) and Network Interactive Compression via Knowledge Exchange and Learning (NICKEL). DiME employs foundation models as embedding kernels for efficient distribution matching, leveraging maximum mean discrepancy to facilitate effective knowledge distillation. Simultaneously, NICKEL employs an interactive compression method that enhances the communication between the student generator and discriminator, achieving a balanced and stable compression process. Our comprehensive evaluation on the StyleGAN2 architecture with the FFHQ dataset shows the effectiveness of our approach, with NICKEL &amp; DiME achieving FID scores of 10.45 and 15.93 at compression rates of 95.73% and 98.92%, respectively. Remarkably, our methods sustain generative quality even at an extreme compression rate of 99.69%, surpassing the previous state-of-the-art performance by a large margin. These findings not only demonstrate our methodologies' capacity to significantly lower GANs' computational demands but also pave the way for deploying high-quality GAN models in settings with limited resources. Our code will be released soon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11614v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sangyeop Yeo, Yoojin Jang, Jaejun Yoo</dc:creator>
    </item>
    <item>
      <title>Refining Coded Image in Human Vision Layer Using CNN-Based Post-Processing</title>
      <link>https://arxiv.org/abs/2405.11894</link>
      <description>arXiv:2405.11894v1 Announce Type: cross 
Abstract: Scalable image coding for both humans and machines is a technique that has gained a lot of attention recently. This technology enables the hierarchical decoding of images for human vision and image recognition models. It is a highly effective method when images need to serve both purposes. However, no research has yet incorporated the post-processing commonly used in popular image compression schemes into scalable image coding method for humans and machines. In this paper, we propose a method to enhance the quality of decoded images for humans by integrating post-processing into scalable coding scheme. Experimental results show that the post-processing improves compression performance. Furthermore, the effectiveness of the proposed method is validated through comparisons with traditional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11894v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takahiro Shindo, Yui Tatsumi, Taiju Watanabe, Hiroshi Watanabe</dc:creator>
    </item>
    <item>
      <title>Surrogate-based cross-correlation for particle image velocimetry</title>
      <link>https://arxiv.org/abs/2112.05303</link>
      <description>arXiv:2112.05303v2 Announce Type: replace 
Abstract: This paper presents a novel surrogate-based cross-correlation (SBCC) framework to improve the correlation performance for practical particle image velocimetry~(PIV). The basic idea is that an optimized surrogate filter/image, replacing one raw image, will produce a more accurate and robust correlation signal. Specifically, the surrogate image is encouraged to generate perfect Gaussian-shaped correlation map to tracking particles (PIV image pair) while producing zero responses to image noise (context images). And the problem is formularized with an objective function composed of surrogate loss and consistency loss. As a result, the closed-form solution provides an efficient multivariate operator that could consider other negative context images. Compared with the state-of-the-art baseline methods (background subtraction, robust phase correlation, etc.), our SBCC method exhibits significant performance improvement (accuracy and robustness) on the synthetic dataset and several challenging experimental PIV cases. Besides, our implementation with experimental details (\url{https://github.com/yongleex/SBCC}) is also available for interested researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.05303v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>eess.SP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yong Lee, Fuqiang Gu, Zeyu Gong, Ding Pan, Wenhui Zeng</dc:creator>
    </item>
    <item>
      <title>The Perception-Robustness Tradeoff in Deterministic Image Restoration</title>
      <link>https://arxiv.org/abs/2311.09253</link>
      <description>arXiv:2311.09253v3 Announce Type: replace 
Abstract: We study the behavior of deterministic methods for solving inverse problems in imaging. These methods are commonly designed to achieve two goals: (1) attaining high perceptual quality, and (2) generating reconstructions that are consistent with the measurements. We provide a rigorous proof that the better a predictor satisfies these two requirements, the larger its Lipschitz constant must be, regardless of the nature of the degradation involved. In particular, to approach perfect perceptual quality and perfect consistency, the Lipschitz constant of the model must grow to infinity. This implies that such methods are necessarily more susceptible to adversarial attacks. We demonstrate our theory on single image super-resolution algorithms, addressing both noisy and noiseless settings. We also show how this undesired behavior can be leveraged to explore the posterior distribution, thereby allowing the deterministic model to imitate stochastic methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.09253v3</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guy Ohayon, Tomer Michaeli, Michael Elad</dc:creator>
    </item>
    <item>
      <title>Super-High-Fidelity Image Compression via Hierarchical-ROI and Adaptive Quantization</title>
      <link>https://arxiv.org/abs/2403.13030</link>
      <description>arXiv:2403.13030v2 Announce Type: replace 
Abstract: Learned Image Compression (LIC) has achieved dramatic progress regarding objective and subjective metrics. MSE-based models aim to improve objective metrics while generative models are leveraged to improve visual quality measured by subjective metrics. However, they all suffer from blurring or deformation at low bit rates, especially at below $0.2bpp$. Besides, deformation on human faces and text is unacceptable for visual quality assessment, and the problem becomes more prominent on small faces and text. To solve this problem, we combine the advantage of MSE-based models and generative models by utilizing region of interest (ROI). We propose Hierarchical-ROI (H-ROI), to split images into several foreground regions and one background region to improve the reconstruction of regions containing faces, text, and complex textures. Further, we propose adaptive quantization by non-linear mapping within the channel dimension to constrain the bit rate while maintaining the visual quality. Exhaustive experiments demonstrate that our methods achieve better visual quality on small faces and text with lower bit rates, e.g., $0.7X$ bits of HiFiC and $0.5X$ bits of BPG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13030v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jixiang Luo, Yan Wang, Hongwei Qin</dc:creator>
    </item>
    <item>
      <title>Contextual Embedding Learning to Enhance 2D Networks for Volumetric Image Segmentation</title>
      <link>https://arxiv.org/abs/2404.01723</link>
      <description>arXiv:2404.01723v2 Announce Type: replace 
Abstract: The segmentation of organs in volumetric medical images plays an important role in computer-aided diagnosis and treatment/surgery planning. Conventional 2D convolutional neural networks (CNNs) can hardly exploit the spatial correlation of volumetric data. Current 3D CNNs have the advantage to extract more powerful volumetric representations but they usually suffer from occupying excessive memory and computation nevertheless. In this study we aim to enhance the 2D networks with contextual information for better volumetric image segmentation. Accordingly, we propose a contextual embedding learning approach to facilitate 2D CNNs capturing spatial information properly. Our approach leverages the learned embedding and the slice-wisely neighboring matching as a soft cue to guide the network. In such a way, the contextual information can be transferred slice-by-slice thus boosting the volumetric representation of the network. Experiments on challenging prostate MRI dataset (PROMISE12) and abdominal CT dataset (CHAOS) show that our contextual embedding learning can effectively leverage the inter-slice context and improve segmentation performance. The proposed approach is a plug-and-play, and memory-efficient solution to enhance the 2D networks for volumetric segmentation. Our code is publicly available at https://github.com/JuliusWang-7/CE_Block.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01723v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuoyuan Wang, Dong Sun, Xiangyun Zeng, Ruodai Wu, Yi Wang</dc:creator>
    </item>
    <item>
      <title>Raster Forge: Interactive Raster Manipulation Library and GUI for Python</title>
      <link>https://arxiv.org/abs/2404.06389</link>
      <description>arXiv:2404.06389v2 Announce Type: replace 
Abstract: Raster Forge is a Python library and graphical user interface for raster data manipulation and analysis. The tool is focused on remote sensing applications, particularly in wildfire management. It allows users to import, visualize, and process raster layers for tasks such as image compositing or topographical analysis. For wildfire management, it generates fuel maps using predefined models. Its impact extends from disaster management to hydrological modeling, agriculture, and environmental monitoring. Raster Forge can be a valuable asset for geoscientists and researchers who rely on raster data analysis, enhancing geospatial data processing and visualization across various disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06389v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.MS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.simpa.2024.100657</arxiv:DOI>
      <arxiv:journal_reference>Software Impacts, 20, 100657, 2024</arxiv:journal_reference>
      <dc:creator>Afonso Oliveira, Nuno Fachada, Jo\~ao P. Matos-Carvalho</dc:creator>
    </item>
    <item>
      <title>Infrared Image Super-Resolution via Lightweight Information Split Network</title>
      <link>https://arxiv.org/abs/2405.10561</link>
      <description>arXiv:2405.10561v2 Announce Type: replace 
Abstract: Single image super-resolution (SR) is an established pixel-level vision task aimed at reconstructing a high-resolution image from its degraded low-resolution counterpart. Despite the notable advancements achieved by leveraging deep neural networks for SR, most existing deep learning architectures feature an extensive number of layers, leading to high computational complexity and substantial memory demands. These issues become particularly pronounced in the context of infrared image SR, where infrared devices often have stringent storage and computational constraints. To mitigate these challenges, we introduce a novel, efficient, and precise single infrared image SR model, termed the Lightweight Information Split Network (LISN). The LISN comprises four main components: shallow feature extraction, deep feature extraction, dense feature fusion, and high-resolution infrared image reconstruction. A key innovation within this model is the introduction of the Lightweight Information Split Block (LISB) for deep feature extraction. The LISB employs a sequential process to extract hierarchical features, which are then aggregated based on the relevance of the features under consideration. By integrating channel splitting and shift operations, the LISB successfully strikes an optimal balance between enhanced SR performance and a lightweight framework. Comprehensive experimental evaluations reveal that the proposed LISN achieves superior performance over contemporary state-of-the-art methods in terms of both SR quality and model complexity, affirming its efficacy for practical deployment in resource-constrained infrared imaging applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10561v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Liu, Kang Yan, Feiwei Qin, Changmiao Wang, Ruiquan Ge, Kai Zhang, Jie Huang, Yong Peng, Jin Cao</dc:creator>
    </item>
    <item>
      <title>An Urban Water Extraction Method Combining Deep Learning and Google Earth Engine</title>
      <link>https://arxiv.org/abs/1912.10726</link>
      <description>arXiv:1912.10726v2 Announce Type: replace-cross 
Abstract: Urban water is important for the urban ecosystem. Accurate and efficient detection of urban water with remote sensing data is of great significance for urban management and planning. In this paper, we proposed a new method to combine Google Earth Engine (GEE) with multiscale convolutional neural network (MSCNN) to extract urban water from Landsat images, which is summarized as offline training and online prediction (OTOP). That is, the training of MSCNN was completed offline, and the process of urban water extraction was implemented on GEE with the trained parameters of MSCNN. The OTOP can give full play to the respective advantages of GEE and CNN, and make the use of deep learning method on GEE more flexible. It can process available satellite images with high performance without data download and storage, and the overall performance of urban water extraction is also higher than that of the modified normalized difference water index (MNDWI) and random forest. The mean kappa, F1-score and intersection over union (IoU) of urban water extraction with the OTOP in Changchun, Wuhan, Kunming and Guangzhou reached 0.924, 0.930 and 0.869, respectively. The results of the extended validation in the other major cities of China also show that the OTOP is robust and can be used to extract different types of urban water, which benefits from the structural design and training of the MSCNN. Therefore, the OTOP is especially suitable for the study of large-scale and long-term urban water change detection in the background of urbanization.</description>
      <guid isPermaLink="false">oai:arXiv.org:1912.10726v2</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/JSTARS.2020.2971783</arxiv:DOI>
      <arxiv:journal_reference>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 13, pp. 769-782, 2020</arxiv:journal_reference>
      <dc:creator>Yudie Wang, Zhiwei Li, Chao Zeng, Gui-Song Xia, Huanfeng Shen</dc:creator>
    </item>
    <item>
      <title>A recurrent CNN for online object detection on raw radar frames</title>
      <link>https://arxiv.org/abs/2212.11172</link>
      <description>arXiv:2212.11172v3 Announce Type: replace-cross 
Abstract: Automotive radar sensors provide valuable information for advanced driving assistance systems (ADAS). Radars can reliably estimate the distance to an object and the relative velocity, regardless of weather and light conditions. However, radar sensors suffer from low resolution and huge intra-class variations in the shape of objects. Exploiting the time information (e.g., multiple frames) has been shown to help to capture better the dynamics of objects and, therefore, the variation in the shape of objects. Most temporal radar object detectors use 3D convolutions to learn spatial and temporal information. However, these methods are often non-causal and unsuitable for real-time applications. This work presents RECORD, a new recurrent CNN architecture for online radar object detection. We propose an end-to-end trainable architecture mixing convolutions and ConvLSTMs to learn spatio-temporal dependencies between successive frames. Our model is causal and requires only the past information encoded in the memory of the ConvLSTMs to detect objects. Our experiments show such a method's relevance for detecting objects in different radar representations (range-Doppler, range-angle) and outperform state-of-the-art models on the ROD2021 and CARRADA datasets while being less computationally expensive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.11172v3</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Colin Decourt, Rufin VanRullen, Didier Salle, Thomas Oberlin</dc:creator>
    </item>
    <item>
      <title>MADRL-Based Rate Adaptation for 360{\deg} Video Streaming with Multi-Viewpoint Prediction</title>
      <link>https://arxiv.org/abs/2405.07759</link>
      <description>arXiv:2405.07759v2 Announce Type: replace-cross 
Abstract: Over the last few years, 360{\deg} video traffic on the network has grown significantly. A key challenge of 360{\deg} video playback is ensuring a high quality of experience (QoE) with limited network bandwidth. Currently, most studies focus on tile-based adaptive bitrate (ABR) streaming based on single viewport prediction to reduce bandwidth consumption. However, the performance of models for single-viewpoint prediction is severely limited by the inherent uncertainty in head movement, which can not cope with the sudden movement of users very well. This paper first presents a multimodal spatial-temporal attention transformer to generate multiple viewpoint trajectories with their probabilities given a historical trajectory. The proposed method models viewpoint prediction as a classification problem and uses attention mechanisms to capture the spatial and temporal characteristics of input video frames and viewpoint trajectories for multi-viewpoint prediction. After that, a multi-agent deep reinforcement learning (MADRL)-based ABR algorithm utilizing multi-viewpoint prediction for 360{\deg} video streaming is proposed for maximizing different QoE objectives under various network conditions. We formulate the ABR problem as a decentralized partially observable Markov decision process (Dec-POMDP) problem and present a MAPPO algorithm based on centralized training and decentralized execution (CTDE) framework to solve the problem. The experimental results show that our proposed method improves the defined QoE metric by up to 85.5% compared to existing ABR methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07759v2</guid>
      <category>cs.MM</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <category>eess.IV</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/JIOT.2024.3398548</arxiv:DOI>
      <dc:creator>Haopeng Wang, Zijian Long, Haiwei Dong, Abdulmotaleb El Saddik</dc:creator>
    </item>
  </channel>
</rss>
