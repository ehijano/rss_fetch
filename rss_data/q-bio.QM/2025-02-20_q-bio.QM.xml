<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Feb 2025 05:01:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Impact of Optic Nerve Tortuosity, Globe Proptosis, and Size on Retinal Ganglion Cell Thickness Across General, Glaucoma, and Myopic Populations: Insights from the UK Biobank</title>
      <link>https://arxiv.org/abs/2502.13147</link>
      <description>arXiv:2502.13147v1 Announce Type: cross 
Abstract: Purpose: To investigate the impact of optic nerve tortuosity (ONT), and the interaction of globe proptosis and globe size on retinal ganglion cell (RGC) thickness, using Retinal Nerve Fiber Layer (RNFL) thickness, across general, glaucoma, and myopic populations. Methods: We analyzed 17,970 eyes from the UKBiobank cohort (ID 76442), including 371 glaucoma and 2481 myopic eyes. AI models segmented structures from 3D optical coherence tomography (OCT) scans and magnetic resonance images (MRI). RNFL thickness was derived from OCT scans and corrected for ocular magnification, was derived from OCT. From MRIs, we extracted: ONT, globe proptosis, axial length, and a novel interzygomatic line-to-posterior pole (ILPP) distance, a composite marker of globe proptosis and size. GEE models assessed associations between orbital and retinal features across all populations. Results: Segmentation models achieved Dice coefficients over 0.94 for both MRI and OCT. RNFL thickness was positively correlated with both ONT and ILPP distance (r = 0.065, p &lt; 0.001, and r = 0.206, p &lt; 0.001 respectively). The same was true for glaucoma (r = 0.140, p &lt; 0.01, and r = 0.256, p &lt; 0.01), and for myopia (r = 0.071, p &lt; 0.001, and r = 0.100, p &lt; 0.0001). GEE models revealed straighter optic nerves and shorter ILPP distance as predictive of thinner RNFL in all populations. Conclusions: This study emphasizes the impact of ONT, globe size, and proptosis on retinal health, suggesting RNFL thinning may arise from biomechanical stress due to straighter optic nerves or reduced ILPP distance, particularly in glaucoma or myopia. The novel ILPP metric, integrating globe size and position, shows potential as a biomarker for axonal health. These findings highlight the role of orbit structures in RGC axonal health and warrant further exploration of the biomechanical relationship between the orbit and optic nerve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13147v1</guid>
      <category>physics.med-ph</category>
      <category>eess.IV</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Charis Y. N. Chiang, Xiaofei Wang, Stuart K. Gardiner, Martin Buist, Michael J. A. Girard</dc:creator>
    </item>
    <item>
      <title>$\mathtt{GeLLM^3O}$: Generalizing Large Language Models for Multi-property Molecule Optimization</title>
      <link>https://arxiv.org/abs/2502.13398</link>
      <description>arXiv:2502.13398v1 Announce Type: cross 
Abstract: Despite recent advancements, most computational methods for molecule optimization are constrained to single- or double-property optimization tasks and suffer from poor scalability and generalizability to novel optimization tasks. Meanwhile, Large Language Models (LLMs) demonstrate remarkable out-of-domain generalizability to novel tasks. To demonstrate LLMs' potential for molecule optimization, we introduce $\mathtt{MoMUInstruct}$, the first high-quality instruction-tuning dataset specifically focused on complex multi-property molecule optimization tasks. Leveraging $\mathtt{MoMUInstruct}$, we develop $\mathtt{GeLLM^3O}$s, a series of instruction-tuned LLMs for molecule optimization. Extensive evaluations across 5 in-domain and 5 out-of-domain tasks demonstrate that $\mathtt{GeLLM^3O}$s consistently outperform state-of-the-art baselines. $\mathtt{GeLLM^3O}$s also exhibit outstanding zero-shot generalization to unseen tasks, significantly outperforming powerful closed-source LLMs. Such strong generalizability demonstrates the tremendous potential of $\mathtt{GeLLM^3O}$s as foundational models for molecule optimization, thereby tackling novel optimization tasks without resource-intensive retraining. $\mathtt{MoMUInstruct}$, models, and code are accessible through https://github.com/ninglab/GeLLMO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13398v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>physics.chem-ph</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vishal Dey, Xiao Hu, Xia Ning</dc:creator>
    </item>
    <item>
      <title>Semi-supervised classification of bird vocalizations</title>
      <link>https://arxiv.org/abs/2502.13440</link>
      <description>arXiv:2502.13440v1 Announce Type: cross 
Abstract: Changes in bird populations can indicate broader changes in ecosystems, making birds one of the most important animal groups to monitor. Combining machine learning and passive acoustics enables continuous monitoring over extended periods without direct human involvement. However, most existing techniques require extensive expert-labeled datasets for training and cannot easily detect time-overlapping calls in busy soundscapes. We propose a semi-supervised acoustic bird detector designed to allow both the detection of time-overlapping calls (when separated in frequency) and the use of few labeled training samples. The classifier is trained and evaluated on a combination of community-recorded open-source data and long-duration soundscape recordings from Singapore. It achieves a mean F0.5 score of 0.701 across 315 classes from 110 bird species on a hold-out test set, with an average of 11 labeled training samples per class. It outperforms the state-of-the-art BirdNET classifier on a test set of 103 bird species despite significantly fewer labeled training samples. The detector is further tested on 144 microphone-hours of continuous soundscape data. The rich soundscape in Singapore makes suppression of false positives a challenge on raw, continuous data streams. Nevertheless, we demonstrate that achieving high precision in such environments with minimal labeled training data is possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13440v1</guid>
      <category>cs.SD</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>eess.AS</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simen Hexeberg, Mandar Chitre, Matthias Hoffmann-Kuhnt, Bing Wen Low</dc:creator>
    </item>
    <item>
      <title>FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer</title>
      <link>https://arxiv.org/abs/2304.02011</link>
      <description>arXiv:2304.02011v4 Announce Type: replace-cross 
Abstract: In cryo-electron microscopy, accurate particle localization and classification are imperative. Recent deep learning solutions, though successful, require extensive training data sets. The protracted generation time of physics-based models, often employed to produce these data sets, limits their broad applicability. We introduce FakET, a method based on Neural Style Transfer, capable of simulating the forward operator of any cryo transmission electron microscope. It can be used to adapt a synthetic training data set according to reference data producing high-quality simulated micrographs or tilt-series. To assess the quality of our generated data, we used it to train a state-of-the-art localization and classification architecture and compared its performance with a counterpart trained on benchmark data. Remarkably, our technique matches the performance, boosts data generation speed 750 times, uses 33 times less memory, and scales well to typical transmission electron microscope detector sizes. It leverages GPU acceleration and parallel processing. The source code is available at https://github.com/paloha/faket.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.02011v4</guid>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.str.2025.01.020</arxiv:DOI>
      <arxiv:journal_reference>Structure, 2025</arxiv:journal_reference>
      <dc:creator>Pavol Harar, Lukas Herrmann, Philipp Grohs, David Haselbach</dc:creator>
    </item>
    <item>
      <title>Learning stochastic dynamics from snapshots through regularized unbalanced optimal transport</title>
      <link>https://arxiv.org/abs/2410.00844</link>
      <description>arXiv:2410.00844v3 Announce Type: replace-cross 
Abstract: Reconstructing dynamics using samples from sparsely time-resolved snapshots is an important problem in both natural sciences and machine learning. Here, we introduce a new deep learning approach for solving regularized unbalanced optimal transport (RUOT) and inferring continuous unbalanced stochastic dynamics from observed snapshots. Based on the RUOT form, our method models these dynamics without requiring prior knowledge of growth and death processes or additional information, allowing them to be learned directly from data. Theoretically, we explore the connections between the RUOT and Schr\"odinger bridge problem and discuss the key challenges and potential solutions. The effectiveness of our method is demonstrated with a synthetic gene regulatory network, high-dimensional Gaussian Mixture Model, and single-cell RNA-seq data from blood development. Compared with other methods, our approach accurately identifies growth and transition patterns, eliminates false transitions, and constructs the Waddington developmental landscape. Our code is available at: https://github.com/zhenyiizhang/DeepRUOT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00844v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>physics.comp-ph</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhenyi Zhang, Tiejun Li, Peijie Zhou</dc:creator>
    </item>
    <item>
      <title>Bayesian Comparisons Between Representations</title>
      <link>https://arxiv.org/abs/2411.08739</link>
      <description>arXiv:2411.08739v2 Announce Type: replace-cross 
Abstract: Which neural networks are similar is a fundamental question for both machine learning and neuroscience. Here, I propose to base comparisons on the predictive distributions of linear readouts from intermediate representations. In Bayesian statistics, the prior predictive distribution is a full description of the inductive bias and generalization of a model, making it a great basis for comparisons. This distribution directly gives the evidence a dataset would provide in favor of the model. If we want to compare multiple models to each other, we can use a metric for probability distributions like the Jensen-Shannon distance or the total variation distance. As these are metrics, this induces pseudo-metrics for representations, which measure how well two representations could be distinguished based on a linear read out. For a linear readout with a Gaussian prior on the read-out weights and Gaussian noise, we can analytically compute the (prior and posterior) predictive distributions without approximations. These distributions depend only on the linear kernel matrix of the representations in the model. Thus, the Bayesian metrics connect linear read-out based comparisons to kernel based metrics like centered kernel alignment and representational similarity analysis. I demonstrate the new methods with deep neural networks trained on ImageNet-1k comparing them to each other and a small subset of the Natural Scenes Dataset. The Bayesian comparisons broadly agree with existing metrics, but are more stringent. Empirically, evaluations vary less across different random image samples and yield informative results with full uncertainty information. Thus the proposed Bayesian metrics nicely extend our toolkit for comparing representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08739v2</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Heiko H. Sch\"utt</dc:creator>
    </item>
  </channel>
</rss>
