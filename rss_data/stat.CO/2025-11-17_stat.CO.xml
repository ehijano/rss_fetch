<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 17 Nov 2025 05:01:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Influence of Prior Distributions on Gaussian Process Hyperparameter Inference</title>
      <link>https://arxiv.org/abs/2511.10950</link>
      <description>arXiv:2511.10950v1 Announce Type: new 
Abstract: Gaussian processes (GPs) are widely used metamodels for approximating expensive computer simulations, particularly in engineering design and spatial prediction. However, their performance can deteriorate significantly when covariance parameters are poorly estimated, highlighting the importance of accurate inference. The most common approach involves maximizing the marginal likelihood, yielding point estimates of these parameters. However, this approach is highly sensitive to initialization and optimization settings. An alternative is to adopt a fully Bayesian hierarchical framework, where the posterior distribution over the covariance parameters is inferred. This approach provides more robust uncertainty quantification and reduces sensitivity to parameter selection. Yet, a key challenge lies in the careful specification of prior distributions for these parameters. While many available software packages provide default priors, their influence on model behavior is often underexplored. Additionally, the choice of proposal distributions can also influence sampling efficiency and convergence. In this paper, we examine how different prior and proposal distributions over the lengthscale parameters $\theta$ affect predictive performance in a hierarchical GP model, using both simulated and real data experiments. By evaluating various types of priors and proposals, we aim to better understand their influence on predictive accuracy and uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10950v1</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ayumi Mutoh, Junoh Heo</dc:creator>
    </item>
    <item>
      <title>Autocovariance and Optimal Design for Random Walk Metropolis-Hastings Algorithm</title>
      <link>https://arxiv.org/abs/2511.10967</link>
      <description>arXiv:2511.10967v1 Announce Type: new 
Abstract: The Metropolis-Hastings algorithm has been extensively studied in the estimation and simulation literature, with most prior work focusing on convergence behavior and asymptotic theory. However, its covariance structure-an important statistical property for both theory and implementation-remains less understood. In this work, we provide new theoretical insights into the scalar case, focusing primarily on symmetric unimodal target distributions with symmetric random walk proposals, where we also establish an optimal proposal design. In addition, we derive some more general results beyond this setting. For the high-dimensional case, we relate the covariance matrix to the classical 0.23 average acceptance rate tuning criterion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10967v1</guid>
      <category>stat.CO</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyi Zhang, James C. Spall</dc:creator>
    </item>
    <item>
      <title>Dual Riemannian Newton Method on Statistical Manifolds</title>
      <link>https://arxiv.org/abs/2511.11318</link>
      <description>arXiv:2511.11318v1 Announce Type: new 
Abstract: In probabilistic modeling, parameter estimation is commonly formulated as a minimization problem on a parameter manifold. Optimization in such spaces requires geometry-aware methods that respect the underlying information structure. While the natural gradient leverages the Fisher information metric as a form of Riemannian gradient descent, it remains a first-order method and often exhibits slow convergence near optimal solutions. Existing second-order manifold algorithms typically rely on the Levi-Civita connection, thus overlooking the dual-connection structure that is central to information geometry. We propose the dual Riemannian Newton method, a Newton-type optimization algorithm on manifolds endowed with a metric and a pair of dual affine connections. The dual Riemannian Newton method explicates how duality shapes second-order updates: when the retraction (a local surrogate of the exponential map) is defined by one connection, the associated Newton equation is posed with its dual. We establish local quadratic convergence and validate the theory with experiments on representative statistical models. Thus, the dual Riemannian Newton method thus delivers second-order efficiency while remaining compatible with the dual structures that underlie modern information-geometric learning and inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11318v1</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Derun Zhou, Keisuke Yano, Mahito Sugiyama</dc:creator>
    </item>
    <item>
      <title>A Tidy Data Structure and Visualisations for Multiple Variable Correlations and Other Pairwise Scores</title>
      <link>https://arxiv.org/abs/2411.19830</link>
      <description>arXiv:2411.19830v2 Announce Type: replace 
Abstract: We provide a pipeline for calculating, managing and visualising correlations and other pairwise association scores for numerical and categorical data. We present a uniform interface for calculating a plethora of pairwise scores and propose a tidy data structure for organising the results. We also provide new visualisations which simultaneously show multiple and/or grouped pairwise scores. The visualisations are far richer than a traditional heatmap of correlation scores, as they help identify relationships with categorical variables, numeric variable pairs with non-linear associations or those which exhibit Simpson's paradox. These methods are available in our R package bullseye.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19830v2</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amit Chinwan, Catherine B. Hurley</dc:creator>
    </item>
    <item>
      <title>Sequential Markov Chain Monte Carlo for Filtering of State-Space Models with Low or Degenerate Observation Noise</title>
      <link>https://arxiv.org/abs/2511.04975</link>
      <description>arXiv:2511.04975v3 Announce Type: replace 
Abstract: We consider the discrete-time filtering problem in scenarios where the observation noise is degenerate or low. More precisely, one is given access to a discrete time observation sequence which at any time $k$ depends only on the state of an unobserved Markov chain. We specifically assume that the functional relationship between observations and hidden Markov chain has either degenerate or low noise. In this article, under suitable assumptions, we derive the filtering density and its recursions for this class of problems on a specific sequence of manifolds defined through the observation function. We then design sequential Markov chain Monte Carlo methods to approximate the filter serially in time. For a certain linear observation model, we show that using sequential Markov chain Monte Carlo for low noise will converge as the noise disappears to that of using sequential Markov chain Monte Carlo for degenerate noise. We illustrate the performance of our methodology on several challenging stochastic models deriving from Statistics and Applied Mathematics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04975v3</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abylay Zhumekenov, Alexandros Beskos, Dan Crisan, Ajay Jasra, Nikolas Kantas</dc:creator>
    </item>
  </channel>
</rss>
