<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 31 Dec 2024 05:00:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Predicting Human Brain States with Transformer</title>
      <link>https://arxiv.org/abs/2412.19814</link>
      <description>arXiv:2412.19814v1 Announce Type: new 
Abstract: The human brain is a complex and highly dynamic system, and our current knowledge of its functional mechanism is still very limited. Fortunately, with functional magnetic resonance imaging (fMRI), we can observe blood oxygen level-dependent (BOLD) changes, reflecting neural activity, to infer brain states and dynamics. In this paper, we ask the question of whether the brain states rep-resented by the regional brain fMRI can be predicted. Due to the success of self-attention and the transformer architecture in sequential auto-regression problems (e.g., language modelling or music generation), we explore the possi-bility of the use of transformers to predict human brain resting states based on the large-scale high-quality fMRI data from the human connectome project (HCP). Current results have shown that our model can accurately predict the brain states up to 5.04s with the previous 21.6s. Furthermore, even though the prediction error accumulates for the prediction of a longer time period, the gen-erated fMRI brain states reflect the architecture of functional connectome. These promising initial results demonstrate the possibility of developing gen-erative models for fMRI data using self-attention that learns the functional or-ganization of the human brain. Our code is available at: https://github.com/syf0122/brain_state_pred.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19814v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yifei Sun, Mariano Cabezas, Jiah Lee, Chenyu Wang, Wei Zhang, Fernando Calamante, Jinglei Lv</dc:creator>
    </item>
    <item>
      <title>Unveiling Secrets of Brain Function With Generative Modeling: Motion Perception in Primates &amp; Cortical Network Organization in Mice</title>
      <link>https://arxiv.org/abs/2412.19845</link>
      <description>arXiv:2412.19845v1 Announce Type: new 
Abstract: This Dissertation is comprised of two main projects, addressing questions in neuroscience through applications of generative modeling.
  Project #1 (Chapter 4) explores how neurons encode features of the external world. I combine Helmholtz's "Perception as Unconscious Inference" -- paralleled by modern generative models like variational autoencoders (VAE) -- with the hierarchical structure of the visual cortex. This combination leads to the development of a hierarchical VAE model, which I test for its ability to mimic neurons from the primate visual cortex in response to motion stimuli. Results show that the hierarchical VAE perceives motion similar to the primate brain. Additionally, the model identifies causal factors of retinal motion inputs, such as object- and self-motion, in a completely unsupervised manner. Collectively, these results suggest that hierarchical inference underlines the brain's understanding of the world, and hierarchical VAEs can effectively model this understanding.
  Project #2 (Chapter 5) investigates the spatiotemporal structure of spontaneous brain activity and its reflection of brain states like rest. Using simultaneous fMRI and wide-field Ca2+ imaging data, this project demonstrates that the mouse cortex can be decomposed into overlapping communities, with around half of the cortical regions belonging to multiple communities. Comparisons reveal similarities and differences between networks inferred from fMRI and Ca2+ signals.
  The introduction (Chapter 1) is divided similarly to this abstract: sections 1.1 to 1.8 provide background information about Project #1, and sections 1.9 to 1.13 are related to Project #2. Chapter 2 includes historical background, Chapter 3 provides the necessary mathematical background, and finally, Chapter 6 contains concluding remarks and future directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19845v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hadi Vafaii</dc:creator>
    </item>
    <item>
      <title>Neurophenomenal Structuralism and the Role of Computational Context</title>
      <link>https://arxiv.org/abs/2412.20873</link>
      <description>arXiv:2412.20873v1 Announce Type: new 
Abstract: Neurophenomenal structuralism posits that conscious experiences are defined relationally and that their phenomenal structures are mirrored by neural structures. While this approach offers a promising framework for identifying neural correlates of contents of consciousness (NCCCs), we argue that merely establishing structural correspondences between neural and phenomenal structures is insufficient. This paper emphasizes the critical role of computational context in determining the content of neural structures. We introduce four criteria - Sensitivity, Organization, Exploitation, and Contextualization - to evaluate which neural structures are viable NCCC candidates. These criteria highlight that, for neural structures to meaningfully mirror phenomenal structures they have to be actively exploited and be able to influence behavior in a structure-preserving way. Our analysis demonstrates that anatomical and causal neural structures fail to meet certain criteria, whereas activation structures can, provided they are embedded within the appropriate computational context. Our findings challenge both local and rich global structuralist theories for overlooking the content-constituting role of computational context, leading to proposed NCCCs that fail to fully account for conscious content. We conclude that incorporating computational context is essential for any structuralist account of consciousness, as it determines the nature of dimensions within neural activation spaces and, consequently, the content of conscious experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20873v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marlo Pa{\ss}ler, Adrien Doerig</dc:creator>
    </item>
    <item>
      <title>The FlEye camera: Sampling the joint distribution of natural scenes and motion</title>
      <link>https://arxiv.org/abs/2412.21081</link>
      <description>arXiv:2412.21081v1 Announce Type: new 
Abstract: To make efficient use of limited physical resources, the brain must match its coding and computational strategies to the statistical structure of input signals. An attractive testing ground for these principles is the problem of motion estimation in the fly visual system: we understand the optics of the compound eye, have a quantitative description of input signals and noise from the retina, and can record from output neurons that encode estimates of different velocity components. Furthermore, recent work provides a nearly complete wiring diagram of the intervening circuitry. What is missing is a characterization of the visual signals and motions that flies encounter in a natural context. We attack this directly with the development of a specialized camera that matches the high temporal resolution, optical properties, and spectral sensitivity of the fly's eye; inertial motion sensors provide ground truth about rotations and translations through the world. We describe the design, construction, and performance characteristics of this FlEye camera. To illustrate the opportunities created by this instrument we use data on movies and motion to construct optimal local motion estimators that can be compared with the responses of the fly's motion sensitive neurons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.21081v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles J. Edelson, Paul Smith, Sima Setayeshgar, William Bialek, Rob R. de Ruyter van Steveninck</dc:creator>
    </item>
    <item>
      <title>Intrinsic meaning, perception, and matching</title>
      <link>https://arxiv.org/abs/2412.21111</link>
      <description>arXiv:2412.21111v1 Announce Type: new 
Abstract: Integrated information theory (IIT) argues that the substrate of consciousness is a maximally irreducible complex of units. Together, subsets of the complex specify a cause-effect structure, composed of distinctions and their relations, which accounts in full for the quality of experience. The feeling of a specific experience is also its meaning for the subject, which is thus defined intrinsically, regardless of whether the experience occurs in a dream or is triggered by processes in the environment. Here we extend IIT's framework to characterize the relationship between intrinsic meaning, extrinsic stimuli, and causal processes in the environment, illustrated using a simple model of a sensory hierarchy. We argue that perception should be considered as a structured interpretation, where a stimulus from the environment acts merely as a trigger a system's state and the structure is provided by the complex's intrinsic connectivity. We also propose that perceptual differentiation -- the richness and diversity of structures triggered by representative sequences of stimuli -- quantifies the meaningfulness of different environments to a complex. In adaptive systems, this reflects the "matching" between intrinsic meanings and causal processes in an environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.21111v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William G. P. Mayner, Bj{\o}rn Erik Juel, Giulio Tononi</dc:creator>
    </item>
    <item>
      <title>Two-component spatiotemporal template for activation-inhibition of speech in ECoG</title>
      <link>https://arxiv.org/abs/2412.21178</link>
      <description>arXiv:2412.21178v1 Announce Type: new 
Abstract: I compute the average trial-by-trial power of band-limited speech activity across epochs of multi-channel high-density electrocorticography (ECoG) recorded from multiple subjects during a consonant-vowel speaking task. I show that previously seen anti-correlations of average beta frequency activity (12-35 Hz) to high-frequency gamma activity (70-140 Hz) during speech movement are observable between individual ECoG channels in the sensorimotor cortex (SMC). With this I fit a variance-based model using principal component analysis to the band-powers of individual channels of session-averaged ECoG data in the SMC and project SMC channels onto their lower-dimensional principal components.
  Spatiotemporal relationships between speech-related activity and principal components are identified by correlating the principal components of both frequency bands to individual ECoG channels over time using windowed correlation. Correlations of principal component areas to sensorimotor areas reveal a distinct two-component activation-inhibition-like representation for speech that resembles distinct local sensorimotor areas recently shown to have complex interplay in whole-body motor control, inhibition, and posture. Notably the third principal component shows insignificant correlations across all subjects, suggesting two components of ECoG are sufficient to represent SMC activity during speech movement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.21178v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>eess.AS</category>
      <category>eess.SP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Easthope</dc:creator>
    </item>
    <item>
      <title>Sparse chaos in cortical circuits</title>
      <link>https://arxiv.org/abs/2412.21188</link>
      <description>arXiv:2412.21188v1 Announce Type: new 
Abstract: Nerve impulses, the currency of information flow in the brain, are generated by an instability of the neuronal membrane potential dynamics. Neuronal circuits exhibit collective chaos that appears essential for learning, memory, sensory processing, and motor control. However, the factors controlling the nature and intensity of collective chaos in neuronal circuits are not well understood. Here we use computational ergodic theory to demonstrate that basic features of nerve impulse generation profoundly affect collective chaos in neuronal circuits. Numerically exact calculations of Lyapunov spectra, Kolmogorov-Sinai-entropy, and upper and lower bounds on attractor dimension show that changes in nerve impulse generation in individual neurons moderately impact information encoding rates but qualitatively transform phase space structure. Specifically, we find a drastic reduction in the number of unstable manifolds, Kolmogorov-Sinai entropy, and attractor dimension. Beyond a critical point, marked by the simultaneous breakdown of the diffusion approximation, a peak in the largest Lyapunov exponent, and a localization transition of the leading covariant Lyapunov vector, networks exhibit sparse chaos: prolonged periods of near stable dynamics interrupted by short bursts of intense chaos. Analysis of large, more realistically structured networks supports the generality of these findings. In cortical circuits, biophysical properties appear tuned to this regime of sparse chaos. Our results reveal a close link between fundamental aspects of single-neuron biophysics and the collective dynamics of cortical circuits, suggesting that nerve impulse generation mechanisms are adapted to enhance circuit controllability and information flow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.21188v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>nlin.CD</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rainer Engelken, Michael Monteforte, Fred Wolf</dc:creator>
    </item>
    <item>
      <title>Comprehensive Review of EEG-to-Output Research: Decoding Neural Signals into Images, Videos, and Audio</title>
      <link>https://arxiv.org/abs/2412.19999</link>
      <description>arXiv:2412.19999v1 Announce Type: cross 
Abstract: Electroencephalography (EEG) is an invaluable tool in neuroscience, offering insights into brain activity with high temporal resolution. Recent advancements in machine learning and generative modeling have catalyzed the application of EEG in reconstructing perceptual experiences, including images, videos, and audio. This paper systematically reviews EEG-to-output research, focusing on state-of-the-art generative methods, evaluation metrics, and data challenges. Using PRISMA guidelines, we analyze 1800 studies and identify key trends, challenges, and opportunities in the field. The findings emphasize the potential of advanced models such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Transformers, while highlighting the pressing need for standardized datasets and cross-subject generalization. A roadmap for future research is proposed that aims to improve decoding accuracy and broadening real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19999v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yashvir Sabharwal, Balaji Rama</dc:creator>
    </item>
    <item>
      <title>An analytic theory of creativity in convolutional diffusion models</title>
      <link>https://arxiv.org/abs/2412.20292</link>
      <description>arXiv:2412.20292v1 Announce Type: cross 
Abstract: We obtain the first analytic, interpretable and predictive theory of creativity in convolutional diffusion models. Indeed, score-based diffusion models can generate highly creative images that lie far from their training data. But optimal score-matching theory suggests that these models should only be able to produce memorized training examples. To reconcile this theory-experiment gap, we identify two simple inductive biases, locality and equivariance, that: (1) induce a form of combinatorial creativity by preventing optimal score-matching; (2) result in a fully analytic, completely mechanistically interpretable, equivariant local score (ELS) machine that, (3) without any training can quantitatively predict the outputs of trained convolution only diffusion models (like ResNets and UNets) with high accuracy (median $r^2$ of $0.90, 0.91, 0.94$ on CIFAR10, FashionMNIST, and MNIST). Our ELS machine reveals a locally consistent patch mosaic model of creativity, in which diffusion models create exponentially many novel images by mixing and matching different local training set patches in different image locations. Our theory also partially predicts the outputs of pre-trained self-attention enabled UNets (median $r^2 \sim 0.75$ on CIFAR10), revealing an intriguing role for attention in carving out semantic coherence from local patch mosaics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20292v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mason Kamb, Surya Ganguli</dc:creator>
    </item>
    <item>
      <title>Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey)</title>
      <link>https://arxiv.org/abs/2307.10246</link>
      <description>arXiv:2307.10246v3 Announce Type: replace 
Abstract: Can artificial intelligence unlock the secrets of the human brain? How do the inner mechanisms of deep learning models relate to our neural circuits? Is it possible to enhance AI by tapping into the power of brain recordings? These captivating questions lie at the heart of an emerging field at the intersection of neuroscience and artificial intelligence. Our survey dives into this exciting domain, focusing on human brain recording studies and cutting-edge cognitive neuroscience datasets that capture brain activity during natural language processing, visual perception, and auditory experiences. We explore two fundamental approaches: encoding models, which attempt to generate brain activity patterns from sensory inputs; and decoding models, which aim to reconstruct our thoughts and perceptions from neural signals. These techniques not only promise breakthroughs in neurological diagnostics and brain-computer interfaces but also offer a window into the very nature of cognition. In this survey, we first discuss popular representations of language, vision, and speech stimuli, and present a summary of neuroscience datasets. We then review how the recent advances in deep learning transformed this field, by investigating the popular deep learning based encoding and decoding architectures, noting their benefits and limitations across different sensory modalities. From text to images, speech to videos, we investigate how these models capture the brain's response to our complex, multimodal world. While our primary focus is on human studies, we also highlight the crucial role of animal models in advancing our understanding of neural mechanisms. Throughout, we mention the ethical implications of these powerful technologies, addressing concerns about privacy and cognitive liberty. We conclude with a summary and discussion of future trends in this rapidly evolving field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10246v3</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Published in Transactions on Machine Learning Research (12/2024)</arxiv:journal_reference>
      <dc:creator>Subba Reddy Oota, Zijiao Chen, Manish Gupta, Raju S. Bapi, Gael Jobard, Frederic Alexandre, Xavier Hinaut</dc:creator>
    </item>
    <item>
      <title>How to optimize neuroscience data utilization and experiment design for advancing primate visual and linguistic brain models?</title>
      <link>https://arxiv.org/abs/2401.03376</link>
      <description>arXiv:2401.03376v2 Announce Type: replace 
Abstract: In recent years, neuroscience has made significant progress in building large-scale artificial neural network (ANN) models of brain activity and behavior. However, there is no consensus on the most efficient ways to collect data and design experiments to develop the next generation of models. This article explores the controversial opinions that have emerged on this topic in the domain of vision and language. Specifically, we address two critical points. First, we weigh the pros and cons of using qualitative insights from empirical results versus raw experimental data to train models. Second, we consider model-free (intuition-based) versus model-based approaches for data collection, specifically experimental design and stimulus selection, for optimal model development. Finally, we consider the challenges of developing a synergistic approach to experimental design and model building, including encouraging data and model sharing and the implications of iterative additions to existing models. The goal of the paper is to discuss decision points and propose directions for both experimenters and model developers in the quest to understand the brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03376v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Greta Tuckute, Dawn Finzi, Eshed Margalit, Joel Zylberberg, SueYeon Chung, Alona Fyshe, Evelina Fedorenko, Nikolaus Kriegeskorte, Jacob Yates, Kalanit Grill-Spector, Kohitij Kar</dc:creator>
    </item>
    <item>
      <title>Differentiable Optimization of Similarity Scores Between Models and Brains</title>
      <link>https://arxiv.org/abs/2407.07059</link>
      <description>arXiv:2407.07059v3 Announce Type: replace 
Abstract: How do we know if two systems - biological or artificial - process information in a similar way? Similarity measures such as linear regression, Centered Kernel Alignment (CKA), Normalized Bures Similarity (NBS), and angular Procrustes distance, are often used to quantify this similarity. However, it is currently unclear what drives high similarity scores and even what constitutes a "good" score. Here, we introduce a novel tool to investigate these questions by differentiating through similarity measures to directly maximize the score. Surprisingly, we find that high similarity scores do not guarantee encoding task-relevant information in a manner consistent with neural data; and this is particularly acute for CKA and even some variations of cross-validated and regularized linear regression. We find no consistent threshold for a good similarity score - it depends on both the measure and the dataset. In addition, synthetic datasets optimized to maximize similarity scores initially learn the highest variance principal component of the target dataset, but some methods like angular Procrustes capture lower variance dimensions much earlier than methods like CKA. To shed light on this, we mathematically derive the sensitivity of CKA, angular Procrustes, and NBS to the variance of principal component dimensions, and explain the emphasis CKA places on high variance components. Finally, by jointly optimizing multiple similarity measures, we characterize their allowable ranges and reveal that some similarity measures are more constraining than others. While current measures offer a seemingly straightforward way to quantify the similarity between neural systems, our work underscores the need for careful interpretation. We hope the tools we developed will be used by practitioners to better understand current and future similarity measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07059v3</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Cloos, Moufan Li, Markus Siegel, Scott L. Brincat, Earl K. Miller, Guangyu Robert Yang, Christopher J. Cueva</dc:creator>
    </item>
    <item>
      <title>Comparing Methodological Variations in Seizure Onset Localisation Algorithms using intracranial EEG</title>
      <link>https://arxiv.org/abs/2410.13466</link>
      <description>arXiv:2410.13466v2 Announce Type: replace 
Abstract: During clinical treatment for epilepsy, the area of the brain thought to be responsible for pathological activity is identified. This identification is typically performed through visual assessment of EEG recordings; however, this is time consuming and prone to subjective inconsistency. Automated onset localisation algorithms provide objective identification of the onset location by highlighting changes in signal features associated with seizure onset. In this work we investigate how methodological differences in such algorithms can result in different onset locations being identified.
  We analysed ictal intracranial EEG (icEEG) recordings in 16 subjects (100 seizures) with drug-resistant epilepsy from the SWEZ-ETHZ public database. We identified a series of key methodological differences that must be considered when designing or selecting an onset localisation algorithm. These differences were demonstrated using three distinct algorithms that capture different, but complementary, seizure onset features: Imprint, Epileptogenicity Index, and Low Entropy Map. We assessed methodological differences (or Decision Points), and their impact on the identified onset locations.
  Our independent application of all three algorithms to the same ictal icEEG dataset revealed low agreement between them: 27-60% of onset channels showed minimal or no overlap. Therefore, we investigated the effect of three key differences: (i) how to define a baseline, (ii) whether low-frequency components are considered, and finally (iii) whether electrodecrement is considered. Changes at each Decision Point were found to substantially influence resultant onset channels (r&gt;0.3).
  Our results demonstrate how seemingly small methodological changes can result in large differences in onset locations. We propose that key Decision Points must be considered when using or designing an onset localisation algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13466v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarah J. Gascoigne, Manel Vila-Vidal, Nathan Evans, Christopher Thornton, Heather Woodhouse, Billy Smith, Anderson Brito Da Silva, Rhys H. Thomas, Kevin Wilson, Peter N. Taylor, Adria Tauste Campo, Yujiang Wang</dc:creator>
    </item>
    <item>
      <title>How EEG preprocessing shapes decoding performance</title>
      <link>https://arxiv.org/abs/2410.14453</link>
      <description>arXiv:2410.14453v3 Announce Type: replace 
Abstract: EEG preprocessing varies widely between studies, but its impact on classification performance remains poorly understood. To address this gap, we analyzed seven experiments with 40 participants drawn from the public ERP CORE dataset. We systematically varied key preprocessing steps, such as filtering, referencing, baseline interval, detrending, and multiple artifact correction steps. Then we performed trial-wise binary classification (i.e., decoding) using neural networks (EEGNet), or time-resolved logistic regressions. Our findings demonstrate that preprocessing choices influenced decoding performance considerably. All artifact correction steps reduced decoding performance across all experiments and models, while higher high-pass filter cutoffs consistently enhanced decoding. For EEGNet, baseline correction further improved performance, and for time-resolved classifiers, linear detrending and lower low-pass filter cutoffs were beneficial. Other optimal preprocessing choices were specific for each experiment. The current results underline the importance of carefully selecting preprocessing steps for EEG-based decoding. If not corrected, artifacts facilitate decoding but compromise conclusive interpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14453v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roman Kessler, Alexander Enge, Michael A. Skeide</dc:creator>
    </item>
    <item>
      <title>Foveated Retinotopy Improves Classification and Localization in CNNs</title>
      <link>https://arxiv.org/abs/2402.15480</link>
      <description>arXiv:2402.15480v3 Announce Type: replace-cross 
Abstract: From a falcon detecting prey to humans recognizing faces, many species exhibit extraordinary abilities in rapid visual localization and classification. These are made possible by a specialized retinal region called the fovea, which provides high acuity at the center of vision while maintaining lower resolution in the periphery. This distinctive spatial organization, preserved along the early visual pathway through retinotopic mapping, is fundamental to biological vision, yet remains largely unexplored in machine learning. Our study investigates how incorporating foveated retinotopy may benefit deep convolutional neural networks (CNNs) in image classification tasks. By implementing a foveated retinotopic transformation in the input layer of standard ResNet models and re-training them, we maintain comparable classification accuracy while enhancing the network's robustness to scale and rotational perturbations. Although this architectural modification introduces increased sensitivity to fixation point shifts, we demonstrate how this apparent limitation becomes advantageous: variations in classification probabilities across different gaze positions serve as effective indicators for object localization. Our findings suggest that foveated retinotopic mapping encodes implicit knowledge about visual object geometry, offering an efficient solution to the visual search problem - a capability crucial for many living species.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15480v3</guid>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jean-Nicolas J\'er\'emie, Emmanuel Dauc\'e, Laurent U Perrinet</dc:creator>
    </item>
    <item>
      <title>EEG Right &amp; Left Voluntary Hand Movement-based Virtual Brain-Computer Interfacing Keyboard Using Hybrid Deep Learning Approach</title>
      <link>https://arxiv.org/abs/2409.00035</link>
      <description>arXiv:2409.00035v2 Announce Type: replace-cross 
Abstract: Brain-machine interfaces (BMIs), particularly those based on electroencephalography (EEG), offer promising solutions for assisting individuals with motor disabilities. However, challenges in reliably interpreting EEG signals for specific tasks, such as simulating keystrokes, persist due to the complexity and variability of brain activity. Current EEG-based BMIs face limitations in adaptability, usability, and robustness, especially in applications like virtual keyboards, as traditional machine-learning models struggle to handle high-dimensional EEG data effectively. To address these gaps, we developed an EEG-based BMI system capable of accurately identifying voluntary keystrokes, specifically leveraging right and left voluntary hand movements. Using a publicly available EEG dataset, the signals were pre-processed with band-pass filtering, segmented into 22-electrode arrays, and refined into event-related potential (ERP) windows, resulting in a 19x200 feature array categorized into three classes: resting state (0), 'd' key press (1), and 'l' key press (2). Our approach employs a hybrid neural network architecture with BiGRU-Attention as the proposed model for interpreting EEG signals, achieving superior test accuracy of 90% and a mean accuracy of 91% in 10-fold stratified cross-validation. This performance outperforms traditional ML methods like Support Vector Machines (SVMs) and Naive Bayes, as well as advanced architectures such as Transformers, CNN-Transformer hybrids, and EEGNet. Finally, the BiGRU-Attention model is integrated into a real-time graphical user interface (GUI) to simulate and predict keystrokes from brain activity. Our work demonstrates how deep learning can advance EEG-based BMI systems by addressing the challenges of signal interpretation and classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00035v2</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Biplov Paneru, Bipul Thapa, Bishwash Paneru, Sanjog Chhetri Sapkota</dc:creator>
    </item>
    <item>
      <title>How the (Tensor-) Brain uses Embeddings and Embodiment to Encode Senses and Symbols</title>
      <link>https://arxiv.org/abs/2409.12846</link>
      <description>arXiv:2409.12846v2 Announce Type: replace-cross 
Abstract: The Tensor Brain (TB) has been introduced as a computational model for perception and memory. This paper provides an overview of the TB model, incorporating recent developments and insights into its functionality. The TB is composed of two primary layers: the representation layer and the index layer. The representation layer serves as a model for the subsymbolic global workspace, a concept derived from consciousness research. Its state represents the cognitive brain state, capturing the dynamic interplay of sensory and cognitive processes. The index layer, in contrast, contains symbolic representations for concepts, time instances, and predicates. In a bottom-up operation, sensory input activates the representation layer, which then triggers associated symbolic labels in the index layer. Conversely, in a top-down operation, symbols in the index layer activate the representation layer, which in turn influences earlier processing layers through embodiment. This top-down mechanism underpins semantic memory, enabling the integration of abstract knowledge into perceptual and cognitive processes. A key feature of the TB is its use of concept embeddings, which function as connection weights linking the index layer to the representation layer. As a concept's ``DNA,'' these embeddings consolidate knowledge from diverse experiences, sensory modalities, and symbolic representations, providing a unified framework for learning and memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12846v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Volker Tresp, Hang Li</dc:creator>
    </item>
    <item>
      <title>"Efficient Complexity": a Constrained Optimization Approach to the Evolution of Natural Intelligence</title>
      <link>https://arxiv.org/abs/2410.13881</link>
      <description>arXiv:2410.13881v2 Announce Type: replace-cross 
Abstract: A fundamental question in the conjunction of information theory, biophysics, bioinformatics and thermodynamics relates to the principles and processes that guide the development of natural intelligence in natural environments where information about external stimuli may not be available at prior. A novel approach in the description of the information processes of natural learning is proposed in the framework of constrained optimization, where the objective function represented by the information entropy of the internal states of the system with the states of the external environment is maximized under the natural constraints of memory, computing power, energy and other essential resources. The progress of natural intelligence can be interpreted in this framework as a strategy of approximation of the solutions of the optimization problem via a traversal over the extrema network of the objective function under the natural constraints that were examined and described. Non-trivial conclusions on the relationships between the complexity, variability and efficiency of the structure, or architecture of learning models made on the basis of the proposed formalism can explain the effectiveness of neural networks as collaborative groups of small intelligent units in biological and artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13881v2</guid>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Serge Dolgikh</dc:creator>
    </item>
    <item>
      <title>RealMind: Advancing Visual Decoding and Language Interaction via EEG Signals</title>
      <link>https://arxiv.org/abs/2410.23754</link>
      <description>arXiv:2410.23754v2 Announce Type: replace-cross 
Abstract: Decoding visual stimuli from neural recordings is a critical challenge in the development of brain-computer interfaces (BCIs). Although recent EEG-based decoding approaches have made progress in tasks such as visual classification, retrieval, and reconstruction, they remain constrained by unstable representation learning and a lack of interpretability. This gap highlights the need for more efficient representation learning and the integration of effective language interaction to enhance both understanding and practical usability in visual decoding tasks.To address this limitation, we introduce RealMind, a novel EEG-based framework designed to handle a diverse range of downstream tasks. Specifically, RealMind leverages both semantic and geometric consistency learning to enhance feature representation and improve alignment across tasks. Notably, beyond excelling in traditional tasks, our framework marks the first attempt at visual captioning from EEG data through vision-language model (VLM). It achieves a Top-1 decoding accuracy of 27.58% in a 200-class zero-shot retrieval task and a BLEU-1 score of 26.59% in a 200-class zero-shot captioning task. Overall, RealMind provides a comprehensive multitask EEG decoding framework, establishing a foundational approach for EEG-based visual decoding in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23754v2</guid>
      <category>cs.HC</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongyang Li, Haoyang Qin, Mingyang Wu, Jiahua Tang, Yuang Cao, Chen Wei, Quanying Liu</dc:creator>
    </item>
  </channel>
</rss>
