<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 16 Sep 2024 04:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Explainable Metrics for the Assessment of Neurodegenerative Diseases through Handwriting Analysis</title>
      <link>https://arxiv.org/abs/2409.08303</link>
      <description>arXiv:2409.08303v1 Announce Type: new 
Abstract: Motor changes are early signs of neurodegenerative diseases (NDs) such as Parkinson's disease (PD) and Alzheimer's disease (AD), but are often difficult to detect, especially in the early stages. In this work, we examine the behavior of a wide array of explainable metrics extracted from the handwriting signals of 113 subjects performing multiple tasks on a digital tablet. The aim is to measure their effectiveness in characterizing and assessing multiple NDs, including AD and PD. To this end, task-agnostic and task-specific metrics are extracted from 14 distinct tasks. Subsequently, through statistical analysis and a series of classification experiments, we investigate which metrics provide greater discriminative power between NDs and healthy controls and among different NDs. Preliminary results indicate that the various tasks at hand can all be effectively leveraged to distinguish between the considered set of NDs, specifically by measuring the stability, the speed of writing, the time spent not writing, and the pressure variations between groups from our handcrafted explainable metrics, which shows p-values lower than 0.0001 for multiple tasks. Using various classification algorithms on the computed metrics, we obtain up to 87% accuracy to discriminate AD and healthy controls (CTL), and up to 69% for PD vs CTL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08303v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Thebaud, Anna Favaro, Casey Chen, Gabrielle Chavez, Laureano Moro-Velazquez, Ankur Butala, Najim Dehak</dc:creator>
    </item>
    <item>
      <title>Turing Video-based Cognitive Tests to Handle Entangled Concepts</title>
      <link>https://arxiv.org/abs/2409.08868</link>
      <description>arXiv:2409.08868v1 Announce Type: new 
Abstract: We have proved in both human-based and computer-based tests that natural concepts generally `entangle' when they combine to form complex sentences, violating the rules of classical compositional semantics. In this article, we present the results of an innovative video-based cognitive test on a specific conceptual combination, which significantly violates the Clauser--Horne--Shimony--Holt version of Bell's inequalities (`CHSH inequality'). We also show that collected data can be faithfully modelled within a quantum-theoretic framework elaborated by ourselves and a `strong form of entanglement' occurs between the component concepts. While the video-based test confirms previous empirical results on entanglement in human cognition, our ground-breaking empirical approach surpasses language barriers and eliminates the need for prior knowledge, enabling universal accessibility. Finally, this transformative methodology allows one to unravel the underlying connections that drive our perception of reality. As a matter of fact, we provide a novel explanation for the appearance of entanglement in both physics and cognitive realms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08868v1</guid>
      <category>q-bio.NC</category>
      <category>quant-ph</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Diederik Aerts, Roberto Leporini, Sandro Sozzo</dc:creator>
    </item>
    <item>
      <title>Exploring Action-Centric Representations Through the Lens of Rate-Distortion Theory</title>
      <link>https://arxiv.org/abs/2409.08892</link>
      <description>arXiv:2409.08892v1 Announce Type: cross 
Abstract: Organisms have to keep track of the information in the environment that is relevant for adaptive behaviour. Transmitting information in an economical and efficient way becomes crucial for limited-resourced agents living in high-dimensional environments. The efficient coding hypothesis claims that organisms seek to maximize the information about the sensory input in an efficient manner. Under Bayesian inference, this means that the role of the brain is to efficiently allocate resources in order to make predictions about the hidden states that cause sensory data. However, neither of those frameworks accounts for how that information is exploited downstream, leaving aside the action-oriented role of the perceptual system. Rate-distortion theory, which defines optimal lossy compression under constraints, has gained attention as a formal framework to explore goal-oriented efficient coding. In this work, we explore action-centric representations in the context of rate-distortion theory. We also provide a mathematical definition of abstractions and we argue that, as a summary of the relevant details, they can be used to fix the content of action-centric representations. We model action-centric representations using VAEs and we find that such representations i) are efficient lossy compressions of the data; ii) capture the task-dependent invariances necessary to achieve successful behaviour; and iii) are not in service of reconstructing the data. Thus, we conclude that full reconstruction of the data is rarely needed to achieve optimal behaviour, consistent with a teleological approach to perception.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08892v1</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>4th International Workshop on Active Inference, 2023</arxiv:journal_reference>
      <dc:creator>Miguel de Llanza Varona, Christopher L. Buckley, Beren Millidge</dc:creator>
    </item>
    <item>
      <title>Yes, Prime Minister, question order does matter -- and it's certainly not classical! But is it quantum?</title>
      <link>https://arxiv.org/abs/2409.08930</link>
      <description>arXiv:2409.08930v1 Announce Type: cross 
Abstract: Response to a poll can be manipulated by means of a series of leading questions. We show that such phenomena cannot be explained by use of classical probability theory, whereas quantum probability theory admits a possibility of offering an explanation. Admissible transformation rules in quantum probability, however, do impose some constraints on the modelling of cognitive behaviour, which are highlighted here. Focusing on a recent poll conducted by Ipsos on a set of questions posed by Sir Humphrey Appleby in an episode of the British political satire \textit{Yes, Prime Minister}, we show that the resulting data cannot be explained quite so simply using quantum rules, although it seems not impossible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08930v1</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <category>quant-ph</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dorje C. Brody</dc:creator>
    </item>
    <item>
      <title>The Origin of Quantum Mechanical Statistics: Some Insights from the Research on Human Language</title>
      <link>https://arxiv.org/abs/2407.14924</link>
      <description>arXiv:2407.14924v2 Announce Type: replace 
Abstract: Identical systems, or entities, are indistinguishable in quantum mechanics (QM), and the symmetrization postulate rules the possible statistical distributions of a large number of identical quantum entities. However, a thorough analysis on the historical development of QM attributes the origin of quantum statistics, in particular, Bose-Einstein statistics, to a lack of statistical independence of the micro-states of identical quantum entities. We have recently identified Bose-Einstein statistics in the combination of words in large texts, as a consequence of the entanglement created by the meaning carried by words when they combine in human language. Relying on this investigation, we put forward the hypothesis that entanglement, hence the lack of statistical independence, is due to a mechanism of contextual updating, which provides deeper reasons for the appearance of Bose-Einstein statistics in human language. However, this investigation also contributes to a better understanding of the origin of quantum mechanical statistics in physics. Finally, we provide new insights into the intrinsically random behaviour of microscopic entities that is generally assumed within classical statistical mechanics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14924v2</guid>
      <category>q-bio.NC</category>
      <category>physics.hist-ph</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Diederik Aerts, Jonito Aerts Argu\=elles, Lester Beltran, Massimiliano Sassoli de Bianchi, Sandro Sozzo</dc:creator>
    </item>
    <item>
      <title>Emergent mechanisms for long timescales depend on training curriculum and affect performance in memory tasks</title>
      <link>https://arxiv.org/abs/2309.12927</link>
      <description>arXiv:2309.12927v2 Announce Type: replace-cross 
Abstract: Recurrent neural networks (RNNs) in the brain and in silico excel at solving tasks with intricate temporal dependencies. Long timescales required for solving such tasks can arise from properties of individual neurons (single-neuron timescale, $\tau$, e.g., membrane time constant in biological neurons) or recurrent interactions among them (network-mediated timescale). However, the contribution of each mechanism for optimally solving memory-dependent tasks remains poorly understood. Here, we train RNNs to solve $N$-parity and $N$-delayed match-to-sample tasks with increasing memory requirements controlled by $N$ by simultaneously optimizing recurrent weights and $\tau$s. We find that for both tasks RNNs develop longer timescales with increasing $N$, but depending on the learning objective, they use different mechanisms. Two distinct curricula define learning objectives: sequential learning of a single-$N$ (single-head) or simultaneous learning of multiple $N$s (multi-head). Single-head networks increase their $\tau$ with $N$ and are able to solve tasks for large $N$, but they suffer from catastrophic forgetting. However, multi-head networks, which are explicitly required to hold multiple concurrent memories, keep $\tau$ constant and develop longer timescales through recurrent connectivity. Moreover, we show that the multi-head curriculum increases training speed and network stability to ablations and perturbations, and allows RNNs to generalize better to tasks beyond their training regime. This curriculum also significantly improves training GRUs and LSTMs for large-$N$ tasks. Our results suggest that adapting timescales to task requirements via recurrent interactions allows learning more complex objectives and improves the RNN's performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.12927v2</guid>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>The Twelfth International Conference on Learning Representations (2024)</arxiv:journal_reference>
      <dc:creator>Sina Khajehabdollahi, Roxana Zeraati, Emmanouil Giannakakis, Tim Jakob Sch\"afer, Georg Martius, Anna Levina</dc:creator>
    </item>
    <item>
      <title>Dendrites endow artificial neural networks with accurate, robust and parameter-efficient learning</title>
      <link>https://arxiv.org/abs/2404.03708</link>
      <description>arXiv:2404.03708v2 Announce Type: replace-cross 
Abstract: Artificial neural networks (ANNs) are at the core of most Deep learning (DL) algorithms that successfully tackle complex problems like image recognition, autonomous driving, and natural language processing. However, unlike biological brains who tackle similar problems in a very efficient manner, DL algorithms require a large number of trainable parameters, making them energy-intensive and prone to overfitting. Here, we show that a new ANN architecture that incorporates the structured connectivity and restricted sampling properties of biological dendrites counteracts these limitations. We find that dendritic ANNs are more robust to overfitting and outperform traditional ANNs on several image classification tasks while using significantly fewer trainable parameters. These advantages are likely the result of a different learning strategy, whereby most of the nodes in dendritic ANNs respond to multiple classes, unlike classical ANNs that strive for class-specificity. Our findings suggest that the incorporation of dendritic properties can make learning in ANNs more precise, resilient, and parameter-efficient and shed new light on how biological features can impact the learning strategies of ANNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03708v2</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Spyridon Chavlis, Panayiota Poirazi</dc:creator>
    </item>
    <item>
      <title>Motion Capture Analysis of Verb and Adjective Types in Austrian Sign Language</title>
      <link>https://arxiv.org/abs/2405.05161</link>
      <description>arXiv:2405.05161v2 Announce Type: replace-cross 
Abstract: Across a number of sign languages, temporal and spatial characteristics of dominant hand articulation are used to express semantic and grammatical features. In this study of Austrian Sign Language (\"Osterreichische Geb\"ardensprache, or \"OGS), motion capture data of four Deaf signers is used to quantitatively characterize the kinematic parameters of sign production in verbs and adjectives. We investigate (1) the difference in production between verbs involving a natural endpoint (telic verbs; e.g. arrive) and verbs lacking an endpoint (atelic verbs; e.g. analyze), and (2) adjective signs in intensified vs. non-intensified (plain) forms. Motion capture data analysis using linear-mixed effects models (LME) indicates that both the endpoint marking in verbs, as well as marking of intensification in adjectives, are expressed by movement modulation in \"OGS. While the semantic distinction between verb types (telic/atelic) is marked by higher peak velocity and shorter duration for telic signs compared to atelic ones, the grammatical distinction (intensification) in adjectives is expressed by longer duration for intensified compared to non-intensified adjectives. The observed individual differences of signers might be interpreted as personal signing style.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05161v2</guid>
      <category>cs.CL</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 16 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proc of the International Conference on Computational Linguistics (2024)</arxiv:journal_reference>
      <dc:creator>Julia Krebs, Evie Malaia, Ronnie B. Wilbur, Isabella Fessl, Hans-Peter Wiesinger, Hermann Schwameder, Dietmar Roehm</dc:creator>
    </item>
  </channel>
</rss>
