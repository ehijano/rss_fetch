<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 28 Oct 2024 04:00:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>VisionCoder: Empowering Multi-Agent Auto-Programming for Image Processing with Hybrid LLMs</title>
      <link>https://arxiv.org/abs/2410.19245</link>
      <description>arXiv:2410.19245v1 Announce Type: new 
Abstract: In the field of automated programming, large language models (LLMs) have demonstrated foundational generative capabilities when given detailed task descriptions. However, their current functionalities are primarily limited to function-level development, restricting their effectiveness in complex project environments and specific application scenarios, such as complicated image-processing tasks. This paper presents a multi-agent framework that utilises a hybrid set of LLMs, including GPT-4o and locally deployed open-source models, which collaboratively complete auto-programming tasks. Each agent plays a distinct role in the software development cycle, collectively forming a virtual organisation that works together to produce software products. By establishing a tree-structured thought distribution and development mechanism across project, module, and function levels, this framework offers a cost-effective and efficient solution for code generation. We evaluated our approach using benchmark datasets, and the experimental results demonstrate that VisionCoder significantly outperforms existing methods in image processing auto-programming tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19245v1</guid>
      <category>cs.SE</category>
      <category>cs.CV</category>
      <category>cs.MA</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zixiao Zhao, Jing Sun, Zhiyuan Wei, Cheng-Hao Cai, Zhe Hou, Jin Song Dong</dc:creator>
    </item>
    <item>
      <title>A Practical Approach to Combinatorial Test Design</title>
      <link>https://arxiv.org/abs/2410.19522</link>
      <description>arXiv:2410.19522v1 Announce Type: new 
Abstract: Typical software has a huge input space. The number of inputs may be astronomical or even infinite. Thus, the task of validating that the software is correct seems hopeless. To deal with this difficult task, Combinatorial Test Design (CTD) can be used to provide reduction of the testing space and high quality and efficient testing. The application of CTD is largely determined by the quality of the CTD model. This book covers the CTD test design methodology and CTD modeling in details. It elaborates on the process of constraints definition. It also explains how to best define your coverage requirements to direct and focus your tests. It is hard to create good CTD models without a good grasp of the implementation of CTD tooling. To that hand, the book also takes a deeper dive into covering principles and algorithms needed to build CTD tooling. Hands on exercises are used throughout the text and help create a clear understanding of the concepts covered within this book.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19522v1</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Eitan Farchi, Debbie Furman</dc:creator>
    </item>
    <item>
      <title>Mirror Matrix on the Wall: coding and vector notation as tools for introspection</title>
      <link>https://arxiv.org/abs/2410.19549</link>
      <description>arXiv:2410.19549v1 Announce Type: new 
Abstract: The vector notation adopted by GNU Octave plays a significant role as a tool for introspection, aligning itself with the vision of Kenneth E. Iverson. He believed that, just like mathematics, a programming language should be an effective thinking tool for representing and reasoning about problems we wish to address. This work aims to explore the use of vector notation in GNU Octave through the analysis of operators and functions, providing a closer alignment with mathematical notation and enhancing code efficiency. We will delve into fundamental concepts such as indexing, broadcasting, and function handles, and present case studies for a deeper understanding of these concepts. By adopting vector notation, GNU Octave becomes a powerful tool for mathematicians, scientists and engineers, enabling them to express and solve complex problems more effectively and intuitively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19549v1</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonardo Ara\'ujo</dc:creator>
    </item>
    <item>
      <title>DeMuVGN: Effective Software Defect Prediction Model by Learning Multi-view Software Dependency via Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2410.19550</link>
      <description>arXiv:2410.19550v1 Announce Type: new 
Abstract: Software defect prediction (SDP) aims to identify high-risk defect modules in software development, optimizing resource allocation. While previous studies show that dependency network metrics improve defect prediction, most methods focus on code-based dependency graphs, overlooking developer factors. Current metrics, based on handcrafted features like ego and global network metrics, fail to fully capture defect-related information. To address this, we propose DeMuVGN, a defect prediction model that learns multi-view software dependency via graph neural networks. We introduce a Multi-view Software Dependency Graph (MSDG) that integrates data, call, and developer dependencies. DeMuVGN also leverages the Synthetic Minority Oversampling Technique (SMOTE) to address class imbalance and enhance defect module identification. In a case study of eight open-source projects across 20 versions, DeMuVGN demonstrates significant improvements: i) models based on multi-view graphs improve F1 scores by 11.1% to 12.1% over single-view models; ii) DeMuVGN improves F1 scores by 17.4% to 45.8% in within-project contexts and by 17.9% to 41.0% in cross-project contexts. Additionally, DeMuVGN excels in software evolution, showing more improvement in later-stage software versions. Its strong performance across different projects highlights its generalizability. We recommend future research focus on multi-view dependency graphs for defect prediction in both mature and newly developed projects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19550v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Qiao, Lina Gong, Yu Zhao, Yongwei Wang, Mingqiang Wei</dc:creator>
    </item>
    <item>
      <title>CoqPilot, a plugin for LLM-based generation of proofs</title>
      <link>https://arxiv.org/abs/2410.19605</link>
      <description>arXiv:2410.19605v1 Announce Type: new 
Abstract: We present CoqPilot, a VS Code extension designed to help automate writing of Coq proofs. The plugin collects the parts of proofs marked with the admit tactic in a Coq file, i.e., proof holes, and combines LLMs along with non-machine-learning methods to generate proof candidates for the holes. Then, CoqPilot checks if each proof candidate solves the given subgoal and, if successful, replaces the hole with it. The focus of CoqPilot is twofold. Firstly, we want to allow users to seamlessly combine multiple Coq generation approaches and provide a zero-setup experience for our tool. Secondly, we want to deliver a platform for LLM-based experiments on Coq proof generation. We developed a benchmarking system for Coq generation methods, available in the plugin, and conducted an experiment using it, showcasing the framework's possibilities. Demo of CoqPilot is available at: https://youtu.be/oB1Lx-So9Lo. Code at: https://github.com/JetBrains-Research/coqpilot</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19605v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3691620.3695357</arxiv:DOI>
      <dc:creator>Andrei Kozyrev, Gleb Solovev, Nikita Khramov, Anton Podkopaev</dc:creator>
    </item>
    <item>
      <title>Enhancing Resilience and Scalability in Travel Booking Systems: A Microservices Approach to Fault Tolerance, Load Balancing, and Service Discovery</title>
      <link>https://arxiv.org/abs/2410.19701</link>
      <description>arXiv:2410.19701v1 Announce Type: new 
Abstract: This paper investigates the inclusion of microservices architecture in the development of scalable and reliable airline reservation systems. Most of the traditional reservation systems are very rigid and centralized which makes them prone to bottlenecks and a single point of failure. As such, systems do not meet the requirements of modern airlines which are dynamic. Microservices offer better resiliency and scalability because the services do not depend on one another and can be deployed independently. The approach is grounded on the Circuit Breaker Pattern to maintain fault tolerance while consuming foreign resources such as flight APIs and payment systems. This avoided the failure propagation to the systems by 60% enabling the systems to function under external failures. Traffic rerouting also bolstered this with a guarantee of above 99.95% uptime in systems where high availability was demanded. To address this, load balancing was used, particularly the Round-Robin method which managed to enhance performance by 35% through the equal distribution of user requests among the service instances. Health checks, as well as monitoring in real-time, helped as well with failure management as they helped to contain failures before the users of the system were affected. The results suggest that the use of microservices led to a 40% increase in system scalability, a 50% decrease in downtime and a support for 30% more concurrent users than the use of monolithic architectures. These findings affirm the capability of microservices in the development of robust and flexible airline ticket booking systems that are responsive to change and recover from external system unavailability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19701v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Biman Barua, M. Shamim Kaiser</dc:creator>
    </item>
    <item>
      <title>Anti-patterns in Students' Conditional Statements</title>
      <link>https://arxiv.org/abs/2410.18989</link>
      <description>arXiv:2410.18989v1 Announce Type: cross 
Abstract: Producing high-quality code is essential as it makes a codebase more maintainable, reducing the cost and effort associated with a project. However, students learning to program are often given short, automatically graded programming tasks that they do not need to alter or maintain in the future. This can lead to poor-quality code that, although it may pass the test cases associated with the problem, contains anti-patterns - commonly occurring but ineffective or counterproductive programming patterns. This study investigates anti-patterns relating to conditional statements in code submissions made by students in an introductory Python course. Our primary motivation is to understand the prevalence and types of anti-patterns that occur in novice code. We analyzed 41,032 Python code submissions from 398 first-year students, using the open-source "qChecker" tool to identify 15 specific anti-patterns related to conditional statements. Our findings reveal that the most common anti-patterns are "if/else return bool", "confusing else", and "nested if", with "if/else return bool" and "confusing else" alone constituting nearly 60% of the total anti-patterns observed. These anti-patterns were prevalent across various lab exercises, suggesting a need for targeted educational interventions. Our main contribution includes a detailed analysis of anti-patterns in student code, and recommendations for improving coding practices in computing education contexts. The submissions we analyse were also collected prior to the emergence of generative AI tools, providing a snapshot of the issues present in student code before the availability of AI tool support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18989v1</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Etienne Naude, Paul Denny, Andrew Luxton-Reilly</dc:creator>
    </item>
    <item>
      <title>Completeness of FSM Test Suites Reconsidered</title>
      <link>https://arxiv.org/abs/2410.19405</link>
      <description>arXiv:2410.19405v1 Announce Type: cross 
Abstract: A fault domain that has been widely studied in black-box conformance testing is the class of finite state machines (FSMs) with at most $k$ extra states. Numerous methods for generating test suites have been proposed that guarantee fault coverage for this class. These test suites grow exponentially in $k$, so one can only run them for small $k$. But the assumption that $k$ is small is not realistic in practice. As a result, completeness for this fault domain has limited practical significance. As an alternative, we propose (much larger) fault domains that capture the assumption that when bugs in an implementation introduce extra states, these states can be reached via a few (at most $k$) transitions from states reachable via a set $A$ of common scenarios. Preliminary evidence suggests these fault domains, which contain FSMs with an exponential number of extra states (in $k$), are of practical use for testing network protocols. We present a sufficient condition for \emph{$k$-$A$-completeness} of test suites with respect to these fault domains, phrased entirely in terms of properties of their testing tree. Our condition implies $k$-$A$-completeness of two prominent test suite generation algorithms, the Wp and HSI methods. Counterexamples show that three other approaches, the H, SPY and SPYH methods, do not always generate $k$-$A$-complete test suites.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19405v1</guid>
      <category>cs.FL</category>
      <category>cs.SE</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Frits Vaandrager (Radboud University), Paul Fiter\u{a}u-Bro\c{s}tean (Uppsala University), Ivo Melse (Radboud University)</dc:creator>
    </item>
    <item>
      <title>AgentForge: A Flexible Low-Code Platform for Reinforcement Learning Agent Design</title>
      <link>https://arxiv.org/abs/2410.19528</link>
      <description>arXiv:2410.19528v1 Announce Type: cross 
Abstract: Developing a reinforcement learning (RL) agent often involves identifying effective values for a large number of parameters, covering the policy, reward function, environment, and the agent's internal architecture, such as parameters controlling how the peripheral vision and memory modules work. Critically, since these parameters are interrelated in complex ways, optimizing them can be viewed as a black box optimization problem, which is especially challenging for non-experts. Although existing optimization-as-a-service platforms (e.g., Vizier, Optuna) can handle such problems, they are impractical for RL systems, as users must manually map each parameter to different components, making the process cumbersome and error-prone. They also require deep understanding of the optimization process, limiting their application outside ML experts and restricting access for fields like cognitive science, which models human decision-making. To tackle these challenges, we present AgentForge, a flexible low-code framework to optimize any parameter set across an RL system. AgentForge allows the user to perform individual or joint optimization of parameter sets. An optimization problem can be defined in a few lines of code and handed to any of the interfaced optimizers. We evaluated its performance in a challenging vision-based RL problem. AgentForge enables practitioners to develop RL agents without requiring extensive coding or deep expertise in optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19528v1</guid>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francisco Erivaldo Fernandes Junior, Antti Oulasvirta</dc:creator>
    </item>
    <item>
      <title>Self-planning Code Generation with Large Language Models</title>
      <link>https://arxiv.org/abs/2303.06689</link>
      <description>arXiv:2303.06689v4 Announce Type: replace 
Abstract: Although large language models (LLMs) have demonstrated impressive ability in code generation, they are still struggling to address the complicated intent provided by humans. It is widely acknowledged that humans typically employ planning to decompose complex problems and schedule solution steps prior to implementation. To this end, we introduce planning into code generation to help the model understand complex intent and reduce the difficulty of problem-solving. This paper proposes a self-planning code generation approach with large language models, which consists of two phases, namely planning phase and implementation phase. Specifically, in the planning phase, LLM plans out concise solution steps from the intent combined with few-shot prompting. Subsequently, in the implementation phase, the model generates code step by step, guided by the preceding solution steps. We conduct extensive experiments on various code-generation benchmarks across multiple programming languages. Experimental results show that self-planning code generation achieves a relative improvement of up to 25.4% in Pass@1 compared to direct code generation, and up to 11.9% compared to Chain-of-Thought of code generation. Moreover, our self-planning approach also enhances the quality of the generated code with respect to correctness, readability, and robustness, as assessed by humans.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.06689v4</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xue Jiang, Yihong Dong, Lecheng Wang, Zheng Fang, Qiwei Shang, Ge Li, Zhi Jin, Wenpin Jiao</dc:creator>
    </item>
    <item>
      <title>Augmenting Interpolation-Based Model Checking with Auxiliary Invariants (Extended Version)</title>
      <link>https://arxiv.org/abs/2403.07821</link>
      <description>arXiv:2403.07821v2 Announce Type: replace 
Abstract: Software model checking is a challenging problem, and generating relevant invariants is a key factor in proving the safety properties of a program. Program invariants can be obtained by various approaches, including lightweight procedures based on data-flow analysis and intensive techniques using Craig interpolation. Although data-flow analysis runs efficiently, it often produces invariants that are too weak to prove the properties. By contrast, interpolation-based approaches build strong invariants from interpolants, but they might not scale well due to expensive interpolation procedures. Invariants can also be injected into model-checking algorithms to assist the analysis. Invariant injection has been studied for many well-known approaches, including k-induction, predicate abstraction, and symbolic execution. We propose an augmented interpolation-based verification algorithm that injects external invariants into interpolation-based model checking (McMillan, 2003), a hardware model-checking algorithm recently adopted for software verification. The auxiliary invariants help prune unreachable states in Craig interpolants and confine the analysis to the reachable parts of a program. We implemented the proposed technique in the verification framework CPAchecker and evaluated it against mature SMT-based methods in CPAchecker as well as other state-of-the-art software verifiers. We found that injecting invariants reduces the number of interpolation queries needed to prove safety properties and improves the run-time efficiency. Consequently, the proposed invariant-injection approach verified difficult tasks that none of its plain version (i.e., without invariants), the invariant generator, or any compared tools could solve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07821v2</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dirk Beyer, Po-Chun Chien, Nian-Ze Lee</dc:creator>
    </item>
    <item>
      <title>Automatically Generating UI Code from Screenshot: A Divide-and-Conquer-Based Approach</title>
      <link>https://arxiv.org/abs/2406.16386</link>
      <description>arXiv:2406.16386v2 Announce Type: replace 
Abstract: Websites are critical in today's digital world, with over 1.11 billion currently active and approximately 252,000 new sites launched daily. Converting website layout design into functional UI code is a time-consuming yet indispensable step of website development. Manual methods of converting visual designs into functional code present significant challenges, especially for non-experts. To explore automatic design-to-code solutions, we first conduct a motivating study on GPT-4o and identify three types of issues in generating UI code: element omission, element distortion, and element misarrangement. We further reveal that a focus on smaller visual segments can help multimodal large language models (MLLMs) mitigate these failures in the generation process. In this paper, we propose DCGen, a divide-and-conquer-based approach to automate the translation of webpage design to UI code. DCGen starts by dividing screenshots into manageable segments, generating descriptions for each segment, and then reassembling them into complete UI code for the entire screenshot. We conduct extensive testing with a dataset comprised of real-world websites and various MLLMs and demonstrate that DCGen achieves up to a 14% improvement in visual similarity over competing methods. To the best of our knowledge, DCGen is the first segment-aware prompt-based approach for generating UI code directly from screenshots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16386v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuxuan Wan, Chaozheng Wang, Yi Dong, Wenxuan Wang, Shuqing Li, Yintong Huo, Michael R. Lyu</dc:creator>
    </item>
    <item>
      <title>How much does AI impact development speed? An enterprise-based randomized controlled trial</title>
      <link>https://arxiv.org/abs/2410.12944</link>
      <description>arXiv:2410.12944v2 Announce Type: replace 
Abstract: How much does AI assistance impact developer productivity? To date, the software engineering literature has provided a range of answers, targeting a diversity of outcomes: from perceived productivity to speed on task and developer throughput. Our randomized controlled trial with 96 full-time Google software engineers contributes to this literature by sharing an estimate of the impact of three AI features on the time developers spent on a complex, enterprise-grade task. We found that AI significantly shortened the time developers spent on task. Our best estimate of the size of this effect, controlling for factors known to influence developer time on task, stands at about 21\%, although our confidence interval is large. We also found an interesting effect whereby developers who spend more hours on code-related activities per day were faster with AI. Product and future research considerations are discussed. In particular, we invite further research that explores the impact of AI at the ecosystem level and across multiple suites of AI-enhanced tools, since we cannot assume that the effect size obtained in our lab study will necessarily apply more broadly, or that the effect of AI found using internal Google tooling in the summer of 2024 will translate across tools and over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12944v2</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elise Paradis, Kate Grey, Quinn Madison, Daye Nam, Andrew Macvean, Vahid Meimand, Nan Zhang, Ben Ferrari-Church, Satish Chandra</dc:creator>
    </item>
    <item>
      <title>Assessing Quantum Extreme Learning Machines for Software Testing in Practice</title>
      <link>https://arxiv.org/abs/2410.15494</link>
      <description>arXiv:2410.15494v2 Announce Type: replace 
Abstract: Machine learning has been extensively applied for various classical software testing activities such as test generation, minimization, and prioritization. Along the same lines, recently, there has been interest in applying quantum machine learning to software testing. For example, Quantum Extreme Learning Machines (QELMs) were recently applied for testing classical software of industrial elevators. However, most studies on QELMs, whether in software testing or other areas, used ideal quantum simulators that fail to account for the noise in current quantum computers. While ideal simulations offer insight into QELM's theoretical capabilities, they do not enable studying their performance on current noisy quantum computers. To this end, we study how quantum noise affects QELM in three industrial and real-world classical software testing case studies, providing insights into QELMs' robustness to noise. Such insights assess QELMs potential as a viable solution for industrial software testing problems in today's noisy quantum computing. Our results show that QELMs are significantly affected by quantum noise, with a performance drop of 250% in regression tasks and 50% in classification tasks. Although introducing noise during both ML training and testing phases can improve results, the reduction is insufficient for practical applications. While error mitigation techniques can enhance noise resilience, achieving an average 3.0% performance drop in classification, but their effectiveness varies by context, highlighting the need for QELM-tailored error mitigation strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15494v2</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Asmar Muqeet, Hassan Sartaj, Aitor Arreieta, Shaukat Ali, Paolo Arcaini, Maite Arratibel, Julie Marie Gj{\o}by, Narasimha Raghavan Veeraragavan, Jan F. Nyg{\aa}rd</dc:creator>
    </item>
    <item>
      <title>Software Frugality in an Accelerating World: the Case of Continuous Integration</title>
      <link>https://arxiv.org/abs/2410.15816</link>
      <description>arXiv:2410.15816v2 Announce Type: replace 
Abstract: The acceleration of software development and delivery requires rigorous continuous testing and deployment of software systems, which are being deployed in increasingly diverse, complex, and dynamic environments. In recent years, the popularization of DevOps and integrated software forges like GitLab and GitHub has largely democratized Continuous Integration (CI) practices for a growing number of software. However, this trend intersects significantly with global energy consumption concerns and the growing demand for frugality in the Information and Communication Technology (ICT) sector. CI pipelines typically run in data centers which contribute significantly to the environmental footprint of ICT, yet there is little information available regarding their environmental impact. This article aims to bridge this gap by conducting the first large-scale analysis of the energy footprint of CI pipelines implemented with GitHub Actions and to provide a first overview of the energy impact of CI. We collect, instrument, and reproduce 838 workflows from 396 Java repositories hosted on GitHub to measure their energy consumption. We observe that the average unitary energy cost of a pipeline is relatively low, at 10 Wh. However, due to repeated invocations of these pipelines in real settings, the aggregated energy consumption cost per project is high, averaging 22 kWh. When evaluating CO2 emissions based on regional Wh-to-CO2 estimates, we observe that the average aggregated CO2 emissions are significant, averaging 10.5 kg. To put this into perspective, this is akin to the emissions produced by driving approximately 100 kilometers in a typical European car (110 gCO2/km). In light of our results, we advocate that developers should have the means to better anticipate and reflect on the environmental consequences of their CI choices when implementing DevOps practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15816v2</guid>
      <category>cs.SE</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Quentin Perez, Romain Lefeuvre, Thomas Degueule, Olivier Barais, Benoit Combemale</dc:creator>
    </item>
    <item>
      <title>AIM: Automated Input Set Minimization for Metamorphic Security Testing</title>
      <link>https://arxiv.org/abs/2402.10773</link>
      <description>arXiv:2402.10773v4 Announce Type: replace-cross 
Abstract: Although the security testing of Web systems can be automated by generating crafted inputs, solutions to automate the test oracle, i.e., vulnerability detection, remain difficult to apply in practice. Specifically, though previous work has demonstrated the potential of metamorphic testing, security failures can be determined by metamorphic relations that turn valid inputs into malicious inputs, metamorphic relations are typically executed on a large set of inputs, which is time-consuming and thus makes metamorphic testing impractical. We propose AIM, an approach that automatically selects inputs to reduce testing costs while preserving vulnerability detection capabilities. AIM includes a clustering-based black-box approach, to identify similar inputs based on their security properties. It also relies on a novel genetic algorithm to efficiently select diverse inputs while minimizing their total cost. Further, it contains a problem-reduction component to reduce the search space and speed up the minimization process. We evaluated the effectiveness of AIM on two well-known Web systems, Jenkins and Joomla, with documented vulnerabilities. We compared AIM's results with four baselines involving standard search approaches. Overall, AIM reduced metamorphic testing time by 84% for Jenkins and 82% for Joomla, while preserving the same level of vulnerability detection. Furthermore, AIM significantly outperformed all the considered baselines regarding vulnerability coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10773v4</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nazanin Bayati Chaleshtari, Yoann Marquer, Fabrizio Pastore, Lionel C. Briand</dc:creator>
    </item>
    <item>
      <title>Measuring memorization in RLHF for code completion</title>
      <link>https://arxiv.org/abs/2406.11715</link>
      <description>arXiv:2406.11715v2 Announce Type: replace-cross 
Abstract: Reinforcement learning with human feedback (RLHF) has become the dominant method to align large models to user preferences. Unlike fine-tuning, for which there are many studies regarding training data memorization, it is not clear how memorization is affected by or introduced in the RLHF alignment process. Understanding this relationship is important as real user data may be collected and used to align large models; if user data is memorized during RLHF and later regurgitated, this could raise privacy concerns. In addition to RLHF, other methods such as Direct Preference Optimization (DPO) and $\Psi$PO have gained popularity for learning directly from human preferences, removing the need for optimizing intermediary reward models with reinforcement learning. In this work, we analyze how training data memorization can surface and propagate through each phase of RLHF and direct preference learning. We focus our study on code completion models, as code completion is one of the most popular use cases for large language models. We find that RLHF significantly decreases the chance that data used for reward modeling and reinforcement learning is memorized in comparison to directly fine-tuning on this data, but that examples already memorized during the fine-tuning stage of RLHF, will, in the majority of cases, remain memorized after RLHF. In contrast, we find that aligning by learning directly from human preference data via a special case of $\Psi$PO, Identity Preference Optimization (IPO), increases the likelihood that training data is regurgitated compared to RLHF. Our work suggests that RLHF, as opposed to direct preference learning, is a safer way to mitigate the risk of regurgitating sensitive preference data when aligning large language models. We find our conclusions are robust across multiple code completion datasets, tasks, and model scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11715v2</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.SE</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aneesh Pappu, Billy Porter, Ilia Shumailov, Jamie Hayes</dc:creator>
    </item>
  </channel>
</rss>
