<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Apr 2024 04:03:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Fast and simple inner-loop algorithms of static / dynamic BLP estimations</title>
      <link>https://arxiv.org/abs/2404.04494</link>
      <description>arXiv:2404.04494v1 Announce Type: new 
Abstract: This study investigates computationally efficient inner-loop algorithms for estimating static / dynamic BLP models. It provides the following ideas to reduce the number of inner-loop iterations: (1). Add a term concerning the outside option share in the BLP contraction mapping; (2). Analytically represent mean product utilities as a function of value functions and solve for the value functions (for dynamic BLP); (3-1). Combine the spectral / SQUAREM algorithms; (3-2). Choice of the step sizes. These methods are independent and easy to implement. This study shows good performance of these ideas by numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04494v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takeshi Fukasawa</dc:creator>
    </item>
    <item>
      <title>Absolute Technical Efficiency Indices</title>
      <link>https://arxiv.org/abs/2404.04590</link>
      <description>arXiv:2404.04590v1 Announce Type: new 
Abstract: Technical efficiency indices (TEIs) can be estimated using the traditional stochastic frontier analysis approach, which yields relative indices that do not allow self-interpretations. In this paper, we introduce a single-step estimation procedure for TEIs that eliminates the need to identify best practices and avoids imposing restrictive hypotheses on the error term. The resulting indices are absolute and allow for individual interpretation. In our model, we estimate a distance function using the inverse coefficient of resource utilization, rather than treating it as unobservable. We employ a Tobit model with a translog distance function as our econometric framework. Applying this model to a sample of 19 airline companies from 2012 to 2021, we find that: (1) Absolute technical efficiency varies considerably between companies with medium-haul European airlines being technically the most efficient, while Asian airlines are the least efficient; (2) Our estimated TEIs are consistent with the observed data with a decline in efficiency especially during the Covid-19 crisis and Brexit period; (3) All airlines contained in our sample would be able to increase their average technical efficiency by 0.209% if they reduced their average kerosene consumption by 1%; (4) Total factor productivity (TFP) growth slowed between 2013 and 2019 due to a decrease in Disembodied Technical Change (DTC) and a small effect from Scale Economies (SE). Toward the end of our study period, TFP growth seemed increasingly driven by the SE effect, with a sharp decline in 2020 followed by an equally sharp recovery in 2021 for most airlines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04590v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Montacer Ben Cheikh Larbi, Sina Belkhiria</dc:creator>
    </item>
    <item>
      <title>Stratifying on Treatment Status</title>
      <link>https://arxiv.org/abs/2404.04700</link>
      <description>arXiv:2404.04700v1 Announce Type: new 
Abstract: We investigate the estimation of treatment effects from a sample that is stratified on the binary treatment status. In the case of unconfounded assignment where the potential outcomes are independent of the treatment given covariates, we show that standard estimators of the average treatment effect are inconsistent. In the case of an endogenous treatment and a binary instrument, we show that the IV estimator is inconsistent for the local average treatment effect. In both cases, we propose simple alternative estimators that are consistent in stratified samples, assuming that the fraction treated in the population is known or can be estimated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04700v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyong Hahn, John Ham, Geert Ridder, Shuyang Sheng</dc:creator>
    </item>
    <item>
      <title>Neural Network Modeling for Forecasting Tourism Demand in Stopi\'{c}a Cave: A Serbian Cave Tourism Study</title>
      <link>https://arxiv.org/abs/2404.04974</link>
      <description>arXiv:2404.04974v1 Announce Type: new 
Abstract: For modeling the number of visits in Stopi\'{c}a cave (Serbia) we consider the classical Auto-regressive Integrated Moving Average (ARIMA) model, Machine Learning (ML) method Support Vector Regression (SVR), and hybrid NeuralPropeth method which combines classical and ML concepts. The most accurate predictions were obtained with NeuralPropeth which includes the seasonal component and growing trend of time-series. In addition, non-linearity is modeled by shallow Neural Network (NN), and Google Trend is incorporated as an exogenous variable. Modeling tourist demand represents great importance for management structures and decision-makers due to its applicability in establishing sustainable tourism utilization strategies in environmentally vulnerable destinations such as caves. The data provided insights into the tourist demand in Stopi\'{c}a cave and preliminary data for addressing the issues of carrying capacity within the most visited cave in Serbia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04974v1</guid>
      <category>econ.EM</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Buda Baji\'c, Sr{\dj}an Mili\'cevi\'c, Aleksandar Anti\'c, Slobodan Markovi\'c, Nemanja Tomi\'c</dc:creator>
    </item>
    <item>
      <title>CAVIAR: Categorical-Variable Embeddings for Accurate and Robust Inference</title>
      <link>https://arxiv.org/abs/2404.04979</link>
      <description>arXiv:2404.04979v1 Announce Type: new 
Abstract: Social science research often hinges on the relationship between categorical variables and outcomes. We introduce CAVIAR, a novel method for embedding categorical variables that assume values in a high-dimensional ambient space but are sampled from an underlying manifold. Our theoretical and numerical analyses outline challenges posed by such categorical variables in causal inference. Specifically, dynamically varying and sparse levels can lead to violations of the Donsker conditions and a failure of the estimation functionals to converge to a tight Gaussian process. Traditional approaches, including the exclusion of rare categorical levels and principled variable selection models like LASSO, fall short. CAVIAR embeds the data into a lower-dimensional global coordinate system. The mapping can be derived from both structured and unstructured data, and ensures stable and robust estimates through dimensionality reduction. In a dataset of direct-to-consumer apparel sales, we illustrate how high-dimensional categorical variables, such as zip codes, can be succinctly represented, facilitating inference and analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04979v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anirban Mukherjee, Hannah Hanwen Chang</dc:creator>
    </item>
    <item>
      <title>Towards a generalized accessibility measure for transportation equity and efficiency</title>
      <link>https://arxiv.org/abs/2404.04985</link>
      <description>arXiv:2404.04985v1 Announce Type: new 
Abstract: Locational measures of accessibility are widely used in urban and transportation planning to understand the impact of the transportation system on influencing people's access to places. However, there is a considerable lack of measurement standards and publicly available data. We propose a generalized measure of locational accessibility that has a comprehensible form for transportation planning analysis. This metric combines the cumulative opportunities approach with gravity-based measures and is capable of catering to multiple trip purposes, travel modes, cost thresholds, and scales of analysis. Using data from multiple publicly available datasets, this metric is computed by trip purpose and travel time threshold for all block groups in the United States, and the data is made publicly accessible. Further, case studies of three large metropolitan areas reveal substantial inefficiencies in transportation infrastructure, with the most inefficiency observed in sprawling and non-core urban areas, especially for bicycling. Subsequently, it is shown that targeted investment in facilities can contribute to a more equitable distribution of accessibility to essential shopping and service facilities. By assigning greater weights to socioeconomically disadvantaged neighborhoods, the proposed metric formally incorporates equity considerations into transportation planning, contributing to a more equitable distribution of accessibility to essential services and facilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04985v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajat Verma, Mithun Debnath, Shagun Mittal, Satish V. Ukkusuri</dc:creator>
    </item>
    <item>
      <title>Context-dependent Causality (the Non-Nonotonic Case)</title>
      <link>https://arxiv.org/abs/2404.05021</link>
      <description>arXiv:2404.05021v1 Announce Type: new 
Abstract: We develop a novel identification strategy as well as a new estimator for context-dependent causal inference in non-parametric triangular models with non-separable disturbances. Departing from the common practice, our analysis does not rely on the strict monotonicity assumption. Our key contribution lies in leveraging on diffusion models to formulate the structural equations as a system evolving from noise accumulation to account for the influence of the latent context (confounder) on the outcome. Our identifiability strategy involves a system of Fredholm integral equations expressing the distributional relationship between a latent context variable and a vector of observables. These integral equations involve an unknown kernel and are governed by a set of structural form functions, inducing a non-monotonic inverse problem. We prove that if the kernel density can be represented as an infinite mixture of Gaussians, then there exists a unique solution for the unknown function. This is a significant result, as it shows that it is possible to solve a non-monotonic inverse problem even when the kernel is unknown. On the methodological front we leverage on a novel and enriched Contaminated Generative Adversarial (Neural) Networks (CONGAN) which we provide as a solution to the non-monotonic inverse problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05021v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nir Billfeld, Moshe Kim</dc:creator>
    </item>
    <item>
      <title>Estimating granular house price distributions in the Australian market using Gaussian mixtures</title>
      <link>https://arxiv.org/abs/2404.05178</link>
      <description>arXiv:2404.05178v1 Announce Type: new 
Abstract: A new methodology is proposed to approximate the time-dependent house price distribution at a fine regional scale using Gaussian mixtures. The means, variances and weights of the mixture components are related to time, location and dwelling type through a non linear function trained by a deep functional approximator. Price indices are derived as means, medians, quantiles or other functions of the estimated distributions. Price densities for larger regions, such as a city, are calculated via a weighted sum of the component density functions. The method is applied to a data set covering all of Australia at a fine spatial and temporal resolution. In addition to enabling a detailed exploration of the data, the proposed index yields lower prediction errors in the practical task of individual dwelling price projection from previous sales values within the three major Australian cities. The estimated quantiles are also found to be well calibrated empirically, capturing the complexity of house price distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05178v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Willem P Sijp, Anastasios Panagiotelis</dc:creator>
    </item>
    <item>
      <title>Maximally Forward-Looking Core Inflation</title>
      <link>https://arxiv.org/abs/2404.05209</link>
      <description>arXiv:2404.05209v1 Announce Type: new 
Abstract: Timely monetary policy decision-making requires timely core inflation measures. We create a new core inflation series that is explicitly designed to succeed at that goal. Precisely, we introduce the Assemblage Regression, a generalized nonnegative ridge regression problem that optimizes the price index's subcomponent weights such that the aggregate is maximally predictive of future headline inflation. Ordering subcomponents according to their rank in each period switches the algorithm to be learning supervised trimmed inflation - or, put differently, the maximally forward-looking summary statistic of the realized price changes distribution. In an extensive out-of-sample forecasting experiment for the US and the euro area, we find substantial improvements for signaling medium-term inflation developments in both the pre- and post-Covid years. Those coming from the supervised trimmed version are particularly striking, and are attributable to a highly asymmetric trimming which contrasts with conventional indicators. We also find that this metric was indicating first upward pressures on inflation as early as mid-2020 and quickly captured the turning point in 2022. We also consider extensions, like assembling inflation from geographical regions, trimmed temporal aggregation, and building core measures specialized for either upside or downside inflation risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05209v1</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Philippe Goulet Coulombe, Karin Klieber, Christophe Barrette, Maximilian Goebel</dc:creator>
    </item>
    <item>
      <title>Common Trends and Long-Run Multipliers in Nonlinear Structural VARs</title>
      <link>https://arxiv.org/abs/2404.05349</link>
      <description>arXiv:2404.05349v1 Announce Type: new 
Abstract: While it is widely recognised that linear (structural) VARs may omit important features of economic time series, the use of nonlinear SVARs has to date been almost entirely confined to the modelling of stationary time series, because of a lack of understanding as to how common stochastic trends may be accommodated within nonlinear VAR models. This has unfortunately circumscribed the range of series to which such models can be applied -- and/or required that these series be first transformed to stationarity, a potential source of misspecification -- and prevented the use of long-run identifying restrictions in these models. To address these problems, we develop a flexible class of additively time-separable nonlinear SVARs, which subsume models with threshold-type endogenous regime switching, both of the piecewise linear and smooth transition varieties. We extend the Granger-Johansen representation theorem to this class of models, obtaining conditions that specialise exactly to the usual ones when the model is linear. We further show that, as a corollary, these models are capable of supporting the same kinds of long-run identifying restrictions as are available in linear cointegrated SVARs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05349v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James A. Duffy, Sophocles Mavroeidis</dc:creator>
    </item>
    <item>
      <title>Nonlinear Fore(Back)casting and Innovation Filtering for Causal-Noncausal VAR Models</title>
      <link>https://arxiv.org/abs/2205.09922</link>
      <description>arXiv:2205.09922v3 Announce Type: replace 
Abstract: We introduce closed-form formulas of out-of-sample predictive densities for forecasting and backcasting of mixed causal-noncausal (Structural) Vector Autoregressive VAR models. These nonlinear and time irreversible non-Gaussian VAR processes are shown to satisfy the Markov property in both calendar and reverse time. A post-estimation inference method for assessing the forecast interval uncertainty due to the preliminary estimation step is introduced too. The nonlinear past-dependent innovations of a mixed causal-noncausal VAR model are defined and their filtering and identification methods are discussed. Our approach is illustrated by a simulation study, and an application to cryptocurrency prices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.09922v3</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Gourieroux, Joann Jasiak</dc:creator>
    </item>
    <item>
      <title>Statistical Inference of Optimal Allocations I: Regularities and their Implications</title>
      <link>https://arxiv.org/abs/2403.18248</link>
      <description>arXiv:2403.18248v2 Announce Type: replace 
Abstract: In this paper, we develop a functional differentiability approach for solving statistical optimal allocation problems. We first derive Hadamard differentiability of the value function through a detailed analysis of the general properties of the sorting operator. Central to our framework are the concept of Hausdorff measure and the area and coarea integration formulas from geometric measure theory. Building on our Hadamard differentiability results, we demonstrate how the functional delta method can be used to directly derive the asymptotic properties of the value function process for binary constrained optimal allocation problems, as well as the two-step ROC curve estimator. Moreover, leveraging profound insights from geometric functional analysis on convex and local Lipschitz functionals, we obtain additional generic Fr\'echet differentiability results for the value functions of optimal allocation problems. These compelling findings motivate us to study carefully the first order approximation of the optimal social welfare. In this paper, we then present a double / debiased estimator for the value functions. Importantly, the conditions outlined in the Hadamard differentiability section validate the margin assumption from the statistical classification literature employing plug-in methods that justifies a faster convergence rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18248v2</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kai Feng, Han Hong</dc:creator>
    </item>
    <item>
      <title>Do t-Statistic Hurdles Need to be Raised?</title>
      <link>https://arxiv.org/abs/2204.10275</link>
      <description>arXiv:2204.10275v4 Announce Type: replace-cross 
Abstract: Many scholars have called for raising statistical hurdles to guard against false discoveries in academic publications. I show these calls may be difficult to justify empirically. Published data exhibit bias: results that fail to meet existing hurdles are often unobserved. These unobserved results must be extrapolated, which can lead to weak identification of revised hurdles. In contrast, statistics that can target only published findings (e.g. empirical Bayes shrinkage and the FDR) can be strongly identified, as data on published findings is plentiful. I demonstrate these results theoretically and in an empirical analysis of the cross-sectional return predictability literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.10275v4</guid>
      <category>q-fin.GN</category>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Y. Chen</dc:creator>
    </item>
  </channel>
</rss>
