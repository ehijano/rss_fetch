<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.PM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.PM</link>
    <description>q-fin.PM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.PM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Apr 2025 04:00:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Efficient Portfolio Selection through Preference Aggregation with Quicksort and the Bradley--Terry Model</title>
      <link>https://arxiv.org/abs/2504.16093</link>
      <description>arXiv:2504.16093v1 Announce Type: new 
Abstract: How to allocate limited resources to projects that will yield the greatest long-term benefits is a problem that often arises in decision-making under uncertainty. For example, organizations may need to evaluate and select innovation projects with risky returns. Similarly, when allocating resources to research projects, funding agencies are tasked with identifying the most promising proposals based on idiosyncratic criteria. Finally, in participatory budgeting, a local community may need to select a subset of public projects to fund. Regardless of context, agents must estimate the uncertain values of a potentially large number of projects. Developing parsimonious methods to compare these projects, and aggregating agent evaluations so that the overall benefit is maximized, are critical in assembling the best project portfolio. Unlike in standard sorting algorithms, evaluating projects on the basis of uncertain long-term benefits introduces additional complexities. We propose comparison rules based on Quicksort and the Bradley--Terry model, which connects rankings to pairwise "win" probabilities. In our model, each agent determines win probabilities of a pair of projects based on his or her specific evaluation of the projects' long-term benefit. The win probabilities are then appropriately aggregated and used to rank projects. Several of the methods we propose perform better than the two most effective aggregation methods currently available. Additionally, our methods can be combined with sampling techniques to significantly reduce the number of pairwise comparisons. We also discuss how the Bradley--Terry portfolio selection approach can be implemented in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16093v1</guid>
      <category>q-fin.PM</category>
      <category>cs.AI</category>
      <category>math.PR</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yurun Ge, Lucas B\"ottcher, Tom Chou, Maria R. D'Orsogna</dc:creator>
    </item>
    <item>
      <title>Collective Defined Contribution Schemes Without Intergenerational Cross-Subsidies</title>
      <link>https://arxiv.org/abs/2504.16892</link>
      <description>arXiv:2504.16892v1 Announce Type: new 
Abstract: We present an architecture for managing Collective Defined Contribution (CDC) schemes. The current approach to UK CDC can be described as shared-indexation, where the nominal benefit of every member in a scheme receives the same level of indexation each year. The design of such schemes rely on the use of approximate discounting methodologies to value liabilities, and this leads to intergenerational cross-subsidies which can be large and unpredictable. We present an alternative approach which we call Collective-Drawdown CDC. This approach does not result in intergenerational cross-subsidies since all pooling is performed by explicit insurance contracts. It is therefore completely fair. Moreover, this scheme results in better pension outcomes when compared to shared-indexation CDC under the same model parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16892v1</guid>
      <category>q-fin.PM</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John Armstrong, James Dalby, Rohan Hobbs</dc:creator>
    </item>
    <item>
      <title>Synthetic Data for Portfolios: A Throw of the Dice Will Never Abolish Chance</title>
      <link>https://arxiv.org/abs/2501.03993</link>
      <description>arXiv:2501.03993v5 Announce Type: replace 
Abstract: Simulation methods have always been instrumental in finance, and data-driven methods with minimal model specification, commonly referred to as generative models, have attracted increasing attention, especially after the success of deep learning in a broad range of fields. However, the adoption of these models in financial applications has not matched the growing interest, probably due to the unique complexities and challenges of financial markets. This paper contributes to a deeper understanding of the limitations of generative models, particularly in portfolio and risk management. To this end, we begin by presenting theoretical results on the importance of initial sample size, and point out the potential pitfalls of generating far more data than originally available. We then highlight the inseparable nature of model development and the desired uses by touching on a paradox: usual generative models inherently care less about what is important for constructing portfolios (in particular the long-short ones). Based on these findings, we propose a pipeline for the generation of multivariate returns that meets conventional evaluation standards on a large universe of US equities while being compliant with stylized facts observed in asset returns and turning around the pitfalls we previously identified. Moreover, we insist on the need for more accurate evaluation methods, and suggest, through an example of mean-reversion strategies, a method designed to identify poor models for a given application based on regurgitative training, i.e. retraining the model using the data it has itself generated, which is commonly referred to in statistics as identifiability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03993v5</guid>
      <category>q-fin.PM</category>
      <category>q-fin.RM</category>
      <category>stat.ML</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adil Rengim Cetingoz, Charles-Albert Lehalle</dc:creator>
    </item>
  </channel>
</rss>
