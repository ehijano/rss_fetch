<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Mar 2024 04:00:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 21 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Parameter Estimation from Single Patient, Single Time-Point Sequencing Data of Recurrent Tumors</title>
      <link>https://arxiv.org/abs/2403.13081</link>
      <description>arXiv:2403.13081v1 Announce Type: new 
Abstract: In this study, we develop consistent estimators for key parameters that govern the dynamics of tumor cell populations when subjected to pharmacological treatments. While these treatments often lead to an initial reduction in the abundance of drug-sensitive cells, a population of drug-resistant cells frequently emerges over time, resulting in cancer recurrence. Samples from recurrent tumors present as an invaluable data source that can offer crucial insights into the ability of cancer cells to adapt and withstand treatment interventions. To effectively utilize the data obtained from recurrent tumors, we derive several large number limit theorems, specifically focusing on the metrics that quantify the clonal diversity of cancer cell populations at the time of cancer recurrence. These theorems then serve as the foundation for constructing our estimators. A distinguishing feature of our approach is that our estimators only require a single time-point sequencing data from a single tumor, thereby enhancing the practicality of our approach and enabling the understanding of cancer recurrence at the individual level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13081v1</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <category>q-bio.PE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kevin Leder, Ruping Sun, Zicheng Wang, Xuanming Zhang</dc:creator>
    </item>
    <item>
      <title>Longitudinal changes in deep learning-estimated breast density and their impact on the risk of screen-detected breast cancer: The DeepJoint Algorithm</title>
      <link>https://arxiv.org/abs/2403.13488</link>
      <description>arXiv:2403.13488v1 Announce Type: new 
Abstract: Mammography-based screening programs play a crucial role in reducing breast cancer mortality through early detection. Their efficacy is influenced by breast density, a dynamic factor that evolves over time, modifying breast cancer risk. Women with high breast density face increased breast cancer risk, coupled with reduced mammographic sensitivity. In this work, we present the DeepJoint algorithm, a pipeline for quantitative breast density assessment, investigating its association with breast cancer risk. First, we develop a lightweight deep-learning segmentation model that uses processed mammography images acquired from multiple manufacturers to assess quantitative breast density metrics such as dense area and percent density. Then, we fit a joint model to evaluate the association of these metrics with the time-to-breast cancer occurrence using an extensive database of 77,298 women participating in breast cancer screening in the United States. We demonstrate the impact of the current value and slope of the biomarker on breast cancer risk. We also derive individual and dynamic breast cancer risk predictions to describe how the individual longitudinal evolution of dense area and percent density impacts the risk of breast cancer. This innovative approach aligns with the growing interest in personalized risk monitoring during screening, offering valuable insights for improving breast cancer prevention strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13488v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Manel Rakez, Julien Guillaumin, Aurelien Chick, Gaelle Coureau, Foucauld Chamming's, Pierre Fillard, Brice Amadeo, Virginie Rondeau</dc:creator>
    </item>
    <item>
      <title>SportsNGEN: Sustained Generation of Multi-player Sports Gameplay</title>
      <link>https://arxiv.org/abs/2403.12977</link>
      <description>arXiv:2403.12977v1 Announce Type: cross 
Abstract: We present a transformer decoder based model, SportsNGEN, that is trained on sports player and ball tracking sequences that is capable of generating realistic and sustained gameplay. We train and evaluate SportsNGEN on a large database of professional tennis tracking data and demonstrate that by combining the generated simulations with a shot classifier and logic to start and end rallies, the system is capable of simulating an entire tennis match. In addition, a generic version of SportsNGEN can be customized to a specific player by fine-tuning on match data that includes that player. We show that our model is well calibrated and can be used to derive insights for coaches and broadcasters by evaluating counterfactual or what if options. Finally, we show qualitative results indicating the same approach works for football.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12977v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lachlan Thorpe, Lewis Bawden, Karanjot Vendal, John Bronskill, Richard E. Turner</dc:creator>
    </item>
    <item>
      <title>Uncertainty quantification for data-driven weather models</title>
      <link>https://arxiv.org/abs/2403.13458</link>
      <description>arXiv:2403.13458v1 Announce Type: cross 
Abstract: Artificial intelligence (AI)-based data-driven weather forecasting models have experienced rapid progress over the last years. Recent studies, with models trained on reanalysis data, achieve impressive results and demonstrate substantial improvements over state-of-the-art physics-based numerical weather prediction models across a range of variables and evaluation metrics. Beyond improved predictions, the main advantages of data-driven weather models are their substantially lower computational costs and the faster generation of forecasts, once a model has been trained. However, most efforts in data-driven weather forecasting have been limited to deterministic, point-valued predictions, making it impossible to quantify forecast uncertainties, which is crucial in research and for optimal decision making in applications. Our overarching aim is to systematically study and compare uncertainty quantification methods to generate probabilistic weather forecasts from a state-of-the-art deterministic data-driven weather model, Pangu-Weather. Specifically, we compare approaches for quantifying forecast uncertainty based on generating ensemble forecasts via perturbations to the initial conditions, with the use of statistical and machine learning methods for post-hoc uncertainty quantification. In a case study on medium-range forecasts of selected weather variables over Europe, the probabilistic forecasts obtained by using the Pangu-Weather model in concert with uncertainty quantification methods show promising results and provide improvements over ensemble forecasts from the physics-based ensemble weather model of the European Centre for Medium-Range Weather Forecasts for lead times of up to 5 days.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13458v1</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher B\"ulte, Nina Horat, Julian Quinting, Sebastian Lerch</dc:creator>
    </item>
    <item>
      <title>Scalable Scalar-on-Image Cortical Surface Regression with a Relaxed-Thresholded Gaussian Process Prior</title>
      <link>https://arxiv.org/abs/2403.13628</link>
      <description>arXiv:2403.13628v1 Announce Type: cross 
Abstract: In addressing the challenge of analysing the large-scale Adolescent Brain Cognition Development (ABCD) fMRI dataset, involving over 5,000 subjects and extensive neuroimaging data, we propose a scalable Bayesian scalar-on-image regression model for computational feasibility and efficiency. Our model employs a relaxed-thresholded Gaussian process (RTGP), integrating piecewise-smooth, sparse, and continuous functions capable of both hard- and soft-thresholding. This approach introduces additional flexibility in feature selection in scalar-on-image regression and leads to scalable posterior computation by adopting a variational approximation and utilising the Karhunen-Lo\`eve expansion for Gaussian processes. This advancement substantially reduces the computational costs in vertex-wise analysis of cortical surface data in large-scale Bayesian spatial models. The model's parameter estimation and prediction accuracy and feature selection performance are validated through extensive simulation studies and an application to the ABCD study. Here, we perform regression analysis correlating intelligence scores with task-based functional MRI data, taking into account confounding factors including age, sex, and parental education level. This validation highlights our model's capability to handle large-scale neuroimaging data while maintaining computational feasibility and accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13628v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Menacher, Thomas E. Nichols, Timothy D. Johnson, Jian Kang</dc:creator>
    </item>
    <item>
      <title>Long-term memory effects of an incremental blood pressure intervention in a mortal cohort</title>
      <link>https://arxiv.org/abs/2311.00357</link>
      <description>arXiv:2311.00357v4 Announce Type: replace 
Abstract: In the present study we investigate overall population effects on episodic memory of an intervention over 15 years that reduces systolic blood pressure in individuals with hypertension. A limitation with previous research on the potential risk reduction of such interventions is that they do not properly account for the reduction of mortality rates. Hence, one can only speculate whether the effect is due to changes in memory or changes in mortality. Therefore, we extend previous research by providing both an etiological and a prognostic effect estimate. To do this, we propose a Bayesian semi-parametric estimation approach for an incremental threshold intervention, using the extended G-formula. Additionally, we introduce a novel sparsity-inducing Dirichlet hyperprior for longitudinal data, that exploits the longitudinal structure of the data. We demonstrate the usefulness of our approach in simulations, and compare its performance to other Bayesian decision tree ensemble approaches. In our analysis of the data from the Betula cohort, we found no significant prognostic or etiological effects across all ages. This suggests that systolic blood pressure interventions likely do not strongly affect memory, whether at the overall population level or in the population that would survive under both the natural course and the intervention (the always survivor stratum).</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00357v4</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria Josefsson, Nina Karalija, Michael Daniels</dc:creator>
    </item>
    <item>
      <title>A Causal Framework to Evaluate Racial Bias in Law Enforcement Systems</title>
      <link>https://arxiv.org/abs/2402.14959</link>
      <description>arXiv:2402.14959v2 Announce Type: replace 
Abstract: We are interested in developing a data-driven method to evaluate race-induced biases in law enforcement systems. While the recent works have addressed this question in the context of police-civilian interactions using police stop data, they have two key limitations. First, bias can only be properly quantified if true criminality is accounted for in addition to race, but it is absent in prior works. Second, law enforcement systems are multi-stage and hence it is important to isolate the true source of bias within the "causal chain of interactions" rather than simply focusing on the end outcome; this can help guide reforms. In this work, we address these challenges by presenting a multi-stage causal framework incorporating criminality. We provide a theoretical characterization and an associated data-driven method to evaluate (a) the presence of any form of racial bias, and (b) if so, the primary source of such a bias in terms of race and criminality. Our framework identifies three canonical scenarios with distinct characteristics: in settings like (1) airport security, the primary source of observed bias against a race is likely to be bias in law enforcement against innocents of that race; (2) AI-empowered policing, the primary source of observed bias against a race is likely to be bias in law enforcement against criminals of that race; and (3) police-civilian interaction, the primary source of observed bias against a race could be bias in law enforcement against that race or bias from the general public in reporting against the other race. Through an extensive empirical study using police-civilian interaction data and 911 call data, we find an instance of such a counter-intuitive phenomenon: in New Orleans, the observed bias is against the majority race and the likely reason for it is the over-reporting (via 911 calls) of incidents involving the minority race by the general public.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14959v2</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jessy Xinyi Han, Andrew Miller, S. Craig Watkins, Christopher Winship, Fotini Christia, Devavrat Shah</dc:creator>
    </item>
    <item>
      <title>Surgery duration prediction using multi-task feature selection</title>
      <link>https://arxiv.org/abs/2403.09791</link>
      <description>arXiv:2403.09791v2 Announce Type: replace 
Abstract: Efficient optimization of operating room (OR) activity poses a significant challenge for hospital managers due to the complex and risky nature of the environment. The traditional "one size fits all" approach to OR scheduling is no longer practical, and personalized medicine is required to meet the diverse needs of patients, care providers, medical procedures, and system constraints within limited resources. This paper aims to introduce a scientific and practical tool for predicting surgery durations and improving OR performance for maximum benefit to patients and the hospital. Previous works used machine-learning models for surgery duration prediction based on preoperative data. The models consider covariates known to the medical staff at the time of scheduling the surgery. Given a large number of covariates, model selection becomes crucial, and the number of covariates used for prediction depends on the available sample size. Our proposed approach utilizes multi-task regression to select a common subset of predicting covariates for all tasks with the same sample size while allowing the model's coefficients to vary between them. A regression task can refer to a single surgeon or operation type or the interaction between them. By considering these diverse factors, our method provides an overall more accurate estimation of the surgery durations, and the selected covariates that enter the model may help to identify the resources required for a specific surgery. We found that when the regression tasks were surgeon-based or based on the pair of operation type and surgeon, our suggested approach outperformed the compared baseline suggested in a previous study. However, our approach failed to reach the baseline for an operation-type-based task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09791v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Azriel, Yosef Rinott, Orna Tal, Benyamine Abbou, Nadav Rappoport</dc:creator>
    </item>
    <item>
      <title>A Statistical Primer on Classical Period-Finding Techniques in Astronomy</title>
      <link>https://arxiv.org/abs/2205.10417</link>
      <description>arXiv:2205.10417v2 Announce Type: replace-cross 
Abstract: The aim of our paper is to investigate the properties of the classical phase-dispersion minimization (PDM), analysis of variance (AOV), string-length (SL), and Lomb-Scargle (LS) power statistics from a statistician's perspective. We confirm that when the data are perturbations of a constant function, i.e. under the null hypothesis of no period in the data, a scaled version of the PDM statistic follows a beta distribution, the AOV statistic follows an F distribution, and the LS power follows a chi-squared distribution with two degrees of freedom. However, the SL statistic does not have a closed-form distribution. We further verify these theoretical distributions through simulations and demonstrate that the extreme values of these statistics (over a range of trial periods), often used for period estimation and determination of the false alarm probability (FAP), follow different distributions than those derived for a single period. We emphasize that multiple-testing considerations are needed to correctly derive FAP bounds. Though, in fact, multiple-testing controls are built into the FAP bound for these extreme-value statistics, e.g. the FAP bound derived specifically for the maximum LS power statistic over a range of trial periods. Additionally, we find that all of these methods are robust to heteroscedastic noise aimed to mimic the degradation or miscalibration of an instrument over time. Finally, we examine the ability of these statistics to detect a non-constant periodic function via simulating data that mimics a well-detached binary system, and we find that the AOV statistic has the most power to detect the correct period, which agrees with what has been observed in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.10417v2</guid>
      <category>astro-ph.EP</category>
      <category>astro-ph.IM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naomi Giertych, Ahmed Shaban, Pragya Haravu, Jonathan P Williams</dc:creator>
    </item>
    <item>
      <title>Extracting the Multiscale Causal Backbone of Brain Dynamics</title>
      <link>https://arxiv.org/abs/2311.00118</link>
      <description>arXiv:2311.00118v2 Announce Type: replace-cross 
Abstract: The bulk of the research effort on brain connectivity revolves around statistical associations among brain regions, which do not directly relate to the causal mechanisms governing brain dynamics. Here we propose the multiscale causal backbone (MCB) of brain dynamics, shared by a set of individuals across multiple temporal scales, and devise a principled methodology to extract it.
  Our approach leverages recent advances in multiscale causal structure learning and optimizes the trade-off between the model fit and its complexity. Empirical assessment on synthetic data shows the superiority of our methodology over a baseline based on canonical functional connectivity networks. When applied to resting-state fMRI data, we find sparse MCBs for both the left and right brain hemispheres. Thanks to its multiscale nature, our approach shows that at low-frequency bands, causal dynamics are driven by brain regions associated with high-level cognitive functions; at higher frequencies instead, nodes related to sensory processing play a crucial role. Finally, our analysis of individual multiscale causal structures confirms the existence of a causal fingerprint of brain connectivity, thus supporting the existing extensive research in brain connectivity fingerprinting from a causal perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00118v2</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriele D'Acunto, Francesco Bonchi, Gianmarco De Francisci Morales, Giovanni Petri</dc:creator>
    </item>
    <item>
      <title>Detecting Multiple Change Points in Distributional Sequences Derived from Structural Health Monitoring Data: An Application to Bridge Damage Detection</title>
      <link>https://arxiv.org/abs/2312.12823</link>
      <description>arXiv:2312.12823v2 Announce Type: replace-cross 
Abstract: Detecting damage in critical structures using monitored data is a fundamental task of structural health monitoring, which is extremely important for maintaining structures' safety and life-cycle management. Based on statistical pattern recognition paradigm, damage detection can be conducted by assessing changes in the distribution of properly extracted damage-sensitive features (DSFs). This can be naturally formulated as a distributional change-point detection problem. A good change-point detector for damage detection should be scalable to large DSF datasets, applicable to different types of changes, and capable of controlling for false-positive indications. This study proposes a new distributional change-point detection method for damage detection to address these challenges. We embed the elements of a DSF distributional sequence into the Wasserstein space and construct a moving sum (MOSUM) multiple change-point detector based on Fr\'echet statistics and establish theoretical properties. Extensive simulation studies demonstrate the superiority of our proposed approach against other competitors to address the aforementioned practical requirements. We apply our method to the cable-tension measurements monitored from a long-span cable-stayed bridge for cable damage detection. We conduct a comprehensive change-point analysis for the extracted DSF data, and reveal interesting patterns from the detected changes, which provides valuable insights into cable system damage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12823v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Lei, Zhicheng Chen</dc:creator>
    </item>
  </channel>
</rss>
