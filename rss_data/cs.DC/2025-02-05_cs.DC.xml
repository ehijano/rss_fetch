<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Feb 2025 02:49:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Multi-Objective Framework for Optimizing GPU-Enabled VM Placement in Cloud Data Centers with Multi-Instance GPU Technology</title>
      <link>https://arxiv.org/abs/2502.01909</link>
      <description>arXiv:2502.01909v1 Announce Type: new 
Abstract: The extensive use of GPUs in cloud computing and the growing need for multitenancy have driven the development of innovative solutions for efficient GPU resource management. Multi-Instance GPU (MIG) technology from NVIDIA enables shared GPU usage in cloud data centers by providing isolated instances. However, MIG placement rules often lead to fragmentation and suboptimal resource utilization. In this work, we formally model the MIG-enabled VM placement as a multi-objective Integer Linear Programming (ILP) problem aimed at maximizing request acceptance, minimizing active hardware usage, and reducing migration overhead. Building upon this formulation, we propose GRMU, a multi-stage placement framework designed to address MIG placement challenges. GRMU performs intra-GPU migrations for defragmentation of a single GPU and inter-GPU migrations for consolidation and resource efficiency. It also employs a quota-based partitioning approach to allocate GPUs into two distinct baskets: one for large-profile workloads and another for smaller-profile workloads. Each basket has predefined capacity limits, ensuring fair resource distribution and preventing large-profile workloads from monopolizing resources. Evaluations on a real-world Alibaba GPU cluster trace reveal that GRMU improves acceptance rates by 22%, reduces active hardware by 17%, and incurs migration for only 1% of MIG-enabled VMs, demonstrating its effectiveness in minimizing fragmentation and improving resource utilization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01909v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmad Siavashi, Mahmoud Momtazpour</dc:creator>
    </item>
    <item>
      <title>Evaluating Fault Tolerance and Scalability in Distributed File Systems: A Case Study of GFS, HDFS, and MinIO</title>
      <link>https://arxiv.org/abs/2502.01981</link>
      <description>arXiv:2502.01981v1 Announce Type: new 
Abstract: Distributed File Systems (DFS) are essential for managing vast datasets across multiple servers, offering benefits in scalability, fault tolerance, and data accessibility. This paper presents a comprehensive evaluation of three prominent DFSs - Google File System (GFS), Hadoop Distributed File System (HDFS), and MinIO - focusing on their fault tolerance mechanisms and scalability under varying data loads and client demands. Through detailed analysis, how these systems handle data redundancy, server failures, and client access protocols, ensuring reliability in dynamic, large-scale environments is assessed. In addition, the impact of system design on performance, particularly in distributed cloud and computing architectures is assessed. By comparing the strengths and limitations of each DFS, the paper provides practical insights for selecting the most appropriate system for different enterprise needs, from high availability storage to big data analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01981v1</guid>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.PF</category>
      <category>cs.SE</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shubham Malhotra</dc:creator>
    </item>
    <item>
      <title>Optimal Broadcast on Congested Random Graphs</title>
      <link>https://arxiv.org/abs/2502.02165</link>
      <description>arXiv:2502.02165v1 Announce Type: new 
Abstract: We study the problem of broadcasting multiple messages in the CONGEST model. In this problem, a dedicated node $s$ possesses a set $M$ of messages with every message being of the size $O(\log n)$ where $n$ is the total number of nodes. The objective is to ensure that every node in the network learns all messages in $M$. The execution of an algorithm progresses in rounds and we focus on optimizing the round complexity of broadcasting multiple messages.
  Our primary contribution is a randomized algorithm designed for networks modeled as random graphs. The algorithm succeeds with high probability and achieves round complexity that is optimal up to a polylogarithmic factor. It leverages a multi-COBRA primitive, which uses multiple branching random walks running in parallel. To the best of our knowledge, this approach has not been applied in distributed algorithms before. A crucial aspect of our method is the use of these branching random walks to construct an optimal (up to a polylogarithmic factor) tree packing of a random graph, which is then used for efficient broadcasting. This result is of independent interest.
  We also prove the problem to be NP-hard in a centralized setting and provide insights into why straightforward lower bounds, namely graph diameter and $\frac{|M|}{minCut}$, can not be tight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02165v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anton Paramonov, Roger Wattenhofer</dc:creator>
    </item>
    <item>
      <title>Efficient Extensions for Asynchronous Byzantine Agreement via Weak Agreement</title>
      <link>https://arxiv.org/abs/2502.02320</link>
      <description>arXiv:2502.02320v1 Announce Type: new 
Abstract: We consider an asynchronous network of $n$ parties, up to $t$ of which are byzantine, and against $t &lt; \frac{n}{3}$ failures reduce byzantine agreement (BA) on $\ell$-bit inputs to one instance of weak agreement (WA) on $\ell$-bit inputs, one instance of binary BA and $\Theta(\ell n + n^2)$ bits of additional communication. Then, to use in this reduction, we design two information-theoretic WA protocols. In the first one, we use almost-universal hashing to achieve statistical security except with probability $2^{-\lambda}$ against $t &lt; \frac{n}{3}$ failures with $\Theta(n^2(\lambda + \log n + \log \ell)) = \mathcal{O}(\ell n + n^2(\lambda + \log n))$ bits of communication. Then, we modify our first protocol by replacing the hashes with error correcting code symbols and introducing a preliminary step based on the COOL protocol for synchronous multivalued BA [DISC '21] to achieve perfect security against $t \leq \frac{n}{3 + \varepsilon}$ failures with $\Theta(\ell n + n^2)$ bits of communication. Our WA protocols both lead to state-of-the-art information-theoretic extensions from binary BA to multivalued BA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02320v1</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mose Mizrahi Erbes, Roger Wattenhofer</dc:creator>
    </item>
    <item>
      <title>An inherently parallel H2-ULV factorization for solving dense linear systems on GPUs</title>
      <link>https://arxiv.org/abs/2502.02395</link>
      <description>arXiv:2502.02395v1 Announce Type: new 
Abstract: Hierarchical low-rank approximation of dense matrices can reduce the complexity of their factorization from O(N^3) to O(N). However, the complex structure of such hierarchical matrices makes them difficult to parallelize. The block size and ranks can vary between the sub-blocks, which creates load imbalance. The dependency between the sub-blocks during factorization results in serialization. Since many sub-blocks are low-rank, their small computational load exposes the overhead of runtime systems. The combination of these factors makes it challenging to implement these methods on GPUs. In this work, we show that dense matrices can be factorized with linear complexity, while extracting the potential parallelism of GPUs. This is made possible through the H2-ULV factorization, which removes the dependency on trailing sub-matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02395v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1177/10943420241242021</arxiv:DOI>
      <arxiv:journal_reference>The International Journal of High Performance Computing Applications, Volume 38, Issue 4, 2024</arxiv:journal_reference>
      <dc:creator>Qianxiang Ma, Rio Yokota</dc:creator>
    </item>
    <item>
      <title>H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed Criticality Systems</title>
      <link>https://arxiv.org/abs/2502.02437</link>
      <description>arXiv:2502.02437v1 Announce Type: new 
Abstract: Recent advancements in fields such as automotive and aerospace have driven a growing demand for robust computational resources. Applications that were once designed for basic MCUs are now deployed on highly heterogeneous SoC platforms. While these platforms deliver the necessary computational performance, they also present challenges related to resource sharing and predictability. These challenges are particularly pronounced when consolidating safety and non-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to adhere to strict SWaP-C requirements. MCS consolidation on shared platforms requires stringent spatial and temporal isolation to comply with functional safety standards. Virtualization, mainly leveraged by hypervisors, is a key technology that ensures spatial isolation across multiple OSes and applications; however, ensuring temporal isolation remains challenging due to contention on shared hardwar resources, which impacts real-time performance and predictability. To mitigate this problem, several strategies as cache coloring and memory bandwidth reservation have been proposed. Although cache coloring is typically implemented on state-of-the-art hypervisors, memory bandwidth reservation approaches are commonly implemented at the Linux kernel level or rely on dedicated hardware and typically do not consider the concept of VMs that can run different OSes. To fill the gap between current memory bandwidth reservation solutions and the deployment of MCSs that operate on a hypervisor, this work introduces H-MBR, an open-source VM-centric memory bandwidth reservation mechanism. H-MBR features (i) VM-centric bandwidth reservation, (ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results evidenced no overhead on non-regulated workloads, and negligible overhead (&lt;1%) for regulated workloads for regulation periods of 2 us or higher.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02437v1</guid>
      <category>cs.DC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Afonso Oliveira, Diogo Costa, Gon\c{c}alo Moreira, Jos\'e Martins, Sandro Pinto</dc:creator>
    </item>
    <item>
      <title>Hecate: Unlocking Efficient Sparse Model Training via Fully Sharded Sparse Data Parallelism</title>
      <link>https://arxiv.org/abs/2502.02581</link>
      <description>arXiv:2502.02581v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) has emerged as a promising sparse paradigm for scaling up pre-trained models (PTMs) with remarkable cost-effectiveness. However, the dynamic nature of MoE leads to rapid fluctuations and imbalances in expert loads during training, resulting in significant straggler effects that hinder training performance when using expert parallelism (EP). Existing MoE training systems attempt to mitigate these effects through expert rearrangement strategies, but they face challenges in terms of memory efficiency and timeliness of rearrangement. This paper proposes Fully Sharded Sparse Data Parallelism (FSSDP), an innovative approach that tackles the parallelization of MoE layers and potential straggler effects caused by imbalanced expert loads from a new perspective. FSSDP fully shards the parameters and optimizer states of MoE layers across devices and sparsely materializes MoE parameters from scratch in each iteration with two sparse collectives SparseAllGather and SparseReduceScatter. We build Hecate, a high-performance MoE training system that incorporates FSSDP to fully unlock its potential. Hecate introduces heterogeneous sharding, sparse materialization, and re-materialization techniques to construct flexible and efficient expert placements with low memory and communication overhead. Our evaluation reveals that Hecate achieves up to 3.54x speedup compared over state-of-the-art MoE training systems and consistently demonstrates improvements across model architectures and hardware environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02581v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhao Qing, Guichao Zhu, Fanxin Li, Lintian Lei, Zekai Sun, Xiuxian Guan, Shixiong Zhao, Xusheng Chen, Dong Huang, Sen Wang, Heming Cui</dc:creator>
    </item>
    <item>
      <title>How to Build a Quantum Supercomputer: Scaling from Hundreds to Millions of Qubits</title>
      <link>https://arxiv.org/abs/2411.10406</link>
      <description>arXiv:2411.10406v2 Announce Type: cross 
Abstract: In the span of four decades, quantum computation has evolved from an intellectual curiosity to a potentially realizable technology. Today, small-scale demonstrations have become possible for quantum algorithmic primitives on hundreds of physical qubits and proof-of-principle error-correction on a single logical qubit. Nevertheless, despite significant progress and excitement, the path toward a full-stack scalable technology is largely unknown. There are significant outstanding quantum hardware, fabrication, software architecture, and algorithmic challenges that are either unresolved or overlooked. These issues could seriously undermine the arrival of utility-scale quantum computers for the foreseeable future. Here, we provide a comprehensive review of these scaling challenges. We show how the road to scaling could be paved by adopting existing semiconductor technology to build much higher-quality qubits, employing system engineering approaches, and performing distributed quantum computation within heterogeneous high-performance computing infrastructures. These opportunities for research and development could unlock certain promising applications, in particular, efficient quantum simulation/learning of quantum data generated by natural or engineered quantum systems. To estimate the true cost of such promises, we provide a detailed resource and sensitivity analysis for classically hard quantum chemistry calculations on surface-code error-corrected quantum computers given current, target, and desired hardware specifications based on superconducting qubits, accounting for a realistic distribution of errors. Furthermore, we argue that, to tackle industry-scale classical optimization and machine learning problems in a cost-effective manner, heterogeneous quantum-probabilistic computing with custom-designed accelerators should be considered as a complementary path toward scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10406v2</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masoud Mohseni, Artur Scherer, K. Grace Johnson, Oded Wertheim, Matthew Otten, Navid Anjum Aadit, Yuri Alexeev, Kirk M. Bresniker, Kerem Y. Camsari, Barbara Chapman, Soumitra Chatterjee, Gebremedhin A. Dagnew, Aniello Esposito, Farah Fahim, Marco Fiorentino, Archit Gajjar, Abdullah Khalid, Xiangzhou Kong, Bohdan Kulchytskyy, Elica Kyoseva, Ruoyu Li, P. Aaron Lott, Igor L. Markov, Robert F. McDermott, Giacomo Pedretti, Pooja Rao, Eleanor Rieffel, Allyson Silva, John Sorebo, Panagiotis Spentzouris, Ziv Steiner, Boyan Torosov, Davide Venturelli, Robert J. Visser, Zak Webb, Xin Zhan, Yonatan Cohen, Pooya Ronagh, Alan Ho, Raymond G. Beausoleil, John M. Martinis</dc:creator>
    </item>
    <item>
      <title>Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques</title>
      <link>https://arxiv.org/abs/2502.01659</link>
      <description>arXiv:2502.01659v1 Announce Type: cross 
Abstract: Transformers have demonstrated great success in numerous domains including natural language processing and bioinformatics. This success stems from the use of the attention mechanism by these models in order to represent and propagate pairwise interactions between individual tokens of sequential data. However, the primary limitation of this operation is its quadratic memory and time complexity in relation to the input's context length - the length of a sequence over which the interactions need to be captured. This significantly limits the length of sequences that can be inferred upon by these models. Extensive research has been conducted to reduce the number of pairwise interactions to sub-quadratic in relation to the context length by introducing sparsity into the attention mechanism through the development of sparse attention masks. However, efficient implementations that achieve "true sparsity" are lacking.
  In this work, we address this issue by proposing a graph computing view of attention where tokens are perceived as nodes of the graph and the attention mask determines the edges of the graph. Using this view, we develop graph processing algorithms to implement the attention mechanism. Both theoretically and empirically, we demonstrate that our algorithms only perform the needed computations, i.e., they are work optimal. We also perform extensive experimentation using popular attention masks to explore the impact of sparsity on execution time and achievable context length. Our experiments demonstrate significant speedups in execution times compared to state-of-the-art attention implementations such as FlashAttention for large sequence lengths. We also demonstrate that our algorithms are able to achieve extremely long sequence lengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB).</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01659v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathaniel Tomczak, Sanmukh Kuppannagari</dc:creator>
    </item>
    <item>
      <title>Optimizing Spot Instance Reliability and Security Using Cloud-Native Data and Tools</title>
      <link>https://arxiv.org/abs/2502.01966</link>
      <description>arXiv:2502.01966v1 Announce Type: cross 
Abstract: This paper represents "Cloudlab", a comprehensive, cloud - native laboratory designed to support network security research and training. Built on Google Cloud and adhering to GitOps methodologies, Cloudlab facilitates the the creation, testing, and deployment of secure, containerized workloads using Kubernetes and serverless architectures. The lab integrates tools like Palo Alto Networks firewalls, Bridgecrew for "Security as Code," and automated GitHub workflows to establish a robust Continuous Integration/Continuous Machine Learning pipeline. By providing an adaptive and scalable environment, Cloudlab supports advanced security concepts such as role-based access control, Policy as Code, and container security. This initiative enables data scientists and engineers to explore cutting-edge practices in a dynamic cloud-native ecosystem, fostering innovation and improving operational resilience in modern IT infrastructures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01966v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.SE</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shubham Malhotra</dc:creator>
    </item>
    <item>
      <title>Ilargi: a GPU Compatible Factorized ML Model Training Framework</title>
      <link>https://arxiv.org/abs/2502.01985</link>
      <description>arXiv:2502.01985v1 Announce Type: cross 
Abstract: The machine learning (ML) training over disparate data sources traditionally involves materialization, which can impose substantial time and space overhead due to data movement and replication. Factorized learning, which leverages direct computation on disparate sources through linear algebra (LA) rewriting, has emerged as a viable alternative to improve computational efficiency. However, the adaptation of factorized learning to leverage the full capabilities of modern LA-friendly hardware like GPUs has been limited, often requiring manual intervention for algorithm compatibility. This paper introduces Ilargi, a novel factorized learning framework that utilizes matrix-represented data integration (DI) metadata to facilitate automatic factorization across CPU and GPU environments without the need for costly relational joins. Ilargi incorporates an ML-based cost estimator to intelligently selects between factorization and materialization based on data properties, algorithm complexity, hardware environments, and their interactions. This strategy ensures up to 8.9x speedups on GPUs and achieves over 20% acceleration in batch ML training workloads, thereby enhancing the practicability of ML training across diverse data integration scenarios and hardware platforms. To our knowledge, this work is the very first effort in GPU-compatible factorized learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01985v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wenbo Sun, Rihan Hai</dc:creator>
    </item>
    <item>
      <title>SMTFL: Secure Model Training to Untrusted Participants in Federated Learning</title>
      <link>https://arxiv.org/abs/2502.02038</link>
      <description>arXiv:2502.02038v1 Announce Type: cross 
Abstract: Federated learning is an essential distributed model training technique. However, threats such as gradient inversion attacks and poisoning attacks pose significant risks to the privacy of training data and the model correctness. We propose a novel approach called SMTFL to achieve secure model training in federated learning without relying on trusted participants. To safeguard gradients privacy against gradient inversion attacks, clients are dynamically grouped, allowing one client's gradient to be divided to obfuscate the gradients of other clients within the group. This method incorporates checks and balances to reduce the collusion for inferring specific client data. To detect poisoning attacks from malicious clients, we assess the impact of aggregated gradients on the global model's performance, enabling effective identification and exclusion of malicious clients. Each client's gradients are encrypted and stored, with decryption collectively managed by all clients. The detected poisoning gradients are invalidated from the global model through a unlearning method. To our best knowledge, we present the first practical secure aggregation scheme, which does not require trusted participants, avoids the performance degradation associated with traditional noise-injection, and aviods complex cryptographic operations during gradient aggregation. Evaluation results are encouraging based on four datasets and two models: SMTFL is effective against poisoning attacks and gradient inversion attacks, achieving an accuracy rate of over 95% in locating malicious clients, while keeping the false positive rate for honest clients within 5%. The model accuracy is also nearly restored to its pre-attack state when SMTFL is deployed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02038v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhihui Zhao, Xiaorong Dong, Yimo Ren, Jianhua Wang, Dan Yu, Hongsong Zhu, Yongle Chen</dc:creator>
    </item>
    <item>
      <title>Comparative Analysis of FPGA and GPU Performance for Machine Learning-Based Track Reconstruction at LHCb</title>
      <link>https://arxiv.org/abs/2502.02304</link>
      <description>arXiv:2502.02304v1 Announce Type: cross 
Abstract: In high-energy physics, the increasing luminosity and detector granularity at the Large Hadron Collider are driving the need for more efficient data processing solutions. Machine Learning has emerged as a promising tool for reconstructing charged particle tracks, due to its potentially linear computational scaling with detector hits. The recent implementation of a graph neural network-based track reconstruction pipeline in the first level trigger of the LHCb experiment on GPUs serves as a platform for comparative studies between computational architectures in the context of high-energy physics. This paper presents a novel comparison of the throughput of ML model inference between FPGAs and GPUs, focusing on the first step of the track reconstruction pipeline$\unicode{x2013}$an implementation of a multilayer perceptron. Using HLS4ML for FPGA deployment, we benchmark its performance against the GPU implementation and demonstrate the potential of FPGAs for high-throughput, low-latency inference without the need for an expertise in FPGA development and while consuming significantly less power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02304v1</guid>
      <category>hep-ex</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>physics.ins-det</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fotis I. Giasemis, Vladimir Lon\v{c}ar, Bertrand Granado, Vladimir Vava Gligorov</dc:creator>
    </item>
    <item>
      <title>Random Adaptive Cache Placement Policy</title>
      <link>https://arxiv.org/abs/2502.02349</link>
      <description>arXiv:2502.02349v1 Announce Type: cross 
Abstract: This paper presents a new hybrid cache replacement algorithm that combines random allocation with a modified V-Way cache implementation. Our RAC adapts to complex cache access patterns and optimizes cache usage by improving the utilization of cache sets, unlike traditional cache policies. The algorithm utilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic allocation and flexible tag management. RAC extends the V-Way cache design and its variants by optimizing tag and data storage for enhanced efficiency.
  We evaluated the algorithm using the ChampSim simulator with four diverse benchmark traces and observed significant improvements in cache hit rates up to 80.82% hit rate. Although the improvements in the instructions per cycle (IPC) were moderate, our findings emphasize the algorithm's potential to enhance cache utilization and reduce memory access times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02349v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.OS</category>
      <category>cs.PF</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vrushank Ahire, Pranav Menon, Aniruddh Muley, Abhinandan S. Prasad</dc:creator>
    </item>
    <item>
      <title>FPGA Innovation Research in the Netherlands: Present Landscape and Future Outlook</title>
      <link>https://arxiv.org/abs/2502.02404</link>
      <description>arXiv:2502.02404v1 Announce Type: cross 
Abstract: FPGAs have transformed digital design by enabling versatile and customizable solutions that balance performance and power efficiency, yielding them essential for today's diverse computing challenges. Research in the Netherlands, both in academia and industry, plays a major role in developing new innovative FPGA solutions. This survey presents the current landscape of FPGA innovation research in the Netherlands by delving into ongoing projects, advancements, and breakthroughs in the field. Focusing on recent research outcome (within the past 5 years), we have identified five key research areas: a) FPGA architecture, b) FPGA robustness, c) data center infrastructure and high-performance computing, d) programming models and tools, and e) applications. This survey provides in-depth insights beyond a mere snapshot of the current innovation research landscape by highlighting future research directions within each key area; these insights can serve as a foundational resource to inform potential national-level investments in FPGA technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02404v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikolaos Alachiotis, Sjoerd van den Belt, Steven van der Vlugt, Reinier van der Walle, Mohsen Safari, Bruno Endres Forlin, Tiziano De Matteis, Zaid Al-Ars, Roel Jordans, Ant\'onio J. Sousa de Almeida, Federico Corradi, Christiaan Baaij, Ana-Lucia Varbanescu</dc:creator>
    </item>
    <item>
      <title>LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models</title>
      <link>https://arxiv.org/abs/2502.02406</link>
      <description>arXiv:2502.02406v1 Announce Type: cross 
Abstract: Cross-attention is commonly adopted in multimodal large language models (MLLMs) for integrating visual information into the language backbone. However, in applications with large visual inputs, such as video understanding, processing a large number of visual tokens in cross-attention layers leads to high memory demands and often necessitates distributed computation across multiple GPUs. Existing distributed attention mechanisms face significant communication overheads, making cross-attention layers a critical bottleneck for efficient training and inference of MLLMs. To address this, we propose LV-XAttn, a distributed, exact cross-attention mechanism with minimal communication overhead. We observe that in applications involving large visual inputs the size of the query block is typically much smaller than that of the key-value blocks. Thus, in LV-XAttn we keep the large key-value blocks locally on each GPU and exchange smaller query blocks across GPUs. We also introduce an efficient activation recomputation technique enabling support for longer visual context. We theoretically analyze the communication benefits of LV-XAttn and show that it can achieve speedups for a wide range of models. Our evaluations with mPLUG-Owl3 and OpenFlamingo models find that LV-XAttn achieves up to 5.58$\times$ end-to-end speedup compared to existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02406v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tzu-Tao Chang, Shivaram Venkataraman</dc:creator>
    </item>
    <item>
      <title>A subquadratic certification scheme for P5-free graphs</title>
      <link>https://arxiv.org/abs/2410.14658</link>
      <description>arXiv:2410.14658v2 Announce Type: replace 
Abstract: In local certification, vertices of a $n$-vertex graph perform a local verification to check if a given property is satisfied by the graph. This verification is performed thanks to certificates, which are pieces of information that are given to the vertices. In this work, we focus on the local certification of $P_5$-freeness, and we prove a $O(n^{3/2})$ upper bound on the size of the certificates, which is (to our knowledge) the first subquadratic upper bound for this property.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14658v2</guid>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Bousquet, S\'ebastien Zeitoun</dc:creator>
    </item>
    <item>
      <title>Scaling Large Language Model Training on Frontier with Low-Bandwidth Partitioning</title>
      <link>https://arxiv.org/abs/2501.04266</link>
      <description>arXiv:2501.04266v2 Announce Type: replace 
Abstract: Scaling up Large Language Model(LLM) training involves fitting a tremendous amount of training parameters across a limited number of workers. However, methods like ZeRO-3 that drastically reduce GPU memory pressure often incur heavy communication to ensure global synchronization and consistency. Established efforts such as ZeRO++ use secondary partitions to avoid inter-node communications, given that intra-node GPU-GPU transfer generally has more bandwidth and lower latency than inter-node connections. However, as more capable infrastructure like Frontier, equipped with AMD GPUs, emerged with impressive computing capability, there is a need for investigations on the hardware topology and to develop targeted strategies to improve training efficiency. In this work, we propose a collection of communication and optimization strategies for ZeRO++ to reduce communication costs and improve memory utilization. In this paper, we propose a 3-level hierarchical partitioning specifically for the current 2nd ranked supercomputing cluster, Frontier, which aims at leveraging various bandwidths across layers of communications (GCD-GCD, GPU-GPU, and inter-node) to reduce communication overhead. For a 20B GPT model, we observe a 1.71x increase in TFLOPS per GPU when compared with ZeRO++ up to 384 GCDs and a scaling efficiency of 0.94 for up to 384 GCDs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04266v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lang Xu (DK), Quentin Anthony (DK), Jacob Hatef (DK), Aamir Shafi (DK), Hari Subramoni (DK), Dhabaleswar K. (DK),  Panda</dc:creator>
    </item>
    <item>
      <title>WaterWise: Co-optimizing Carbon- and Water-Footprint Toward Environmentally Sustainable Cloud Computing</title>
      <link>https://arxiv.org/abs/2501.17944</link>
      <description>arXiv:2501.17944v2 Announce Type: replace 
Abstract: The carbon and water footprint of large-scale computing systems poses serious environmental sustainability risks. In this study, we discover that, unfortunately, carbon and water sustainability are at odds with each other - and, optimizing one alone hurts the other. Toward that goal, we introduce, WaterWise, a novel job scheduler for parallel workloads that intelligently co-optimizes carbon and water footprint to improve the sustainability of geographically distributed data centers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17944v2</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yankai Jiang, Rohan Basu Roy, Raghavendra Kanakagiri, Devesh Tiwari</dc:creator>
    </item>
    <item>
      <title>Decentralized Federated Learning with Model Caching on Mobile Agents</title>
      <link>https://arxiv.org/abs/2408.14001</link>
      <description>arXiv:2408.14001v2 Announce Type: replace-cross 
Abstract: Federated Learning (FL) trains a shared model using data and computation power on distributed agents coordinated by a central server. Decentralized FL (DFL) utilizes local model exchange and aggregation between agents to reduce the communication and computation overheads on the central server. However, when agents are mobile, the communication opportunity between agents can be sporadic, largely hindering the convergence and accuracy of DFL. In this paper, we propose Cached Decentralized Federated Learning (Cached-DFL) to investigate delay-tolerant model spreading and aggregation enabled by model caching on mobile agents. Each agent stores not only its own model, but also models of agents encountered in the recent past. When two agents meet, they exchange their own models as well as the cached models. Local model aggregation utilizes all models stored in the cache. We theoretically analyze the convergence of Cached-DFL, explicitly taking into account the model staleness introduced by caching. We design and compare different model caching algorithms for different DFL and mobility scenarios. We conduct detailed case studies in a vehicular network to systematically investigate the interplay between agent mobility, cache staleness, and model convergence. In our experiments, Cached-DFL converges quickly, and significantly outperforms DFL without caching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14001v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyu Wang, Guojun Xiong, Houwei Cao, Jian Li, Yong Liu</dc:creator>
    </item>
  </channel>
</rss>
