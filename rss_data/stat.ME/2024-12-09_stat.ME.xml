<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 09 Dec 2024 05:00:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Robust Quickest Change Detection in Multi-Stream Non-Stationary Processes</title>
      <link>https://arxiv.org/abs/2412.04493</link>
      <description>arXiv:2412.04493v1 Announce Type: new 
Abstract: The problem of robust quickest change detection (QCD) in non-stationary processes under a multi-stream setting is studied. In classical QCD theory, optimal solutions are developed to detect a sudden change in the distribution of stationary data. Most studies have focused on single-stream data. In non-stationary processes, the data distribution both before and after change varies with time and is not precisely known. The multi-dimension data even complicates such issues. It is shown that if the non-stationary family for each dimension or stream has a least favorable law (LFL) or distribution in a well-defined sense, then the algorithm designed using the LFLs is robust optimal. The notion of LFL defined in this work differs from the classical definitions due to the dependence of the post-change model on the change point. Examples of multi-stream non-stationary processes encountered in public health monitoring and aviation applications are provided. Our robust algorithm is applied to simulated and real data to show its effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04493v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingze Hou, Hoda Bidkhori, Taposh Banerjee</dc:creator>
    </item>
    <item>
      <title>Multi-Quantile Estimators for the parameters of Generalized Extreme Value distribution</title>
      <link>https://arxiv.org/abs/2412.04640</link>
      <description>arXiv:2412.04640v1 Announce Type: new 
Abstract: We introduce and study Multi-Quantile estimators for the parameters $( \xi, \sigma, \mu)$ of Generalized Extreme Value (GEV) distributions to provide a robust approach to extreme value modeling. Unlike classical estimators, such as the Maximum Likelihood Estimation (MLE) estimator and the Probability Weighted Moments (PWM) estimator, which impose strict constraints on the shape parameter $\xi$, our estimators are always asymptotically normal and consistent across all values of the GEV parameters. The asymptotic variances of our estimators decrease with the number of quantiles increasing and can approach the Cram\'er-Rao lower bound very closely whenever it exists. Our Multi-Quantile Estimators thus offer a more flexible and efficient alternative for practical applications. We also discuss how they can be implemented in the context of Block Maxima method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04640v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sen Lin, Ao Kong, Robert Azencott</dc:creator>
    </item>
    <item>
      <title>Fairness-aware Principal Component Analysis for Mortality Forecasting and Annuity Pricing</title>
      <link>https://arxiv.org/abs/2412.04663</link>
      <description>arXiv:2412.04663v1 Announce Type: new 
Abstract: Fairness-aware statistical learning is critical for data-driven decision-making to mitigate discrimination against protected attributes, such as gender, race, and ethnicity. This is especially important for high-stake decision-making, such as insurance underwriting and annuity pricing. This paper proposes a new fairness-regularized principal component analysis - Fair PCA, in the context of high-dimensional factor models. An efficient gradient descent algorithm is constructed with adaptive selection criteria for hyperparameter tuning. The Fair PCA is applied to mortality modelling to mitigate gender discrimination in annuity pricing. The model performance has been validated through both simulation studies and empirical data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04663v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fei Huang, Junhao Shen, Yanrong Yang, Ran Zhao</dc:creator>
    </item>
    <item>
      <title>Modeling High-Dimensional Dependent Data in the Presence of Many Explanatory Variables and Weak Signals</title>
      <link>https://arxiv.org/abs/2412.04736</link>
      <description>arXiv:2412.04736v1 Announce Type: new 
Abstract: This article considers a novel and widely applicable approach to modeling high-dimensional dependent data when a large number of explanatory variables are available and the signal-to-noise ratio is low. We postulate that a $p$-dimensional response series is the sum of a linear regression with many observable explanatory variables and an error term driven by some latent common factors and an idiosyncratic noise. The common factors have dynamic dependence whereas the covariance matrix of the idiosyncratic noise can have diverging eigenvalues to handle the situation of low signal-to-noise ratio commonly encountered in applications. The regression coefficient matrix is estimated using penalized methods when the dimensions involved are high. We apply factor modeling to the regression residuals, employ a high-dimensional white noise testing procedure to determine the number of common factors, and adopt a projected Principal Component Analysis when the signal-to-noise ratio is low. We establish asymptotic properties of the proposed method, both for fixed and diverging numbers of regressors, as $p$ and the sample size $T$ approach infinity. Finally, we use simulations and empirical applications to demonstrate the efficacy of the proposed approach in finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04736v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaoxing Gao, Ruey S. Tsay</dc:creator>
    </item>
    <item>
      <title>Marginally interpretable spatial logistic regression with bridge processes</title>
      <link>https://arxiv.org/abs/2412.04744</link>
      <description>arXiv:2412.04744v1 Announce Type: new 
Abstract: In including random effects to account for dependent observations, the odds ratio interpretation of logistic regression coefficients is changed from population-averaged to subject-specific. This is unappealing in many applications, motivating a rich literature on methods that maintain the marginal logistic regression structure without random effects, such as generalized estimating equations. However, for spatial data, random effect approaches are appealing in providing a full probabilistic characterization of the data that can be used for prediction. We propose a new class of spatial logistic regression models that maintain both population-averaged and subject-specific interpretations through a novel class of bridge processes for spatial random effects. These processes are shown to have appealing computational and theoretical properties, including a scale mixture of normal representation. The new methodology is illustrated with simulations and an analysis of childhood malaria prevalence data in the Gambia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04744v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changwoo J. Lee, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Robust and Optimal Tensor Estimation via Robust Gradient Descent</title>
      <link>https://arxiv.org/abs/2412.04773</link>
      <description>arXiv:2412.04773v1 Announce Type: new 
Abstract: Low-rank tensor models are widely used in statistics and machine learning. However, most existing methods rely heavily on the assumption that data follows a sub-Gaussian distribution. To address the challenges associated with heavy-tailed distributions encountered in real-world applications, we propose a novel robust estimation procedure based on truncated gradient descent for general low-rank tensor models. We establish the computational convergence of the proposed method and derive optimal statistical rates under heavy-tailed distributional settings of both covariates and noise for various low-rank models. Notably, the statistical error rates are governed by a local moment condition, which captures the distributional properties of tensor variables projected onto certain low-dimensional local regions. Furthermore, we present numerical results to demonstrate the effectiveness of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04773v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoyu Zhang, Di Wang, Guodong Li, Defeng Sun</dc:creator>
    </item>
    <item>
      <title>Regression Analysis of Cure Rate Models with Competing Risks Subjected to Interval Censoring</title>
      <link>https://arxiv.org/abs/2412.04803</link>
      <description>arXiv:2412.04803v1 Announce Type: new 
Abstract: In this work, we present two defective regression models for the analysis of interval-censored competing risk data in the presence of cured individuals, viz., defective Gompertz and defective inverse Gaussian regression models. The proposed models enable us to estimate the cure fraction directly from the model. Simultaneously, we estimate the regression parameters corresponding to each cause of failure using the method of maximum likelihood. The finite sample behaviour of the proposed models is evaluated through Monte Carlo simulation studies. We illustrate the practical applicability of the models using a real-life data set on HIV patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04803v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Silpa K., Sreedevi E. P., P. G. Sankaran</dc:creator>
    </item>
    <item>
      <title>Fast Estimation of the Composite Link Model for Multidimensional Grouped Counts</title>
      <link>https://arxiv.org/abs/2412.04956</link>
      <description>arXiv:2412.04956v1 Announce Type: new 
Abstract: This paper presents a significant advancement in the estimation of the Composite Link Model within a penalized likelihood framework, specifically designed to address indirect observations of grouped count data. While the model is effective in these contexts, its application becomes computationally challenging in large, high-dimensional settings. To overcome this, we propose a reformulated iterative estimation procedure that leverages Generalized Linear Array Models, enabling the disaggregation and smooth estimation of latent distributions in multidimensional data. Through applications to high-dimensional mortality datasets, we demonstrate the model's capability to capture fine-grained patterns while comparing its computational performance to the conventional algorithm. The proposed methodology offers notable improvements in computational speed, storage efficiency, and practical applicability, making it suitable for a wide range of fields where high-dimensional data are provided in grouped formats.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04956v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Carlo G. Camarda, Mar\'ia Durb\'an</dc:creator>
    </item>
    <item>
      <title>Application of generalized linear models in big data: a divide and recombine (D&amp;R) approach</title>
      <link>https://arxiv.org/abs/2412.05018</link>
      <description>arXiv:2412.05018v1 Announce Type: new 
Abstract: D&amp;R is a statistical approach designed to handle large and complex datasets. It partitions the dataset into several manageable subsets and subsequently applies the analytic method to each subset independently to obtain results. Finally, the results from each subset are combined to yield the results for the entire dataset. D&amp;R strategies can be implemented to fit GLMs to datasets too large for conventional methods. Several D&amp;R strategies are available for different GLMs, some of which are theoretically justified but lack practical validation. A significant limitation is the theoretical and practical justification for estimating combined standard errors and confidence intervals. This paper reviews D&amp;R strategies for GLMs and proposes a method to determine the combined standard error for D&amp;R-based estimators. In addition to the traditional dataset division procedures, we propose a different division method named sequential partitioning for D&amp;R-based estimators on GLMs. We show that the obtained D&amp;R estimator with the proposed standard error attains equivalent efficiency as the full data estimate. We illustrate this on a large synthetic dataset and verify that the results from D&amp;R are accurate and identical to those from other available R packages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05018v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md. Mahadi Hassan Nayem, Soma Chowdhury Biswas</dc:creator>
    </item>
    <item>
      <title>moonboot: An R Package Implementing m-out-of-n Bootstrap Methods</title>
      <link>https://arxiv.org/abs/2412.05032</link>
      <description>arXiv:2412.05032v1 Announce Type: new 
Abstract: The m-out-of-n bootstrap is a possible workaround to compute confidence intervals for bootstrap inconsistent estimators, because it works under weaker conditions than the n-out-of-n bootstrap. It has the disadvantage, however, that it requires knowledge of an appropriate scaling factor {\tau}n and that the coverage probability for finite n depends on the choice of m. This article presents an R package moonboot which implements the computation of m-out-of-n bootstrap confidence intervals and provides functions for estimating the parameters {\tau}n and m. By means of Monte Carlo simulations, we evaluate the different methods and compare them for different estimators</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05032v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christoph Dalitz, Felix L\"ogler</dc:creator>
    </item>
    <item>
      <title>Piecewise-linear modeling of multivariate geometric extremes</title>
      <link>https://arxiv.org/abs/2412.05195</link>
      <description>arXiv:2412.05195v1 Announce Type: new 
Abstract: A recent development in extreme value modeling uses the geometry of the dataset to perform inference on the multivariate tail. A key quantity in this inference is the gauge function, whose values define this geometry. Methodology proposed to date for capturing the gauge function either lacks flexibility due to parametric specifications, or relies on complex neural network specifications in dimensions greater than three. We propose a semiparametric gauge function that is piecewise-linear, making it simple to interpret and provides a good approximation for the true underlying gauge function. This linearity also makes optimization tasks computationally inexpensive. The piecewise-linear gauge function can be used to define both a radial and an angular model, allowing for the joint fitting of extremal pseudo-polar coordinates, a key aspect of this geometric framework. We further expand the toolkit for geometric extremal modeling through the estimation of high radial quantiles at given angular values via kernel density estimation. We apply the new methodology to air pollution data, which exhibits a complex extremal dependence structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05195v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ryan Campbell, Jennifer Wadsworth</dc:creator>
    </item>
    <item>
      <title>Energy Based Equality of Distributions Testing for Compositional Data</title>
      <link>https://arxiv.org/abs/2412.05199</link>
      <description>arXiv:2412.05199v1 Announce Type: new 
Abstract: Not many tests exist for testing the equality for two or more multivariate distributions with compositional data, perhaps due to their constrained sample space. At the moment, there is only one test suggested that relies upon random projections. We propose a novel test termed {\alpha}-Energy Based Test ({\alpha}-EBT) to compare the multivariate distributions of two (or more) compositional data sets. Similar to the aforementioned test, the new test makes no parametric assumptions about the data and, based on simulation studies it exhibits higher power levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05199v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Volkan Sevinc, Michail Tsagris</dc:creator>
    </item>
    <item>
      <title>Linear Regressions with Combined Data</title>
      <link>https://arxiv.org/abs/2412.04816</link>
      <description>arXiv:2412.04816v1 Announce Type: cross 
Abstract: We study best linear predictions in a context where the outcome of interest and some of the covariates are observed in two different datasets that cannot be matched. Traditional approaches obtain point identification by relying, often implicitly, on exclusion restrictions. We show that without such restrictions, coefficients of interest can still be partially identified and we derive a constructive characterization of the sharp identified set. We then build on this characterization to develop computationally simple and asymptotically normal estimators of the corresponding bounds. We show that these estimators exhibit good finite sample performances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04816v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xavier D'Haultfoeuille, Christophe Gaillac, Arnaud Maurel</dc:creator>
    </item>
    <item>
      <title>Constructing optimal treatment length strategies to maximize quality-adjusted lifetimes</title>
      <link>https://arxiv.org/abs/2412.05108</link>
      <description>arXiv:2412.05108v1 Announce Type: cross 
Abstract: Real-world clinical decision making is a complex process that involves balancing the risks and benefits of treatments. Quality-adjusted lifetime is a composite outcome that combines patient quantity and quality of life, making it an attractive outcome in clinical research. We propose methods for constructing optimal treatment length strategies to maximize this outcome. Existing methods for estimating optimal treatment strategies for survival outcomes cannot be applied to a quality-adjusted lifetime due to induced informative censoring. We propose a weighted estimating equation that adjusts for both confounding and informative censoring. We also propose a nonparametric estimator of the mean counterfactual quality-adjusted lifetime survival curve under a given treatment length strategy, where the weights are estimated using an undersmoothed sieve-based estimator. We show that the estimator is asymptotically linear and provide a data-dependent undersmoothing criterion. We apply our method to obtain the optimal time for percutaneous endoscopic gastrostomy insertion in patients with amyotrophic lateral sclerosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05108v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Sun, Ashkan Ertefaie, Luke Duttweiler, Brent A. Johnson</dc:creator>
    </item>
    <item>
      <title>Compound Gaussian Radar Clutter Model With Positive Tempered Alpha-Stable Texture</title>
      <link>https://arxiv.org/abs/2412.05174</link>
      <description>arXiv:2412.05174v1 Announce Type: cross 
Abstract: The compound Gaussian (CG) family of distributions has achieved great success in modeling sea clutter. This work develops a flexible-tailed CG model to improve generality in clutter modeling, by introducing the positive tempered $\alpha$-stable (PT$\alpha$S) distribution to model clutter texture. The PT$\alpha$S distribution exhibits widely tunable tails by tempering the heavy tails of the positive $\alpha$-stable (P$\alpha$S) distribution, thus providing greater flexibility in texture modeling. Specifically, we first develop a bivariate isotropic CG-PT$\alpha$S complex clutter model that is defined by an explicit characteristic function, based on which the corresponding amplitude model is derived. Then, we prove that the amplitude model can be expressed as a scale mixture of Rayleighs, just as the successful compound K and Pareto models. Furthermore, a characteristic function-based method is developed to estimate the parameters of the amplitude model. Finally, real-world sea clutter data analysis indicates the amplitude model's flexibility in modeling clutter data with various tail behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05174v1</guid>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingxing Liao, Junhao Xie, Jie Zhou</dc:creator>
    </item>
    <item>
      <title>Estimation of Over-parameterized Models from an Auto-Modeling Perspective</title>
      <link>https://arxiv.org/abs/2206.01824</link>
      <description>arXiv:2206.01824v5 Announce Type: replace 
Abstract: From a model-building perspective, we propose a paradigm shift for fitting over-parameterized models. Philosophically, the mindset is to fit models to future observations rather than to the observed sample. Technically, given an imputation method to generate future observations, we fit over-parameterized models to these future observations by optimizing an approximation of the desired expected loss function based on its sample counterpart and an adaptive $\textit{duality function}$. The required imputation method is also developed using the same estimation technique with an adaptive $m$-out-of-$n$ bootstrap approach. We illustrate its applications with the many-normal-means problem, $n &lt; p$ linear regression, and neural network-based image classification of MNIST digits. The numerical results demonstrate its superior performance across these diverse applications. While primarily expository, the paper conducts an in-depth investigation into the theoretical aspects of the topic. It concludes with remarks on some open problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.01824v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Jiang, Chuanhai Liu</dc:creator>
    </item>
    <item>
      <title>Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models</title>
      <link>https://arxiv.org/abs/2310.12000</link>
      <description>arXiv:2310.12000v4 Announce Type: replace 
Abstract: Latent Gaussian process (GP) models are flexible probabilistic non-parametric function models. Vecchia approximations are accurate approximations for GPs to overcome computational bottlenecks for large data, and the Laplace approximation is a fast method with asymptotic convergence guarantees to approximate marginal likelihoods and posterior predictive distributions for non-Gaussian likelihoods. Unfortunately, the computational complexity of combined Vecchia-Laplace approximations grows faster than linearly in the sample size when used in combination with direct solver methods such as the Cholesky decomposition. Computations with Vecchia-Laplace approximations can thus become prohibitively slow precisely when the approximations are usually the most accurate, i.e., on large data sets. In this article, we present iterative methods to overcome this drawback. Among other things, we introduce and analyze several preconditioners, derive new convergence results, and propose novel methods for accurately approximating predictive variances. We analyze our proposed methods theoretically and in experiments with simulated and real-world data. In particular, we obtain a speed-up of an order of magnitude compared to Cholesky-based calculations and a threefold increase in prediction accuracy in terms of the continuous ranked probability score compared to a state-of-the-art method on a large satellite data set. All methods are implemented in a free C++ software library with high-level Python and R packages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12000v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2024.2410004</arxiv:DOI>
      <arxiv:journal_reference>Journal of the American Statistical Association (2024)</arxiv:journal_reference>
      <dc:creator>Pascal K\"undig, Fabio Sigrist</dc:creator>
    </item>
    <item>
      <title>Evaluating Treatment Benefit Predictors using Observational Data: Contending with Identification and Confounding Bias</title>
      <link>https://arxiv.org/abs/2407.05585</link>
      <description>arXiv:2407.05585v2 Announce Type: replace 
Abstract: A treatment benefit predictor (TBP) maps patient characteristics into an estimate of the treatment benefit for that patient, which can support optimizing treatment decisions. However, evaluating the predictive performance of a TBP is challenging, as it often must be conducted in a sample where treatment assignment is not random. We show conceptually how to approach validating a pre-specified TBP using observational data from the target population, in the context of a binary treatment decision at a single time point. We exemplify with a particular measure of discrimination (the concentration of benefit index) and a particular measure of calibration (the moderate calibration curve). The population-level definitions of these metrics involve the latent (counterfactual) treatment benefit variable, but we show identification by re-expressing the respective estimands in terms of the distribution of observable data only. We also show that in the absence of full confounding control, bias propagates in a more complex manner than when targeting more commonly encountered estimands (such as the average treatment effect, or the average treatment effect amongst the treated). Our findings reveal the patterns of biases are often unpredictable and underscore the necessity of accounting for confounding factors when evaluating TBPs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05585v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Xia, Mohsen Sadatsafavi, Paul Gustafson</dc:creator>
    </item>
    <item>
      <title>Chain-linked multiple matrix integration via embedding alignment</title>
      <link>https://arxiv.org/abs/2412.02791</link>
      <description>arXiv:2412.02791v2 Announce Type: replace 
Abstract: Motivated by the increasing demand for multi-source data integration in various scientific fields, in this paper we study matrix completion in scenarios where the data exhibits certain block-wise missing structures -- specifically, where only a few noisy submatrices representing (overlapping) parts of the full matrix are available. We propose the Chain-linked Multiple Matrix Integration (CMMI) procedure to efficiently combine the information that can be extracted from these individual noisy submatrices. CMMI begins by deriving entity embeddings for each observed submatrix, then aligns these embeddings using overlapping entities between pairs of submatrices, and finally aggregates them to reconstruct the entire matrix of interest. We establish, under mild regularity conditions, entrywise error bounds and normal approximations for the CMMI estimates. Simulation studies and real data applications show that CMMI is computationally efficient and effective in recovering the full matrix, even when overlaps between the observed submatrices are minimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02791v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Runbing Zheng, Minh Tang</dc:creator>
    </item>
    <item>
      <title>Treatment Effects with Targeting Instruments</title>
      <link>https://arxiv.org/abs/2007.10432</link>
      <description>arXiv:2007.10432v5 Announce Type: replace-cross 
Abstract: Multivalued treatments are commonplace in applications. We explore the use of discrete-valued instruments to control for selection bias in this setting. Our discussion revolves around the concept of targeting: which instruments target which treatments. It allows us to establish conditions under which counterfactual averages and treatment effects are point- or partially-identified for composite complier groups. We illustrate the usefulness of our framework by applying it to data from the Head Start Impact Study. Under a plausible positive selection assumption, we derive informative bounds that suggest less beneficial effects of Head Start expansions than the parametric estimates of Kline and Walters (2016).</description>
      <guid isPermaLink="false">oai:arXiv.org:2007.10432v5</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sokbae Lee, Bernard Salani\'e</dc:creator>
    </item>
    <item>
      <title>Learning the Effect of Persuasion via Difference-In-Differences</title>
      <link>https://arxiv.org/abs/2410.14871</link>
      <description>arXiv:2410.14871v2 Announce Type: replace-cross 
Abstract: The persuasion rate is a key parameter for measuring the causal effect of a directional message on influencing the recipient's behavior. Its identification has relied on exogenous treatment or the availability of credible instruments, but the requirements are not always satisfied in observational studies. Therefore, we develop a novel econometric framework for the average persuasion rate on the treated and other related parameters by using the difference-in-differences approach. The average treatment effect on the treated is a standard parameter in difference-in-differences, but we show that it is an overly conservative measure in the context of persuasion. For estimation and inference, we propose regression-based approaches as well as semiparametrically efficient estimators. Beginning with the two-period case, we extend the framework to staggered treatment settings, where we show how to conduct richer analyses like the event-study design. We investigate the British election and the Chinese curriculum reform as empirical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14871v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sung Jae Jun, Sokbae Lee</dc:creator>
    </item>
  </channel>
</rss>
