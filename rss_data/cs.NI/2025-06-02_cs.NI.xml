<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Jun 2025 03:04:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>NASP: Network Slice as a Service Platform for 5G Networks</title>
      <link>https://arxiv.org/abs/2505.24051</link>
      <description>arXiv:2505.24051v1 Announce Type: new 
Abstract: With 5G's rapid global uptake, demand for agile private networks has exploded. A defining beyond-5G capability is network slicing. 3GPP specifies three core slice categories, massive Machine-Type Communications (mMTC), enhanced Mobile Broadband (eMBB), and Ultra-Reliable Low-Latency Communications (URLLC), while ETSI's Zero-Touch Network and Service Management (ZSM) targets human-less operation. Yet existing documents do not spell out end-to-end (E2E) management spanning multiple domains and subnet instances. We introduce the Network Slice-as-a-Service Platform (NASP), designed to work across 3GPP and non-3GPP networks. NASP (i) translates business-level slice requests into concrete physical instances and inter-domain interfaces, (ii) employs a hierarchical orchestrator that aligns distributed management functions, and (iii) exposes clean south-bound APIs toward domain controllers. A prototype was built by unifying guidance from 3GPP, ETSI, and O-RAN, identifying overlaps and gaps among them. We tested NASP with two exemplary deployments, 3GPP and non-3GPP, over four scenarios: mMTC, URLLC, 3GPP-Shared, and non-3GPP. The Communication Service Management Function handled all requests, underlining the platform's versatility. Measurements show that core-network configuration dominates slice-creation time (68 %), and session setup in the URLLC slice is 93 % faster than in the Shared slice. Cost analysis for orchestrating five versus ten concurrent slices reveals a 112 % delta between edge and centralized deployments. These results demonstrate that NASP delivers flexible, standards-aligned E2E slicing while uncovering opportunities to reduce latency and operational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24051v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felipe Hauschild Grings, Gustavo Zanatta Bruno, Lucio Rene Prade, Cristiano Bonato Both, Jos\'e Marcos Camara Brito</dc:creator>
    </item>
    <item>
      <title>B2LoRa: Boosting LoRa Transmission for Satellite-IoT Systems with Blind Coherent Combining</title>
      <link>https://arxiv.org/abs/2505.24140</link>
      <description>arXiv:2505.24140v1 Announce Type: new 
Abstract: With the rapid growth of Low Earth Orbit (LEO) satellite networks, satellite-IoT systems using the LoRa technique have been increasingly deployed to provide widespread Internet services to low-power and low-cost ground devices. However, the long transmission distance and adverse environments from IoT satellites to ground devices pose a huge challenge to link reliability, as evidenced by the measurement results based on our real-world setup. In this paper, we propose a blind coherent combining design named B2LoRa to boost LoRa transmission performance. The intuition behind B2LoRa is to leverage the repeated broadcasting mechanism inherent in satellite-IoT systems to achieve coherent combining under the low-power and low-cost constraints, where each re-transmission at different times is regarded as the same packet transmitted from different antenna elements within an antenna array. Then, the problem is translated into aligning these packets at a fine granularity despite the time, frequency, and phase offsets between packets in the case of frequent packet loss. To overcome this challenge, we present three designs - joint packet sniffing, frequency shift alignment, and phase drift mitigation to deal with ultra-low SNRs and Doppler shifts featured in satellite-IoT systems, respectively. Finally, experiment results based on our real-world deployments demonstrate the high efficiency of B2LoRa.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24140v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yimin Zhao, Weibo Wang, Xiong Wang, Linghe Kong, Jiadi Yu, Yifei Zhu, Shiyuan Li, Chong He, Guihai Chen</dc:creator>
    </item>
    <item>
      <title>INSIGHT: A Survey of In-Network Systems for Intelligent, High-Efficiency AI and Topology Optimization</title>
      <link>https://arxiv.org/abs/2505.24269</link>
      <description>arXiv:2505.24269v1 Announce Type: new 
Abstract: In-network computation represents a transformative approach to addressing the escalating demands of Artificial Intelligence (AI) workloads on network infrastructure. By leveraging the processing capabilities of network devices such as switches, routers, and Network Interface Cards (NICs), this paradigm enables AI computations to be performed directly within the network fabric, significantly reducing latency, enhancing throughput, and optimizing resource utilization. This paper provides a comprehensive analysis of optimizing in-network computation for AI, exploring the evolution of programmable network architectures, such as Software-Defined Networking (SDN) and Programmable Data Planes (PDPs), and their convergence with AI. It examines methodologies for mapping AI models onto resource-constrained network devices, addressing challenges like limited memory and computational capabilities through efficient algorithm design and model compression techniques. The paper also highlights advancements in distributed learning, particularly in-network aggregation, and the potential of federated learning to enhance privacy and scalability. Frameworks like Planter and Quark are discussed for simplifying development, alongside key applications such as intelligent network monitoring, intrusion detection, traffic management, and Edge AI. Future research directions, including runtime programmability, standardized benchmarks, and new applications paradigms, are proposed to advance this rapidly evolving field. This survey underscores the potential of in-network AI to create intelligent, efficient, and responsive networks capable of meeting the demands of next-generation AI applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24269v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aleksandr Algazinov, Joydeep Chandra, Matt Laing</dc:creator>
    </item>
    <item>
      <title>5G RAN Slicing with Load Balanced Handovers</title>
      <link>https://arxiv.org/abs/2505.24295</link>
      <description>arXiv:2505.24295v1 Announce Type: new 
Abstract: With increasing density of small cells in modern multi-cell deployments, a given user can have multiple options for its serving cell. The serving cell for each user must be carefully chosen such that the user achieves reasonably high channel quality from it, and the load on each cell is well balanced. It is relatively straightforward to reason about this without slicing, where all users can share a global load balancing criteria set by the network operator. In this paper, we identify the unique challenges that arise when balancing load in a multi-cell setting with 5G slicing, where users are grouped into slices, and each slice has its own optimization criteria, resource quota, and demand distributions, making it hard to even define which cells are overloaded vs underloaded. We address these challenges through our system, RadioWeaver, that co-designs load balancing with dynamic quota allocation for each slice and each cell. RadioWeaver defines a novel global load balancing criteria across slices, that allows it to easily determine which cells are overloaded despite the fact that different slices optimize for different criteria. Our evaluation, using large-scale trace-driven simulations and a small-scale OpenRAN testbed, show how RadioWeaver achieves 16-365% better performance when compared to several baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24295v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yongzhou Chen, Muhammad Taimoor Tariq, Haitham Hassanieh, Radhika Mittal</dc:creator>
    </item>
    <item>
      <title>Design and Analysis of Power Consumption Models for Open-RAN Architectures</title>
      <link>https://arxiv.org/abs/2505.24552</link>
      <description>arXiv:2505.24552v1 Announce Type: new 
Abstract: The open radio access network (O-RAN) Alliance developed an architecture and specifications for open and disaggregated cellular networks including many elements that are being widely adopted and implemented in both commercial and research networks. In this paper, we develop transaction-based power consumption models of a centralized O-RAN architecture based on commercial hardware and considering the full end-to-end data path from the radio unit to the data center. We focus on recent fanout limitations and early baseband processing requirements related to current implementations of O-RAN and assess the power consumption impact when baseband processing is employed at different centralization points in the network. Additionally, we explore how greater fanout and sharing deeper into the network impact the balance of processing and transmission. Low processing fanout restrictions motivate greater centralization of the processing. At the same time, allowing for more open radio units per open distributed unit will quickly increase the transmission capacity requirements and related energy use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24552v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Urooj Tariq, Rishu Raj, Dan Kilper</dc:creator>
    </item>
    <item>
      <title>Explaining Sustained Blockchain Decentralization with Quasi-Experiments: Resource Flexibility of Consensus Mechanisms</title>
      <link>https://arxiv.org/abs/2505.24663</link>
      <description>arXiv:2505.24663v1 Announce Type: new 
Abstract: Decentralization is a fundamental design element of the Web3 economy. Blockchains and distributed consensus mechanisms are touted as fault-tolerant, attack-resistant, and collusion-proof because they are decentralized. Recent analyses, however, find some blockchains are decentralized, others are centralized, and that there are trends towards both centralization and decentralization in the blockchain economy. Despite the importance and variability of decentralization across blockchains, we still know little about what enables or constrains blockchain decentralization. We hypothesize that the resource flexibility of consensus mechanisms is a key enabler of the sustained decentralization of blockchain networks. We test this hypothesis using three quasi-experimental shocks -- policy-related, infrastructure-related, and technical -- to resources used in consensus. We find strong suggestive evidence that the resource flexibility of consensus mechanisms enables sustained blockchain decentralization and discuss the implications for the design, regulation, and implementation of blockchains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24663v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harang Ju, Madhav Kumar, Ehsan Valavi, Sinan Aral</dc:creator>
    </item>
    <item>
      <title>Trustworthy Provenance for Big Data Science: a Modular Architecture Leveraging Blockchain in Federated Settings</title>
      <link>https://arxiv.org/abs/2505.24675</link>
      <description>arXiv:2505.24675v1 Announce Type: new 
Abstract: Ensuring the trustworthiness and long-term verifiability of scientific data is a foundational challenge in the era of data-intensive, collaborative research. Provenance metadata plays a key role in this context, capturing the origin, transformation, and usage of research artifacts. However, existing solutions often fall short when applied to distributed, multi-institutional settings. This paper introduces a modular, domain-agnostic architecture for provenance tracking in federated environments, leveraging permissioned blockchain infrastructure to guarantee integrity, immutability, and auditability. The system supports decentralized interaction, persistent identifiers for artifact traceability, and a provenance versioning model that preserves the history of updates. Designed to interoperate with diverse scientific domains, the architecture promotes transparency, accountability, and reproducibility across organizational boundaries. Ongoing work focuses on validating the system through a distributed prototype and exploring its performance in collaborative settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24675v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicola Giuseppe Marchioro, Yannis Velegrakis, Valentine Anantharaj, Ian Foster, Sandro Luigi Fiore</dc:creator>
    </item>
    <item>
      <title>Optimizing Server Load Distribution in Multimedia IoT Environments through LSTM-Based Predictive Algorithms</title>
      <link>https://arxiv.org/abs/2505.24806</link>
      <description>arXiv:2505.24806v1 Announce Type: new 
Abstract: The Internet of Multimedia Things (IoMT) represents a significant advancement in the evolution of IoT technologies, focusing on the transmission and management of multimedia streams. As the volume of data continues to surge and the number of connected devices grows exponentially, internet traffic has reached unprecedented levels, resulting in challenges such as server overloads and deteriorating service quality. Traditional computer network architectures were not designed to accommodate this rapid increase in demand, leading to the necessity for innovative solutions. In response, Software-Defined Networks (SDNs) have emerged as a promising framework, offering enhanced management capabilities by decoupling the control layer from the data layer. This study explores the load balancing of servers within software-defined multimedia IoT networks. The Long Short-Term Memory (LSTM) prediction algorithm is employed to accurately estimate server loads and fuzzy systems are integrated to optimize load distribution across servers. The findings from the simulations indicate that the proposed approach enhances the optimization and management of IoT networks, resulting in improved service quality, reduced operational costs, and increased productivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24806v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Somaye Imanpour, Ahmadreza Montazerolghaem, Saeed Afshari</dc:creator>
    </item>
    <item>
      <title>Searching Neural Architectures for Sensor Nodes on IoT Gateways</title>
      <link>https://arxiv.org/abs/2505.23939</link>
      <description>arXiv:2505.23939v1 Announce Type: cross 
Abstract: This paper presents an automatic method for the design of Neural Networks (NNs) at the edge, enabling Machine Learning (ML) access even in privacy-sensitive Internet of Things (IoT) applications. The proposed method runs on IoT gateways and designs NNs for connected sensor nodes without sharing the collected data outside the local network, keeping the data in the site of collection. This approach has the potential to enable ML for Healthcare Internet of Things (HIoT) and Industrial Internet of Things (IIoT), designing hardware-friendly and custom NNs at the edge for personalized healthcare and advanced industrial services such as quality control, predictive maintenance, or fault diagnosis. By preventing data from being disclosed to cloud services, this method safeguards sensitive information, including industrial secrets and personal data. The outcomes of a thorough experimental session confirm that -- on the Visual Wake Words dataset -- the proposed approach can achieve state-of-the-art results by exploiting a search procedure that runs in less than 10 hours on the Raspberry Pi Zero 2.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23939v1</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Mattia Garavagno, Edoardo Ragusa, Antonio Frisoli, Paolo Gastaldo</dc:creator>
    </item>
    <item>
      <title>Detecting Airborne Objects with 5G NR Radars</title>
      <link>https://arxiv.org/abs/2505.24763</link>
      <description>arXiv:2505.24763v1 Announce Type: cross 
Abstract: The integration of sensing capabilities into 5G New Radio (5G NR) networks offers an opportunity to enable the detection of airborne objects without the need for dedicated radars. This paper investigates the feasibility of using standardized Positioning Reference Signals (PRS) to detect UAVs in Urban Micro (UMi) and Urban Macro (UMa) propagation environments. A full 5G NR radar processing chain is implemented, including clutter suppression, angle and range estimation, and 3D position reconstruction. Simulation results show that performance strongly depends on the propagation environment. 5G NR radars exhibit the highest missed detection rate, up to 16%, in UMi, due to severe clutter. Positioning error increases with target distance, resulting in larger errors in UMa scenarios and at higher UAV altitudes. In particular, the system achieves a position error within 4m in the UMi environment and within 8m in UMa. The simulation platform has been released as open-source software to support reproducible research in integrated sensing and communication (ISAC) systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24763v1</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steve Blandino, Nada Golmie, Anirudha Sahoo, Thao Nguyen, Tanguy Ropitault, David Griffith, Amala Sonny</dc:creator>
    </item>
    <item>
      <title>JPPO++: Joint Power and Denoising-inspired Prompt Optimization for Mobile LLM Services</title>
      <link>https://arxiv.org/abs/2412.03621</link>
      <description>arXiv:2412.03621v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly integrated into mobile services over wireless networks to support complex user requests. This trend has led to longer prompts, which improve LLMs' performance but increase data transmission costs and require more processing time, thereby reducing overall system efficiency and negatively impacting user experience. To address these challenges, we propose Joint Prompt and Power Optimization (JPPO), a framework that jointly optimizes prompt compression and wireless transmission power for mobile LLM services. JPPO leverages a Small Language Model (SLM) deployed at edge devices to perform lightweight prompt compression, reducing communication load before transmission to the cloud-based LLM. A Deep Reinforcement Learning (DRL) agent dynamically adjusts both the compression ratio and transmission power based on network conditions and service constraints, aiming to minimize service time while preserving response fidelity. We further extend the framework to JPPO++, which introduces a denoising-inspired compression scheme. This design performs iterative prompt refinement by progressively removing less informative tokens, allowing for more aggressive yet controlled compression. Experimental results show that JPPO++ reduces service time by 17% compared to the no-compression baseline while maintaining output quality. Under compression-prioritized settings, a reduction of up to 16x in prompt length can be achieved with an acceptable loss in accuracy. Specifically, JPPO with a 16x ratio reduces total service time by approximately 42.3%, and JPPO++ further improves this reduction to 46.5%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03621v4</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Feiran You, Hongyang Du, Kaibin Huang, Abbas Jamalipour</dc:creator>
    </item>
    <item>
      <title>Leveraging Large Language Models to Contextualize Network Measurements</title>
      <link>https://arxiv.org/abs/2505.19305</link>
      <description>arXiv:2505.19305v2 Announce Type: replace 
Abstract: With the worldwide growth of remote communication and telepresence, network measurements form a cornerstone of effective performance assessment and diagnostics for Internet users. Most often, users seek for overall connection performance measurement using publicly available tools (also known as `speed tests') that provide an overview of their connection's throughput and latency. However, extracting meaningful insights from these measurements remains a challenging task for a non-technical audience. Interpreting network measurement data often requires considerable domain expertise to account not only for subtle variations of the connection stability and metrics, but even for simpler concepts such as latency under load or packet loss influence towards connection performance. In the absence of proper expertise, common misconceptions can easily arise. To address these issues, researchers should recognize the importance of making network measurements not only more comprehensive but also more accessible for wider audience without deep technical knowledge. A promising direction to achieve this goal involves leveraging recent advancements in large language models (LLMs), which have demonstrated capabilities in conducting an analysis of complex data in other fields, such as laboratory test results interpretation, news summarization, and personal assistance.
  In this paper, we describe an ongoing effort to apply large language models and historical data to enhance the interpretation of network measurements in real-world environments. We aim to automate the translation of low-level metric data into accessible explanations, allowing non-experts to make more informed decisions regarding network performance and reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19305v2</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roman Beltiukov, Karthik Bhattaram, Evania Cheng, Vinod Kanigicherla, Akul Singh, Ken Thampiratwong, Arpit Gupta</dc:creator>
    </item>
    <item>
      <title>Quantum Hilbert Transform</title>
      <link>https://arxiv.org/abs/2505.23581</link>
      <description>arXiv:2505.23581v2 Announce Type: replace-cross 
Abstract: The Hilbert transform has been one of the foundational transforms in signal processing, finding it's way into multiple disciplines from cryptography to biomedical sciences. However, there does not exist any quantum analogue for the Hilbert transform. In this work, we introduce a formulation for the quantum Hilbert transform (QHT)and apply it to a quantum steganography protocol. By bridging classical phase-shift techniques with quantum operations, QHT opens new pathways in quantum signal processing, communications, sensing, and secure information hiding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23581v2</guid>
      <category>quant-ph</category>
      <category>cs.CR</category>
      <category>cs.DM</category>
      <category>cs.NI</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nitin Jha, Abhishek Parakh</dc:creator>
    </item>
  </channel>
</rss>
