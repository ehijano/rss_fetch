<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 05 May 2025 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>LLM Ethics Benchmark: A Three-Dimensional Assessment System for Evaluating Moral Reasoning in Large Language Models</title>
      <link>https://arxiv.org/abs/2505.00853</link>
      <description>arXiv:2505.00853v1 Announce Type: new 
Abstract: This study establishes a novel framework for systematically evaluating the moral reasoning capabilities of large language models (LLMs) as they increasingly integrate into critical societal domains. Current assessment methodologies lack the precision needed to evaluate nuanced ethical decision-making in AI systems, creating significant accountability gaps. Our framework addresses this challenge by quantifying alignment with human ethical standards through three dimensions: foundational moral principles, reasoning robustness, and value consistency across diverse scenarios. This approach enables precise identification of ethical strengths and weaknesses in LLMs, facilitating targeted improvements and stronger alignment with societal values. To promote transparency and collaborative advancement in ethical AI development, we are publicly releasing both our benchmark datasets and evaluation codebase at https://github.com/ The-Responsible-AI-Initiative/LLM_Ethics_Benchmark.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00853v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junfeng Jiao, Saleh Afroogh, Abhejay Murali, Kevin Chen, David Atkinson, Amit Dhurandhar</dc:creator>
    </item>
    <item>
      <title>The AI Fairness Myth: A Position Paper on Context-Aware Bias</title>
      <link>https://arxiv.org/abs/2505.00965</link>
      <description>arXiv:2505.00965v1 Announce Type: new 
Abstract: Defining fairness in AI remains a persistent challenge, largely due to its deeply context-dependent nature and the lack of a universal definition. While numerous mathematical formulations of fairness exist, they sometimes conflict with one another and diverge from social, economic, and legal understandings of justice. Traditional quantitative definitions primarily focus on statistical comparisons, but they often fail to simultaneously satisfy multiple fairness constraints. Drawing on philosophical theories (Rawls' Difference Principle and Dworkin's theory of equality) and empirical evidence supporting affirmative action, we argue that fairness sometimes necessitates deliberate, context-aware preferential treatment of historically marginalized groups. Rather than viewing bias solely as a flaw to eliminate, we propose a framework that embraces corrective, intentional biases to promote genuine equality of opportunity. Our approach involves identifying unfairness, recognizing protected groups/individuals, applying corrective strategies, measuring impact, and iterating improvements. By bridging mathematical precision with ethical and contextual considerations, we advocate for an AI fairness paradigm that goes beyond neutrality to actively advance social justice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00965v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kessia Nepomuceno, Fabio Petrillo</dc:creator>
    </item>
    <item>
      <title>Artificial Intelligence in Government: Why People Feel They Lose Control</title>
      <link>https://arxiv.org/abs/2505.01085</link>
      <description>arXiv:2505.01085v1 Announce Type: new 
Abstract: The use of Artificial Intelligence (AI) in public administration is expanding rapidly, moving from automating routine tasks to deploying generative and agentic systems that autonomously act on goals. While AI promises greater efficiency and responsiveness, its integration into government functions raises concerns about fairness, transparency, and accountability. This article applies principal-agent theory (PAT) to conceptualize AI adoption as a special case of delegation, highlighting three core tensions: assessability (can decisions be understood?), dependency (can the delegation be reversed?), and contestability (can decisions be challenged?). These structural challenges may lead to a "failure-by-success" dynamic, where early functional gains obscure long-term risks to democratic legitimacy. To test this framework, we conducted a pre-registered factorial survey experiment across tax, welfare, and law enforcement domains. Our findings show that although efficiency gains initially bolster trust, they simultaneously reduce citizens' perceived control. When the structural risks come to the foreground, institutional trust and perceived control both drop sharply, suggesting that hidden costs of AI adoption significantly shape public attitudes. The study demonstrates that PAT offers a powerful lens for understanding the institutional and political implications of AI in government, emphasizing the need for policymakers to address delegation risks transparently to maintain public trust.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01085v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Wuttke, Adrian Rauchfleisch, Andreas Jungherr</dc:creator>
    </item>
    <item>
      <title>Investigating Middle School Students Question-Asking and Answer-Evaluation Skills When Using ChatGPT for Science Investigation</title>
      <link>https://arxiv.org/abs/2505.01106</link>
      <description>arXiv:2505.01106v1 Announce Type: new 
Abstract: Generative AI (GenAI) tools such as ChatGPT allow users, including school students without prior AI expertise, to explore and address a wide range of tasks. Surveys show that most students aged eleven and older already use these tools for school-related activities. However, little is known about how they actually use GenAI and how it impacts their learning.
  This study addresses this gap by examining middle school students ability to ask effective questions and critically evaluate ChatGPT responses, two essential skills for active learning and productive interactions with GenAI. 63 students aged 14 to 15 were tasked with solving science investigation problems using ChatGPT. We analyzed their interactions with the model, as well as their resulting learning outcomes.
  Findings show that students often over-relied on ChatGPT in both the question-asking and answer-evaluation phases. Many struggled to use clear questions aligned with task goals and had difficulty judging the quality of responses or knowing when to seek clarification. As a result, their learning performance remained moderate: their explanations of the scientific concepts tended to be vague, incomplete, or inaccurate, even after unrestricted use of ChatGPT. This pattern held even in domains where students reported strong prior knowledge.
  Furthermore, students self-reported understanding and use of ChatGPT were negatively associated with their ability to select effective questions and evaluate responses, suggesting misconceptions about the tool and its limitations. In contrast, higher metacognitive skills were positively linked to better QA-related skills.
  These findings underscore the need for educational interventions that promote AI literacy and foster question-asking strategies to support effective learning with GenAI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01106v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rania Abdelghani, Kou Murayama, Celeste Kidd, H\'el\`ene Sauz\'eon, Pierre-Yves Oudeyer</dc:creator>
    </item>
    <item>
      <title>The Great Data Standoff: Researchers vs. Platforms Under the Digital Services Act</title>
      <link>https://arxiv.org/abs/2505.01122</link>
      <description>arXiv:2505.01122v1 Announce Type: new 
Abstract: To facilitate accountability and transparency, the Digital Services Act (DSA) sets up a process through which Very Large Online Platforms (VLOPs) need to grant vetted researchers access to their internal data (Article 40(4)). Operationalising such access is challenging for at least two reasons. First, data access is only available for research on systemic risks affecting European citizens, a concept with high levels of legal uncertainty. Second, data access suffers from an inherent standoff problem. Researchers need to request specific data but are not in a position to know all internal data processed by VLOPs, who, in turn, expect data specificity for potential access. In light of these limitations, data access under the DSA remains a mystery. To contribute to the discussion of how Article 40 can be interpreted and applied, we provide a concrete illustration of what data access can look like in a real-world systemic risk case study. We focus on the 2024 Romanian presidential election interference incident, the first event of its kind to trigger systemic risk investigations by the European Commission. During the elections, one candidate is said to have benefited from TikTok algorithmic amplification through a complex dis- and misinformation campaign. By analysing this incident, we can comprehend election-related systemic risk to explore practical research tasks and compare necessary data with available TikTok data. In particular, we make two contributions: (i) we combine insights from law, computer science and platform governance to shed light on the complexities of studying systemic risks in the context of election interference, focusing on two relevant factors: platform manipulation and hidden advertising; and (ii) we provide practical insights into various categories of available data for the study of TikTok, based on platform documentation, data donations and the Research API.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01122v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Catalina Goanta, Savvas Zannettou, Rishabh Kaushal, Jacob van de Kerkhof, Thales Bertaglia, Taylor Annabell, Haoyang Gui, Gerasimos Spanakis, Adriana Iamnitchi</dc:creator>
    </item>
    <item>
      <title>Methodological Foundations for AI-Driven Survey Question Generation</title>
      <link>https://arxiv.org/abs/2505.01150</link>
      <description>arXiv:2505.01150v1 Announce Type: new 
Abstract: This paper presents a methodological framework for using generative AI in educational survey research. We explore how Large Language Models (LLMs) can generate adaptive, context-aware survey questions and introduce the Synthetic Question-Response Analysis (SQRA) framework, which enables iterative testing and refinement of AI-generated prompts prior to deployment with human participants. Guided by Activity Theory, we analyze how AI tools mediate participant engagement and learning, and we examine ethical issues such as bias, privacy, and transparency. Through sentiment, lexical, and structural analyses of both AI-to-AI and AI-to-human survey interactions, we evaluate the alignment and effectiveness of these questions. Our findings highlight the promise and limitations of AI-driven survey instruments, emphasizing the need for robust prompt engineering and validation to support trustworthy, scalable, and contextually relevant data collection in engineering education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01150v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ted K. Mburu, Kangxuan Rong, Campbell J. McColley, Alexandra Werth</dc:creator>
    </item>
    <item>
      <title>Emerging Media Use and Acceptance of Digital Immortality: A Cluster Analysis among Chinese Young Generations</title>
      <link>https://arxiv.org/abs/2505.01355</link>
      <description>arXiv:2505.01355v1 Announce Type: new 
Abstract: The rapid technological advancements made the concept of digital immortality less fantastical and more plausible, sparking academic and industrial interest. Existing literature mainly discusses philosophical and societal aspects, lacking specific empirical observation. To address this gap, we conducted a study among Chinese youth to gauge their acceptance of digital immortality. Using cluster analysis, we classified participants into three groups: "geeks," "video game players," and "laggards" based on their media usage. Those most receptive to digital immortality, termed "geeks" tend to be male, with higher income levels, openness, conscientiousness, extensive engagement with emerging media technology, and surprisingly, more adhering to Buddhism and Daoism. Overall, this study examined media usage patterns and youth perspectives on digital immortality, shedding light on technology's role in shaping views on life and death. It highlights the importance of further research on the profound implications of digital immortality in the context of contemporary society.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01355v1</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Mou, Jianfeng Lan, Jingyao Lu, Jilong Wang</dc:creator>
    </item>
    <item>
      <title>PHSafe: Disclosure Avoidance for the 2020 Census Supplemental Demographic and Housing Characteristics File (S-DHC)</title>
      <link>https://arxiv.org/abs/2505.01254</link>
      <description>arXiv:2505.01254v1 Announce Type: cross 
Abstract: This article describes the disclosure avoidance algorithm that the U.S. Census Bureau used to protect the 2020 Census Supplemental Demographic and Housing Characteristics File (S-DHC). The tabulations contain statistics of counts of U.S. persons living in certain types of households, including averages. The article describes the PHSafe algorithm, which is based on adding noise drawn from a discrete Gaussian distribution to the statistics of interest. We prove that the algorithm satisfies a well-studied variant of differential privacy, called zero-concentrated differential privacy. We then describe how the algorithm was implemented on Tumult Analytics and briefly outline the parameterization and tuning of the algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01254v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Sexton, Skye Berghel, Bayard Carlson, Sam Haney, Luke Hartman, Michael Hay, Ashwin Machanavajjhala, Gerome Miklau, Amritha Pai, Simran Rajpal, David Pujol, Ruchit Shrestha, Daniel Simmons-Marengo</dc:creator>
    </item>
    <item>
      <title>Identifying discreditable firms in a large-scale ownership network</title>
      <link>https://arxiv.org/abs/2211.14316</link>
      <description>arXiv:2211.14316v2 Announce Type: replace 
Abstract: Violations of laws and regulations about food safety, production safety, quality standard and environmental protection, or negative consequences from loan, guarantee and pledge contracts, may result in operating and credit risks of firms. The above illegal or trust-breaking activities are collectively called discreditable activities, and firms with discreditable activities are named as discreditable firms. Identification of discreditable firms is of great significance for investment attraction, bank lending, equity investment, supplier selection, job seeking, and so on. In this paper, we collect registration records of about 113 million Chinese firms and construct an ownership network with about 6 million nodes, where each node is a firm who has invested at least one firm or has been invested by at least one firm. Analysis of publicly available records of discreditable activities show strong network effect, namely the probability of a firm to be discreditable is remarkably higher than the average probability given the fact that one of its investors or investees is discreditable. In comparison, for the risk of being a discreditable firm, an investee has higher impact than an investor in average. The impact of a firm on surrounding firms decays along with the increasing topological distance, analogous to the well-known "three degrees of separation" phenomenon. The uncovered correlation of discreditable activities can be considered as a representative example of network effect, in addition to the propagation of diseases, opinions and human behaviors. Lastly, we show that the utilization of the network effect largely improves the accuracy of the algorithm to identify discreditable firms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.14316v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tao Zhou, Yan-Li Lee, Qian Li, Duanbing Chen, Wenbo Xie, Tong Wu, Tu Zeng</dc:creator>
    </item>
    <item>
      <title>Cyber Food Swamps: Investigating the Impacts of Online-to-Offline Food Delivery Platforms on Healthy Food Choices</title>
      <link>https://arxiv.org/abs/2409.16601</link>
      <description>arXiv:2409.16601v3 Announce Type: replace 
Abstract: Online-to-offline (O2O) food delivery platforms have greatly expanded urban residents' access to a wide range of food options by allowing convenient ordering from distant food outlets. However, concerns persist regarding the nutritional quality of delivered food, particularly as the impact of O2O food delivery platforms on users' healthy food remains unclear. This study leverages large-scale empirical data from a leading O2O delivery platform to comprehensively analyze online food choice behaviors and how they are influenced by the online exposure to fast food restaurants, i.e., online food environment. Our analyses reveal significant variations in food preferences across demographic groups and city sizes, where male, low-income, and younger users are more likely to order fast food via O2O platforms. Besides, we also perform a comparative analysis on the food exposure differences in offline and online environments, confirming that the extended service ranges of O2O platforms can create larger "cyber food swamps". Furthermore, regression analysis highlights that a higher ratio of fast food orders is associated with "cyber food swamps", areas characterized by a higher proportion of accessible fast food restaurants. A 10% increase in this proportion raises the probability of ordering fast food by 22.0%. Moreover, a quasi-natural experiment substantiates the long-term causal effect of online food environment changes on healthy food choices. These findings underscore the need for O2O food delivery platforms to address the health implications of online food choice exposure, offering critical insights for stakeholders aiming to improve dietary health among urban populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16601v3</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunke Zhang, Yiran Fan, Peijie Liu, Fengli Xu, Yong Li</dc:creator>
    </item>
    <item>
      <title>Human-centered explanation does not fit all: The interplay of sociotechnical, cognitive, and individual factors in the effect AI explanations in algorithmic decision-making</title>
      <link>https://arxiv.org/abs/2502.12354</link>
      <description>arXiv:2502.12354v2 Announce Type: replace 
Abstract: Recent XAI studies have investigated what constitutes a \textit{good} explanation in AI-assisted decision-making. Despite the widely accepted human-friendly properties of explanations, such as contrastive and selective, existing studies have yielded inconsistent findings. To address these gaps, our study focuses on the cognitive dimensions of explanation evaluation, by evaluating six explanations with different contrastive strategies and information selectivity and scrutinizing factors behind their valuation process. Our analysis results find that contrastive explanations are not the most preferable or understandable in general; Rather, different contrastive and selective explanations were appreciated to a different extent based on who they are, when, how, and what to explain -- with different level of cognitive load and engagement and sociotechnical contexts. Given these findings, we call for a nuanced view of explanation strategies, with implications for designing AI interfaces to accommodate individual and contextual differences in AI-assisted decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12354v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongsu Ahn, Yu-Ru Lin, Malihe Alikhani, Eunjeong Cheon</dc:creator>
    </item>
    <item>
      <title>Generative AI in Academic Writing: A Comparison of DeepSeek, Qwen, ChatGPT, Gemini, Llama, Mistral, and Gemma</title>
      <link>https://arxiv.org/abs/2503.04765</link>
      <description>arXiv:2503.04765v2 Announce Type: replace 
Abstract: DeepSeek v3, developed in China, was released in December 2024, followed by Alibaba's Qwen 2.5 Max in January 2025 and Qwen3 235B in April 2025. These free and open-source models offer significant potential for academic writing and content creation. This study evaluates their academic writing performance by comparing them with ChatGPT, Gemini, Llama, Mistral, and Gemma. There is a critical gap in the literature concerning how extensively these tools can be utilized and their potential to generate original content in terms of quality, readability, and effectiveness. Using 40 papers on Digital Twin and Healthcare, texts were generated through AI tools based on posed questions and paraphrased abstracts. The generated content was analyzed using plagiarism detection, AI detection, word count comparisons, semantic similarity, and readability assessments. Results indicate that paraphrased abstracts showed higher plagiarism rates, while question-based responses also exceeded acceptable levels. AI detection tools consistently identified all outputs as AI-generated. Word count analysis revealed that all chatbots produced a sufficient volume of content. Semantic similarity tests showed a strong overlap between generated and original texts. However, readability assessments indicated that the texts were insufficient in terms of clarity and accessibility. This study comparatively highlights the potential and limitations of popular and latest large language models for academic writing. While these models generate substantial and semantically accurate content, concerns regarding plagiarism, AI detection, and readability must be addressed for their effective use in scholarly work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04765v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omer Aydin, Enis Karaarslan, Fatih Safa Erenay, Nebojsa Bacanin</dc:creator>
    </item>
    <item>
      <title>Should you use LLMs to simulate opinions? Quality checks for early-stage deliberation</title>
      <link>https://arxiv.org/abs/2504.08954</link>
      <description>arXiv:2504.08954v2 Announce Type: replace 
Abstract: The emergent capabilities of large language models (LLMs) have sparked interest in assessing their ability to simulate human opinions in a variety of contexts, potentially serving as surrogates for human subjects in opinion surveys. However, previous evaluations of this capability have depended heavily on costly, domain-specific human survey data, and mixed empirical results about LLM effectiveness create uncertainty for managers about whether investing in this technology is justified in early-stage research. To address these challenges, we introduce a series of quality checks to support early-stage deliberation about the viability of using LLMs for simulating human opinions. These checks emphasize logical constraints, model stability, and alignment with stakeholder expectations of model outputs, thereby reducing dependence on human-generated data in the initial stages of evaluation. We demonstrate the usefulness of the proposed quality control tests in the context of AI-assisted content moderation, an application that both advocates and critics of LLMs' capabilities to simulate human opinion see as a desirable potential use case. None of the tested models passed all quality control checks, revealing several failure modes. We conclude by discussing implications of these failure modes and recommend how organizations can utilize our proposed tests for prompt engineering and in their risk management practices when considering the use of LLMs for opinion simulation. We make our crowdsourced dataset of claims with human and LLM annotations publicly available for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08954v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Terrence Neumann, Maria De-Arteaga, Sina Fazelpour</dc:creator>
    </item>
  </channel>
</rss>
