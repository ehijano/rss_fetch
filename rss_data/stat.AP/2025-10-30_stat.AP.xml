<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 Oct 2025 04:01:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Tractable Algorithms for Changepoint Detection in Player Performance Metrics</title>
      <link>https://arxiv.org/abs/2510.25961</link>
      <description>arXiv:2510.25961v1 Announce Type: new 
Abstract: We present tractable methods for detecting changes in player performance metrics and apply these methods to Major League Baseball (MLB) batting and pitching data from the 2023 and 2024 seasons. First, we derive principled benchmarks for when performance metrics can be considered statistically reliable, assuming no underlying change, using distributional assumptions and standard concentration inequalities. We then propose a changepoint detection algorithm that combines a likelihood-based approach with split-sample inference to control false positives, using either nonparametric tests or tests appropriate to the underlying data distribution. These tests incorporate a shift parameter, allowing users to specify the minimum magnitude of change to detect. We demonstrate the utility of this approach across several baseball applications: detecting changes in batter plate discipline metrics (e.g., chase and whiff rate), identifying velocity changes in pitcher fastballs, and validating velocity changepoints against a curated ground-truth dataset of pitchers who transitioned from relief to starting roles. Our method flags meaningful changes in 91% of these `ground-truth' cases and reveals that, for some metrics, more than 60% of detected changes occur in-season. While developed for baseball, the proposed framework is broadly applicable to any setting involving monitoring of individual performance over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25961v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amanda Glazer</dc:creator>
    </item>
    <item>
      <title>Variational System Identification of Aircraft</title>
      <link>https://arxiv.org/abs/2510.26496</link>
      <description>arXiv:2510.26496v1 Announce Type: new 
Abstract: Variational system identification is a new formulation of maximum likelihood for estimation of parameters of dynamical systems subject to process and measurement noise, such as aircraft flying in turbulence. This formulation is an alternative to the filter-error method that circumvents the solution of a Riccati equation and does not have problems with unstable predictors. In this paper, variational system identification is demonstrated for estimating aircraft parameters from real flight-test data. The results show that, in real applications of practical interest, it has better convergence properties than the filter-error method, reaching the optimum even when null initial guesses are used for all parameters and decision variables. This paper also presents the theory behind the method and practical recommendations for its use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26496v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.2514/6.2025-1253</arxiv:DOI>
      <arxiv:journal_reference>Dutra, Dimas A. A., "Variational System Identification of Aircraft", AIAA SCITECH 2025 Forum, Orlando, FL, Jan. 6-10, 2025. (AIAA 2025-1253)</arxiv:journal_reference>
      <dc:creator>Dimas Abreu Archanjo Dutra</dc:creator>
    </item>
    <item>
      <title>Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models</title>
      <link>https://arxiv.org/abs/2510.26524</link>
      <description>arXiv:2510.26524v1 Announce Type: cross 
Abstract: Heavy-tailed distributions, prevalent in a lot of real-world applications such as finance, telecommunications, queuing theory, and natural language processing, are challenging to model accurately owing to their slow tail decay. Bernstein phase-type (BPH) distributions, through their analytical tractability and good approximations in the non-tail region, can present a good solution, but they suffer from an inability to reproduce these heavy-tailed behaviors exactly, thus leading to inadequate performance in important tail areas. On the contrary, while highly adaptable to heavy-tailed distributions, hyperexponential (HE) models struggle in the body part of the distribution. Additionally, they are highly sensitive to initial parameter selection, significantly affecting their precision.
  To solve these issues, we propose a novel hybrid model of BPH and HE distributions, borrowing the most desirable features from each for enhanced approximation quality. Specifically, we leverage an optimization to set initial parameters for the HE component, significantly enhancing its robustness and reducing the possibility that the associated procedure results in an invalid HE model. Experimental validation demonstrates that the novel hybrid approach is more performant than individual application of BPH or HE models. More precisely, it can capture both the body and the tail of heavy-tailed distributions, with a considerable enhancement in matching parameters such as mean and coefficient of variation. Additional validation through experiments utilizing queuing theory proves the practical usefulness, accuracy, and precision of our hybrid approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26524v1</guid>
      <category>cs.PF</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Abdelhakim Ziani, Andr\'as Horv\'ath, Paolo Ballarini</dc:creator>
    </item>
    <item>
      <title>Combining Unsupervised Learning and Statistical Inference For Multimodal N-of-1 Trials</title>
      <link>https://arxiv.org/abs/2309.06455</link>
      <description>arXiv:2309.06455v3 Announce Type: replace 
Abstract: N-of-1 trials are within-person crossover trials allowing both personalized and population-level inference on the effect of health interventions. Using the full potential of modern technologies, multimodal N-of-1 trials can integrate multimedia data for measuring health outcomes. However, methodology required for automated applications in large multimodal trials is not available yet. Here, we present an unsupervised approach for modeling multimodal N-of-1 trials, bypassing the need for expensive outcome labeling by medical experts. First, an autoencoder is trained on the outcome medical images. Then, the dimensionality of embeddings is reduced by extracting the first principal component, which is finally tested for its association with the treatment. Results from imaging simulation studies show high power in detecting a treatment effect while controlling type I error rates. An application to imaging N-of-1 trials of acne severity identifies individual treatment effects and supports that our methodology can enable large clinical multimodal N-of-1 trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.06455v3</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juliana Schneider, Thomas G\"artner, Stefan Konigorski</dc:creator>
    </item>
    <item>
      <title>Comparative Analysis of Weather-Based Indexes and the Actuaries Climate Index$^{TM}$ for Crop Yield Prediction and Weather-Derivative Pricing</title>
      <link>https://arxiv.org/abs/2504.21143</link>
      <description>arXiv:2504.21143v2 Announce Type: replace 
Abstract: Climate change poses significant challenges to the agricultural and financial sectors, affecting crop productivity and overall financial stability. This study evaluates the robustness of the Actuaries Climate Index$^{TM}$ (ACI), a newer entrant in the field as a tool for measuring climate impacts, by comparing its explanatory power with well-established weather-based indexes (WBIs) across two key sectors. In the agricultural context, the yields of three major crops are predicted using generalized statistical models and advanced machine learning algorithms with climate indexes as explanatory variables. To enhance model reliability and address multicollinearity among weather-related variables, the study also incorporates both principal component analysis and functional principal component analysis. A total of 22 models, each constructed with different sets of explanatory variables, demonstrate the significant impact of wind speed and sea-level changes, alongside temperature and precipitation, on crop yield variability across six regions of the United States. For the financial market application, the analysis adapts the weather derivative framework, as it is a critical instrument for energy companies, insurers, and agribusinesses seeking to hedge against weather-related risks. By analyzing the payoffs of derivative contracts that use WBIs and ACI components as underlying variables, the findings reveal that the ACI framework holds a strong potential as a comprehensive climate risk indicator, not only for the agricultural sector but also for the finance and insurance industries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21143v2</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cem Yavrum, A. Sevtap Selcuk-Kestel, Jos\'e Garrido</dc:creator>
    </item>
    <item>
      <title>PoPStat-COVID19: Leveraging Population Pyramids to Quantify Demographic Vulnerability to COVID-19</title>
      <link>https://arxiv.org/abs/2509.14213</link>
      <description>arXiv:2509.14213v2 Announce Type: replace 
Abstract: Understanding how population age structure shapes COVID-19 burden is crucial for pandemic preparedness, yet common summary measures such as median age ignore key distributional features like skewness, bimodality, and the proportional weight of high-risk cohorts. We extend the PoPStat framework, originally devised to link entire population pyramids with cause-specific mortality by applying it to COVID-19. Using 2019 United Nations World Population Prospects age-sex distributions together with cumulative cases and deaths per million recorded up to 5 May 2023 by Our World in Data, we calculate PoPDivergence (the Kullback-Leibler divergence from an optimised reference pyramid) for 180+ countries and derive PoPStat-COVID19 as the Pearson correlation between that divergence and log-transformed incidence or mortality. Optimisation selects Malta's old-skewed pyramid as the reference, yielding strong negative correlations for cases (r=-0.86, p&lt;0.001, R^2=0.74) and deaths (r=-0.82, p&lt;0.001, R^2=0.67). Sensitivity tests across twenty additional, similarly old-skewed references confirm that these associations are robust to reference choice. Benchmarking against eight standard indicators like gross domestic product per capita, Gini index, Human Development Index, life expectancy at birth, median age, population density, Socio-demographic Index, and Universal Health Coverage Index shows that PoPStat-COVID19 surpasses GDP per capita, median age, population density, and several other traditional measures, and outperforms every comparator for fatality burden. PoPStat-COVID19 therefore provides a concise, distribution-aware scalar for quantifying demographic vulnerability to COVID-19.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14213v2</guid>
      <category>stat.AP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Buddhi Wijenayake, Athulya Ratnayake, Lelumi Edirisinghe, Uditha Wijeratne, Tharaka Fonseka, Roshan Godaliyadda, Samath Dharmaratne, Parakrama Ekanayake, Vijitha Herath, Insoha Alwis, Supun Manathunga</dc:creator>
    </item>
    <item>
      <title>Survey Data Integration for Distribution Function Estimation</title>
      <link>https://arxiv.org/abs/2409.14284</link>
      <description>arXiv:2409.14284v5 Announce Type: replace-cross 
Abstract: Estimates of finite population cumulativedistribution functions (CDFs) and quantiles are critical forpolicy-making, resource allocation, and public health planning. For instance, federal finance agencies may require accurate estimates of the proportion of individuals with income below the federal poverty line to determine funding eligibility, while health organizations may rely on precise quantile estimates of key health variables to guide local health interventions. Despite growing interest in survey data integration, research on the integration of probability and nonprobability samples toestimate CDFs and quantiles remains limited. In this study, we propose a novel residual-based CDF estimator that integrates information from a probability sample with data from potentially large nonprobability samples. Our approach leverages shared covariates observed in both datasets, while the response variable is available only in the nonprobability sample. Using a semiparametric approach, we train an outcome model on the nonprobability sample and incorporate model residuals with sampling weights from the probability sample to estimate the CDF of the target variable. Based on this CDF estimator, we define a quantile estimator and introduce linearization and bootstrap methods for variance estimation of both the CDF and quantile estimators. Under certain regularity conditions, we establish the asymptotic properties, including bias and variance, of the CDF estimator. Our empirical findings support the theoretical results and demonstrate the favorable performance of the proposed estimators relative to plug-in mass imputation estimators and the na\"ive estimators derived from the nonprobability sample only. A real data example is presented to illustrate the proposed estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14284v5</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy Flood, Sayed Mostafa</dc:creator>
    </item>
    <item>
      <title>Estimation and Inference in Boundary Discontinuity Designs: Location-Based Methods</title>
      <link>https://arxiv.org/abs/2505.05670</link>
      <description>arXiv:2505.05670v2 Announce Type: replace-cross 
Abstract: Boundary discontinuity designs are used to learn about causal treatment effects along a continuous assignment boundary that splits units into control and treatment groups according to a bivariate location score. We analyze the statistical properties of local polynomial treatment effect estimators employing location information for each unit. We develop pointwise and uniform estimation and inference methods for both the conditional treatment effect function at the assignment boundary as well as for transformations thereof, which aggregate information along the boundary. We illustrate our methods with an empirical application. Companion general-purpose software is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05670v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Rocio Titiunik, Ruiqi Rae Yu</dc:creator>
    </item>
  </channel>
</rss>
