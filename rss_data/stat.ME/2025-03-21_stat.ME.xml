<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Mar 2025 04:00:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Statistical Inference for Heterogeneous Treatment Effect with Right-censored Data from Synthesizing Randomized Clinical Trials and Real-world Data</title>
      <link>https://arxiv.org/abs/2503.15745</link>
      <description>arXiv:2503.15745v1 Announce Type: new 
Abstract: The heterogeneous treatment effect plays a crucial role in precision medicine. There is evidence that real-world data, even subject to biases, can be employed as supplementary evidence for randomized clinical trials to improve the statistical efficiency of the heterogeneous treatment effect estimation. In this paper, for survival data with right censoring, we consider estimating the heterogeneous treatment effect, defined as the difference of the treatment-specific conditional restricted mean survival times given covariates, by synthesizing evidence from randomized clinical trials and the real-world data with possible biases. We define an omnibus confounding function to characterize the effect of biases caused by unmeasured confounders, censoring, outcome heterogeneity, and measurement error, and further, identify it by combining the trial and real-world data. We propose a penalized sieve method to estimate the heterogeneous treatment effect and the confounding function and further study the theoretical properties of the proposed integrative estimators based on the theory of reproducing kernel Hilbert space and empirical process. The proposed methodology is shown to outperform the approach solely based on the trial data through simulation studies and an integrative analysis of the data from a randomized trial and a real-world registry on early-stage non-small-cell lung cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15745v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guangcai Mao, Shu Yang, Xiaofei Wang</dc:creator>
    </item>
    <item>
      <title>Alignment of Continuous Brain Connectivity</title>
      <link>https://arxiv.org/abs/2503.15830</link>
      <description>arXiv:2503.15830v1 Announce Type: new 
Abstract: Brain networks are typically represented by adjacency matrices, where each node corresponds to a brain region. In traditional brain network analysis, nodes are assumed to be matched across individuals, but the methods used for node matching often overlook the underlying connectivity information. This oversight can result in inaccurate node alignment, leading to inflated edge variability and reduced statistical power in downstream connectivity analyses. To overcome this challenge, we propose a novel framework for registering high resolution continuous connectivity (ConCon), defined as a continuous function on a product manifold space specifically, the cortical surface capturing structural connectivity between all pairs of cortical points. Leveraging ConCon, we formulate an optimal diffeomorphism problem to align both connectivity profiles and cortical surfaces simultaneously. We introduce an efficient algorithm to solve this problem and validate our approach using data from the Human Connectome Project (HCP). Results demonstrate that our method substantially improves the accuracy and robustness of connectome-based analyses compared to existing techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15830v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Cole, Yang Xiang, Will Consagra, Anuj Srivastava, Xing Qiu, Zhengwu Zhang</dc:creator>
    </item>
    <item>
      <title>Integrative Analysis of High-dimensional RCT and RWD Subject to Censoring and Hidden Confounding</title>
      <link>https://arxiv.org/abs/2503.15967</link>
      <description>arXiv:2503.15967v1 Announce Type: new 
Abstract: In this study, we focus on estimating the heterogeneous treatment effect (HTE) for survival outcome. The outcome is subject to censoring and the number of covariates is high-dimensional. We utilize data from both the randomized controlled trial (RCT), considered as the gold standard, and real-world data (RWD), possibly affected by hidden confounding factors. To achieve a more efficient HTE estimate, such integrative analysis requires great insight into the data generation mechanism, particularly the accurate characterization of unmeasured confounding effects/bias. With this aim, we propose a penalized-regression-based integrative approach that allows for the simultaneous estimation of parameters, selection of variables, and identification of the existence of unmeasured confounding effects. The consistency, asymptotic normality, and efficiency gains are rigorously established for the proposed estimate.
  Finally, we apply the proposed method to estimate the HTE of lobar/sublobar resection on the survival of lung cancer patients. The RCT is a multicenter non-inferiority randomized phase 3 trial, and the RWD comes from a clinical oncology cancer registry in the United States. The analysis reveals that the unmeasured confounding exists and the integrative approach does enhance the efficiency for the HTE estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15967v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Ye, Shu Yang, Xiaofei Wang, Yanyan Liu</dc:creator>
    </item>
    <item>
      <title>Outcome-Informed Weighting for Robust ATE Estimation</title>
      <link>https://arxiv.org/abs/2503.15989</link>
      <description>arXiv:2503.15989v1 Announce Type: new 
Abstract: Reliable causal effect estimation from observational data requires adjustment for confounding and sufficient overlap in covariate distributions between treatment groups. However, in high-dimensional settings, lack of overlap often inflates the variance and weakens the robustness of inverse propensity score weighting (IPW) based estimators. Although many approaches that rely on covariate adjustment have been proposed to mitigate these issues, we instead shift the focus to the outcome space. In this paper, we introduce the Augmented Marginal outcome density Ratio (AMR) estimator, an outcome-informed weighting method that naturally filters out irrelevant information, alleviates practical positivity violations and outperforms standard augmented IPW and covariate adjustment-based methods in terms of both efficiency and robustness. Additionally, by eliminating the need for strong a priori assumptions, our post-hoc calibration framework is also effective in settings with high-dimensional covariates. We present experimental results on synthetic data, the NHANES dataset and text applications, demonstrating the robustness of AMR and its superior performance under weak overlap and high-dimensional covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15989v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linying Yang, Robin J. Evans</dc:creator>
    </item>
    <item>
      <title>Weighted Average Ensemble for Cholesky-based Covariance Matrix Estimation</title>
      <link>https://arxiv.org/abs/2503.15991</link>
      <description>arXiv:2503.15991v1 Announce Type: new 
Abstract: The modified Cholesky decomposition (MCD) is an efficient technique for estimating a covariance matrix. However, it is known that the MCD technique often requires a pre-specified variable ordering in the estimation procedure. In this work, we propose a weighted average ensemble covariance estimation for high-dimensional data based on the MCD technique. It can flexibly accommodate the high-dimensional case and ensure the positive definiteness property of the resultant estimate. Our key idea is to obtain different weights for different candidate estimates by minimizing an appropriate risk function with respect to the Frobenius norm. Different from the existing ensemble estimation based on the MCD, the proposed method provides a sparse weighting scheme such that one can distinguish which variable orderings employed in the MCD are useful for the ensemble matrix estimate. The asymptotically theoretical convergence rate of the proposed ensemble estimate is established under regularity conditions. The merits of the proposed method are examined by the simulation studies and a portfolio allocation example of real stock data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15991v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Statistical Theory and Related Fields 2025</arxiv:journal_reference>
      <dc:creator>Xiaoning Kang, Zhenguo Gao, Xi Liang, Xinwei Deng</dc:creator>
    </item>
    <item>
      <title>Feedback-augmented Non-homogeneous Hidden Markov Models for Longitudinal Causal Inference</title>
      <link>https://arxiv.org/abs/2503.16014</link>
      <description>arXiv:2503.16014v1 Announce Type: new 
Abstract: Hidden Markov models are widely used for modeling sequential data but typically have limited applicability in observational causal inference due to their strong conditional independence assumptions. I introduce feedback-augmented non-homogeneous hidden Markov model (FAN-HMM), which incorporate time-varying covariates and feedback mechanisms from past observations to latent states and future responses. Integrating these models with the structural causal model framework allows flexible causal inference in longitudinal data with time-varying unobserved heterogeneity and multiple causal pathways. I show how, in a common case of categorical response variables, long-term causal effects can be estimated efficiently without the need for simulating counterfactual trajectories. Using simulation experiments, I study the performance of FAN-HMM under the common misspecification of the number of latent states, and finally apply the proposed approach to estimate the effect of the 2013 parental leave reform on fathers' paternal leave uptake in Finnish workplaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16014v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jouni Helske</dc:creator>
    </item>
    <item>
      <title>Cognitive factor-based selection increases power in Alzheimer's dementia randomized clinical trials</title>
      <link>https://arxiv.org/abs/2503.16044</link>
      <description>arXiv:2503.16044v1 Announce Type: new 
Abstract: Alzheimer's dementia (AD) is of increasing concern as populations achieve longer lifespans. Many of the recent failed AD clinical trials recruiting cognitively intact individuals had a low number of AD events and were thus underpowered. Previous trials have attempted to address this issue by requiring signs of cognitive decline in brain imaging for trial enrollment. However, this method systematically excludes people of color and those without access to healthcare and results in a selected sample that is not representative of the target population. We therefore propose the use of a predictive model based on cognitive test scores to enroll cognitively normal yet high risk participants in a hypothetical clinical trial. Cognitive test scores are a widely accessible tool, so their use in enrollment would be less likely to exclude marginalized populations than biomarkers (such as imaging), which are overwhelmingly available to exclusively high-income patients. We developed a novel longitudinal factor model to predict AD conversion within a 3-year window based on data from the National Alzheimer's Coordinating Center. Through simulation, we demonstrate that our predictive model provides substantial improvements in statistical power and required sample size in hypothetical clinical trials across a range of drug effects compared to other methods of subject selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16044v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julia Gallini, Zach Baucom, Yorghos Tripodis</dc:creator>
    </item>
    <item>
      <title>On prior smoothing with discrete spatial data in the context of disease mapping</title>
      <link>https://arxiv.org/abs/2503.16151</link>
      <description>arXiv:2503.16151v1 Announce Type: new 
Abstract: Disease mapping attempts to explain observed health event counts across areal units, typically using Markov random field models. These models rely on spatial priors to account for variation in raw relative risk or rate estimates. Spatial priors introduce some degree of smoothing, wherein, for any particular unit, empirical risk or incidence estimates are either adjusted towards a suitable mean or incorporate neighbor-based smoothing. While model explanation may be the primary focus, the literature lacks a comparison of the amount of smoothing introduced by different spatial priors. Additionally, there has been no investigation into how varying the parameters of these priors influences the resulting smoothing. This study examines seven commonly used spatial priors through both simulations and real data analyses. Using areal maps of peninsular Spain and England, we analyze smoothing effects with two datasets with associated populations at risk. We propose empirical metrics to quantify the smoothing achieved by each model and theoretical metrics to calibrate the expected extent of smoothing as a function of model parameters. We employ areal maps in order to quantitatively characterize the extent of smoothing within and across the models as well as to link the theoretical metrics to the empirical metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16151v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Garazi Retegui, Alan E. Gelfand, Jaione Etxeberria, Mar\'ia Dolores Ugarte</dc:creator>
    </item>
    <item>
      <title>Balancing the effective sample size in prior across different doses in the curve-free Bayesian decision-theoretic design for dose-finding trials</title>
      <link>https://arxiv.org/abs/2503.16321</link>
      <description>arXiv:2503.16321v1 Announce Type: new 
Abstract: The primary goal of dose allocation in phase I trials is to minimize patient exposure to subtherapeutic or excessively toxic doses, while accurately recommending a phase II dose that is as close as possible to the maximum tolerated dose (MTD). Fan et al. (2012) introduced a curve-free Bayesian decision-theoretic design (CFBD), which leverages the assumption of a monotonic dose-toxicity relationship without directly modeling dose-toxicity curves. This approach has also been extended to drug combinations for determining the MTD (Lee et al., 2017). Although CFBD has demonstrated improved trial efficiency by using fewer patients while maintaining high accuracy in identifying the MTD, it may artificially inflate the effective sample sizes for the updated prior distributions, particularly at the lowest and highest dose levels. This can lead to either overshooting or undershooting the target dose. In this paper, we propose a modification to CFBD's prior distribution updates that balances effective sample sizes across different doses. Simulation results show that with the modified prior specification, CFBD achieves a more focused dose allocation at the MTD and offers more precise dose recommendations with fewer patients on average. It also demonstrates robustness to other well-known dose finding designs in literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16321v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiapeng Xu, Dehua Bi, Shenghua Kelly Fan, Bee Leng Lee, Ying Lu</dc:creator>
    </item>
    <item>
      <title>Heterogeneous Treatment Effects in Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2503.13696</link>
      <description>arXiv:2503.13696v1 Announce Type: cross 
Abstract: Empirical studies using Regression Discontinuity (RD) designs often explore heterogeneous treatment effects based on pretreatment covariates. However, the lack of formal statistical methods has led to the widespread use of ad hoc approaches in applications. Motivated by common empirical practice, we develop a unified, theoretically grounded framework for RD heterogeneity analysis. We show that a fully interacted local linear (in functional parameters) model effectively captures heterogeneity while still being tractable and interpretable in applications. The model structure holds without loss of generality for discrete covariates, while for continuous covariates our proposed (local functional linear-in-parameters) model can be potentially restrictive, but it nonetheless naturally matches standard empirical practice and offers a causal interpretation for RD applications. We establish principled bandwidth selection and robust bias-corrected inference methods to analyze heterogeneous treatment effects and test group differences. We provide companion software to facilitate implementation of our results. An empirical application illustrates the practical relevance of our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13696v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebastian Calonico, Matias D. Cattaneo, Max H. Farrell, Filippo Palomba, Rocio Titiunik</dc:creator>
    </item>
    <item>
      <title>Distribution of Deep Gaussian process Gradients and Sequential Design for Simulators with Sharp Variations</title>
      <link>https://arxiv.org/abs/2503.16027</link>
      <description>arXiv:2503.16027v1 Announce Type: cross 
Abstract: Deep Gaussian Processes (DGPs), multi-layered extensions of GPs, better emulate simulators with regime transitions or sharp changes than standard GPs. Gradient information is crucial for tasks like sensitivity analysis and dimension reduction. Although gradient posteriors are well-defined in GPs, extending them to DGPs is challenging due to their hierarchical structure. We propose a novel method to approximate the DGP emulator's gradient distribution, enabling efficient gradient computation with uncertainty quantification (UQ). Our approach derives an analytical gradient mean and the covariance. The numerical results show that our method outperforms GP and DGP with finite difference methods in gradient accuracy, offering the extra unique benefit of UQ. Based on the gradient information, we further propose a sequential design criterion to identify the sharp variation regions efficiently, with the gradient norm as a key indicator whose distribution can be readily evaluated in our framework. We evaluated the proposed sequential design using synthetic examples and empirical applications, demonstrating its superior performance in emulating functions with sharp changes compared to existing design methods. The DGP gradient computation is seamlessly integrated into the advanced Python package dgpsi for DGP emulation, along with the proposed sequential design available at https://github.com/yyimingucl/DGP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16027v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiming Yang, Deyu Ming, Serge Guillas</dc:creator>
    </item>
    <item>
      <title>On the efficiency-loss free ordering-robustness of product-PCA</title>
      <link>https://arxiv.org/abs/2302.11124</link>
      <description>arXiv:2302.11124v2 Announce Type: replace 
Abstract: This article studies the robustness of the eigenvalue ordering, an important issue when estimating the leading eigen-subspace by principal component analysis (PCA). In Yata and Aoshima (2010), cross-data-matrix PCA (CDM-PCA) was proposed and shown to have smaller bias than PCA in estimating eigenvalues. While CDM-PCA has the potential to achieve better estimation of the leading eigen-subspace than the usual PCA, its robustness is not well recognized. In this article, we first develop a more stable variant of CDM-PCA, which we call product-PCA (PPCA), that provides a more convenient formulation for theoretical investigation. Secondly, we prove that, in the presence of outliers, PPCA is more robust than PCA in maintaining the correct ordering of leading eigenvalues. The robustness gain in PPCA comes from the random data partition, and it does not rely on a data down-weighting scheme as most robust statistical methods do. This enables us to establish the surprising finding that, when there are no outliers, PPCA and PCA share the same asymptotic distribution. That is, the robustness gain of PPCA in estimating the leading eigen-subspace has no efficiency loss in comparison with PCA. Simulation studies and a face data example are presented to show the merits of PPCA. In conclusion, PPCA has a good potential to replace the role of the usual PCA in real applications whether outliers are present or not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.11124v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hung Hung, Su-Yun Huang</dc:creator>
    </item>
    <item>
      <title>General Bayesian inference for causal effects using covariate balancing procedure</title>
      <link>https://arxiv.org/abs/2404.09414</link>
      <description>arXiv:2404.09414v3 Announce Type: replace 
Abstract: In observational studies, the propensity score plays a central role in estimating causal effects of interest. The inverse probability weighting (IPW) estimator is commonly used for this purpose. However, if the propensity score model is misspecified, the IPW estimator may produce biased estimates of causal effects. Previous studies have proposed some robust propensity score estimation procedures. However, these methods require considering parameters that dominate the uncertainty of sampling and treatment allocation. This study proposes a novel Bayesian estimating procedure that necessitates probabilistically deciding the parameter, rather than deterministically. Since the IPW estimator and propensity score estimator can be derived as solutions to certain loss functions, the general Bayesian paradigm, which does not require the considering the full likelihood, can be applied. Therefore, our proposed method only requires the same level of assumptions as ordinary causal inference contexts. The proposed Bayesian method demonstrates equal or superior results compared to some previous methods in simulation experimentss, and is also applied to real data, namely the Whitehall dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09414v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunichiro Orihara, Tomotaka Momozaki, Tomoyuki Nakagawa</dc:creator>
    </item>
    <item>
      <title>Expected Information Gain Estimation via Density Approximations: Sample Allocation and Dimension Reduction</title>
      <link>https://arxiv.org/abs/2411.08390</link>
      <description>arXiv:2411.08390v2 Announce Type: replace 
Abstract: Computing expected information gain (EIG) from prior to posterior (equivalently, mutual information between candidate observations and model parameters or other quantities of interest) is a fundamental challenge in Bayesian optimal experimental design. We formulate flexible transport-based schemes for EIG estimation in general nonlinear/non-Gaussian settings, compatible with both standard and implicit Bayesian models. These schemes are representative of two-stage methods for estimating or bounding EIG using marginal and conditional density estimates. In this setting, we analyze the optimal allocation of samples between training (density estimation) and approximation of the outer prior expectation. We show that with this optimal sample allocation, the MSE of the resulting EIG estimator converges more quickly than that of a standard nested Monte Carlo scheme. We then address the estimation of EIG in high dimensions, by deriving gradient-based upper bounds on the mutual information lost by projecting the parameters and/or observations to lower-dimensional subspaces. Minimizing these upper bounds yields projectors and hence low-dimensional EIG approximations that outperform approximations obtained via other linear dimension reduction schemes. Numerical experiments on a PDE-constrained Bayesian inverse problem also illustrate a favorable trade-off between dimension truncation and the modeling of non-Gaussianity, when estimating EIG from finite samples in high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08390v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fengyi Li, Ricardo Baptista, Youssef Marzouk</dc:creator>
    </item>
    <item>
      <title>Prior-Posterior Derived-Predictive Consistency Checks for Post-Estimation Calculated Quantities of Interest (QOI-Check)</title>
      <link>https://arxiv.org/abs/2412.15809</link>
      <description>arXiv:2412.15809v2 Announce Type: replace 
Abstract: With flexible modeling software - such as the probabilistic programming language Stan - growing in popularity, quantities of interest (QOIs) calculated post-estimation are increasingly desired and customly implemented, both by statistical software developers and applied scientists. Examples of QOI include the marginal expectation of a multilevel model with a non-linear link function, or an ANOVA decomposition of a bivariate regression spline. For this, the QOI-Check is introduced, a systematic approach to ensure proper calibration and correct interpretation of QOIs. It contributes to Bayesian Workflow, and aims to improve the interpretability and trust in post-estimation conclusions based on QOIs. The QOI-Check builds upon Simulation Based Calibration (SBC), and the Holdout Predictive Check (HPC). SBC verifies computational reliability of Bayesian inference algorithms by consistency check of posterior with prior when the posterior is estimated on prior-predicted data, while HPC ensures robust inference by assessing consistency of model predictions with holdout data. SBC and HPC are combined in QOI-Checking for validating post-estimation QOI calculation and interpretation in the context of a (hypothetical) population definition underlying the QOI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15809v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Holger Sennhenn-Reulen</dc:creator>
    </item>
    <item>
      <title>Statistical modeling of categorical trajectories with multivariate functional principal components</title>
      <link>https://arxiv.org/abs/2502.09986</link>
      <description>arXiv:2502.09986v2 Announce Type: replace 
Abstract: There are many examples in which the statistical units of interest are samples of a continuous time categorical random process, that is to say a continuous time stochastic process taking values in a finite state space. Without loosing any information, we associate to each state a binary random function, taking values in $\{0,1\}$, and turn the problem of statistical modeling of a categorical process into a multivariate functional data analysis issue. The (multivariate) covariance operator has nice interpretations in terms of departure from independence of the joint probabilities and the multivariate functional principal components are simple to interpret. Under the weak hypothesis assuming only continuity in probability of the $0-1$ trajectories, it is simple to build consistent estimators of the covariance kernel and perform multivariate functional principal components analysis. The sample paths being piecewise constant, with a finite number of jumps, this a rare case in functional data analysis in which the trajectories are not supposed to be continuous and can be observed exhaustively. The approach is illustrated on a data set of sensory perceptions, considering different gustometer-controlled stimuli experiments. We also show how it can be easily extended to analyze experiments, such as temporal check-all-that-apply, in which two states or more can be observed at the same time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09986v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Herv\'e Cardot, Caroline Peltier</dc:creator>
    </item>
    <item>
      <title>Proposal for the Application of Fractional Operators in Polynomial Regression Models to Enhance the Determination Coefficient $R^2$ on Unseen Data</title>
      <link>https://arxiv.org/abs/2503.11749</link>
      <description>arXiv:2503.11749v2 Announce Type: replace 
Abstract: Since polynomial regression models are generally quite reliable for data with a linear trend, it is important to note that, in some cases, they may encounter overfitting issues during the training phase, which could result in negative values of the coefficient of determination $R^2$ for unseen data. For this reason, this work proposes the partial implementation of fractional operators in polynomial regression models to generate a fractional regression model. The goal of this proposal is to attempt to mitigate overfitting, which could improve the value of the coefficient of determination for unseen data, compared to the polynomial model, under the assumption that this would contribute to generating predictive models with better performance. The methodology for constructing these fractional regression models is detailed, and examples applicable to both Riemann-Liouville and Caputo fractional operators are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11749v2</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anthony Torres-Hernandez</dc:creator>
    </item>
    <item>
      <title>Comparison study of variable selection procedures in high-dimensional Gaussian linear regression</title>
      <link>https://arxiv.org/abs/2109.12006</link>
      <description>arXiv:2109.12006v3 Announce Type: replace-cross 
Abstract: We propose an extensive simulation study to compare some variable selection procedures in a high-dimensional framework. Assuming that the relationship between the actives variables and the response variable is linear, the high-dimensional Gaussian linear regression provides a relevant statistical framework to identify active variables related to the response variable. Many variable selection procedures exist, and in this article, we focus on methods based on regularization paths. We perform a comparison study by considering different simulation settings with various dependency structures for variables and evaluate the performance of the methods by computing several metrics. As expected, no method is optimal for all the evaluated performances but we provide recommendations for the best procedures according to the metric to control. Lastly, we test the importance of some assumptions of the model, especially the high dimensionality and the Gaussian ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.12006v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Perrine Lacroix, M\'elina Gallopin, Marie-Laure Martin</dc:creator>
    </item>
    <item>
      <title>Robust mean change point testing in high-dimensional data with heavy tails</title>
      <link>https://arxiv.org/abs/2305.18987</link>
      <description>arXiv:2305.18987v3 Announce Type: replace-cross 
Abstract: We study mean change point testing problems for high-dimensional data, with exponentially- or polynomially-decaying tails. In each case, depending on the $\ell_0$-norm of the mean change vector, we separately consider dense and sparse regimes. We characterise the boundary between the dense and sparse regimes under the above two tail conditions for the first time in the change point literature and propose novel testing procedures that attain optimal rates in each of the four regimes up to a poly-iterated logarithmic factor. By comparing with previous results under Gaussian assumptions, our results quantify the costs of heavy-tailedness on the fundamental difficulty of change point testing problems for high-dimensional data.
  To be specific, when the error distributions possess exponentially-decaying tails, a CUSUM-type statistic is shown to achieve a minimax testing rate up to $\sqrt{\log\log(8n)}$. As for polynomially-decaying tails, admitting bounded $\alpha$-th moments for some $\alpha \geq 4$, we introduce a median-of-means-type test statistic that achieves a near-optimal testing rate in both dense and sparse regimes. In the sparse regime, we further propose a computationally-efficient test to achieve optimality. Our investigation in the even more challenging case of $2 \leq \alpha &lt; 4$, unveils a new phenomenon that the minimax testing rate has no sparse regime, i.e.\ testing sparse changes is information-theoretically as hard as testing dense changes. Finally, we consider various extensions where we also obtain near-optimal performances, including testing against multiple change points, allowing temporal dependence as well as fewer than two finite moments in the data generating mechanisms. We also show how sub-Gaussian rates can be achieved when an additional minimal spacing condition is imposed under the alternative hypothesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.18987v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengchu Li, Yudong Chen, Tengyao Wang, Yi Yu</dc:creator>
    </item>
    <item>
      <title>A Shrinkage Likelihood Ratio Test for High-Dimensional Subgroup Analysis with a Logistic-Normal Mixture Model</title>
      <link>https://arxiv.org/abs/2307.10272</link>
      <description>arXiv:2307.10272v4 Announce Type: replace-cross 
Abstract: In subgroup analysis, testing the existence of a subgroup with a differential treatment effect serves as protection against spurious subgroup discovery. Despite its importance, this hypothesis testing possesses a complicated nature: parameter characterizing subgroup classification is not identified under the null hypothesis of no subgroup. Due to this irregularity, the existing methods have the following two limitations. First, the asymptotic null distribution of test statistics often takes an intractable form, which necessitates computationally demanding resampling methods to calculate the critical value. Second, the dimension of personal attributes characterizing subgroup membership is not allowed to be of high dimension. To solve these two problems simultaneously, this study develops a shrinkage likelihood ratio test for the existence of a subgroup using a logistic-normal mixture model. The proposed test statistics are built on a modified likelihood function that shrinks possibly high-dimensional unidentified parameters toward zero under the null hypothesis while retaining power under the alternative. This shrinkage helps handle the irregularity and restore the simple chi-square-type asymptotics even under the high-dimensional regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10272v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shota Takeishi</dc:creator>
    </item>
    <item>
      <title>Distributionally Robust Learning for Multi-source Unsupervised Domain Adaptation</title>
      <link>https://arxiv.org/abs/2309.02211</link>
      <description>arXiv:2309.02211v4 Announce Type: replace-cross 
Abstract: Empirical risk minimization often performs poorly when the distribution of the target domain differs from those of source domains. To address such potential distribution shifts, we develop an unsupervised domain adaptation approach that leverages labeled data from multiple source domains and unlabeled data from the target domain. We introduce a distributionally robust model that optimizes an adversarial reward based on the explained variance across a class of target distributions, ensuring generalization to the target domain. We show that the proposed robust model is a weighted average of conditional outcome models from source domains. This formulation allows us to compute the robust model through the aggregation of source models, which can be estimated using various machine learning algorithms of the users' choice, such as random forests, boosting, and neural networks. Additionally, we introduce a bias-correction step to obtain a more accurate aggregation weight, which is effective for various machine learning algorithms. Our framework can be interpreted as a distributionally robust federated learning approach that satisfies privacy constraints while providing insights into the importance of each source for prediction on the target domain. The performance of our method is evaluated on both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.02211v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenyu Wang, Peter B\"uhlmann, Zijian Guo</dc:creator>
    </item>
    <item>
      <title>Asymptotically Optimal Sequential Multiple Testing Procedures for Correlated Normal</title>
      <link>https://arxiv.org/abs/2309.16657</link>
      <description>arXiv:2309.16657v2 Announce Type: replace-cross 
Abstract: Simultaneous statistical inference has been a cornerstone in the statistics methodology literature because of its fundamental theory and paramount applications. The mainstream multiple testing literature has traditionally considered two frameworks: the sample size is deterministic, and the test statistics corresponding to different tests are independent. However, in many modern scientific avenues, these assumptions are often violated. There is little study that explores the multiple testing problem in a sequential framework where the test statistics corresponding to the various streams are dependent. This work fills this gap in a unified way by considering the classical means-testing problem in an equicorrelated Gaussian and sequential framework. We focus on sequential test procedures that control the type I and type II familywise error probabilities at pre-specified levels. We establish that our proposed test procedures achieve the optimal expected sample sizes under every possible signal configuration asymptotically, as the two error probabilities vanish at arbitrary rates. Towards this, we elucidate that the ratio of the expected sample size of our proposed rule and that of the classical SPRT goes to one asymptotically, thus illustrating their connection. Generalizing this, we show that our proposed procedures, with appropriately adjusted critical values, are asymptotically optimal for controlling any multiple testing error metric lying between multiples of FWER in a certain sense. This class of metrics includes FDR/FNR, pFDR/pFNR, the per-comparison and per-family error rates, and the false positive rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.16657v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Monitirtha Dey, Subir Kumar Bhandari</dc:creator>
    </item>
    <item>
      <title>An Efficient Permutation-Based Kernel Two-Sample Test</title>
      <link>https://arxiv.org/abs/2502.13570</link>
      <description>arXiv:2502.13570v2 Announce Type: replace-cross 
Abstract: Two-sample hypothesis testing-determining whether two sets of data are drawn from the same distribution-is a fundamental problem in statistics and machine learning with broad scientific applications. In the context of nonparametric testing, maximum mean discrepancy (MMD) has gained popularity as a test statistic due to its flexibility and strong theoretical foundations. However, its use in large-scale scenarios is plagued by high computational costs. In this work, we use a Nystr\"om approximation of the MMD to design a computationally efficient and practical testing algorithm while preserving statistical guarantees. Our main result is a finite-sample bound on the power of the proposed test for distributions that are sufficiently separated with respect to the MMD. The derived separation rate matches the known minimax optimal rate in this setting. We support our findings with a series of numerical experiments, emphasizing realistic scientific data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13570v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antoine Chatalic, Marco Letizia, Nicolas Schreuder, Lorenzo Rosasco</dc:creator>
    </item>
  </channel>
</rss>
