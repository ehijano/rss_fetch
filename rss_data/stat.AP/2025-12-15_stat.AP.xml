<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Dec 2025 05:00:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>umx version 4.5: Extending Twin and Path-Based SEM in R with CLPM, MR-DoC, Definition Variables, $\Omega$nyx Integration, and Censored distributions</title>
      <link>https://arxiv.org/abs/2512.11063</link>
      <description>arXiv:2512.11063v1 Announce Type: new 
Abstract: Structural Equation Modeling (SEM) provides a powerful and flexible framework widely used in behavioral genetics and social sciences. Building on the original design of the umx package, which enhanced accessibility to OpenMx using concise syntax and helpful defaults, umx v4.5 significantly extends functionality for longitudinal and causal twin designs while improving interoperability with graphical modelling tools such as Onyx. New capabilities include: classic and modern cross-lagged panel model; Mendelian Randomization Direction-of-Causation (MR-DoC) twin models incorporating polygenic scores as instruments; expanded support for definition variables directly in umxRAM(); streamlined workflows for importing paths from $\Omega$nyx; a dedicated tool for analyzing censored variables, particularly valuable in biomarker research; improved covariate placeholder handling for definition variables; umxSexLim() for simplified sex-limitation modelling across five twin groups, accommodating quantitative and qualitative sex differences; and umx_residualize() for efficient covariate residualization in wide- or long-format data. These advances accelerate reproducible, reliable, publication-ready twin and family modelling using intelligent defaults, and integrated journal-quality reporting, thereby lowering barriers to genetic epidemiological analyzes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11063v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luis FS Castro-de-Araujo, Nathan Gillespie, Michael C Neale, Timothy Bates</dc:creator>
    </item>
    <item>
      <title>A Unified Micro-Model for Loss Reserves, IBNR and Unearned Premium Risk with Dependence, Inflation, and Discounting</title>
      <link>https://arxiv.org/abs/2512.11197</link>
      <description>arXiv:2512.11197v1 Announce Type: new 
Abstract: This paper introduces a unified micro-level stochastic framework for the joint modeling of loss reserves (RBNS), incurred but not reported (IBNR) reserves, and unearned premium risk under dependence, inflation, and discounting. The proposed framework accommodates interactions between indemnities, expenses, reporting delays, and settlement delays, while allowing for flexible parametric dependence structures and dynamic financial adjustments. An Aggregate Trend Renewal Process (ATRP) is used as one possible implementation of the joint model for payments, expenses, and delays; however, the methodological contribution of the paper lies in the unified micro-level reserving architecture rather than in the ATRP itself. The framework produces forward-looking reserve and premium risk measures with direct applications to pricing, reserving, and capital management.
  We implement the framework using an aggregate trend renewal process at the individual claim level, which can be applied to the usual run-off triangle to obtain predictions for each accident-development year. Closed-form expressions for the first two raw and joint conditional moments of predicted payments are derived, together with approximations of their distribution functions. A detailed case study on medical malpractice insurance illustrates the practical relevance of the approach and its calibration on real-world data. We also investigate data heterogeneity, parameter uncertainty, distributional approximations, premium risk, UPR sensitivity to operational delays and inflation, and risk capital implications under alternative assumptions. The results highlight the advantages of unified micro-level modeling for dynamic liability and premium risk assessment in long-tailed lines of business.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11197v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emmanuel Hamel, Anas Abdallah, Ghislain L\'eveill\'e</dc:creator>
    </item>
    <item>
      <title>Neural Network-based Partial-Linear Single-Index Models for Environmental Mixtures Analysis</title>
      <link>https://arxiv.org/abs/2512.11593</link>
      <description>arXiv:2512.11593v1 Announce Type: new 
Abstract: Evaluating the health effects of complex environmental mixtures remains a central challenge in environmental health research. Existing approaches vary in their flexibility, interpretability, scalability, and support for diverse outcome types, often limiting their utility in real-world applications. To address these limitations, we propose a neural network-based partial-linear single-index (NeuralPLSI) modeling framework that bridges semiparametric regression modeling interpretability with the expressive power of deep learning. The NeuralPLSI model constructs an interpretable exposure index via a learnable projection and models its relationship with the outcome through a flexible neural network. The framework accommodates continuous, binary, and time-to-event outcomes, and supports inference through a bootstrap-based procedure that yields confidence intervals for key model parameters. We evaluated NeuralPLSI through simulation studies under a range of scenarios and applied it to data from the National Health and Nutrition Examination Survey (NHANES) to demonstrate its practical utility. Together, our contributions establish NeuralPLSI as a scalable, interpretable, and versatile modeling tool for mixture analysis. To promote adoption and reproducibility, we release a user-friendly open-source software package that implements the proposed methodology and supports downstream visualization and inference (\texttt{https://github.com/hyungrok-do/NeuralPLSI}).</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11593v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyungrok Do, Yuyan Wang, Mengling Liu, Myeonggyun Lee</dc:creator>
    </item>
    <item>
      <title>Euclidean Ideal Point Estimation From Roll-Call Data via Distance-Based Bipartite Network Models</title>
      <link>https://arxiv.org/abs/2512.11610</link>
      <description>arXiv:2512.11610v1 Announce Type: new 
Abstract: Conventional ideal point models rely on Gaussian or quadratic utility functions that violate the triangle inequality, producing non-metric distances that complicate geometric interpretation and undermine clustering and dispersion-based analyses. We introduce a distance-based alternative that adapts the Latent Space Item Response Model (LSIRM) to roll-call data, treating legislators and bills as nodes in a bipartite network jointly embedded in a Euclidean metric space. Through controlled simulations, Euclidean LSIRM consistently recovers latent coalition structure with superior cluster separation relative to existing methods. Applied to the 118th U.S. House, the model improves vote prediction and yields bill embeddings that clarify cross-cutting issue alignments. The results show that restoring metric structure to ideal point estimation provides a clearer and more coherent inference about party cohesion, factional divisions, and multidimensional legislative behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11610v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seungju Lee, In Kyun Kim, Jong Hee Park, Ick Hoon Jin</dc:creator>
    </item>
    <item>
      <title>Dynamic Conditional SKEPTIC</title>
      <link>https://arxiv.org/abs/2512.11648</link>
      <description>arXiv:2512.11648v1 Announce Type: new 
Abstract: We introduce the Dynamic Conditional SKEPTIC (DCS), a semiparametric approach for efficiently and robustly estimating time-varying correlations in multivariate models. We exploit nonparametric rank-based statistics, namely Spearman's rho and Kendall's tau, to estimate the unknown correlation matrix and discuss the stationarity, beta- and rho- mixing conditions of the model. We illustrate the methodology by estimating the time-varying conditional correlation matrix of the stocks included in the S&amp;P100 and S&amp;P500 during the period from 02/01/2013 to 23/01/2025. The results show that DCS improves diagnostic checks compared to the classical Dynamic Conditional Correlation (DCC) models, providing uncorrelated and normally distributed residuals. A risk management application shows that global minimum variance portfolios estimated using the DCS model exhibit lower turnover than those based on the DCC and DCC-NL models, while also achieving higher Sharpe ratios for portfolios constructed from S&amp;P 100 constituents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11648v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriele Di Luzio, Giacomo Morelli</dc:creator>
    </item>
    <item>
      <title>Maritime Vessel Tracking</title>
      <link>https://arxiv.org/abs/2512.11707</link>
      <description>arXiv:2512.11707v1 Announce Type: new 
Abstract: The Automatic Identification System (AIS) provides time stamped vessel positions and kinematic reports that enable maritime authorities to monitor traffic. We consider the problem of relabeling AIS trajectories when vessel identifiers are missing, focusing on a challenging nationwide setting in which tracks are heavily downsampled and span diverse operating environments across continental U.S. waters. We propose a hybrid pipeline that first applies a physics-based screening step to project active track endpoints forward in time and select a small set of plausible ancestors for each new observation. A supervised neural classifier then chooses among these candidates, or initiates a new track, using engineered space time and kinematic consistency features. On held out data, this approach improves posit accuracy relative to unsupervised baselines, demonstrating that combining simple motion models with learned disambiguation can scale vessel relabeling to heterogeneous, high volume AIS streams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11707v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Mahlon Scott, Hsin-Hsiung Huang</dc:creator>
    </item>
    <item>
      <title>Rice Price Dynamics during the 1945--1947 Famine in Post-War Taiwan: A Quantitative Reassessment</title>
      <link>https://arxiv.org/abs/2512.07492</link>
      <description>arXiv:2512.07492v1 Announce Type: cross 
Abstract: We compiled the first high-frequency rice price panel for Taiwan from August 1945 to March 1947, during the transition from Japanese rule to China rule. Using regression models, we found that the pattern of rice price changes could be divided into four stages, each with distinct characteristics. Based on different stages, we combined the policies formulated by the Taiwan government at the time to demonstrate the correlation between rice prices and policies. The research results highlight the dominant role of policy systems in post-war food crises.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07492v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Huaide Chen, Hailiang Yang</dc:creator>
    </item>
    <item>
      <title>Applying NLP to iMessages: Understanding Topic Avoidance, Responsiveness, and Sentiment</title>
      <link>https://arxiv.org/abs/2512.11079</link>
      <description>arXiv:2512.11079v1 Announce Type: cross 
Abstract: What is your messaging data used for? While many users do not often think about the information companies can gather based off of their messaging platform of choice, it is nonetheless important to consider as society increasingly relies on short-form electronic communication. While most companies keep their data closely guarded, inaccessible to users or potential hackers, Apple has opened a door to their walled-garden ecosystem, providing iMessage users on Mac with one file storing all their messages and attached metadata. With knowledge of this locally stored file, the question now becomes: What can our data do for us? In the creation of our iMessage text message analyzer, we set out to answer five main research questions focusing on topic modeling, response times, reluctance scoring, and sentiment analysis. This paper uses our exploratory data to show how these questions can be answered using our analyzer and its potential in future studies on iMessage data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11079v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alan Gerber, Sam Cooperman</dc:creator>
    </item>
    <item>
      <title>Gaussian random field's anisotropy using excursion sets</title>
      <link>https://arxiv.org/abs/2512.11085</link>
      <description>arXiv:2512.11085v1 Announce Type: cross 
Abstract: This paper addresses the problem of detecting and estimating the anisotropy of a stationary real-valued random field from a single realization of one of its excursion sets. This setting is challenging as it relies on observing a binary image without prior knowledge of the field's mean, variance, or the specific threshold value.
  Our first contribution is to propose a generalization of Caba\~na's contour method to arbitrary dimensions by analyzing the Palm distribution of normal vectors along the excursion set boundaries. We demonstrate that the anisotropy parameters can be recovered by solving a smooth and strongly convex optimization problem involving the eigenvalues of the empirical covariance matrix of these normal vectors.
  Our second main contribution is a new, model-agnostic statistical test for isotropy in dimension two. We introduce a statistic based on the contour method which is asymptotically distributed as a chi-squared variable with two degrees of freedom under the null hypothesis of quasi-isotropy. Unlike existing methods based on Lipschitz-Killing curvatures, this procedure does not require knowledge of the random field's covariance structure.
  Extensive numerical experiments show that our test is well-calibrated and more powerful than model-based alternatives as well as that the estimation of the anisotropy parameters, including the directions, is robust and efficient. Finally, we apply this framework to test the quasi-isotropy of the Cosmic Microwave Background (CMB) using the Planck data release 3 mission.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11085v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jean-Marc Aza\"is, Federico Dalmao, Yohann De Castro</dc:creator>
    </item>
    <item>
      <title>Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems</title>
      <link>https://arxiv.org/abs/2512.11150</link>
      <description>arXiv:2512.11150v1 Announce Type: cross 
Abstract: LLM-as-judge evaluation has become the de facto standard for scaling model assessment, but the practice is statistically unsound: uncalibrated scores can invert preferences, naive confidence intervals on uncalibrated scores achieve near-0% coverage, and importance-weighted estimators collapse under limited overlap despite high effective sample size (ESS). We introduce Causal Judge Evaluation (CJE), a framework that fixes all three failures. On n=4,961 Chatbot Arena prompts (after filtering from 5k), CJE achieves 99% pairwise ranking accuracy at full sample size (94% averaged across configurations), matching oracle quality, at 14x lower cost (for ranking 5 policies) by calibrating a 16x cheaper judge on just 5% oracle labels (~250 labels). CJE combines three components: (i) AutoCal-R, reward calibration via mean-preserving isotonic regression; (ii) SIMCal-W, weight stabilization via stacking of S-monotone candidates; and (iii) Oracle-Uncertainty Aware (OUA) inference that propagates calibration uncertainty into confidence intervals. We formalize the Coverage-Limited Efficiency (CLE) diagnostic, which explains why IPS-style estimators fail even when ESS exceeds 90%: the logger rarely visits regions where target policies concentrate. Key findings: SNIPS inverts rankings even with reward calibration (38% pairwise, negative Kendall's tau) due to weight instability; calibrated IPS remains near-random (47%) despite weight stabilization, consistent with CLE; OUA improves coverage from near-0% to ~86% (Direct) and ~96% (stacked-DR), where naive intervals severely under-cover.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11150v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eddie Landesberg</dc:creator>
    </item>
    <item>
      <title>Estimation of Contextual Exposure to HIV from GPS Data</title>
      <link>https://arxiv.org/abs/2512.11159</link>
      <description>arXiv:2512.11159v1 Announce Type: cross 
Abstract: We present a comprehensive statistical methodological framework for estimating contextual exposure to HIV that includes local (grid-cell level) estimation of HIV prevalence and human activity space estimation based on GPS data. The development of our framework was necessary to analyze HIV surveillance and sociodemographic survey data in conjunction with GPS data collected in rural KwaZulu-Natal, South Africa, to study the mobility patterns of young people. Based on mobility and contextual exposure measures, we examine whether the sex and age of study participants systematically influence the extent and structure of their mobility patterns. We discuss techniques for investigating how the study participants' contextual exposure to HIV changes as their activity spaces expand beyond residential locations, as well as methods for identifying study participants who may be at increased risk of acquiring HIV. KEYWORDS: Contextual HIV exposure; GPS-based mobility analysis; Activity space; HIV prevalence mapping</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11159v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoyang Wu, Zhaoxing Wu, Thulile Mathenjwa, Elphas Okango, Khai Hoan Tram, Margot Otto, Maxime Inghels, Paul Mee, Diego Cuadros, Hae-Young Kim, Till Barnighausen, Frank Tanser, Adrian Dobra</dc:creator>
    </item>
    <item>
      <title>Unified Approach to Portfolio Optimization using the `Gain Probability Density Function' and Applications</title>
      <link>https://arxiv.org/abs/2512.11649</link>
      <description>arXiv:2512.11649v1 Announce Type: cross 
Abstract: This article proposes a unified framework for portfolio optimization (PO), recognizing an object called the `gain probability density function (PDF)' as the fundamental object of the problem from which any objective function could be derived. The gain PDF has the advantage of being 1-dimensional for any given portfolio and thus is easy to visualize and interpret. The framework allows us to naturally incorporate all existing approaches (Markowitz, CVaR-deviation, higher moments...) and represents an interesting basis to develop new approaches. It leads us to propose a method to directly match a target PDF defined by the portfolio manager, giving them maximal control on the PO problem and moving beyond approaches that focus only on expected return and risk. As an example, we develop an application involving a new objective function to control high profits, to be applied after a conventional PO (including expected return and risk criteria) and thus leading to sub-optimality w.r.t. the conventional objective function. We then propose a methodology to quantify a cost associated with this optimality deviation in a common budget unit, providing a meaningful information to portfolio managers. Numerical experiments considering portfolios with energy-producing assets illustrate our approach. The framework is flexible and can be applied to other sectors (financial assets, etc).</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11649v1</guid>
      <category>q-fin.PM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean-Patrick Mascom\`ere, J\'er\'emie Messud, Yagnik Chatterjee, Isabel Barros Garcia</dc:creator>
    </item>
    <item>
      <title>A Relational Model of Neighborhood Mobility: The Role of Amenities and Cultural Alignment</title>
      <link>https://arxiv.org/abs/2512.11662</link>
      <description>arXiv:2512.11662v1 Announce Type: cross 
Abstract: Why are some neighborhoods strongly connected while others remain isolated? Although standard explanations focus on demographics, economics, and geography, movement across the city may also depend on cultural styles and amenity mix. This study proposes a relational, cross-national model in which local culture and amenity mix alignment creates a "soft infrastructure" of urban mobility, i.e., symbolic cues and functional features that shape expectations about the character of places. Using ~650 million Google Places reviews to measure co-visitation between U.S. ZIP codes and ~30 million Canadian change-of-address to track residential mobility, results show that neighborhoods with similar cultural styles and amenities are significantly more connected. These effects persist even after controlling for race, income, education, politics, housing costs, and distance. Urban cohesion and segregation depend not only on who lives where or how far apart neighborhoods are, but on the shared cultural and material ecologies that structure movement across the city.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11662v1</guid>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thiago H Silva, Daniel Silver, Gustavo Santos, Myriam Delgado</dc:creator>
    </item>
    <item>
      <title>Forest Kernel Balancing Weights: Outcome-Guided Features for Causal Inference</title>
      <link>https://arxiv.org/abs/2512.11751</link>
      <description>arXiv:2512.11751v1 Announce Type: cross 
Abstract: While balancing covariates between groups is central for observational causal inference, selecting which features to balance remains a challenging problem. Kernel balancing is a promising approach that first estimates a kernel that captures similarity across units and then balances a (possibly low-dimensional) summary of that kernel, indirectly learning important features to balance. In this paper, we propose forest kernel balancing, which leverages the underappreciated fact that tree-based machine learning models, namely random forests and Bayesian additive regression trees (BART), implicitly estimate a kernel based on the co-occurrence of observations in the same terminal leaf node. Thus, even though the resulting kernel is solely a function of baseline features, the selected nonlinearities and other interactions are important for predicting the outcome -- and therefore are important for addressing confounding. Through simulations and applied illustrations, we show that forest kernel balancing leads to meaningful computational and statistical improvement relative to standard kernel methods, which do not incorporate outcome information when learning features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11751v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andy A. Shen, Eli Ben-Michael, Avi Feller, Luke Keele, Jared Murray</dc:creator>
    </item>
    <item>
      <title>Approaches to biological species delimitation based on genetic and spatial dissimilarity</title>
      <link>https://arxiv.org/abs/2401.12126</link>
      <description>arXiv:2401.12126v5 Announce Type: replace-cross 
Abstract: The delimitation of biological species, i.e., deciding which individuals belong to the same species and whether and how many different species are represented in a data set, is key to the conservation of biodiversity. Much existing work uses only genetic data for species delimitation, often employing some kind of cluster analysis. This can be misleading, because geographically distant groups of individuals can be genetically quite different even if they belong to the same species. We investigate the problem of testing whether two potentially separated groups of individuals can belong to a single species or not based on genetic and spatial data. Existing methods such as the partial Mantel test and jackknife-based distance-distance regression are considered. New approaches, i.e., an adaptation of a mixed effects model, a bootstrap approach, and a jackknife version of partial Mantel, are proposed. All these methods address the issue that distance data violate the independence assumption for standard inference regarding correlation and regression; a standard linear regression is also considered. The approaches are compared on simulated meta-populations generated with SLiM and GSpace - two software packages that can simulate spatially-explicit genetic data at an individual level. Simulations show that the new jackknife version of the partial Mantel test provides a good compromise between power and respecting the nominal type I error rate. Mixed-effects models have larger power than jackknife-based methods, but tend to display type I error rates slightly above the significance level. An application on brassy ringlets concludes the paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12126v5</guid>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriele d'Angella, Christian Hennig</dc:creator>
    </item>
    <item>
      <title>Assumption-Lean Post-Integrated Inference with Surrogate Control Outcomes</title>
      <link>https://arxiv.org/abs/2410.04996</link>
      <description>arXiv:2410.04996v5 Announce Type: replace-cross 
Abstract: Data integration methods aim to extract low-dimensional embeddings from high-dimensional outcomes to remove unwanted variations, such as batch effects and unmeasured covariates, across heterogeneous datasets. However, multiple hypothesis testing after integration can be biased due to data-dependent processes. We introduce a robust post-integrated inference method that accounts for latent heterogeneity by utilizing control outcomes. Leveraging causal interpretations, we derive nonparametric identifiability of the direct effects using negative control outcomes. By utilizing surrogate control outcomes as an extension of negative control outcomes, we develop semiparametric inference on projected direct effect estimands, accounting for hidden mediators, confounders, and moderators. These estimands remain statistically meaningful under model misspecifications and with error-prone embeddings. We provide bias quantifications and finite-sample linear expansions with uniform concentration bounds. The proposed doubly robust estimators are consistent and efficient under minimal assumptions and potential misspecification, facilitating data-adaptive estimation with machine learning algorithms. Our proposal is evaluated using random forests through simulations and analysis of single-cell CRISPR perturbed datasets, which may contain potential unmeasured confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04996v5</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin-Hong Du, Kathryn Roeder, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Confirmatory Biomarker Identification with k-FWER Control Using Derandomized Knockoffs with Cox Regression</title>
      <link>https://arxiv.org/abs/2504.03907</link>
      <description>arXiv:2504.03907v2 Announce Type: replace-cross 
Abstract: Selecting important features in high-dimensional survival analysis is critical for identifying confirmatory biomarkers while maintaining rigorous error control. In this paper, we propose a derandomized knockoffs procedure for Cox regression that enhances stability in feature selection while maintaining rigorous control over the k-familywise error rate (k-FWER). By aggregating across multiple randomized knockoff realizations, our approach mitigates the instability commonly observed with conventional knockoffs. Through extensive simulations, we demonstrate that our method consistently outperforms standard knockoffs in both selection power and error control. Moreover, we apply our procedure to a clinical dataset on primary biliary cirrhosis (PBC) to identify key prognostic biomarkers associated with patient survival. The results confirm the superior stability of the derandomized knockoffs method, allowing for a more reliable identification of important clinical variables. Additionally, our approach is applicable to datasets containing both continuous and categorical covariates, broadening its utility in real-world biomedical studies. This framework provides a robust and interpretable solution for high-dimensional survival analysis, making it particularly suitable for applications requiring precise and stable variable selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03907v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rui Liu, Nan Sun</dc:creator>
    </item>
    <item>
      <title>Designing Efficient Hybrid and Single-Arm Trials: External Control Borrowing and Sample Size Calculation</title>
      <link>https://arxiv.org/abs/2511.09353</link>
      <description>arXiv:2511.09353v2 Announce Type: replace-cross 
Abstract: External controls (ECs) from historical trials or real-world data have gained increasing attention as a way to augment hybrid and single-arm trials, especially when balanced randomization is infeasible. While most existing work has focused on post-trial inference using ECs, their role in prospective trial design remains less explored. We address this gap by focusing on the sample size determination and power analysis for an experimental design problem that encompasses standard randomized controlled trials (RCTs), hybrid trials, and single-arm trials. Building on estimators derived from the efficient influence function, we develop hybrid and single-arm design strategies that leverage comparable EC data to reduce the required sample size of the current study. We derive asymptotic variance expressions for these estimators in terms of interpretable, population-level quantities and introduce a pre-experimental variance estimation procedure to guide sample size calculation, ensuring prespecified type I error and power for the relevant hypothesis test. Simulation studies demonstrate that the proposed hybrid and single-arm designs maintain valid type I error and achieve target power across diverse scenarios while requiring substantially fewer subjects in the current study than RCT designs. A real data application further illustrates the practical utility and advantages of the proposed hybrid and single-arm designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09353v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujing Gao, Xiang Zhang, Shu Yang</dc:creator>
    </item>
    <item>
      <title>Estimating the Effects of Heatwaves on Health: A Causal Inference Framework</title>
      <link>https://arxiv.org/abs/2511.11433</link>
      <description>arXiv:2511.11433v2 Announce Type: replace-cross 
Abstract: The harmful relationship between heatwaves and health has been extensively documented in medical and epidemiological literature. However, most evidence is associational and cannot be interpreted causally unless strong assumptions are made. In this paper, we first make explicit the assumptions underlying the statistical methods frequently used in the heatwave literature and demonstrate when these assumptions might break down in heatwave contexts. To address these shortcomings, we propose a causal inference framework that transparently elicits causal identification assumptions. Within this new framework, we first introduce synthetic controls (SC) for estimating heatwave effects, then propose a spatially augmented Bayesian synthetic control (SA-SC) method that accounts for spatial dependence and spillovers. Empirical Monte Carlo simulations show both methods perform well, with SA-SC reducing root mean squared error and improving posterior interval coverage under spillovers and spatial dependence. Finally, we apply the proposed methods to estimate the causal effects of heatwaves on Medicare heat-related hospitalizations among 13,753,273 beneficiaries residing in Northeastern U.S. from 2000 to 2019. This causal inference framework provides spatially coherent counterfactual outcomes and robust, interpretable, and transparent causal estimates while explicitly addressing the unexamined assumptions in existing methods that pervade the heatwave effect literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11433v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giulio Grossi, Leo Vanciu, Veronica Ballerini, Danielle Braun, Falco J. Bargagli Stoffi</dc:creator>
    </item>
    <item>
      <title>Any Old Tom, Dick or Harry: The Citation Impact of First Name Genderedness</title>
      <link>https://arxiv.org/abs/2512.08219</link>
      <description>arXiv:2512.08219v2 Announce Type: replace-cross 
Abstract: This paper attempts a first analysis of citation distributions based on the genderedness of authors' first name. Following the extraction of first name and sex data from all human entity triplets contained in Wikidata, a first name genderedness table is first created based on compiled sex frequencies, then merged with bibliometric data from eponymous, US-affiliated authors. Comparisons of various cumulative distributions show that citation concentrations fluctuations are highest at the opposite ends of the genderedness spectrum, as authors with very feminine and masculine first names respectively get a lower and higher share of citations for every article published, irrespective of their contribution role.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08219v2</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maxime Holmberg Sainte-Marie, Vincent Larivi\`ere</dc:creator>
    </item>
  </channel>
</rss>
