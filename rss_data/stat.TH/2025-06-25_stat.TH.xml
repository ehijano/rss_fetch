<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Jun 2025 01:37:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Statistical Inference for Optimal Transport Maps: Recent Advances and Perspectives</title>
      <link>https://arxiv.org/abs/2506.19025</link>
      <description>arXiv:2506.19025v1 Announce Type: new 
Abstract: In many applications of optimal transport (OT), the object of primary interest is the optimal transport map. This map rearranges mass from one probability distribution to another in the most efficient way possible by minimizing a specified cost. In this paper we review recent advances in estimating and developing limit theorems for the OT map, using samples from the underlying distributions. We also review parallel lines of work that establish similar results for special cases and variants of the basic OT setup. We conclude with a discussion of key directions for future research with the goal of providing practitioners with reliable inferential tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19025v1</guid>
      <category>math.ST</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sivaraman Balakrishnan, Tudor Manole, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Regularity of the score function in generative models</title>
      <link>https://arxiv.org/abs/2506.19559</link>
      <description>arXiv:2506.19559v1 Announce Type: new 
Abstract: We study the regularity of the score function in score-based generative models and show that it naturally adapts to the smoothness of the data distribution. Under minimal assumptions, we establish Lipschitz estimates that directly support convergence and stability analyses in both diffusion and ODE-based generative models. In addition, we derive higher-order regularity bounds, which simplify existing arguments for optimally approximating the score function using neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19559v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arthur St\'ephanovitch</dc:creator>
    </item>
    <item>
      <title>Generative model for optimal density estimation on unknown manifold</title>
      <link>https://arxiv.org/abs/2506.19587</link>
      <description>arXiv:2506.19587v1 Announce Type: new 
Abstract: We propose a generative model that achieves minimax-optimal convergence rates for estimating probability distributions supported on unknown low-dimensional manifolds. Building on Fefferman's solution to the geometric Whitney problem, our estimator is itself supported on a submanifold that matches the regularity of the data's support. This geometric adaptation enables the estimator to be simultaneously minimax-optimal for all \( \gamma \)-H\"older Integral Probability Metrics (IPMs) with \( \gamma \geq 1 \). We validate our approach through experiments on synthetic and real datasets, demonstrating competitive or superior performance compared to Wasserstein GAN and score-based generative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19587v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arthur St\'ephanovitch</dc:creator>
    </item>
    <item>
      <title>Copula-Based Modeling of Fractional Inaccuracy: A Unified Framework</title>
      <link>https://arxiv.org/abs/2506.19748</link>
      <description>arXiv:2506.19748v1 Announce Type: new 
Abstract: We introduce novel information-theoretic measures termed the multivariate cumulative copula fractional inaccuracy measure and the multivariate survival copula fractional inaccuracy measure, constructed respectively from multivariate copulas and multivariate survival copulas. These measures generalize the concept of fractional inaccuracy to multivariate settings by incorporating dependence structures through copulas. We establish bounds for these measures using the Frechet-Hoeffding bounds and investigate their behavior under lower and upper orthant stochastic orderings to facilitate comparative analysis. Furthermore, we define the multivariate co-copula fractional inaccuracy measure and the multivariate dual copula fractional inaccuracy measure, derived from the multivariate co-copula and dual copula, respectively, and examine several analogous properties for these extended forms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19748v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aman Pandey, Chanchal Kundu</dc:creator>
    </item>
    <item>
      <title>Bayesian community detection in assortative stochastic block model with unknown number of communities</title>
      <link>https://arxiv.org/abs/2506.19576</link>
      <description>arXiv:2506.19576v1 Announce Type: cross 
Abstract: Structured data in the form of networks is increasingly common in a number of fields, including social sciences, biology, physics, computer science, and many others. A key task in network analysis is community detection, which typically consists of dividing the nodes into groups such that nodes within a group are strongly connected, while connections between groups are relatively scarcer. A generative model well-suited for the formation of such communities is the assortative stochastic block model, which prescribes a higher probability of a connection between nodes belonging to the same block rather than to different blocks. A recent line of work has utilized Bayesian nonparametric methods to recover communities in the SBM by placing a prior distribution on the number of blocks and estimating block assignments via collapsed Gibbs samplers. However, efficiently incorporating the assortativity constraint through the prior remains an open problem. In this work, we address this gap, aiming to study the effect of enforcing assortativity on Bayesian community detection and so identify under what scenario it pays its dividends in comparison with standard SBM. We illustrate our findings through an extensive simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19576v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martina Amongero, Pierpaolo De Blasi</dc:creator>
    </item>
    <item>
      <title>When Can We Reuse a Calibration Set for Multiple Conformal Predictions?</title>
      <link>https://arxiv.org/abs/2506.19689</link>
      <description>arXiv:2506.19689v1 Announce Type: cross 
Abstract: Reliable uncertainty quantification is crucial for the trustworthiness of machine learning applications. Inductive Conformal Prediction (ICP) offers a distribution-free framework for generating prediction sets or intervals with user-specified confidence. However, standard ICP guarantees are marginal and typically require a fresh calibration set for each new prediction to maintain their validity. This paper addresses this practical limitation by demonstrating how e-conformal prediction, in conjunction with Hoeffding's inequality, can enable the repeated use of a single calibration set with a high probability of preserving the desired coverage. Through a case study on the CIFAR-10 dataset, we train a deep neural network and utilise a calibration set to estimate a Hoeffding correction. This correction allows us to apply a modified Markov's inequality, leading to the construction of prediction sets with quantifiable confidence. Our results illustrate the feasibility of maintaining provable performance in conformal prediction while enhancing its practicality by reducing the need for repeated calibration. The code for this work is publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19689v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. A. Balinsky, A. D. Balinsky</dc:creator>
    </item>
    <item>
      <title>Cross-regularization: Adaptive Model Complexity through Validation Gradients</title>
      <link>https://arxiv.org/abs/2506.19755</link>
      <description>arXiv:2506.19755v1 Announce Type: cross 
Abstract: Model regularization requires extensive manual tuning to balance complexity against overfitting. Cross-regularization resolves this tradeoff by directly adapting regularization parameters through validation gradients during training. The method splits parameter optimization - training data guides feature learning while validation data shapes complexity controls - converging provably to cross-validation optima. When implemented through noise injection in neural networks, this approach reveals striking patterns: unexpectedly high noise tolerance and architecture-specific regularization that emerges organically during training. Beyond complexity control, the framework integrates seamlessly with data augmentation, uncertainty calibration and growing datasets while maintaining single-run efficiency through a simple gradient-based approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19755v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlos Stein Brito</dc:creator>
    </item>
    <item>
      <title>The Shape of Consumer Behavior: A Symbolic and Topological Analysis of Time Series</title>
      <link>https://arxiv.org/abs/2506.19759</link>
      <description>arXiv:2506.19759v1 Announce Type: cross 
Abstract: Understanding temporal patterns in online search behavior is crucial for real-time marketing and trend forecasting. Google Trends offers a rich proxy for public interest, yet the high dimensionality and noise of its time-series data present challenges for effective clustering. This study evaluates three unsupervised clustering approaches, Symbolic Aggregate approXimation (SAX), enhanced SAX (eSAX), and Topological Data Analysis (TDA), applied to 20 Google Trends keywords representing major consumer categories. Our results show that while SAX and eSAX offer fast and interpretable clustering for stable time series, they struggle with volatility and complexity, often producing ambiguous ``catch-all'' clusters. TDA, by contrast, captures global structural features through persistent homology and achieves more balanced and meaningful groupings.
  We conclude with practical guidance for using symbolic and topological methods in consumer analytics and suggest that hybrid approaches combining both perspectives hold strong potential for future applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19759v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pola Bereta, Ioannis Diamantis</dc:creator>
    </item>
    <item>
      <title>Bootstrap-based tests for the total time on test and the excess wealth orders</title>
      <link>https://arxiv.org/abs/2310.13339</link>
      <description>arXiv:2310.13339v2 Announce Type: replace 
Abstract: Given a pair of non-negative random variables $X$ and $Y$, we introduce a class of nonparametric tests for the null hypothesis that $X$ dominates $Y$ in the total time on test order. Critical values are determined using bootstrap-based inference, and the tests are shown to be consistent. The same approach is used to construct tests for the excess wealth order. As a byproduct, we also obtain a class of goodness-of-fit tests for the NBUE (New Better than Used in Expectation) family of distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13339v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jspi.2025.106315</arxiv:DOI>
      <dc:creator>Tommaso Lando, Sirio Legramanti</dc:creator>
    </item>
    <item>
      <title>On estimation of skewed stable linear regression</title>
      <link>https://arxiv.org/abs/2404.10448</link>
      <description>arXiv:2404.10448v4 Announce Type: replace 
Abstract: We study the parameter estimation method for linear regression models with possibly skewed stable distributed errors. Our estimation procedure consists of two stages: first, for the regression coefficients, the Cauchy quasi-maximum likelihood estimator (CQMLE) is considered after taking the differences to remove the skewness of noise, and we prove its asymptotic normality and tail-probability estimate; second, as for stable-distribution parameters, we consider the moment estimators based on the symmetrized and centered residuals and prove their $\sqrt{n}$-consistency. To derive the $\sqrt{n}$-consistency, we essentially used the tail-probability estimate of the CQMLE. The proposed estimation procedure has a very low computational load and is much less time-consuming compared with the maximum-likelihood estimator. Further, our estimator can be effectively used as an initial value of the numerical optimization of the log-likelihood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10448v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eitaro Kawamo, Hiroki Masuda</dc:creator>
    </item>
    <item>
      <title>A new class of tests for convex-ordered families based on expected order statistics</title>
      <link>https://arxiv.org/abs/2501.14075</link>
      <description>arXiv:2501.14075v2 Announce Type: replace 
Abstract: Consider a pair of cumulative distribution functions $F$ and $G$, where $F$ is unknown and $G$ is a known reference distribution. Given a sample from $F$, we propose tests to detect the convexity or the concavity of $G^{-1}\circ F$ versus equality in distribution (up to location and scale transformations). This framework encompasses well-known cases, including increasing hazard rate distributions, as well as some other relevant families that have garnered attention more recently, for which no tests are currently available. We introduce test statistics based on the estimated probability that the random variable of interest does not exceed a given expected order statistic, which, in turn, is estimated via L-estimation. The tests are unbiased, consistent, and exhibit monotone power with respect to the convex transform order. To ensure consistency, we show that our L-estimators satisfy a strong law of large numbers, even when the mean is not finite, thereby making the tests suitable for heavy-tailed distributions. Unlike other approaches, these tests are broadly applicable, regardless of the choice of $G$ and without support restrictions. The performance of the method under various conditions is demonstrated via simulations, and its applicability is illustrated through a concrete example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14075v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/25-EJS2398</arxiv:DOI>
      <dc:creator>Tommaso Lando, Mohammed Es-Salih Benjrada</dc:creator>
    </item>
    <item>
      <title>Sharp Asymptotic Minimaxity for One-Group Priors in Sparse Normal Means Problem</title>
      <link>https://arxiv.org/abs/2505.16428</link>
      <description>arXiv:2505.16428v2 Announce Type: replace 
Abstract: In this paper, we consider the asymptotic properties of the Bayesian multiple testing rules when the mean parameter of the sparse normal means problem is modeled by a broad class of global-local priors, expressed as a scale mixture of normals. We are interested in studying the least possible risk, i.e., the minimax risk for two frequentist losses, one being the usual misclassification (or Hamming) loss, and the other one, measured as the sum of FDR and FNR. Under the betamin separation condition, at first, assuming the level of sparsity to be known, we propose a condition on the global parameter of our chosen class of priors, such that the resultant decision rule attains the minimax risk for both of the losses mentioned above. When the level of sparsity is unknown, we either use an estimate of the global parameter obtained from the data, or propose an absolutely continuous prior on it. For both of the procedures, under some assumption on the unknown level of sparsity, we show that the decision rules also attain the minimax risk, again for both of the losses. Our results also provide a guideline regarding the selection of priors, in the sense that beyond a subclass(horseshoe type priors) of our chosen class of priors, the minimax risk is not achievable with respect to any one of the two loss functions considered in this article. However, the subclass, horseshoe-type priors, is such a large subclass that it contains Horseshoe, Strawderman Berger, standard double Pareto, inverse gamma priors, just to name a few. In this way, along with the most popular BH procedure and approach using spike and slab prior, a multiple testing rule based on one group priors also achieves the optimal boundary. To the best of our knowledge, these are the first results in the literature of global local priors which ensure the optimal minimax risk can be achieved exactly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16428v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sayantan Paul, Prasenjit Ghosh, Arijit Chakrabarti</dc:creator>
    </item>
    <item>
      <title>The high-dimensional asymptotics of first order methods with random data</title>
      <link>https://arxiv.org/abs/2112.07572</link>
      <description>arXiv:2112.07572v2 Announce Type: replace-cross 
Abstract: We study a class of deterministic flows in ${\mathbb R}^{d\times k}$, parametrized by a random matrix ${\boldsymbol X}\in {\mathbb R}^{n\times d}$ with i.i.d. centered subgaussian entries. We characterize the asymptotic behavior of these flows over bounded time horizons, in the high-dimensional limit in which $n,d\to\infty$ with $k$ fixed and converging aspect ratios $n/d\to\delta$. The asymptotic characterization we prove is in terms of a system of a nonlinear stochastic process in $k$ dimensions, whose parameters are determined by a fixed point condition. This type of characterization is known in physics as dynamical mean field theory. Rigorous results of this type have been obtained in the past for a few spin glass models. Our proof is based on time discretization and a reduction to certain iterative schemes known as approximate message passing (AMP) algorithms, as opposed to earlier work that was based on large deviations theory and stochastic processes theory. The new approach provides a unified view of a general class of algorithms and implies that the high-dimensional behavior of the flow is universal with respect to the distribution of the entries of ${\boldsymbol X}$.
  As specific applications, we obtain high-dimensional characterizations of gradient flow in some classical models from statistics and machine learning, under a random design assumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.07572v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Celentano, Chen Cheng, Andrea Montanari</dc:creator>
    </item>
    <item>
      <title>Confidence set for mixture order selection</title>
      <link>https://arxiv.org/abs/2503.18790</link>
      <description>arXiv:2503.18790v2 Announce Type: replace-cross 
Abstract: A fundamental challenge in approximating an unknown density using finite Gaussian mixture models is selecting the number of mixture components, also known as order. Traditional approaches choose a single best model using information criteria. However, often models with different orders yield similar fits, leading to substantial model selection uncertainty and making it challenging to identify the optimal number of components. In this paper, we introduce the Model Selection Confidence Set (MSCS) for order selection in Gaussian mixtures - a set-valued estimator that, with a predefined confidence level, includes the true mixture order across repeated samples. Rather than selecting a single model, our MSCS identifies all plausible orders by determining whether each candidate model is at least as plausible as the best-selected one, using a screening based on a penalized likelihood ratio statistic. We provide theoretical guarantees for asymptotic coverage, and demonstrate its practical advantages through simulations and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18790v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Casa, Davide Ferrari</dc:creator>
    </item>
    <item>
      <title>Berk-Nash Rationalizability</title>
      <link>https://arxiv.org/abs/2505.20708</link>
      <description>arXiv:2505.20708v2 Announce Type: replace-cross 
Abstract: Misspecified learning -- where agents rely on simplified or biased models -- offers a unifying framework for analyzing behavioral biases, cognitive constraints, and systematic misperceptions. We introduce Berk--Nash rationalizability, a new solution concept for such settings that parallels rationalizability in games. Our main result shows that, with probability one, every limit action -- any action played or approached infinitely often -- is Berk--Nash rationalizable. This holds regardless of whether behavior converges and offers a tractable way to bound long-run behavior without solving complex learning dynamics. We illustrate this advantage with a known example and identify general classes of environments where the rationalizable set can be easily characterized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20708v2</guid>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ignacio Esponda, Demian Pouzo</dc:creator>
    </item>
  </channel>
</rss>
