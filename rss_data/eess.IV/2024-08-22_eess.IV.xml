<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.IV updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.IV</link>
    <description>eess.IV updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.IV" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 22 Aug 2024 04:05:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 22 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Deep Learning for Automated Wound Classification And Segmentation</title>
      <link>https://arxiv.org/abs/2408.11064</link>
      <description>arXiv:2408.11064v1 Announce Type: new 
Abstract: Wounds, such as foot ulcers, pressure ulcers, leg ulcers, and infected wounds, come up with substantial problems for healthcare professionals. Prompt and accurate segmentation is crucial for effective treatment. However, contemporary methods need an exhaustive model that is qualified for both classification and segmentation, especially lightweight ones. In this work, we tackle this issue by presenting a new architecture that incorporates U-Net, which is optimized for both wound classification and effective segmentation. We curated four extensive and diverse collections of wound images, utilizing the publicly available Medetec Dataset, and supplemented with additional data sourced from the Internet. Our model performed exceptionally well, with an F1 score of 0.929, a Dice score of 0.931 in segmentation, and an accuracy of 0.915 in classification, proving its effectiveness in both classification and segmentation work. This accomplishment highlights the potential of our approach to automating wound care management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11064v1</guid>
      <category>eess.IV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md. Zihad Bin Jahangir, Sumaiya Akter, MD Abdullah Al Nasim, Kishor Datta Gupta, Roy George</dc:creator>
    </item>
    <item>
      <title>Ophthalmic Biomarker Detection: Highlights from the IEEE Video and Image Processing Cup 2023 Student Competition</title>
      <link>https://arxiv.org/abs/2408.11170</link>
      <description>arXiv:2408.11170v1 Announce Type: new 
Abstract: The VIP Cup offers a unique experience to undergraduates, allowing students to work together to solve challenging, real-world problems with video and image processing techniques. In this iteration of the VIP Cup, we challenged students to balance personalization and generalization when performing biomarker detection in 3D optical coherence tomography (OCT) images. Balancing personalization and generalization is an important challenge to tackle, as the variation within OCT scans of patients between visits can be minimal while the difference in manifestation of the same disease across different patients may be substantial. The domain difference between OCT scans can arise due to pathology manifestation across patients, clinical labels, and the visit along the treatment process when the scan is taken. Hence, we provided a multimodal OCT dataset to allow teams to effectively target this challenge. Overall, this competition gave undergraduates an opportunity to learn about how artificial intelligence can be a powerful tool for the medical field, as well as the unique challenges one faces when applying machine learning to biomedical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11170v1</guid>
      <category>eess.IV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ghassan AlRegib, Mohit Prabhushankar, Kiran Kokilepersaud, Prithwijit Chowdhury, Zoe Fowler, Stephanie Trejo Corona, Lucas Thomaz, Angshul Majumdar</dc:creator>
    </item>
    <item>
      <title>OCTCube: A 3D foundation model for optical coherence tomography that improves cross-dataset, cross-disease, cross-device and cross-modality analysis</title>
      <link>https://arxiv.org/abs/2408.11227</link>
      <description>arXiv:2408.11227v1 Announce Type: new 
Abstract: Optical coherence tomography (OCT) has become critical for diagnosing retinal diseases as it enables 3D images of the retina and optic nerve. OCT acquisition is fast, non-invasive, affordable, and scalable. Due to its broad applicability, massive numbers of OCT images have been accumulated in routine exams, making it possible to train large-scale foundation models that can generalize to various diagnostic tasks using OCT images. Nevertheless, existing foundation models for OCT only consider 2D image slices, overlooking the rich 3D structure. Here, we present OCTCube, a 3D foundation model pre-trained on 26,605 3D OCT volumes encompassing 1.62 million 2D OCT images. OCTCube is developed based on 3D masked autoencoders and exploits FlashAttention to reduce the larger GPU memory usage caused by modeling 3D volumes. OCTCube outperforms 2D models when predicting 8 retinal diseases in both inductive and cross-dataset settings, indicating that utilizing the 3D structure in the model instead of 2D data results in significant improvement. OCTCube further shows superior performance on cross-device prediction and when predicting systemic diseases, such as diabetes and hypertension, further demonstrating its strong generalizability. Finally, we propose a contrastive-self-supervised-learning-based OCT-IR pre-training framework (COIP) for cross-modality analysis on OCT and infrared retinal (IR) images, where the OCT volumes are embedded using OCTCube. We demonstrate that COIP enables accurate alignment between OCT and IR en face images. Collectively, OCTCube, a 3D OCT foundation model, demonstrates significantly better performance against 2D models on 27 out of 29 tasks and comparable performance on the other two tasks, paving the way for AI-based retinal disease diagnosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11227v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zixuan Liu, Hanwen Xu, Addie Woicik, Linda G. Shapiro, Marian Blazes, Yue Wu, Cecilia S. Lee, Aaron Y. Lee, Sheng Wang</dc:creator>
    </item>
    <item>
      <title>HMT-UNet: A hybird Mamba-Transformer Vision UNet for Medical Image Segmentation</title>
      <link>https://arxiv.org/abs/2408.11289</link>
      <description>arXiv:2408.11289v1 Announce Type: new 
Abstract: In the field of medical image segmentation, models based on both CNN and Transformer have been thoroughly investigated. However, CNNs have limited modeling capabilities for long-range dependencies, making it challenging to exploit the semantic information within images fully. On the other hand, the quadratic computational complexity poses a challenge for Transformers. State Space Models (SSMs), such as Mamba, have been recognized as a promising method. They not only demonstrate superior performance in modeling long-range interactions, but also preserve a linear computational complexity. The hybrid mechanism of SSM (State Space Model) and Transformer, after meticulous design, can enhance its capability for efficient modeling of visual features. Extensive experiments have demonstrated that integrating the self-attention mechanism into the hybrid part behind the layers of Mamba's architecture can greatly improve the modeling capacity to capture long-range spatial dependencies. In this paper, leveraging the hybrid mechanism of SSM, we propose a U-shape architecture model for medical image segmentation, named Hybird Transformer vision Mamba UNet (HTM-UNet). We conduct comprehensive experiments on the ISIC17, ISIC18, CVC-300, CVC-ClinicDB, Kvasir, CVC-ColonDB, ETIS-Larib PolypDB public datasets and ZD-LCI-GIM private dataset. The results indicate that HTM-UNet exhibits competitive performance in medical image segmentation tasks. Our code is available at https://github.com/simzhangbest/HMT-Unet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11289v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingya Zhang, Limei Gu, Tingshen Ling, Xianping Tao</dc:creator>
    </item>
    <item>
      <title>Automated Optical Reading of Scanned ECGs</title>
      <link>https://arxiv.org/abs/2408.11425</link>
      <description>arXiv:2408.11425v1 Announce Type: new 
Abstract: Electrocardiogram (ECG) is a valuable tool for medical diagnosis used worldwide. Its use has contributed significantly to the prevention of cardiovascular diseases including infarctions. Although physicians need to see the printed curves for a diagnosis, nowadays there exist automated tools based on machine learning that can help diagnosis of arrhythmias and other pathologies, these tools operate on digitalized ECG data that are merely one-dimensional discrete signals (a kind of information that is much similar to digitized audio). Thus, it is interesting to have both the graphical information and the digitized data. This is possible with modern, digital equipment. Nevertheless, there still exist many analog electrocardiogram machines that plot results on paper with a printed gris measured in millimeters. This paper presents a novel image analysis method that is capable of reading a printed ECG and converting it into a sampled digital signal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11425v1</guid>
      <category>eess.IV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuel Pazos-Santom\'e, Fernando Mart\'in-Rodr\'iguez, M\'onica Fern\'andez-Barciela</dc:creator>
    </item>
    <item>
      <title>OAPT: Offset-Aware Partition Transformer for Double JPEG Artifacts Removal</title>
      <link>https://arxiv.org/abs/2408.11480</link>
      <description>arXiv:2408.11480v1 Announce Type: new 
Abstract: Deep learning-based methods have shown remarkable performance in single JPEG artifacts removal task. However, existing methods tend to degrade on double JPEG images, which are prevalent in real-world scenarios. To address this issue, we propose Offset-Aware Partition Transformer for double JPEG artifacts removal, termed as OAPT. We conduct an analysis of double JPEG compression that results in up to four patterns within each 8x8 block and design our model to cluster the similar patterns to remedy the difficulty of restoration. Our OAPT consists of two components: compression offset predictor and image reconstructor. Specifically, the predictor estimates pixel offsets between the first and second compression, which are then utilized to divide different patterns. The reconstructor is mainly based on several Hybrid Partition Attention Blocks (HPAB), combining vanilla window-based self-attention and sparse attention for clustered pattern features. Extensive experiments demonstrate that OAPT outperforms the state-of-the-art method by more than 0.16dB in double JPEG image restoration task. Moreover, without increasing any computation cost, the pattern clustering module in HPAB can serve as a plugin to enhance other transformer-based image restoration methods. The code will be available at https://github.com/QMoQ/OAPT.git .</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11480v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiao Mo, Yukang Ding, Jinhua Hao, Qiang Zhu, Ming Sun, Chao Zhou, Feiyu Chen, Shuyuan Zhu</dc:creator>
    </item>
    <item>
      <title>An Improved CovidConvLSTM model for pneumonia-COVID-19 detection and classification</title>
      <link>https://arxiv.org/abs/2408.11507</link>
      <description>arXiv:2408.11507v1 Announce Type: new 
Abstract: Recently, COVID-19 pandemic has rapidly evolved into a critical global health crisis, profoundly impacting daily life. As a result, CAD systems have gained significant interest for its massive computational capabilities, which facilitate the rapid analysis and interpretation of medical imaging. In particular, Deep Learning (DL )techniques have emerged as critical tools to assist radiologists and pulmonologists in distinguishing COVID-19 patients from other pneumonia types and healthy cases. Unfortunately, existing DL techniques face several challenges such as overfitting, performance degradation, feature irrelevance and redundancy, vanishing gradient problem, and high computational complexity. In this paper we address these challenges by introducing an enhanced Convolutional Neural Network algorithm that combines a bottleneck based model RegNetX002, ConvLstm layer, and Squeeze and Excitation block (SE). Specifically, the RegNetx002 and the ConvLstm layer are used for features map extraction and feature quality enhancement, while the attention mechanism SE block is employed to improve feature representation by highlighting important channel features and suppressing unimportant features. More importantly, The bottleneck module facilitates the extraction of more abstract features while lowering computational costs. Additionally, it incorporates residual connections that helps reducing the vanishing gradient problem. Balanced CPN-CXRPA and imbalanced CXRI-P/C-CXR datasets are used to assess the proposed model. Performance metrics such as accuracy and F1 score are used to evaluate the model efficiency. Using the CPN-CXRPA dataset, our model achieved an accuracy of 98.22%. For the CXRI-P-C-CXR dataset, it achieved 98.78% of both accuracy and F1 score. The experimental results show that this framework outperforms existing models in terms of performance and computational complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11507v1</guid>
      <category>eess.IV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Imane Beghoura, Mustapha Benssalah, Fazia Sbargoud</dc:creator>
    </item>
    <item>
      <title>Classification of Mitral Regurgitation from Cardiac Cine MRI using Clinically-Interpretable Morphological Features</title>
      <link>https://arxiv.org/abs/2408.11532</link>
      <description>arXiv:2408.11532v1 Announce Type: new 
Abstract: The assessment of mitral regurgitation (MR) using cardiac MRI, particularly Cine MRI, is a promising technique due to its wide availability. However, some of the temporal information available in clinical Cine MRI may not be fully utilised, as it requires detailed temporal analysis across different cardiac views. We propose a new approach to identify MR which automatically extracts 4-dimensional (3D + Time) morphological features from the reconstructed mitral annulus (MA) using Cine long-axis (LAX) views MRI.
  Our feature extraction involves locating the MA insertion points to derive the reconstructed MA geometry and displacements, resulting in a total of 187 candidate features. We identify the 25 most relevant mitral valve features using minimum-redundancy maximum-relevance (MRMR) feature selection technique. We then apply linear discriminant analysis (LDA) and random forest (RF) model to determine the presence of MR. Both LDA and RF demonstrate good performance, with accuracies of 0.72 +/- 0.05 and 0.73 +/- 0.09, respectively, in a 5-fold cross-validation analysis.
  This approach will be incorporated in an automatic tool to identify valvular diseases from Cine MRI by integrating both handcrafted and deep features. Our tool will facilitate the diagnosis of valvular disease from conventional cardiac MRI scans with no additional scanning or image analysis penalty.
  All code is made available on an open-source basis at: https://github.com/HenryOn2021/MA_Morphological_Features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11532v1</guid>
      <category>eess.IV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Y. On, K. Vimalesvaran, S. Zaman, M. Shun-Shin, J. Howard, N. Linton, G. Cole, A. A. Bharath, M. Varela</dc:creator>
    </item>
    <item>
      <title>LiFCal: Online Light Field Camera Calibration via Bundle Adjustment</title>
      <link>https://arxiv.org/abs/2408.11682</link>
      <description>arXiv:2408.11682v1 Announce Type: new 
Abstract: We propose LiFCal, a novel geometric online calibration pipeline for MLA-based light field cameras. LiFCal accurately determines model parameters from a moving camera sequence without precise calibration targets, integrating arbitrary metric scaling constraints. It optimizes intrinsic parameters of the light field camera model, the 3D coordinates of a sparse set of scene points and camera poses in a single bundle adjustment defined directly on micro image points.
  We show that LiFCal can reliably and repeatably calibrate a focused plenoptic camera using different input sequences, providing intrinsic camera parameters extremely close to state-of-the-art methods, while offering two main advantages: it can be applied in a target-free scene, and it is implemented online in a complete and continuous pipeline.
  Furthermore, we demonstrate the quality of the obtained camera parameters in downstream tasks like depth estimation and SLAM.
  Webpage: https://lifcal.github.io/</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11682v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aymeric Fleith, Doaa Ahmed, Daniel Cremers, Niclas Zeller</dc:creator>
    </item>
    <item>
      <title>FedGS: Federated Gradient Scaling for Heterogeneous Medical Image Segmentation</title>
      <link>https://arxiv.org/abs/2408.11701</link>
      <description>arXiv:2408.11701v1 Announce Type: new 
Abstract: Federated Learning (FL) in Deep Learning (DL)-automated medical image segmentation helps preserving privacy by enabling collaborative model training without sharing patient data. However, FL faces challenges with data heterogeneity among institutions, leading to suboptimal global models. Integrating Disentangled Representation Learning (DRL) in FL can enhance robustness by separating data into distinct representations. Existing DRL methods assume heterogeneity lies solely in style features, overlooking content-based variability like lesion size and shape. We propose FedGS, a novel FL aggregation method, to improve segmentation performance on small, under-represented targets while maintaining overall efficacy. FedGS demonstrates superior performance over FedAvg, particularly for small lesions, across PolypGen and LiTS datasets. The code and pre-trained checkpoints are available at the following link: https://github.com/Trustworthy-AI-UU-NKI/Federated-Learning-Disentanglement</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11701v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philip Schutte, Valentina Corbetta, Regina Beets-Tan, Wilson Silva</dc:creator>
    </item>
    <item>
      <title>NuSegDG: Integration of Heterogeneous Space and Gaussian Kernel for Domain-Generalized Nuclei Segmentation</title>
      <link>https://arxiv.org/abs/2408.11787</link>
      <description>arXiv:2408.11787v1 Announce Type: new 
Abstract: Domain-generalized nuclei segmentation refers to the generalizability of models to unseen domains based on knowledge learned from source domains and is challenged by various image conditions, cell types, and stain strategies. Recently, the Segment Anything Model (SAM) has made great success in universal image segmentation by interactive prompt modes (e.g., point and box). Despite its strengths, the original SAM presents limited adaptation to medical images. Moreover, SAM requires providing manual bounding box prompts for each object to produce satisfactory segmentation masks, so it is laborious in nuclei segmentation scenarios. To address these limitations, we propose a domain-generalizable framework for nuclei image segmentation, abbreviated to NuSegDG. Specifically, we first devise a Heterogeneous Space Adapter (HS-Adapter) to learn multi-dimensional feature representations of different nuclei domains by injecting a small number of trainable parameters into the image encoder of SAM. To alleviate the labor-intensive requirement of manual prompts, we introduce a Gaussian-Kernel Prompt Encoder (GKP-Encoder) to generate density maps driven by a single point, which guides segmentation predictions by mixing position prompts and semantic prompts. Furthermore, we present a Two-Stage Mask Decoder (TSM-Decoder) to effectively convert semantic masks to instance maps without the manual demand for morphological shape refinement. Based on our experimental evaluations, the proposed NuSegDG demonstrates state-of-the-art performance in nuclei instance segmentation, exhibiting superior domain generalization capabilities. The source code is available at https://github.com/xq141839/NuSegDG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11787v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenye Lou, Qing Xu, Zekun Jiang, Xiangjian He, Zhen Chen, Yi Wang, Chenxin Li, Maggie M. He, Wenting Duan</dc:creator>
    </item>
    <item>
      <title>Improving the Scan-rescan Precision of AI-based CMR Biomarker Estimation</title>
      <link>https://arxiv.org/abs/2408.11754</link>
      <description>arXiv:2408.11754v1 Announce Type: cross 
Abstract: Quantification of cardiac biomarkers from cine cardiovascular magnetic resonance (CMR) data using deep learning (DL) methods offers many advantages, such as increased accuracy and faster analysis. However, only a few studies have focused on the scan-rescan precision of the biomarker estimates, which is important for reproducibility and longitudinal analysis. Here, we propose a cardiac biomarker estimation pipeline that not only focuses on achieving high segmentation accuracy but also on improving the scan-rescan precision of the computed biomarkers, namely left and right ventricular ejection fraction, and left ventricular myocardial mass. We evaluate two approaches to improve the apical-basal resolution of the segmentations used for estimating the biomarkers: one based on image interpolation and one based on segmentation interpolation. Using a database comprising scan-rescan cine CMR data acquired from 92 subjects, we compare the performance of these two methods against ground truth (GT) segmentations and DL segmentations obtained before interpolation (baseline). The results demonstrate that both the image-based and segmentation-based interpolation methods were able to narrow Bland-Altman scan-rescan confidence intervals for all biomarkers compared to the GT and baseline performances. Our findings highlight the importance of focusing not only on segmentation accuracy but also on the consistency of biomarkers across repeated scans, which is crucial for longitudinal analysis of cardiac function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11754v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <category>eess.IV</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dewmini Hasara Wickremasinghe, Yiyang Xu, Esther Puyol-Ant\'on, Paul Aljabar, Reza Razavi, Andrew P. King</dc:creator>
    </item>
    <item>
      <title>Vessel-Promoted OCT to OCTA Image Translation by Heuristic Contextual Constraints</title>
      <link>https://arxiv.org/abs/2303.06807</link>
      <description>arXiv:2303.06807v2 Announce Type: replace 
Abstract: Optical Coherence Tomography Angiography (OCTA) is a crucial tool in the clinical screening of retinal diseases, allowing for accurate 3D imaging of blood vessels through non-invasive scanning. However, the hardware-based approach for acquiring OCTA images presents challenges due to the need for specialized sensors and expensive devices. In this paper, we introduce a novel method called TransPro, which can translate the readily available 3D Optical Coherence Tomography (OCT) images into 3D OCTA images without requiring any additional hardware modifications. Our TransPro method is primarily driven by two novel ideas that have been overlooked by prior work. The first idea is derived from a critical observation that the OCTA projection map is generated by averaging pixel values from its corresponding B-scans along the Z-axis. Hence, we introduce a hybrid architecture incorporating a 3D adversarial generative network and a novel Heuristic Contextual Guidance (HCG) module, which effectively maintains the consistency of the generated OCTA images between 3D volumes and projection maps. The second idea is to improve the vessel quality in the translated OCTA projection maps. As a result, we propose a novel Vessel Promoted Guidance (VPG) module to enhance the attention of network on retinal vessels. Experimental results on two datasets demonstrate that our TransPro outperforms state-of-the-art approaches, with relative improvements around 11.4% in MAE, 2.7% in PSNR, 2% in SSIM, 40% in VDE, and 9.1% in VDC compared to the baseline method. The code is available at: https://github.com/ustlsh/TransPro.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.06807v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuhan Li, Dong Zhang, Xiaomeng Li, Chubin Ou, Lin An, Yanwu Xu, Kwang-Ting Cheng</dc:creator>
    </item>
    <item>
      <title>Quantifying the effect of X-ray scattering for data generation in real-time defect detection</title>
      <link>https://arxiv.org/abs/2305.12822</link>
      <description>arXiv:2305.12822v2 Announce Type: replace 
Abstract: Background: X-ray imaging is widely used for the non-destructive detection of defects in industrial products on a conveyor belt. In-line detection requires highly accurate, robust, and fast algorithms. Deep Convolutional Neural Networks (DCNNs) satisfy these requirements when a large amount of labeled data is available. To overcome the challenge of collecting these data, different methods of X-ray image generation are considered.
  Objective: Depending on the desired degree of similarity to real data, different physical effects should either be simulated or can be ignored. X-ray scattering is known to be computationally expensive to simulate, and this effect can greatly affect the accuracy of a generated X-ray image. We aim to quantitatively evaluate the effect of scattering on defect detection.
  Methods: Monte-Carlo simulation is used to generate X-ray scattering distribution. DCNNs are trained on the data with and without scattering and applied to the same test datasets. Probability of Detection (POD) curves are computed to compare their performance, characterized by the size of the smallest detectable defect.
  Results: We apply the methodology to a model problem of defect detection in cylinders. When trained on data without scattering, DCNNs reliably detect defects larger than 1.3 mm, and using data with scattering improves performance by less than 5%. If the analysis is performed on the cases with large scattering-to-primary ratio ($1 &lt; SPR &lt; 5$), the difference in performance could reach 15% (approx. 0.4 mm).
  Conclusion: Excluding the scattering signal from the training data has the largest effect on the smallest detectable defects, and the difference decreases for larger defects. The scattering-to-primary ratio has a significant effect on detection performance and the required accuracy of data generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.12822v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3233/XST-230389</arxiv:DOI>
      <arxiv:journal_reference>Journal of X-Ray Science and Technology, vol. 32, no. 4, pp. 1099-1119, 2024</arxiv:journal_reference>
      <dc:creator>Vladyslav Andriiashen, Robert van Liere, Tristan van Leeuwen, K. Joost Batenburg</dc:creator>
    </item>
    <item>
      <title>Boosting Cardiac Color Doppler Frame Rates with Deep Learning</title>
      <link>https://arxiv.org/abs/2404.00067</link>
      <description>arXiv:2404.00067v2 Announce Type: replace 
Abstract: Color Doppler echocardiography enables visualization of blood flow within the heart. However, the limited frame rate impedes the quantitative assessment of blood velocity throughout the cardiac cycle, thereby compromising a comprehensive analysis of ventricular filling. Concurrently, deep learning is demonstrating promising outcomes in post-processing of echocardiographic data for various applications. This work explores the use of deep learning models for intracardiac Doppler velocity estimation from a reduced number of filtered I/Q signals. We used a supervised learning approach by simulating patient-based cardiac color Doppler acquisitions and proposed data augmentation strategies to enlarge the training dataset. We implemented architectures based on convolutional neural networks. In particular, we focused on comparing the U-Net model and the recent ConvNeXt models, alongside assessing real-valued versus complex-valued representations. We found that both models outperformed the state-of-the-art autocorrelator method, effectively mitigating aliasing and noise. We did not observe significant differences between the use of real and complex data. Finally, we validated the models on in vitro and in vivo experiments. All models produced quantitatively comparable results to the baseline and were more robust to noise. ConvNeXt emerged as the sole model to achieve high-quality results on in vivo aliased samples. These results demonstrate the interest of supervised deep learning methods for Doppler velocity estimation from a reduced number of acquisitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00067v2</guid>
      <category>eess.IV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Puig, Denis Friboulet, Hang Jung Ling, Fran\c{c}ois Varray, Jonathan Por\'ee, Jean Provost, Damien Garcia, Fabien Millioz</dc:creator>
    </item>
    <item>
      <title>MR Optimized Reconstruction of Simultaneous Multi-Slice Imaging Using Diffusion Model</title>
      <link>https://arxiv.org/abs/2408.08883</link>
      <description>arXiv:2408.08883v2 Announce Type: replace 
Abstract: Diffusion model has been successfully applied to MRI reconstruction, including single and multi-coil acquisition of MRI data. Simultaneous multi-slice imaging (SMS), as a method for accelerating MR acquisition, can significantly reduce scanning time, but further optimization of reconstruction results is still possible. In order to optimize the reconstruction of SMS, we proposed a method to use diffusion model based on slice-GRAPPA and SPIRiT method. approach: Specifically, our method characterizes the prior distribution of SMS data by score matching and characterizes the k-space redundant prior between coils and slices based on self-consistency. With the utilization of diffusion model, we achieved better reconstruction results.The application of diffusion model can further reduce the scanning time of MRI without compromising image quality, making it more advantageous for clinical application</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08883v2</guid>
      <category>eess.IV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>ISMRM 2024 Digital poster 4024</arxiv:journal_reference>
      <dc:creator>Ting Zhao, Zhuoxu Cui, Sen Jia, Qingyong Zhu, Congcong Liu, Yihang Zhou, Yanjie Zhu, Dong Liang, Haifeng Wang</dc:creator>
    </item>
    <item>
      <title>Reconstruct Spine CT from Biplanar X-Rays via Diffusion Learning</title>
      <link>https://arxiv.org/abs/2408.09731</link>
      <description>arXiv:2408.09731v2 Announce Type: replace 
Abstract: Intraoperative CT imaging serves as a crucial resource for surgical guidance; however, it may not always be readily accessible or practical to implement. In scenarios where CT imaging is not an option, reconstructing CT scans from X-rays can offer a viable alternative. In this paper, we introduce an innovative method for 3D CT reconstruction utilizing biplanar X-rays. Distinct from previous research that relies on conventional image generation techniques, our approach leverages a conditional diffusion process to tackle the task of reconstruction. More precisely, we employ a diffusion-based probabilistic model trained to produce 3D CT images based on orthogonal biplanar X-rays. To improve the structural integrity of the reconstructed images, we incorporate a novel projection loss function. Experimental results validate that our proposed method surpasses existing state-of-the-art benchmarks in both visual image quality and multiple evaluative metrics. Specifically, our technique achieves a higher Structural Similarity Index (SSIM) of 0.83, a relative increase of 10\%, and a lower Fr\'echet Inception Distance (FID) of 83.43, which represents a relative decrease of 25\%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09731v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhi Qiao, Xuhui Liu, Xiaopeng Wang, Runkun Liu, Xiantong Zhen, Pei Dong, Zhen Qian</dc:creator>
    </item>
    <item>
      <title>S$^3$-MonoDETR: Supervised Shape&amp;Scale-perceptive Deformable Transformer for Monocular 3D Object Detection</title>
      <link>https://arxiv.org/abs/2309.00928</link>
      <description>arXiv:2309.00928v2 Announce Type: replace-cross 
Abstract: Recently, transformer-based methods have shown exceptional performance in monocular 3D object detection, which can predict 3D attributes from a single 2D image. These methods typically use visual and depth representations to generate query points on objects, whose quality plays a decisive role in the detection accuracy. However, current unsupervised attention mechanisms without any geometry appearance awareness in transformers are susceptible to producing noisy features for query points, which severely limits the network performance and also makes the model have a poor ability to detect multi-category objects in a single training process. To tackle this problem, this paper proposes a novel ``Supervised Shape&amp;Scale-perceptive Deformable Attention'' (S$^3$-DA) module for monocular 3D object detection. Concretely, S$^3$-DA utilizes visual and depth features to generate diverse local features with various shapes and scales and predict the corresponding matching distribution simultaneously to impose valuable shape&amp;scale perception for each query. Benefiting from this, S$^3$-DA effectively estimates receptive fields for query points belonging to any category, enabling them to generate robust query features. Besides, we propose a Multi-classification-based Shape&amp;Scale Matching (MSM) loss to supervise the above process. Extensive experiments on KITTI and Waymo Open datasets demonstrate that S$^3$-DA significantly improves the detection accuracy, yielding state-of-the-art performance of single-category and multi-category 3D object detection in a single training process compared to the existing approaches. The source code will be made publicly available at https://github.com/mikasa3lili/S3-MonoDETR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.00928v2</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <category>eess.IV</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuan He, Jin Yuan, Kailun Yang, Zhenchao Zeng, Zhiyong Li</dc:creator>
    </item>
    <item>
      <title>Predicting Gradient is Better: Exploring Self-Supervised Learning for SAR ATR with a Joint-Embedding Predictive Architecture</title>
      <link>https://arxiv.org/abs/2311.15153</link>
      <description>arXiv:2311.15153v5 Announce Type: replace-cross 
Abstract: The growing Synthetic Aperture Radar (SAR) data has the potential to build a foundation model through Self-Supervised Learning (SSL) methods, which can achieve various SAR Automatic Target Recognition (ATR) tasks with pre-training in large-scale unlabeled data and fine-tuning in small labeled samples. SSL aims to construct supervision signals directly from the data, which minimizes the need for expensive expert annotation and maximizes the use of the expanding data pool for a foundational model. This study investigates an effective SSL method for SAR ATR, which can pave the way for a foundation model in SAR ATR. The primary obstacles faced in SSL for SAR ATR are the small targets in remote sensing and speckle noise in SAR images, corresponding to the SSL approach and signals. To overcome these challenges, we present a novel Joint-Embedding Predictive Architecture for SAR ATR (SAR-JEPA), which leverages local masked patches to predict the multi-scale SAR gradient representations of unseen context. The key aspect of SAR-JEPA is integrating SAR domain features to ensure high-quality self-supervised signals as target features. Besides, we employ local masks and multi-scale features to accommodate the various small targets in remote sensing. By fine-tuning and evaluating our framework on three target recognition datasets (vehicle, ship, and aircraft) with four other datasets as pre-training, we demonstrate its outperformance over other SSL methods and its effectiveness with increasing SAR data. This study showcases the potential of SSL for SAR target recognition across diverse targets, scenes, and sensors.Our codes and weights are available in \url{https://github.com/waterdisappear/SAR-JEPA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15153v5</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weijie Li, Yang Wei, Tianpeng Liu, Yuenan Hou, Yuxuan Li, Zhen Liu, Yongxiang Liu, Li Liu</dc:creator>
    </item>
    <item>
      <title>OmniCount: Multi-label Object Counting with Semantic-Geometric Priors</title>
      <link>https://arxiv.org/abs/2403.05435</link>
      <description>arXiv:2403.05435v4 Announce Type: replace-cross 
Abstract: Object counting is pivotal for understanding the composition of scenes. Previously, this task was dominated by class-specific methods, which have gradually evolved into more adaptable class-agnostic strategies. However, these strategies come with their own set of limitations, such as the need for manual exemplar input and multiple passes for multiple categories, resulting in significant inefficiencies. This paper introduces a more practical approach enabling simultaneous counting of multiple object categories using an open-vocabulary framework. Our solution, OmniCount, stands out by using semantic and geometric insights (priors) from pre-trained models to count multiple categories of objects as specified by users, all without additional training. OmniCount distinguishes itself by generating precise object masks and leveraging varied interactive prompts via the Segment Anything Model for efficient counting. To evaluate OmniCount, we created the OmniCount-191 benchmark, a first-of-its-kind dataset with multi-label object counts, including points, bounding boxes, and VQA annotations. Our comprehensive evaluation in OmniCount-191, alongside other leading benchmarks, demonstrates OmniCount's exceptional performance, significantly outpacing existing solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05435v4</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anindya Mondal, Sauradip Nag, Xiatian Zhu, Anjan Dutta</dc:creator>
    </item>
    <item>
      <title>KunquDB: An Attempt for Speaker Verification in the Chinese Opera Scenario</title>
      <link>https://arxiv.org/abs/2403.13356</link>
      <description>arXiv:2403.13356v2 Announce Type: replace-cross 
Abstract: This work aims to promote Chinese opera research in both musical and speech domains, with a primary focus on overcoming the data limitations. We introduce KunquDB, a relatively large-scale, well-annotated audio-visual dataset comprising 339 speakers and 128 hours of content. Originating from the Kunqu Opera Art Canon (Kunqu yishu dadian), KunquDB is meticulously structured by dialogue lines, providing explicit annotations including character names, speaker names, gender information, vocal manner classifications, and accompanied by preliminary text transcriptions. KunquDB provides a versatile foundation for role-centric acoustic studies and advancements in speech-related research, including Automatic Speaker Verification (ASV). Beyond enriching opera research, this dataset bridges the gap between artistic expression and technological innovation. Pioneering the exploration of ASV in Chinese opera, we construct four test trials considering two distinct vocal manners in opera voices: stage speech (ST) and singing (S). Implementing domain adaptation methods effectively mitigates domain mismatches induced by these vocal manner variations while there is still room for further improvement as a benchmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13356v2</guid>
      <category>eess.AS</category>
      <category>cs.SD</category>
      <category>eess.IV</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huali Zhou, Yuke Lin, Dong Liu, Ming Li</dc:creator>
    </item>
    <item>
      <title>Enhancing Ship Classification in Optical Satellite Imagery: Integrating Convolutional Block Attention Module with ResNet for Improved Performance</title>
      <link>https://arxiv.org/abs/2404.02135</link>
      <description>arXiv:2404.02135v4 Announce Type: replace-cross 
Abstract: In this study, we present an advanced convolutional neural network (CNN) architecture for ship classification based on optical satellite imagery, which significantly enhances performance through the integration of a convolutional block attention module (CBAM) and additional architectural innovations. Building upon the foundational ResNet50 model, we first incorporated a standard CBAM to direct the model's focus toward more informative features, achieving an accuracy of 87% compared to 85% of the baseline ResNet50. Further augmentations involved multiscale feature integration, depthwise separable convolutions, and dilated convolutions, culminating in an enhanced ResNet model with improved CBAM. This model demonstrated a remarkable accuracy of 95%, with precision, recall, and F1 scores all witnessing substantial improvements across various ship classes. In particular, the bulk carrier and oil tanker classes exhibited nearly perfect precision and recall rates, underscoring the enhanced capability of the model to accurately identify and classify ships. Attention heatmap analyses further validated the efficacy of the improved model, revealing more focused attention on relevant ship features regardless of background complexities. These findings underscore the potential of integrating attention mechanisms and architectural innovations into CNNs for high-resolution satellite imagery classification. This study navigates through the class imbalance and computational costs and proposes future directions for scalability and adaptability in new or rare ship-type recognition. This study lays the groundwork for applying advanced deep learning techniques in remote sensing, offering insights into scalable and efficient satellite image classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02135v4</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ryan Donghan Kwon, Gangjoo Robin Nam, Jisoo Tak, Junseob Shin, Hyerin Cha, Seung Won Lee</dc:creator>
    </item>
    <item>
      <title>Recognizing Beam Profiles from Silicon Photonics Gratings using Transformer Model</title>
      <link>https://arxiv.org/abs/2408.10287</link>
      <description>arXiv:2408.10287v2 Announce Type: replace-cross 
Abstract: Over the past decade, there has been extensive work in developing integrated silicon photonics (SiPh) gratings for the optical addressing of trapped ion qubits in the ion trap quantum computing community. However, when viewing beam profiles from infrared (IR) cameras, it is often difficult to determine the corresponding heights where the beam profiles are located. In this work, we developed transformer models to recognize the corresponding height categories of beam profiles of light from SiPh gratings. The model is trained using two techniques: (1) input patches, and (2) input sequence. For model trained with input patches, the model achieved recognition accuracy of 0.938. Meanwhile, model trained with input sequence shows lower accuracy of 0.895. However, when repeating the model-training 150 cycles, model trained with input patches shows inconsistent accuracy ranges between 0.445 to 0.959, while model trained with input sequence exhibit higher accuracy values between 0.789 to 0.936. The obtained outcomes can be expanded to various applications, including auto-focusing of light beam and auto-adjustment of z-axis stage to acquire desired beam profiles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10287v2</guid>
      <category>physics.optics</category>
      <category>cs.AI</category>
      <category>eess.IV</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Dian Lim, Hong Yu Li, Simon Chun Kiat Goh, Xiangyu Wang, Peng Zhao, Chuan Seng Tan</dc:creator>
    </item>
    <item>
      <title>Generative AI in Industrial Machine Vision -- A Review</title>
      <link>https://arxiv.org/abs/2408.10775</link>
      <description>arXiv:2408.10775v2 Announce Type: replace-cross 
Abstract: Machine vision enhances automation, quality control, and operational efficiency in industrial applications by enabling machines to interpret and act on visual data. While traditional computer vision algorithms and approaches remain widely utilized, machine learning has become pivotal in current research activities. In particular, generative AI demonstrates promising potential by improving pattern recognition capabilities, through data augmentation, increasing image resolution, and identifying anomalies for quality control. However, the application of generative AI in machine vision is still in its early stages due to challenges in data diversity, computational requirements, and the necessity for robust validation methods. A comprehensive literature review is essential to understand the current state of generative AI in industrial machine vision, focusing on recent advancements, applications, and research trends. Thus, a literature review based on the PRISMA guidelines was conducted, analyzing over 1,200 papers on generative AI in industrial machine vision. Our findings reveal various patterns in current research, with the primary use of generative AI being data augmentation, for machine vision tasks such as classification and object detection. Furthermore, we gather a collection of application challenges together with data requirements to enable a successful application of generative AI in industrial machine vision. This overview aims to provide researchers with insights into the different areas and applications within current research, highlighting significant advancements and identifying opportunities for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10775v2</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hans Aoyang Zhou, Dominik Wolfschl\"ager, Constantinos Florides, Jonas Werheid, Hannes Behnen, Jan-Henrick Woltersmann, Tiago C. Pinto, Marco Kemmerling, Anas Abdelrazeq, Robert H. Schmitt</dc:creator>
    </item>
  </channel>
</rss>
