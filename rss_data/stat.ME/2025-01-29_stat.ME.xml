<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 Jan 2025 02:30:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Bias-Corrected and Variance-Corrected MLE for the New Median Based Unit Weibull Distribution (MBUW)</title>
      <link>https://arxiv.org/abs/2501.16853</link>
      <description>arXiv:2501.16853v1 Announce Type: new 
Abstract: As the maximum likelihood method is the most commonly used method for parameters estimation being unbiased, consistent, efficient, and asymptotically normal, MLE is used to fit the new distribution (MBUW). But in small to moderate sample size, this MLE estimator is biased unlike the MLE estimators obtained from large sample sizes. In this paper, the Bias-corrected approach for this distribution is discussed and applied to real data analysis. The MLE estimators of MBUW obtained from some optimization techniques like derivative free Nelder Mead algorithm suffers from significant high correlation that is reflected on high covariance between the parameters. Also this association between the parameters affects the variances which may be inflated enough to approach infinity hampering construction of confidence intervals for each parameter. This problem may arise with any optimization technique which necessitates remedies trying to fix it. The author also elaborates a variance correction approach heavily relaying on re-parameterizing the negative log likelihood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16853v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Mohammed Attia</dc:creator>
    </item>
    <item>
      <title>Rethinking the Win Ratio: A Causal Framework for Hierarchical Outcome Analysis</title>
      <link>https://arxiv.org/abs/2501.16933</link>
      <description>arXiv:2501.16933v1 Announce Type: new 
Abstract: Quantifying causal effects in the presence of complex and multivariate outcomes is a key challenge to evaluate treatment effects. For \emph{hierarchical} multivarariates outcomes, the FDA recommends the Win Ratio and Generalized Pairwise Comparisons approaches \cite{Pocock2011winratio,Buyse2010}. However, as far as we know, these empirical methods lack causal or statistical foundations to justify their broader use in recent studies. To address this gap, we establish causal foundations for hierarchical comparison methods. We define related causal effect measures, and highlight that depending on the methodology used to compute Win Ratios or Net Benefits of treatments, the causal estimand targeted can be different, as proved by our consistency results. Quite dramatically, it appears that the causal estimand related to the historical estimation approach can yield reversed and incorrect treatment recommendations in heterogeneous populations, as we illustrate through striking examples. In order to compensate for this fallacy, we introduce a novel, individual-level yet identifiable causal effect measure that better approximates the ideal, non-identifiable individual-level estimand. We prove that computing Win Ratio or Net Benefits using a Nearest Neighbor pairing approach between treated and controlled patients, an approach that can be seen as an extreme form of stratification, leads to estimating this new causal estimand measure. We extend our methods to observational settings via propensity weighting, distributional regression to address the curse of dimensionality, and a doubly robust framework. We prove the consistency of our methods, and the double robustness of our augmented estimator. These methods are straightforward to implement, making them accessible to practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16933v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathieu Even, Julie Josse</dc:creator>
    </item>
    <item>
      <title>A totally non-compensatory multi-criteria method for evaluating and improving level of satisfaction (LoS): proposal and application on Airport Terminal of Passengers</title>
      <link>https://arxiv.org/abs/2501.16979</link>
      <description>arXiv:2501.16979v1 Announce Type: new 
Abstract: To evaluate and assign a service according customer's level of satisfaction (LoS) is a relevant issue in operations management. This is a typical situation in which the evaluators, have passed by heterogeneous experiences along their life which implies they could consider different variables when evaluating a product. Despite it, the models for measuring Los usually consider a homogeneous set of criteria when facing LoS evaluation. This study applies a totally non-compensatory modeling that allows each customer to select the criteria, from a whole set of aspects, the customer wants to use for evaluating LoS. The proposal was tested in evaluating LoS regarding the services provided by Airport Terminal of Passengers (ATPs) in Brazil, with data collected in a survey involving 19,240 passengers, interviewed at 15 Brazilian international airports. The data collected was imputed into ELECTRE TRI ME algorithm to obtain the a credibility degree of sorting the instances. The values of credibility degree were them used to obtain groups of ATPs. Finally, the statistical modes of the evaluations in each group were analyzed and compared. The proposal allowed a full non-compensatory approach to obtain the credibility degree even when considering perceptions from several evaluators that could use different criteria. As a result, it was identified, for each cluster of ATP, the criteria sets to be improved and even those to be prioritized. The pioneer modeling proposed in this article for evaluating LoS plus its instancing in ATPs terminals represents an original advance in the establishment of a multi-criteria decision aid (MCDA) model to assess the quality of services and fills a relevant gap for a full non-compensatory approach able to classify the LoS in the airport context, considering perceptions of multiple evaluators even if they use different criteria in their evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16979v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Phelipe Medeiros da Rocha, Helder Gomes Costa</dc:creator>
    </item>
    <item>
      <title>Nonparametric methods controlling the median of the false discovery proportion</title>
      <link>https://arxiv.org/abs/2501.16985</link>
      <description>arXiv:2501.16985v2 Announce Type: new 
Abstract: When testing many hypotheses, often we do not have strong expectations about the directions of the effects. In some situations however, the alternative hypotheses are that the parameters lie in a certain direction or interval, and it is in fact expected that most hypotheses are false. This is often the case when researchers perform multiple noninferiority or equivalence tests, e.g. when testing food safety with metabolite data. The goal is then to use data to corroborate the expectation that most hypotheses are false. We propose a nonparametric multiple testing approach that is powerful in such situations. If the user's expectations are wrong, our approach will still be valid but have low power. Of course all multiple testing methods become more powerful when appropriate one-sided instead of two-sided tests are used, but our approach has superior power then. The methods in this paper control the median of the false discovery proportion (FDP), which is the fraction of false discoveries among the rejected hypotheses. This approach is comparable to false discovery rate control, where one ensures that the mean rather than the median of the FDP is small. Our procedures make use of a symmetry property of the test statistics, do not require independence and are valid for finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16985v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesse Hemerik</dc:creator>
    </item>
    <item>
      <title>A Bayesian semi-parametric model for longitudinal growth and appetite phenotypes in children</title>
      <link>https://arxiv.org/abs/2501.17040</link>
      <description>arXiv:2501.17040v1 Announce Type: new 
Abstract: This study develops a Bayesian semi-parametric model to examine the longitudinal growth and appetite phenotypes in children from the GUSTO cohort, with a focus on understanding the relationship between eating behaviours and growth outcomes over time. While eating behaviours, such as picky eating, have been shown to influence future weight and obesity risk, their developmental patterns and associations with growth trajectories remain under-explored. This work addresses these gaps by modelling longitudinal data, including both growth metrics (e.g., BMI) and eating behaviours (e.g., Child Eating Behaviour Questionnaire, CEBQ), across multiple time points. We extend the Partial Credit Model, commonly used for questionnaire data analysis, to accommodate repeated measurements and incorporate covariates. The growth outcomes are modelled using flexible splines regression. The two components of the model are linked through a shared Bayesian nonparametric prior distribution, specifically a Normalized Generalized Gamma process, allowing to identify clinically relevant subgroups. This joint modelling approach offers a more nuanced understanding of how early eating behaviours relate to growth patterns and developmental outcomes, providing insights into childhood obesity risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17040v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andrea Cremaschi, Beatrice Franzolini, Maria De Iorio, Mary Chong, Toh Jia Ying, Navin Michael, Varsha Gupta, Fabian Yap, Yung Seng Lee, Johan Erikkson, Anna Fogel</dc:creator>
    </item>
    <item>
      <title>Goodness of Fit for Bayesian Generative Models with Applications in Population Genetics</title>
      <link>https://arxiv.org/abs/2501.17107</link>
      <description>arXiv:2501.17107v1 Announce Type: new 
Abstract: In population genetics and other application fields, models with intractable likelihood are common. Approximate Bayesian Computation (ABC) or more generally Simulation-Based Inference (SBI) methods work by simulating instrumental data sets from the models under study and comparing them with the observed data set, using advanced machine learning tools for tasks such as model selection and parameter inference. The present work focuses on model criticism, and more specifically on Goodness of fit (GoF) tests, for intractable likelihood models. We introduce two new GoF tests: the pre-inference \gof tests whether the observed dataset is distributed from the prior predictive distribution, while the post-inference GoF tests whether there is a parameter value such that the observed dataset is distributed from the likelihood with that value. The pre-inference test can be used to prune a large set of models using a limited amount of simulations, while the post-inference test is used to assess the fit of a selected model. Both tests are based on the Local Outlier Factor (LOF, Breunig et al., 2000). This indicator was initially defined for outlier and novelty detection. It is able to quantify local density deviations, capturing subtleties that a more traditional k-NN-based approach may miss. We evaluated the performance of our two GoF tests on simulated datasets from three different model settings of varying complexity. We then illustrate the utility of these approaches on a dataset of single nucleotide polymorphism (SNP) markers for the evaluation of complex evolutionary scenarios of modern human populations. Our dual-test GoF approach highlights the flexibility of our method: the pre-inference \gof test provides insight into model validity from a Bayesian perspective, while the post-inference test provides a more general and traditional view of assessing goodness of fit</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17107v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guillaume Le Mailloux, Paul Bastide, Jean-Michel Marin, Arnaud Estoup</dc:creator>
    </item>
    <item>
      <title>Ancestral Inference and Learning for Branching Processes in Random Environments</title>
      <link>https://arxiv.org/abs/2501.16526</link>
      <description>arXiv:2501.16526v1 Announce Type: cross 
Abstract: Ancestral inference for branching processes in random environments involves determining the ancestor distribution parameters using the population sizes of descendant generations. In this paper, we introduce a new methodology for ancestral inference utilizing the generalized method of moments. We demonstrate that the estimator's behavior is critically influenced by the coefficient of variation of the environment sequence. Furthermore, despite the process's evolution being heavily dependent on the offspring means of various generations, we show that the joint limiting distribution of the ancestor and offspring estimators of the mean, under appropriate centering and scaling, decouple and converge to independent Gaussian random variables when the ratio of the number of generations to the logarithm of the number of replicates converges to zero. Additionally, we provide estimators for the limiting variance and illustrate our findings through numerical experiments and data from Polymerase Chain Reaction experiments and COVID-19 data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16526v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoran Jiang, Anand N. Vidyashankar</dc:creator>
    </item>
    <item>
      <title>C-HDNet: A Fast Hyperdimensional Computing Based Method for Causal Effect Estimation from Networked Observational Data</title>
      <link>https://arxiv.org/abs/2501.16562</link>
      <description>arXiv:2501.16562v1 Announce Type: cross 
Abstract: We consider the problem of estimating causal effects from observational data in the presence of network confounding. In this context, an individual's treatment assignment and outcomes may be affected by their neighbors within the network. We propose a novel matching technique which leverages hyperdimensional computing to model network information and improve predictive performance. We present results of extensive experiments which show that the proposed method outperforms or is competitive with the state-of-the-art methods for causal effect estimation from network data, including advanced computationally demanding deep learning methods. Further, our technique benefits from simplicity and speed, with roughly an order of magnitude lower runtime compared to state-of-the-art methods, while offering similar causal effect estimation error rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16562v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhishek Dalvi, Neil Ashtekar, Vasant Honavar</dc:creator>
    </item>
    <item>
      <title>A New Family of Regression Models for $[0,1]$ Outcome Data: Expanding the Palette</title>
      <link>https://arxiv.org/abs/2306.04708</link>
      <description>arXiv:2306.04708v2 Announce Type: replace 
Abstract: Beta regression is a popular methodology when the outcome variable $y$ is on the open interval $(0,1)$. When $y$ is in the closed interval $[0,1]$, it is commonly accepted that beta regression is inapplicable. Instead, common solutions are to use augmented beta regression or censoring models or else to subjectively rescale the endpoints to allow beta regression. We provide an attractive new approach with a family of models that treats the entirety of $y\in[0,1]$ in a single model without rescaling or the need for the complications of augmentation or censoring. This family provides the interpretational convenience of a single straightforward model for the expectation of $y \in [0,1]$ over its entirety. We establish the conditions for the existence of a unique MLE and then examine this new family of models from both maximum-likelihood and Bayesian perspectives. We successfully apply the models to employment data in which augmented beta regression was difficult due to data separation. We also apply the models to healthcare panel data that were originally examined by way of rescaling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.04708v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eugene D. Hahn</dc:creator>
    </item>
    <item>
      <title>Two-sample inference for sparse functional data</title>
      <link>https://arxiv.org/abs/2312.07727</link>
      <description>arXiv:2312.07727v3 Announce Type: replace 
Abstract: We propose a novel test procedure for comparing mean functions across two groups within the reproducing kernel Hilbert space (RKHS) framework. Our proposed method is adept at handling sparsely and irregularly sampled functional data when observation times are random for each subject. Conventional approaches, which are built upon functional principal components analysis, usually assume a homogeneous covariance structure across groups. Nonetheless, justifying this assumption in real-world scenarios can be challenging. To eliminate the need for a homogeneous covariance structure, we first develop a linear approximation for the mean estimator under the RKHS framework; this approximation is a sum of i.i.d. random elements, which naturally leads to the desirable pointwise limiting distributions. Moreover, we establish weak convergence for the mean estimator, allowing us to construct a test statistic for the mean difference. Our method is easily implementable and outperforms some conventional tests in controlling type I errors across various settings. We demonstrate the finite sample performance of our approach through extensive simulations and two real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07727v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chi Zhang, Peijun Sang, Yingli Qin</dc:creator>
    </item>
    <item>
      <title>Differentially Private Boxplots</title>
      <link>https://arxiv.org/abs/2405.20415</link>
      <description>arXiv:2405.20415v3 Announce Type: replace 
Abstract: Despite the potential of differentially private data visualization to harmonize data analysis and privacy, research in this area remains underdeveloped. Boxplots are a widely popular visualization used for summarizing a dataset and for comparison of multiple datasets. Consequentially, we introduce a differentially private boxplot. We evaluate its effectiveness for displaying location, scale, skewness and tails of a given empirical distribution. In our theoretical exposition, we show that the location and scale of the boxplot are estimated with optimal sample complexity, and the skewness and tails are estimated consistently, which is not always the case for a boxplot naively constructed from a single existing differentially private quantile algorithm. As a byproduct of this exposition, we introduce several new results concerning private quantile estimation. In simulations, we show that this boxplot performs similarly to a non-private boxplot, and it outperforms the naive boxplot. Additionally, we conduct a real data analysis of Airbnb listings, which shows that comparable analysis can be achieved through differentially private boxplot visualization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20415v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kelly Ramsay, Jairo Diaz-Rodriguez</dc:creator>
    </item>
    <item>
      <title>Addressing Confounding and Continuous Exposure Measurement Error Using Corrected Score Functions</title>
      <link>https://arxiv.org/abs/2407.09443</link>
      <description>arXiv:2407.09443v2 Announce Type: replace 
Abstract: Confounding and exposure measurement error can introduce bias when drawing inference about the marginal effect of an exposure on an outcome of interest. While there are broad methodologies for addressing each source of bias individually, confounding and exposure measurement error frequently co-occur, and there is a need for methods that address them simultaneously. In this paper, corrected score methods are derived under classical additive measurement error to draw inference about marginal exposure effects using only measured variables. Three estimators are proposed based on g-formula, inverse probability weighting, and doubly-robust estimation techniques. The estimators are shown to be consistent and asymptotically normal, and the doubly-robust estimator is shown to exhibit its namesake property. The methods, which are implemented in the R package mismex, perform well in finite samples under both confounding and measurement error as demonstrated by simulation studies. The proposed doubly-robust estimator is applied to study the effects of two biomarkers on HIV-1 infection using data from the HVTN 505 preventative vaccine trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09443v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brian D. Richardson, Bryan S. Blette, Peter B. Gilbert, Michael G. Hudgens</dc:creator>
    </item>
    <item>
      <title>Sample size and power calculation for propensity score analysis of observational studies</title>
      <link>https://arxiv.org/abs/2501.11181</link>
      <description>arXiv:2501.11181v2 Announce Type: replace 
Abstract: This paper develops theoretically justified analytical formulas for sample size and power calculation in the propensity score analysis of causal inference using observational data. By analyzing the variance of the inverse probability weighting estimator of the average treatment effect (ATE), we clarify the three key components for sample size calculations: propensity score distribution, potential outcome distribution, and their correlation. We devise analytical procedures to identify these components based on commonly available and interpretable summary statistics. We elucidate the critical role of covariate overlap between treatment groups in determining the sample size. In particular, we propose to use the Bhattacharyya coefficient as a measure of covariate overlap, which, together with the treatment proportion, leads to a uniquely identifiable and easily computable propensity score distribution. The proposed method is applicable to both continuous and binary outcomes. We show that the standard two-sample $z$-test and variance inflation factor methods often lead to, sometimes vastly, inaccurate sample size estimates, especially with limited overlap. We also derive formulas for the average treatment effects for the treated (ATT) and overlapped population (ATO) estimands. We provide simulated and real examples to illustrate the proposed method. We develop an associated R package PSpower.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11181v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bo Liu, Xiaoxiao Zhou, Fan Li</dc:creator>
    </item>
    <item>
      <title>Causal clustering: design of cluster experiments under network interference</title>
      <link>https://arxiv.org/abs/2310.14983</link>
      <description>arXiv:2310.14983v3 Announce Type: replace-cross 
Abstract: This paper studies the design of cluster experiments to estimate the global treatment effect in the presence of network spillovers. We provide a framework to choose the clustering that minimizes the worst-case mean-squared error of the estimated global effect. We show that optimal clustering solves a novel penalized min-cut optimization problem computed via off-the-shelf semi-definite programming algorithms. Our analysis also characterizes simple conditions to choose between any two cluster designs, including choosing between a cluster or individual-level randomization. We illustrate the method's properties using unique network data from the universe of Facebook's users and existing data from a field experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.14983v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Davide Viviano, Lihua Lei, Guido Imbens, Brian Karrer, Okke Schrijvers, Liang Shi</dc:creator>
    </item>
    <item>
      <title>A Bayesian hierarchical mixture cure modelling framework to utilize multiple survival datasets for long-term survivorship estimates: A case study from previously untreated metastatic melanoma</title>
      <link>https://arxiv.org/abs/2401.13820</link>
      <description>arXiv:2401.13820v2 Announce Type: replace-cross 
Abstract: Time to an event of interest over a lifetime is a central measure of the clinical benefit of an intervention used in a health technology assessment (HTA). Within the same trial, multiple end-points may also be considered. For example, overall and progression-free survival time for different drugs in oncology studies. A common challenge is when an intervention is only effective for some proportion of the population who are not clinically identifiable. Therefore, latent group membership as well as separate survival models for identified groups need to be estimated. However, follow-up in trials may be relatively short leading to substantial censoring. We present a general Bayesian hierarchical framework that can handle this complexity by exploiting the similarity of cure fractions between end-points; accounting for the correlation between them and improving the extrapolation beyond the observed data. Assuming exchangeability between cure fractions facilitates the borrowing of information between end-points. We undertake a comprehensive simulation study to evaluate the model performance under different scenarios. We also show the benefits of using our approach with a motivating example, the CheckMate 067 phase 3 trial consisting of patients with metastatic melanoma treated with first line therapy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13820v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Green, Murat Kurt, Andriy Moshyk, James Larkin, Gianluca Baio</dc:creator>
    </item>
    <item>
      <title>Predictive variational inference: Learn the predictively optimal posterior distribution</title>
      <link>https://arxiv.org/abs/2410.14843</link>
      <description>arXiv:2410.14843v2 Announce Type: replace-cross 
Abstract: Vanilla variational inference finds an optimal approximation to the Bayesian posterior distribution, but even the exact Bayesian posterior is often not meaningful under model misspecification. We propose predictive variational inference (PVI): a general inference framework that seeks and samples from an optimal posterior density such that the resulting posterior predictive distribution is as close to the true data generating process as possible, while this closeness is measured by multiple scoring rules. By optimizing the objective, the predictive variational inference is generally not the same as, or even attempting to approximate, the Bayesian posterior, even asymptotically. Rather, we interpret it as implicit hierarchical expansion. Further, the learned posterior uncertainty detects heterogeneity of parameters among the population, enabling automatic model diagnosis. This framework applies to both likelihood-exact and likelihood-free models. We demonstrate its application in real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14843v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinlin Lai, Yuling Yao</dc:creator>
    </item>
  </channel>
</rss>
