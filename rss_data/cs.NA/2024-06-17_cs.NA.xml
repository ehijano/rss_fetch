<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Jun 2024 02:54:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Mixed finite element methods for elliptic obstacle problems</title>
      <link>https://arxiv.org/abs/2406.09605</link>
      <description>arXiv:2406.09605v1 Announce Type: new 
Abstract: Mixed variational formulations for the first-order system of the elastic membrane obstacle problem and the second-order system of the Kirchhoff--Love plate obstacle problem are proposed. The force exerted by the rigid obstacle is included as a new unknown. A priori and a posteriori error estimates are derived for both obstacle problems. The a posteriori error estimates are based on conforming postprocessed solutions. Numerical experiments conclude this work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09605v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas F\"uhrer, Francisco Fuica</dc:creator>
    </item>
    <item>
      <title>Convergence rate of nonlinear delayed McKean-Vlasov SDEs driven by fractional Brownian motions</title>
      <link>https://arxiv.org/abs/2406.09678</link>
      <description>arXiv:2406.09678v1 Announce Type: new 
Abstract: In this paper, our main aim is to investigate the strong convergence for a McKean-Vlasov stochastic differential equation with super-linear delay driven by fractional Brownian motion with Hurst exponent $H\in(1/2, 1)$. After giving uniqueness and existence for the exact solution, we analyze the properties including boundedness of moment and propagation of chaos. Besides, we give the Euler-Maruyama (EM) scheme and show that the numerical solution converges strongly to the exact solution. Furthermore, a corresponding numerical example is given to illustrate the theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09678v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shengrong Wang, Jie Xie, Li Tan</dc:creator>
    </item>
    <item>
      <title>Differentiable Programming for Differential Equations: A Review</title>
      <link>https://arxiv.org/abs/2406.09699</link>
      <description>arXiv:2406.09699v1 Announce Type: new 
Abstract: The differentiable programming paradigm is a cornerstone of modern scientific computing. It refers to numerical methods for computing the gradient of a numerical model's output. Many scientific models are based on differential equations, where differentiable programming plays a crucial role in calculating model sensitivities, inverting model parameters, and training hybrid models that combine differential equations with data-driven approaches. Furthermore, recognizing the strong synergies between inverse methods and machine learning offers the opportunity to establish a coherent framework applicable to both fields. Differentiating functions based on the numerical solution of differential equations is non-trivial. Numerous methods based on a wide variety of paradigms have been proposed in the literature, each with pros and cons specific to the type of problem investigated. Here, we provide a comprehensive review of existing techniques to compute derivatives of numerical solutions of differential equations. We first discuss the importance of gradients of solutions of differential equations in a variety of scientific domains. Second, we lay out the mathematical foundations of the various approaches and compare them with each other. Third, we cover the computational considerations and explore the solutions available in modern scientific software. Last but not least, we provide best-practices and recommendations for practitioners. We hope that this work accelerates the fusion of scientific models and data, and fosters a modern approach to scientific modelling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09699v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>physics.comp-ph</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Facundo Sapienza, Jordi Bolibar, Frank Sch\"afer, Brian Groenke, Avik Pal, Victor Boussange, Patrick Heimbach, Giles Hooker, Fernando P\'erez, Per-Olof Persson, Christopher Rackauckas</dc:creator>
    </item>
    <item>
      <title>A semi-implicit stochastic multiscale method for radiative heat transfer problem</title>
      <link>https://arxiv.org/abs/2406.09775</link>
      <description>arXiv:2406.09775v1 Announce Type: new 
Abstract: In this paper, we propose and analyze a new semi-implicit stochastic multiscale method for the radiative heat transfer problem with additive noise fluctuation in composite materials. In the proposed method, the strong nonlinearity term induced by heat radiation is first approximated, by a semi-implicit predictor-corrected numerical scheme, for each fixed time step, resulting in a spatially random multiscale heat transfer equation. Then, the infinite-dimensional stochastic processes are modeled and truncated using a complete orthogonal system, facilitating the reduction of the model's dimensionality in the random space. The resulting low-rank random multiscale heat transfer equation is approximated and computed by using efficient spatial basis functions based multiscale method. The main advantage of the proposed method is that it separates the computational difficulty caused by the spatial multiscale properties, the high-dimensional randomness and the strong nonlinearity of the solution, so they can be overcome separately using different strategies. The convergence analysis is carried out, and the optimal rate of convergence is also obtained for the proposed semi-implicit stochastic multiscale method. Numerical experiments on several test problems for composite materials with various microstructures are also presented to gauge the efficiency and accuracy of the proposed semi-implicit stochastic multiscale method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09775v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shan Zhang, Yajun Wang, Xiaofei Guan</dc:creator>
    </item>
    <item>
      <title>Localized subspace iteration methods for elliptic multiscale problems</title>
      <link>https://arxiv.org/abs/2406.09789</link>
      <description>arXiv:2406.09789v1 Announce Type: new 
Abstract: This paper proposes localized subspace iteration (LSI) methods to construct generalized finite element basis functions for elliptic problems with multiscale coefficients. The key components of the proposed method consist of the localization of the original differential operator and the subspace iteration of the corresponding local spectral problems, where the localization is conducted by enforcing the local homogeneous Dirichlet condition and the partition of the unity functions. From a novel perspective, some multiscale methods can be regarded as one iteration step under approximating the eigenspace of the corresponding local spectral problems. Vice versa, new multiscale methods can be designed through subspaces of spectral problem algorithms. Then, we propose the efficient localized standard subspace iteration (LSSI) method and the localized Krylov subspace iteration (LKSI) method based on the standard subspace and Krylov subspace, respectively. Convergence analysis is carried out for the proposed method. Various numerical examples demonstrate the effectiveness of our methods. In addition, the proposed methods show significant superiority in treating long-channel cases over other well-known multiscale methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09789v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaofei Guan, Lijian Jiang, Yajun Wang, Zihao Yang</dc:creator>
    </item>
    <item>
      <title>A-posteriori-steered $p$-robust multigrid and domain decomposition methods with optimal step-sizes for mixed finite element discretizations of elliptic problems</title>
      <link>https://arxiv.org/abs/2406.09872</link>
      <description>arXiv:2406.09872v1 Announce Type: new 
Abstract: In this work, we develop algebraic solvers for linear systems arising from the discretization of second-order elliptic problems by saddle-point mixed finite element methods of arbitrary polynomial degree $p \ge 0$. We present a multigrid and a two-level domain decomposition approach in two or three space dimensions, which are steered by their respective a posteriori estimators of the algebraic error. First, we extend the results of [A. Mira\c{c}i, J. Pape\v{z}, and M. Vohral\'ik, SIAM J. Sci. Comput. 43 (2021), S117--S145] to the mixed finite element setting. Extending the multigrid procedure itself is rather natural. To obtain analogous theoretical results, however, a multilevel stable decomposition of the velocity space is needed. In two space dimensions, we can treat the velocity space as the curl of a stream-function space, for which the previous results apply. In three space dimensions, we design a novel stable decomposition by combining a one-level high-order local stable decomposition of [Chaumont-Frelet and Vohral\'ik, SIAM J. Numer. Anal. 61 (2023), 1783--1818] and a multilevel lowest-order stable decomposition of [Hiptmair, Wu, and Zheng, Numer. Math. Theory Methods Appl. 5 (2012), 297--332]. This allows us to prove that our multigrid solver contracts the algebraic error at each iteration and, simultaneously, that the associated a posteriori estimator is efficient. A $p$-robust contraction is shown in two space dimensions. Next, we use this multilevel methodology to define a two-level domain decomposition method where the subdomains consist of overlapping patches of coarse-level elements sharing a common coarse-level vertex. We again establish a ($p$-robust) contraction of the solver and efficiency of the a posteriori estimator. Numerical results presented both for the multigrid approach and the domain decomposition method confirm the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09872v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ani Mira\c{c}i, Jan Pape\v{z}, Martin Vohral\'ik, Ivan Yotov</dc:creator>
    </item>
    <item>
      <title>Asymptotic quadratic convergence of the Gauss-Newton method for complex phase retrieval</title>
      <link>https://arxiv.org/abs/2406.09903</link>
      <description>arXiv:2406.09903v1 Announce Type: new 
Abstract: In this paper, we introduce a Gauss-Newton method for solving the complex phase retrieval problem. In contrast to the real-valued setting, the Gauss-Newton matrix for complex-valued signals is rank-deficient and, thus, non-invertible. To address this, we utilize a Gauss-Newton step that moves orthogonally to certain trivial directions. We establish that this modified Gauss-Newton step has a closed-form solution, which corresponds precisely to the minimal-norm solution of the associated least squares problem. Additionally, using the leave-one-out technique, we demonstrate that $m\ge O( n\log^3 n)$ independent complex Gaussian random measurements ensures that the entire trajectory of the Gauss-Newton iterations remains confined within a specific region of incoherence and contraction with high probability. This finding allows us to establish the asymptotic quadratic convergence rate of the Gauss-Newton method without the need of sample splitting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09903v1</guid>
      <category>math.NA</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meng Huang</dc:creator>
    </item>
    <item>
      <title>Sparse Nystrom Approximation of Currents and Varifolds</title>
      <link>https://arxiv.org/abs/2406.09932</link>
      <description>arXiv:2406.09932v1 Announce Type: new 
Abstract: We derive an algorithm for compression of the currents and varifolds representations of shapes, using the Nystrom approximation in Reproducing Kernel Hilbert Spaces. Our method is faster than existing compression techniques, and comes with theoretical guarantees on the rate of convergence of the compressed approximation, as a function of the smoothness of the associated shape representation. The obtained compression are shown to be useful for down-line tasks such as nonlinear shape registration in the Large Deformation Metric Mapping (LDDMM) framework, even for very high compression ratios. The performance of our algorithm is demonstrated on large-scale shape data from modern geometry processing datasets, and is shown to be fast and scalable with rapid error decay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09932v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Allen Paul, Neill Campbell, Tony Shardlow</dc:creator>
    </item>
    <item>
      <title>Convergence of splitting methods on rotating grids for the magnetized Vlasov equation</title>
      <link>https://arxiv.org/abs/2406.09941</link>
      <description>arXiv:2406.09941v1 Announce Type: new 
Abstract: Semi-Lagrangian solvers for the Vlasov system offer noiseless solutions compared to Lagrangian particle methods and can handle larger time steps compared to Eulerian methods. In order to reduce the computational complexity of the interpolation steps, it is common to use a directional splitting. However, this typically yields the wrong angular velocity. In this paper, we analyze a semi-Lagrangian method that treats the $v \times B$ term with a rotational grid and combines this with a directional splitting for the remaining terms. We analyze the convergence properties of the scheme both analytically and numerically. The favorable numerical properties of the rotating grid solution are demonstrated for the case of ion Bernstein waves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09941v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nils Schild, Mario Raeth Klaus Hallatschek, Katharina Kormann</dc:creator>
    </item>
    <item>
      <title>Function Reconstruction Using Rank-1 Lattices and Lower Sets</title>
      <link>https://arxiv.org/abs/2406.10145</link>
      <description>arXiv:2406.10145v1 Announce Type: new 
Abstract: Our study focuses on constructing optimal rank-1 lattices that enable exact integration and reconstruction of functions in the Chebyshev space, based on finite index sets. We introduce novel theoretical lower bounds on the minimum number of integrands needed for reconstruction, show equivalence between different plans for reconstruction under certain conditions and propose an innovative algorithm for generating the optimal generator vector of rank-1 lattices. By leveraging the inherent structure of the set of interpolators, our approach ensures admissibility conditions through exhaustive search and verification, outperforming existing methods in terms of computation time and memory usage. Numerical experiments validate the efficiency and practical applicability of our algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10145v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.NT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moulay Abdellah Chkifa, Abdelqoddous Moussa</dc:creator>
    </item>
    <item>
      <title>Approximate Contraction of Arbitrary Tensor Networks with a Flexible and Efficient Density Matrix Algorithm</title>
      <link>https://arxiv.org/abs/2406.09769</link>
      <description>arXiv:2406.09769v1 Announce Type: cross 
Abstract: Tensor network contractions are widely used in statistical physics, quantum computing, and computer science. We introduce a method to efficiently approximate tensor network contractions using low-rank approximations, where each intermediate tensor generated during the contractions is approximated as a low-rank binary tree tensor network. The proposed algorithm has the flexibility to incorporate a large portion of the environment when performing low-rank approximations, which can lead to high accuracy for a given rank. Here, the environment refers to the remaining set of tensors in the network, and low-rank approximations with larger environments can generally provide higher accuracy. For contracting tensor networks defined on lattices, the proposed algorithm can be viewed as a generalization of the standard boundary-based algorithms. In addition, the algorithm includes a cost-efficient density matrix algorithm for approximating a tensor network with a general graph structure into a tree structure, whose computational cost is asymptotically upper-bounded by that of the standard algorithm that uses canonicalization. Experimental results indicate that the proposed technique outperforms previously proposed approximate tensor network contraction algorithms for multiple problems in terms of both accuracy and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09769v1</guid>
      <category>quant-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linjian Ma, Matthew Fishman, Miles Stoudenmire, Edgar Solomonik</dc:creator>
    </item>
    <item>
      <title>A lightweight residual network for unsupervised deformable image registration</title>
      <link>https://arxiv.org/abs/2406.09774</link>
      <description>arXiv:2406.09774v1 Announce Type: cross 
Abstract: Accurate volumetric image registration is highly relevant for clinical routines and computer-aided medical diagnosis. Recently, researchers have begun to use transformers in learning-based methods for medical image registration, and have achieved remarkable success. Due to the strong global modeling capability, Transformers are considered a better option than convolutional neural networks (CNNs) for registration. However, they use bulky models with huge parameter sets, which require high computation edge devices for deployment as portable devices or in hospitals. Transformers also need a large amount of training data to produce significant results, and it is often challenging to collect suitable annotated data. Although existing CNN-based image registration can offer rich local information, their global modeling capability is poor for handling long-distance information interaction and limits registration performance. In this work, we propose a CNN-based registration method with an enhanced receptive field, a low number of parameters, and significant results on a limited training dataset. For this, we propose a residual U-Net with embedded parallel dilated-convolutional blocks to enhance the receptive field. The proposed method is evaluated on inter-patient and atlas-based datasets. We show that the performance of the proposed method is comparable and slightly better than transformer-based methods by using only $\SI{1.5}{\percent}$ of its number of parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09774v1</guid>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahsan Raza Siyal, Astrid Ellen Grams, Markus Haltmeier</dc:creator>
    </item>
    <item>
      <title>DeltaPhi: Learning Physical Trajectory Residual for PDE Solving</title>
      <link>https://arxiv.org/abs/2406.09795</link>
      <description>arXiv:2406.09795v1 Announce Type: cross 
Abstract: Although neural operator networks theoretically approximate any operator mapping, the limited generalization capability prevents them from learning correct physical dynamics when potential data biases exist, particularly in the practical PDE solving scenario where the available data amount is restricted or the resolution is extremely low. To address this issue, we propose and formulate the Physical Trajectory Residual Learning (DeltaPhi), which learns to predict the physical residuals between the pending solved trajectory and a known similar auxiliary trajectory. First, we transform the direct operator mapping between input-output function fields in original training data to residual operator mapping between input function pairs and output function residuals. Next, we learn the surrogate model for the residual operator mapping based on existing neural operator networks. Additionally, we design helpful customized auxiliary inputs for efficient optimization. Through extensive experiments, we conclude that, compared to direct learning, physical residual learning is preferred for PDE solving.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09795v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xihang Yue, Linchao Zhu, Yi Yang</dc:creator>
    </item>
    <item>
      <title>Sparse Tensors and Subdivision Methods for Finding the Zero Set of Polynomial Equations</title>
      <link>https://arxiv.org/abs/2406.09857</link>
      <description>arXiv:2406.09857v1 Announce Type: cross 
Abstract: Finding the solutions to a system of multivariate polynomial equations is a fundamental problem in mathematics and computer science. It involves evaluating the polynomials at many points, often chosen from a grid.  In most current methods, such as subdivision, homotopy continuation, or marching cube algorithms, polynomial evaluation is treated as a black box, repeating the process for each point. We propose a new approach that partially evaluates the polynomials, allowing us to efficiently reuse computations across multiple points in a grid. Our method leverages the Compressed Sparse Fiber data structure to efficiently store and process subsets of grid points. We integrated our amortized evaluation scheme into a subdivision algorithm. Experimental results show that our approach is efficient in practice. Notably, our software \texttt{voxelize} can successfully enclose curves defined by two trivariate polynomial equations of degree $100$, a problem that was previously intractable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09857v1</guid>
      <category>cs.CG</category>
      <category>cs.NA</category>
      <category>cs.SC</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Computer Algebra in Scientific Computing, Sep 2024, Rennes, France</arxiv:journal_reference>
      <dc:creator>Guillaume Moroz (GAMBLE)</dc:creator>
    </item>
    <item>
      <title>Group and Shuffle: Efficient Structured Orthogonal Parametrization</title>
      <link>https://arxiv.org/abs/2406.10019</link>
      <description>arXiv:2406.10019v1 Announce Type: cross 
Abstract: The increasing size of neural networks has led to a growing demand for methods of efficient fine-tuning. Recently, an orthogonal fine-tuning paradigm was introduced that uses orthogonal matrices for adapting the weights of a pretrained model. In this paper, we introduce a new class of structured matrices, which unifies and generalizes structured classes from previous works. We examine properties of this class and build a structured orthogonal parametrization upon it. We then use this parametrization to modify the orthogonal fine-tuning framework, improving parameter and computational efficiency. We empirically validate our method on different domains, including adapting of text-to-image diffusion models and downstream task fine-tuning in language modeling. Additionally, we adapt our construction for orthogonal convolutions and conduct experiments with 1-Lipschitz neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10019v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikhail Gorbunov, Nikolay Yudin, Vera Soboleva, Aibek Alanov, Alexey Naumov, Maxim Rakhuba</dc:creator>
    </item>
    <item>
      <title>CP decomposition and low-rank approximation of antisymmetric tensors</title>
      <link>https://arxiv.org/abs/2212.13389</link>
      <description>arXiv:2212.13389v2 Announce Type: replace 
Abstract: For the antisymmetric tensors the paper examines a low-rank approximation which is represented via only three vectors. We describe a suitable low-rank format and propose an alternating least squares structure-preserving algorithm for finding such approximation. Moreover, we show that this approximation problem is equivalent to the problem of finding the best multilinear low-rank antisymmetric approximation and, consequently, equivalent to the problem of finding the best unstructured rank-$1$ approximation. The case of partial antisymmetry is also discussed. The algorithms are implemented in Julia programming language and their numerical performance is discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.13389v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erna Begovic, Lana Perisa</dc:creator>
    </item>
    <item>
      <title>Structured condition numbers for a linear function of the solution of the generalized saddle point problem</title>
      <link>https://arxiv.org/abs/2305.05629</link>
      <description>arXiv:2305.05629v3 Announce Type: replace 
Abstract: This paper addresses structured normwise, mixed, and componentwise condition numbers (CNs) for a linear function of the solution to the generalized saddle point problem (GSPP). We present a general framework that enables us to measure the structured CNs of the individual components of the solution. Then, we derive their explicit formulae when the input matrices have symmetric, Toeplitz, or some general linear structures. In addition, compact formulae for the unstructured CNs are obtained, which recover previous results on CNs for GSPPs for specific choices of the linear function. Furthermore, applications of the derived structured CNs are provided to determine the structured CNs for the weighted Toeplitz regularized least-squares problems and Tikhonov regularization problems, which retrieves some previous studies in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.05629v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sk. Safique Ahmad, Pinki Khatun</dc:creator>
    </item>
    <item>
      <title>A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions</title>
      <link>https://arxiv.org/abs/2310.00177</link>
      <description>arXiv:2310.00177v5 Announce Type: replace 
Abstract: We introduce a neural-preconditioned iterative solver for Poisson equations with mixed boundary conditions. Typical Poisson discretizations yield large, ill-conditioned linear systems. Iterative solvers can be effective for these problems, but only when equipped with powerful preconditioners. Unfortunately, effective preconditioners like multigrid require costly setup phases that must be re-executed every time domain shapes or boundary conditions change, forming a severe bottleneck for problems with evolving boundaries. In contrast, we present a neural preconditioner trained to efficiently approximate the inverse of the discrete Laplacian in the presence of such changes. Our approach generalizes to domain shapes, boundary conditions, and grid sizes outside the training set. The key to our preconditioner's success is a novel, lightweight neural network architecture featuring spatially varying convolution kernels and supporting fast inference. We demonstrate that our solver outperforms state-of-the-art methods like algebraic multigrid as well as recently proposed neural preconditioners on challenging test cases arising from incompressible fluid simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00177v5</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kai Weixian Lan, Elias Gueidon, Ayano Kaneda, Julian Panetta, Joseph Teran</dc:creator>
    </item>
    <item>
      <title>Upwind summation-by-parts finite differences: error estimates and WENO methodology</title>
      <link>https://arxiv.org/abs/2312.11452</link>
      <description>arXiv:2312.11452v2 Announce Type: replace 
Abstract: High order upwind summation-by-parts finite difference operators have recently been developed. When combined with the simultaneous-approximation-term method to impose boundary conditions, the method converges faster than using traditional summation-by-parts operators. We prove the convergence rate by the normal mode analysis for such methods for a class of hyperbolic partial differential equations. Our analysis shows that the penalty parameter for imposing boundary conditions affects the convergence rate for stable methods. In addition, to solve problems with discontinuous data, we extend the method to also have the weighted essentially nonoscillatory property. The overall method is stable, achieves high order accuracy for smooth problems, and is capable of solving problems with discontinuities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.11452v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Jiang, Siyang Wang</dc:creator>
    </item>
    <item>
      <title>Unique Ergodicity of Stochastic Theta Method for Monotone SDEs driven by Nondegenerate Multiplicative Noise</title>
      <link>https://arxiv.org/abs/2401.01112</link>
      <description>arXiv:2401.01112v2 Announce Type: replace 
Abstract: We establish the unique ergodicity of the Markov chain generated by the stochastic theta method (STM) with $\theta \in [1/2, 1]$ for monotone SODEs, without growth restriction on the coefficients, driven by nondegenerate multiplicative noise. The main ingredient of the arguments lies in constructing new Lyapunov functions involving the coefficients, the stepsize, and $\theta$, and the irreducibility and the strong Feller property for the STM. We also generalize the arguments to the temporal drift-implicit Euler (DIE) method and its Galerkin-based full discretizations for a class of monotone SPDEs driven by infinite-dimensional nondegenerate multiplicative trace-class noise. Applying these results to the stochastic Allen--Cahn equation indicates that its DIE scheme is uniquely ergodic for any interface thickness, which gives an affirmative answer to a question proposed in (J. Cui, J. Hong, and L. Sun, Stochastic Process. Appl. (2021): 55--93). Numerical experiments verify our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01112v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhihui Liu, Zhizhou Liu</dc:creator>
    </item>
    <item>
      <title>On the Complexity of Interpolation by Polynomials with Non-negative Real Coefficients</title>
      <link>https://arxiv.org/abs/2402.00409</link>
      <description>arXiv:2402.00409v2 Announce Type: replace 
Abstract: In this paper, we consider interpolation by \textit{completely monotonous} polynomials (CMPs for short), that is, polynomials with non-negative real coefficients. In particular, given a finite set $S\subset \mathbb{R}_{&gt;0} \times \mathbb{R}_{\geq 0}$, we consider \textit{the minimal polynomial} of $S$, introduced by Berg [1985], which is `minimal,' in the sense that it is eventually majorized by all the other CMPs interpolating $S$. We give an upper bound of the degree of the minimal polynomial of $S$ when it exists. Furthermore, we give another algorithm for computing the minimal polynomial of given $S$ which utilizes an order structure on sign sequences. Applying the upper bound above, we also analyze the computational complexity of algorithms for computing minimal polynomials including ours.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00409v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katsuyuki Bando, Eitetsu Ken, Hirotaka Onuki</dc:creator>
    </item>
    <item>
      <title>Selected aspects of tractability analysis</title>
      <link>https://arxiv.org/abs/2402.02396</link>
      <description>arXiv:2402.02396v2 Announce Type: replace 
Abstract: We give an overview of certain aspects of tractability analysis of multivariate problems. This paper is not intended to give a complete account of the subject, but provides an insight into how the theory works for particular types of problems. We mainly focus on linear problems on Hilbert spaces, and mostly allow arbitrary linear information. In such cases, tractability analysis is closely linked to an analysis of the singular values of the operator under consideration. We also highlight the more recent developments regarding exponential and generalized tractability. The theoretical results are illustrated by several examples throughout the article.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02396v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Kritzer</dc:creator>
    </item>
    <item>
      <title>Exponential Expressivity of ReLU$^k$ Neural Networks on Gevrey Classes with Point Singularities</title>
      <link>https://arxiv.org/abs/2403.02035</link>
      <description>arXiv:2403.02035v2 Announce Type: replace 
Abstract: We analyze deep Neural Network emulation rates of smooth functions with point singularities in bounded, polytopal domains $\mathrm{D} \subset \mathbb{R}^d$, $d=2,3$. We prove exponential emulation rates in Sobolev spaces in terms of the number of neurons and in terms of the number of nonzero coefficients for Gevrey-regular solution classes defined in terms of weighted Sobolev scales in $\mathrm{D}$, comprising the countably-normed spaces of I.M. Babu\v{s}ka and B.Q. Guo.
  As intermediate result, we prove that continuous, piecewise polynomial high order (``$p$-version'') finite elements with elementwise polynomial degree $p\in\mathbb{N}$ on arbitrary, regular, simplicial partitions of polyhedral domains $\mathrm{D} \subset \mathbb{R}^d$, $d\geq 2$ can be exactly emulated by neural networks combining ReLU and ReLU$^2$ activations. On shape-regular, simplicial partitions of polytopal domains $\mathrm{D}$, both the number of neurons and the number of nonzero parameters are proportional to the number of degrees of freedom of the finite element space, in particular for the $hp$-Finite Element Method of I.M. Babu\v{s}ka and B.Q. Guo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02035v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joost A. A. Opschoor, Christoph Schwab</dc:creator>
    </item>
    <item>
      <title>Adaptive Preconditioned Gradient Descent with Energy</title>
      <link>https://arxiv.org/abs/2310.06733</link>
      <description>arXiv:2310.06733v2 Announce Type: replace-cross 
Abstract: We propose an adaptive step size with an energy approach for a suitable class of preconditioned gradient descent methods. We focus on settings where the preconditioning is applied to address the constraints in optimization problems, such as the Hessian-Riemannian and natural gradient descent methods. More specifically, we incorporate these preconditioned gradient descent algorithms in the recently introduced Adaptive Energy Gradient Descent (AEGD) framework. In particular, we discuss theoretical results on the unconditional energy-stability and convergence rates across three classes of objective functions. Furthermore, our numerical results demonstrate excellent performance of the proposed method on several test bed optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06733v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hailiang Liu, Levon Nurbekyan, Xuping Tian, Yunan Yang</dc:creator>
    </item>
  </channel>
</rss>
