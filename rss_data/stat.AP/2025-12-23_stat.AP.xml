<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Dec 2025 05:01:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Empirical parameterization of the Elo Rating System</title>
      <link>https://arxiv.org/abs/2512.18013</link>
      <description>arXiv:2512.18013v1 Announce Type: new 
Abstract: This study aims to provide a data-driven approach for empirically tuning and validating rating systems, focusing on the Elo system. Well-known rating frameworks, such as Elo, Glicko, TrueSkill systems, rely on parameters that are usually chosen based on probabilistic assumptions or conventions, and do not utilize game-specific data. To address this issue, we propose a methodology that learns optimal parameter values by maximizing the predictive accuracy of match outcomes. The proposed parameter-tuning framework is a generalizable method that can be extended to any rating system, even for multiplayer setups, through suitable modification of the parameter space. Implementation of the rating system on real and simulated gameplay data demonstrates the suitability of the data-driven rating system in modeling player performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18013v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shirsa Maitra, Tathagata Banerjee, Anushka De, Diganta Mukherjee, Tridib Mukherjee</dc:creator>
    </item>
    <item>
      <title>Distribution-Free Selection of Low-Risk Oncology Patients for Survival Beyond a Time Horizon</title>
      <link>https://arxiv.org/abs/2512.18118</link>
      <description>arXiv:2512.18118v1 Announce Type: new 
Abstract: We study the problem of selecting a subset of patients who are unlikely to experience an event within a specified time horizon, by calibrating a screening rule based on the output of a black-box survival model. This statistics problem has many applications in medicine, including identifying candidates for treatment de-escalation and prioritizing the allocation of limited medical resources. In this paper, we compare two families of methods that can provide different types of distribution-free guarantees for this task: (i) high-probability risk control and (ii) expectation-based false discovery rate control using conformal $p$-values. We clarify the relation between these two frameworks, which have important conceptual differences, and explain how each can be adapted to analyze time-to-event data using inverse probability of censoring weighting. Through experiments on semi-synthetic and real oncology data from the Flatiron Health Research Database, we find that both approaches often achieve the desired survival rate among selected patients, but with distinct efficiency profiles. The conformal method tends to be more powerful, whereas high-probability risk control offers stronger guarantees at the cost of some additional conservativeness. Finally, we provide practical guidance on implementation and parameter tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18118v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Sesia, Vladimir Svetnik</dc:creator>
    </item>
    <item>
      <title>Analysing Skill Predominance in Generalized Fantasy Cricket</title>
      <link>https://arxiv.org/abs/2512.18467</link>
      <description>arXiv:2512.18467v1 Announce Type: new 
Abstract: In fantasy sports, strategic thinking-not mere luck-often defines who wins and who falls short. As fantasy cricket grows in popularity across India, understanding whether success stems from skill or chance has become both an analytical and regulatory question. This study introduces a new limited-selection contest framework in which participants choose from four expert-designed teams and share prizes based on the highest cumulative score. By combining simulation experiments with real performance data from the 2024 Indian Premier League (IPL), we evaluate whether measurable skill emerges within this structure. Results reveal that strategic and informed team selection consistently outperforms random choice, underscoring a clear skill advantage that persists despite stochastic variability. The analysis quantifies how team composition, inter-team correlation, and participant behaviour jointly influence winning probabilities, highlighting configurations where skill becomes statistically dominant. These findings provide actionable insights for players seeking to maximise returns through strategy and for platform designers aiming to develop fair, transparent, and engaging skill-based gaming ecosystems that balance competition with regulatory compliance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18467v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Supratim Das, Sarthak Sarkar, Subhamoy Maitra, Tridib Mukherjee</dc:creator>
    </item>
    <item>
      <title>Functional Modeling of Learning and Memory Dynamics in Cognitive Disorders</title>
      <link>https://arxiv.org/abs/2512.18760</link>
      <description>arXiv:2512.18760v1 Announce Type: new 
Abstract: Deficits in working memory, which includes both the ability to learn and to retain information short-term, are a hallmark of many cognitive disorders. Our study analyzes data from a neuroscience experiment on animal subjects, where performance on a working memory task was recorded as repeated binary success or failure data. We estimate continuous probability of success curves from this binary data in the context of functional data analysis, which is largely used in biological processes that are intrinsically continuous. We then register these curves to decompose each function into its amplitude, representing overall performance, and its phase, representing the speed of learning or response. Because we are able to separate speed from performance, we can address the crucial question of whether a cognitive disorder impacts not only how well subjects can learn and remember, but also how fast. This allows us to analyze the components jointly to uncover how speed and performance co-vary, and to compare them separately to pinpoint whether group differences stem from a deficit in peak performance or a change in speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18760v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria Laura Battagliola, Laura J. Benoit, Sarah Canetta, Shizhe Zhang, R. Todd Ogden</dc:creator>
    </item>
    <item>
      <title>Dyadic Flow Models for Nonstationary Gene Flow in Landscape Genomics</title>
      <link>https://arxiv.org/abs/2512.19035</link>
      <description>arXiv:2512.19035v1 Announce Type: new 
Abstract: The field of landscape genomics aims to infer how landscape features affect gene flow across space. Most landscape genomic frameworks assume the isolation-by-distance and isolation-by-resistance hypotheses, which propose that genetic dissimilarity increases as a function of distance and as a function of cumulative landscape resistance, respectively. While these hypotheses are valid in certain settings, other mechanisms may affect gene flow. For example, the gene flow of invasive species may depend on founder effects and multiple introductions. Such mechanisms are not considered in modern landscape genomic models. We extend dyadic models to allow for mechanisms that range-shifting and/or invasive species may experience by introducing dyadic spatially-varying coefficients (DSVCs) defined on source-destination pairs. The DSVCs allow the effects of landscape on gene flow to vary across space, capturing nonstationary and asymmetric connectivity. Additionally, we incorporate explicit landscape features as connectivity covariates, which are localized to specific regions of the spatial domain and may function as barriers or corridors to gene flow. Such covariates are central to colonization and invasion, where spread accelerates along corridors and slows across landscape barriers. The proposed framework accommodates colonization-specific processes while retaining the ability to assess landscape influences on gene flow. Our case study of the highly invasive cheatgrass (Bromus tectorum) demonstrates the necessity of accounting for nonstationarity gene flow in range-shifting species.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19035v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael R. Schwob, Nicholas M. Calzada, Justin J. Van Ee, Diana Gamba, Rebecca A. Nelson, Megan L. Vahsen, Peter B. Adler, Jesse R. Lasky, Mevin B. Hooten</dc:creator>
    </item>
    <item>
      <title>Unraveling time-varying causal effects of multiple exposures: integrating Functional Data Analysis with Multivariable Mendelian Randomization</title>
      <link>https://arxiv.org/abs/2512.19064</link>
      <description>arXiv:2512.19064v1 Announce Type: new 
Abstract: Mendelian Randomization is a widely used instrumental variable method for assessing causal effects of lifelong exposures on health outcomes. Many exposures, however, have causal effects that vary across the life course and often influence outcomes jointly with other exposures or indirectly through mediating pathways. Existing approaches to multivariable Mendelian Randomization assume constant effects over time and therefore fail to capture these dynamic relationships. We introduce Multivariable Functional Mendelian Randomization (MV-FMR), a new framework that extends functional Mendelian Randomization to simultaneously model multiple time-varying exposures. The method combines functional principal component analysis with a data-driven cross-validation strategy for basis selection and accounts for overlapping instruments and mediation effects. Through extensive simulations, we assessed MV-FMR's ability to recover time-varying causal effects under a range of data-generating scenarios and compared the performance of joint versus separate exposure effect estimation strategies. Across scenarios involving nonlinear effects, horizontal pleiotropy, mediation, and sparse data, MV-FMR consistently recovered the true causal functions and outperformed univariable approaches. To demonstrate its practical value, we applied MV-FMR to UK Biobank data to investigate the time-varying causal effects of systolic blood pressure and body mass index on coronary artery disease. MV-FMR provides a flexible and interpretable framework for disentangling complex time-dependent causal processes and offers new opportunities for identifying life-course critical periods and actionable drivers relevant to disease prevention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19064v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicole Fontana, Francesca Ieva, Luisa Zuccolo, Emanuele Di Angelantonio, Piercesare Secchi</dc:creator>
    </item>
    <item>
      <title>Ant Colony Optimisation applied to the Travelling Santa Problem</title>
      <link>https://arxiv.org/abs/2512.19627</link>
      <description>arXiv:2512.19627v1 Announce Type: new 
Abstract: The hypothetical global delivery schedule of Santa Claus must follow strict rolling night-time windows that vary with the Earth's rotation and obey an energy budget that depends on payload size and cruising speed. To design this schedule, the Travelling-Santa Ant-Colony Optimisation framework (TSaP-ACO) was developed. This heuristic framework constructs potential routes via a population of artificial ants that iteratively extend partial paths. Ants make their decisions much like they do in nature, following pheromones left by other ants, but with a degree of permitted exploration. This approach: (i) embeds local darkness feasibility directly into the pheromone heuristic, (ii) seeks to minimise aerodynamic work via a shrinking sleigh cross sectional area, (iii) uses a low-cost "rogue-ant" reversal to capture direction-sensitive time-zones, and (iv) tunes leg-specific cruise speeds on the fly. On benchmark sets of 15 and 30 capital cities, the TSaP-ACO eliminates all daylight violations and reduces total work by up to 10% compared to a distance-only ACO. In a 40-capital-city stress test, it cuts energy use by 88%, and shortens tour length by around 67%. Population-first routing emerges naturally from work minimisation (50% served by leg 11 of 40). These results demonstrate that rolling-window, energy-aware ACO has potential applications more realistic global delivery scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19627v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elliot Fisher, Robin Smith</dc:creator>
    </item>
    <item>
      <title>An Adaptive Graphical Lasso Approach to Modeling Symptom Networks of Common Mental Disorders in Eritrean Refugee Population</title>
      <link>https://arxiv.org/abs/2512.19681</link>
      <description>arXiv:2512.19681v1 Announce Type: new 
Abstract: Despite the significant public health burden of common mental disorders (CMDs) among refugee populations, their underlying symptom structures remain underexplored. This study uses Gaussian graphical modeling to examine the symptom network of post-traumatic stress disorder (PTSD), depression, anxiety, and somatic distress among Eritrean refugees in the Greater Washington, DC area. Given the small sample size (n) and high-dimensional symptom space (p), we propose a novel extension of the standard graphical LASSO by incorporating adaptive penalization, which improves sparsity selection and network estimation stability under n &lt; p conditions. To evaluate the reliability of the network, we apply bootstrap resampling and use centrality measures to identify the most influential symptoms. Our analysis identifies six distinct symptom clusters, with somatic-anxiety symptoms forming the most interconnected group. Notably, symptoms such as nausea and reliving past experiences emerge as central symptoms linking PTSD, anxiety, depression, and somatic distress. Additionally, we identify symptoms like feeling fearful, sleep problems, and loss of interest in activities as key symptoms, either being closely positioned to many others or acting as important bridges that help maintain the overall network connectivity, thereby highlighting their potential importance as possible intervention targets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19681v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elizabeth B. Amona, Indranil Sahoo, David Chan, Marianne B. Lund, Miriam Kuttikat</dc:creator>
    </item>
    <item>
      <title>A curated UK rain radar data set for training and benchmarking nowcasting models</title>
      <link>https://arxiv.org/abs/2512.17924</link>
      <description>arXiv:2512.17924v1 Announce Type: cross 
Abstract: This paper documents a data set of UK rain radar image sequences for use in statistical modeling and machine learning methods for nowcasting. The main dataset contains 1,000 randomly sampled sequences of length 20 steps (15-minute increments) of 2D radar intensity fields of dimension 40x40 (at 5km spatial resolution). Spatially stratified sampling ensures spatial homogeneity despite removal of clear-sky cases by threshold-based truncation. For each radar sequence, additional atmospheric and geographic features are made available, including date, location, mean elevation, mean wind direction and speed and prevailing storm type. New R functions to extract data from the binary "Nimrod" radar data format are provided. A case study is presented to train and evaluate a simple convolutional neural network for radar nowcasting, including self-contained R code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17924v1</guid>
      <category>physics.ao-ph</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Viv Atureta, Rifki Priansyah Jasin, Stefan Siegert</dc:creator>
    </item>
    <item>
      <title>Comparative Evaluation of Explainable Machine Learning Versus Linear Regression for Predicting County-Level Lung Cancer Mortality Rate in the United States</title>
      <link>https://arxiv.org/abs/2512.17934</link>
      <description>arXiv:2512.17934v1 Announce Type: cross 
Abstract: Lung cancer (LC) is a leading cause of cancer-related mortality in the United States. Accurate prediction of LC mortality rates is crucial for guiding targeted interventions and addressing health disparities. Although traditional regression-based models have been commonly used, explainable machine learning models may offer enhanced predictive accuracy and deeper insights into the factors influencing LC mortality. This study applied three models: random forest (RF), gradient boosting regression (GBR), and linear regression (LR) to predict county-level LC mortality rates across the United States. Model performance was evaluated using R-squared and root mean squared error (RMSE). Shapley Additive Explanations (SHAP) values were used to determine variable importance and their directional impact. Geographic disparities in LC mortality were analyzed through Getis-Ord (Gi*) hotspot analysis. The RF model outperformed both GBR and LR, achieving an R2 value of 41.9% and an RMSE of 12.8. SHAP analysis identified smoking rate as the most important predictor, followed by median home value and the percentage of the Hispanic ethnic population. Spatial analysis revealed significant clusters of elevated LC mortality in the mid-eastern counties of the United States. The RF model demonstrated superior predictive performance for LC mortality rates, emphasizing the critical roles of smoking prevalence, housing values, and the percentage of Hispanic ethnic population. These findings offer valuable actionable insights for designing targeted interventions, promoting screening, and addressing health disparities in regions most affected by LC in the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17934v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1200/CCI-24-00310</arxiv:DOI>
      <arxiv:journal_reference>JCO Clin Cancer Inform JCO Clinical Cancer Informatics, 2025 Nov:9:e2400310</arxiv:journal_reference>
      <dc:creator>Soheil Hashtarkhani, Brianna M. White, Benyamin Hoseini, David L. Schwartz, Arash Shaban-Nejad</dc:creator>
    </item>
    <item>
      <title>Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis</title>
      <link>https://arxiv.org/abs/2512.17979</link>
      <description>arXiv:2512.17979v1 Announce Type: cross 
Abstract: Industrial symbiosis fosters circularity by enabling firms to repurpose residual resources, yet its emergence is constrained by socio-spatial frictions that shape costs, matching opportunities, and market efficiency. Existing models often overlook the interaction between spatial structure, market design, and adaptive firm behavior, limiting our understanding of where and how symbiosis arises. We develop an agent-based model where heterogeneous firms trade byproducts through a spatially embedded double-auction market, with prices and quantities emerging endogenously from local interactions. Leveraging reinforcement learning, firms adapt their bidding strategies to maximize profit while accounting for transport costs, disposal penalties, and resource scarcity. Simulation experiments reveal the economic and spatial conditions under which decentralized exchanges converge toward stable and efficient outcomes. Counterfactual regret analysis shows that sellers' strategies approach a near Nash equilibrium, while sensitivity analysis highlights how spatial structures and market parameters jointly govern circularity. Our model provides a basis for exploring policy interventions that seek to align firm incentives with sustainability goals, and more broadly demonstrates how decentralized coordination can emerge from adaptive agents in spatially constrained markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17979v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>AAMAS 2026, Paphos, IFAAMAS, 10 pages</arxiv:journal_reference>
      <dc:creator>Matthieu Mastio, Paul Saves, Benoit Gaudou, Nicolas Verstaevel</dc:creator>
    </item>
    <item>
      <title>cardinalR: Generating Interesting High-Dimensional Data Structures</title>
      <link>https://arxiv.org/abs/2512.18172</link>
      <description>arXiv:2512.18172v1 Announce Type: cross 
Abstract: Simulated high-dimensional data is useful for testing, validating, and improving algorithms used in dimension reduction, supervised and unsupervised learning. High-dimensional data is characterized by multiple variables that are dependent or associated in some way, such as linear, nonlinear, clustering or anomalies. Here we provide new methods for generating a variety of high-dimensional structures using mathematical functions and statistical distributions organized into the R package cardinalR. Several example data sets are also provided. These will be useful for researchers to better understand how different analytical methods work and can be improved, with a special focus on nonlinear dimension reduction methods. This package enriches the existing toolset of benchmark datasets for evaluating algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18172v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jayani P. Gamage, Dianne Cook, Paul Harrison, Michael Lydeamore, Thiyanga S. Talagala</dc:creator>
    </item>
    <item>
      <title>Privacy Data Pricing: A Stackelberg Game Approach</title>
      <link>https://arxiv.org/abs/2512.18296</link>
      <description>arXiv:2512.18296v1 Announce Type: cross 
Abstract: Data markets are emerging as key mechanisms for trading personal and organizational data. Traditional data pricing studies -- such as query-based or arbitrage-free pricing models -- mainly emphasize price consistency and profit maximization but often neglect privacy constraints and strategic interactions. The widespread adoption of differential privacy (DP) introduces a fundamental privacy-utility trade-off: noise protects individuals' privacy but reduces data accuracy and market value. This paper develops a Stackelberg game framework for pricing DP data, where the market maker (leader) sets the price function and the data buyer (follower) selects the optimal query precision under DP constraints. We derive the equilibrium strategies for both parties under a balanced pricing function where the pricing decision variable enters linearly into the original pricing model. We obtain closed-form solutions for the optimal variance and pricing level, and determine the boundary conditions for market participation. Furthermore, we extend the analysis to Stackelberg games involving nonlinear power pricing functions. The model bridges DP and economic mechanism design, offering a unified foundation for incentive-compatible and privacy-conscious data pricing in data markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18296v1</guid>
      <category>cs.GT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lijun Bo, Weiqiang Chang</dc:creator>
    </item>
    <item>
      <title>Detecting stellar flares in the presence of a deterministic trend and stochastic volatility</title>
      <link>https://arxiv.org/abs/2512.18559</link>
      <description>arXiv:2512.18559v1 Announce Type: cross 
Abstract: We develop a new and powerful method to analyze time series to rigorously detect flares in the presence of an irregularly oscillatory baseline, and apply it to stellar light curves observed with TESS. First, we remove the underlying non-stochastic trend using a time-varying amplitude harmonic model. We then model the stochastic component of the light curves in a manner analogous to financial time series, as an ARMA+GARCH process, allowing us to detect and characterize impulsive flares as large deviations inconsistent with the correlation structure in the light curve. We apply the method to exemplar light curves from TIC13955147 (a G5V eruptive variable), TIC269797536 (an M4 high-proper motion star), and TIC441420236 (AU Mic, an active dMe flare star), detecting up to $145$, $460$, and $403$ flares respectively, at rates ranging from ${\approx}0.4$--$8.5$~day$^{-1}$ over different sectors and under different detection thresholds. We detect flares down to amplitudes of $0.03$%, $0.29$%, and $0.007$% of the bolometric luminosity for each star respectively. We model the distributions of flare energies and peak fluxes as power-laws, and find that the solar-like star exhibits values similar to that on the Sun ($\alpha_{E,P}\approx1.85,2.36$), while for the less- and highly-active low-mass stars $\alpha_{E,P}&gt;2$ and $&lt;2$ respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18559v1</guid>
      <category>astro-ph.SR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiyuan Wang, Giovanni Motta, Genaro Sucarrat, Vinay L. Kashyap</dc:creator>
    </item>
    <item>
      <title>Consistent Bayesian meta-analysis on subgroup specific effects and interactions</title>
      <link>https://arxiv.org/abs/2512.18785</link>
      <description>arXiv:2512.18785v1 Announce Type: cross 
Abstract: Commonly, clinical trials report effects not only for the full study population but also for patient subgroups. Meta-analyses of subgroup-specific effects and treatment-by-subgroup interactions may be inconsistent, especially when trials apply different subgroup weightings. We show that meta-regression can, in principle, with a contribution adjustment, recover the same interaction inference regardless of whether interaction data or subgroup data are used. Our Bayesian framework for subgroup-data interaction meta-analysis inherently (i) adjusts for varying relative subgroup contribution, quantified by the information fraction (IF) within a trial; (ii) is robust to prevalence imbalance and variation; (iii) provides a self-contained, model-based approach; and (iv) can be used to incorporate prior information into interaction meta-analyses with few studies.The method is demonstrated using an example with as few as seven trials of disease-modifying therapies in relapsing-remitting multiple sclerosis. The Bayesian Contribution-adjusted Meta-analysis by Subgroup (CAMS) indicates a stronger treatment-by-disability interaction (relapse rate reduction) in patients with lower disability (EDSS &lt;= 3.5) compared with the unadjusted model, while results for younger patients (age &lt; 40 years) are unchanged.By controlling subgroup contribution while retaining subgroup interpretability, this approach enables reliable interaction decision-making when published subgroup data are available.Although the proposed CAMS approach is presented in a Bayesian context, it can also be implemented in frequentist or likelihood frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18785v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renato Panaro, Christian R\"over, Tim Friede</dc:creator>
    </item>
    <item>
      <title>Adapting Skill Ratings to Luck-Based Hidden-Information Games</title>
      <link>https://arxiv.org/abs/2512.18858</link>
      <description>arXiv:2512.18858v1 Announce Type: cross 
Abstract: Rating systems play a crucial role in evaluating player skill across competitive environments. The Elo rating system, originally designed for deterministic and information-complete games such as chess, has been widely adopted and modified in various domains. However, the traditional Elo rating system only considers game outcomes for rating calculation and assumes uniform initial states across players. This raises important methodological challenges in skill modelling for popular partially randomized incomplete-information games such as Rummy. In this paper, we examine the limitations of conventional Elo ratings when applied to luck-driven environments and propose a modified Elo framework specifically tailored for Rummy. Our approach incorporates score-based performance metrics and explicitly models the influence of initial hand quality to disentangle skill from luck. Through extensive simulations involving 270,000 games across six strategies of varying sophistication, we demonstrate that our proposed system achieves stable convergence, superior discriminative power, and enhanced predictive accuracy compared to traditional Elo formulations. The framework maintains computational simplicity while effectively capturing the interplay of skill, strategy, and randomness, with broad applicability to other stochastic competitive environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18858v1</guid>
      <category>cs.GT</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Avirup Chakraborty, Shirsa Maitra, Tathagata Banerjee, Diganta Mukherjee, Tridib Mukherjee</dc:creator>
    </item>
    <item>
      <title>A Universal Framework for Factorial Matched Observational Studies with General Treatment Types: Design, Analysis, and Applications</title>
      <link>https://arxiv.org/abs/2512.18997</link>
      <description>arXiv:2512.18997v1 Announce Type: cross 
Abstract: Matching is one of the most widely used causal inference frameworks in observational studies. However, all the existing matching-based causal inference methods are designed for either a single treatment with general treatment types (e.g., binary, ordinal, or continuous) or factorial (multiple) treatments with binary treatments only. To our knowledge, no existing matching-based causal methods can handle factorial treatments with general treatment types. This critical gap substantially hinders the applicability of matching in many real-world problems, in which there are often multiple, potentially non-binary (e.g., continuous) treatment components. To address this critical gap, this work develops a universal framework for the design and analysis of factorial matched observational studies with general treatment types (e.g., binary, ordinal, or continuous). We first propose a two-stage non-bipartite matching algorithm that constructs matched sets of units with similar covariates but distinct combinations of treatment doses, thereby enabling valid estimation of both main and interaction effects. We then introduce a new class of generalized factorial Neyman-type estimands that provide model-free, finite-population-valid definitions of marginal and interaction causal effects under factorial treatments with general treatment types. Randomization-based Fisher-type and Neyman-type inference procedures are developed, including unbiased estimators, asymptotically valid variance estimators, and variance adjustments incorporating covariate information for improved efficiency. Finally, we illustrate the proposed framework through a county-level application that evaluates the causal impacts of work- and non-work-trip reductions (social distancing practices) on COVID-19-related and drug-related outcomes during the COVID-19 pandemic in the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18997v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianan Zhu, Tianruo Zhang, Diana Silver, Ellicott Matthay, Omar El-Shahawy, Hyunseung Kang, Siyu Heng</dc:creator>
    </item>
    <item>
      <title>The asymptotic distribution of the likelihood ratio test statistic in two-peak discovery experiments</title>
      <link>https://arxiv.org/abs/2512.19333</link>
      <description>arXiv:2512.19333v1 Announce Type: cross 
Abstract: Likelihood ratio tests are widely used in high-energy physics, where the test statistic is usually assumed to follow a chi-squared distribution with a number of degrees of freedom specified by Wilks' theorem. This assumption breaks down when parameters such as signal or coupling strengths are restricted to be non-negative and their values under the null hypothesis lie on the boundary of the parameter space. Based on a recent clarification concerning the correct asymptotic distribution of the likelihood ratio test statistic for cases where two of the parameters are on the boundary, we revisit the the question of significance estimation for two-peak signal-plus-background counting experiments. In the high-energy physics literature, such experiments are commonly analyzed using Wilks' chi-squared distribution or the one-parameter Chernoff limit. We demonstrate that these approaches can lead to strongly miscalibrated significances, and that the test statistic distribution is instead well described by a chi-squared mixture with weights determined by the Fisher information matrix. Our results highlight the need for boundary-aware asymptotics in the analysis of two-peak counting experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19333v1</guid>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Clara Bertinelli Salucci, Hedvig Borgen Reiersrud, A. L. Read, Anders Kvellestad, Riccardo De Bin</dc:creator>
    </item>
    <item>
      <title>A hybrid-Hill estimator enabled by heavy-tailed block maxima</title>
      <link>https://arxiv.org/abs/2512.19338</link>
      <description>arXiv:2512.19338v1 Announce Type: cross 
Abstract: When analysing extreme values, two alternative statistical approaches have historically been held in contention: the seminal block maxima method (or annual maxima method, spurred by hydrological applications) and the peaks-over-threshold. Clamoured amongst statisticians as wasteful of potentially informative data, the block maxima method gradually fell into disfavour whilst peaks-over-threshold-based methodologies were ushered to the centre stage of extreme value statistics. This paper proposes a hybrid method which reconciles these two hitherto disconnected approaches. Appealing in its simplicity, our main result introduces a new universal limiting characterisation of extremes that eschews the customary requirement of a sufficiently large block size for the plausible block maxima-fit to an extreme value distribution. We advocate that inference should be drawn solely on larger block maxima, from which practice the mainstream peaks-over-threshold methodology coalesces. The asymptotic properties of the promised hybrid-Hill estimator herald more than its efficiency, but rather that a fully-fledged unified semi-parametric stream of statistics for extreme values is viable. A finite sample simulation study demonstrates that a reduced-bias off-shoot of the hybrid-Hill estimator fares exceptionally well against the incumbent maximum likelihood estimation that relies on a numerical fit to the entire sample of block maxima.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19338v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claudia Neves, Chang Xu</dc:creator>
    </item>
    <item>
      <title>A Markov Chain Modeling Approach for Predicting Relative Risks of Spatial Clusters in Public Health</title>
      <link>https://arxiv.org/abs/2512.19635</link>
      <description>arXiv:2512.19635v1 Announce Type: cross 
Abstract: Predicting relative risk (RR) of spatial clusters is a complex task in public health that can be achieved through various statistical and machine-learning methods for different time intervals. However, high-resolution longitudinal data is often unavailable to successfully apply such methods. The goal of the present study is to further develop and test a new methodology proposed in our previous work for accurate sequential RR predictions in the case of limited lon gitudinal data. In particular, we first use a well-known likelihood ratio test to identify significant spatial clusters over user-defined time intervals. Then we apply a Markov chain modeling ap approach to predict RR values for each time interval. Our findings demonstrate that the proposed approach yields better performance with COVID-19 morbidity data compared to the previous study on mortality data. Additionally, increasing the number of time intervals enhances the accuracy of the proposed Markov chain modeling method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19635v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lyza Iamrache, Kamel Rekab, Majid Bani-Yagoub, Julia Pluta, Abdelghani Mehailia</dc:creator>
    </item>
    <item>
      <title>Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight</title>
      <link>https://arxiv.org/abs/2512.19691</link>
      <description>arXiv:2512.19691v1 Announce Type: cross 
Abstract: Automating the calculation of clinical risk scores offers a significant opportunity to reduce physician administrative burden and enhance patient care. The current standard for evaluating this capability is MedCalc-Bench, a large-scale dataset constructed using LLM-based feature extraction and rule-based aggregation. However, treating such model-generated benchmarks as static oracles risks enshrining historical model errors as evaluation gold standards, a problem dangerously amplified when these datasets serve as reward signals for Reinforcement Learning (RL). In this work, we propose viewing benchmarks for complex tasks such as clinical score computation as ''in-progress living documents'' that should be periodically re-evaluated as the processes for creating them improve. We introduce a systematic, physician-in-the-loop pipeline that leverages advanced agentic verifiers to audit and relabel MedCalc-Bench, utilizing automated triage to reserve scarce clinician attention for the most contentious instances. Our audit reveals that a notable fraction of original labels diverge from medical ground truth due to extraction errors, calculator logic mismatches, and clinical ambiguity. To study whether this label noise meaningfully impacts downstream RL training, we fine-tune a Qwen3-8B model via Group Relative Policy Optimization (GRPO) and demonstrate that training on corrected labels yields an 8.7% absolute improvement in accuracy over the original baseline -- validating that label noise materially affects model evaluation. These findings underscore that in safety-critical domains, rigorous benchmark maintenance is a prerequisite for genuine model alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19691v1</guid>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junze Ye, Daniel Tawfik, Alex J. Goodell, Nikhil V. Kotha, Mark K. Buyyounouski, Mohsen Bayati</dc:creator>
    </item>
    <item>
      <title>Joint Object Tracking and Intent Recognition</title>
      <link>https://arxiv.org/abs/2311.06139</link>
      <description>arXiv:2311.06139v2 Announce Type: replace 
Abstract: This paper presents a Bayesian framework for inferring the posterior of the augmented state of a target, incorporating its underlying goal or intent, such as any intermediate waypoints and/or final destination. The methodology is thus for joint tracking and intent recognition. Several latent intent models are proposed here within a virtual leader formulation. They capture the influence of the target's hidden goal on its instantaneous behaviour. In this context, various motion models, including for highly maneuvering objects, are also considered. The a priori unknown target intent (e.g. destination) can dynamically change over time and take any value within the state space (e.g. a location or spatial region). A sequential Monte Carlo (particle filtering) approach is introduced for the simultaneous estimation of the target's (kinematic) state and its intent. Rao-Blackwellisation is employed to enhance the statistical performance of the inference routine. Simulated data and real radar measurements are used to demonstrate the efficacy of the proposed techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.06139v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaming Liang, Bashar I. Ahmad, Simon Godsill</dc:creator>
    </item>
    <item>
      <title>Estimating velocities of infectious disease spread through spatio-temporal log-Gaussian Cox point processes</title>
      <link>https://arxiv.org/abs/2409.05036</link>
      <description>arXiv:2409.05036v2 Announce Type: replace 
Abstract: Understanding the spread of infectious diseases such as COVID-19 is crucial for informed decision-making and resource allocation. A critical component of disease behavior is the velocity with which disease spreads, defined as the rate of change between time and space. In this paper, we propose a spatio-temporal modeling approach to determine the velocities of infectious disease spread. Our approach assumes that the locations and times of people infected can be considered as a spatio-temporal point pattern that arises as a realization of a spatio-temporal log-Gaussian Cox process. The intensity of this process is estimated using fast Bayesian inference by employing the integrated nested Laplace approximation (INLA) and the Stochastic Partial Differential Equations (SPDE) approaches. The velocity is then calculated using finite differences that approximate the derivatives of the intensity function. Finally, the directions and magnitudes of the velocities can be mapped at specific times to examine better the spread of the disease throughout the region. We demonstrate our method by analyzing COVID-19 spread in Cali, Colombia, during the 2020-2021 pandemic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05036v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando Rodriguez Avellaneda, Jorge Mateu, Paula Moraga</dc:creator>
    </item>
    <item>
      <title>A dual approach to proving electoral fraud using statistics and forensic evidence (Dvojnoe dokazatel'stvo falsifikazij na vyborah statistikoj i kriminalistikoj)</title>
      <link>https://arxiv.org/abs/2412.04535</link>
      <description>arXiv:2412.04535v3 Announce Type: replace 
Abstract: Electoral fraud often manifests itself as statistical anomalies in election results, yet its extent can rarely be reliably confirmed by other evidence. Here we report the complete results of municipal elections in the town of Vlasikha near Moscow, where we observe both statistical irregularities in the vote-counting transcripts and forensic evidence of tampering with ballots during their overnight storage. We evaluate two types of statistical signatures in the vote sequence that can prove batches of fraudulent ballots have been injected. We find that pairs of factory-made security bags with identical serial numbers are used in this fraud scheme. At 8 out of our 9 polling stations, the statistical and forensic evidence agrees (identifying 7 as fraudulent and 1 as honest), while at the remaining station the statistical evidence detects the fraud while the forensic one is insufficient. We also illustrate that the use of tamper-indicating seals at elections is inherently unreliable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04535v3</guid>
      <category>stat.AP</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Elect. Polit. 14, issue 2, 4 (2025); https://electoralpolitics.org/en/articles/dvoinoe-dokazatelstvo-falsifikatsii-na-vyborakh-statistikoi-i-kriminalistikoi/</arxiv:journal_reference>
      <dc:creator>Andrey Podlazov, Vadim Makarov</dc:creator>
    </item>
    <item>
      <title>Estimating excess mortality during the Covid-19 pandemic in Aotearoa New Zealand</title>
      <link>https://arxiv.org/abs/2412.08927</link>
      <description>arXiv:2412.08927v3 Announce Type: replace 
Abstract:   Background. The excess mortality rate in Aotearoa New Zealand during the Covid-19 pandemic is frequently estimated to be among the lowest in the world. However, to facilitate international comparisons, many of the methods that have been used to estimate excess mortality do not use age-stratified data on deaths and population size, which may compromise their accuracy.
  Methods. We used a quasi-Poisson regression model for monthly all-cause deaths among New Zealand residents, controlling for age, sex and seasonality. We fitted the model to deaths data for 2014-19. We estimated monthly excess mortality for 2020-23 as the difference between actual deaths and projected deaths according to the model. We conducted sensitivity analysis on the length of the pre-pandemic period used to fit the model. We benchmarked our results against a simple linear regression on the standardised annual mortality rate.
  Results. We estimated cumulative excess mortality in New Zealand in 2020-23 was 1040 (95% confidence interval [-1134, 2927]), equivalent to 0.7% [-0.8%, 2.0%] of expected mortality. Excess mortality was negative in 2020-21. The magnitude, timing and age-distribution of the positive excess mortality in 2022-23 were closely matched with confirmed Covid-19 deaths.
  Conclusions. Negative excess mortality in 2020-21 reflects very low levels of Covid-19 and major reductions in seasonal respiratory diseases during this period. In 2022-23, Covid-19 deaths were the main contributor to excess mortality and there was little or no net non-Covid-19 excess. Overall, New Zealand experienced one of the lowest rates of pandemic excess mortality in the world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08927v3</guid>
      <category>stat.AP</category>
      <category>q-bio.PE</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1093/ije/dyaf093</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Epidemiology (2025), 54(4): dyaf093</arxiv:journal_reference>
      <dc:creator>Michael John Plank, Pubudu Senanayake, Richard Lyon</dc:creator>
    </item>
    <item>
      <title>Stable EEG Source Estimation for Standardized Kalman Filter using Change Rate Tracking</title>
      <link>https://arxiv.org/abs/2504.01984</link>
      <description>arXiv:2504.01984v2 Announce Type: replace 
Abstract: This article focuses on the measurement and evolution modeling of Standardized Kalman filtering for brain activity estimation using non-invasive electroencephalography data. Here, we propose new parameter tuning and a model that uses the rate of change in the brain activity distribution to improve the stability of otherwise accurate estimates. Namely, we propose a backward-differentiation-based measurement model for the change rate, which notably improves the filtering-parametrization-stability of the tracking. Simulated data and data from a real subject were used in experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01984v2</guid>
      <category>stat.AP</category>
      <category>cs.NA</category>
      <category>eess.SP</category>
      <category>math.NA</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Joonas Lahtinen</dc:creator>
    </item>
    <item>
      <title>Reorienting Age-Friendly Frameworks for Rural Contexts: A Spatial Competence-Press Framework for Aging in Chinese Villages</title>
      <link>https://arxiv.org/abs/2510.20343</link>
      <description>arXiv:2510.20343v2 Announce Type: replace 
Abstract: While frameworks such as the WHO Age-Friendly Cities have advanced urban aging policy, rural contexts demand fundamentally different analytical approaches. The spatial dispersion, terrain variability, and agricultural labor dependencies that characterize rural aging experiences require moving beyond service-domain frameworks toward spatial stress assessment models. Current research on rural aging in China exhibits methodological gaps, systematically underrepresenting the spatial stressors that older adults face daily, including terrain barriers, infrastructure limitations, climate exposure, and agricultural labor burdens. Existing rural revitalization policies emphasize standardized interventions while inadequately addressing spatial heterogeneity and the spatially-differentiated needs of aging populations. This study developed a GIS-based spatial stress analysis framework that applies Lawton and Nahemow's competence-press model to quantify aging-related stressors and classify rural villages by intervention needs. Using data from 27 villages in Mamuchi Township, Shandong Province, we established four spatial stress indicators: slope gradient index (SGI), solar radiation exposure index (SREI), walkability index (WI), and agricultural intensity index (AII). Analysis of variance and hierarchical clustering revealed significant variation in spatial pressures across villages and identified distinct typologies that require targeted intervention strategies. The framework produces both quantitative stress measurements for individual villages and a classification system that groups villages with similar stress patterns, providing planners and policymakers with practical tools for designing spatially-targeted age-friendly interventions in rural China and similar contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20343v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1177/00420980251394180</arxiv:DOI>
      <arxiv:journal_reference>Urban Studies, First published online December 13, 2025</arxiv:journal_reference>
      <dc:creator>Ziyuan Gao</dc:creator>
    </item>
    <item>
      <title>COBASE: A new copula-based shuffling method for ensemble weather forecast postprocessing</title>
      <link>https://arxiv.org/abs/2510.25610</link>
      <description>arXiv:2510.25610v2 Announce Type: replace 
Abstract: Weather predictions are often provided as ensembles generated by repeated runs of numerical weather prediction models. These forecasts typically exhibit bias and inaccurate dependence structures due to numerical and dispersion errors, requiring statistical postprocessing for improved precision. A common correction strategy is the two-step approach: first adjusting the univariate forecasts, then reconstructing the multivariate dependence. The second step is usually handled with nonparametric methods, which can underperform when historical data are limited. Parametric alternatives, such as the Gaussian Copula Approach (GCA), offer theoretical advantages but often produce poorly calibrated multivariate forecasts due to random sampling of the corrected univariate margins. In this work, we introduce COBASE, a novel copula-based postprocessing framework that preserves the flexibility of parametric modeling while mimicking the nonparametric techniques through a rank-shuffling mechanism. This design ensures calibrated margins and realistic dependence reconstruction. We evaluate COBASE on multi-site 2-meter temperature forecasts from the ALADIN-LAEF ensemble over Austria and on joint forecasts of temperature and dew point temperature from the ECMWF system in the Netherlands. Across all regions, COBASE variants consistently outperform traditional copula-based approaches, such as GCA, and achieve performance on par with state-of-the-art nonparametric methods like SimSchaake and ECC, with only minimal differences across settings. These results position COBASE as a competitive and robust alternative for multivariate ensemble postprocessing, offering a principled bridge between parametric and nonparametric dependence reconstruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25610v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maurits Flos, Bastien Fran\c{c}ois, Irene Schicker, Kirien Whan, Elisa Perrone</dc:creator>
    </item>
    <item>
      <title>Identification of Separable OTUs for Multinomial Classification in Compositional Data Analysis</title>
      <link>https://arxiv.org/abs/2511.02509</link>
      <description>arXiv:2511.02509v2 Announce Type: replace 
Abstract: High-throughput sequencing has transformed microbiome research, but it also produces inherently compositional data that challenge standard statistical and machine learning methods. In this work, we propose a multinomial classification framework for compositional microbiome data based on penalized log-ratio regression and pairwise separability screening. The method quantifies the discriminative ability of each OTU through the area under the receiver operating characteristic curve ($AUC$) for all pairwise log-ratios and aggregates these values into a global separability index $S_k$, yielding interpretable rankings of taxa together with confidence intervals. We illustrate the approach by reanalyzing the Baxter colorectal adenoma dataset and comparing our results with Greenacre's ordination-based analysis using Correspondence Analysis and Canonical Correspondence Analysis. Our models consistently recover a core subset of taxa previously identified as discriminant, thereby corroborating Greenacre's main findings, while also revealing additional OTUs that become important once demographic covariates are taken into account. In particular, adjustment for age, gender, and diabetes medication improves the precision of the separation index and highlights new, potentially relevant taxa, suggesting that part of the original signal may have been influenced by confounding. Overall, the integration of log-ratio modeling, covariate adjustment, and uncertainty estimation provides a robust and interpretable framework for OTU selection in compositional microbiome data. The proposed method complements existing ordination-based approaches by adding a probabilistic and inferential perspective, strengthening the identification of biologically meaningful microbial signatures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02509v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R. Alberich, N. A. Cruz, R. Fern\'andez, I. Garc\'ia Mosquera, A. Mir, F. Rossell\'o</dc:creator>
    </item>
    <item>
      <title>Persistence diagrams as morphological signatures of cells: A method to measure and compare cells within a population</title>
      <link>https://arxiv.org/abs/2310.20644</link>
      <description>arXiv:2310.20644v3 Announce Type: replace-cross 
Abstract: Cell biologists study in parallel the morphology of cells with the regulation mechanisms that modify this morphology. Such studies are complicated by the inherent heterogeneity present in the cell population. It remains difficult to define the morphology of a cell with parameters that can quantify this heterogeneity, leaving the cell biologist to rely on manual inspection of cell images. We propose an alternative to this manual inspection that is based on topological data analysis. We characterise the shape of a cell by its contour and nucleus. We build a filtering of the edges defining the contour using a radial distance function initiated from the nucleus. This filtering is then used to construct a persistence diagram that serves as a signature of the cell shape. Two cells can then be compared by computing the Wasserstein distance between their persistence diagrams. Given a cell population, we then compute a distance matrix that includes all pairwise distances between its members. We analyse this distance matrix using hierarchical clustering with different linkage schemes and define a purity score that quantifies consistency between those different schemes, which can then be used to assess homogeneity within the cell population. We illustrate and validate our approach to identify sub-populations in human mesenchymal stem cell populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.20644v3</guid>
      <category>q-bio.QM</category>
      <category>math.AT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yossi Bokor Bleile, Pooja Yadav, Patrice Koehl, Florian Rehfeldt</dc:creator>
    </item>
    <item>
      <title>Understanding Spatial Regression Models from a Weighting Perspective in an Observational Study of Superfund Remediation</title>
      <link>https://arxiv.org/abs/2508.19572</link>
      <description>arXiv:2508.19572v2 Announce Type: replace-cross 
Abstract: A key challenge in environmental health research is unmeasured spatial confounding, driven by unobserved spatially structured variables that influence both treatment and outcome. A common approach is to fit a spatial regression that models the outcome as a linear function of treatment and covariates, with a spatially structured error term to account for unmeasured spatial confounding. However, it remains unclear to what extent spatial regression actually accounts for such forms of confounding in finite samples, and whether this regression adjustment can be reformulated from a design-based perspective. Motivated by an observational study on the effect of Superfund site remediation on birth outcomes, we present a weighting framework for causal inference that unifies three canonical classes of spatial regression models$\unicode{x2013}$random effects, conditional autoregressive, and Gaussian process models$\unicode{x2013}$and reveals how they implicitly construct causal contrasts across space. Specifically, we show that: (i) the spatial error term induces approximate balance on a latent set of covariates and therefore adjusts for a specific form of unmeasured confounding; and (ii) the covariance structure of the spatial error can be equivalently represented as regressors in a linear model. Building on these insights, we introduce a new estimator that jointly addresses multiple forms of unmeasured spatial confounding and develop visual diagnostics. Using our new estimator, we find evidence of a small but beneficial effect of remediation on the percentage of small vulnerable newborns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19572v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sophie M. Woodward, Francesca Dominici, Jose R. Zubizarreta</dc:creator>
    </item>
    <item>
      <title>The limit joint distributions of some statistics used in testing the quality of random number generators</title>
      <link>https://arxiv.org/abs/2512.08002</link>
      <description>arXiv:2512.08002v2 Announce Type: replace-cross 
Abstract: The limit joint distribution of statistics that are generalizations of some statistics from the NIST STS, TestU01, and other packages is found under the following hypotheses $H_0$ and $H_1$. Hypothesis $H_0$ states that the tested sequence is a sequence of independent random vectors with a known distribution, and the simple alternative hypothesis $H_1$ converges in some sense to $H_0$ with increasing sample size. In addition, an analogue of the Berry-Esseen inequality is obtained for the statistics under consideration, and conditions for their asymptotic independence are found.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08002v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. P. Savelov</dc:creator>
    </item>
  </channel>
</rss>
