<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 May 2025 01:43:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Physics of Local Optimization in Complex Disordered Systems</title>
      <link>https://arxiv.org/abs/2505.02927</link>
      <description>arXiv:2505.02927v1 Announce Type: new 
Abstract: Limited resources motivate decomposing large-scale problems into smaller, "local" subsystems and stitching together the so-found solutions. We explore the physics underlying this approach and discuss the concept of "local hardness", i.e., complexity from the local solver perspective, in determining the ground states of both P- and NP-hard spin-glasses and related systems. Depending on the model considered, we observe varying scaling behaviors in how errors associated with local predictions decay as a function of the size of the solved subsystem. These errors stem from global critical threshold instabilities, characterized by gapless, avalanche-like excitations that follow scale-invariant size distributions. Away from criticality, local solvers quickly achieve high accuracy, aligning closely with the results of the more computationally intensive global minimization. These findings shed light on how Nature may operate solely through local actions at her disposal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02927v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.comp-ph</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mutian Shen, Gerardo Ortiz, Zhiqiao Dong, Martin Weigel, Zohar Nussinov</dc:creator>
    </item>
    <item>
      <title>Neighbor-induced damage percolation</title>
      <link>https://arxiv.org/abs/2505.03026</link>
      <description>arXiv:2505.03026v1 Announce Type: new 
Abstract: We consider neighbor-induced damage percolation, a model describing systems where the inactivation of some elements may damage their neighboring active ones, making them unusable. We present an exact solution for the size of the giant usable component (GUC) and the giant damaged component (GDC) in uncorrelated random graphs. We show that, even for strongly heterogeneous distributions, the GUC always appears at a finite threshold and its formation is characterized by homogeneous mean-field percolation critical exponents. The threshold is a nonmonotonic function of connectivity: robustness is maximized by networks with finite optimal average degree. We also show that, if the average degree is large enough, a damaged phase appears, characterized by the existence of a GDC, bounded by two distinct percolation transitions. The birth and the dismantling of the GDC are characterized by standard percolation critical exponents in networks, except for the dismantling in scale-free networks where new critical exponents are found. Numerical simulations on regular lattices in D = 2 show that the existence of a GDC depends not only on the spatial dimension but also on the lattice coordination number.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03026v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Cirigliano, Claudio Castellano</dc:creator>
    </item>
    <item>
      <title>Hyperchaos and complex dynamical regimes in $N$-dimensional neuron lattices</title>
      <link>https://arxiv.org/abs/2505.03051</link>
      <description>arXiv:2505.03051v1 Announce Type: cross 
Abstract: We study the dynamics of $N$-dimensional lattices of nonchaotic Rulkov neurons coupled with a flow of electrical current. We consider both nearest-neighbor and next-nearest-neighbor couplings, homogeneous and heterogeneous neurons, and small and large lattices over a wide range of electrical coupling strengths. As the coupling strength is varied, the neurons exhibit a number of complex dynamical regimes, including unsynchronized chaotic spiking, local quasi-bursting, synchronized chaotic bursting, and synchronized hyperchaos. For lattices in higher spatial dimensions, we discover dynamical effects arising from the ``destructive interference'' of many connected neurons and miniature ``phase transitions'' from coordinated spiking threshold crossings. In large two- and three-dimensional neuron lattices, we observe emergent dynamics such as local synchronization, quasi-synchronization, and lag synchronization. These results illustrate the rich dynamics that emerge from coupled neurons in multiple spatial dimensions, highlighting how dimensionality, connectivity, and heterogeneity critically shape the collective behavior of neuronal systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03051v1</guid>
      <category>nlin.CD</category>
      <category>cond-mat.dis-nn</category>
      <category>math.DS</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brandon B. Le, Dima Watkins</dc:creator>
    </item>
    <item>
      <title>Chess variation entropy and engine relevance for humans</title>
      <link>https://arxiv.org/abs/2505.03251</link>
      <description>arXiv:2505.03251v1 Announce Type: cross 
Abstract: Modern chess engines significantly outperform human players and are essential for evaluating positions and move quality. These engines assign a numerical evaluation $E$ to positions, indicating an advantage for either white or black, but similar evaluations can mask varying levels of move complexity. While some move sequences are straightforward, others demand near-perfect play, limiting the practical value of these evaluations for most players. To quantify this problem, we use entropy to measure the complexity of the principal variation (the sequence of best moves). Variations with forced moves have low entropy, while those with multiple viable alternatives have high entropy. Our results show that, except for experts, most human players struggle with high-entropy variations, especially when $|E|&lt;100$ centipawns, which accounts for about $2/3$ of positions. This underscores the need for AI-generated evaluations to convey the complexity of underlying move sequences, as they often exceed typical human cognitive capabilities, reducing their practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03251v1</guid>
      <category>physics.soc-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.HC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marc Barthelemy</dc:creator>
    </item>
    <item>
      <title>Design principles of deep translationally-symmetric neural quantum states for frustrated magnets</title>
      <link>https://arxiv.org/abs/2505.03466</link>
      <description>arXiv:2505.03466v1 Announce Type: cross 
Abstract: Deep neural network quantum states have emerged as a leading method for studying the ground states of quantum magnets. Successful architectures exploit translational symmetry, but the root of their effectiveness and differences between architectures remain unclear. Here, we apply the ConvNext architecture, designed to incorporate elements of transformers into convolutional networks, to quantum many-body ground states. We find that it is remarkably similar to the factored vision transformer, which has been employed successfully for several frustrated spin systems, allowing us to relate this architecture to more conventional convolutional networks. Through a series of numerical experiments we design the ConvNext to achieve greatest performance at lowest computational cost, then apply this network to the Shastry-Sutherland and J1-J2 models, obtaining variational energies comparable to the state of the art, providing a blueprint for network design choices of translationally-symmetric architectures to tackle challenging ground-state problems in frustrated magnetism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03466v1</guid>
      <category>cond-mat.str-el</category>
      <category>cond-mat.dis-nn</category>
      <category>quant-ph</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajah P. Nutakki, Ahmedeo Shokry, Filippo Vicentini</dc:creator>
    </item>
    <item>
      <title>Information-theoretic reduction of deep neural networks to linear models in the overparametrized proportional regime</title>
      <link>https://arxiv.org/abs/2505.03577</link>
      <description>arXiv:2505.03577v1 Announce Type: cross 
Abstract: We rigorously analyse fully-trained neural networks of arbitrary depth in the Bayesian optimal setting in the so-called proportional scaling regime where the number of training samples and width of the input and all inner layers diverge proportionally. We prove an information-theoretic equivalence between the Bayesian deep neural network model trained from data generated by a teacher with matching architecture, and a simpler model of optimal inference in a generalized linear model. This equivalence enables us to compute the optimal generalization error for deep neural networks in this regime. We thus prove the "deep Gaussian equivalence principle" conjectured in Cui et al. (2023) (arXiv:2302.00375). Our result highlights that in order to escape this "trivialisation" of deep neural networks (in the sense of reduction to a linear model) happening in the strongly overparametrized proportional regime, models trained from much more data have to be considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03577v1</guid>
      <category>math.ST</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.IT</category>
      <category>math-ph</category>
      <category>math.IT</category>
      <category>math.MP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Camilli, Daria Tieplova, Eleonora Bergamin, Jean Barbier</dc:creator>
    </item>
    <item>
      <title>Expedited thermalization dynamics in incommensurate systems</title>
      <link>https://arxiv.org/abs/2505.03645</link>
      <description>arXiv:2505.03645v1 Announce Type: cross 
Abstract: We study the thermalization dynamics of a quantum system embedded in an incommensurate potential and coupled to a Markovian thermal reservoir. The dephasing induced by the bath drives the system toward an infinite-temperature steady state, erasing all initial information-including signatures of localization. We find that initially localized states can relax to the homogeneous steady state faster than delocalized states. Moreover, low-temperature initial states thermalize to infinite temperature more rapidly than high-temperature states-a phenomenon reminiscent of the Mpemba effect, where hotter liquids freeze faster than warmer ones. The slowest relaxation mode in the Liouvillian plays critical role in the expedited thermalization for localized or cold initial states. Our results reveal that the combination of disordered structure and environmental dissipation may lead to non-trivial thermalization behavior, which advances both the conceptual framework of the Mpemba effect and the theoretical understanding of nonequilibrium processes in dissipative disordered systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03645v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingdi Xu, Zijun Wei, Xiang-Ping Jiang, Lei Pan</dc:creator>
    </item>
    <item>
      <title>Incremental universality of Wigner random matrices</title>
      <link>https://arxiv.org/abs/2505.03714</link>
      <description>arXiv:2505.03714v1 Announce Type: cross 
Abstract: Properties of universality have essential relevance for the theory of random matrices usually called the Wigner ensemble. The issue was analysed up to recent years with detailed and relevant results. We present a slightly different view and compare the large-$n$ limit of connected correlators of distinct ensembles: universality has steps or degrees, precisely counted by the number of probability moments of the matrix entries, which match among distinct ensembles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03714v1</guid>
      <category>math-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>math.MP</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giovanni M. Cicuta, Mario Pernici</dc:creator>
    </item>
    <item>
      <title>Neural network distillation of orbital dependent density functional theory</title>
      <link>https://arxiv.org/abs/2410.16408</link>
      <description>arXiv:2410.16408v2 Announce Type: replace-cross 
Abstract: Density functional theory (DFT) offers a desirable balance between quantitative accuracy and computational efficiency in practical many-electron calculations. Its central component, the exchange-correlation energy functional, has been approximated with increasing levels of complexity ranging from strictly local approximations to nonlocal and orbital-dependent expressions with many tuned parameters. In this paper, we formulate a general way of rewriting complex density functionals using deep neural networks in a way that allows for simplified computation of Kohn-Sham potentials as well as higher functional derivatives through automatic differentiation, enabling access to highly nonlinear response functions and forces. These goals are achieved by using a recently developed class of robust neural network models capable of modeling functionals, as opposed to functions, with explicitly enforced spatial symmetries. Functionals treated in this way are then called global density approximations and can be seamlessly integrated with existing DFT workflows. Tests are performed for a dataset featuring a large variety of molecular structures and popular meta-generalized gradient approximation density functionals, where we successfully eliminate orbital dependencies coming from the kinetic energy density, and discover a high degree of transferability to a variety of physical systems. The presented framework is general and could be extended to more complex orbital and energy dependent functionals as well as refined with specialized datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16408v2</guid>
      <category>physics.chem-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>physics.comp-ph</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevResearch.7.023113</arxiv:DOI>
      <arxiv:journal_reference>M. Medvidovi\'c, J. C. Umana, I. Ahmadabadi, D. Di Sante, J. Flick, and A. Rubio, Neural network distillation of orbital dependent density functional theory, Phys. Rev. Research 7, 023113 (2025)</arxiv:journal_reference>
      <dc:creator>Matija Medvidovi\'c, Jaylyn C. Umana, Iman Ahmadabadi, Domenico Di Sante, Johannes Flick, Angel Rubio</dc:creator>
    </item>
    <item>
      <title>Neural Network Matrix Product Operator: A Multi-Dimensionally Integrable Machine Learning Potential</title>
      <link>https://arxiv.org/abs/2410.23858</link>
      <description>arXiv:2410.23858v3 Announce Type: replace-cross 
Abstract: A neural network-based machine learning potential energy surface (PES) expressed in a matrix product operator (NN-MPO) is proposed. The MPO form enables efficient evaluation of high-dimensional integrals that arise in solving the time-dependent and time-independent Schr\"odinger equation and effectively overcomes the so-called curse of dimensionality. This starkly contrasts with other neural network-based machine learning PES methods, such as multi-layer perceptrons (MLPs), where evaluating high-dimensional integrals is not straightforward due to the fully connected topology in their backbone architecture. Nevertheless, the NN-MPO retains the high representational capacity of neural networks. NN-MPO can achieve spectroscopic accuracy with a test mean absolute error (MAE) of 3.03 cm$^{-1}$ for a fully coupled six-dimensional ab initio PES, using only 625 training points distributed across a 0 to 17,000 cm$^{-1}$ energy range. Our Python implementation is available at https://github.com/KenHino/Pompon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23858v3</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.chem-ph</category>
      <category>quant-ph</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kentaro Hino, Yuki Kurashige</dc:creator>
    </item>
  </channel>
</rss>
