<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Dec 2024 02:45:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Use of diverse data sources to control which topics emerge in a science map</title>
      <link>https://arxiv.org/abs/2412.07550</link>
      <description>arXiv:2412.07550v1 Announce Type: new 
Abstract: Traditional science maps visualize topics by clustering documents, but they are inherently biased toward clustering certain topics over others. If these topics could be chosen, then the science maps could be tailored for different needs. In this paper, we explore the use of document networks from diverse data sources as a tool to control the topic clustering bias of a science map. We analyze this by evaluating the clustering effectiveness of several topic categories over two traditional and six non-traditional data sources. We found that the topics favored in each non-traditional data source are about: Health for Facebook users, biotechnology for patent families, government and social issues for policy documents, food for Twitter conversations, nursing for Twitter users, and geographical entities for document authors (the favoring in this latter source was particularly strong). Our results show that diverse data sources can be used to control topic bias, which opens up the possibility of creating science maps tailored for different needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07550v1</guid>
      <category>cs.DL</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Pablo Bascur, Rodrigo Costas, Suzan Verberne</dc:creator>
    </item>
    <item>
      <title>Benchmark for Evaluation and Analysis of Citation Recommendation Models</title>
      <link>https://arxiv.org/abs/2412.07713</link>
      <description>arXiv:2412.07713v1 Announce Type: cross 
Abstract: Citation recommendation systems have attracted much academic interest, resulting in many studies and implementations. These systems help authors automatically generate proper citations by suggesting relevant references based on the text they have written. However, the methods used in citation recommendation differ across various studies and implementations. Some approaches focus on the overall content of papers, while others consider the context of the citation text. Additionally, the datasets used in these studies include different aspects of papers, such as metadata, citation context, or even the full text of the paper in various formats and structures. The diversity in models, datasets, and evaluation metrics makes it challenging to assess and compare citation recommendation methods effectively. To address this issue, a standardized dataset and evaluation metrics are needed to evaluate these models consistently. Therefore, we propose developing a benchmark specifically designed to analyze and compare citation recommendation models. This benchmark will evaluate the performance of models on different features of the citation context and provide a comprehensive evaluation of the models across all these tasks, presenting the results in a standardized way. By creating a benchmark with standardized evaluation metrics, researchers and practitioners in the field of citation recommendation will have a common platform to assess and compare different models. This will enable meaningful comparisons and help identify promising approaches for further research and development in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07713v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.DL</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Puja Maharjan</dc:creator>
    </item>
    <item>
      <title>Mitigating Consequences of Prestige in Citations of Publications</title>
      <link>https://arxiv.org/abs/2411.05584</link>
      <description>arXiv:2411.05584v2 Announce Type: replace 
Abstract: For many public research organizations, funding creation of science and maximizing scientific output is of central interest. Typically, when evaluating scientific production for funding, citations are utilized as a proxy, although these are severely influenced by factors beyond scientific impact. This study aims to mitigate the consequences of the Matthew effect in citations, where prominent authors and prestigious journals receive more citations regardless of the scientific content of the publications. To this end, the study presents an approach to predicting citations of papers based solely on observable characteristics available at the submission stage of a double-blind peer-review process. Combining classical linear models, generalized linear models and utilizing large-scale data sets on biomedical papers based on the PubMed database, the results demonstrate that it is possible to make fairly accurate predictions of citations using only observable characteristics of papers excluding information on authors and journals, thereby mitigating the Matthew effect. Thus, the outcomes have important implications for the field of scientometrics, providing a more objective method for citation prediction by relying on pre-publication variables that are immune to manipulation by authors and journals, thereby enhancing the objectivity of the evaluation process. Our approach is thus important for government agencies responsible for funding the creation of high-quality scientific content rather than perpetuating prestige.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05584v2</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Balzer, Adhen Benlahlou</dc:creator>
    </item>
  </channel>
</rss>
