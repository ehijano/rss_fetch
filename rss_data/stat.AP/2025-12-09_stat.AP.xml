<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Dec 2025 02:44:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Spatial Analysis for AI-segmented Histopathology Images: Methods and Implementation</title>
      <link>https://arxiv.org/abs/2512.06116</link>
      <description>arXiv:2512.06116v1 Announce Type: new 
Abstract: Quantitatively characterizing the spatial organization of cells and their interaction is essential for understanding cancer progression and immune response. Recent advances in machine intelligence have enabled large-scale segmentation and classification of cell nuclei from digitized histopathology slides, generating massive point pattern and marked point pattern datasets. However, accessible tools for quantitative analysis of such complex cellular spatial organization remain limited. In this paper, we first review 27 traditional spatial summary statistics, areal indices, and topological features applicable to point pattern data. Then, we introduce SASHIMI (Spatial Analysis for Segmented Histopathology Images using Machine Intelligence), a browser-based tool for real-time spatial analysis of artificial intelligence (AI)-segmented histopathology images. SASHIMI computes a comprehensive suite of mathematically grounded descriptors, including spatial statistics, proximity-based measures, grid-level similarity indices, spatial autocorrelation measures, and topological descriptors, to quantify cellular abundance and cell-cell interaction. Applied to two cancer datasets, oral potentially malignant disorders (OPMD) and non-small-cell lung cancer (NSCLC), SASHIMI identified multiple spatial features significantly associated with patient survival outcomes. SASHIMI provides an accessible and reproducible platform for single-cell-level spatial profiling of tumor morphological architecture, offering a robust framework for quantitative exploration of tissue organization across cancer types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06116v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Y. Park, F. Wu, X. Feng, S. Yang, E. H. Wang, B. Yao, C. Moon, G. Xiao, Q. Li</dc:creator>
    </item>
    <item>
      <title>Mode Choice Heterogeneity Among Zero-Vehicle Households: A Latent Class Cluster Approach</title>
      <link>https://arxiv.org/abs/2512.06127</link>
      <description>arXiv:2512.06127v1 Announce Type: new 
Abstract: In transportation planning, Zero-Vehicle Households (ZVHs) are often treated as a uniform group with limited mobility options and assumed to rely heavily on walking or public transit. However, such assumptions overlook the diverse travel strategies ZVHs employ in response to varying trip needs and sociodemographic factors. This study addresses this gap by applying a weighted Latent Class Cluster Analysis (LCCA) to data from the 2022 National Household Travel Survey (NHTS) to uncover distinct mobility patterns within the ZVH population. Using travel mode and trip purpose as indicators and demographic, economic, and built environment variables as covariates, we identified three latent classes :Shared mobility errand workers (36.3%), who primarily use transit and ridehailing for commuting and essential activities; car based shoppers (29.9%), who depend on informal vehicle access for longer discretionary trips and active travel Shoppers (33.8%), who rely on walking or cycling for short, local shopping oriented travel. These behavioral findings enable policymakers to develop differentiated planning solutions to the specific needs of each segment among the ZVHs population across varied geographic and demographic settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06127v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nancy Kasamala, Arthur Mukwaya, Nana Kankam Gyimah, Judith Mwakalonge, Gurcan Comert, Saidi Siuhi, Akinbobola Jegede</dc:creator>
    </item>
    <item>
      <title>Forests of Uncertaint(r)ees: Using tree-based ensembles to estimate probability distributions of future conflict</title>
      <link>https://arxiv.org/abs/2512.06210</link>
      <description>arXiv:2512.06210v1 Announce Type: new 
Abstract: Predictions of fatalities from violent conflict on the PRIO-GRID-month (pgm) level are characterized by high levels of uncertainty, limiting their usefulness in practical applications. We discuss the two main sources of uncertainty for this prediction task, the nature of violent conflict and data limitations, embedding this in the wider literature on uncertainty quantification in machine learning. We develop a strategy to quantify uncertainty in conflict forecasting, shifting from traditional point predictions to full predictive distributions. Our approach compares and combines multiple tree-based classifiers and distributional regressors in a custom auto-ML setup, estimating distributions for each pgm individually. We also test the integration of regional models in spatial ensembles as a potential avenue to reduce uncertainty. The models are able to consistently outperform a suite of benchmarks derived from conflict history in predictions up to one year in advance, with performance driven by regions where conflict was observed. With our evaluation, we emphasize the need to understand how a metric behaves for a given prediction problem, in our case characterized by extremely high zero-inflatedness. While not resulting in better predictions, the integration of smaller models does not decrease performance for this prediction task, opening avenues to integrate data sources with less spatial coverage in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06210v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Mittermaier, Tobias Bohne, Martin Hofer, Daniel Racek</dc:creator>
    </item>
    <item>
      <title>Spatio-temporal Shared-Field Modeling of Beluga and Bowhead Whale Sightings Using a Joint Marked Log-Gaussian Cox Process</title>
      <link>https://arxiv.org/abs/2512.06450</link>
      <description>arXiv:2512.06450v1 Announce Type: new 
Abstract: We analyze a decade of aerial survey whale sighting data (2010-2019) to model the spatio-temporal distributions and group sizes of beluga (Delphinapterus leucas) and bowhead (Balaena mysticetus) whales in the United States Arctic. To jointly model these species, we develop a multi-species Log-Gaussian Cox Process (LGCP) in which species specific intensity surfaces are linked through a shared latent spatial Gaussian field. This structure allows the model to capture broad spatial patterns common to both species while still accommodating species level responses to environmental covariates and seasonal variation. The latent field is represented using the Stochastic Partial Differential Equation (SPDE) approach with an anisotropic Matern covariance, implemented on an ocean constrained triangulated mesh so that spatial dependence aligns with marine geography. Whale group size is incorporated through a marked point process extension with species specific negative binomial marks, allowing occurrence and group sizes to be jointly analyzed within a unified framework. Inference is carried out using the Integrated Nested Laplace Approximation (INLA), enabling efficient model fitting over a decade of survey effort. The results highlight persistent multi-species hotspots and distinct environmental associations for each species, demonstrating the value of shared field LGCPs for joint species distribution modeling in data sparse and heterogeneous survey settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06450v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mauli Pant, Linda Fernandez, Indranil Sahoo</dc:creator>
    </item>
    <item>
      <title>A Latent Variable Framework for Scaling Laws in Large Language Models</title>
      <link>https://arxiv.org/abs/2512.06553</link>
      <description>arXiv:2512.06553v1 Announce Type: new 
Abstract: We propose a statistical framework built on latent variable modeling for scaling laws of large language models (LLMs). Our work is motivated by the rapid emergence of numerous new LLM families with distinct architectures and training strategies, evaluated on an increasing number of benchmarks. This heterogeneity makes a single global scaling curve inadequate for capturing how performance varies across families and benchmarks. To address this, we propose a latent variable modeling framework in which each LLM family is associated with a latent variable that captures the common underlying features in that family. An LLM's performance on different benchmarks is then driven by its latent skills, which are jointly determined by the latent variable and the model's own observable features. We develop an estimation procedure for this latent variable model and establish its statistical properties. We also design efficient numerical algorithms that support estimation and various downstream tasks. Empirically, we evaluate the approach on 12 widely used benchmarks from the Open LLM Leaderboard (v1/v2).</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06553v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peiyao Cai, Chengyu Cui, Felipe Maia Polo, Seamus Somerstep, Leshem Choshen, Mikhail Yurochkin, Moulinath Banerjee, Yuekai Sun, Kean Ming Tan, Gongjun Xu</dc:creator>
    </item>
    <item>
      <title>Disentangling the Mediation Pathways of Depression in Asian Students and Workers</title>
      <link>https://arxiv.org/abs/2512.06654</link>
      <description>arXiv:2512.06654v1 Announce Type: new 
Abstract: Depression is a major global mental health issue shaped by cultural, demographic, and occupational factors. This study compares predictors of depression across student and worker populations using datasets from India, Malaysia, and China. The India dataset was split into student and worker groups, while the Malaysia dataset includes only students and the China (CHARLS) dataset includes only workers. After harmonizing variables, we applied logistic regression, random forest, and causal forest models to identify key predictors and subgroup-specific effects, and conducted causal mediation analysis (CMA) to assess whether variables operate through intermediaries such as perceived pressure. Among students, pressure, age, workload, financial stress, mental health history, and satisfaction were significant predictors; similar factors emerged for workers. Notably, age showed opposite effects across groups: younger students were more likely to experience depression, whereas older workers showed higher risk. Model performance showed moderate internal accuracy but weaker external generalizability across countries, with random forest outperforming logistic regression. Causal forest results indicated limited heterogeneity in the effect of pressure, while CMA showed that pressure does not mediate the effect of age but operates more directly, and satisfaction influences depression partly through pressure. Overall, pressure consistently emerged as the strongest predictor, suggesting that interventions targeting academic and occupational stress may help reduce depressive symptoms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06654v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaojin Nan, Ran Chen</dc:creator>
    </item>
    <item>
      <title>Partially Observable Markov Decision Process Framework for Operating Condition Optimization Using Real-Time Degradation Signals</title>
      <link>https://arxiv.org/abs/2512.06682</link>
      <description>arXiv:2512.06682v1 Announce Type: new 
Abstract: In many engineering systems, proper predictive maintenance and operational control are essential to increase efficiency and reliability while reducing maintenance costs. However, one of the major challenges is that many sensors are used for system monitoring. Analyzing these sensors simultaneously for better predictive maintenance optimization is often very challenging. In this paper, we propose a systematic decision-making framework to improve the system performance in manufacturing practice, considering the real-time degradation signals generated by multiple sensors. Specifically, we propose a partially observed Markov decision process (POMDP) model to generate the optimal capacity and predictive maintenance policies, given the fact that the observation of the system state is imperfect. Such work provides a systematic approach that focuses on jointly controlling the operating conditions and preventive maintenance utilizing the real-time machine deterioration signals by incorporating the degradation constraint and non-observable states. We apply this technique to the bearing degradation data and NASA aircraft turbofan engine dataset, demonstrating the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06682v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Boyang Xu, Yunyi Kang, Xinyu Zhao, Hao Yan, Feng Ju</dc:creator>
    </item>
    <item>
      <title>Machine Learning-based Unfolding for Cross Section Measurements in the Presence of Nuisance Parameters</title>
      <link>https://arxiv.org/abs/2512.07074</link>
      <description>arXiv:2512.07074v1 Announce Type: new 
Abstract: Statistically correcting measured cross sections for detector effects is an important step across many applications. In particle physics, this inverse problem is known as \textit{unfolding}. In cases with complex instruments, the distortions they introduce are often known only implicitly through simulations of the detector. Modern machine learning has enabled efficient simulation-based approaches for unfolding high-dimensional data. Among these, one of the first methods successfully deployed on experimental data is the \textsc{OmniFold} algorithm, a classifier-based Expectation-Maximization procedure. In practice, however, the forward model is only approximately specified, and the corresponding uncertainty is encoded through nuisance parameters. Building on the well-studied \textsc{OmniFold} algorithm, we show how to extend machine learning-based unfolding to incorporate nuisance parameters. Our new algorithm, called Profile \textsc{OmniFold}, is demonstrated using a Gaussian example as well as a particle physics case study using simulated data from the CMS Experiment at the Large Hadron Collider.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07074v1</guid>
      <category>stat.AP</category>
      <category>hep-ex</category>
      <category>hep-ph</category>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huanbiao Zhu, Krish Desai, Mikael Kuusela, Vinicius Mikuni, Benjamin Nachman, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Big shells, bigger data: cohort analysis of Chesapeake Bay Crassostrea virginica reefs</title>
      <link>https://arxiv.org/abs/2512.07080</link>
      <description>arXiv:2512.07080v1 Announce Type: new 
Abstract: Oysters in Virginia Chesapeake Bay oyster reefs are "age-truncated", possibly due to a combination of historical overfishing, disease epizootics, environmental degradation, and climate change. Research has suggested that oysters exhibit resilience to environmental stressors; however, that evidence is based on the current limited understanding of oyster lifespan. Until this paper, the Virginia Oyster Stock Assessment and Replenishment Archive (VOSARA), a spatially and temporally expansive dataset (222 reefs across 2003-2023) of shell lengths (SL, mm), had yet to be examined comprehensively in the context of resilience. We develop a novel method using Gaussian mixture modeling (GMM) to identify the age groups in each reef using yearly SL data and then link those age groups over time to identify cohorts and estimate their lifespan. Sixty-four reefs (29%) are deemed to have sufficient data (at least 300 oysters sampled for a minimum of 8 consecutive years) for this analysis. We fit univariate GMMs for each year ($t$) and reef ($r$) for each of the seven river strata ($R$) to estimate 1) the mean and standard deviation of SL for each $a_{Rrt}$th age group, and 2) the mixture percentage of each $a_{Rrt}$th age group. We link age groups across time to infer age cohorts by developing a mechanistic algorithm that prevents the shrinking of shell length when an $a_{Rrt}$th group becomes an ($a_{R,r,t+1}$)th group. Our method shows promise in identifying oyster cohorts and estimating lifespan solely using SL data. Our results show signals of resiliency in almost all river systems: oyster cohorts live longer and grow larger in the mid-to-late 2010s compared to the early 2000s.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07080v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <category>stat.CO</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Madison D. Griffin, Grace S. Chiu, Roger L. Mann, Melissa J. Southworth, John K. Thomas</dc:creator>
    </item>
    <item>
      <title>Facilitating Conditions as an Enabler, Not a Direct Motivator: A Robustness and Mediation Analysis of E-Learning Adoption</title>
      <link>https://arxiv.org/abs/2512.07185</link>
      <description>arXiv:2512.07185v1 Announce Type: new 
Abstract: Despite substantial institutional investment in e-learning infrastructure, student engagement often fails to meet expectations--a persistent paradox that challenges the established direct relationship between Facilitating Conditions (FC) and behavioral intention within the classic UTAUT framework. To resolve this theoretical puzzle, we reconceptualized the role of FC through an empirical study of 470 Indonesian university students. Our robust, multi-stage analytical approach first confirmed the significant influence of established drivers--Performance Expectancy (beta=0.190), Effort Expectancy (beta=0.198), Social Influence (beta=0.151), and Perceived Enjoyment (beta=0.472)--on Behavioral Intention (BI), which in turn strongly predicted Use Behavior (beta=0.666). Crucially, however, the direct effect of FC on BI proved non-significant (beta=-0.085). A subsequent mediation model revealed FC's true function as a foundational enabling construct that operates indirectly by powerfully enhancing both Performance Expectancy (beta=0.556) and Effort Expectancy (beta=0.419). Our findings demonstrate that the value of technological infrastructure lies not in its mere presence, but in its dynamic capacity to enable learning and optimize user experience. This research advances a refined "enabling pathway" theoretical framework, guiding administrators to shift the focus of technological investment from merely providing tools to strategically crafting learning experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07185v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jaka Nugraha, Noyyn Sun, Xinlin Zhao, Vindi Kusuma Wardani, Inna Koblianska, Jiunn-Woei Lian</dc:creator>
    </item>
    <item>
      <title>Bridging CORDEX and CMIP6: Machine Learning Downscaling for Wind and Solar Energy Droughts in Central Europe</title>
      <link>https://arxiv.org/abs/2512.07429</link>
      <description>arXiv:2512.07429v1 Announce Type: new 
Abstract: Reliable regional climate information is essential for assessing the impacts of climate change and for planning in sectors such as renewable energy; yet, producing high-resolution projections through coordinated initiatives like CORDEX that run multiple physical regional climate models is both computationally demanding and difficult to organize. Machine learning emulators that learn the mapping between global and regional climate fields offer a promising way to address these limitations. Here we introduce the application of such an emulator: trained on CMIP5 and CORDEX simulations, it reproduces regional climate model data with sufficient accuracy. When applied to CMIP6 simulations not seen during training, it also produces realistic results, indicating stable performance. Using CORDEX data, CMIP5 and CMIP6 simulations, as well as regional data generated by two machine learning models, we analyze the co-occurrence of low wind speed and low solar radiation and find indications that the number of such energy drought days is likely to decrease in the future. Our results highlight that downscaling with machine learning emulators provides an efficient complement to efforts such as CORDEX, supplying the higher-resolution information required for impact assessments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07429v1</guid>
      <category>stat.AP</category>
      <category>physics.ao-ph</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nina Effenberger, Maxim Samarin, Maybritt Schillinger, Reto Knutti</dc:creator>
    </item>
    <item>
      <title>Permanent and transitory crime risk in variable-density hot spot analysis</title>
      <link>https://arxiv.org/abs/2512.07467</link>
      <description>arXiv:2512.07467v1 Announce Type: new 
Abstract: Crime prevention measures, aiming for the effective and efficient spending of public resources, rely on the empirical analysis of spatial and temporal data for public safety outcomes. We perform a variable-density cluster analysis on crime incident reports in the City of Chicago for the years 2001--2022 to investigate changes in crime share composition for hot spots of different densities. Contributing to and going beyond the existing wealth of research on criminological applications in the operational research literature, we study the evolution of crime type shares in clusters over the course of two decades and demonstrate particularly notable impacts of the COVID-19 pandemic and its associated social contact avoidance measures, as well as a dependence of these effects on the primary function of city areas. Our results also indicate differences in the relative difficulty to address specific crime types, and an analysis of spatial autocorrelations further shows variations in incident uniformity between clusters and outlier areas at different distance radii. We discuss our findings in the context of the interplay between operational research and criminal justice, the practice of hot spot policing and public safety optimization, and the factors contributing to, and challenges and risks due to, data biases as an often neglected factor in criminological applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07467v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ben Moews</dc:creator>
    </item>
    <item>
      <title>Meta-analyses of dietary exposures must consider energy adjustment: recommendations from a meta-scientific review</title>
      <link>https://arxiv.org/abs/2512.07531</link>
      <description>arXiv:2512.07531v1 Announce Type: new 
Abstract: In observational studies of dietary exposures, the energy adjustment strategy has a critical impact on the effect being estimated. Adjusting for total energy intake or expressing the exposure as a percentage of total energy, leads to a substitution effect being estimated. This impacts the interpretation of primary studies and meta-analyses. Unless energy adjustment strategies are considered, meta-analyses may end up pooling estimates for incomparable effects. This meta-scientific review aimed to investigate the extent to which meta-analyses of dietary exposures may be pooling incomparable effects by reviewing the energy adjustment strategies. We identified all meta-analyses examining the relationship between saturated fat and fish and cardiovascular disease. The two most recent and two most cited reviews for each exposure were examined, along with all primary studies. Information on the study aims, targeted effects, and interpretations were summarized. The eight meta-analyses summarised results from 82 primary studies including 144 unique models. Only one meta-analysis explicitly considered the energy adjustment strategy of the primary studies to determine eligibility for a substitution subgroup analysis. None of the meta-analyses acknowledged that they were pooling estimates for different effects. 82% of the models from the primary studies were implicitly estimating substitution effects but this was not explicitly stated in most study aims, interpretation or conclusions. Our meta-scientific review found little evidence that the energy adjustment strategies of the primary studies were being considered in the synthesis or interpretation of evidence. Consequently, the pooled estimates reflect ill-defined quantities with unclear interpretations. We offer recommendations to improve the conduct of future meta-analyses and the quality of evidence that informs nutritional recommendations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07531v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Natalia Ortega, Peter WG Tennant, Darren C Greenwood, Octavio Pano, Christina C Dahm, Russell J de Souza, Daniel B Ibsen, Conor J MacDonald, Deirdre K Tobias, Georgia D Tomova</dc:creator>
    </item>
    <item>
      <title>Detrended cross-correlations and their random matrix limit: an example from the cryptocurrency market</title>
      <link>https://arxiv.org/abs/2512.06473</link>
      <description>arXiv:2512.06473v1 Announce Type: cross 
Abstract: Correlations in complex systems are often obscured by nonstationarity, long-range memory, and heavy-tailed fluctuations, which limit the usefulness of traditional covariance-based analyses. To address these challenges, we construct scale and fluctuation-dependent correlation matrices using the multifractal detrended cross-correlation coefficient $\rho_r$ that selectively emphasizes fluctuations of different amplitudes. We examine the spectral properties of these detrended correlation matrices and compare them to the spectral properties of the matrices calculated in the same way from synthetic Gaussian and $q$Gaussian signals. Our results show that detrending, heavy tails, and the fluctuation-order parameter $r$ jointly produce spectra, which substantially depart from the random case even under absence of cross-correlations in time series. Applying this framework to one-minute returns of 140 major cryptocurrencies from 2021-2024 reveals robust collective modes, including a dominant market factor and several sectoral components whose strength depends on the analyzed scale and fluctuation order. After filtering out the market mode, the empirical eigenvalue bulk aligns closely with the limit of random detrended cross-correlations, enabling clear identification of structurally significant outliers. Overall, the study provides a refined spectral baseline for detrended cross-correlations and offers a promising tool for distinguishing genuine interdependencies from noise in complex, nonstationary, heavy-tailed systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06473v1</guid>
      <category>q-fin.ST</category>
      <category>cs.CE</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/e27121236</arxiv:DOI>
      <arxiv:journal_reference>Entropy 2025, 27(12), 1236</arxiv:journal_reference>
      <dc:creator>Stanis{\l}aw Dro\.zd\.z, Pawe{\l} Jarosz, Jaros{\l}aw Kwapie\'n, Maria Skupie\'n, Marcin W\k{a}torek</dc:creator>
    </item>
    <item>
      <title>AI as "Co-founder": GenAI for Entrepreneurship</title>
      <link>https://arxiv.org/abs/2512.06506</link>
      <description>arXiv:2512.06506v1 Announce Type: cross 
Abstract: This paper studies whether, how, and for whom generative artificial intelligence (GenAI) facilitates firm creation. Our identification strategy exploits the November 2022 release of ChatGPT as a global shock that lowered start-up costs and leverages variations across geo-coded grids with differential pre-existing AI-specific human capital. Using high-resolution and universal data on Chinese firm registrations by the end of 2024, we find that grids with stronger AI-specific human capital experienced a sharp surge in new firm formation$\unicode{x2013}$driven entirely by small firms, contributing to 6.0% of overall national firm entry. Large-firm entry declines, consistent with a shift toward leaner ventures. New firms are smaller in capital, shareholder number, and founding team size, especially among small firms. The effects are strongest among firms with potential AI applications, weaker financing needs, and among first-time entrepreneurs. Overall, our results highlight that GenAI serves as a pro-competitive force by disproportionately boosting small-firm entry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06506v1</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junhui Jeff Cai, Xian Gu, Liugang Sheng, Mengjia Xia, Linda Zhao, Wu Zhu</dc:creator>
    </item>
    <item>
      <title>Diagnosis-based mortality prediction for intensive care unit patients via transfer learning</title>
      <link>https://arxiv.org/abs/2512.06511</link>
      <description>arXiv:2512.06511v1 Announce Type: cross 
Abstract: In the intensive care unit, the underlying causes of critical illness vary substantially across diagnoses, yet prediction models accounting for diagnostic heterogeneity have not been systematically studied. To address the gap, we evaluate transfer learning approaches for diagnosis-specific mortality prediction and apply both GLM- and XGBoost-based models to the eICU Collaborative Research Database. Our results demonstrate that transfer learning consistently outperforms models trained only on diagnosis-specific data and those using a well-known ICU severity-of-illness score, i.e., APACHE IVa, alone, while also achieving better calibration than models trained on the pooled data. Our findings also suggest that the Youden cutoff is a more appropriate decision threshold than the conventional 0.5 for binary outcomes, and that transfer learning maintains consistently high predictive performance across various cutoff criteria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06511v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengqi Xu, Subha Maity, Joel Dubin</dc:creator>
    </item>
    <item>
      <title>Market Reactions and Information Spillovers in Bank Mergers: A Multi-Method Analysis of the Japanese Banking Sector</title>
      <link>https://arxiv.org/abs/2512.06550</link>
      <description>arXiv:2512.06550v1 Announce Type: cross 
Abstract: Major bank mergers and acquisitions (M&amp;A) transform the financial market structure, but their valuation and spillover effects remain open to question. This study examines the market reaction to two M&amp;A events: the 2005 creation of Mitsubishi UFJ Financial Group following the Financial Big Bang in Japan, and the 2018 merger involving Resona Holdings after the global financial crisis. The multi-method analysis in this research combines several distinct methods to explore these M&amp;A events. An event study using the market model, the capital asset pricing model (CAPM), and the Fama-French three-factor model is implemented to estimate cumulative abnormal returns (CAR) for valuation purposes. Vector autoregression (VAR) models are used to test for Granger causality and map dynamic effects using impulse response functions (IRFs) to investigate spillovers. Propensity score matching (PSM) helps provide a causal estimate of the average treatment effect on the treated (ATT). The analysis detected a significant positive market reaction to the mergers. The findings also suggest the presence of prolonged positive spillovers to other banks, which may indicate a synergistic effect among Japanese banks. Combining these methods provides a unique perspective on M&amp;A events in the Japanese banking sector, offering valuable insights for investors, managers, and regulators concerned with market efficiency and systemic stability</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06550v1</guid>
      <category>q-fin.CP</category>
      <category>econ.EM</category>
      <category>q-fin.PM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haibo Wang, Takeshi Tsuyuguchi</dc:creator>
    </item>
    <item>
      <title>Application of Time-Controlled Critical Point in Pressure Reducing Valves. A Case Study in North Spain</title>
      <link>https://arxiv.org/abs/2512.06735</link>
      <description>arXiv:2512.06735v1 Announce Type: cross 
Abstract: Potable water utilities are currently making great efforts to reduce leakage rates and assure long-term supply to the population due to the challenges of climate change, growing population and water shortage scenarios that have been on them over the last years. One of the most employed methods to reduce leakage includes the installation of pressurereducing valves along the water distribution network and the utilization of pressure management schemes. Pressure management includes different types of control models, which are applied according to the requirements of each site. The most advanced and sophisticated scheme is critical point control, which relies on a flow signal from a measuring device or online communication between the critical point and the valve. This paper proposes the utilization of a seasonal autoregressive integrated moving average, or the SARIMA model, to correlate pressure at the outlet of the valve and pressure on the critical point of the area supplied, aiming to set a fixed pressure in the critical point. The SARIMA model is developed according to historical data logged in the field and then validated. Later, the SARIMA model was tested on a real location in the village of Noja, Spain. The analysis of the field test results prove that the proposed model is feasible to be used since there is no significance difference between the target values set in the critical point and the real values measured in the field. The research proves that the SARIMA model can be used as an alternative for critical point control in water distribution networks when no flow signal is available or when communication between the critical point and the pressure reducing valve is not an option.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06735v1</guid>
      <category>physics.app-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/app13105845</arxiv:DOI>
      <arxiv:journal_reference>Appl. Sci. 2023, 13(10), 5845</arxiv:journal_reference>
      <dc:creator>Andres Ortega-Ballesteros, David Munoz-Rodriguez, Maria-Jesus Aguilera-Urena, Francisco Javier de los Santos-Zarco, Alberto-Jesus Perea-Moreno</dc:creator>
    </item>
    <item>
      <title>Latency-Response Theory Model: Evaluating Large Language Models via Response Accuracy and Chain-of-Thought Length</title>
      <link>https://arxiv.org/abs/2512.07019</link>
      <description>arXiv:2512.07019v1 Announce Type: cross 
Abstract: The proliferation of Large Language Models (LLMs) necessitates valid evaluation methods to provide guidance for both downstream applications and actionable future improvements. The Item Response Theory (IRT) model with Computerized Adaptive Testing has recently emerged as a promising framework for evaluating LLMs via their response accuracy. Beyond simple response accuracy, LLMs' chain of thought (CoT) lengths serve as a vital indicator of their reasoning ability. To leverage the CoT length information to assist the evaluation of LLMs, we propose the Latency-Response Theory (LaRT) model, which jointly models both the response accuracy and CoT length by introducing a key correlation parameter between the latent ability and the latent speed. We derive an efficient stochastic approximation Expectation-Maximization algorithm for parameter estimation. We establish rigorous identifiability results for the latent ability and latent speed parameters to ensure the statistical validity of their estimation. Through both theoretical asymptotic analyses and simulation studies, we demonstrate LaRT's advantages over IRT in terms of superior estimation accuracy and shorter confidence intervals for latent trait estimation. To evaluate LaRT in real data, we collect responses from diverse LLMs on popular benchmark datasets. We find that LaRT yields different LLM rankings than IRT and outperforms IRT across multiple key evaluation metrics including predictive power, item efficiency, ranking validity, and LLM evaluation efficiency. Code and data are available at https://github.com/Toby-X/Latency-Response-Theory-Model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07019v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyu Xu, Jia Liu, Yixin Wang, Yuqi Gu</dc:creator>
    </item>
    <item>
      <title>Parallel Algorithms for Combined Regularized Support Vector Machines: Application in Music Genre Classification</title>
      <link>https://arxiv.org/abs/2512.07463</link>
      <description>arXiv:2512.07463v1 Announce Type: cross 
Abstract: In the era of rapid development of artificial intelligence, its applications span across diverse fields, relying heavily on effective data processing and model optimization. Combined Regularized Support Vector Machines (CR-SVMs) can effectively handle the structural information among data features, but there is a lack of efficient algorithms in distributed-stored big data. To address this issue, we propose a unified optimization framework based on consensus structure. This framework is not only applicable to various loss functions and combined regularization terms but can also be effectively extended to non-convex regularization terms, showing strong scalability. Based on this framework, we develop a distributed parallel alternating direction method of multipliers (ADMM) algorithm to efficiently compute CR-SVMs when data is stored in a distributed manner. To ensure the convergence of the algorithm, we also introduce the Gaussian back-substitution method. Meanwhile, for the integrity of the paper, we introduce a new model, the sparse group lasso support vector machine (SGL-SVM), and apply it to music information retrieval. Theoretical analysis confirms that the computational complexity of the proposed algorithm is not affected by different regularization terms and loss functions, highlighting the universality of the parallel algorithm. Experiments on synthetic and free music archiv datasets demonstrate the reliability, stability, and efficiency of the algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07463v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rongmei Liang, Zizheng Liu, Xiaofei Wu, Jingwen Tu</dc:creator>
    </item>
    <item>
      <title>Symmetric Vaccine Efficacy</title>
      <link>https://arxiv.org/abs/2512.07739</link>
      <description>arXiv:2512.07739v1 Announce Type: cross 
Abstract: Traditional measures of vaccine efficacy (VE) are inherently asymmetric, constrained above by $1$ but unbounded below. As a result, VE estimates and corresponding confidence intervals can extend far below zero, making interpretation difficult and potentially obscuring whether the apparent effect reflects true harm or simply statistical uncertainty. The proposed symmetric vaccine efficacy (SVE) is a bounded and interpretable alternative to VE that maintains desirable statistical properties while resolving these asymmetries. SVE is defined as a symmetric transformation of infection risks, with possible values within $[-1, 1]$, providing a common scale for both beneficial and harmful vaccine effects. This paper describes the relationship between SVE and traditional VE, considers inference about SVE, and illustrates the utility of the proposed measure by reanalyzing data from a randomized trial of a candidate HIV vaccine. Open-source tools for computing estimates of SVE and corresponding confidence intervals are available in R through the sve package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07739v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucy D'Agostino McGowan, Sarah C. Lotspeich, Michael G. Hudgens</dc:creator>
    </item>
    <item>
      <title>A 4% withdrawal rate for American retirement spending, derived from a discrete-time model of stochastic returns on assets and their sample moments</title>
      <link>https://arxiv.org/abs/2508.10273</link>
      <description>arXiv:2508.10273v2 Announce Type: replace 
Abstract: What grounds the rule of thumb that a(n American) retiree can safely withdraw 4% of their initial retirement wealth in their first year of retirement, then increase that rate of consumption with inflation? I address that question with a discrete-time model of returns to a retirement portfolio consumed at a rate that grows by $s$ per period. The model's key parameter is $\gamma$, an $s$-adjusted rate of return to wealth, derived from the first 2-4 moments of the portfolio's probability distribution of returns; for a retirement lasting $t$ periods the model recommends a rate of consumption of $\gamma / (1 - (1 - \gamma)^t)$. Estimation of $\gamma$ (and hence of the implied rate of spending in retirement) reveals that the 4% rule emerges from adjusting high expected rates of return down for: consumption growth, the variance in (and kurtosis of) returns to wealth, the longevity risk of a retiree potentially underestimating $t$, and the inclusion of bonds in retirement portfolios without leverage. The model supports leverage of retirement portfolios dominated by the S&amp;P 500, with leverage ratios $&gt; 1.6$ having been historically optimal under the model's approximations. Historical simulations of 30-year retirements suggest that the model proposes withdrawal rates having roughly even odds of success, that leverage greatly improves those odds for stocks-heavy portfolios, and that investing on margin could have allowed safe withdrawal rates $&gt; 6$% per year.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10273v2</guid>
      <category>stat.AP</category>
      <category>q-fin.PM</category>
      <category>q-fin.ST</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Drew M. Thomas</dc:creator>
    </item>
    <item>
      <title>Multimodal Fusion and Interpretability in Human Activity Recognition: A Reproducible Framework for Sensor-Based Modeling</title>
      <link>https://arxiv.org/abs/2510.22410</link>
      <description>arXiv:2510.22410v2 Announce Type: replace 
Abstract: The research introduces a reproducible framework for transforming raw, heterogeneous sensor streams into aligned, semantically meaningful representations for multimodal human activity recognition. Grounded in the Carnegie Mellon University Multi-Modal Activity Database (CMU-MMAC) database and focused on the naturalistic Subject 07 Brownie session, the study traces the full pipeline from data ingestion to modeling and interpretation. Unlike black box preprocessing, a unified preprocessing workflow is proposed that temporally aligns video, audio, and RFID through resampling, grayscale conversion, sliding-window segmentation, and modality-specific normalization, producing standardized fused tensors suitable for downstream learning. Building on this foundation, the work systematically compares early, late, and hybrid fusion strategies using LSTM-based models implemented with PyTorch and TensorFlow, showing that late fusion consistently achieves the highest validation accuracy, with hybrid fusion outperforming early fusion. To evaluate interpretability and modality contribution, PCA and t-SNE visualizations reveal coherent temporal structure and confirm that the video carries stronger discriminative power than audio, while their combination yields substantial performance gains. Incorporating sparse, asynchronous RFID signals further improves accuracy by over 50% and boosts macro-averaged ROC-AUC, demonstrating the added value of object-interaction cues. Overall, the framework contributes a modular, empirically validated approach to multimodal fusion that links preprocessing design, fusion architecture, and interpretability, offering a transferable template for intelligent systems operating in complex, real-world activity settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22410v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yiyao Yang, Yasemin Gulbahar</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Estimation of High-dimensional Conditional Factor Models</title>
      <link>https://arxiv.org/abs/2209.00391</link>
      <description>arXiv:2209.00391v2 Announce Type: replace-cross 
Abstract: This paper presents a general framework for estimating high-dimensional conditional latent factor models via constrained nuclear norm regularization. We establish large sample properties of the estimators and provide efficient algorithms for their computation. To improve practical applicability, we propose a cross-validation procedure for selecting the regularization parameter. Our framework unifies the estimation of various conditional factor models, enabling the derivation of new asymptotic results while addressing limitations of existing methods, which are often model-specific or restrictive. Empirical analyses of the cross section of individual US stock returns suggest that imposing homogeneity improves the model's out-of-sample predictability, with our new method outperforming existing alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.00391v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qihui Chen</dc:creator>
    </item>
    <item>
      <title>Interpret the estimand framework from a causal inference perspective</title>
      <link>https://arxiv.org/abs/2407.00292</link>
      <description>arXiv:2407.00292v3 Announce Type: replace-cross 
Abstract: The estimand framework proposed by ICH in 2017 has brought fundamental changes in the pharmaceutical industry. It clearly describes how a treatment effect in a clinical question should be precisely defined and estimated, through attributes including treatments, endpoints and intercurrent events. However, ideas around the estimand framework are commonly in text, and different interpretations on this framework may exist. This article aims to interpret the estimand framework through its underlying theories, the causal inference framework based on potential outcomes. The statistical origin and formula of an estimand is given through the causal inference framework, with all attributes translated into statistical terms. We describe how five strategies proposed by ICH to analyze intercurrent events are incorporated in the statistical formula of an estimand, and we also suggest a new strategy to analyze intercurrent events. The roles of target populations and analysis sets in the estimand framework are compared and discussed based on the statistical formula of an estimand. This article recommends continuing studying causal inference theories behind the estimand framework and improving the estimand framework with greater methodological comprehensibility and availability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00292v3</guid>
      <category>stat.OT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jinghong Zeng</dc:creator>
    </item>
    <item>
      <title>"6 choose 4": A framework to understand and facilitate discussion of strategies for overall survival safety monitoring</title>
      <link>https://arxiv.org/abs/2410.04020</link>
      <description>arXiv:2410.04020v3 Announce Type: replace-cross 
Abstract: Advances in anticancer therapies have significantly contributed to declining death rates in certain disease and clinical settings. However, they have also made it difficult to power a clinical trial in these settings with overall survival (OS) as the primary efficacy endpoint. Therefore, two approaches have been recently proposed for the pre-specified analysis of OS as a safety endpoint (Fleming et al., 2024; Rodriguez et al., 2024). In this paper, we provide a simple, unifying framework that includes the aforementioned approaches (and a couple others) as special cases. By highlighting each approach's focus, priority, tolerance for risk, and strengths or challenges for practical implementation, this framework can help to facilitate discussions between stakeholders on "fit-for-purpose OS data collection and assessment of harm" (American Association for Cancer Research, 2024). We apply this framework to a real clinical trial in large B-cell lymphoma to illustrate its application and value. Several recommendations and open questions are also raised.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04020v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Godwin Yung, Kaspar Rufibach, Marcel Wolbers, Mark Yan, Jue Wang</dc:creator>
    </item>
    <item>
      <title>Recorded Versus Synthetic Spectral-compatible Ground Motions: A Comparative Analysis of Structural Seismic Responses</title>
      <link>https://arxiv.org/abs/2502.19549</link>
      <description>arXiv:2502.19549v2 Announce Type: replace-cross 
Abstract: This paper presents a comparative analysis of structural seismic responses under two types of ground motion inputs: (i) synthetic motions generated by stochastic spectral-compatible ground motion models and (ii) recorded motions from an earthquake database. Both ground motion datasets are calibrated to a shared target response spectrum to ensure consistent spectral median, variance, and correlation structure. Five key stochastic response metrics-probability distributions, statistical moments, correlations, tail indices, and variance-based global sensitivity indices-are systematically evaluated for two representative structures: a medium-period building and a limiting case of a long-period tower. The comparison accounts for uncertainties both from ground motion and structural parameters. The results reveal that synthetic motions closely replicate recorded motions in terms of global response behavior-including distributions, mean and variance, correlation structure, and dominant uncertainty sources-indicating their suitability for routine seismic design and parametric studies. However, substantial differences emerge in response extremes for long-period structures, particularly in metrics governed by rare events, such as higher-order moments and tail behavior. These differences, which often exceed 50%, can be attributed to the non-Gaussian features and complex characteristics inherent in recorded motions, which are less pronounced in synthetic datasets. The findings support the use of synthetic ground motions for evaluating global seismic response characteristics, while highlighting their limitations in capturing rare-event behavior and long-period structural dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19549v2</guid>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jungho Kim, Maijia Su, Ziqi Wang, Marco Broccardo</dc:creator>
    </item>
    <item>
      <title>EnScale: Temporally-consistent multivariate generative downscaling via proper scoring rules</title>
      <link>https://arxiv.org/abs/2509.26258</link>
      <description>arXiv:2509.26258v2 Announce Type: replace-cross 
Abstract: The practical use of future climate projections from global circulation models (GCMs) is often limited by their coarse spatial resolution, requiring downscaling to generate high-resolution data. Regional climate models (RCMs) provide this refinement, but are computationally expensive. To address this issue, machine learning models can learn the downscaling function, mapping coarse GCM outputs to high-resolution fields. Among these, generative approaches aim to capture the full conditional distribution of RCM data given coarse-scale GCM data, which is characterized by large variability and thus challenging to model accurately. We introduce EnScale, a generative machine learning framework that emulates the full GCM-to-RCM map by training on multiple pairs of GCM and corresponding RCM data. It first adjusts large-scale mismatches between GCM and coarsened RCM data, followed by a super-resolution step to generate high-resolution fields. Both steps employ generative models optimized with the energy score, a proper scoring rule. Compared to state-of-the-art ML downscaling approaches, our setup reduces computational cost by about one order of magnitude. EnScale jointly emulates multiple variables -- temperature, precipitation, solar radiation, and wind -- spatially consistent over an area in Central Europe. In addition, we propose a variant EnScale-t that enables temporally consistent downscaling. We establish a comprehensive evaluation framework across various categories including calibration, spatial structure, extremes, and multivariate dependencies. Comparison with diverse benchmarks demonstrates EnScale's strong performance and computational efficiency. EnScale offers a promising approach for accurate and temporally consistent RCM emulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26258v2</guid>
      <category>physics.ao-ph</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Maybritt Schillinger, Maxim Samarin, Xinwei Shen, Reto Knutti, Nicolai Meinshausen</dc:creator>
    </item>
    <item>
      <title>Model Monitoring: A General Framework with an Application to Non-life Insurance Pricing</title>
      <link>https://arxiv.org/abs/2510.04556</link>
      <description>arXiv:2510.04556v2 Announce Type: replace-cross 
Abstract: Maintaining the predictive performance of pricing models is challenging when insurance portfolios and data-generating mechanisms evolve over time. Focusing on non-life insurance, we adopt the concept-drift terminology from machine learning and distinguish virtual drift from real concept drift in an actuarial setting. Methodologically, we (i) formalize deviance loss and Murphy's score decomposition to assess global and local auto-calibration; (ii) study the Gini score as a rank-based performance measure, derive its asymptotic distribution, and develop a consistent bootstrap estimator of its asymptotic variance; and (iii) combine these results into a statistically grounded, model-agnostic monitoring framework that integrates a Gini-based ranking drift test with global and local auto-calibration tests. An application to a modified motor insurance portfolio with controlled concept-drift scenarios illustrates how the framework guides decisions on refitting or recalibrating pricing models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04556v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alexej Brauer, Paul Menzel, Mario V. W\"uthrich</dc:creator>
    </item>
    <item>
      <title>Correlation-Weighted Communicability Curvature as a Structural Driver of Dengue Spread: A Bayesian Spatial Analysis of Recife (2015-2024)</title>
      <link>https://arxiv.org/abs/2512.00315</link>
      <description>arXiv:2512.00315v2 Announce Type: replace-cross 
Abstract: We investigate whether the structural connectivity of urban road networks helps explain dengue incidence in Recife, Brazil (2015--2024). For each neighborhood, we compute the average \emph{communicability curvature}, a graph-theoretic measure capturing the ability of a locality to influence others through multiple network paths. We integrate this metric into Negative Binomial models, fixed-effects regressions, SAR/SAC spatial models, and a hierarchical INLA/BYM2 specification. Across all frameworks, curvature is the strongest and most stable predictor of dengue risk. In the BYM2 model, the structured spatial component collapses ($\phi \approx 0$), indicating that functional network connectivity explains nearly all spatial dependence typically attributed to adjacency-based CAR terms. The results show that dengue spread in Recife is driven less by geographic contiguity and more by network-mediated structural flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00315v2</guid>
      <category>physics.soc-ph</category>
      <category>math.PR</category>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc\'ilio Ferreira dos Santos, Cleiton de Lima Ricardo, Andreza dos Santos Rodrigues de Melo</dc:creator>
    </item>
  </channel>
</rss>
