<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Nov 2024 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Object Recognition in Human Computer Interaction:- A Comparative Analysis</title>
      <link>https://arxiv.org/abs/2411.04263</link>
      <description>arXiv:2411.04263v1 Announce Type: new 
Abstract: Human-computer interaction (HCI) has been a widely researched area for many years, with continuous advancements in technology leading to the development of new techniques that change the way we interact with computers. With the recent advent of powerful computers, we recognize human actions and interact accordingly, thus revolutionizing the way we interact with computers. The purpose of this paper is to provide a comparative analysis of various algorithms used for recognizing user faces and gestures in the context of computer vision and HCI. This study aims to explore and evaluate the performance of different algorithms in terms of accuracy, robustness, and efficiency. This study aims to provide a comprehensive analysis of algorithms for face and gesture recognition in the context of computer vision and HCI, with the goal of improving the design and development of interactive systems that are more intuitive, efficient, and user-friendly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04263v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaushik Ranade, Tanmay Khule, Riddhi More</dc:creator>
    </item>
    <item>
      <title>Survival of the Notable: Gender Asymmetry in Wikipedia Collective Deliberations</title>
      <link>https://arxiv.org/abs/2411.04340</link>
      <description>arXiv:2411.04340v1 Announce Type: new 
Abstract: Communities on the web rely on open conversation forums for a number of tasks, including governance, information sharing, and decision making. However these forms of collective deliberation can often result in biased outcomes. A prime example are Articles for Deletion (AfD) discussions on Wikipedia, which allow editors to gauge the notability of existing articles, and that, as prior work has suggested, may play a role in perpetuating the notorious gender gap of Wikipedia. Prior attempts to address this question have been hampered by access to narrow observation windows, reliance on limited subsets of both biographies and editorial outcomes, and by potential confounding factors. To address these limitations, here we adopt a competing risk survival framework to fully situate biographical AfD discussions within the full editorial cycle of Wikipedia content. We find that biographies of women are nominated for deletion faster than those of men, despite editors taking longer to reach a consensus for deletion of women, even after controlling for the size of the discussion. Furthermore, we find that AfDs about historical figures show a strong tendency to result into the redirecting or merging of the biography under discussion into other encyclopedic entries, and that there is a striking gender asymmetry: biographies of women are redirected or merged into biographies of men more often than the other way round. Our study provides a more complete picture of the role of AfD in the gender gap of Wikipedia, with implications for the governance of the open knowledge infrastructure of the web.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04340v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>stat.CO</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Khandaker Tasnim Huq, Giovanni Luca Ciampaglia</dc:creator>
    </item>
    <item>
      <title>An Equitable Experience? How HCI Research Conceptualizes Accessibility of Virtual Reality in the Context of Disability</title>
      <link>https://arxiv.org/abs/2411.04489</link>
      <description>arXiv:2411.04489v1 Announce Type: new 
Abstract: Creating accessible Virtual Reality (VR) is an ongoing concern in the Human-Computer Interaction (HCI) research community. However, there is little reflection on how accessibility should be conceptualized in the context of an experiential technology. We address this gap in our work: We first explore how accessibility is currently defined, highlighting a growing recognition of the importance of equitable and enriching experiences. We then carry out a literature study (N=28) to examine how accessibility and its relationship with experience is currently conceptualized in VR research. Our results show that existing work seldom defines accessibility in the context of VR, and that barrier-centric research is prevalent. Likewise, we show that experience - e.g., that of presence or immersion - is rarely designed for or evaluated, while participant feedback suggests that it is relevant for disabled users of VR. On this basis, we contribute a working definition of VR accessibility that considers experience a necessary condition for equitable access, and discuss the need for future work to focus on experience in the same way as VR research addressing non-disabled persons does.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04489v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kathrin Gerling, Anna-Lena Meiners, Louisa Schumm, Jan Rixen, Marvin Wolf, Zeynep Yildiz, Dmitry Alexandrovsky, Merlin Opp</dc:creator>
    </item>
    <item>
      <title>Memory Remedy: An AI-Enhanced Interactive Story Exploring Human-Robot Interaction and Companionship</title>
      <link>https://arxiv.org/abs/2411.04499</link>
      <description>arXiv:2411.04499v1 Announce Type: new 
Abstract: We present our approach to using AI-generated content (AIGC) and multiple media to develop an immersive, game-based, interactive story experience. The narrative of the story, "Memory Remedy", unfolds through flashbacks, allowing the audience to gradually uncover the story and the complex relationship between the robot protagonist and the older adults. This exploration explores important themes such as the journey of life, the profound influence of memories, and the concept of post-human emotional care. By engaging with this AIGC-based interactive story, audiences are encouraged to reflect on the potential role of robotic companionship in the lives of older adults in the future; and to encourage deeper reflection on the complex relationship between artificial intelligence and humanity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04499v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3678698.3687186</arxiv:DOI>
      <dc:creator>Lei Han, Yu Zhou, Qiongyan Chen, David Yip</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Review of Multimodal XR Applications, Risks, and Ethical Challenges in the Metaverse</title>
      <link>https://arxiv.org/abs/2411.04508</link>
      <description>arXiv:2411.04508v1 Announce Type: new 
Abstract: This scoping review examines the broad applications, risks, and ethical challenges associated with Extended Reality (XR) technologies, including Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality (MR), within the context of Metaverse. XR is revolutionizing fields such as immersive learning in education, medical and professional training, neuropsychological assessment, therapeutic interventions, arts, entertainment, retail, e-commerce, remote work, sports, architecture, urban planning, and cultural heritage preservation. The integration of multimodal technologies such as haptics, eye-tracking, face- and body-tracking, and brain-computer interfaces, enhances user engagement and interactivity, playing a key role in shaping the immersive experiences in the Metaverse. However, XR's expansion raises serious concerns, including data privacy risks, cybersecurity vulnerabilities, cybersickness, addiction, dissociation, harassment, bullying, and misinformation. These psychological, social, and security challenges are further complicated by intense advertising, manipulation of public opinion, and social inequality, which could disproportionately affect vulnerable individuals and social groups. This review emphasizes the urgent need for robust ethical frameworks and regulatory guidelines to address these risks while promoting equitable access, privacy, autonomy, and mental well-being. As XR technologies increasingly integrate with artificial intelligence, responsible governance is essential to ensure the safe and beneficial development of the Metaverse and the broader application of XR in enhancing human development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04508v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/mti8110098</arxiv:DOI>
      <arxiv:journal_reference>Interact. 2024, 8, 98</arxiv:journal_reference>
      <dc:creator>Panagiotis Kourtesis</dc:creator>
    </item>
    <item>
      <title>Emotion Analysis of Social Media Bangla Text and Its Impact on Identifying the Author's Gender</title>
      <link>https://arxiv.org/abs/2411.04524</link>
      <description>arXiv:2411.04524v1 Announce Type: new 
Abstract: The Gender Identification (GI) problem is concerned with determining the gender of the author from a given text. It has numerous applications in different fields like forensics, literature, security, marketing, trade, etc. Due to its importance, researchers have put extensive efforts into identifying gender from the text for different languages. Unfortunately, the same statement is not true for the Bangla language despite its being the 7th most spoken language in the world. In this work, we explore Gender Identification from Social media Bangla Text. Specially, we consider two approaches for feature extraction. The first one is Bag-Of-Words(BOW) approach and another one is based on computing features from sentiment and emotions. There is a common stereotype that female authors write in a more emotional way than male authors. One goal of this work is to validate this stereotype for the Bangla language.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04524v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sultan Ahmed, Salman Rakin, Khadija Urmi, Chandan Kumar Nag, Md. Mostofa Akbar</dc:creator>
    </item>
    <item>
      <title>Exploring the Danmaku Content Moderation on Video-Sharing Platforms: Existing Limitations, Challenges, and Design Opportunities</title>
      <link>https://arxiv.org/abs/2411.04529</link>
      <description>arXiv:2411.04529v1 Announce Type: new 
Abstract: Video-sharing platforms (VSPs) have been increasingly embracing social features such as likes, comments, and Danmaku to boost user engagement. However, viewers may post inappropriate content through video commentary to gain attention or express themselves anonymously and even toxically. For example, on VSPs that support Danmaku, users may even intentionally create a "flood" of Danmaku with inappropriate content shown overlain on videos, disrupting the overall user experience. Despite of the prevalence of inappropriate Danmaku on these VSPs, there is a lack of understanding about the challenges and limitations of Danmaku content moderation on video-sharing platforms. To explore how users perceive the challenges and limitations of current Danmaku moderation methods on VSPs, we conducted probe-based interviews and co-design activities with 21 active end-users. Our findings reveal that the one-size-fits-all rules set by users or customizaibility moderation cannot accurately match the continuous Danmaku. Additionally, the moderation requirements of the Danmaku and the definition of offensive content must dynamically adjust to the video content. Non-intrusive methods should be used to maintain the coherence of the video browsing experience. Our findings inform the design of future Danmaku moderation tools on video-sharing platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04529v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siying Hu, Zhicong Lu</dc:creator>
    </item>
    <item>
      <title>Automatic Identification of Political Hate Articles from Social Media using Recurrent Neural Networks</title>
      <link>https://arxiv.org/abs/2411.04542</link>
      <description>arXiv:2411.04542v1 Announce Type: new 
Abstract: The increasing growth of social media provides us with an instant opportunity to be informed of the opinions of a large number of politically active individuals in real-time. We can get an overall idea of the ideologies of these individuals on governmental issues by analyzing the social media texts. Nowadays, different kinds of news websites and popular social media such as Facebook, YouTube, Instagram, etc. are the most popular means of communication for the mass population. So the political perception of the users toward different parties in the country is reflected in the data collected from these social sites. In this work, we have extracted three types of features, such as the stylometric feature, the word-embedding feature, and the TF-IDF feature. Traditional machine learning classifiers and deep learning models are employed to identify political ideology from the text. We have compared our methodology with the research work in different languages. Among them, the word embedding feature with LSTM outperforms all other models with 88.28% accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04542v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sultan Ahmed, Salman Rakin, Khadija Urmi, Chandan Kumar Nag, Md. Mostofa Akbar</dc:creator>
    </item>
    <item>
      <title>Dynamic-Attention-based EEG State Transition Modeling for Emotion Recognition</title>
      <link>https://arxiv.org/abs/2411.04568</link>
      <description>arXiv:2411.04568v1 Announce Type: new 
Abstract: Electroencephalogram (EEG)-based emotion decoding can objectively quantify people's emotional state and has broad application prospects in human-computer interaction and early detection of emotional disorders. Recently emerging deep learning architectures have significantly improved the performance of EEG emotion decoding. However, existing methods still fall short of fully capturing the complex spatiotemporal dynamics of neural signals, which are crucial for representing emotion processing. This study proposes a Dynamic-Attention-based EEG State Transition (DAEST) modeling method to characterize EEG spatiotemporal dynamics. The model extracts spatiotemporal components of EEG that represent multiple parallel neural processes and estimates dynamic attention weights on these components to capture transitions in brain states. The model is optimized within a contrastive learning framework for cross-subject emotion recognition. The proposed method achieved state-of-the-art performance on three publicly available datasets: FACED, SEED, and SEED-V. It achieved 75.4% accuracy in the binary classification of positive and negative emotions and 59.3% in nine-class discrete emotion classification on the FACED dataset, 88.1% in the three-class classification of positive, negative, and neutral emotions on the SEED dataset, and 73.6% in five-class discrete emotion classification on the SEED-V dataset. The learned EEG spatiotemporal patterns and dynamic transition properties offer valuable insights into neural dynamics underlying emotion processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04568v1</guid>
      <category>cs.HC</category>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xinke Shen, Runmin Gan, Kaixuan Wang, Shuyi Yang, Qingzhu Zhang, Quanying Liu, Dan Zhang, Sen Song</dc:creator>
    </item>
    <item>
      <title>"I Always Felt that Something Was Wrong.": Understanding Compliance Risks and Mitigation Strategies when Professionals Use Large Language Models</title>
      <link>https://arxiv.org/abs/2411.04576</link>
      <description>arXiv:2411.04576v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have been increasingly adopted by professionals for work tasks. However, using LLMs also introduces compliance risks relating to privacy, ethics, and regulations. This study investigated the compliance risks professionals perceive with LLM use and their risk mitigation strategies. Semi-structured interviews were conducted with 24 law, healthcare, and academia professionals. Results showed that the main compliance concerns centered around potential exposure to sensitive customer/patient information through LLMs. To address risks, professionals reported proactively inputting distorted data to preserve privacy. However, full compliance proved challenging, given the complex interactions between user inputs, LLM behaviors, and regulations. This research provides valuable insights into designing LLMs with built-in privacy and risk controls to support professionals' evaluation and adoption of emerging AI technologies while meeting compliance obligations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04576v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siying Hu, Piaohong Wang, Yaxing Yao, Zhicong Lu</dc:creator>
    </item>
    <item>
      <title>CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational Agents in XR</title>
      <link>https://arxiv.org/abs/2411.04671</link>
      <description>arXiv:2411.04671v1 Announce Type: new 
Abstract: Recent developments in computer graphics, machine learning, and sensor technologies enable numerous opportunities for extended reality (XR) setups for everyday life, from skills training to entertainment. With large corporations offering consumer-grade head-mounted displays (HMDs) in an affordable way, it is likely that XR will become pervasive, and HMDs will develop as personal devices like smartphones and tablets. However, having intelligent spaces and naturalistic interactions in XR is as important as technological advances so that users grow their engagement in virtual and augmented spaces. To this end, large language model (LLM)--powered non-player characters (NPCs) with speech-to-text (STT) and text-to-speech (TTS) models bring significant advantages over conventional or pre-scripted NPCs for facilitating more natural conversational user interfaces (CUIs) in XR. In this paper, we provide the community with an open-source, customizable, extensible, and privacy-aware Unity package, CUIfy, that facilitates speech-based NPC-user interaction with various LLMs, STT, and TTS models. Our package also supports multiple LLM-powered NPCs per environment and minimizes the latency between different computational models through streaming to achieve usable interactions between users and NPCs. We publish our source code in the following repository: https://gitlab.lrz.de/hctl/cuify</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04671v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kadir Burak Buldu, S\"uleyman \"Ozdel, Ka Hei Carrie Lau, Mengdi Wang, Daniel Saad, Sofie Sch\"onborn, Auxane Boch, Enkelejda Kasneci, Efe Bozkir</dc:creator>
    </item>
    <item>
      <title>AWARE Narrator and the Utilization of Large Language Models to Extract Behavioral Insights from Smartphone Sensing Data</title>
      <link>https://arxiv.org/abs/2411.04691</link>
      <description>arXiv:2411.04691v1 Announce Type: new 
Abstract: Smartphones, equipped with an array of sensors, have become valuable tools for personal sensing. Particularly in digital health, smartphones facilitate the tracking of health-related behaviors and contexts, contributing significantly to digital phenotyping, a process where data from digital interactions is analyzed to infer behaviors and assess mental health. Traditional methods process raw sensor data into information features for statistical and machine learning analyses. In this paper, we introduce a novel approach that systematically converts smartphone-collected data into structured, chronological narratives. The AWARE Narrator translates quantitative smartphone sensing data into English language descriptions, forming comprehensive narratives of an individual's activities. We apply the framework to the data collected from university students over a week, demonstrating the potential of utilizing the narratives to summarize individual behavior, and analyzing psychological states by leveraging large language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04691v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyi Zhang, Miu Kojima, Simon D'Alfonso</dc:creator>
    </item>
    <item>
      <title>Orbit: A Framework for Designing and Evaluating Multi-objective Rankers</title>
      <link>https://arxiv.org/abs/2411.04798</link>
      <description>arXiv:2411.04798v1 Announce Type: new 
Abstract: Machine learning in production needs to balance multiple objectives: This is particularly evident in ranking or recommendation models, where conflicting objectives such as user engagement, satisfaction, diversity, and novelty must be considered at the same time. However, designing multi-objective rankers is inherently a dynamic wicked problem -- there is no single optimal solution, and the needs evolve over time. Effective design requires collaboration between cross-functional teams and careful analysis of a wide range of information. In this work, we introduce Orbit, a conceptual framework for Objective-centric Ranker Building and Iteration. The framework places objectives at the center of the design process, to serve as boundary objects for communication and guide practitioners for design and evaluation. We implement Orbit as an interactive system, which enables stakeholders to interact with objective spaces directly and supports real-time exploration and evaluation of design trade-offs. We evaluate Orbit through a user study involving twelve industry practitioners, showing that it supports efficient design space exploration, leads to more informed decision-making, and enhances awareness of the inherent trade-offs of multiple objectives. Orbit (1) opens up new opportunities of an objective-centric design process for any multi-objective ML models, as well as (2) sheds light on future designs that push practitioners to go beyond a narrow metric-centric or example-centric mindset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04798v1</guid>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenyang Yang, Tesi Xiao, Michael Shavlovsky, Christian K\"astner, Tongshuang Wu</dc:creator>
    </item>
    <item>
      <title>Understanding Generative AI in Robot Logic Parametrization</title>
      <link>https://arxiv.org/abs/2411.04273</link>
      <description>arXiv:2411.04273v1 Announce Type: cross 
Abstract: Leveraging generative AI (for example, Large Language Models) for language understanding within robotics opens up possibilities for LLM-driven robot end-user development (EUD). Despite the numerous design opportunities it provides, little is understood about how this technology can be utilized when constructing robot program logic. In this paper, we outline the background in capturing natural language end-user intent and summarize previous use cases of LLMs within EUD. Taking the context of filmmaking as an example, we explore how a cinematography practitioner's intent to film a certain scene can be articulated using natural language, captured by an LLM, and further parametrized as low-level robot arm movement. We explore the capabilities of an LLM interpreting end-user intent and mapping natural language to predefined, cross-modal data in the process of iterative program development. We conclude by suggesting future opportunities for domain exploration beyond cinematography to support language-driven robotic camera navigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04273v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuna Hwang, Arissa J. Sato, Pragathi Praveena, Nathan Thomas White, Bilge Mutlu</dc:creator>
    </item>
    <item>
      <title>Multi-Agents are Social Groups: Investigating Social Influence of Multiple Agents in Human-Agent Interactions</title>
      <link>https://arxiv.org/abs/2411.04578</link>
      <description>arXiv:2411.04578v1 Announce Type: cross 
Abstract: Multi-agent systems - systems with multiple independent AI agents working together to achieve a common goal - are becoming increasingly prevalent in daily life. Drawing inspiration from the phenomenon of human group social influence, we investigate whether a group of AI agents can create social pressure on users to agree with them, potentially changing their stance on a topic. We conducted a study in which participants discussed social issues with either a single or multiple AI agents, and where the agents either agreed or disagreed with the user's stance on the topic. We found that conversing with multiple agents (holding conversation content constant) increased the social pressure felt by participants, and caused a greater shift in opinion towards the agents' stances on each topic. Our study shows the potential advantages of multi-agent systems over single-agent platforms in causing opinion change. We discuss design implications for possible multi-agent systems that promote social good, as well as the potential for malicious actors to use these systems to manipulate public opinion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04578v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianqi Song, Yugin Tan, Zicheng Zhu, Yibin Feng, Yi-Chieh Lee</dc:creator>
    </item>
    <item>
      <title>GUI Agents with Foundation Models: A Comprehensive Survey</title>
      <link>https://arxiv.org/abs/2411.04890</link>
      <description>arXiv:2411.04890v1 Announce Type: cross 
Abstract: Recent advances in foundation models, particularly Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs), facilitate intelligent agents being capable of performing complex tasks. By leveraging the ability of (M)LLMs to process and interpret Graphical User Interfaces (GUIs), these agents can autonomously execute user instructions by simulating human-like interactions such as clicking and typing. This survey consolidates recent research on (M)LLM-based GUI agents, highlighting key innovations in data, frameworks, and applications. We begin by discussing representative datasets and benchmarks. Next, we summarize a unified framework that captures the essential components used in prior research, accompanied by a taxonomy. Additionally, we explore commercial applications of (M)LLM-based GUI agents. Drawing from existing work, we identify several key challenges and propose future research directions. We hope this paper will inspire further developments in the field of (M)LLM-based GUI agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04890v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuai Wang, Weiwen Liu, Jingxuan Chen, Weinan Gan, Xingshan Zeng, Shuai Yu, Xinlong Hao, Kun Shao, Yasheng Wang, Ruiming Tang</dc:creator>
    </item>
    <item>
      <title>Public Procurement for Responsible AI? Understanding U.S. Cities' Practices, Challenges, and Needs</title>
      <link>https://arxiv.org/abs/2411.04994</link>
      <description>arXiv:2411.04994v1 Announce Type: cross 
Abstract: Most AI tools adopted by governments are not developed internally, but instead are acquired from third-party vendors in a process called public procurement. While scholars and regulatory proposals have recently turned towards procurement as a site of intervention to encourage responsible AI governance practices, little is known about the practices and needs of city employees in charge of AI procurement. In this paper, we present findings from semi-structured interviews with 18 city employees across 7 US cities. We find that AI acquired by cities often does not go through a conventional public procurement process, posing challenges to oversight and governance. We identify five key types of challenges to leveraging procurement for responsible AI that city employees face when interacting with colleagues, AI vendors, and members of the public. We conclude by discussing recommendations and implications for governments, researchers, and policymakers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04994v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nari Johnson, Elise Silva, Harrison Leon, Motahhare Eslami, Beth Schwanke, Ravit Dotan, Hoda Heidari</dc:creator>
    </item>
    <item>
      <title>CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract Patients</title>
      <link>https://arxiv.org/abs/2402.04620</link>
      <description>arXiv:2402.04620v4 Announce Type: replace 
Abstract: The healthcare landscape is evolving, with patients seeking reliable information about their health conditions and available treatment options. Despite the abundance of information sources, the digital age overwhelms individuals with excess, often inaccurate information. Patients primarily trust medical professionals, highlighting the need for expert-endorsed health information. However, increased patient loads on experts has led to reduced communication time, impacting information sharing. To address this gap, we developed CataractBot, an experts-in-the-loop chatbot powered by LLMs, in collaboration with an eye hospital in India. CataractBot answers cataract surgery related questions instantly by querying a curated knowledge base and provides expert-verified responses asynchronously. It has multimodal and multilingual capabilities. In an in-the-wild deployment study with 55 participants, CataractBot proved valuable, providing anytime accessibility, saving time, accommodating diverse literacy levels, alleviating power differences, and adding a privacy layer between patients and doctors. Users reported that their trust in the system was established through expert verification. Broadly, our results could inform future work on designing expert-mediated LLM bots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04620v4</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Pragnya Ramjee, Bhuvan Sachdeva, Satvik Golechha, Shreyas Kulkarni, Geeta Fulari, Kaushik Murali, Mohit Jain</dc:creator>
    </item>
    <item>
      <title>RemixTape: Enriching Narratives about Metrics with Semantic Alignment and Contextual Recommendation</title>
      <link>https://arxiv.org/abs/2406.03415</link>
      <description>arXiv:2406.03415v2 Announce Type: replace 
Abstract: The temporal dynamics of quantitative metrics or key performance indicators (KPIs) are central to decision-making in enterprise organizations. Recently, major business intelligence providers have introduced new infrastructure for defining, sharing, and monitoring metric values. However, these values are often presented in isolation and appropriate context is seldom externalized. In this design study, we present REMIXTAPE, an application for constructing structured narratives around metrics. With design imperatives grounded in prior work and a formative interview study, REMIXTAPE provides a hierarchical canvas for collecting and coordinating sequences of line chart representations of metrics, along with the ability to externalize situational context around them. REMIXTAPE includes affordances to semantically align and annotate juxtaposed charts and text, as well as recommendations of complementary charts based on metrics already present on the canvas. We evaluated REMIXTAPE in a study in which six enterprise data professionals reproduced and extended partial narratives. They appreciated REMIXTAPE as a novel alternative to dashboards, galleries, and slide presentations for supporting conversations about metrics. We conclude with a reflection on our design choices and process, with a call to define a conceptual foundation for remixing in the context of visualization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03415v2</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Brehmer, Margaret Drouhard, Arjun Srinivasan</dc:creator>
    </item>
    <item>
      <title>ChartifyText: Automated Chart Generation from Data-Involved Texts via LLM</title>
      <link>https://arxiv.org/abs/2410.14331</link>
      <description>arXiv:2410.14331v2 Announce Type: replace 
Abstract: Text documents with numerical values involved are widely used in various applications such as scientific research, economy, public health and journalism. However, it is difficult for readers to quickly interpret such data-involved texts and gain deep insights. To fill this research gap, this work aims to automatically generate charts to accurately convey the underlying data and ideas to readers, which is essentially a challenging task. The challenges originate from text ambiguities, intrinsic sparsity and uncertainty of data in text documents, and subjective sentiment differences. Specifically, we propose ChartifyText, a novel fully-automated approach that leverages Large Language Models (LLMs) to convert complex data-involved texts to expressive charts. It consists of two major modules: tabular data inference and expressive chart generation. The tabular data inference module employs systematic prompt engineering to guide the LLM (e.g., GPT-4) to infer table data, where data ranges, uncertainties, missing data values and corresponding subjective sentiments are explicitly considered. The expressive chart generation module augments standard charts with intuitive visual encodings and concise texts to accurately convey the underlying data and insights. We extensively evaluate the effectiveness of ChartifyText on real-world data-involved text documents through case studies, in-depth interviews with three visualization experts, and a carefully-designed user study with 15 participants. The results demonstrate the usefulness and effectiveness of ChartifyText in helping readers efficiently and effectively make sense of data-involved texts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14331v2</guid>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Songheng Zhang, Lei Wang, Toby Jia-Jun Li, Qiaomu Shen, Yixin Cao, Yong Wang</dc:creator>
    </item>
    <item>
      <title>A New Heuristic Algorithm for Balanced Deliberation Groups</title>
      <link>https://arxiv.org/abs/2410.21451</link>
      <description>arXiv:2410.21451v2 Announce Type: replace 
Abstract: We here present an improved version of the Sortition Foundation's GROUPSELECT software package, which aims to repeatedly allocate participants of a deliberative process to discussion groups in a way that balances demographics in each group and maximises distinct meetings over time. Our result, DREAM, significantly outperforms the prior algorithmic approach LEGACY. We also add functionalities to the GROUPSELECT software to help the end user. The GROUPOPT algorithm utilises random shuffles and Pareto swaps to find a locally optimal solution that maximises demographic balance and minimises the number of pairwise previous meetings, with the relative importance of these two metrics defined by the user.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21451v2</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jake Barrett, Philipp C Verpoort, Kobi Gal</dc:creator>
    </item>
    <item>
      <title>SplatOverflow: Asynchronous Hardware Troubleshooting</title>
      <link>https://arxiv.org/abs/2411.02332</link>
      <description>arXiv:2411.02332v2 Announce Type: replace 
Abstract: As tools for designing and manufacturing hardware become more accessible, smaller producers can develop and distribute novel hardware. However, there aren't established tools to support end-user hardware troubleshooting or routine maintenance. As a result, technical support for hardware remains ad-hoc and challenging to scale. Inspired by software troubleshooting workflows like StackOverflow, we propose a workflow for asynchronous hardware troubleshooting: SplatOverflow. SplatOverflow creates a novel boundary object, the SplatOverflow scene, that users reference to communicate about hardware. The scene comprises a 3D Gaussian Splat of the user's hardware registered onto the hardware's CAD model. The splat captures the current state of the hardware, and the registered CAD model acts as a referential anchor for troubleshooting instructions. With SplatOverflow, maintainers can directly address issues and author instructions in the user's workspace. The instructions define workflows that can easily be shared between users and recontextualized in new environments. In this paper, we describe the design of SplatOverflow, detail the workflows it enables, and illustrate its utility to different kinds of users. We also validate that non-experts can use SplatOverflow to troubleshoot common problems with a 3D printer in a user study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02332v2</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Amritansh Kwatra, Tobias Wienberg, Ilan Mandel, Ritik Batra, Peter He, Francois Guimbretiere, Thijs Roumen</dc:creator>
    </item>
    <item>
      <title>C3T: Cross-modal Transfer Through Time for Human Action Recognition</title>
      <link>https://arxiv.org/abs/2407.16803</link>
      <description>arXiv:2407.16803v2 Announce Type: replace-cross 
Abstract: In order to unlock the potential of diverse sensors, we investigate a method to transfer knowledge between modalities using the structure of a unified multimodal representation space for Human Action Recognition (HAR). We formalize and explore an understudied cross-modal transfer setting we term Unsupervised Modality Adaptation (UMA), where the modality used in testing is not used in supervised training, i.e. zero labeled instances of the test modality are available during training. We develop three methods to perform UMA: Student-Teacher (ST), Contrastive Alignment (CA), and Cross-modal Transfer Through Time (C3T). Our extensive experiments on various camera+IMU datasets compare these methods to each other in the UMA setting, and to their empirical upper bound in the supervised setting. The results indicate C3T is the most robust and highest performing by at least a margin of 8%, and nears the supervised setting performance even in the presence of temporal noise. This method introduces a novel mechanism for aligning signals across time-varying latent vectors, extracted from the receptive field of temporal convolutions. Our findings suggest that C3T has significant potential for developing generalizable models for time-series sensor data, opening new avenues for multi-modal learning in various applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16803v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Abhi Kamboj, Anh Duy Nguyen, Minh Do</dc:creator>
    </item>
    <item>
      <title>What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations</title>
      <link>https://arxiv.org/abs/2409.02449</link>
      <description>arXiv:2409.02449v3 Announce Type: replace-cross 
Abstract: This paper explores the pitfalls in evaluating multilingual automatic speech recognition (ASR) models, with a particular focus on Indic language scripts. We investigate the text normalization routine employed by leading ASR models, including OpenAI Whisper, Meta's MMS, Seamless, and Assembly AI's Conformer, and their unintended consequences on performance metrics. Our research reveals that current text normalization practices, while aiming to standardize ASR outputs for fair comparison, by removing inconsistencies such as variations in spelling, punctuation, and special characters, are fundamentally flawed when applied to Indic scripts. Through empirical analysis using text similarity scores and in-depth linguistic examination, we demonstrate that these flaws lead to artificially improved performance metrics for Indic languages. We conclude by proposing a shift towards developing text normalization routines that leverage native linguistic expertise, ensuring more robust and accurate evaluations of multilingual ASR models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02449v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kavya Manohar, Leena G Pillai, Elizabeth Sherly</dc:creator>
    </item>
    <item>
      <title>Learning to Assist Humans without Inferring Rewards</title>
      <link>https://arxiv.org/abs/2411.02623</link>
      <description>arXiv:2411.02623v2 Announce Type: replace-cross 
Abstract: Assistive agents should make humans' lives easier. Classically, such assistance is studied through the lens of inverse reinforcement learning, where an assistive agent (e.g., a chatbot, a robot) infers a human's intention and then selects actions to help the human reach that goal. This approach requires inferring intentions, which can be difficult in high-dimensional settings. We build upon prior work that studies assistance through the lens of empowerment: an assistive agent aims to maximize the influence of the human's actions such that they exert a greater control over the environmental outcomes and can solve tasks in fewer steps. We lift the major limitation of prior work in this area--scalability to high-dimensional settings--with contrastive successor representations. We formally prove that these representations estimate a similar notion of empowerment to that studied by prior work and provide a ready-made mechanism for optimizing it. Empirically, our proposed method outperforms prior methods on synthetic benchmarks, and scales to Overcooked, a cooperative game setting. Theoretically, our work connects ideas from information theory, neuroscience, and reinforcement learning, and charts a path for representations to play a critical role in solving assistive problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02623v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vivek Myers, Evan Ellis, Sergey Levine, Benjamin Eysenbach, Anca Dragan</dc:creator>
    </item>
    <item>
      <title>Taming Toxicity or Fueling It? The Great Ban`s Role in Shifting Toxic User Behavior and Engagement</title>
      <link>https://arxiv.org/abs/2411.04037</link>
      <description>arXiv:2411.04037v2 Announce Type: replace-cross 
Abstract: In today's online environments users experience harm and abuse on a daily basis. Therefore, content moderation is crucial to ensure their safety and well-being. However, the effectiveness of many moderation interventions is still uncertain. We evaluate the effectiveness of The Great Ban, one of the largest deplatforming interventions carried out by Reddit that affected almost 2,000 communities. We analyze 53M comments shared by nearly 34K users, providing in-depth results on both the intended and unintended consequences of this ban. We found that 15.6% of the moderated users abandoned the platform while the remaining ones decreased their overall toxicity by 4.1%. Nonetheless, a subset of those users increased their toxicity by 70% after the intervention. In any case, increases in toxicity did not lead to marked increases in activity or engagement, meaning that the most toxic users had overall a limited impact. Our findings bring to light new insights on the effectiveness of deplatforming. Furthermore, they also contribute to informing future content moderation strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04037v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Cima, Benedetta Tessa, Stefano Cresci, Amaury Trujillo, Marco Avvenuti</dc:creator>
    </item>
    <item>
      <title>A Collaborative Content Moderation Framework for Toxicity Detection based on Conformalized Estimates of Annotation Disagreement</title>
      <link>https://arxiv.org/abs/2411.04090</link>
      <description>arXiv:2411.04090v2 Announce Type: replace-cross 
Abstract: Content moderation typically combines the efforts of human moderators and machine learning models. However, these systems often rely on data where significant disagreement occurs during moderation, reflecting the subjective nature of toxicity perception. Rather than dismissing this disagreement as noise, we interpret it as a valuable signal that highlights the inherent ambiguity of the content,an insight missed when only the majority label is considered. In this work, we introduce a novel content moderation framework that emphasizes the importance of capturing annotation disagreement. Our approach uses multitask learning, where toxicity classification serves as the primary task and annotation disagreement is addressed as an auxiliary task. Additionally, we leverage uncertainty estimation techniques, specifically Conformal Prediction, to account for both the ambiguity in comment annotations and the model's inherent uncertainty in predicting toxicity and disagreement.The framework also allows moderators to adjust thresholds for annotation disagreement, offering flexibility in determining when ambiguity should trigger a review. We demonstrate that our joint approach enhances model performance, calibration, and uncertainty estimation, while offering greater parameter efficiency and improving the review process in comparison to single-task methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04090v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guillermo Villate-Castillo, Javier Del Ser, Borja Sanz</dc:creator>
    </item>
  </channel>
</rss>
