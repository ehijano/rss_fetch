<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 04 Aug 2025 04:00:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>E2ATST: A Temporal-Spatial Optimized Energy-Efficient Architecture for Training Spiking Transformer</title>
      <link>https://arxiv.org/abs/2508.00475</link>
      <description>arXiv:2508.00475v1 Announce Type: new 
Abstract: (1) Pengcheng Laboratory, (2) Southern University of Science and Technology, (3) Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, (4) University of Chinese Academy of Sciences</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00475v1</guid>
      <category>cs.AR</category>
      <category>cs.NE</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunhao Ma, Yanyu Lin, Mingjing Li, Puli Quan, Chenlin Zhou, Wenyue Zhang, Zhiwei Zhong, Wanyi Jia, Xueke Zhu, Qingyan Meng, Huihui Zhou, Fengwei An</dc:creator>
    </item>
    <item>
      <title>Generative Logic: A New Computer Architecture for Deterministic Reasoning and Knowledge Generation</title>
      <link>https://arxiv.org/abs/2508.00017</link>
      <description>arXiv:2508.00017v1 Announce Type: cross 
Abstract: We present Generative Logic (GL), a deterministic architecture that begins from user-supplied axiomatic definitions -- written in a minimalist Mathematical Programming Language (MPL) -- and systematically explores their deductive neighborhood. Definitions are compiled into a distributed grid of simple Logic Blocks (LBs) that exchange messages; any time several expressions unify under an inference rule, a new fact is emitted with full provenance to its sources, yielding replayable, auditable proof graphs.
  A prototype software implementation instantiates the workflow on first-order Peano arithmetic. Starting only from the Peano axioms, GL enumerates candidate implications, applies normalization and type filters, and automatically reconstructs machine-checkable proofs of foundational arithmetic laws including associativity and commutativity of addition, associativity and commutativity of multiplication, and distributivity. Generated proofs export to navigable HTML so that every inference step can be inspected independently.
  We outline a hardware-software co-design path toward massively parallel realizations and describe prospective integration with probabilistic models (e.g., Large Language Models (LLMs)) for autoformalization and conjecture seeding. The Python and MPL code to reproduce the Peano experiments, along with the full HTML proof graphs, are available in the project's GitHub repository at https://github.com/Generative-Logic/GL/tree/35a111ea9ba53afe051703d6050be0c3923e9724 and are permanently archived at https://doi.org/10.5281/zenodo.16408441. We invite community feedback and collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00017v1</guid>
      <category>cs.LO</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikolai Sergeev</dc:creator>
    </item>
    <item>
      <title>Reimagining Voltage-Controlled Cryogenic Boolean Logic Paradigm with Quantum-Enhanced Josephson Junction FETs</title>
      <link>https://arxiv.org/abs/2508.00295</link>
      <description>arXiv:2508.00295v1 Announce Type: cross 
Abstract: The growing demand for ultra low power computing and the emergence of quantum technologies have intensified interest in cryogenic electronics, particularly superconducting devices.Despite their promise, current controlled superconducting components face fundamental challenges in cascadability, limiting their effectiveness in complex logic architectures.To overcome this, recent efforts have focused on developing gate tunable superconducting devices, such as Josephson Junction Field Effect Transistors (JJFETs).However, achieving robust control and sufficient supercurrent gain, both critical for transistor-like performance in logic circuits remains a key challenge.A recent advancement in JJFET design, based on InAs and GaSb heterostructures, demonstrates enhanced gain and favorable device characteristics suitable for circuit integration.Building on this innovation, we propose and analyze fundamental voltage controlled logic topologies using the quantum enhanced JJFET. We develop a Verilog A based circuit compatible compact model of the quantum enhanced JJFET which accurately captures the experimentally observed device characteristics.To ensure cascadability, our logic circuits incorporate the multilayered Heater Nanocryotron (nTron), a superconducting nanowire-based thermal switch.Through simulation based analysis, we demonstrate the successful implementation of fundamental logic gates, including NOT, NAND, and NOR. Furthermore, we design a 3 input majority gate, which plays a pivotal role in quantum and reversible computing due to its universality.Finally, to demonstrate the cascadability of our proposed logic topology, we demonstrate the operation of a 2 input XOR gate based on our designed JJFET based NOT, NAND, and NOR gate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00295v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <category>physics.app-ph</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Mazharul Islam, Diego Ferrer, Shamiul Alam, Juan P. Mendez, Denis Mamaluy, Wei Pan, Ahmedullah Aziz</dc:creator>
    </item>
    <item>
      <title>DGEMM without FP64 Arithmetic -- using FP64 Emulation and FP8 Tensor Cores with Ozaki Scheme</title>
      <link>https://arxiv.org/abs/2508.00441</link>
      <description>arXiv:2508.00441v1 Announce Type: cross 
Abstract: Since AI computations require low-precision matrix multiplications, processors with enhanced performance for these operations are increasing along with the growing demand for AI computations. However, it is difficult to use these operations directly for scientific computations. The Ozaki scheme, an accurate matrix multiplication method proposed by Ozaki et al. in 2012, enables FP64 matrix multiplication (DGEMM) using low-precision floating-point operations such as FP16. The method was subsequently extended to utilize integer arithmetic. The use of integer operations reduces computational cost compared to the floating-point based approach. It has also demonstrated higher performance than hardware FP64 operations on GPUs with fast INT8 Tensor Cores for AI workloads. However, the latest hardware tends to enhance low-precision floating-point operation performance such as FP8 instead of INT8. This study revisits the utilization of low-precision floating-point operations in the Ozaki scheme, considering the latest AI hardware. Specifically, we consider the use of FP6 and FP8 Tensor Cores. Moreover, for processors that support very slow FP64 operations or do not support them at all, we consider the use of the FP64 emulation based on integer arithmetic. We also examine a new blocking strategy. We demonstrate the effectiveness of these methods by evaluating the performance of DGEMM using FP8 Tensor Cores and FP64 emulation on a Blackwell architecture GPU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00441v1</guid>
      <category>cs.PF</category>
      <category>cs.AR</category>
      <category>cs.MS</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daichi Mukunoki</dc:creator>
    </item>
    <item>
      <title>ChatModel: Automating Reference Model Design and Verification with LLMs</title>
      <link>https://arxiv.org/abs/2506.15066</link>
      <description>arXiv:2506.15066v3 Announce Type: replace 
Abstract: As the complexity of integrated circuit designs continues to escalate, the functional verification becomes increasingly challenging. Reference models, critical for accelerating the verification process, are themselves becoming more intricate and time-consuming to develop. Despite the promise shown by large language models (LLMs) in code programming, effectively generating complex reference models remains a significant hurdle. To address these challenges, we introduce ChatModel, the first LLM-aided agile reference model generation and verification platform. ChatModel streamlines the transition from design specifications to fully functional reference models by integrating design standardization and hierarchical agile modeling. Employing a building-block generation strategy, it not only enhances the design capabilities of LLMs for reference models but also significantly boosts verification efficiency. We evaluated ChatModel on 300 designs of varying complexity, demonstrating substantial improvements in both efficiency and quality of reference model generation. ChatModel achieved a peak performance improvement of 55.02% compared to alternative methods, with notable enhancements in generation stability, and delivered a 9.18x increase in its capacity to produce reference model designs. Furthermore, it accelerated the iterative process of reference model design and validation by an average of 5.90x compared to traditional approaches. These results highlight the potential of ChatModel to significantly advance the automation of reference model generation and validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15066v3</guid>
      <category>cs.AR</category>
      <category>cs.MA</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianmin Ye, Tianyang Liu, Qi Tian, Shengchu Su, Zhe Jiang, Xi Wang</dc:creator>
    </item>
    <item>
      <title>Nonlinear Computation with Linear Optics via Source-Position Encoding</title>
      <link>https://arxiv.org/abs/2504.20401</link>
      <description>arXiv:2504.20401v2 Announce Type: replace-cross 
Abstract: Optical computing systems provide an alternate hardware model which appears to be aligned with the demands of neural network workloads. However, the challenge of implementing energy efficient nonlinearities in optics -- a key requirement for realizing neural networks -- is a conspicuous missing link. In this work we introduce a novel method to achieve nonlinear computation in fully linear media. Our method can operate at low power and requires only the ability to drive the optical system at a data-dependent spatial position. Leveraging this positional encoding, we formulate a fully automated, topology-optimization-based hardware design framework for extremely specialized optical neural networks, drawing on modern advancements in optimization and machine learning. We evaluate our optical designs on machine learning classification tasks: demonstrating significant improvements over linear methods, and competitive performance when compared to standard artificial neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20401v2</guid>
      <category>physics.optics</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Mon, 04 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>N. Richardson, C. Bosch, R. P. Adams</dc:creator>
    </item>
  </channel>
</rss>
