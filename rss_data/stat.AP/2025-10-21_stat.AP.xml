<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 21 Oct 2025 04:01:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Cultural Mapping and Pattern Analysis (CMAP) Visualization Toolkit: Open Source Text Analysis for Qualitative and Computational Social Science</title>
      <link>https://arxiv.org/abs/2510.16140</link>
      <description>arXiv:2510.16140v1 Announce Type: new 
Abstract: The CMAP (cultural mapping and pattern analysis) visualization toolkit introduced in this paper is an open-source suite for analyzing and visualizing text data - from qualitative fieldnotes and in-depth interview transcripts to historical documents and web-scaped data like message board posts or blogs. The toolkit is designed for scholars integrating pattern analysis, data visualization, and explanation in qualitative and/or computational social science (CSS). Despite the existence of off-the-shelf commercial qualitative data analysis software, there is a dearth of highly scalable open source options that can work with large data sets, and allow advanced statistical and language modeling. The foundation of the toolkit is a pragmatic approach that aligns research tools with social science project goals- empirical explanation, theory-guided measurement, comparative design, or evidence-based recommendations- guided by the principle that research paradigm and questions should determine methods. Consequently, the CMAP visualization toolkit offers a range of possibilities through the adjustment of relatively small number of parameters, and allows integration with other python tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16140v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Corey M. Abramson (Victoria),  Yuhan (Victoria),  Nian</dc:creator>
    </item>
    <item>
      <title>COWs and their Hybrids: A Statistical View of Custom Orthogonal Weights</title>
      <link>https://arxiv.org/abs/2510.16174</link>
      <description>arXiv:2510.16174v1 Announce Type: new 
Abstract: A recurring challenge in high energy physics is inference of the signal component from a distribution for which observations are assumed to be a mixture of signal and background events. A standard assumption is that there exists information encoded in a discriminant variable that is effective at separating signal and background. This can be used to assign a signal weight to each event, with these weights used in subsequent analyses of one or more control variables of interest. The custom orthogonal weights (COWs) approach of Dembinski, et al.(2022), a generalization of the sPlot approach of Barlow (1987) and Pivk and Le Diberder (2005), is tailored to address this objective. The problem, and this method, present interesting and novel statistical issues. Here we formalize the assumptions needed and the statistical properties, while also considering extensions and alternative approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16174v1</guid>
      <category>stat.AP</category>
      <category>hep-ex</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chad Schafer, Larry Wasserman, Mikael Kuusela</dc:creator>
    </item>
    <item>
      <title>Estimating Time-Varying Epidemic Severity Rates with Adaptive Deconvolution</title>
      <link>https://arxiv.org/abs/2510.16180</link>
      <description>arXiv:2510.16180v1 Announce Type: new 
Abstract: Several key metrics in public health convey the probability that a primary event will lead to a more serious secondary event in the future. These "severity rates" can change over the course of an epidemic in response to shifting conditions like new therapeutics, variants, or public health interventions. In practice, time-varying parameters such as the case-fatality rate are typically estimated from aggregate count data. Prior work has demonstrated that commonly-used ratio-based estimators can be highly biased, motivating the development of new methods. In this paper, we develop an adaptive deconvolution approach based on approximating a Poisson-binomial model for secondary events, and we regularize the maximum likelihood solution in this model with a trend filtering penalty to produce smooth but locally adaptive estimates of severity rates over time. This enables us to compute severity rates both retrospectively and in real time. Experiments based on COVID-19 death and hospitalization data, both real and simulated, demonstrate that our deconvolution estimator is generally more accurate than the standard ratio-based methods, and displays reasonable robustness to model misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16180v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeremy Goldwasser, Addison J. Hu, Alyssa Bilinski, Daniel J. McDonald, Ryan J. Tibshirani</dc:creator>
    </item>
    <item>
      <title>A Compositional Approach to Modelling Cause-specific Mortality with Zero Counts</title>
      <link>https://arxiv.org/abs/2510.16244</link>
      <description>arXiv:2510.16244v1 Announce Type: new 
Abstract: Understanding and forecasting mortality by cause is an essential branch of actuarial science, with wide-ranging implications for decision-makers in public policy and industry. To accurately capture trends in cause-specific mortality, it is critical to consider dependencies between causes of death and produce forecasts by age and cause coherent with aggregate mortality forecasts. One way to achieve these aims is to model cause-specific deaths using compositional data analysis (CODA), treating the density of deaths by age and cause as a set of dependent, non-negative values that sum to one. A major drawback of standard CODA methods is the challenge of zero values, which frequently occur in cause-of-death mortality modelling. Thus, we propose using a compositional power transformation, the $\alpha$-transformation, to model cause-specific life-table death counts. The $\alpha$-transformation offers a statistically rigorous approach to handling zero value subgroups in CODA compared to \emph{ad-hoc} techniques: adding an arbitrarily small amount. We illustrate the $\alpha$-transformation on England and Wales, and US death counts by cause from the Human Cause-of-Death database, for cardiovascular-related causes of death. Results demonstrate the $\alpha$-transformation improves forecast accuracy of cause-specific life-table death counts compared with log-ratio-based CODA transformations. The forecasts suggest declines in proportions of deaths from major cardiovascular causes (myocardial infarction and other ischemic heart diseases (IHD)).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16244v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhe Michelle Dong, Han Lin Shang, Francis Hui, Aaron Bruhn</dc:creator>
    </item>
    <item>
      <title>Mortality Modeling and Forecasting with the Actuaries Climate Index</title>
      <link>https://arxiv.org/abs/2510.16266</link>
      <description>arXiv:2510.16266v1 Announce Type: new 
Abstract: Climate change poses increasing challenges for mortality modeling and underscores the need to integrate climate-related variables into mortality forecasting. This study introduces a two-step approach that incorporates climate information from the Actuaries Climate Index (ACI) into mortality models. In the first step, we model region-specific seasonal mortality dynamics using the Lee-Carter model with SARIMA processes, a cosine-sine decomposition, and a cyclic spline-based function. In the second step, residual deviations from the baseline model are explained by ACI components using Generalized Linear Models, Generalized Additive Models, and Extreme Gradient Boosting. To further capture the dependence between mortality and climate, we develop a SARIMA-Copula forecasting approach linking mortality period effects with temperature extremes. Our results show that incorporating ACI components systematically enhances out-of-sample accuracy, underscoring the value of integrating climate-related variables into stochastic mortality modeling. The proposed framework offers actuaries and policymakers a practical tool for anticipating and managing climate-related mortality risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16266v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karim Barigou, Melanie Patten, Kenneth Q. Zhou</dc:creator>
    </item>
    <item>
      <title>Synergizing chemical and AI communities for advancing laboratories of the future</title>
      <link>https://arxiv.org/abs/2510.16293</link>
      <description>arXiv:2510.16293v1 Announce Type: new 
Abstract: The development of automated experimental facilities and the digitization of experimental data have introduced numerous opportunities to radically advance chemical laboratories. As many laboratory tasks involve predicting and understanding previously unknown chemical relationships, machine learning (ML) approaches trained on experimental data can substantially accelerate the conventional design-build-test-learn process. This outlook article aims to help chemists understand and begin to adopt ML predictive models for a variety of laboratory tasks, including experimental design, synthesis optimization, and materials characterization. Furthermore, this article introduces how artificial intelligence (AI) agents based on large language models can help researchers acquire background knowledge in chemical or data science and accelerate various aspects of the discovery process. We present three case studies in distinct areas to illustrate how ML models and AI agents can be leveraged to reduce time-consuming experiments and manual data analysis. Finally, we highlight existing challenges that require continued synergistic effort from both experimental and computational communities to address.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16293v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saejin Oh, Xinyi Fang, I-Hsin Lin, Paris Dee, Christopher S. Dunham, Stacy M. Copp, Abigail G. Doyle, Javier Read de Alaniz, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>A hierarchical Bayesian approach for population-based structural health monitoring in ship hull structures</title>
      <link>https://arxiv.org/abs/2510.16316</link>
      <description>arXiv:2510.16316v1 Announce Type: new 
Abstract: Structural health monitoring (SHM) strategies involve the processing of structural response data to indirectly assess an asset's condition. These strategies can be enhanced for a group of structures, especially when they are similar, since mutual underlying physics are expected to exist. The concept behind population-based SHM exploits the sharing of data among individuals, so that data-rich members can support data-scarce ones. One approach to population-level modeling is the hierarchical Bayesian method, where the model is structured hierarchically in terms of its parameters, and correlation among learning tasks is enabled by conditioning on shared latent variables.
  This work investigates the application of a hierarchical Bayesian model to infer expected distributions of deflection amplitudes at both the population and domain levels, with the aim of detecting excessive initial deflections in a population of plate elements. Although these damages are typically localized, they can trigger unexpected events, if not properly monitored. The work is conducted in a numerical setting using a Finite Element model to generate strain response data, which serve as the monitoring data. Bayesian inference was conducted using Markov Chain Monte Carlo (MCMC), with a surrogate model employed to calculate the likelihood function. The hierarchical approach was compared to an independent model for a plate component with few data. The results revealed that, under data sparsity conditions, the hierarchical model can offer more robust results in terms of uncertainty, which is essential for decision-making tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16316v1</guid>
      <category>stat.AP</category>
      <category>cs.CE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Georgios Aravanis (Politecnico di Milano, Italy), Nicholas Silionis (National Technical University of Athens, Greece), Jacopo Bardiani (Politecnico di Milano, Italy), Marco Giglio (Politecnico di Milano, Italy), Konstantinos Anyfantis (National Technical University of Athens, Greece), Claudio Sbarufatti (Politecnico di Milano, Italy)</dc:creator>
    </item>
    <item>
      <title>Time-Varying Confounding Bias in Observational Geoscience with Application to Induced Seismicity</title>
      <link>https://arxiv.org/abs/2510.16360</link>
      <description>arXiv:2510.16360v1 Announce Type: new 
Abstract: Evidence derived primarily from physical models has identified saltwater disposal as the dominant causal factor that contributes to induced seismicity. To complement physical models, statistical/machine learning (ML) models are designed to measure associations from observational data, either with parametric regression models or more flexible ML models. However, it is often difficult to interpret the statistical significance of a parameter or the predicative power of a model as evidence of causation. We adapt a causal inference framework with the potential outcomes perspective to explicitly define what we meant by causal effect and declare necessary identification conditions to recover unbiased causal effect estimates. In particular, we illustrate the threat of time-varying confounding in observational longitudinal geoscience data through simulations and adapt established statistical methods for longitudinal analysis from the causal interference literature to estimate the effect of wastewater disposal on earthquakes in the Fort-Worth Basin of North Central Texas from 2013 to 2016.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16360v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuchen Xiao, Corwin Zigler, Peter H. Hennings, Alexandros Savvaidis</dc:creator>
    </item>
    <item>
      <title>Bayesian reliability acceptance sampling plans for competing risks data under interval censoring</title>
      <link>https://arxiv.org/abs/2510.16740</link>
      <description>arXiv:2510.16740v1 Announce Type: new 
Abstract: We obtain a reliability acceptance sampling plan for independent competing risk data under interval censoring schemes using the Bayesian approach. At first, the Bayesian reliability acceptance sampling plan is obtained where the decision criteria of accepting a lot is pre-fixed. For large samples, computing Bayes risk is computationally intensive. Therefore, an approximate Bayes risk is obtained using the asymptotic properties of the maximum likelihood estimators. Lastly, the Bayesian reliability acceptance sampling plan is obtained, where the decision function is arbitrary. The manufacturer can derive an optimal decision function by minimizing the Bayes risk among all decision functions. This optimal decision function is known as Bayes decision function. The optimal sampling plan is obtained by minimizing the Bayes risk. The algorithms are provided for the computation of optimum Bayesian reliability acceptance sampling plan. Numerical results are provided and comparisons between the Bayesian reliability acceptance sampling plans are carried out.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16740v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Biswabrata Pradhan, Rathin Das</dc:creator>
    </item>
    <item>
      <title>Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter</title>
      <link>https://arxiv.org/abs/2510.15954</link>
      <description>arXiv:2510.15954v1 Announce Type: cross 
Abstract: As wildfires become increasingly destructive and expensive to control, effective management of active wildfires requires accurate, real-time fire spread predictions. To enhance the forecasting accuracy of active fires, data assimilation plays a vital role by integrating observations (such as remote-sensing data) and fire predictions generated from numerical models. This paper provides a comprehensive investigation on the application of a recently proposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter (EnSF) -- to the data assimilation problem for real-time active wildfire spread predictions. Leveraging a score-based generative diffusion model, EnSF has been shown to have superior accuracy for high-dimensional nonlinear filtering problems, making it an ideal candidate for the filtering problems of wildfire spread models. Technical details are provided, and our numerical investigations demonstrate that EnSF provides superior accuracy, stability, and computational efficiency, establishing it as a robust and practical method for wildfire data assimilation. Our code has been made publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15954v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongzheng Shi, Yuhang Wang, Xiao Liu</dc:creator>
    </item>
    <item>
      <title>A tutorial on discovering and quantifying the effect of latent causal sources of multimodal EHR data</title>
      <link>https://arxiv.org/abs/2510.16026</link>
      <description>arXiv:2510.16026v1 Announce Type: cross 
Abstract: We provide an accessible description of a peer-reviewed generalizable causal machine learning pipeline to (i) discover latent causal sources of large-scale electronic health records observations, and (ii) quantify the source causal effects on clinical outcomes. We illustrate how imperfect multimodal clinical data can be processed, decomposed into probabilistic independent latent sources, and used to train taskspecific causal models from which individual causal effects can be estimated. We summarize the findings of the two real-world applications of the approach to date as a demonstration of its versatility and utility for medical discovery at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16026v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marco Barbero-Mota, Eric V. Strobl, John M. Still, William W. Stead, Thomas A. Lasko</dc:creator>
    </item>
    <item>
      <title>FSRF: Factorization-guided Semantic Recovery for Incomplete Multimodal Sentiment Analysis</title>
      <link>https://arxiv.org/abs/2510.16086</link>
      <description>arXiv:2510.16086v1 Announce Type: cross 
Abstract: In recent years, Multimodal Sentiment Analysis (MSA) has become a research hotspot that aims to utilize multimodal data for human sentiment understanding. Previous MSA studies have mainly focused on performing interaction and fusion on complete multimodal data, ignoring the problem of missing modalities in real-world applications due to occlusion, personal privacy constraints, and device malfunctions, resulting in low generalizability.
  To this end, we propose a Factorization-guided Semantic Recovery Framework (FSRF) to mitigate the modality missing problem in the MSA task.
  Specifically, we propose a de-redundant homo-heterogeneous factorization module that factorizes modality into modality-homogeneous, modality-heterogeneous, and noisy representations and design elaborate constraint paradigms for representation learning.
  Furthermore, we design a distribution-aligned self-distillation module that fully recovers the missing semantics by utilizing bidirectional knowledge transfer.
  Comprehensive experiments on two datasets indicate that FSRF has a significant performance advantage over previous methods with uncertain missing modalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16086v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>In Proceedings of the IEEE International Conference on Multimedia and Expo (ICME 2025)</arxiv:journal_reference>
      <dc:creator>Ziyang Liu, Pengjunfei Chu, Shuming Dong, Chen Zhang, Mingcheng Li, Jin Wang</dc:creator>
    </item>
    <item>
      <title>Variable Selection with Broken Adaptive Ridge Regression for Interval-Censored Competing Risks Data</title>
      <link>https://arxiv.org/abs/2510.17084</link>
      <description>arXiv:2510.17084v1 Announce Type: cross 
Abstract: Competing risks data refer to situations where the occurrence of one event pre- cludes the possibility of other events happening, resulting in multiple mutually exclusive events. This data type is commonly encountered in medical research and clinical trials, exploring the interplay between different events and informing decision-making in fields such as healthcare and epidemiology. We develop a penal- ized variable selection procedure to handle such complex data in an interval-censored setting. We consider a broad class of semiparametric transformation regression mod- els, including popular models such as proportional and non-proportional hazards models. To promote sparsity and select variables specific to each event, we employ the broken adaptive ridge (BAR) penalty. This approach allows us to simultane- ously select important risk factors and estimate their effects for each event under investigation. We establish the oracle property of the BAR procedure and evaluate its performance through simulation studies. The proposed method is applied to a real-life HIV cohort dataset, further validating its applicability in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17084v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fatemeh Mahmoudi, Chenxi Li, Kaida Cai, Xuewen Lu</dc:creator>
    </item>
    <item>
      <title>Defining Utility as a Measure of Preference Under Uncertainty in Phase I-II Oncology Dose Finding Trials</title>
      <link>https://arxiv.org/abs/2510.17550</link>
      <description>arXiv:2510.17550v1 Announce Type: cross 
Abstract: The main objective of dose finding trials is to find an optimal dose amongst a candidate set for further research. The trial design in oncology proceeds in stages with a decision as to how to treat the next group of patients made at every stage until a final sample size is reached or the trial stopped early.
  This work applies a Bayesian decision-theoretic approach to the problem, proposing a new utility function based on both efficacy and toxicity and grounded in von Neumann-Morgenstern (VNM) utility theory. Our proposed framework seeks to better capture real clinical judgements by allowing attitudes to risk to vary when the judgements are of gains or losses, which are defined with respect to an intermediate outcome known as a reference point. We call this method Reference Dependent Decision Theoretic dose finding (R2DT).
  A simulation study demonstrates that the framework can perform well and produce good operating characteristics. The simulation results demonstrate that R2DT is better at detecting the optimal dose in scenarios where candidate doses are around minimum acceptable efficacy and maximum acceptable toxicity thresholds.
  The proposed framework shows that a flexible utility function, which better captures clinician beliefs, can lead to trials with good operating characteristics, including a high probability of finding the optimal dose. Our work demonstrates proof-of-concept for this framework, which should be evaluated in a broader range of settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17550v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Hall, Duncan Wilson, Stuart Barber, Sarah R Brown</dc:creator>
    </item>
    <item>
      <title>Are penalty shootouts better than a coin toss? Evidence from European football</title>
      <link>https://arxiv.org/abs/2510.17641</link>
      <description>arXiv:2510.17641v1 Announce Type: cross 
Abstract: Penalty shootouts play an important role in the knockout stage of major football tournaments, especially since the 2021/22 season, when the Union of European Football Associations (UEFA) scrapped the away goals rule in its club competitions. Inspired by this rule change, our paper examines whether the outcome of a penalty shootout can be predicted in UEFA club competitions. Based on all shootouts between 2000 and 2025, we find no evidence for the effect of the kicking order, the field of the match, and psychological momentum. In contrast to previous results, stronger teams, defined first by Elo ratings, do not perform better than their weaker opponents. Consequently, penalty shootouts are equivalent to a perfect lottery in top European football.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17641v1</guid>
      <category>econ.GN</category>
      <category>physics.soc-ph</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o, D\'ora Gr\'eta Petr\'oczy</dc:creator>
    </item>
    <item>
      <title>Atlas-based Manifold Representations for Interpretable Riemannian Machine Learning</title>
      <link>https://arxiv.org/abs/2510.17772</link>
      <description>arXiv:2510.17772v1 Announce Type: cross 
Abstract: Despite the popularity of the manifold hypothesis, current manifold-learning methods do not support machine learning directly on the latent $d$-dimensional data manifold, as they primarily aim to perform dimensionality reduction into $\mathbb{R}^D$, losing key manifold features when the embedding dimension $D$ approaches $d$.
  On the other hand, methods that directly learn the latent manifold as a differentiable atlas have been relatively underexplored.
  In this paper, we aim to give a proof of concept of the effectiveness and potential of atlas-based methods. To this end, we implement a generic data structure to maintain a differentiable atlas that enables Riemannian optimization over the manifold. We complement this with an unsupervised heuristic that learns a differentiable atlas from point cloud data. We experimentally demonstrate that this approach has advantages in terms of efficiency and accuracy in selected settings. Moreover, in a supervised classification task over the Klein bottle and in RNA velocity analysis of hematopoietic data, we showcase the improved interpretability and robustness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17772v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan A. Robinett, Sophia A. Madejski, Kyle Ruark, Samantha J. Riesenfeld, Lorenzo Orecchia</dc:creator>
    </item>
    <item>
      <title>Admittance Matrix Concentration Inequalities for Understanding Uncertain Power Networks</title>
      <link>https://arxiv.org/abs/2510.17798</link>
      <description>arXiv:2510.17798v1 Announce Type: cross 
Abstract: This paper presents probabilistic bounds for the spectrum of the admittance matrix and classical linear power flow models under uncertain network parameters; for example, probabilistic line contingencies. Our proposed approach imports tools from probability theory, such as concentration inequalities for random matrices with independent entries. It yields error bounds for common approximations of the AC power flow equations under parameter uncertainty, including the DC and LinDistFlow approximations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17798v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Talkington, Cameron Khanpour, Rahul K. Gupta, Sergio A. Dorado-Rojas, Daniel Turizo, Hyeongon Park, Dmitrii M. Ostrovskii, Daniel K. Molzahn</dc:creator>
    </item>
    <item>
      <title>A Statistical Model with Qualitative Input</title>
      <link>https://arxiv.org/abs/1712.05135</link>
      <description>arXiv:1712.05135v2 Announce Type: replace 
Abstract: A statistical estimation model with qualitative input provides a mechanism to fuse human intuition in the form of qualitative information into a statistical model. We investigate the statistical properties of this model and devise a numerical computation method for a model subclass with a uniform correlation structure. We show that, within this subclass, qualitative information can be as useful as quantitative information. We also show that the correlation between variables compromises the accuracy of the statistical estimate. However, the adverse effect from the correlation can be minimal, as is illustrated in an application to portfolio selection. The proposed model, when used in conjunction with approximation techniques, is shown to have potential for portfolio selection with financial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:1712.05135v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seksan Kiatsupaibul, Pariyakorn Maneekul</dc:creator>
    </item>
    <item>
      <title>Product Relation Correlation and Its Use in Product Clustering</title>
      <link>https://arxiv.org/abs/2201.12140</link>
      <description>arXiv:2201.12140v2 Announce Type: replace 
Abstract: This paper introduces product relation correlation, a measure of product relatedness that assesses the extent to which products may function as substitutes or complements through analysis of shared purchasing patterns. Product relation correlation can be used for tasks such as product clustering and shelf space optimization, enabling retailers to arrange items in ways that enhance customer experience. Applied to data from a retail drugstore chain, the measure demonstrates an alignment with cross-price elasticity, increasing as products diverge from independence. With computational simplicity, requirement for only commonly available data, and a robust theoretical interpretation, product relation correlation serves as a practical and efficient tool for deriving useful product insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.12140v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Petr Krautwurm, Ond\v{r}ej Sokol, Vladim\'ir Hol\'y</dc:creator>
    </item>
    <item>
      <title>Bayesian Projection of Extant Refugee and Asylum Seeker Populations</title>
      <link>https://arxiv.org/abs/2405.06857</link>
      <description>arXiv:2405.06857v2 Announce Type: replace 
Abstract: Estimates of future migration patterns are of broad interest in demography. Forced migration, including refugee and asylum seekers, plays an important role in overall migration patterns, but is notoriously difficult to forecast. Focusing on refugees and asylum seekers, we propose a modeling pipeline based on Bayesian hierarchical time-series modeling for projecting refugee population official statistics by country of origin using data from the United Nations High Commissioner for Refugees (UNHCR). Our approach is based on a conceptual model of refugee and asylum seeker populations following growth and decline phases, separated by a peak. The growth and decline phases are modeled by logistic growth and decline through an interrupted logistic process model. We evaluate our method through a set of validation exercises that show it has good performance for forecasts at 1, 5, and 10 year horizons, and we present projections for 35 countries of origin of large refugee and asylum seeker population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06857v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Herbert Susmann, Adrian E. Raftery</dc:creator>
    </item>
    <item>
      <title>MMformer with Adaptive Transferable Attention: Advancing Multivariate Time Series Forecasting for Environmental Applications</title>
      <link>https://arxiv.org/abs/2504.14050</link>
      <description>arXiv:2504.14050v2 Announce Type: replace 
Abstract: Environmental crisis remains a global challenge that affects public health and environmental quality. Despite extensive research, accurately forecasting environmental change trends to inform targeted policies and assess prediction efficiency remains elusive. Conventional methods for multivariate time series (MTS) analysis often fail to capture the complex dynamics of environmental change. To address this, we introduce an innovative meta-learning MTS model, MMformer with Adaptive Transferable Multi-head Attention (ATMA), which combines self-attention and meta-learning for enhanced MTS forecasting. Specifically, MMformer is used to model and predict the time series of seven air quality indicators across 331 cities in China from January 2018 to June 2021 and the time series of precipitation and temperature at 2415 monitoring sites during the summer (276 days) from 2012 to 2014, validating the network's ability to perform and forecast MTS data successfully. Experimental results demonstrate that in these datasets, the MMformer model reaching SOTA outperforms iTransformer, Transformer, and the widely used traditional time series prediction algorithm SARIMAX in the prediction of MTS, reducing by 50\% in MSE, 20\% in MAE as compared to others in air quality datasets, reducing by 20\% in MAPE except SARIMAX. Compared with Transformer and SARIMAX in the climate datasets, MSE, MAE, and MAPE are decreased by 30\%, and there is an improvement compared to iTransformer. This approach represents a significant advance in our ability to forecast and respond to dynamic environmental quality challenges in diverse urban and rural environments. Its predictive capabilities provide valuable public health and environmental quality information, informing targeted interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14050v2</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ning Xin, Jionglong Su, Md Maruf Hasan</dc:creator>
    </item>
    <item>
      <title>Forecasting the U.S. Renewable-Energy Mix with an ALR-BDARMA Compositional Time-Series Framework</title>
      <link>https://arxiv.org/abs/2507.04087</link>
      <description>arXiv:2507.04087v4 Announce Type: replace 
Abstract: Accurate forecasts of the US renewable-generation mix are critical for planning transmission upgrades, sizing storage, and setting balancing-market rules. We present a Bayesian Dirichlet ARMA (BDARMA) model for monthly shares of hydro, geothermal, solar, wind, wood, municipal waste, and biofuels from January 2010 to January 2025. The mean vector follows a parsimonious VAR(2) in additive-log-ratio space, while the Dirichlet concentration parameter combines an intercept with ten Fourier harmonics, letting predictive dispersion expand or contract with the seasons.
  A 61-split rolling-origin study generates twelve-month density forecasts from January 2019 to January 2024. Relative to three benchmarks, a Gaussian VAR(2) in transform space, a seasonal naive copy of last year's proportions, and a drift-free additive-log-ratio random walk, BDARMA lowers the mean continuous ranked probability score by fifteen to sixty percent, achieves component-wise ninety percent interval coverage close to nominal, and matches Gaussian VAR point accuracy through eight months with a maximum loss of 0.02 Aitchison units thereafter. BDARMA therefore delivers sharp, well-calibrated probabilistic forecasts of multivariate renewable-energy shares without sacrificing point precision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04087v4</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison Katz, Thomas Maierhofer</dc:creator>
    </item>
    <item>
      <title>Large-scale spatial variable gene atlas for spatial transcriptomics</title>
      <link>https://arxiv.org/abs/2510.07653</link>
      <description>arXiv:2510.07653v2 Announce Type: replace 
Abstract: Spatial variable genes (SVGs) reveal critical information about tissue architecture, cellular interactions, and disease microenvironments. As spatial transcriptomics (ST) technologies proliferate, accurately identifying SVGs across diverse platforms, tissue types, and disease contexts has become both a major opportunity and a significant computational challenge. Here, we present a comprehensive benchmarking study of 20 state-of-the-art SVG detection methods using human slides from STimage-1K4M, a large-scale resource of ST data comprising 662 slides from more than 18 tissue types. We evaluate each method across a range of biologically and technically meaningful criteria, including recovery of pathologist-annotated domain-specific markers, cross-slide reproducibility, scalability to high-resolution data, and robustness to technical variation. Our results reveal marked differences in performance depending on tissue type, spatial resolution, and study design. Beyond benchmarking, we construct the first cross-tissue atlas of SVGs, enabling comparative analysis of spatial gene programs across cancer and normal tissues. We observe similarities between pairs of tissues that reflect developmental and functional relationships, such as high overlap between thymus and lymph node, and uncover spatial gene programs associated with metastasis, immune infiltration, and tissue-of-origin identity in cancer. Together, our work defines a framework for evaluating and interpreting spatial gene expression and establishes a reference resource for the ST community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07653v2</guid>
      <category>stat.AP</category>
      <category>cs.DB</category>
      <category>q-bio.GN</category>
      <category>q-bio.TO</category>
      <category>stat.CO</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawen Chen, Jinwei Zhang, Dongshen Peng, Yutong Song, Aitong Ruan, Yun Li, Didong Li</dc:creator>
    </item>
    <item>
      <title>P3LS: Point Process Partial Least Squares</title>
      <link>https://arxiv.org/abs/2412.11267</link>
      <description>arXiv:2412.11267v2 Announce Type: replace-cross 
Abstract: Many studies collect data that can be considered as a realization of a point process. Included are medical imaging data where photon counts are recorded by a gamma camera from patients being injected with a gamma emitting tracer. It is of interest to develop analytic methods that can help with diagnosis as well as in the training of inexpert radiologists. Partial least squares (PLS) is a popular analytic approach that combines features from linear modeling as well as dimension reduction to provide parsimonious prediction and classification. However, existing PLS methodologies do not include the analysis of point process predictors. In this article, we introduce point process PLS (P3LS) for analyzing latent time-varying intensity functions from collections of inhomogeneous point processes. A novel estimation procedure for $P^3LS$ is developed that utilizes the properties of log-Gaussian Cox processes, and its empirical properties are examined in simulation studies. The method is used to analyze kidney functionality in patients with renal disease in order to aid in the diagnosis of kidney obstruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11267v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jamshid Namdari, Robert T Krafty, Amita Manatunga</dc:creator>
    </item>
    <item>
      <title>Do Determinants of EV Purchase Intent vary across the Spectrum? Evidence from Bayesian Analysis of US Survey Data</title>
      <link>https://arxiv.org/abs/2504.09854</link>
      <description>arXiv:2504.09854v2 Announce Type: replace-cross 
Abstract: While electric vehicle (EV) adoption has been widely studied, most research focuses on the average effects of predictors on purchase intent, overlooking variation across the distribution of EV purchase intent. This paper makes a threefold contribution by analyzing four unique explanatory variables, leveraging large-scale US survey data from 2021 to 2023, and employing Bayesian ordinal probit and Bayesian ordinal quantile modeling to evaluate the effects of these variables-while controlling for other commonly used covariates-on EV purchase intent, both on average and across its full distribution. By modeling purchase intent as an ordered outcome-from "not at all likely" to "very likely"-we reveal how covariate effects differ across levels of interest. This is the first application of ordinal quantile modeling in the EV adoption literature, uncovering heterogeneity in how potential buyers respond to key factors. For instance, confidence in development of charging infrastructure and belief in environmental benefits are linked not only to higher interest among likely adopters but also to reduced resistance among more skeptical respondents. Notably, we identify a gap between the prevalence and influence of key predictors: although few respondents report strong infrastructure confidence or frequent EV information exposure, both factors are strongly associated with increased intent across the spectrum. These findings suggest clear opportunities for targeted communication and outreach, alongside infrastructure investment, to support widespread EV adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09854v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nafisa Lohawala, Mohammad Arshad Rahman</dc:creator>
    </item>
    <item>
      <title>Prenatal phthalate exposures and adiposity outcomes trajectories: a multivariate Bayesian factor regression approach</title>
      <link>https://arxiv.org/abs/2506.02518</link>
      <description>arXiv:2506.02518v2 Announce Type: replace-cross 
Abstract: Experimental animal evidence and a growing body of observational studies suggest that prenatal exposure to phthalates may be a risk factor for childhood obesity. Using data from the Mount Sinai Children's Environmental Health Study (MSCEHS), which measured urinary phthalate metabolites (including MEP, MnBP, MiBP, MCPP, MBzP, MEHP, MEHHP, MEOHP, and MECPP) during the third trimester of pregnancy (between 25 and 40 weeks) of 382 mothers, we examined adiposity outcomes: body mass index (BMI), fat mass percentage, waist-to-hip ratio, and waist circumference, of 180 children between ages 4 and 9. We aimed to assess the effects of prenatal exposure to phthalates on these adiposity outcomes, with potential time-varying and sex-specific effects. We applied a novel Bayesian multivariate factor regression (BMFR) that (1) represents phthalate mixtures as latent factors, a DEHP and a non-DEHP factor, (2) borrows information across highly correlated adiposity outcomes to improve estimation precision, (3) models potentially non-linear time-varying effects of the latent factors on adiposity outcomes, and (4) fully quantifies uncertainty using state-of-the-art prior specifications. The results show that in boys, at younger ages (4-6), all phthalate components are associated with lower adiposity outcomes; however, after age 7, they are associated with higher outcomes. In girls, there is no evidence of associations between phthalate factors and adiposity outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02518v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Phuc H. Nguyen, Stephanie M. Engel, Amy H. Herring</dc:creator>
    </item>
    <item>
      <title>Spatial and Temporal Boundaries in Difference-in-Differences: A Framework from Navier-Stokes Equation</title>
      <link>https://arxiv.org/abs/2510.11013</link>
      <description>arXiv:2510.11013v2 Announce Type: replace-cross 
Abstract: This paper develops a unified framework for identifying spatial and temporal boundaries of treatment effects in difference-in-differences designs. Starting from fundamental fluid dynamics equations (Navier-Stokes), we derive conditions under which treatment effects decay exponentially in space and time, enabling researchers to calculate explicit boundaries beyond which effects become undetectable. The framework encompasses both linear (pure diffusion) and nonlinear (advection-diffusion with chemical reactions) regimes, with testable scope conditions based on dimensionless numbers from physics (P\'eclet and Reynolds numbers). We demonstrate the framework's diagnostic capability using air pollution from coal-fired power plants. Analyzing 791 ground-based PM$_{2.5}$ monitors and 189,564 satellite-based NO$_2$ grid cells in the Western United States over 2019-2021, we find striking regional heterogeneity: within 100 km of coal plants, both pollutants show positive spatial decay (PM$_{2.5}$: $\kappa_s = 0.00200$, $d^* = 1,153$ km; NO$_2$: $\kappa_s = 0.00112$, $d^* = 2,062$ km), validating the framework. Beyond 100 km, negative decay parameters correctly signal that urban sources dominate and diffusion assumptions fail. Ground-level PM$_{2.5}$ decays approximately twice as fast as satellite column NO$_2$, consistent with atmospheric transport physics. The framework successfully diagnoses its own validity in four of eight analyzed regions, providing researchers with physics-based tools to assess whether their spatial difference-in-differences setting satisfies diffusion assumptions before applying the estimator. Our results demonstrate that rigorous boundary detection requires both theoretical derivation from first principles and empirical validation of underlying physical assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11013v2</guid>
      <category>econ.EM</category>
      <category>econ.GN</category>
      <category>math.ST</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Dynamic Spatial Treatment Effect Boundaries: A Continuous Functional Framework from Navier-Stokes Equations</title>
      <link>https://arxiv.org/abs/2510.14409</link>
      <description>arXiv:2510.14409v2 Announce Type: replace-cross 
Abstract: I develop a comprehensive theoretical framework for dynamic spatial treatment effect boundaries using continuous functional definitions grounded in Navier-Stokes partial differential equations. Rather than discrete treatment effect estimators, the framework characterizes treatment intensity as a continuous function $\tau(\mathbf{x}, t)$ over space-time, enabling rigorous analysis of propagation dynamics, boundary evolution, and cumulative exposure patterns. Building on exact self-similar solutions expressible through Kummer confluent hypergeometric and modified Bessel functions, I establish that treatment effects follow scaling laws $\tau(d, t) = t^{-\alpha} f(d/t^\beta)$ where exponents characterize diffusion mechanisms. Empirical validation using 42 million TROPOMI satellite observations of NO$_2$ pollution from U.S. coal-fired power plants demonstrates strong exponential spatial decay ($\kappa_s = 0.004$ per km, $R^2 = 0.35$) with detectable boundaries at 572 km. Monte Carlo simulations confirm superior performance over discrete parametric methods in boundary detection and false positive avoidance (94\% vs 27\% correct rejection). Regional heterogeneity analysis validates diagnostic capability: positive decay parameters within 100 km confirm coal plant dominance; negative parameters beyond 100 km correctly signal when urban sources dominate. The continuous functional perspective unifies spatial econometrics with mathematical physics, providing theoretically grounded methods for boundary detection, exposure quantification, and policy evaluation across environmental economics, banking, and healthcare applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14409v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Dynamic Spatial Treatment Effects as Continuous Functionals: Theory and Evidence from Healthcare Access</title>
      <link>https://arxiv.org/abs/2510.15324</link>
      <description>arXiv:2510.15324v2 Announce Type: replace-cross 
Abstract: I develop a continuous functional framework for spatial treatment effects grounded in Navier-Stokes partial differential equations. Rather than discrete treatment parameters, the framework characterizes treatment intensity as continuous functions $\tau(\mathbf{x}, t)$ over space-time, enabling rigorous analysis of boundary evolution, spatial gradients, and cumulative exposure. Empirical validation using 32,520 U.S. ZIP codes demonstrates exponential spatial decay for healthcare access ($\kappa = 0.002837$ per km, $R^2 = 0.0129$) with detectable boundaries at 37.1 km. The framework successfully diagnoses when scope conditions hold: positive decay parameters validate diffusion assumptions near hospitals, while negative parameters correctly signal urban confounding effects. Heterogeneity analysis reveals 2-13 $\times$ stronger distance effects for elderly populations and substantial education gradients. Model selection strongly favors logarithmic decay over exponential ($\Delta \text{AIC} &gt; 10,000$), representing a middle ground between exponential and power-law decay. Applications span environmental economics, banking, and healthcare policy. The continuous functional framework provides predictive capability ($d^*(t) = \xi^* \sqrt{t}$), parameter sensitivity ($\partial d^*/\partial \nu$), and diagnostic tests unavailable in traditional difference-in-differences approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15324v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
  </channel>
</rss>
