<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 May 2024 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Radical Solution and Computational Complexity</title>
      <link>https://arxiv.org/abs/2405.15790</link>
      <description>arXiv:2405.15790v1 Announce Type: new 
Abstract: The radical solution of polynomials with rational coefficients is a famous solved problem. This paper found that it is a $\mathbb{NP}$ problem. Furthermore, this paper found that arbitrary $ \mathscr{P} \in \mathbb{P}$ shall have a one-way running graph $G$, and have a corresponding $\mathscr{Q} \in \mathbb{NP}$ which have a two-way running graph $G'$, $G$ and $G'$ is isomorphic, i.e., $G'$ is combined by $G$ and its reverse $G^{-1}$. When $\mathscr{P}$ is an algorithm for solving polynomials, $G^{-1}$ is the radical formula. According to Galois' Theory, a general radical formula does not exist. Therefore, there exists an $\mathbb{NP}$, which does not have a general, deterministic and polynomial time-complexity algorithm, i.e., $\mathbb{P} \neq \mathbb{NP}$. Moreover, this paper pointed out that this theorem actually is an impossible trinity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15790v1</guid>
      <category>cs.CC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bojin Zheng, Weiwu Wang</dc:creator>
    </item>
    <item>
      <title>Complexity of Multiple-Hamiltonicity in Graphs of Bounded Degree</title>
      <link>https://arxiv.org/abs/2405.16270</link>
      <description>arXiv:2405.16270v1 Announce Type: new 
Abstract: We study the following generalization of the Hamiltonian cycle problem: Given integers $a,b$ and graph $G$, does there exist a closed walk in $G$ that visits every vertex at least $a$ times and at most $b$ times? Equivalently, does there exist a connected $[2a,2b]$ factor of $2b \cdot G$ with all degrees even? This problem is NP-hard for any constants $1 \leq a \leq b$. However, the graphs produced by known reductions have maximum degree growing linearly in $b$. The case $a = b = 1 $ -- i.e. Hamiltonicity -- remains NP-hard even in $3$-regular graphs; a natural question is whether this is true for other $a$, $b$.
  In this work, we study which $a, b$ permit polynomial time algorithms and which lead to NP-hardness in graphs with constrained degrees. We give tight characterizations for regular graphs and graphs of bounded max-degree, both directed and undirected.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16270v1</guid>
      <category>cs.CC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brian Liu, Nathan S. Sheffield, Alek Westover</dc:creator>
    </item>
    <item>
      <title>A Strong Direct Sum Theorem for Distributional Query Complexity</title>
      <link>https://arxiv.org/abs/2405.16340</link>
      <description>arXiv:2405.16340v1 Announce Type: new 
Abstract: Consider the expected query complexity of computing the $k$-fold direct product $f^{\otimes k}$ of a function $f$ to error $\varepsilon$ with respect to a distribution $\mu^k$. One strategy is to sequentially compute each of the $k$ copies to error $\varepsilon/k$ with respect to $\mu$ and apply the union bound. We prove a strong direct sum theorem showing that this naive strategy is essentially optimal. In particular, computing a direct product necessitates a blowup in both query complexity and error.
  Strong direct sum theorems contrast with results that only show a blowup in query complexity or error but not both. There has been a long line of such results for distributional query complexity, dating back to (Impagliazzo, Raz, Wigderson 1994) and (Nisan, Rudich, Saks 1994), but a strong direct sum theorem had been elusive.
  A key idea in our work is the first use of the Hardcore Theorem (Impagliazzo 1995) in the context of query complexity. We prove a new "resilience lemma" that accompanies it, showing that the hardcore of $f^{\otimes k}$ is likely to remain dense under arbitrary partitions of the input space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16340v1</guid>
      <category>cs.CC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy Blanc, Caleb Koch, Carmen Strassle, Li-Yang Tan</dc:creator>
    </item>
    <item>
      <title>Game Derandomization</title>
      <link>https://arxiv.org/abs/2405.16353</link>
      <description>arXiv:2405.16353v1 Announce Type: new 
Abstract: Using Kolmogorov Game Derandomization, upper bounds of the Kolmogorov complexity of deterministic winning players against deterministic environments can be proved. This paper gives improved upper bounds of the Kolmogorov complexity of such players. This paper also generalizes this result to probabilistic games. This applies to computable, lower computable, and uncomputable environments. We characterize the classic even-odds game and then generalize these results to time bounded players and also to all zero-sum repeated games. We characterize partial game derandomization. But first, we start with an illustrative example of game derandomization, taking place on the island of Crete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16353v1</guid>
      <category>cs.CC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuel Epstein</dc:creator>
    </item>
    <item>
      <title>Half-duplex communication complexity with adversary can be less than the classical communication complexity</title>
      <link>https://arxiv.org/abs/2405.16881</link>
      <description>arXiv:2405.16881v1 Announce Type: new 
Abstract: Half-duplex communication complexity with adversary was defined in [Hoover, K., Impagliazzo, R., Mihajlin, I., Smal, A. V. Half-Duplex Communication Complexity, ISAAC 2018.] Half-duplex communication protocols generalize classical protocols defined by Andrew Yao in [Yao, A. C.-C. Some Complexity Questions Related to Distributive Computing (Preliminary Report), STOC 1979]. It has been unknown so far whether the communication complexities defined by these models are different or not. In the present paper we answer this question: we exhibit a function whose half-duplex communication complexity with adversary is strictly less than its classical communication complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16881v1</guid>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikhail Dektiarev, Nikolay Vereshchagin</dc:creator>
    </item>
    <item>
      <title>Unconventional complexity classes in unconventional computing (extended abstract)</title>
      <link>https://arxiv.org/abs/2405.16896</link>
      <description>arXiv:2405.16896v1 Announce Type: new 
Abstract: Many unconventional computing models, including some that appear to be quite different from traditional ones such as Turing machines, happen to characterise either the complexity class P or PSPACE when working in deterministic polynomial time (and in the maximally parallel way, where this applies). We discuss variants of cellular automata and membrane systems that escape this dichotomy and characterise intermediate complexity classes, usually defined in terms of Turing machines with oracles, as well as some possible reasons why this happens.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16896v1</guid>
      <category>cs.CC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonio E. Porreca</dc:creator>
    </item>
    <item>
      <title>Delta-modular ILP Problems of Bounded Co-dimension, Discrepancy, and Convolution</title>
      <link>https://arxiv.org/abs/2405.17001</link>
      <description>arXiv:2405.17001v1 Announce Type: new 
Abstract: For $k, n \geq 0$, and $c \in Z^n$, we consider ILP problems \begin{gather*}
  \max\bigl\{ c^\top x \colon A x = b,\, x \in Z^n_{\geq 0} \bigr\}\text{ with $A \in Z^{k \times n}$, $rank(A) = k$, $b \in Z^{k}$ and}
  \max\bigl\{ c^\top x \colon A x \leq b,\, x \in Z^n \bigr\} \text{ with $A \in Z^{(n+k) \times n}$, $rank(A) = n$, $b \in Z^{n+k}$.} \end{gather*} The first problem is called an \emph{ILP problem in the standard form of the codimension $k$}, and the second problem is called an \emph{ILP problem in the canonical form with $n+k$ constraints.} We show that, for any sufficiently large $\Delta$, both problems can be solved with $$ 2^{O(k)} \cdot (f_{k,d} \cdot \Delta)^2 / 2^{\Omega\bigl(\sqrt{\log(f_{k,d} \cdot \Delta)}\bigr)} $$ operations, where $
  f_{k,d} = \min \Bigl\{ k^{k/2},
  \bigl(\log k \cdot \log (d + k)\bigr)^{k/2}
  \Bigr\} $, $d$ is the dimension of a corresponding polyhedron and $\Delta$ is the maximum absolute value of $rank(A) \times rank(A)$ sub-determinants of $A$.
  As our second main result, we show that the feasibility variants of both problems can be solved with $$ 2^{O(k)} \cdot f_{k,d} \cdot \Delta \cdot \log^3(f_{k,d} \cdot \Delta) $$ operations. The constant $f_{k,d}$ can be replaced by other constant $g_{k,\Delta} = \bigl(\log k \cdot \log (k \Delta)\bigr)^{k/2}$ that depends only on $k$ and $\Delta$. Additionally, we consider different partial cases with $k=0$ and $k=1$, which have interesting applications.
  As a result of independent interest, we propose an $n^2/2^{\Omega\bigl(\sqrt{\log n}\bigr)}$-time algorithm for the tropical convolution problem on sequences, indexed by elements of a finite Abelian group of the order $n$. This result is obtained, reducing the above problem to the matrix multiplication problem on a tropical semiring and using seminal algorithm by R. Williams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17001v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.AC</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>D. Gribanov, D. Malyshev, P. M. Pardalos</dc:creator>
    </item>
    <item>
      <title>Maximizing Phylogenetic Diversity under Ecological Constraints: A Parameterized Complexity Study</title>
      <link>https://arxiv.org/abs/2405.17314</link>
      <description>arXiv:2405.17314v1 Announce Type: new 
Abstract: In the NP-hard Optimizing PD with Dependencies (PDD) problem, the input consists of a phylogenetic tree $T$ over a set of taxa $X$, a food-web that describes the prey-predator relationships in $X$, and integers $k$ and $D$. The task is to find a set $S$ of $k$ species that is viable in the food-web such that the subtree of $T$ obtained by retaining only the vertices of $S$ has total edge weight at least $D$. Herein, viable means that for every predator taxon of $S$, the set $S$ contains at least one prey taxon. We provide the first systematic analysis of PDD and its special case s-PDD from a parameterized complexity perspective. For solution-size related parameters, we show that PDD is FPT with respect to $D$ and with respect to $k$ plus the height of the phylogenetic tree. Moreover, we consider structural parameterizations of the food-web. For example, we show an FPT-algorithm for the parameter that measures the vertex deletion distance to graphs where every connected component is a complete graph. Finally, we show that s-PDD admits an FPT-algorithm for the treewidth of the food-web. This disproves a conjecture of Faller et al. [Annals of Combinatorics, 2011] who conjectured that s-PDD is NP-hard even when the food-web is a tree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17314v1</guid>
      <category>cs.CC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Komusiewicz, Jannik Schestag</dc:creator>
    </item>
    <item>
      <title>Small unsatisfiable $k$-CNFs with bounded literal occurrence</title>
      <link>https://arxiv.org/abs/2405.16149</link>
      <description>arXiv:2405.16149v1 Announce Type: cross 
Abstract: We obtain the smallest unsatisfiable formulas in subclasses of $k$-CNF (exactly $k$ distinct literals per clause) with bounded variable or literal occurrences. Smaller unsatisfiable formulas of this type translate into stronger inapproximability results for MaxSAT in the considered formula class. Our results cover subclasses of 3-CNF and 4-CNF; in all subclasses of 3-CNF we considered we were able to determine the smallest size of an unsatisfiable formula; in the case of 4-CNF with at most 5 occurrences per variable we decreased the size of the smallest known unsatisfiable formula. Our methods combine theoretical arguments and symmetry-breaking exhaustive search based on SAT Modulo Symmetries (SMS), a recent framework for isomorph-free SAT-based graph generation. To this end, and as a standalone result of independent interest, we show how to encode formulas as graphs efficiently for SMS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16149v1</guid>
      <category>cs.DM</category>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianwei Zhang, Tom\'a\v{s} Peitl, Stefan Szeider</dc:creator>
    </item>
    <item>
      <title>Limits of Deep Learning: Sequence Modeling through the Lens of Complexity Theory</title>
      <link>https://arxiv.org/abs/2405.16674</link>
      <description>arXiv:2405.16674v1 Announce Type: cross 
Abstract: Deep learning models have achieved significant success across various applications but continue to struggle with tasks requiring complex reasoning over sequences, such as function composition and compositional tasks. Despite advancements, models like Structured State Space Models (SSMs) and Transformers underperform in deep compositionality tasks due to inherent architectural and training limitations. Maintaining accuracy over multiple reasoning steps remains a primary challenge, as current models often rely on shortcuts rather than genuine multi-step reasoning, leading to performance degradation as task complexity increases. Existing research highlights these shortcomings but lacks comprehensive theoretical and empirical analysis for SSMs. Our contributions address this gap by providing a theoretical framework based on complexity theory to explain SSMs' limitations. Moreover, we present extensive empirical evidence demonstrating how these limitations impair function composition and algorithmic task performance. Our experiments reveal significant performance drops as task complexity increases, even with Chain-of-Thought (CoT) prompting. Models frequently resort to shortcuts, leading to errors in multi-step reasoning. This underscores the need for innovative solutions beyond current deep learning paradigms to achieve reliable multi-step reasoning and compositional task-solving in practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16674v1</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikola Zubi\'c, Federico Sold\'a, Aurelio Sulser, Davide Scaramuzza</dc:creator>
    </item>
    <item>
      <title>Finding Maximum Common Contractions Between Phylogenetic Networks</title>
      <link>https://arxiv.org/abs/2405.16713</link>
      <description>arXiv:2405.16713v1 Announce Type: cross 
Abstract: In this paper, we lay the groundwork on the comparison of phylogenetic networks based on edge contractions and expansions as edit operations, as originally proposed by Robinson and Foulds to compare trees. We prove that these operations connect the space of all phylogenetic networks on the same set of leaves, even if we forbid contractions that create cycles. This allows to define an operational distance on this space, as the minimum number of contractions and expansions required to transform one network into another. We highlight the difference between this distance and the computation of the maximum common contraction between two networks. Given its ability to outline a common structure between them, which can provide valuable biological insights, we study the algorithmic aspects of the latter. We first prove that computing a maximum common contraction between two networks is NP-hard, even when the maximum degree, the size of the common contraction, or the number of leaves is bounded. We also provide lower bounds to the problem based on the Exponential-Time Hypothesis. Nonetheless, we do provide a polynomial-time algorithm for weakly-galled networks, a generalization of galled trees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16713v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bertrand Marchand, Nadia Tahiri, Olivier Tremblay-Savard, Manuel Lafond</dc:creator>
    </item>
    <item>
      <title>Sparsity comparison of polytopal finite element methods</title>
      <link>https://arxiv.org/abs/2405.16864</link>
      <description>arXiv:2405.16864v1 Announce Type: cross 
Abstract: In this work we compare crucial parameters for efficiency of different finite element methods for solving partial differential equations (PDEs) on polytopal meshes. We consider the Virtual Element Method (VEM) and different Discontinuous Galerkin (DG) methods, namely the Hybrid DG and Trefftz DG methods. The VEM is a conforming method, that can be seen as a generalization of the classic finite element method to arbitrary polytopal meshes. DG methods are non-conforming methods that offer high flexibility, but also come with high computational costs. Hybridization reduces these costs by introducing additional facet variables, onto which the computational costs can be transfered to. Trefftz DG methods achieve a similar reduction in complexity by selecting a special and smaller set of basis functions on each element. The association of computational costs to different geometrical entities (elements or facets) leads to differences in the performance of these methods on different grid types. This paper aims to compare the dependency of these approaches across different grid configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16864v1</guid>
      <category>math.NA</category>
      <category>cs.CC</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Lehrenfeld, Paul Stocker, Maximilian Zienecker</dc:creator>
    </item>
    <item>
      <title>Program Synthesis is $\Sigma_3^0$-Complete</title>
      <link>https://arxiv.org/abs/2405.16997</link>
      <description>arXiv:2405.16997v1 Announce Type: cross 
Abstract: This paper considers program synthesis in the context of computational hardness, asking the question: How hard is it to determine whether a given synthesis problem has a solution or not?
  To answer this question, this paper studies program synthesis for a basic imperative, Turing-complete language IMP, for which this paper proves that program synthesis is $\Sigma_3^0$-\emph{complete} in the arithmetical hierarchy. The proof of this fact relies on a fully constructive encoding of program synthesis (which is typically formulated as a second-order query) as a first-order formula in the standard model of arithmetic (i.e., Peano arithmetic). Constructing such a formula then allows us to reduce the decision problem for COF (the set of functions which diverge only on a finite set of inputs), which is well-known to be a $\Sigma_3^0$-complete problem, into the constructed first-order representation of synthesis.
  In addition to this main result, we also consider the hardness of variants of synthesis problems, such as those introduced in previous work to make program synthesis more tractable (e.g., synthesis over finite examples). To the best of our knowledge, this paper is the first to give a first-order characterization of program synthesis in general, and precisely define the computability of synthesis problems and their variants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16997v1</guid>
      <category>cs.LO</category>
      <category>cs.CC</category>
      <category>cs.PL</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinwoo Kim</dc:creator>
    </item>
    <item>
      <title>Using continuation methods to analyse the difficulty of problems solved by Ising machines</title>
      <link>https://arxiv.org/abs/2405.17112</link>
      <description>arXiv:2405.17112v1 Announce Type: cross 
Abstract: Ising machines are dedicated hardware solvers of NP-hard optimization problems. However, they do not always find the most optimal solution. The probability of finding this optimal solution depends on the problem at hand. Using continuation methods, we show that this is closely linked to the bifurcation sequence of the optimal solution. From this bifurcation analysis, we can determine the effectiveness of solution schemes. Moreover, we find that the proper choice of implementation of the Ising machine can drastically change this bifurcation sequence and therefore vastly increase the probability of finding the optimal solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17112v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.CC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacob Lamers, Guy Verschaffelt, Guy Van der Sande</dc:creator>
    </item>
    <item>
      <title>Streaming Zero-Knowledge Proofs</title>
      <link>https://arxiv.org/abs/2301.02161</link>
      <description>arXiv:2301.02161v2 Announce Type: replace 
Abstract: Streaming interactive proofs (SIPs) enable a space-bounded algorithm with one-pass access to a massive stream of data to verify a computation that requires large space, by communicating with a powerful but untrusted prover.
  This work initiates the study of zero-knowledge proofs for data streams. We define the notion of zero-knowledge in the streaming setting and construct zero-knowledge SIPs for the two main algorithmic building blocks in the streaming interactive proofs literature: the sumcheck and polynomial evaluation protocols. To the best of our knowledge all known streaming interactive proofs are based on either of these tools, and indeed, this allows us to obtain zero-knowledge SIPs for central streaming problems such as index, point and range queries, median, frequency moments, and inner product.
  Our protocols are efficient in terms of time and space, as well as communication: the verifier algorithm's space complexity is $\mathrm{polylog}(n)$ and, after a non-interactive setup that uses a random string of near-linear length, the remaining parameters are $n^{o(1)}$.
  En route, we develop an algorithmic toolkit for designing zero-knowledge data stream protocols, consisting of an algebraic streaming commitment protocol and a temporal commitment protocol.Our analyses rely on delicate algebraic and information-theoretic arguments and reductions from average-case communication complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.02161v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.CCC.2024.2</arxiv:DOI>
      <dc:creator>Graham Cormode, Marcel Dall'Agnol, Tom Gur, Chris Hickey</dc:creator>
    </item>
    <item>
      <title>Algorithms and Turing Kernels for Detecting and Counting Small Patterns in Unit Disk Graphs</title>
      <link>https://arxiv.org/abs/2312.06377</link>
      <description>arXiv:2312.06377v2 Announce Type: replace 
Abstract: In this paper we investigate the parameterized complexity of the task of counting and detecting occurrences of small patterns in unit disk graphs: Given an $n$-vertex unit disk graph $G$ with an embedding of ply $p$ (that is, the graph is represented as intersection graph with closed disks of unit size, and each point is contained in at most $p$ disks) and a $k$-vertex unit disk graph $P$, count the number of (induced) copies of $P$ in $G$.
  For general patterns $P$, we give an $2^{O(p k /\log k)}n^{O(1)}$ time algorithm for counting pattern occurrences. We show this is tight, even for ply $p=2$ and $k=n$: any $2^{o(n/\log n)}n^{O(1)}$ time algorithm violates the Exponential Time Hypothesis (ETH).
  For most natural classes of patterns, such as connected graphs and independent sets we present the following results: First, we give an $(pk)^{O(\sqrt{pk})}n^{O(1)}$ time algorithm, which is nearly tight under the ETH for bounded ply and many patterns. Second, for $p= k^{O(1)}$ we provide a Turing kernelization (i.e. we give a polynomial time preprocessing algorithm to reduce the instance size to $k^{O(1)}$).
  Our approach combines previous tools developed for planar subgraph isomorphism such as `efficient inclusion-exclusion' from [Nederlof STOC'20], and `isomorphisms checks' from [Bodlaender et al. ICALP'16] with a different separator hierarchy and a new bound on the number of non-isomorphic separations of small order tailored for unit disk graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.06377v2</guid>
      <category>cs.CC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesper Nederlof, Krisztina Szil\'agyi</dc:creator>
    </item>
    <item>
      <title>Even quantum advice is unlikely to solve PP</title>
      <link>https://arxiv.org/abs/2403.09994</link>
      <description>arXiv:2403.09994v2 Announce Type: replace 
Abstract: We give a corrected proof that if PP $\subseteq$ BQP/qpoly, then the Counting Hierarchy collapses, as originally claimed by [Aaronson 2006 arXiv:cs/0504048]. This recovers the related unconditional claim that PP does not have circuits of any fixed size $n^k$ even with quantum advice. We do so by proving that YQP*, an oblivious version of (QMA $\cap$ coQMA), is contained in APP, and so is PP-low.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09994v2</guid>
      <category>cs.CC</category>
      <category>quant-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Justin Yirka</dc:creator>
    </item>
    <item>
      <title>Datalog-Expressibility for Monadic and Guarded Second-Order Logic</title>
      <link>https://arxiv.org/abs/2010.05677</link>
      <description>arXiv:2010.05677v2 Announce Type: replace-cross 
Abstract: We characterise the sentences in Monadic Second-order Logic (MSO) that are over finite structures equivalent to a Datalog program, in terms of an existential pebble game. We also show that for every class C of finite structures that can be expressed in MSO and is closed under homomorphisms, and for all integers l,k, there exists a canonical Datalog program Pi of width (l,k) in the sense of Feder and Verdi. The same characterisations also hold for Guarded Second-order Logic (GSO), which properly extends MSO. To prove our results, we show that every class C in GSO whose complement is closed under homomorphisms is a finite union of constraint satisfaction problems (CSPs) of countably categorical structures. The intersection of MSO and Datalog is known to contain the class of nested monadically defined queries (Nemodeq); likewise, we show that the intersection of GSO and Datalog contains all problems that can be expressed by the more expressive language of nested guarded queries. Yet, by exploiting our results, we can show that neither of the two query languages can serve as a characterization, as we exhibit a query in the intersection of MSO and Datalog that is not expressible in nested guarded queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2010.05677v2</guid>
      <category>cs.LO</category>
      <category>cs.CC</category>
      <category>math.LO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuel Bodirsky, Simon Kn\"auer, Sebastian Rudolph</dc:creator>
    </item>
    <item>
      <title>TSP Escapes the $O(2^n n^2)$ Curse</title>
      <link>https://arxiv.org/abs/2405.03018</link>
      <description>arXiv:2405.03018v2 Announce Type: replace-cross 
Abstract: The dynamic programming solution to the traveling salesman problem due to Bellman, and independently Held and Karp, runs in time $O(2^n n^2)$, with no improvement in the last sixty years. We break this barrier for the first time by designing an algorithm that runs in deterministic time $2^n n^2 / 2^{\Omega(\sqrt{\log n})}$. We achieve this by strategically remodeling the dynamic programming recursion as a min-plus matrix product, for which faster-than-na\"ive algorithms exist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03018v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mihail Stoian</dc:creator>
    </item>
    <item>
      <title>When Do Low-Rate Concatenated Codes Approach The Gilbert-Varshamov Bound?</title>
      <link>https://arxiv.org/abs/2405.08584</link>
      <description>arXiv:2405.08584v2 Announce Type: replace-cross 
Abstract: The Gilbert--Varshamov (GV) bound is a classical existential result in coding theory. It implies that a random linear binary code of rate $\epsilon^2$ has relative distance at least $\frac{1}{2} - O(\epsilon)$ with high probability. However, it is a major challenge to construct explicit codes with similar parameters.
  One hope to derandomize the Gilbert--Varshamov construction is with code concatenation: We begin with a (hopefully explicit) outer code ${C}_\mathrm{out}$ over a large alphabet, and concatenate that with a small binary random linear code ${C}_\mathrm{in}$. It is known that when we use independent small codes for each coordinate, then the result lies on the GV bound with high probability, but this still uses a lot of randomness. In this paper, we consider the question of whether code concatenation with a single random linear inner code ${C}_\mathrm{in}$ can lie on the GV bound; and if so what conditions on ${C}_\mathrm{out}$ are sufficient for this.
  We show that first, there do exist linear outer codes ${C}_\mathrm{out}$ that are "good" for concatenation in this sense (in fact, most linear codes codes are good). We also provide two sufficient conditions for ${C}_\mathrm{out}$, so that if ${C}_\mathrm{out}$ satisfies these, ${C}_\mathrm{out}\circ {C}_\mathrm{in}$ will likely lie on the GV bound. We hope that these conditions may inspire future work towards constructing explicit codes ${C}_\mathrm{out}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08584v2</guid>
      <category>cs.IT</category>
      <category>cs.CC</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dean Doron, Jonathan Mosheiff, Mary Wootters</dc:creator>
    </item>
  </channel>
</rss>
