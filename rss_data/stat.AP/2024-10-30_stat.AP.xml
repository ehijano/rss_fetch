<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 31 Oct 2024 04:01:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Multicriteria Analysis of Decentralized Wastewater Treatment Technologies for the Philippines</title>
      <link>https://arxiv.org/abs/2410.22484</link>
      <description>arXiv:2410.22484v1 Announce Type: new 
Abstract: This research focuses on decentralized wastewater treatment (DEWAT) technologies for the Philippines that is motivated by the limited suitable wastewater treatment infrastructure in the country. A multi-criteria analysis (MCA), using the Analytic Hierarchy Process (AHP) and Delphi method, was employed to evaluate DEWAT technologies based on life cycle costs and wastewater treatment efficiency parameters such as CODt, BOD5, TSS, NH4-N, TP, and hydraulic retention time. A two-factor Analysis of Variance (ANOVA) without replication was used to assess statistical differences between technologies. The analysis revealed that the Downflow Hanging Sponge (DHS) filter, Multi-Soil Layering (MSL) systems, and Moving Bed Biofilm Reactors (MBBRs) are the top-performing technologies, with no statistically significant differences in their overall performance. The DHS filter ranked highest, excelling in energy efficiency and nutrient removal, making it ideal for resource-scarce environments. MSL systems were noted for their broad-spectrum contaminant removal, while MBBRs demonstrated flexibility and scalability for semi-urban areas. A thorough analysis is carried out for these DEWAT technologies and insights for applicability in the Philippine context are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22484v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Egberto Selerio</dc:creator>
    </item>
    <item>
      <title>Gender disparities in rehospitalisations after coronary artery bypass grafting: evidence from a functional causal mediation analysis of the MIMIC-IV data</title>
      <link>https://arxiv.org/abs/2410.22502</link>
      <description>arXiv:2410.22502v1 Announce Type: new 
Abstract: Hospital readmissions following coronary artery bypass grafting (CABG) not only impose a substantial cost burden on healthcare systems but also serve as a potential indicator of the quality of medical care. Previous studies of gender effects on complications after CABG surgery have consistently revealed that women tend to suffer worse outcomes. To better understand the causal pathway from gender to the number of rehospitalisations, we study the postoperative central venous pressure (CVP), frequently recorded over patients' intensive care unit (ICU) stay after the CABG surgery, as a functional mediator. Confronted with time-varying CVP measurements and zero-inflated rehospitalisation counts within 60 days following discharge, we propose a parameter-simulating quasi-Bayesian Monte Carlo approximation method that accommodates a functional mediator and a zero-inflated count outcome for causal mediation analysis. We find a causal relationship between the female gender and increased rehospitalisation counts after CABG, and that time-varying central venous pressure mediates this causal effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22502v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Henan Xu, Yeying Zhu, Donna L. Coffman</dc:creator>
    </item>
    <item>
      <title>Inaccuracy and divergence measures based on survival extropy, their properties, and applications in testing and image analysis</title>
      <link>https://arxiv.org/abs/2410.22747</link>
      <description>arXiv:2410.22747v1 Announce Type: new 
Abstract: This article introduces novel measures of inaccuracy and divergence based on survival extropy and their dynamic forms and explores their properties and applications. To address the drawbacks of asymmetry and range limitations, we introduce two measures: the survival extropy inaccuracy ratio and symmetric divergence measures. The inaccuracy ratio is utilized for the analysis and classification of images. A goodness-of-fit test for the uniform distribution is developed using the survival extropy divergence. Characterizations of the exponential distribution are derived using the dynamic survival extropy inaccuracy and divergence measures. The article also proposes non-parametric estimators for the divergence measures and conducts simulation studies to validate their performance. Finally, it demonstrates the application of symmetric survival extropy divergence in failure time data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22747v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saranya P., Sunoj S. M</dc:creator>
    </item>
    <item>
      <title>A Bertalanffy-Richards growth model perturbed by a time-dependent pattern, statistical analysis and applications</title>
      <link>https://arxiv.org/abs/2410.22860</link>
      <description>arXiv:2410.22860v1 Announce Type: new 
Abstract: We analyze a modification of the Richards growth model by introducing a time-dependent perturbation in the growth rate. This modification becomes effective at a special switching time, which represents the first-crossing-time of the Richards growth curve through a given constant boundary. The relevant features of the modified growth model are studied and compared with those of the original one. A sensitivity analysis on the switching time is also performed. Then, we define two different stochastic processes, i.e. a non-homogeneous linear birth-death process and a lognormal diffusion process, such that their means identify to the growth curve under investigation. For the diffusion process, we address the problem of parameters estimation through the maximum likelihood method. The estimates are obtained via meta-heuristic algorithms (namely, Simulated Annealing and Ant Lion Optimizer). A simulation study to validate the estimation procedure is also presented, together with a real application to oil production in France. Special attention is devoted to the approximation of switching time density, viewed as the first-passage-time density for the lognormal process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22860v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cnsns.2024.108258</arxiv:DOI>
      <arxiv:journal_reference>Communications in Nonlinear Science and Numerical Simulation, 139,108258, 2024</arxiv:journal_reference>
      <dc:creator>Antonio Di Crescenzo, Paola Paraggio, Francisco Torres-Ruiz</dc:creator>
    </item>
    <item>
      <title>Evolution of global inequality in well-being: A copula-based approach</title>
      <link>https://arxiv.org/abs/2410.22892</link>
      <description>arXiv:2410.22892v1 Announce Type: new 
Abstract: We employ a flexible parametric model to estimate global income, health, and education distributions from 1980 to 2015. Using these marginal distributions within a copula-based framework, we construct a global joint distribution of well-being. This approach allows us to specifically analyze the impact of dependency structures on global well-being inequality. While inequality decreased in each individual dimension, our findings suggest that multidimensional inequality does not necessarily follow this trend. Its evolution is influenced by the interdependence among dimensions and the chosen inequality aversion parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22892v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Koen Decancq, Vanesa Jorda</dc:creator>
    </item>
    <item>
      <title>Log Heston Model for Monthly Average VIX</title>
      <link>https://arxiv.org/abs/2410.22471</link>
      <description>arXiv:2410.22471v1 Announce Type: cross 
Abstract: We model time series of VIX (monthly average) and monthly stock index returns. We use log-Heston model: logarithm of VIX is modeled as an autoregression of order 1. Our main insight is that normalizing monthly stock index returns (dividing them by VIX) makes them much closer to independent identically distributed Gaussian. The resulting model is mean-reverting, and the innovations are non-Gaussian. The combined stochastic volatility model fits well, and captures Pareto-like tails of real-world stock market returns. This works for small and large stock indices, for both price and total returns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22471v1</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jihyun Park, Andrey Sarantsev</dc:creator>
    </item>
    <item>
      <title>The VIX as Stochastic Volatility for Corporate Bonds</title>
      <link>https://arxiv.org/abs/2410.22498</link>
      <description>arXiv:2410.22498v1 Announce Type: cross 
Abstract: Classic stochastic volatility models assume volatility is unobservable. We use the VIX for consider it observable, and use the Volatility Index: S\&amp;P 500 VIX. This index was designed to measure volatility of S\&amp;P 500. We apply it to a different segment: Corporate bond markets. We fit time series models for spreads between corporate and 10-year Treasury bonds. Next, we divide residuals by VIX. Our main idea is such division makes residuals closer to the ideal case of a Gaussian white noise. This is remarkable, since these residuals and VIX come from separate market segments. We conclude with the analysis of long-term behavior of these models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22498v1</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jihyun Park, Andrey Sarantsev</dc:creator>
    </item>
    <item>
      <title>Order of Addition in Orthogonally Blocked Mixture and Component-Amount Designs</title>
      <link>https://arxiv.org/abs/2410.22501</link>
      <description>arXiv:2410.22501v1 Announce Type: cross 
Abstract: Mixture experiments often involve process variables, such as different chemical reactors in a laboratory or varying mixing speeds in a production line. Organizing the runs in orthogonal blocks allows the mixture model to be fitted independently of the process effects, ensuring clearer insights into the role of each mixture component. Current literature on mixture designs in orthogonal blocks ignores the order of addition of mixture components in mixture blends. This paper considers the order of addition of components in mixture and mixture-amount experiments, using the variable total amount taken into orthogonal blocks. The response depends on both the mixture proportions or the amounts of the components and the order of their addition. Mixture designs in orthogonal blocks are constructed to enable the estimation of mixture or component-amount model parameters and the order-of-addition effects. The G-efficiency criterion is used to assess how well the design supports precise and unbiased estimation of the model parameters. The fraction of the Design Space plot is used to provide a visual assessment of the prediction capabilities of a design across the entire design space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22501v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taha Hasan, Touqeer Ahmad</dc:creator>
    </item>
    <item>
      <title>An Iterative Algorithm for Regularized Non-negative Matrix Factorizations</title>
      <link>https://arxiv.org/abs/2410.22698</link>
      <description>arXiv:2410.22698v1 Announce Type: cross 
Abstract: We generalize the non-negative matrix factorization algorithm of Lee and Seung to accept a weighted norm, and to support ridge and Lasso regularization. We recast the Lee and Seung multiplicative update as an additive update which does not get stuck on zero values. We apply the companion R package rnnmf to the problem of finding a reduced rank representation of a database of cocktails.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22698v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Steven E. Pav</dc:creator>
    </item>
    <item>
      <title>Propensity Score Methods for Local Test Score Equating: Stratification and Inverse Probability Weighting</title>
      <link>https://arxiv.org/abs/2410.22989</link>
      <description>arXiv:2410.22989v1 Announce Type: cross 
Abstract: In test equating, ensuring score comparability across different test forms is crucial but particularly challenging when test groups are non-equivalent and no anchor test is available. Local test equating aims to satisfy Lord's equity requirement by conditioning equating transformations on individual-level information, typically using anchor test scores as proxies for latent ability. However, anchor tests are not always available in practice. This paper introduces two novel propensity score-based methods for local equating: stratification and inverse probability weighting (IPW). These methods use covariates to account for group differences, with propensity scores serving as proxies for latent ability differences between test groups. The stratification method partitions examinees into comparable groups based on similar propensity scores, while IPW assigns weights inversely proportional to the probability of group membership. We evaluate these methods through empirical analysis and simulation studies. Results indicate both methods can effectively adjust for group differences, with their relative performance depending on the strength of covariate-ability correlations. The study extends local equating methodology to cases where only covariate information is available, providing testing programs with new tools for ensuring fair score comparability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22989v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Wallin, Marie Wiberg</dc:creator>
    </item>
    <item>
      <title>Theoretical and Practical Limits of Kolmogorov-Zurbenko Periodograms with DiRienzo-Zurbenko Algorithm Smoothing in the Spectral Analysis of Time Series Data</title>
      <link>https://arxiv.org/abs/2007.03031</link>
      <description>arXiv:2007.03031v2 Announce Type: replace 
Abstract: This investigation establishes the theoretical and practical limits of the Kolmogorov-Zurbenko periodogram with DiRienzo-Zurbenko algorithm smoothing with respect to sensitivity (i.e., ability to detect weak signals), accuracy (i.e., ability to correctly identify signal frequencies), resolution (i.e., ability to separate signals with close frequencies), and robustness (i.e., sensitivity, accuracy, and resolution despite high levels of missing data). Compared to standard periodograms that utilize static smoothing with a fixed window width, Kolmogorov-Zurbenko periodograms with DiRienzo-Zurbenko algorithm smoothing utilize dynamic smoothing with a variable window width. This article begins with a summary of its statistical derivation and development followed by instructions for accessing and utilizing this approach within the R statistical program platform. Brief definitions, importance, statistical bases, theoretical and practical limits, and demonstrations are provided for its sensitivity, accuracy, resolution, and robustness. Next using a simulated time series in which two signals close in frequency are embedded in a significant level of random noise, the predictive power of this approach is compared to an autoregressive integral moving average (ARIMA), with support also garnered for its being robust even in the face of a high level of missing data. The article concludes with brief descriptions of studies across a range of scientific disciplines that have capitalized on the power of the Kolmogorov-Zurbenko periodogram with DiRienzo-Zurbenko algorithm smoothing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2007.03031v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barry Loneck, Igor Zurbenko, Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>Low-rank longitudinal factor regression with application to chemical mixtures</title>
      <link>https://arxiv.org/abs/2311.16470</link>
      <description>arXiv:2311.16470v3 Announce Type: replace 
Abstract: Developmental epidemiology commonly focuses on assessing the association between multiple early life exposures and childhood health. Statistical analyses of data from such studies focus on inferring the contributions of individual exposures, while also characterizing time-varying and interacting effects. Such inferences are made more challenging by correlations among exposures, nonlinearity, and the curse of dimensionality. Motivated by studying the effects of prenatal bisphenol A (BPA) and phthalate exposures on glucose metabolism in adolescence using data from the ELEMENT study, we propose a low-rank longitudinal factor regression (LowFR) model for tractable inference on flexible longitudinal exposure effects. LowFR handles highly-correlated exposures using a Bayesian dynamic factor model, which is fit jointly with a health outcome via a novel factor regression approach. The model collapses on simpler and intuitive submodels when appropriate, while expanding to allow considerable flexibility in time-varying and interaction effects when supported by the data. After demonstrating LowFR's effectiveness in simulations, we use it to analyze the ELEMENT data and find that diethyl and dibutyl phthalate metabolite levels in trimesters 1 and 2 are associated with altered glucose metabolism in adolescence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16470v3</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Glenn Palmer, Amy H. Herring, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>The DeepJoint algorithm: An innovative approach for studying the longitudinal evolution of quantitative mammographic density and its association with screen-detected breast cancer risk</title>
      <link>https://arxiv.org/abs/2403.13488</link>
      <description>arXiv:2403.13488v2 Announce Type: replace 
Abstract: Mammographic density is a dynamic risk factor for breast cancer and affects the sensitivity of mammography-based screening. While automated machine and deep learning-based methods provide more consistent and precise measurements compared to subjective BI-RADS assessments, they often fail to account for the longitudinal evolution of density. Many of these methods assess mammographic density in a cross-sectional manner, overlooking correlations in repeated measures, irregular visit intervals, missing data, and informative dropouts. Joint models, however, are well-suited for capturing the longitudinal relationship between biomarkers and survival outcomes. We present the DeepJoint algorithm, an open-source solution that integrates deep learning for quantitative mammographic density estimation with joint modeling to assess the longitudinal relationship between mammographic density and breast cancer risk. Our method efficiently analyzes processed mammograms from various manufacturers, estimating both dense area and percent density--established risk factors for breast cancer. We utilize a joint model to explore their association with breast cancer risk and provide individualized risk predictions. Bayesian inference and the Monte Carlo consensus algorithm make the approach reliable for large screening datasets. Our method allows for accurate analysis of processed mammograms from multiple manufacturers, offering a comprehensive view of breast cancer risk based on individual longitudinal density profiles. The complete pipeline is publicly available, promoting broader application and comparison with other methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13488v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manel Rakez, Julien Guillaumin, Aurelien Chick, Gaelle Coureau, Foucauld Chamming's, Pierre Fillard, Brice Amadeo, Virginie Rondeau</dc:creator>
    </item>
    <item>
      <title>Dynamic prediction of death risk given a renewal hospitalization process</title>
      <link>https://arxiv.org/abs/2406.04849</link>
      <description>arXiv:2406.04849v2 Announce Type: replace-cross 
Abstract: Predicting the risk of death for chronic patients is highly valuable for informed medical decision-making. This paper proposes a general framework for dynamic prediction of the risk of death of a patient given her hospitalization history, which is generally available to physicians. Predictions are based on a joint model for the death and hospitalization processes, thereby avoiding the potential bias arising from selection of survivors. The framework accommodates various submodels for the hospitalization process. In particular, we study prediction of the risk of death in a renewal model for hospitalizations, a common approach to recurrent event modelling. In the renewal model, the distribution of hospitalizations throughout the follow-up period impacts the risk of death. This result differs from prediction in the Poisson model, previously studied, where only the number of hospitalizations matters. We apply our methodology to a prospective, observational cohort study of 512 patients treated for COPD in one of six outpatient respiratory clinics run by the Respiratory Service of Galdakao University Hospital, with a median follow-up of 4.7 years. We find that more concentrated hospitalizations increase the risk of death.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04849v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Telmo J. P\'erez-Izquierdo, Irantzu Barrio, Cristobal Esteban</dc:creator>
    </item>
    <item>
      <title>Gaussian process-based online health monitoring and fault analysis of lithium-ion battery systems from field data</title>
      <link>https://arxiv.org/abs/2406.19015</link>
      <description>arXiv:2406.19015v3 Announce Type: replace-cross 
Abstract: Health monitoring, fault analysis, and detection are critical for the safe and sustainable operation of battery systems. We apply Gaussian process resistance models on lithium iron phosphate battery field data to effectively separate the time-dependent and operating point-dependent resistance. The data set contains 29 battery systems returned to the manufacturer for warranty, each with eight cells in series, totaling 232 cells and 131 million data rows. We develop probabilistic fault detection rules using recursive spatiotemporal Gaussian processes. These processes allow the quick processing of over a million data points, enabling advanced online monitoring and furthering the understanding of battery pack failure in the field. The analysis underlines that often, only a single cell shows abnormal behavior or a knee point, consistent with weakest-link failure for cells connected in series, amplified by local resistive heating. The results further the understanding of how batteries degrade and fail in the field and demonstrate the potential of efficient online monitoring based on data. We open-source the code and publish the large data set upon completion of the review of this article.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19015v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.xcrp.2024.102258</arxiv:DOI>
      <arxiv:journal_reference>Cell Reports Physical Science, 102258 (2024)</arxiv:journal_reference>
      <dc:creator>Joachim Schaeffer, Eric Lenz, Duncan Gulla, Martin Z. Bazant, Richard D. Braatz, Rolf Findeisen</dc:creator>
    </item>
    <item>
      <title>Fundamental properties of linear factor models</title>
      <link>https://arxiv.org/abs/2409.02521</link>
      <description>arXiv:2409.02521v2 Announce Type: replace-cross 
Abstract: We study conditional linear factor models in the context of asset pricing panels. Our analysis focuses on conditional means and covariances to characterize the cross-sectional and inter-temporal properties of returns and factors as well as their interrelationships. We also review the conditions outlined in Kozak and Nagel (2024) and show how the conditional mean-variance efficient portfolio of an unbalanced panel can be spanned by low-dimensional factor portfolios, even without assuming invertibility of the conditional covariance matrices. Our analysis provides a comprehensive foundation for the specification and estimation of conditional linear factor models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02521v2</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Damir Filipovic, Paul Schneider</dc:creator>
    </item>
    <item>
      <title>Research frontiers in ambit stochastics: In memory of Ole E. Barndorff-Nielsen</title>
      <link>https://arxiv.org/abs/2410.00566</link>
      <description>arXiv:2410.00566v2 Announce Type: replace-cross 
Abstract: This article surveys key aspects of ambit stochastics and remembers Ole E. Barndorff-Nielsen's important contributions to the foundation and advancement of this new research field over the last two decades. It also highlights some of the emerging trends in ambit stochastics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00566v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fred Espen Benth, Almut E. D. Veraart</dc:creator>
    </item>
  </channel>
</rss>
