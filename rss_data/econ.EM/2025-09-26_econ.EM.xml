<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 26 Sep 2025 04:01:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Recidivism and Peer Influence with LLM Text Embeddings in Low Security Correctional Facilities</title>
      <link>https://arxiv.org/abs/2509.20634</link>
      <description>arXiv:2509.20634v1 Announce Type: new 
Abstract: We find AI embeddings obtained using a pre-trained transformer-based Large Language Model (LLM) of 80,000-120,000 written affirmations and correction exchanges among residents in low-security correctional facilities to be highly predictive of recidivism. The prediction accuracy is 30\% higher with embedding vectors than with only pre-entry covariates. However, since the text embedding vectors are high-dimensional, we perform Zero-Shot classification of these texts to a low-dimensional vector of user-defined classes to aid interpretation while retaining the predictive power. To shed light on the social dynamics inside the correctional facilities, we estimate peer effects in these LLM-generated numerical representations of language with a multivariate peer effect model, adjusting for network endogeneity. We develop new methodology and theory for peer effect estimation that accommodate sparse networks, multivariate latent variables, and correlated multivariate outcomes. With these new methods, we find significant peer effects in language usage for interaction and feedback.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20634v1</guid>
      <category>econ.EM</category>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shanjukta Nath, Jiwon Hong, Jae Ho Chang, Keith Warren, Subhadeep Paul</dc:creator>
    </item>
    <item>
      <title>Overidentification testing with weak instruments and heteroskedasticity</title>
      <link>https://arxiv.org/abs/2509.21096</link>
      <description>arXiv:2509.21096v1 Announce Type: new 
Abstract: Exogeneity is key for IV estimators, which can assessed via overidentification (OID) tests. We discuss the Kleibergen-Paap (KP) rank test as a heteroskedasticity-robust OID test and compare to the typical J-test. We derive the heteroskedastic weak-instrument limiting distributions for J and KP as special cases of the robust score test estimated via 2SLS and LIML respectively. Monte Carlo simulations show that KP usually performs better than J, which is prone to severe size distortions. Test size depends on model parameters not consistently estimable with weak instruments, so a conservative approach is recommended. This generalises recommendations to use LIML-based OID tests under homoskedasticity. We then revisit the classic problem of estimating the elasticity of intertemporal substitution (EIS) in lifecycle consumption models. Lagged macroeconomic indicators should provide naturally valid but frequently weak instruments. The literature provides a wide range of estimates for this parameter, and J frequently rejects the null of valid instruments. J often rejects the null whereas KP does not; we suggest that J over-rejects, sometimes severely. We argue that KP-test should be used over the J-test. We also argue that instrument invalidity/misspecification is unlikely the cause of the range of EIS estimates in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21096v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stuart Lane, Frank Windmeijer</dc:creator>
    </item>
    <item>
      <title>Inverse Reinforcement Learning Using Just Classification and a Few Regressions</title>
      <link>https://arxiv.org/abs/2509.21172</link>
      <description>arXiv:2509.21172v1 Announce Type: cross 
Abstract: Inverse reinforcement learning (IRL) aims to explain observed behavior by uncovering an underlying reward. In the maximum-entropy or Gumbel-shocks-to-reward frameworks, this amounts to fitting a reward function and a soft value function that together satisfy the soft Bellman consistency condition and maximize the likelihood of observed actions. While this perspective has had enormous impact in imitation learning for robotics and understanding dynamic choices in economics, practical learning algorithms often involve delicate inner-loop optimization, repeated dynamic programming, or adversarial training, all of which complicate the use of modern, highly expressive function approximators like neural nets and boosting. We revisit softmax IRL and show that the population maximum-likelihood solution is characterized by a linear fixed-point equation involving the behavior policy. This observation reduces IRL to two off-the-shelf supervised learning problems: probabilistic classification to estimate the behavior policy, and iterative regression to solve the fixed point. The resulting method is simple and modular across function approximation classes and algorithms. We provide a precise characterization of the optimal solution, a generic oracle-based algorithm, finite-sample error bounds, and empirical results showing competitive or superior performance to MaxEnt IRL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21172v1</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lars van der Laan, Nathan Kallus, Aur\'elien Bibaut</dc:creator>
    </item>
    <item>
      <title>Difference-in-Differences Meets Synthetic Control: Doubly Robust Identification and Estimation</title>
      <link>https://arxiv.org/abs/2503.11375</link>
      <description>arXiv:2503.11375v2 Announce Type: replace 
Abstract: Difference-in-Differences (DiD) and Synthetic Control (SC) are widely used methods for causal inference in panel data, each with distinct strengths and limitations. We propose a novel method for short-panel causal inference that integrates the advantages of both approaches. Our method delivers a doubly robust identification strategy for the average treatment effect on the treated (ATT) under either of two non-nested assumptions: parallel trends or a group-level SC condition. Building on this identification result, we develop a unified semiparametric framework for estimating the ATT. Notably, the identification-robust moment function satisfies Neyman orthogonality under the parallel trends assumption but not under the SC assumption, leading to different asymptotic variances across the two identification strategies. To ensure valid inference, we propose a multiplier bootstrap method that consistently approximates the asymptotic distribution under either assumption. Furthermore, we extend our methodology to accommodate repeated cross-sectional data and staggered treatment designs. As an empirical application, we evaluate the impact of the 2003 minimum wage increase in Alaska on family income. Finally, in simulation studies based on empirically calibrated data-generating processes, we demonstrate that the proposed estimation and inference methods perform well in finite samples under either identification assumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11375v2</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixiao Sun, Haitian Xie, Yuhang Zhang</dc:creator>
    </item>
    <item>
      <title>An Efficient Multi-scale Leverage Effect Estimator under Dependent Microstructure Noise</title>
      <link>https://arxiv.org/abs/2505.08654</link>
      <description>arXiv:2505.08654v2 Announce Type: replace-cross 
Abstract: Estimating the leverage effect from high-frequency data is vital but challenged by complex, dependent microstructure noise, often exhibiting non-Gaussian higher-order moments. This paper introduces a novel multi-scale framework for efficient and robust leverage effect estimation under such flexible noise structures. We develop two new estimators, the Subsampling-and-Averaging Leverage Effect (SALE) and the Multi-Scale Leverage Effect (MSLE), which adapt subsampling and multi-scale approaches holistically using a unique shifted window technique. This design simplifies the multi-scale estimation procedure and enhances noise robustness without requiring the pre-averaging approach. We establish central limit theorems and stable convergence, with MSLE achieving convergence rates of an optimal $n^{-1/4}$ and a near-optimal $n^{-1/9}$ for the noise-free and noisy settings, respectively. A cornerstone of our framework's efficiency is a specifically designed MSLE weighting strategy that leverages covariance structures across scales. This significantly reduces asymptotic variance and, critically, yields substantially smaller finite-sample errors than existing methods under both noise-free and realistic noisy settings. Extensive simulations and empirical analyses confirm the superior efficiency, robustness, and practical advantages of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08654v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyang Xiong, Zhao Chen, Christina Dan Wang</dc:creator>
    </item>
  </channel>
</rss>
