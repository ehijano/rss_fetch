<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Apr 2024 04:00:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 15 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Analyzing the Performance of Large Language Models on Code Summarization</title>
      <link>https://arxiv.org/abs/2404.08018</link>
      <description>arXiv:2404.08018v1 Announce Type: new 
Abstract: Large language models (LLMs) such as Llama 2 perform very well on tasks that involve both natural language and source code, particularly code summarization and code generation. We show that for the task of code summarization, the performance of these models on individual examples often depends on the amount of (subword) token overlap between the code and the corresponding reference natural language descriptions in the dataset. This token overlap arises because the reference descriptions in standard datasets (corresponding to docstrings in large code bases) are often highly similar to the names of the functions they describe. We also show that this token overlap occurs largely in the function names of the code and compare the relative performance of these models after removing function names versus removing code structure. We also show that using multiple evaluation metrics like BLEU and BERTScore gives us very little additional insight since these metrics are highly correlated with each other.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08018v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajarshi Haldar, Julia Hockenmaier</dc:creator>
    </item>
    <item>
      <title>Decentralized FaaS over Multi-Clouds with Blockchain based Management for Supporting Emerging Applications</title>
      <link>https://arxiv.org/abs/2404.08151</link>
      <description>arXiv:2404.08151v1 Announce Type: new 
Abstract: Function-as-a-Service (FaaS) offers a streamlined cloud computing paradigm, but existing centralized systems suffer from vendor lock-in and single points of failure. We propose DeFaaS, a decentralized FaaS system leveraging blockchain technology and decentralized API management. DeFaaS addresses these limitations by establishing a secure, transparent registry of functions on a blockchain and enabling applications to discover and invoke them. This approach fosters scalability, flexibility, enhanced security, and improved reliability. Furthermore, DeFaaS's architecture extends beyond decentralized FaaS, supporting other distributed computing scenarios like dApps, volunteer computing, and multi-cloud service meshes. DeFaaS represents a significant advancement in decentralized computing with the potential to unlock a multitude of novel applications and use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08151v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3605098.3636029</arxiv:DOI>
      <dc:creator>Rabimba Karanjai, Lei Xu, Lin Chen, Nour Diallo, Weidong Shi</dc:creator>
    </item>
    <item>
      <title>A Large Scale Survey of Motivation in Software Development and Analysis of its Validity</title>
      <link>https://arxiv.org/abs/2404.08303</link>
      <description>arXiv:2404.08303v1 Announce Type: new 
Abstract: Context: Motivation is known to improve performance. In software development in particular, there has been considerable interest in the motivation of contributors to open source. Objective: We identify 11 motivators from the literature (enjoying programming, ownership of code, learning, self use, etc.), and evaluate their relative effect on motivation. Since motivation is an internal subjective feeling, we also analyze the validity of the answers. Method: We conducted a survey with 66 questions on motivation which was completed by 521 developers. Most of the questions used an 11 point scale. We evaluated the validity of the answers validity by comparing related questions, comparing to actual behavior on GitHub, and comparison with the same developer in a follow up survey. Results: Validity problems include moderate correlations between answers to related questions, as well as self promotion and mistakes in the answers. Despite these problems, predictive analysis, investigating how diverse motivators influence the probability of high motivation, provided valuable insights. The correlations between the different motivators are low, implying their independence. High values in all 11 motivators predict increased probability of high motivation. In addition, improvement analysis shows that an increase in most motivators predicts an increase in general motivation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08303v1</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Idan Amit, Dror G. Feitelson</dc:creator>
    </item>
    <item>
      <title>Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward</title>
      <link>https://arxiv.org/abs/2404.08517</link>
      <description>arXiv:2404.08517v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) have seen widespread applications across numerous fields, their limited interpretability poses concerns regarding their safe operations from multiple aspects, e.g., truthfulness, robustness, and fairness. Recent research has started developing quality assurance methods for LLMs, introducing techniques such as offline detector-based or uncertainty estimation methods. However, these approaches predominantly concentrate on post-generation analysis, leaving the online safety analysis for LLMs during the generation phase an unexplored area. To bridge this gap, we conduct in this work a comprehensive evaluation of the effectiveness of existing online safety analysis methods on LLMs. We begin with a pilot study that validates the feasibility of detecting unsafe outputs in the early generation process. Following this, we establish the first publicly available benchmark of online safety analysis for LLMs, including a broad spectrum of methods, models, tasks, datasets, and evaluation metrics. Utilizing this benchmark, we extensively analyze the performance of state-of-the-art online safety analysis methods on both open-source and closed-source LLMs. This analysis reveals the strengths and weaknesses of individual methods and offers valuable insights into selecting the most appropriate method based on specific application scenarios and task requirements. Furthermore, we also explore the potential of using hybridization methods, i.e., combining multiple methods to derive a collective safety conclusion, to enhance the efficacy of online safety analysis for LLMs. Our findings indicate a promising direction for the development of innovative and trustworthy quality assurance methodologies for LLMs, facilitating their reliable deployments across diverse domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08517v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuan Xie, Jiayang Song, Zhehua Zhou, Yuheng Huang, Da Song, Lei Ma</dc:creator>
    </item>
    <item>
      <title>Automatic Recommendations for Evolving Relational Databases Schema</title>
      <link>https://arxiv.org/abs/2404.08525</link>
      <description>arXiv:2404.08525v1 Announce Type: new 
Abstract: Relational databases play a central role in many information systems. Their schema contains structural (e.g. tables and columns) and behavioral (e.g. stored procedures or views) entity descriptions. Then, just like for ``normal'' software, changes in legislation, offered functionalities, or functional contexts, impose to evolve databases and their schemas. But in some scenarios, it is not so easy to deconstruct a wished evolution of the schema into a precise sequence of operations. Changing a database schema may impose manually dropping and recreating dependent entities, or manually searching for dependencies in stored procedures. This is important because getting even the order of application of the operators can be difficult and have profound consequences. This meta-model allows us to compute the impact of planned changes and recommend additional changes that will ensure that the RDBMS constraints are always verified. The recommendations can then be compiled into a valid SQL patch actually updating the database schema in an orderly way. We replicated a past evolution showing that, without detailed knowledge of the database, we could perform the same change in 75\% less time than the expert database architect. We also exemplify the use of our approach on other planned changes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08525v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anne Etien, Nicolas Anquetil</dc:creator>
    </item>
    <item>
      <title>Using Information Flow to estimate interference between developers same method contributions</title>
      <link>https://arxiv.org/abs/2404.08619</link>
      <description>arXiv:2404.08619v1 Announce Type: new 
Abstract: This work's main goal is to understand if Information Flow Control (IFC), a security technique used for discovering leaks in software, could be used to indicate the presence of dynamic semantic conflicts between developers contributions in merge scenarios. However, as defining if a dynamic semantic conflict exists involves understanding the expected behaviour of a system, and as such behavioural specifications are often hard to capture, formalize and reason about, we instead try to detect a code level adaptation of the notion of interference from Goguen and Meseguer. We limit our scope to interference caused by developers contributions on the same method. Therefore, we conduct an evaluation to understand if information flow may be used to estimate interference. In particular, we use Java Object-sensitive Analysis (JOANA) to do the IFC for Java programs. JOANA does the IFC of Java programs by using a System Dependence Graph (SDG), a directed graph representing the information flow through a program. Additionally, we bring evidence that information flow between developers same-method contributions occurred for around 64% of the scenarios we evaluated. Finally, we conducted a manual analysis, on 35 scenarios with information flow between developers same-method contributions, to understand the limitations of using information flow to estimate interference between same-method contributions. From the 35 analysed scenarios, for only 15 we considered that an interference in fact existed. We found three different major reasons for detecting information flow and no interference: cases related to the nature of changes, to excessive annotation from our strategy and to the conservativeness of the flows identified by JOANA. We conclude that information flow may be used to estimate interference, but, ideally, the number of false positives should be reduced.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08619v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Souto Maior de Barros Filho, Paulo Borba</dc:creator>
    </item>
    <item>
      <title>A Multi-Expert Large Language Model Architecture for Verilog Code Generation</title>
      <link>https://arxiv.org/abs/2404.08029</link>
      <description>arXiv:2404.08029v1 Announce Type: cross 
Abstract: Recently, there has been a surging interest in using large language models (LLMs) for Verilog code generation. However, the existing approaches are limited in terms of the quality of the generated Verilog code. To address such limitations, this paper introduces an innovative multi-expert LLM architecture for Verilog code generation (MEV-LLM). Our architecture uniquely integrates multiple LLMs, each specifically fine-tuned with a dataset that is categorized with respect to a distinct level of design complexity. It allows more targeted learning, directly addressing the nuances of generating Verilog code for each category. Empirical evidence from experiments highlights notable improvements in terms of the percentage of generated Verilog outputs that are syntactically and functionally correct. These findings underscore the efficacy of our approach, promising a forward leap in the field of automated hardware design through machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08029v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bardia Nadimi, Hao Zheng</dc:creator>
    </item>
    <item>
      <title>LinguaQuanta: Towards a Quantum Transpiler Between OpenQASM and Quipper (Extended)</title>
      <link>https://arxiv.org/abs/2404.08147</link>
      <description>arXiv:2404.08147v1 Announce Type: cross 
Abstract: As quantum computing evolves, many important questions emerge, such as how best to represent quantum programs, and how to promote interoperability between quantum program analysis tools. These questions arise naturally in the design of quantum transpilers, which translate between quantum programming languages. In this paper, we take a step towards answering these questions by identifying challenges and best practices in quantum transpiler design. We base these recommendations on our experience designing LinguaQuanta, a quantum transpiler between Quipper and OpenQASM. First, we provide categorical specifications for quantum transpilers, which aim to encapsulate the core principles of the UNIX philosophy. We then identify quantum circuit decompositions which we expect to be useful in quantum transpilation. With these foundations in place, we then discuss challenges faced during the implementation of LinguaQuanta, such as ancilla management and stability under round translation. To show that LinguaQuanta works in practice, a short tutorial is given for the example of quantum phase estimation. We conclude with recommendations for the future of LinguaQuanta, and for quantum software development tools more broadly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08147v1</guid>
      <category>quant-ph</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Scott Wesley</dc:creator>
    </item>
    <item>
      <title>Prompt-Enhanced Software Vulnerability Detection Using ChatGPT</title>
      <link>https://arxiv.org/abs/2308.12697</link>
      <description>arXiv:2308.12697v2 Announce Type: replace 
Abstract: With the increase in software vulnerabilities that cause significant economic and social losses, automatic vulnerability detection has become essential in software development and maintenance. Recently, large language models (LLMs) like GPT have received considerable attention due to their stunning intelligence, and some studies consider using ChatGPT for vulnerability detection. However, they do not fully consider the characteristics of LLMs, since their designed questions to ChatGPT are simple without a specific prompt design tailored for vulnerability detection. This paper launches a study on the performance of software vulnerability detection using ChatGPT with different prompt designs. Firstly, we complement previous work by applying various improvements to the basic prompt. Moreover, we incorporate structural and sequential auxiliary information to improve the prompt design. Besides, we leverage ChatGPT's ability of memorizing multi-round dialogue to design suitable prompts for vulnerability detection. We conduct extensive experiments on two vulnerability datasets to demonstrate the effectiveness of prompt-enhanced vulnerability detection using ChatGPT. We also analyze the merit and demerit of using ChatGPT for vulnerability detection. Repository: https://github.com/KDEGroup/LLMVulnerabilityDetection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12697v2</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyuan Zhang, Hao Liu, Jiutian Zeng, Kejing Yang, Yuhong Li, Hui Li</dc:creator>
    </item>
    <item>
      <title>On the Model Update Strategies for Supervised Learning in AIOps Solutions</title>
      <link>https://arxiv.org/abs/2311.03213</link>
      <description>arXiv:2311.03213v2 Announce Type: replace 
Abstract: AIOps (Artificial Intelligence for IT Operations) solutions leverage the massive data produced during the operation of large-scale systems and machine learning models to assist software engineers in their system operations. As operation data produced in the field are constantly evolving due to factors such as the changing operational environment and user base, the models in AIOps solutions need to be constantly maintained after deployment. While prior works focus on innovative modeling techniques to improve the performance of AIOps models before releasing them into the field, when and how to update AIOps models remain an under-investigated topic. In this work, we performed a case study on three large-scale public operation data and empirically assessed five different types of model update strategies for supervised learning regarding their performance, updating cost, and stability. We observed that active model update strategies (e.g., periodical retraining, concept drift guided retraining, time-based model ensembles, and online learning) achieve better and more stable performance than a stationary model. Particularly, applying sophisticated model update strategies could provide better performance, efficiency, and stability than simply retraining AIOps models periodically. In addition, we observed that, although some update strategies can save model training time, they significantly sacrifice model testing time, which could hinder their applications in AIOps solutions where the operation data arrive at high pace and volume and where immediate inferences are required. Our findings highlight that practitioners should consider the evolution of operation data and actively maintain AIOps models over time. Our observations can also guide researchers and practitioners in investigating more efficient and effective model update strategies that fit in the context of AIOps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.03213v2</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingzhe Lyu (Jack), Heng Li (Jack), Zhen Ming (Jack),  Jiang, Ahmed E. Hassan</dc:creator>
    </item>
    <item>
      <title>A higher-order transformation approach to the formalization and analysis of BPMN using graph transformation systems</title>
      <link>https://arxiv.org/abs/2311.05243</link>
      <description>arXiv:2311.05243v3 Announce Type: replace 
Abstract: The Business Process Modeling Notation (BPMN) is a widely used standard notation for defining intra- and inter-organizational workflows. However, the informal description of the BPMN execution semantics leads to different interpretations of BPMN elements and difficulties in checking behavioral properties. In this article, we propose a formalization of the execution semantics of BPMN that, compared to existing approaches, covers more BPMN elements while also facilitating property checking. Our approach is based on a higher-order transformation from BPMN models to graph transformation systems. To show the capabilities of our approach, we implemented it as an open-source web-based tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05243v3</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim Kr\"auter, Adrian Rutle, Harald K\"onig, Yngve Lamo</dc:creator>
    </item>
    <item>
      <title>Foundational Competencies and Responsibilities of a Research Software Engineer</title>
      <link>https://arxiv.org/abs/2311.11457</link>
      <description>arXiv:2311.11457v2 Announce Type: replace 
Abstract: The term Research Software Engineer, or RSE, emerged a little over 10 years ago as a way to represent individuals working in the research community but focusing on software development. The term has been widely adopted and there are a number of high-level definitions of what an RSE is. However, the roles of RSEs vary depending on the institutional context they work in. At one end of the spectrum, RSE roles may look similar to a traditional research role. At the other extreme, they resemble that of a software engineer in industry. Most RSE roles inhabit the space between these two extremes. Therefore, providing a straightforward, comprehensive definition of what an RSE does and what experience, skills and competencies are required to become one is challenging. In this community paper we define the broad notion of what an RSE is, explore the different types of work they undertake, and define a list of fundamental competencies as well as values that define the general profile of an RSE. On this basis, we elaborate on the progression of these skills along different dimensions, looking at specific types of RSE roles, proposing recommendations for organisations, and giving examples of future specialisations. An appendix details how existing curricula fit into this framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11457v2</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Goth, Renato Alves, Matthias Braun, Leyla Jael Castro, Gerasimos Chourdakis, Simon Christ, Jeremy Cohen, Fredo Erxleben, Jean-No\"el Grad, Magnus Hagdorn, Toby Hodges, Guido Juckeland, Dominic Kempf, Anna-Lena Lamprecht, Jan Linxweiler, Frank L\"offler, Michele Martone, Moritz Schwarzmeier, Heidi Seibold, Jan Philipp Thiele, Harald von Waldow, Samantha Wittke</dc:creator>
    </item>
    <item>
      <title>QCSHQD: Quantum computing as a service for Hybrid classical-quantum software development: A Vision</title>
      <link>https://arxiv.org/abs/2403.08663</link>
      <description>arXiv:2403.08663v3 Announce Type: replace 
Abstract: Quantum Computing (QC) is transitioning from theoretical frameworks to an indispensable powerhouse of computational capability, resulting in extensive adoption across both industrial and academic domains. QC presents exceptional advantages, including unparalleled processing speed and the potential to solve complex problems beyond the capabilities of classical computers. Nevertheless, academic researchers and industry practitioners encounter various challenges in harnessing the benefits of this technology. The limited accessibility of QC resources for classical developers, and a general lack of domain knowledge and expertise, represent insurmountable barrier, hence to address these challenges, we introduce a framework- Quantum Computing as a Service for Hybrid Classical-Quantum Software Development (QCSHQD), which leverages service-oriented strategies. Our framework comprises three principal components: an Integrated Development Environment (IDE) for user interaction, an abstraction layer dedicated to orchestrating quantum services, and a service provider responsible for executing services on quantum computer. This study presents a blueprint for QCSHQD, designed to democratize access to QC resources for classical developers who want to seamless harness QC power. The vision of QCSHQD paves the way for groundbreaking innovations by addressing key challenges of hybridization between classical and quantum computers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08663v3</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maryam Tavassoli Sabzevari, Matteo Esposito, Arif Ali Khan, Davide Taibi</dc:creator>
    </item>
    <item>
      <title>Few-Shot Cross-System Anomaly Trace Classification for Microservice-based systems</title>
      <link>https://arxiv.org/abs/2403.18998</link>
      <description>arXiv:2403.18998v3 Announce Type: replace 
Abstract: Microservice-based systems (MSS) may experience failures in various fault categories due to their complex and dynamic nature. To effectively handle failures, AIOps tools utilize trace-based anomaly detection and root cause analysis. In this paper, we propose a novel framework for few-shot abnormal trace classification for MSS. Our framework comprises two main components: (1) Multi-Head Attention Autoencoder for constructing system-specific trace representations, which enables (2) Transformer Encoder-based Model-Agnostic Meta-Learning to perform effective and efficient few-shot learning for abnormal trace classification. The proposed framework is evaluated on two representative MSS, Trainticket and OnlineBoutique, with open datasets. The results show that our framework can adapt the learned knowledge to classify new, unseen abnormal traces of novel fault categories both within the same system it was initially trained on and even in the different MSS. Within the same MSS, our framework achieves an average accuracy of 93.26\% and 85.2\% across 50 meta-testing tasks for Trainticket and OnlineBoutique, respectively, when provided with 10 instances for each task. In a cross-system context, our framework gets an average accuracy of 92.19\% and 84.77\% for the same meta-testing tasks of the respective system, also with 10 instances provided for each task. Our work demonstrates the applicability of achieving few-shot abnormal trace classification for MSS and shows how it can enable cross-system adaptability. This opens an avenue for building more generalized AIOps tools that require less system-specific data labeling for anomaly detection and root cause analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18998v3</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuqing Wang, Mika V. M\"antyl\"a, Serge Demeyer, Mutlu Beyazit, Joanna Kisaakye, Jesse Nyyss\"ol\"a</dc:creator>
    </item>
  </channel>
</rss>
