<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.CP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.CP</link>
    <description>q-fin.CP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.CP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Sep 2025 06:52:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Volatility Calibration via Automatic Local Regression</title>
      <link>https://arxiv.org/abs/2509.16334</link>
      <description>arXiv:2509.16334v1 Announce Type: new 
Abstract: Managing exotic derivatives requires accurate mark-to-market pricing and stable Greeks for reliable hedging. The Local Volatility (LV) model distinguishes itself from other pricing models by its ability to match observable market prices across all strikes and maturities with high accuracy. However, LV calibration is fundamentally ill-posed: finite market observables must determine a continuously-defined surface with infinite local volatility parameters. This ill-posed nature often causes spiky LV surfaces that are particularly problematic for finite-difference-based valuation, and induces high-frequency oscillations in solutions, thus leading to unstable Greeks. To address this challenge, we propose a pre-calibration smoothing method that can be integrated seamlessly into any LV calibration workflow. Our method pre-processes market observables using local regression that automatically minimizes asymptotic conditional mean squared error to generate denoised inputs for subsequent LV calibration. Numerical experiments demonstrate that the proposed pre-calibration smoothing yields significantly smoother LV surfaces and greatly improves Greek stability for exotic options with negligible additional computational cost, while preserving the LV model's ability to fit market observables with high fidelity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16334v1</guid>
      <category>q-fin.CP</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruozhong Yang, Hao Qin, Charlie Che, Liming Feng</dc:creator>
    </item>
    <item>
      <title>Analysis of the Impact of an Execution Algorithm with an Order Book Imbalance Strategy on a Financial Market Using an Agent-based Simulation</title>
      <link>https://arxiv.org/abs/2509.16912</link>
      <description>arXiv:2509.16912v1 Announce Type: new 
Abstract: Order book imbalance (OBI) - buy orders minus sell orders near the best quote - measures supply-demand imbalance that can move prices. OBI is positively correlated with returns, and some investors try to use it to improve performance. Large orders placed at once can reveal intent, invite front-running, raise volatility, and cause losses. Execution algorithms therefore split parent orders into smaller lots to limit price distortion. In principle, using OBI inside such algorithms could improve execution, but prior evidence is scarce because isolating OBI's effect in real markets is nearly impossible amid many external factors.
  Multi-agent simulation offers a way to study this. In an artificial market, individual actors are agents whose rules and interactions form the model. This study builds an execution algorithm that accounts for OBI, tests it across several market patterns in artificial markets, and analyzes mechanisms, comparing it with a conventional (OBI-agnostic) algorithm.
  Results: (i) In stable markets, the OBI strategy's performance depends on the number of order slices; outcomes vary with how the parent order is partitioned. (ii) In markets with unstable prices, the OBI-based algorithm outperforms the conventional approach. (iii) Under spoofing manipulation, the OBI strategy is not significantly worse than the conventional algorithm, indicating limited vulnerability to spoofing.
  Overall, OBI provides a useful signal for execution. Incorporating OBI can add value - especially in volatile conditions - while remaining reasonably robust to spoofing; in calm markets, benefits are sensitive to slicing design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16912v1</guid>
      <category>q-fin.CP</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1527/tjsai.39-4_FIN23-I</arxiv:DOI>
      <arxiv:journal_reference>The Journal of the Japanese Society for Artificial Intelligence (Vol. 39, No. 4, 2024)</arxiv:journal_reference>
      <dc:creator>Shuto Endo, Takanobu Mizuta, Isao Yagi</dc:creator>
    </item>
    <item>
      <title>FinFlowRL: An Imitation-Reinforcement Learning Framework for Adaptive Stochastic Control in Finance</title>
      <link>https://arxiv.org/abs/2509.17964</link>
      <description>arXiv:2509.17964v1 Announce Type: new 
Abstract: Traditional stochastic control methods in finance rely on simplifying assumptions that often fail in real world markets. While these methods work well in specific, well defined scenarios, they underperform when market conditions change. We introduce FinFlowRL, a novel framework for financial stochastic control that combines imitation learning with reinforcement learning. The framework first pretrains an adaptive meta policy by learning from multiple expert strategies, then finetunes it through reinforcement learning in the noise space to optimize the generation process. By employing action chunking, that is generating sequences of actions rather than single decisions, it addresses the non Markovian nature of financial markets. FinFlowRL consistently outperforms individually optimized experts across diverse market conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17964v1</guid>
      <category>q-fin.CP</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Li, Zhi Chen, Steve Y. Yang, Ruixun Zhang</dc:creator>
    </item>
    <item>
      <title>Quantum Adaptive Self-Attention for Financial Rebalancing: An Empirical Study on Automated Market Makers in Decentralized Finance</title>
      <link>https://arxiv.org/abs/2509.16955</link>
      <description>arXiv:2509.16955v1 Announce Type: cross 
Abstract: We formulate automated market maker (AMM) \emph{rebalancing} as a binary detection problem and study a hybrid quantum--classical self-attention block, \textbf{Quantum Adaptive Self-Attention (QASA)}. QASA constructs quantum queries/keys/values via variational quantum circuits (VQCs) and applies standard softmax attention over Pauli-$Z$ expectation vectors, yielding a drop-in attention module for financial time-series decision making. Using daily data for \textbf{BTCUSDC} over \textbf{Jan-2024--Jan-2025} with a 70/15/15 time-series split, we compare QASA against classical ensembles, a transformer, and pure quantum baselines under Return, Sharpe, and Max Drawdown. The \textbf{QASA-Sequence} variant attains the \emph{best single-model risk-adjusted performance} (\textbf{13.99\%} return; \textbf{Sharpe 1.76}), while hybrid models average \textbf{11.2\%} return (vs.\ 9.8\% classical; 4.4\% pure quantum), indicating a favorable performance--stability--cost trade-off.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16955v1</guid>
      <category>quant-ph</category>
      <category>cs.LG</category>
      <category>q-fin.CP</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chi-Sheng Chen, Aidan Hung-Wen Tsai</dc:creator>
    </item>
    <item>
      <title>Alpha-GPT: Human-AI Interactive Alpha Mining for Quantitative Investment</title>
      <link>https://arxiv.org/abs/2308.00016</link>
      <description>arXiv:2308.00016v2 Announce Type: replace 
Abstract: One of the most important tasks in quantitative investment research is mining new alphas (effective trading signals or factors). Traditional alpha mining methods, either hand-crafted factor synthesizing or algorithmic factor mining (e.g., search with genetic programming), have inherent limitations, especially in implementing the ideas of quants. In this work, we propose a new alpha mining paradigm by introducing human-AI interaction, and a novel prompt engineering algorithmic framework to implement this paradigm by leveraging the power of large language models. Moreover, we develop Alpha-GPT, a new interactive alpha mining system framework that provides a heuristic way to ``understand'' the ideas of quant researchers and outputs creative, insightful, and effective alphas. We demonstrate the effectiveness and advantage of Alpha-GPT via a number of alpha mining experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.00016v2</guid>
      <category>q-fin.CP</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saizhuo Wang, Hang Yuan, Leon Zhou, Lionel M. Ni, Heung-Yeung Shum, Jian Guo</dc:creator>
    </item>
    <item>
      <title>Robust Reinforcement Learning with Dynamic Distortion Risk Measures</title>
      <link>https://arxiv.org/abs/2409.10096</link>
      <description>arXiv:2409.10096v3 Announce Type: replace-cross 
Abstract: In a reinforcement learning (RL) setting, the agent's optimal strategy heavily depends on her risk preferences and the underlying model dynamics of the training environment. These two aspects influence the agent's ability to make well-informed and time-consistent decisions when facing testing environments. In this work, we devise a framework to solve robust risk-aware RL problems where we simultaneously account for environmental uncertainty and risk with a class of dynamic robust distortion risk measures. Robustness is introduced by considering all models within a Wasserstein ball around a reference model. We estimate such dynamic robust risk measures using neural networks by making use of strictly consistent scoring functions, derive policy gradient formulae using the quantile representation of distortion risk measures, and construct an actor-critic algorithm to solve this class of robust risk-aware RL problems. We demonstrate the performance of our algorithm on a portfolio allocation example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10096v3</guid>
      <category>cs.LG</category>
      <category>q-fin.CP</category>
      <category>q-fin.PM</category>
      <category>q-fin.RM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anthony Coache, Sebastian Jaimungal</dc:creator>
    </item>
  </channel>
</rss>
