<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Dec 2025 02:39:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Iterated sampling importance resampling with adaptive number of proposals</title>
      <link>https://arxiv.org/abs/2512.00220</link>
      <description>arXiv:2512.00220v1 Announce Type: new 
Abstract: Iterated sampling importance resampling (i-SIR) is a Markov chain Monte Carlo (MCMC) algorithm which is based on $N$ independent proposals. As $N$ grows, its samples become nearly independent, but with an increased computational cost. We discuss a method which finds an approximately optimal number of proposals $N$ in terms of the asymptotic efficiency. The optimal $N$ depends on both the mixing properties of the i-SIR chain and the (parallel) computing costs. Our method for finding an appropriate $N$ is based on an approximate asymptotic variance of the i-SIR, which has similar properties as the i-SIR asymptotic variance, and a generalised i-SIR transition having fractional `number of proposals.' These lead to an adaptive i-SIR algorithm, which tunes the number of proposals automatically during sampling. Our experiments demonstrate that our approximate efficiency and the adaptive i-SIR algorithm have promising empirical behaviour. We also present new theoretical results regarding the i-SIR, such as the convexity of asymptotic variance in the number of proposals, which can be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00220v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pietari Laitinen, Matti Vihola</dc:creator>
    </item>
    <item>
      <title>Yet Another Smacof -- Square Symmetric Case</title>
      <link>https://arxiv.org/abs/2512.00232</link>
      <description>arXiv:2512.00232v1 Announce Type: new 
Abstract: We rewrite the metric/nonmetric and weighted/unweighted versions of the smacof program for square symmetric data as one monolithic C program. R is used for taking care of the data and parameter setup, the I/O, and of issuing a single call to .C() to start the computations. This makes this new smacofSS() program five to fifty times as fast (for our examples) as the smacofSym() function from the R smacof package. Utilities for various initial configurations and plots are included in the package. Examples are included to compare output and time for the R and C versions and to illustrate the various plots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00232v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jan de Leeuw</dc:creator>
    </item>
    <item>
      <title>An hybrid stochastic Newton algorithm for logistic regression</title>
      <link>https://arxiv.org/abs/2512.01790</link>
      <description>arXiv:2512.01790v1 Announce Type: new 
Abstract: In this paper, we investigate a second-order stochastic algorithm for solving large-scale binary classification problems. We propose to make use of a new hybrid stochastic Newton algorithm that includes two weighted components in the Hessian matrix estimation: the first one coming from the natural Hessian estimate and the second associated with the stochastic gradient information. Our motivation comes from the fact that both parts evaluated at the true parameter of logistic regression, are equal to the Hessian matrix. This new formulation has several advantages and it enables us to prove the almost sure convergence of our stochastic algorithm to the true parameter. Moreover, we significantly improve the almost sure rate of convergence to the Hessian matrix. Furthermore, we establish the central limit theorem for our hybrid stochastic Newton algorithm. Finally, we show a surprising result on the almost sure convergence of the cumulative excess risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01790v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bernard Bercu, Luis Fredes, Em\'eric Gbaguidi</dc:creator>
    </item>
    <item>
      <title>R\'enyi's $\alpha$-divergence variational Bayes for spike-and-slab high-dimensional linear regression</title>
      <link>https://arxiv.org/abs/2512.00627</link>
      <description>arXiv:2512.00627v1 Announce Type: cross 
Abstract: Sparse high-dimensional linear regression is a central problem in statistics, where the goal is often variable selection and/or coefficient estimation. We propose a mean-field variational Bayes approximation for sparse regression with spike-and-slab Laplace priors that replaces the standard Kullback-Leibler (KL) divergence objective with the R\'enyi's $\alpha$ divergence: a one-parameter generalization of KL divergence indexed by $\alpha \in (0, \infty) \setminus \{1\}$ that allows flexibility between zero-forcing and mass-covering behavior. We derive coordinate ascent variational inference (CAVI) updates via a second-order delta method and develop a stochastic variational inference algorithm based on a Monte Carlo surrogate R\'enyi lower bound. In simulations, our two methods perform comparably to state-of-the-art Bayesian variable selection procedures across a range of sparsity configurations and $\alpha$ values for both variable selection and estimation, and our numerical results illustrate how different choices of $\alpha$ can be advantageous in different sparsity configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00627v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chadi Bsila, Yiqi Tang, Kaiwen Wang, Laurie Heyer</dc:creator>
    </item>
    <item>
      <title>ARCADIA: Scalable Causal Discovery for Corporate Bankruptcy Analysis Using Agentic AI</title>
      <link>https://arxiv.org/abs/2512.00839</link>
      <description>arXiv:2512.00839v1 Announce Type: cross 
Abstract: This paper introduces ARCADIA, an agentic AI framework for causal discovery that integrates large-language-model reasoning with statistical diagnostics to construct valid, temporally coherent causal structures. Unlike traditional algorithms, ARCADIA iteratively refines candidate DAGs through constraint-guided prompting and causal-validity feedback, leading to stable and interpretable models for real-world high-stakes domains. Experiments on corporate bankruptcy data show that ARCADIA produces more reliable causal graphs than NOTEARS, GOLEM, and DirectLiNGAM while offering a fully explainable, intervention-ready pipeline. The framework advances AI by demonstrating how agentic LLMs can participate in autonomous scientific modeling and structured causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00839v1</guid>
      <category>cs.AI</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabrizio Maturo, Donato Riccio, Andrea Mazzitelli, Giuseppe Bifulco, Francesco Paolone, Iulia Brezeanu</dc:creator>
    </item>
    <item>
      <title>A Scalable Variational Bayes Approach for Fitting Non-Conjugate Spatial Generalized Linear Mixed Models via Basis Expansions</title>
      <link>https://arxiv.org/abs/2512.00895</link>
      <description>arXiv:2512.00895v1 Announce Type: cross 
Abstract: Large spatial datasets with non-Gaussian responses are increasingly common in environmental monitoring, ecology, and remote sensing, yet scalable Bayesian inference for such data remains challenging. Markov chain Monte Carlo (MCMC) methods are often prohibitive for large datasets, and existing variational Bayes methods rely on conjugacy or strong approximations that limit their applicability and can underestimate posterior variances. We propose a scalable variational framework that incorporates semi-implicit variational inference (SIVI) with basis representations of spatial generalized linear mixed models (SGLMMs), which may not have conjugacy. Our approach accommodates gamma, negative binomial, Poisson, Bernoulli, and Gaussian responses on continuous spatial domains. Across 20 simulation scenarios with 50,000 locations, SIVI achieves predictive accuracy and posterior distributions comparable to Metropolis-Hastings and Hamiltonian Monte Carlo while providing notable computational speedups. Applications to MODIS land surface temperature and Blue Jay abundance further demonstrate the utility of the approach for large non-Gaussian spatial datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00895v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin Hyung Lee, Ben Seiyon Lee</dc:creator>
    </item>
    <item>
      <title>pvEBayes: An R Package for Empirical Bayes Methods in Pharmacovigilance</title>
      <link>https://arxiv.org/abs/2512.01057</link>
      <description>arXiv:2512.01057v1 Announce Type: cross 
Abstract: Monitoring the safety of medical products is a core concern of contemporary pharmacovigilance. To support drug safety assessment, Spontaneous Reporting Systems (SRS) collect reports of suspected adverse events of approved medical products offering a critical resource for identifying potential safety concerns that may not emerge during clinical trials. Modern nonparametric empirical Bayes methods are flexible statistical approaches that can accurately identify and estimate the strength of the association between an adverse event and a drug from SRS data. However, there is currently no comprehensive and easily accessible implementation of these methods. Here, we introduce the R package pvEBayes, which implements a suite of nonparametric empirical Bayes methods for pharmacovigilance, along with post-processing tools and graphical summaries for streamlining the application of these methods. Detailed examples are provided to demonstrate the application of the package through analyses of two real-world SRS datasets curated from the publicly available FDA FAERS database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01057v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihao Tan, Marianthi Markatou, Saptarshi Chakraborty</dc:creator>
    </item>
    <item>
      <title>Discriminative classification with generative features: bridging Naive Bayes and logistic regression</title>
      <link>https://arxiv.org/abs/2512.01097</link>
      <description>arXiv:2512.01097v1 Announce Type: cross 
Abstract: We introduce Smart Bayes, a new classification framework that bridges generative and discriminative modeling by integrating likelihood-ratio-based generative features into a logistic-regression-style discriminative classifier. From the generative perspective, Smart Bayes relaxes the fixed unit weights of Naive Bayes by allowing data-driven coefficients on density-ratio features. From a discriminative perspective, it constructs transformed inputs as marginal log-density ratios that explicitly quantify how much more likely each feature value is under one class than another, thereby providing predictors with stronger class separation than the raw covariates. To support this framework, we develop a spline-based estimator for univariate log-density ratios that is flexible, robust, and computationally efficient. Through extensive simulations and real-data studies, Smart Bayes often outperforms both logistic regression and Naive Bayes. Our results highlight the potential of hybrid approaches that exploit generative structure to enhance discriminative performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01097v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zachary Terner, Alexander Petersen, Yuedong Wang</dc:creator>
    </item>
    <item>
      <title>Randomized-Accelerated FEAST: A Hybrid Approach for Large-Scale Eigenvalue Problems</title>
      <link>https://arxiv.org/abs/2512.01257</link>
      <description>arXiv:2512.01257v1 Announce Type: cross 
Abstract: We present Randomized-Accelerated FEAST (RA-FEAST), a hybrid algorithm that combines contour-integration-based eigensolvers with randomized numerical linear algebra techniques for efficiently computing partial eigendecompositions of large-scale matrices arising in statistical applications. By incorporating randomized subspace initialization to enable aggressive quadrature reduction and truncated refinement iterations, our method achieves significant computational speedups (up to 38x on sparse graph Laplacian benchmarks at n = 8000) while maintaining high-accuracy approximations to the target eigenspace. We provide a probabilistic error bound for the randomized warmstart, a stability result for inexact FEAST iterations under general perturbations, and a simple complexity model characterizing the trade-off between initialization cost and solver speedup. Empirically, we demonstrate that RA-FEAST can be more than an order of magnitude faster than standard FEAST while preserving accuracy on sparse Laplacian problems representative of modern spectral methods in statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01257v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ayush Nadiger (University of Massachusetts Amherst, Departments of Mathematics,Statistics,Electrical,Computer Engineering)</dc:creator>
    </item>
    <item>
      <title>Detecting Model Misspecification in Bayesian Inverse Problems via Variational Gradient Descent</title>
      <link>https://arxiv.org/abs/2512.01667</link>
      <description>arXiv:2512.01667v1 Announce Type: cross 
Abstract: Bayesian inference is optimal when the statistical model is well-specified, while outside this setting Bayesian inference can catastrophically fail; accordingly a wealth of post-Bayesian methodologies have been proposed. Predictively oriented (PrO) approaches lift the statistical model $P_\theta$ to an (infinite) mixture model $\int P_\theta \; \mathrm{d}Q(\theta)$ and fit this predictive distribution via minimising an entropy-regularised objective functional. In the well-specified setting one expects the mixing distribution $Q$ to concentrate around the true data-generating parameter in the large data limit, while such singular concentration will typically not be observed if the model is misspecified. Our contribution is to demonstrate that one can empirically detect model misspecification by comparing the standard Bayesian posterior to the PrO `posterior' $Q$. To operationalise this, we present an efficient numerical algorithm based on variational gradient descent. A simulation study, and a more detailed case study involving a Bayesian inverse problem in seismology, confirm that model misspecification can be automatically detected using this framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01667v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qingyang Liu, Matthew A. Fisher, Zheyang Shen, Katy Tant, Xuebin Zhao, Andrew Curtis, Chris. J. Oates</dc:creator>
    </item>
    <item>
      <title>Bayesian Statistical Inversion for High-Dimensional Computer Model Output and Spatially Distributed Counts</title>
      <link>https://arxiv.org/abs/2512.01927</link>
      <description>arXiv:2512.01927v1 Announce Type: cross 
Abstract: Data collected by the Interstellar Boundary Explorer (IBEX) satellite, recording heliospheric energetic neutral atoms (ENAs), exhibit a phenomenon that has caused space scientists to revise hypotheses about the physical processes, and computer simulations under those models, in play at the boundary of our solar system. Evaluating the fit of these computer models involves tuning their parameters to observational data from IBEX. This would be a classic (Bayesian) inverse problem if not for three challenges: (1) the computer simulations are slow, limiting the size of campaigns of runs; so (2) surrogate modeling is essential, but outputs are high-resolution images, thwarting conventional methods; and (3) IBEX observations are counts, whereas most inverse problem techniques assume Gaussian field data. To fill that gap we propose a novel approach to Bayesian inverse problems coupling a Poisson response with a sparse Gaussian process surrogate using the Vecchia approximation. We demonstrate the capabilities of our proposed framework, which compare favorably to alternatives, through multiple simulated examples in terms of recovering "true" computer model parameters and accurate out-of-sample prediction. We then apply this new technology to IBEX satellite data and associated computer models developed at Los Alamos National Laboratory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01927v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Steven D. Barnett, Robert B. Gramacy, Lauren J. Beesley, Dave Osthus, Yifan Huang, Fan Guo, Daniel B. Reisenfeld</dc:creator>
    </item>
    <item>
      <title>Dimension-independent convergence rates of randomized nets using median-of-means</title>
      <link>https://arxiv.org/abs/2505.13815</link>
      <description>arXiv:2505.13815v3 Announce Type: replace 
Abstract: Recent advances in quasi-Monte Carlo integration demonstrate that the median of linearly scrambled digital net estimators achieves near-optimal convergence rates for high-dimensional integrals without requiring a priori knowledge of the integrand's smoothness. Building on this framework, we prove that the median estimator attains dimension-independent convergence, a property known as strong tractability in complexity theory, under tractability conditions characterized by low effective dimensionality. Using a probabilistic, integrand-specific error criterion, our analysis establishes both faster and dimension-independent convergence under weaker assumptions than previously possible in the worst-case setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13815v3</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zexin Pan</dc:creator>
    </item>
    <item>
      <title>Convergence of a Sequential Monte Carlo algorithm towards multimodal distributions on Rd</title>
      <link>https://arxiv.org/abs/2511.22564</link>
      <description>arXiv:2511.22564v2 Announce Type: replace 
Abstract: In an earlier joint work, we studied a sequential Monte Carlo algorithm to sample from the Gibbs measure supported on torus with a non-convex energy function at a low temperature, where we proved that the time complexity of the algorithm is polynomial in the inverse temperature. However, the analysis in that torus setting relied crucially on compactness and does not directly extend to unbounded domains. This work introduces a new approach that resolves this issue and establishes a similar result for sampling from Gibbs measures supported on Rd.
  In particular, our main result shows that when the energy function is double-well with equal depth, the time complexity scales as seventh power of the inverse temperature, and quadratically in both the inverse allowed absolute error and probability error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22564v2</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiyu Han</dc:creator>
    </item>
    <item>
      <title>Computational Approaches for Exponential-Family Factor Analysis</title>
      <link>https://arxiv.org/abs/2403.14925</link>
      <description>arXiv:2403.14925v3 Announce Type: replace-cross 
Abstract: We study a general factor analysis framework where the $n$-by-$p$ data matrix is assumed to follow a general exponential family distribution entry-wise. While this model framework has been proposed before, we here further relax its distributional assumption by using a quasi-likelihood setup. By parameterizing the mean-variance relationship on data entries, we additionally introduce a dispersion parameter and entry-wise weights to model large variations and missing values. The resulting model is thus not only robust to distribution misspecification but also more flexible and able to capture mean-dependent covariance structures of the data matrix. Our main focus is on efficient computational approaches to perform the factor analysis. Previous modeling frameworks rely on simulated maximum likelihood (SML) to find the factorization solution, but this method was shown to lead to asymptotic bias when the simulated sample size grows slower than the square root of the sample size $n$, eliminating its practical application for data matrices with large $n$. Borrowing from expectation-maximization (EM) and stochastic gradient descent (SGD), we investigate three estimation procedures based on iterative factorization updates. Our proposed solution does not show asymptotic biases, and scales even better for large matrix factorizations with error $O(1/p)$. To support our findings, we conduct simulation experiments and discuss its application in four case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14925v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Liang Wang, Luis Carvalho</dc:creator>
    </item>
    <item>
      <title>From Poisson Observations to Fitted Negative Binomial Distribution</title>
      <link>https://arxiv.org/abs/2404.07457</link>
      <description>arXiv:2404.07457v3 Announce Type: replace-cross 
Abstract: The negative binomial distribution has been widely used as a more flexible model than the Poisson distribution for count data. However, when the true data-generating process is Poisson, it is often challenging to distinguish it from a negative binomial distribution with extreme parameter values, and existing maximum likelihood estimation procedures for the negative binomial distribution may fail or produce unstable estimates. To address this issue, we develop a new algorithm for computing the maximum likelihood estimate of negative binomial parameters, which is more efficient and more accurate than existing methods. We further extend negative binomial distributions with a new parameterization to cover Poisson distributions as a special class. We provide theoretical justifications showing that, when applied to a Poisson data, the estimated parameters of the extended negative binomial distribution can consistently recover the true Poisson distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07457v3</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingying Yang, Niloufar Dousti Mousavi, Zhou Yu, Jie Yang</dc:creator>
    </item>
    <item>
      <title>The Performance Of The Unadjusted Langevin Algorithm Without Smoothness Assumptions</title>
      <link>https://arxiv.org/abs/2502.03458</link>
      <description>arXiv:2502.03458v4 Announce Type: replace-cross 
Abstract: In this article, we study the problem of sampling from distributions whose densities are not necessarily smooth nor logconcave. We propose a simple Langevin-based algorithm that does not rely on popular but computationally challenging techniques, such as the Moreau-Yosida envelope or Gaussian smoothing, and show consequently that the performance of samplers like ULA does not necessarily degenerate arbitrarily with low regularity. In particular, we show that the Lipschitz or H\"older continuity assumption can be replaced by a geometric one-sided Lipschitz condition that allows even for discontinuous log-gradients. We derive non-asymptotic guarantees for the convergence of the algorithm to the target distribution in Wasserstein distances. Non-asymptotic bounds are also provided for the performance of the algorithm as an optimizer, specifically for the solution of associated excess risk optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03458v4</guid>
      <category>stat.ML</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim Johnston, Iosif Lytras, Nikolaos Makras, Sotirios Sabanis</dc:creator>
    </item>
    <item>
      <title>hdMTD: An R Package for High-Dimensional Mixture Transition Distribution Models</title>
      <link>https://arxiv.org/abs/2509.01808</link>
      <description>arXiv:2509.01808v2 Announce Type: replace-cross 
Abstract: Several natural phenomena exhibit long-range conditional dependencies. High-order mixture transition distribution (MTD) are parsimonious non-parametric models to study these phenomena. An MTD is a Markov chain in which the transition probabilities are expressed as a convex combination of lower-order conditional distributions. Despite their generality, inference for MTD models has traditionally been limited by the need to estimate high-dimensional joint distributions. In particular, for a sample of size n, the feasible order d of the MTD is typically restricted to d approximately O(log n). To overcome this limitation, Ost and Takahashi (2023) recently introduced a computationally efficient non-parametric inference method that identifies the relevant lags in high-order MTD models, even when d is approximately O(n), provided that the set of relevant lags is sparse. In this article, we introduce hdMTD, an R package allowing us to estimate parameters of such high-dimensional Markovian models. Given a sample from an MTD chain, hdMTD can retrieve the relevant past set using the BIC algorithm or the forward stepwise and cut algorithm described in Ost and Takahashi (2023). The package also computes the maximum likelihood estimate for transition probabilities and estimates high-order MTD parameters through the expectation-maximization algorithm. Additionally, hdMTD also allows for simulating an MTD chain from its stationary invariant distribution using the perfect (exact) sampling algorithm, enabling Monte Carlo simulation of the model. We illustrate the package's capabilities through simulated data and a real-world application involving temperature records from Brazil.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01808v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maiara Gripp, Giulio Iacobelli, Guilherme Ost, Daniel Y. Takahashi</dc:creator>
    </item>
    <item>
      <title>Scalable approximation of the transformation-free linear simplicial-simplicial regression via constrained iterative reweighted least squares</title>
      <link>https://arxiv.org/abs/2511.13296</link>
      <description>arXiv:2511.13296v3 Announce Type: replace-cross 
Abstract: Simplicial-simplicial regression refers to the regression setting where both the responses and predictor variables lie within the simplex space, i.e. they are compositional. \cite{fiksel2022} proposed a transformation-free linear regression model, that minimizes the Kullback-Leibler divergence from the observed to the fitted compositions, where the EM algorithm is used to estimate the regression coefficients. We formulate the model as a constrained logistic regression, in the spirit of \cite{tsagris2025}, and we estimate the regression coefficients using constrained iteratively reweighted least squares. The simulation studies depict that this algorithm makes the estimation procedure significantly faster, and approximates accurately enough the solution of the EM algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13296v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michail Tsagris</dc:creator>
    </item>
  </channel>
</rss>
