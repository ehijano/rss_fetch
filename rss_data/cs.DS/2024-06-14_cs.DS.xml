<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Jun 2024 04:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Approximating Maximum Matching Requires Almost Quadratic Time</title>
      <link>https://arxiv.org/abs/2406.08595</link>
      <description>arXiv:2406.08595v1 Announce Type: new 
Abstract: We study algorithms for estimating the size of maximum matching. This problem has been subject to extensive research. For $n$-vertex graphs, Bhattacharya, Kiss, and Saranurak [FOCS'23] (BKS) showed that an estimate that is within $\varepsilon n$ of the optimal solution can be achieved in $n^{2-\Omega_\varepsilon(1)}$ time, where $n$ is the number of vertices. While this is subquadratic in $n$ for any fixed $\varepsilon &gt; 0$, it gets closer and closer to the trivial $\Theta(n^2)$ time algorithm that reads the entire input as $\varepsilon$ is made smaller and smaller.
  In this work, we close this gap and show that the algorithm of BKS is close to optimal. In particular, we prove that for any fixed $\delta &gt; 0$, there is another fixed $\varepsilon = \varepsilon(\delta) &gt; 0$ such that estimating the size of maximum matching within an additive error of $\varepsilon n$ requires $\Omega(n^{2-\delta})$ time in the adjacency list model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08595v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soheil Behnezhad, Mohammad Roghani, Aviad Rubinstein</dc:creator>
    </item>
    <item>
      <title>A Sublinear Algorithm for Approximate Shortest Paths in Large Networks</title>
      <link>https://arxiv.org/abs/2406.08624</link>
      <description>arXiv:2406.08624v1 Announce Type: new 
Abstract: Computing distances and finding shortest paths in massive real-world networks is a fundamental algorithmic task in network analysis. There are two main approaches to solving this task. On one hand are traversal-based algorithms like bidirectional breadth-first search (BiBFS) with no preprocessing step and slow individual distance inquiries. On the other hand are indexing-based approaches, which maintain a large index. This allows for answering individual inquiries very fast; however, index creation is prohibitively expensive. We seek to bridge these two extremes: quickly answer distance inquiries without the need for costly preprocessing.
  In this work, we propose a new algorithm and data structure, WormHole, for approximate shortest path computations. WormHole leverages structural properties of social networks to build a sublinearly sized index, drawing upon the explicit core-periphery decomposition of Ben-Eliezer et al. Empirically, the preprocessing time of WormHole improves upon index-based solutions by orders of magnitude, and individual inquiries are consistently much faster than in BiBFS. The acceleration comes at the cost of a minor accuracy trade-off. Nonetheless, our empirical evidence demonstrates that WormHole accurately answers essentially all inquiries within a maximum additive error of 2. We complement these empirical results with provable theoretical guarantees, showing that WormHole requires $n^{o(1)}$ node queries per distance inquiry in random power-law networks. In contrast, any approach without a preprocessing step requires $n^{\Omega(1)}$ queries for the same task.
  WormHole does not require reading the whole graph. Unlike the vast majority of index-based algorithms, it returns paths, not just distances. For faster inquiry times, it can be combined effectively with other index-based solutions, by running them only on the sublinear core.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08624v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sabyasachi Basu, Nadia K\=oshima, Talya Eden, Omri Ben-Eliezer, C. Seshadhri</dc:creator>
    </item>
    <item>
      <title>Matching with Nested and Bundled Pandora Boxes</title>
      <link>https://arxiv.org/abs/2406.08711</link>
      <description>arXiv:2406.08711v1 Announce Type: new 
Abstract: We consider max-weighted matching with costs for learning the weights, modeled as a "Pandora's Box" on each endpoint of an edge. Each vertex has an initially-unknown value for being matched to a neighbor, and an algorithm must pay some cost to observe this value. The goal is to maximize the total matched value minus costs. Our model is inspired by two-sided settings, such as matching employees to employers. Importantly for such settings, we allow for negative values which cause existing approaches to fail.
  We first prove upper bounds for algorithms in two natural classes. Any algorithm that "bundles" the two Pandora boxes incident to an edge is an $o(1)$-approximation. Likewise, any "vertex-based" algorithm, which uses properties of the separate Pandora's boxes but does not consider the interaction of their value distributions, is an $o(1)$-approximation. Instead, we utilize Pandora's Nested-Box Problem, i.e. multiple stages of inspection. We give a self-contained, fully constructive optimal solution to the nested-boxes problem, which may have structural observations of interest compared to prior work. By interpreting each edge as a nested box, we leverage this solution to obtain a constant-factor approximation algorithm. Finally, we show any ``edge-based'' algorithm, which considers the interactions of values along an edge but not with the rest of the graph, is also an $o(1)$-approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08711v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robin Bowers, Bo Waggoner</dc:creator>
    </item>
    <item>
      <title>The Behavior of Tree-Width and Path-Width under Graph Operations and Graph Transformations</title>
      <link>https://arxiv.org/abs/2406.08985</link>
      <description>arXiv:2406.08985v1 Announce Type: new 
Abstract: Tree-width and path-width are well-known graph parameters. Many NP-hard graph problems allow polynomial-time solutions, when restricted to graphs of bounded tree-width or bounded path-width. In this work, we study the behavior of tree-width and path-width under various unary and binary graph transformations. Doing so, for considered transformations we provide upper and lower bounds for the tree-width and path-width of the resulting graph in terms of the tree-width and path-width of the initial graphs or argue why such bounds are impossible to specify. Among the studied, unary transformations are vertex addition, vertex deletion, edge addition, edge deletion, subgraphs, vertex identification, edge contraction, edge subdivision, minors, powers of graphs, line graphs, edge complements, local complements, Seidel switching, and Seidel complementation. Among the studied, binary transformations we consider the disjoint union, join, union, substitution, graph product, 1-sum, and corona of two graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08985v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Frank Gurski, Robin Weishaupt</dc:creator>
    </item>
    <item>
      <title>Dynamic Correlation Clustering in Sublinear Update Time</title>
      <link>https://arxiv.org/abs/2406.09137</link>
      <description>arXiv:2406.09137v1 Announce Type: new 
Abstract: We study the classic problem of correlation clustering in dynamic node streams. In this setting, nodes are either added or randomly deleted over time, and each node pair is connected by a positive or negative edge. The objective is to continuously find a partition which minimizes the sum of positive edges crossing clusters and negative edges within clusters. We present an algorithm that maintains an $O(1)$-approximation with $O$(polylog $n$) amortized update time. Prior to our work, Behnezhad, Charikar, Ma, and L. Tan achieved a $5$-approximation with $O(1)$ expected update time in edge streams which translates in node streams to an $O(D)$-update time where $D$ is the maximum possible degree. Finally we complement our theoretical analysis with experiments on real world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09137v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Cohen-Addad, Silvio Lattanzi, Andreas Maggiori, Nikos Parotsidis</dc:creator>
    </item>
    <item>
      <title>Reducing the Space Used by the Sieve of Eratosthenes When Factoring</title>
      <link>https://arxiv.org/abs/2406.09150</link>
      <description>arXiv:2406.09150v1 Announce Type: new 
Abstract: We present a version of the sieve of Eratosthenes that can factor all integers $\le x$ in $O(x \log\log x)$ arithmetic operations using at most $O(\sqrt{x}/\log\log x)$ bits of space. This is an improved space bound under the condition that the algorithm takes at most $O(x\log\log x)$ time. We also show our algorithm performs well in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09150v1</guid>
      <category>cs.DS</category>
      <category>math.NT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuel Hartman, Jonathan P. Sorenson</dc:creator>
    </item>
    <item>
      <title>Compact Parallel Hash Tables on the GPU</title>
      <link>https://arxiv.org/abs/2406.09255</link>
      <description>arXiv:2406.09255v1 Announce Type: new 
Abstract: On the GPU, hash table operation speed is determined in large part by cache line efficiency, and state-of-the-art hashing schemes thus divide tables into cache line-sized buckets. This raises the question whether performance can be further improved by increasing the number of entries that fit in such buckets. Known compact hashing techniques have not yet been adapted to the massively parallel setting, nor have they been evaluated on the GPU. We consider a compact version of bucketed cuckoo hashing, and a version of compact iceberg hashing suitable for the GPU. We discuss the tables from a theoretical perspective, and provide an open source implementation of both schemes in CUDA for comparative benchmarking. In terms of performance, the state-of-the-art cuckoo hashing benefits from compactness on lookups and insertions (most experiments show at least 10-20% increase in throughput), and the iceberg table benefits significantly, to the point of being comparable to compact cuckoo hashing--while supporting performant dynamic operation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09255v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Steef Hegeman, Daan W\"oltgens, Anton Wijs, Alfons Laarman</dc:creator>
    </item>
    <item>
      <title>Efficient Discrepancy Testing for Learning with Distribution Shift</title>
      <link>https://arxiv.org/abs/2406.09373</link>
      <description>arXiv:2406.09373v1 Announce Type: new 
Abstract: A fundamental notion of distance between train and test distributions from the field of domain adaptation is discrepancy distance. While in general hard to compute, here we provide the first set of provably efficient algorithms for testing localized discrepancy distance, where discrepancy is computed with respect to a fixed output classifier. These results imply a broad set of new, efficient learning algorithms in the recently introduced model of Testable Learning with Distribution Shift (TDS learning) due to Klivans et al. (2023).
  Our approach generalizes and improves all prior work on TDS learning: (1) we obtain universal learners that succeed simultaneously for large classes of test distributions, (2) achieve near-optimal error rates, and (3) give exponential improvements for constant depth circuits. Our methods further extend to semi-parametric settings and imply the first positive results for low-dimensional convex sets. Additionally, we separate learning and testing phases and obtain algorithms that run in fully polynomial time at test time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09373v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gautam Chandrasekaran, Adam R. Klivans, Vasilis Kontonis, Konstantinos Stavropoulos, Arsen Vasilyan</dc:creator>
    </item>
    <item>
      <title>Roping in Uncertainty: Robustness and Regularization in Markov Games</title>
      <link>https://arxiv.org/abs/2406.08847</link>
      <description>arXiv:2406.08847v1 Announce Type: cross 
Abstract: We study robust Markov games (RMG) with $s$-rectangular uncertainty. We show a general equivalence between computing a robust Nash equilibrium (RNE) of a $s$-rectangular RMG and computing a Nash equilibrium (NE) of an appropriately constructed regularized MG. The equivalence result yields a planning algorithm for solving $s$-rectangular RMGs, as well as provable robustness guarantees for policies computed using regularized methods. However, we show that even for just reward-uncertain two-player zero-sum matrix games, computing an RNE is PPAD-hard. Consequently, we derive a special uncertainty structure called efficient player-decomposability and show that RNE for two-player zero-sum RMG in this class can be provably solved in polynomial time. This class includes commonly used uncertainty sets such as $L_1$ and $L_\infty$ ball uncertainty sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08847v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy McMahan, Giovanni Artiglio, Qiaomin Xie</dc:creator>
    </item>
    <item>
      <title>Computing congruences of finite inverse semigroups</title>
      <link>https://arxiv.org/abs/2406.09281</link>
      <description>arXiv:2406.09281v1 Announce Type: cross 
Abstract: In this paper we present an algorithm for computing a congruence on an inverse semigroup from a collection of generating pairs. This algorithm uses a myriad of techniques from computational group theory, automata, and the theory of inverse semigroups. An initial implementation of this algorithm outperforms existing implementations by several orders of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09281v1</guid>
      <category>math.GR</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luna Elliott, Alex Levine, James D. Mitchell</dc:creator>
    </item>
    <item>
      <title>ShockHash: Near Optimal-Space Minimal Perfect Hashing Beyond Brute-Force</title>
      <link>https://arxiv.org/abs/2310.14959</link>
      <description>arXiv:2310.14959v2 Announce Type: replace 
Abstract: A minimal perfect hash function (MPHF) maps a set S of n keys to the first n integers without collisions. There is a lower bound of n*log(e)=1.44n bits needed to represent an MPHF. This can be reached by a brute-force algorithm that tries e^n hash function seeds in expectation and stores the first seed leading to an MPHF. The most space-efficient previous algorithms for constructing MPHFs all use such a brute-force approach as a basic building block.
  In this paper, we introduce ShockHash - Small, heavily overloaded cuckoo hash tables for minimal perfect hashing. ShockHash uses two hash functions h_0 and h_1, hoping for the existence of a function f : S-&gt;{0, 1} such that x -&gt; h_{f(x)}(x) is an MPHF on S. It then uses a 1-bit retrieval data structure to store f using n + o(n) bits.
  In graph terminology, ShockHash generates n-edge random graphs until stumbling on a pseudoforest - where each component contains as many edges as nodes. Using cuckoo hashing, ShockHash then derives an MPHF from the pseudoforest in linear time. We show that ShockHash needs to try only about (e/2)^n=1.359^n seeds in expectation. This reduces the space for storing the seed by roughly n bits (maintaining the asymptotically optimal space consumption) and speeds up construction by almost a factor of 2^n compared to brute-force. Bipartite ShockHash reduces the expected construction time again to 1.166^n by maintaining a pool of candidate hash functions and checking all possible pairs.
  ShockHash as a building block within the RecSplit framework can be constructed up to 3 orders of magnitude faster than competing approaches. It can build an MPHF for 10 million keys with 1.489 bits per key in about half an hour. When instead using ShockHash after an efficient k-perfect hash function, it achieves space usage similar to the best competitors, while being significantly faster to construct and query.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.14959v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hans-Peter Lehmann, Peter Sanders, Stefan Walzer</dc:creator>
    </item>
    <item>
      <title>The $k$-Opt algorithm for the Traveling Salesman Problem has exponential running time for $k \ge 5$</title>
      <link>https://arxiv.org/abs/2402.07061</link>
      <description>arXiv:2402.07061v2 Announce Type: replace 
Abstract: The $k$-Opt algorithm is a local search algorithm for the Traveling Salesman Problem. Starting with an initial tour, it iteratively replaces at most $k$ edges in the tour with the same number of edges to obtain a better tour. Krentel (FOCS 1989) showed that the Traveling Salesman Problem with the $k$-Opt neighborhood is complete for the class PLS (polynomial time local search) and that the $k$-Opt algorithm can have exponential running time for any pivot rule. However, his proof requires $k \gg 1000$ and has a substantial gap. We show the two properties above for a much smaller value of $k$, addressing an open question by Monien, Dumrauf, and Tscheuschner (ICALP 2010). In particular, we prove the PLS-completeness for $k \geq 17$ and the exponential running time for $k \geq 5$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07061v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sophia Heimann, Hung P. Hoang, Stefan Hougardy</dc:creator>
    </item>
    <item>
      <title>Quantum chi-squared tomography and mutual information testing</title>
      <link>https://arxiv.org/abs/2305.18519</link>
      <description>arXiv:2305.18519v2 Announce Type: replace-cross 
Abstract: For quantum state tomography on rank-$r$ dimension-$d$ states, we show that $\widetilde{O}(r^{.5}d^{1.5}/\epsilon) \leq \widetilde{O}(d^2/\epsilon)$ copies suffice for accuracy~$\epsilon$ with respect to (Bures) $\chi^2$-divergence, and $\widetilde{O}(rd/\epsilon)$ copies suffice for accuracy~$\epsilon$ with respect to quantum relative entropy. The best previous bound was $\widetilde{O}(rd/\epsilon) \leq \widetilde{O}(d^2/\epsilon)$ with respect to infidelity; our results are an improvement since infidelity is bounded above by both the relative entropy and the $\chi^2$-divergence. For algorithms that are required to use single-copy measurements, we show that $\widetilde{O}(r^{1.5} d^{1.5}/\epsilon) \leq \widetilde{O}(d^3/\epsilon)$ copies suffice for $\chi^2$-divergence, and $\widetilde{O}(r^{2} d/\epsilon)$ suffice for relative entropy.
  Using this tomography algorithm, we show that $\widetilde{O}(d^{2.5}/\epsilon)$ copies of a $d\times d$-dimensional bipartite state suffice to test if it has quantum mutual information~$0$ or at least~$\epsilon$. As a corollary, we also improve the best known sample complexity for the \emph{classical} version of mutual information testing to $\widetilde{O}(d/\epsilon)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.18519v2</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Steven T. Flammia, Ryan O'Donnell</dc:creator>
    </item>
    <item>
      <title>Anytime-Constrained Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2311.05511</link>
      <description>arXiv:2311.05511v3 Announce Type: replace-cross 
Abstract: We introduce and study constrained Markov Decision Processes (cMDPs) with anytime constraints. An anytime constraint requires the agent to never violate its budget at any point in time, almost surely. Although Markovian policies are no longer sufficient, we show that there exist optimal deterministic policies augmented with cumulative costs. In fact, we present a fixed-parameter tractable reduction from anytime-constrained cMDPs to unconstrained MDPs. Our reduction yields planning and learning algorithms that are time and sample-efficient for tabular cMDPs so long as the precision of the costs is logarithmic in the size of the cMDP. However, we also show that computing non-trivial approximately optimal policies is NP-hard in general. To circumvent this bottleneck, we design provable approximation algorithms that efficiently compute or learn an arbitrarily accurate approximately feasible policy with optimal value so long as the maximum supported cost is bounded by a polynomial in the cMDP or the absolute budget. Given our hardness results, our approximation guarantees are the best possible under worst-case analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05511v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, PMLR 238:4321-4329, 2024</arxiv:journal_reference>
      <dc:creator>Jeremy McMahan, Xiaojin Zhu</dc:creator>
    </item>
    <item>
      <title>Bandit Sequential Posted Pricing via Half-Concavity</title>
      <link>https://arxiv.org/abs/2312.12794</link>
      <description>arXiv:2312.12794v2 Announce Type: replace-cross 
Abstract: Sequential posted pricing auctions are popular because of their simplicity in practice and their tractability in theory. A usual assumption in their study is that the Bayesian prior distributions of the buyers are known to the seller, while in reality these priors can only be accessed from historical data. To overcome this assumption, we study sequential posted pricing in the bandit learning model, where the seller interacts with $n$ buyers over $T$ rounds: In each round the seller posts $n$ prices for the $n$ buyers and the first buyer with a valuation higher than the price takes the item. The only feedback that the seller receives in each round is the revenue.
  Our main results obtain nearly-optimal regret bounds for single-item sequential posted pricing in the bandit learning model. In particular, we achieve an $\tilde{O}(\mathsf{poly}(n)\sqrt{T})$ regret for buyers with (Myerson's) regular distributions and an $\tilde{O}(\mathsf{poly}(n)T^{{2}/{3}})$ regret for buyers with general distributions, both of which are tight in the number of rounds $T$. Our result for regular distributions was previously not known even for the single-buyer setting and relies on a new half-concavity property of the revenue function in the value space. For $n$ sequential buyers, our technique is to run a generalized single-buyer algorithm for all the buyers and to carefully bound the regret from the sub-optimal pricing of the suffix buyers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12794v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sahil Singla, Yifan Wang</dc:creator>
    </item>
    <item>
      <title>On the zeros of partition functions with multi-spin interactions</title>
      <link>https://arxiv.org/abs/2406.04179</link>
      <description>arXiv:2406.04179v2 Announce Type: replace-cross 
Abstract: Let $X_1, \ldots, X_n$ be probability spaces, let $X$ be their direct product, let $\phi_1, \ldots, \phi_m: X \longrightarrow {\Bbb C}$ be random variables, each depending only on a few coordinates of a point $x=(x_1, \ldots, x_n)$, and let $f=\phi_1 + \ldots + \phi_m$. The expectation $E\thinspace e^{\lambda f}$, where $\lambda \in {\Bbb C}$, appears in statistical physics as the partition function of a system with multi-spin interactions, and also in combinatorics and computer science, where it is known as the partition function of edge-coloring models, tensor network contractions or a Holant polynomial. Assuming that each $\phi_i$ is 1-Lipschitz in the Hamming metric of $X$, that each $\phi_i(x)$ depends on at most $r \geq 2$ coordinates $x_1, \ldots, x_n$ of $x \in X$, and that for each $j$ there are at most $c \geq 1$ functions $\phi_i$ that depend on the coordinate $x_j$, we prove that $E\thinspace e^{\lambda f} \ne 0$ provided $| \lambda | \leq \ (3 c \sqrt{r-1})^{-1}$ and that the bound is sharp up to a constant factor. Taking a scaling limit, we prove a similar result for functions $\phi_1, \ldots, \phi_m: {\Bbb R}^n \longrightarrow {\Bbb C}$ that are 1-Lipschitz in the $\ell^1$ metric of ${\Bbb R}^n$ and where the expectation is taken with respect to the standard Gaussian measure in ${\Bbb R}^n$. As a corollary, the value of the expectation can be efficiently approximated, provided $\lambda$ lies in a slightly smaller disc.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04179v2</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <category>math-ph</category>
      <category>math.CO</category>
      <category>math.MP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Barvinok</dc:creator>
    </item>
  </channel>
</rss>
