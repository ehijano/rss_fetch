<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Nov 2025 03:49:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On the class of exponential statistical structures of type B</title>
      <link>https://arxiv.org/abs/2510.26863</link>
      <description>arXiv:2510.26863v1 Announce Type: new 
Abstract: The article is devoted to the study of exponential statistical structures of type B, which constitute a subclass of exponential families of probability distributions. This class is characterized by a number of analytical and probabilistic properties that make it a convenient tool for solving both theoretical and applied problems in statistics. The relevance of this research lies in the need to generalize known classes of distributions and to develop a unified framework for their analysis, which is essential for applications in stochastic modeling, machine learning, financial mathematics. The paper proposes a formal definition of type B. Necessary and sufficient conditions for a statistical structure to belong to class B are established, and it is proved that such structures can be represented through a dominating measure with an explicit Laplace transform. The obtained results make it possible to describe a wide range of well-known one-dimensional and multivariate distributions, including the binomial, Poisson, normal, gamma, polynomial, and logarithmic distributions, as well as specific cases such as the Borel-Tanner distribution and random walk distributions. Particular attention is given to the proof of structural theorems that determine the stability of class B under linear transformations and the addition of independent random vectors. Recursive relations for initial and central moments as well as for semi-invariants are obtained, providing an efficient analytical and computational framework for their evaluation. Furthermore, the tails of type B distributions are investigated using the properties of the Laplace transform. New exponential inequalities for estimating the probabilities of large deviations are derived. The obtained results can be applied in theoretical studies and in practical problems of stochastic modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26863v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oleksandr Volkov, Yurii Volkov</dc:creator>
    </item>
    <item>
      <title>Advanced Distribution Theory for Significance in Scale Space</title>
      <link>https://arxiv.org/abs/2510.27023</link>
      <description>arXiv:2510.27023v1 Announce Type: new 
Abstract: Smoothing methods find signals in noisy data. A challenge for Statistical inference is the choice of smoothing parameter. SiZer addressed this challenge in one-dimension by detecting significant slopes across multiple scales, but was not a completely valid testing procedure. This was addressed by the development of an advanced distribution theory that ensures fully valid inference in the 1-D setting by applying extreme value theory. A two-dimensional extension of SiZer, known as Significance in Scale Space (SSS), was developed for image data, enabling the detection of both slopes and curvatures across multiple spatial scales. However, fully valid inference for 2-D SSS has remained unavailable, largely due to the more complex dependence structure of random fields. In this paper, we use a completely different probability methodology which gives an advanced distribution theory for SSS, establishing a valid hypothesis testing procedure for both slope and curvature detection. When applied to pure noise images (no true underlying signal), the proposed method controls the Type I error, whereas the original SSS identifies spurious features across scales. When signal is present, the proposed method maintains a high level of statistical power, successfully identifying important true slopes and curvatures in real data such as gamma camera images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27023v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rui Liu, Jan Hannig, J. S. Marron</dc:creator>
    </item>
    <item>
      <title>Adaptive Algorithms for Infinitely Many-Armed Bandits: A Unified Framework</title>
      <link>https://arxiv.org/abs/2510.27319</link>
      <description>arXiv:2510.27319v2 Announce Type: new 
Abstract: We consider a bandit problem where the buget is smaller than the number of arms, which may be infinite. In this regime, the usual objective in the literature is to minimize simple regret. To analyze broad classes of distributions with potentially unbounded support, where simple regret may not be well-defined, we take a slightly different approach and seek to maximize the expected simple reward of the recommended arm, providing anytime guarantees. To that end, we introduce a distribution-free algorithm, OSE, that adapts to the distribution of arm means and achieves near-optimal rates for several distribution classes. We characterize the sample complexity through the rank-corrected inverse squared gap function. In particular, we recover known upper bounds and transition regimes for $\alpha$ less or greater than $1/2$ when the quantile function is $\lambda_\eta = 1-\eta^{\alpha}$. We additionally identify new transition regimes depending on the noise level relative to $\alpha$, which we conjecture to be nearly optimal. Additionally, we introduce an enhanced practical version, PROSE, that achieves state-of-the-art empirical performance for the main distribution classes considered in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27319v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emmanuel Pilliat (ENSAI, CREST)</dc:creator>
    </item>
    <item>
      <title>Minimax-Optimal Two-Sample Test with Sliced Wasserstein</title>
      <link>https://arxiv.org/abs/2510.27498</link>
      <description>arXiv:2510.27498v1 Announce Type: cross 
Abstract: We study the problem of nonparametric two-sample testing using the sliced Wasserstein (SW) distance. While prior theoretical and empirical work indicates that the SW distance offers a promising balance between strong statistical guarantees and computational efficiency, its theoretical foundations for hypothesis testing remain limited. We address this gap by proposing a permutation-based SW test and analyzing its performance. The test inherits finite-sample Type I error control from the permutation principle. Moreover, we establish non-asymptotic power bounds and show that the procedure achieves the minimax separation rate $n^{-1/2}$ over multinomial and bounded-support alternatives, matching the optimal guarantees of kernel-based tests while building on the geometric foundations of Wasserstein distances. Our analysis further quantifies the trade-off between the number of projections and statistical power. Finally, numerical experiments demonstrate that the test combines finite-sample validity with competitive power and scalability, and -- unlike kernel-based tests, which require careful kernel tuning -- it performs consistently well across all scenarios we consider.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27498v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Binh Thuan Tran, Nicolas Schreuder</dc:creator>
    </item>
    <item>
      <title>Optimal Convergence Analysis of DDPM for General Distributions</title>
      <link>https://arxiv.org/abs/2510.27562</link>
      <description>arXiv:2510.27562v1 Announce Type: cross 
Abstract: Score-based diffusion models have achieved remarkable empirical success in generating high-quality samples from target data distributions. Among them, the Denoising Diffusion Probabilistic Model (DDPM) is one of the most widely used samplers, generating samples via estimated score functions. Despite its empirical success, a tight theoretical understanding of DDPM -- especially its convergence properties -- remains limited.
  In this paper, we provide a refined convergence analysis of the DDPM sampler and establish near-optimal convergence rates under general distributional assumptions. Specifically, we introduce a relaxed smoothness condition parameterized by a constant $L$, which is small for many practical distributions (e.g., Gaussian mixture models). We prove that the DDPM sampler with accurate score estimates achieves a convergence rate of $$\widetilde{O}\left(\frac{d\min\{d,L^2\}}{T^2}\right)~\text{in Kullback-Leibler divergence},$$ where $d$ is the data dimension, $T$ is the number of iterations, and $\widetilde{O}$ hides polylogarithmic factors in $T$. This result substantially improves upon the best-known $d^2/T^2$ rate when $L &lt; \sqrt{d}$. By establishing a matching lower bound, we show that our convergence analysis is tight for a wide array of target distributions. Moreover, it reveals that DDPM and DDIM share the same dependence on $d$, raising an interesting question of why DDIM often appears empirically faster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27562v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchen Jiao, Yuchen Zhou, Gen Li</dc:creator>
    </item>
    <item>
      <title>Subspace Ordering for Maximum Response Preservation in Sufficient Dimension Reduction</title>
      <link>https://arxiv.org/abs/2510.27593</link>
      <description>arXiv:2510.27593v1 Announce Type: cross 
Abstract: Sufficient dimension reduction (SDR) methods aim to identify a dimension reduction subspace (DRS) that preserves all the information about the conditional distribution of a response given its predictor. Traditional SDR methods determine the DRS by solving a method-specific generalized eigenvalue problem and selecting the eigenvectors corresponding to the largest eigenvalues. In this article, we argue against the long-standing convention of using eigenvalues as the measure of subspace importance and propose alternative ordering criteria that directly assess the predictive relevance of each subspace. For a binary response, we introduce a subspace ordering criterion based on the absolute value of the independent Student's t-statistic. Theoretically, our criterion identifies subspaces that achieve the local minimum Bayes' error rate and yields consistent ordering of directions under mild regularity conditions. Additionally, we employ an F-statistic to provide a framework that unifies categorical and continuous responses under a single subspace criterion. We evaluate our proposed criteria within multiple SDR methods through extensive simulation studies and applications to real data. Our empirical results demonstrate the efficacy of reordering subspaces using our proposed criteria, which generally improves classification accuracy and subspace estimation compared to ordering by eigenvalues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27593v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Derik T. Boonstra, Rakheon Kim, Dean M. Young</dc:creator>
    </item>
    <item>
      <title>Adversarially robust clustering with optimality guarantees</title>
      <link>https://arxiv.org/abs/2306.09977</link>
      <description>arXiv:2306.09977v3 Announce Type: replace 
Abstract: We consider the problem of clustering data points coming from sub-Gaussian mixtures. Existing methods that provably achieve the optimal mislabeling error, such as the Lloyd algorithm, are usually vulnerable to outliers. In contrast, clustering methods seemingly robust to adversarial perturbations are not known to satisfy the optimal statistical guarantees. We propose a simple robust algorithm based on the coordinatewise median that obtains the optimal mislabeling rate even when we allow adversarial outliers to be present. Our algorithm achieves the optimal error rate in constant iterations when a weak initialization condition is satisfied. In the absence of outliers, in fixed dimensions, our theoretical guarantees are similar to that of the Lloyd algorithm. Extensive experiments on various simulated and public datasets are conducted to support the theoretical guarantees of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.09977v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TIT.2025.3628160</arxiv:DOI>
      <dc:creator>Soham Jana, Kun Yang, Sanjeev Kulkarni</dc:creator>
    </item>
    <item>
      <title>Testing and estimation in orthosymmetric Gaussian sequence model</title>
      <link>https://arxiv.org/abs/2507.16734</link>
      <description>arXiv:2507.16734v4 Announce Type: replace 
Abstract: We study the Gaussian sequence model, i.e. $X \sim N(\mathbf{\theta}, I_\infty)$, where $\mathbf{\theta} \in \Gamma \subset \ell_2$ is assumed to be convex and compact. We show that goodness-of-fit testing sample complexity is lower bounded by the square-root of the estimation complexity, whenever $\Gamma$ is orthosymmetric. This lower bound is tight when $\Gamma$ is also quadratically convex (as shown by [Donoho et al. 1990, Neykov 2023]). We also completely characterize likelihood-free hypothesis testing (LFHT) complexity for $\ell_p$-bodies, discovering new types of tradeoff between the numbers of simulation and observation samples, compared to the case of ellipsoids (p = 2) studied in [Gerber and Polyanskiy 2024].</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16734v4</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeyu Jia, Yury Polyanskiy</dc:creator>
    </item>
    <item>
      <title>Supervised Quadratic Feature Analysis: Information Geometry Approach for Dimensionality Reduction</title>
      <link>https://arxiv.org/abs/2502.00168</link>
      <description>arXiv:2502.00168v4 Announce Type: replace-cross 
Abstract: Supervised dimensionality reduction maps labeled data into a low-dimensional feature space while preserving class discriminability. A common approach is to maximize a statistical measure of dissimilarity between classes in the feature space. Information geometry provides an alternative framework for measuring class dissimilarity, with the potential for improved insights and novel applications. Information geometry, which is grounded in Riemannian geometry, uses the Fisher information metric, a local measure of discriminability that induces the Fisher-Rao distance. Here, we present Supervised Quadratic Feature Analysis (SQFA), a linear dimensionality reduction method that maximizes Fisher-Rao distances between class-conditional distributions, under Gaussian assumptions. We motivate the Fisher-Rao distance as a good proxy for discriminability. We show that SQFA features support good classification performance with Quadratic Discriminant Analysis (QDA) on three real-world datasets. SQFA provides a novel framework for supervised dimensionality reduction, motivating future research in applying information geometry to machine learning and neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00168v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.DG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Herrera-Esposito, Johannes Burge</dc:creator>
    </item>
    <item>
      <title>Kernel Mean Embedding Topology: Weak and Strong Forms for Stochastic Kernels and Implications for Model Learning</title>
      <link>https://arxiv.org/abs/2502.13486</link>
      <description>arXiv:2502.13486v2 Announce Type: replace-cross 
Abstract: We introduce a novel topology, called Kernel Mean Embedding Topology, for stochastic kernels, in a weak and strong form. This topology, defined on the spaces of Bochner integrable functions from a signal space to a space of probability measures endowed with a Hilbert space structure, allows for a versatile formulation. This construction allows one to obtain both a strong and weak formulation. (i) For its weak formulation, we highlight the utility on relaxed policy spaces, and investigate connections with the Young narrow topology and Borkar (or \( w^* \))-topology, and establish equivalence properties. We report that, while both the \( w^* \)-topology and kernel mean embedding topology are relatively compact, they are not closed. Conversely, while the Young narrow topology is closed, it lacks relative compactness. (ii) We show that the strong form provides an appropriate formulation for placing topologies on spaces of models characterized by stochastic kernels with explicit robustness and learning theoretic implications on optimal stochastic control under discounted or average cost criteria. (iii) We thus show that this topology possesses several properties making it ideal to study optimality and approximations (under the weak formulation) and robustness (under the strong formulation) for many applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13486v2</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naci Saldi, Serdar Yuksel</dc:creator>
    </item>
    <item>
      <title>Valid F-screening in linear regression</title>
      <link>https://arxiv.org/abs/2505.23113</link>
      <description>arXiv:2505.23113v2 Announce Type: replace-cross 
Abstract: Suppose that a data analyst wishes to report the results of a least squares linear regression only if the overall null hypothesis, $H_0^{1:p}: \beta_1= \beta_2 = \ldots = \beta_p=0$, is rejected. This practice, which we refer to as F-screening (since the overall null hypothesis is typically tested using an $F$-statistic), is in fact common practice across a number of applied fields. Unfortunately, it poses a problem: standard guarantees for the inferential outputs of linear regression, such as Type 1 error control of hypothesis tests and nominal coverage of confidence intervals, hold unconditionally, but fail to hold conditional on rejection of the overall null hypothesis. In this paper, we develop an inferential toolbox for the coefficients in a least squares model that are valid conditional on rejection of the overall null hypothesis. We develop selective p-values that lead to tests that are consistent and control the selective Type 1 error, i.e., the Type 1 error conditional on having rejected the overall null hypothesis. Furthermore, they can be computed without access to the raw data, i.e., using only the standard outputs of a least squares linear regression, and therefore are suitable for use in a retrospective analysis of a published study. We also develop confidence intervals that attain nominal selective coverage, and point estimates that account for having rejected the overall null hypothesis. We derive an expression for the Fisher information about the coefficients resulting from the proposed approach, and compare this to the Fisher information that results from an alternative approach that relies on sample splitting. We investigate the proposed approach in simulation and via re-analysis of two datasets from the biomedical literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23113v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Olivia McGough, Daniela Witten, Daniel Kessler</dc:creator>
    </item>
    <item>
      <title>Absorb and Converge: Provable Convergence Guarantee for Absorbing Discrete Diffusion Models</title>
      <link>https://arxiv.org/abs/2506.02318</link>
      <description>arXiv:2506.02318v3 Announce Type: replace-cross 
Abstract: Discrete state space diffusion models have shown significant advantages in applications involving discrete data, such as text and image generation. It has also been observed that their performance is highly sensitive to the choice of rate matrices, particularly between uniform and absorbing rate matrices. While empirical results suggest that absorbing rate matrices often yield better generation quality compared to uniform rate matrices, existing theoretical works have largely focused on the uniform rate matrices case. Notably, convergence guarantees and error analyses for absorbing diffusion models are still missing. In this work, we provide the first finite-time error bounds and convergence rate analysis for discrete diffusion models using absorbing rate matrices. We begin by deriving an upper bound on the KL divergence of the forward process, introducing a surrogate initialization distribution to address the challenge posed by the absorbing stationary distribution, which is a singleton and causes the KL divergence to be ill-defined. We then establish the first convergence guarantees for both the $\tau$-leaping and uniformization samplers under absorbing rate matrices, demonstrating improved rates over their counterparts using uniform rate matrices. Furthermore, under suitable assumptions, we provide convergence guarantees without early stopping. Our analysis introduces several new technical tools to address challenges unique to absorbing rate matrices. These include a Jensen-type argument for bounding forward process convergence, novel techniques for bounding absorbing score functions, and a non-divergent upper bound on the score near initialization that removes the need of early-stopping.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02318v3</guid>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuchen Liang, Renxiang Huang, Lifeng Lai, Ness Shroff, Yingbin Liang</dc:creator>
    </item>
    <item>
      <title>Centered MA Dirichlet ARMA for Financial Compositions: Theory &amp; Empirical Evidence</title>
      <link>https://arxiv.org/abs/2510.18903</link>
      <description>arXiv:2510.18903v2 Announce Type: replace-cross 
Abstract: Observation-driven Dirichlet models for compositional time series commonly use the additive log-ratio (ALR) link and include a moving-average (MA) term based on ALR residuals. In the standard Bayesian Dirichlet Auto-Regressive Moving-Average (B-DARMA) recursion, this MA regressor has a nonzero conditional mean under the Dirichlet likelihood, which biases the mean path and complicates interpretation of the MA coefficients. We propose a minimal change: replace the raw regressor with a centered innovation equal to the ALR residual minus its conditional expectation, computable in closed form using digamma functions. Centering restores mean-zero innovations for the MA block without altering either the likelihood or the ALR link. We provide closed-form identities for the conditional mean and forecast recursion, show first-order equivalence to a digamma-link DARMA while retaining a simple inverse back to the mean composition, and supply ready-to-use code. In a weekly application to the Federal Reserve H.8 bank-asset composition, the centered specification improves log predictive scores with virtually identical point accuracy and markedly cleaner Hamiltonian Monte Carlo diagnostics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18903v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison Katz</dc:creator>
    </item>
    <item>
      <title>On Uncertainty Calibration for Equivariant Functions</title>
      <link>https://arxiv.org/abs/2510.21691</link>
      <description>arXiv:2510.21691v3 Announce Type: replace-cross 
Abstract: Data-sparse settings such as robotic manipulation, molecular physics, and galaxy morphology classification are some of the hardest domains for deep learning. For these problems, equivariant networks can help improve modeling across undersampled parts of the input space, and uncertainty estimation can guard against overconfidence. However, until now, the relationships between equivariance and model confidence, and more generally equivariance and model calibration, has yet to be studied. Since traditional classification and regression error terms show up in the definitions of calibration error, it is natural to suspect that previous work can be used to help understand the relationship between equivariance and calibration error. In this work, we present a theory relating equivariance to uncertainty estimation. By proving lower and upper bounds on uncertainty calibration errors (ECE and ENCE) under various equivariance conditions, we elucidate the generalization limits of equivariant models and illustrate how symmetry mismatch can result in miscalibration in both classification and regression. We complement our theoretical framework with numerical experiments that clarify the relationship between equivariance and uncertainty using a variety of real and simulated datasets, and we comment on trends with symmetry mismatch, group size, and aleatoric and epistemic uncertainties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21691v3</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edward Berman, Jacob Ginesin, Marco Pacini, Robin Walters</dc:creator>
    </item>
  </channel>
</rss>
