<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Nov 2025 02:41:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Stochastic Derivative Estimation for Discontinuous Sample Performances: A Leibniz Integration Perspective</title>
      <link>https://arxiv.org/abs/2511.00006</link>
      <description>arXiv:2511.00006v1 Announce Type: new 
Abstract: We develop a novel stochastic derivative estimation framework for sample performance functions that are discontinuous in the parameter of interest, based on the multidimensional Leibniz integral rule. When discontinuities arise from indicator functions, we embed the indicator functions into the sample space, yielding a continuous performance function over a parameter-dependent domain. Applying the Leibniz integral rule in this case produces a single-run, unbiased derivative estimator. For general discontinuous functions, we apply a change of variables to shift parameter dependence into the sample space and the underlying probability measure. Applying the Leibniz integral rule leads to two terms: a standard likelihood ratio (LR) term from differentiating the underlying probability measure and a surface integral from differentiating the boundary of the domain. Evaluating the surface integral may require simulating multiple sample paths. Our proposed Leibniz integration framework generalizes the generalized LR (GLR) method and provides intuition as to when the surface integral vanishes, thereby enabling single-run, easily implementable estimators. Numerical experiments demonstrate the effectiveness and robustness of our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00006v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xingyu Ren, Michael C. Fu, Pierre L'Ecuyer</dc:creator>
    </item>
    <item>
      <title>Is Representational Similarity Analysis Reliable? A Comparison with Regression</title>
      <link>https://arxiv.org/abs/2511.00395</link>
      <description>arXiv:2511.00395v1 Announce Type: new 
Abstract: Representational Similarity Analysis (RSA) is a popular method for analyzing neuroimaging and behavioral data. Here we evaluate the accuracy and reliability of RSA in the context of model selection, and compare it to that of regression. Although RSA offers flexibility in handling high-dimensional, cross-modal, and cross-species data, its reliance on a transformation of raw data into similarity structures may result in the loss of critical stimulus-response information. Across extensive simulation studies and empirical analyses, we show that RSA leads to lower model selection accuracy, regardless of sample size, noise level, feature dimensionality, or multicollinearity, relative to regression. While principal component analysis and feature reweighting mitigate RSA's deficits driven by multicollinearity, regression remains superior in accurately distinguishing between models. Empirical data and a follow-up fMRI simulation further support these conclusions. Our findings suggest that researchers should carefully consider which approach to use: RSA is less effective than linear regression for model selection and fitting when direct stimulus-response mappings are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00395v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chuanji Gao, Gang Chen, Svetlana V. Shinkareva, Rutvik H. Desai</dc:creator>
    </item>
    <item>
      <title>Latent Modularity in Multi-View Data</title>
      <link>https://arxiv.org/abs/2511.00455</link>
      <description>arXiv:2511.00455v1 Announce Type: new 
Abstract: In this article, we consider the problem of clustering multi-view data, that is, information associated to individuals that form heterogeneous data sources (the views). We adopt a Bayesian model and in the prior structure we assume that each individual belongs to a baseline cluster and conditionally allow each individual in each view to potentially belong to different clusters than the baseline. We call such a structure ''latent modularity''. Then for each cluster, in each view we have a specific statistical model with an associated prior. We derive expressions for the marginal priors on the view-specific cluster labels and the associated partitions, giving several insights into our chosen prior structure. Using simple Markov chain Monte Carlo algorithms, we consider our model in a simulation study, along with a more detailed case study that requires several modeling innovations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00455v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andrea Cremaschi, Maria De Iorio, Garritt Page, Ajay Jasra</dc:creator>
    </item>
    <item>
      <title>Modeling continuous monitoring glucose curves by Beta generalized non-parametric models</title>
      <link>https://arxiv.org/abs/2511.00501</link>
      <description>arXiv:2511.00501v1 Announce Type: new 
Abstract: We present a functional data analysis approach for studying time-dependent, continuous glucose monitoring data with repeated measures for each individual in an experiment. After scaling the glucose concentration curves to the interval [0, 1], we model them by using a Beta distribution with two time-varying parameters. In this context, we develop a local linear maximum likelihood smoothing procedure that is valid when more than one parameter depends on time. Our approach requires much fewer observations than previous functional methods for this setting and is also applicable when only one individual (or a few) is available. We evaluate the performance of our estimator in terms of computation time and model fit using a synthetic dataset as well as a large, real clinical trial dataset. We also compare our method with existing methods in the literature. From a methodological point of view, we contribute to extend local likelihood estimation from one to two time-varying parameters by developing theoretical expressions for estimation and for approximating the leave-one-out cross-validation. Moreover, we show that this kernel-based approach competes with spline-based estimation methods, the dominant line of functional regression models today.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00501v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nihan Acar-Denizli, Pedro Delicado</dc:creator>
    </item>
    <item>
      <title>Robust Bayesian Inference of Causal Effects via Randomization Distributions</title>
      <link>https://arxiv.org/abs/2511.00676</link>
      <description>arXiv:2511.00676v1 Announce Type: new 
Abstract: We present a general framework for Bayesian inference of causal effects that delivers provably robust inferences founded on design-based randomization of treatments. The framework involves fixing the observed potential outcomes and forming a likelihood based on the randomization distribution of a statistic. The method requires specification of a treatment effect model; in many cases, however, it does not require specification of marginal outcome distributions, resulting in weaker assumptions compared to Bayesian superpopulation-based methods. We show that the framework is compatible with posterior model checking in the form of posterior-averaged randomization tests. We prove several theoretical properties for the method, including a Bernstein-von Mises theorem and large-sample properties of posterior expectations. In particular, we show that the posterior mean is asymptotically equivalent to Hodges-Lehmann estimators, which provides a bridge to many classical estimators in causal inference, including inverse-probability-weighted estimators and H\'ajek estimators. We evaluate the theory and utility of the framework in simulation and a case study involving a nutrition experiment. In the latter, our framework uncovers strong evidence of effect heterogeneity despite a lack of evidence for moderation effects. The basic framework allows numerous extensions, including the use of covariates, sensitivity analysis, estimation of assignment mechanisms, and generalization to nonbinary treatments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00676v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Easton Huch, Fred Feinberg, Walter Dempsey</dc:creator>
    </item>
    <item>
      <title>Correcting the Coverage Bias of Quantile Regression</title>
      <link>https://arxiv.org/abs/2511.00820</link>
      <description>arXiv:2511.00820v1 Announce Type: new 
Abstract: We develop a collection of methods for adjusting the predictions of quantile regression to ensure coverage. Our methods are model agnostic and can be used to correct for high-dimensional overfitting bias with only minimal assumptions. Theoretical results show that the estimates we develop are consistent and facilitate accurate calibration in the proportional asymptotic regime where the ratio of the dimension of the data and the sample size converges to a constant. This is further confirmed by experiments on both simulated and real data. A key component of our work is a new connection between the leave-one-out coverage and the fitted values of variables appearing in a dual formulation of the quantile regression problem. This facilitates the use of cross-validation in a variety of settings at significantly reduced computational costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00820v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Isaac Gibbs, John J. Cherian, Emmanuel J. Cand\`es</dc:creator>
    </item>
    <item>
      <title>A Distributed Plug-and-Play MCMC Algorithm for High-Dimensional Inverse Problems</title>
      <link>https://arxiv.org/abs/2511.00870</link>
      <description>arXiv:2511.00870v1 Announce Type: new 
Abstract: Markov Chain Monte Carlo (MCMC) algorithms are standard approaches to solve imaging inverse problems and quantify estimation uncertainties, a key requirement in absence of ground-truth data. To improve estimation quality, Plug-and-Play MCMC algorithms, such as PnP-ULA, have been recently developed to accommodate priors encoded by a denoising neural network. Designing scalable samplers for high-dimensional imaging inverse problems remains a challenge: drawing and storing high-dimensional samples can be prohibitive, especially for high-resolution images. To address this issue, this work proposes a distributed sampler based on approximate data augmentation and PnP-ULA to solve very large problems. The proposed sampler uses lightweight denoising convolutional neural network, to efficiently exploit multiple GPUs on a Single Program Multiple Data architecture. Reconstruction performance and scalability are evaluated on several imaging problems. Communication and computation overheads due to the denoiser are carefully discussed. The proposed distributed approach noticeably combines three very precious qualities: it is scalable, enables uncertainty quantification, for a reconstruction performance comparable to other PnP methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00870v1</guid>
      <category>stat.ME</category>
      <category>cs.DC</category>
      <category>eess.SP</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maxime Bouton, Pierre-Antoine Thouvenin, Audrey Repetti, Pierre Chainais</dc:creator>
    </item>
    <item>
      <title>Classification of realisations of random sets</title>
      <link>https://arxiv.org/abs/2511.00937</link>
      <description>arXiv:2511.00937v1 Announce Type: new 
Abstract: In this paper, the classification task for a family of sets representing the realisation of some random set models is solved. Both unsupervised and supervised classification methods are utilised using the similarity measure between two realisations derived as empirical estimates of $\mathcal N$-distances quantified based on geometric characteristics of the realisations, namely the boundary curvature and the perimeter over area ratios of obtained samples of connected components from the realisations. To justify the proposed methodology, a simulation study is performed using random set models. The methods are used further for classifying histological images of mastopathy and mammary cancer tissue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00937v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Bogdan Radovi\'c, Vesna Gotovac {\DJ}oga\v{s}, Kate\v{r}ina Helisov\'a</dc:creator>
    </item>
    <item>
      <title>On the estimation of leverage effect and volatility of volatility in the presence of jumps</title>
      <link>https://arxiv.org/abs/2511.00944</link>
      <description>arXiv:2511.00944v1 Announce Type: new 
Abstract: We study the estimation of leverage effect and volatility of volatility by using high-frequency data with the presence of jumps. We first construct spot volatility estimator by using the empirical characteristic function of the high-frequency increments to deal with the effect of jumps, based on which the estimators of leverage effect and volatility of volatility are proposed. Compared with existing estimators, our method is valid under more general jumps, making it a better alternative for empirical applications. Under some mild conditions, the asymptotic normality of the estimators is established and consistent estimators of the limiting variances are proposed based on the estimation of volatility functionals. We conduct extensive simulation study to verify the theoretical results. The results demonstrate that our estimators have relative better performance than the existing ones, especially when the jump is of infinite variation. Besides, we apply our estimators to a real high-frequency dataset, which reveals nonzero leverage effect and volatility of volatility in the market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00944v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Qiang Liu, Zhi Liu, Wang Zhou</dc:creator>
    </item>
    <item>
      <title>Variance Estimation for the Inverse Probability of Treatment Weighted Kaplan Meier Estimator</title>
      <link>https://arxiv.org/abs/2511.01110</link>
      <description>arXiv:2511.01110v1 Announce Type: new 
Abstract: In a widely cited paper, Xie and Liu (henceforth XL) proposed to use inverse probability of treatment weighting (IPTW) to account for possible confounding in observational studies with survival endpoints subject to right censoring. Their proposal includes an IPTW Kaplan-Meier (KM) estimator for the survival function of a treatment-specific potential failure time, which can be used to evaluate the causal effect of one treatment versus another. The IPTW KM estimator is remarkably simple and highly effective for confounding bias correction. The method has been implemented in SAS's popular procedure LIFETEST for analyzing survival data and has seen widespread use.
  This letter is concerned with variance estimation for the IPTW KM estimator. The variance estimator provided by XL does not account for the variability of the IPTW weight when the propensity score is estimated from data, as is usually the case in observational studies. In this letter, we provide a rigorous asymptotic analysis of the IPTW KM estimator based on an estimated propensity score. Our analysis indicates that estimating the propensity score does tend to result in a smaller asymptotic variance, which can be estimated consistently using a plug-in variance estimator. We also present a simulation study comparing the variance estimator we propose with the XL variance estimator. Our simulation results confirm that the proposed variance estimator is more accurate than the XL variance estimator, which tends to over-estimate the sampling variance of the IPTW KM estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01110v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Zhiwei Zhang, Yongwu Shao, Zhishen Ye</dc:creator>
    </item>
    <item>
      <title>A structural equation formulation for general quasi-periodic Gaussian processes</title>
      <link>https://arxiv.org/abs/2511.01151</link>
      <description>arXiv:2511.01151v1 Announce Type: new 
Abstract: This paper introduces a structural equation formulation that gives rise to a new family of quasi-periodic Gaussian processes, useful to process a broad class of natural and physiological signals. The proposed formulation simplifies generation and forecasting, and provides hyperparameter estimates, which we exploit in a convergent and consistent iterative estimation algorithm. A bootstrap approach for standard error estimation and confidence intervals is also provided. We demonstrate the computational and scaling benefits of the proposed approach on a broad class of problems, including water level tidal analysis, CO$_{2}$ emission data, and sunspot numbers data. By leveraging the structural equations, our method reduces the cost of likelihood evaluations and predictions from $\mathcal{O}(k^2 p^2)$ to $\mathcal{O}(p^2)$, significantly improving scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01151v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Unnati Nigam, Radhendushka Srivastava, Faezeh Marzbanrad, Michael Burke</dc:creator>
    </item>
    <item>
      <title>Perturbed Double Machine Learning: Nonstandard Inference Beyond the Parametric Length</title>
      <link>https://arxiv.org/abs/2511.01222</link>
      <description>arXiv:2511.01222v1 Announce Type: new 
Abstract: We study inference on a low dimensional functional $\beta$ in the presence of possibly infinite dimensional nuisance parameters. Classical inferential methods are typically based on the Wald interval, whose large sample validity rests on the asymptotic negligibility of the nuisance error; for example, estimators based on the influence curve of the parameter (Double/Debiased Machine Learning DML estimators) are asymptotically Gaussian when the nuisance estimators converge at rates faster than $n^{-1/4}$. Although, under suitable conditions, such negligibility can hold even in nonparametric classes, it can be restrictive. To relax this requirement, we propose Perturbed Double Machine Learning (Perturbed DML) to ensure valid inference even when nuisance estimators converge at rates slower than $n^{-1/4}$. Our proposal is to 1) inject randomness into the nuisance estimation step to generate a collection of perturbed nuisance models, each yielding an estimate of $\beta$ and a corresponding Wald interval, and 2) filter out perturbations whose deviations from the original DML estimate exceed a threshold. For Lasso nuisance learners, we show that, with high probability, at least one perturbation produces nuisance estimates sufficiently close to the truth, so that the associated estimator of $\beta$ is close to an oracle estimator with knowledge of the true nuisances. Taking the union of the retained intervals delivers valid coverage even when the DML estimator converges more slowly than $n^{-1/2}$. The framework extends to general machine learning nuisance learners, and simulations show that Perturbed DML can have coverage when state of the art methods fail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01222v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengchu Zheng, Matteo Bonvini, Zijian Guo</dc:creator>
    </item>
    <item>
      <title>Seamless Phase I--II Cancer Clinical Trials Using Kernel-Based Covariate Similarity</title>
      <link>https://arxiv.org/abs/2511.01290</link>
      <description>arXiv:2511.01290v1 Announce Type: new 
Abstract: In response to the U.S.\ Food and Drug Administration's (FDA) Project Optimus, a paradigm shift is underway in the design of early-phase oncology trials. To accelerate drug development, seamless Phase I/II designs have gained increasing attention, along with growing interest in the efficient reuse of Phase I data. We propose a nonparametric information-borrowing method that adaptively discounts Phase I observations according to the similarity of covariate distributions between Phase I and Phase II. Similarity is quantified using a kernel-based maximum mean discrepancy (MMD) and transformed into a dose-specific weight incorporated into a power-prior framework for Phase II efficacy evaluation, such as for the objective response rate (ORR). Considering the small sample sizes typical of early-phase oncology studies, we analytically derive a confidence interval for the weight, enabling assessment of borrowing precision without resampling procedures. Simulation studies under four toxicity scenarios and five baseline-covariate settings showed that the proposed method improved the probability that the lower bound of the 95\% credible interval for ORR exceeded a prespecified threshold at efficacious doses, while avoiding false threshold crossings at weakly efficacious doses. A case study based on a metastatic pancreatic ductal adenocarcinoma trial illustrates the resulting borrowing weights and posterior estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01290v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kana Makino, Natsumi Makigusa, Masahiro Kojima</dc:creator>
    </item>
    <item>
      <title>Nonparametric Sensitivity Analysis for Unobserved Confounding with Survival Outcomes</title>
      <link>https://arxiv.org/abs/2511.01412</link>
      <description>arXiv:2511.01412v1 Announce Type: new 
Abstract: In observational studies, the observed association between an exposure and outcome of interest may be distorted by unobserved confounding. Causal sensitivity analysis can be used to assess the robustness of observed associations to potential unobserved confounding. For time-to-event outcomes, existing sensitivity analysis methods rely on parametric assumptions on the structure of the unobserved confounders and Cox proportional hazards models for the outcome regression. If these assumptions fail to hold, it is unclear whether the conclusions of the sensitivity analysis remain valid. Additionally, causal interpretation of the hazard ratio is challenging. To address these limitations, in this paper we develop a nonparametric sensitivity analysis framework for time-to-event data. Specifically, we derive nonparametric bounds for the difference between the observed and counterfactual survival curves and propose estimators and inference for these bounds using semiparametric efficiency theory. We also provide nonparametric bounds and inference for the difference between the observed and counterfactual restricted mean survival times. We demonstrate the performance of our proposed methods using numerical studies and an analysis of the causal effect of elective neck dissection on mortality in patients with high-grade parotid carcinoma.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01412v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rui Hu, Ted Westling</dc:creator>
    </item>
    <item>
      <title>Adaptive Change Point Inference for High Dimensional Time Series with Temporal Dependence</title>
      <link>https://arxiv.org/abs/2511.01487</link>
      <description>arXiv:2511.01487v1 Announce Type: new 
Abstract: This paper investigates change point inference in high-dimensional time series. We begin by introducing a max-$L_2$-norm based test procedure, which demonstrates strong performance under dense alternatives. We then establish the asymptotic independence between our proposed statistic and the two max-$L_\infty$-based statistics introduced by Wang and Feng (2023). Building on this result, we develop an adaptive inference approach by applying the Cauchy combination method to integrate these tests. This combined procedure exhibits robust performance across varying levels of sparsity. Extensive simulation studies and real data analysis further confirm the superior effectiveness of our proposed methods in the high-dimensional setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01487v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoyi Wang, Jixuan Liu, Long Feng</dc:creator>
    </item>
    <item>
      <title>Z-Dip: a validated generalization of the Dip Test</title>
      <link>https://arxiv.org/abs/2511.01705</link>
      <description>arXiv:2511.01705v1 Announce Type: new 
Abstract: Detecting multimodality in empirical distributions is a fundamental problem in statistics and data analysis, with applications ranging from clustering to social science. Hartigan's Dip Test is a classical nonparametric procedure for testing unimodality versus multimodality, but its interpretation is hindered by strong dependence on sample size and the need for lookup tables. We introduce the Z-Dip, a standardized extension of the Dip Test that removes sample-size dependence by comparing observed Dip values to simulated null distributions. We calibrate a universal decision threshold for Z-Dip via simulation and bootstrap resampling, providing a unified criterion for multimodality detection. In the final section, we also propose a downsampling-based approach to further mitigate residual sample-size effects in very large datasets. Lookup tables and software implementations are made available for efficient use in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01705v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edoardo Di Martino, Matteo Cinelli, Roy Cerqueti</dc:creator>
    </item>
    <item>
      <title>RESOLVE-IPD: High-Fidelity Individual Patient Data Reconstruction and Uncertainty-Aware Subgroup Meta-Analysis</title>
      <link>https://arxiv.org/abs/2511.01785</link>
      <description>arXiv:2511.01785v1 Announce Type: new 
Abstract: Individual patient data (IPD) from oncology trials are essential for reliable evidence synthesis but are rarely publicly available, necessitating reconstruction from published Kaplan-Meier (KM) curves. Existing reconstruction methods suffer from digitization errors, unrealistic uniform censoring assumptions, and the inability to recover subgroup-level IPD when only aggregate statistics are available. We developed RESOLVE-IPD, a unified computational framework that enables high-fidelity IPD reconstruction and uncertainty-aware subgroup meta-analysis to address these limitations. RESOLVE-IPD comprises two components. The first component, High-Fidelity IPD Reconstruction, integrates the VEC-KM and CEN-KM modules: VEC-KM extracts precise KM coordinates and explicit censoring marks from vectorized figures, minimizing digitization error, while CEN-KM corrects overlapping censor symbols and eliminates the uniform censoring assumption. The second component, Uncertainty-Aware Subgroup Recovery, employs the MAPLE (Marginal Assignment of Plausible Labels and Evidence Propagation) algorithm to infer patient-level subgroup labels consistent with published summary statistics (e.g., hazard ratio, median overall survival) when subgroup KM curves are unavailable. MAPLE generates ensembles of mathematically valid labelings, facilitating a propagating meta-analysis that quantifies and reflects uncertainty from subgroup reconstruction. RESOLVE-IPD was validated through a subgroup meta-analysis of four trials in advanced esophageal squamous cell carcinoma, focusing on the programmed death ligand 1 (PD-L1)-low population. RESOLVE-IPD enables accurate IPD reconstruction and robust, uncertainty-aware subgroup meta-analyses, strengthening the reliability and transparency of secondary evidence synthesis in precision oncology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01785v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lang Lang, Yao Zhao, Qiuxin Gao, Yanxun Xu</dc:creator>
    </item>
    <item>
      <title>flowengineR: A Modular and Extensible Framework for Fair and Reproducible Workflow Design in R</title>
      <link>https://arxiv.org/abs/2511.00079</link>
      <description>arXiv:2511.00079v1 Announce Type: cross 
Abstract: flowengineR is an R package designed to provide a modular and extensible framework for building reproducible algorithmic workflows for general-purpose machine learning pipelines. It is motivated by the rapidly evolving field of algorithmic fairness where new metrics, mitigation strategies, and machine learning methods continuously emerge. A central challenge in fairness, but also far beyond, is that existing toolkits either focus narrowly on single interventions or treat reproducibility and extensibility as secondary considerations rather than core design principles. flowengineR addresses this by introducing a unified architecture of standardized engines for data splitting, execution, preprocessing, training, inprocessing, postprocessing, evaluation, and reporting. Each engine encapsulates one methodological task yet communicates via a lightweight interface, ensuring workflows remain transparent, auditable, and easily extensible. Although implemented in R, flowengineR builds on ideas from workflow languages (CWL, YAWL), graph-oriented visual programming languages (KNIME), and R frameworks (BatchJobs, batchtools). Its emphasis, however, is less on orchestrating engines for resilient parallel execution but rather on the straightforward setup and management of distinct engines as data structures. This orthogonalization enables distributed responsibilities, independent development, and streamlined integration. In fairness context, by structuring fairness methods as interchangeable engines, flowengineR lets researchers integrate, compare, and evaluate interventions across the modeling pipeline. At the same time, the architecture generalizes to explainability, robustness, and compliance metrics without core modifications. While motivated by fairness, it ultimately provides a general infrastructure for any workflow context where reproducibility, transparency, and extensibility are essential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00079v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maximilian Willer, Peter Ruckdeschel</dc:creator>
    </item>
    <item>
      <title>Gradient Boosted Mixed Models: Flexible Joint Estimation of Mean and Variance Components for Clustered Data</title>
      <link>https://arxiv.org/abs/2511.00217</link>
      <description>arXiv:2511.00217v1 Announce Type: cross 
Abstract: Linear mixed models are widely used for clustered data, but their reliance on parametric forms limits flexibility in complex and high-dimensional settings. In contrast, gradient boosting methods achieve high predictive accuracy through nonparametric estimation, but do not accommodate clustered data structures or provide uncertainty quantification.
  We introduce Gradient Boosted Mixed Models (GBMixed), a framework and algorithm that extends boosting to jointly estimate mean and variance components via likelihood-based gradients. In addition to nonparametric mean estimation, the method models both random effects and residual variances as potentially covariate-dependent functions using flexible base learners such as regression trees or splines, enabling nonparametric estimation while maintaining interpretability.
  Simulations and real-world applications demonstrate accurate recovery of variance components, calibrated prediction intervals, and improved predictive accuracy relative to standard linear mixed models and nonparametric methods. GBMixed provides heteroscedastic uncertainty quantification and introduces boosting for heterogeneous random effects. This enables covariate-dependent shrinkage for cluster-specific predictions to adapt between population and cluster-level data. Under standard causal assumptions, the framework enables estimation of heterogeneous treatment effects with reliable uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00217v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mitchell L. Prevett, Francis K. C. Hui, Zhi Yang Tho, A. H. Welsh, Anton H. Westveld</dc:creator>
    </item>
    <item>
      <title>Cross-Validated Causal Inference: a Modern Method to Combine Experimental and Observational Data</title>
      <link>https://arxiv.org/abs/2511.00727</link>
      <description>arXiv:2511.00727v1 Announce Type: cross 
Abstract: We develop new methods to integrate experimental and observational data in causal inference. While randomized controlled trials offer strong internal validity, they are often costly and therefore limited in sample size. Observational data, though cheaper and often with larger sample sizes, are prone to biases due to unmeasured confounders. To harness their complementary strengths, we propose a systematic framework that formulates causal estimation as an empirical risk minimization (ERM) problem. A full model containing the causal parameter is obtained by minimizing a weighted combination of experimental and observational losses--capturing the causal parameter's validity and the full model's fit, respectively. The weight is chosen through cross-validation on the causal parameter across experimental folds. Our experiments on real and synthetic data show the efficacy and reliability of our method. We also provide theoretical non-asymptotic error bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00727v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuelin Yang, Licong Lin, Susan Athey, Michael I. Jordan, Guido W. Imbens</dc:creator>
    </item>
    <item>
      <title>Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering</title>
      <link>https://arxiv.org/abs/2511.01396</link>
      <description>arXiv:2511.01396v1 Announce Type: cross 
Abstract: Cluster DAGs (C-DAGs) provide an abstraction of causal graphs in which nodes represent clusters of variables, and edges encode both cluster-level causal relationships and dependencies arisen from unobserved confounding. C-DAGs define an equivalence class of acyclic causal graphs that agree on cluster-level relationships, enabling causal reasoning at a higher level of abstraction. However, when the chosen clustering induces cycles in the resulting C-DAG, the partition is deemed inadmissible under conventional C-DAG semantics. In this work, we extend the C-DAG framework to support arbitrary variable clusterings by relaxing the partition admissibility constraint, thereby allowing cyclic C-DAG representations. We extend the notions of d-separation and causal calculus to this setting, significantly broadening the scope of causal reasoning across clusters and enabling the application of C-DAGs in previously intractable scenarios. Our calculus is both sound and atomically complete with respect to the do-calculus: all valid interventional queries at the cluster level can be derived using our rules, each corresponding to a primitive do-calculus step.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01396v1</guid>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cl\'ement Yvernes (APTIKAL), Emilie Devijver (APTIKAL), Ad\`ele H. Ribeiro (IECL), Marianne Clausel--Lesourd (IECL), \'Eric Gaussier (LIG, APTIKAL)</dc:creator>
    </item>
    <item>
      <title>Partial Trace-Class Bayesian Neural Networks</title>
      <link>https://arxiv.org/abs/2511.01628</link>
      <description>arXiv:2511.01628v1 Announce Type: cross 
Abstract: Bayesian neural networks (BNNs) allow rigorous uncertainty quantification in deep learning, but often come at a prohibitive computational cost. We propose three different innovative architectures of partial trace-class Bayesian neural networks (PaTraC BNNs) that enable uncertainty quantification comparable to standard BNNs but use significantly fewer Bayesian parameters. These PaTraC BNNs have computational and statistical advantages over standard Bayesian neural networks in terms of speed and memory requirements. Our proposed methodology therefore facilitates reliable, robust, and scalable uncertainty quantification in neural networks. The three architectures build on trace-class neural network priors which induce an ordering of the neural network parameters, and are thus a natural choice in our framework. In a numerical simulation study, we verify the claimed benefits, and further illustrate the performance of our proposed methodology on a real-world dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01628v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arran Carter, Torben Sell</dc:creator>
    </item>
    <item>
      <title>An Open-Access Benchmark of Statistical and Machine-Learning Anomaly Detection Methods for Battery Applications</title>
      <link>https://arxiv.org/abs/2511.01745</link>
      <description>arXiv:2511.01745v1 Announce Type: cross 
Abstract: Battery safety is critical in applications ranging from consumer electronics to electric vehicles and aircraft, where undetected anomalies could trigger safety hazards or costly downtime. In this study, we present OSBAD as an open-source benchmark for anomaly detection frameworks in battery applications. By benchmarking 15 diverse algorithms encompassing statistical, distance-based, and unsupervised machine-learning methods, OSBAD enables a systematic comparison of anomaly detection methods across heterogeneous datasets. In addition, we demonstrate how a physics- and statistics-informed feature transformation workflow enhances anomaly separability by decomposing collective anomalies into point anomalies. To address a major bottleneck in unsupervised anomaly detection due to incomplete labels, we propose a Bayesian optimization pipeline that facilitates automated hyperparameter tuning based on transfer-learning and regression proxies. Through validation on datasets covering both liquid and solid-state chemistries, we further demonstrate the cross-chemistry generalization capability of OSBAD to identify irregularities across different electrochemical systems. By making benchmarking database with open-source reproducible anomaly detection workflows available to the community, OSBAD establishes a unified foundation for developing safe, scalable, and transferable anomaly detection tools in battery analytics. This research underscores the significance of physics- and statistics-informed feature engineering as well as model selection with probabilistic hyperparameter tuning, in advancing trustworthy, data-driven diagnostics for safety-critical energy systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01745v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mei-Chin Pang, Suraj Adhikari, Takuma Kasahara, Nagihiro Haba, Saneyuki Ohno</dc:creator>
    </item>
    <item>
      <title>Selecting the Best Optimizing System</title>
      <link>https://arxiv.org/abs/2201.03065</link>
      <description>arXiv:2201.03065v2 Announce Type: replace 
Abstract: We formulate selecting the best optimizing system (SBOS) problems and provide solutions for those problems. In an SBOS problem, a finite number of systems are contenders. Inside each system, a continuous decision variable affects the system's expected performance. An SBOS problem compares different systems based on their expected performances under their own optimally chosen decision to select the best, without advance knowledge of expected performances of the systems nor the optimizing decision inside each system. We design easy-to-implement algorithms that adaptively chooses a system and a choice of decision to evaluate the noisy system performance, sequentially eliminates inferior systems, and eventually recommends a system as the best after spending a user-specified budget. The proposed algorithms integrate the stochastic gradient descent method and the sequential elimination method to simultaneously exploit the structure inside each system and make comparisons across systems. For the proposed algorithms, we prove exponential rates of convergence to zero for the probability of false selection, as the budget grows to infinity. We conduct three numerical examples that represent three practical cases of SBOS problems. Our proposed algorithms demonstrate consistent and stronger performances in terms of the probability of false selection over benchmark algorithms under a range of problem settings and sampling budgets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.03065v2</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nian Si, Yifu Tang, Zeyu Zheng</dc:creator>
    </item>
    <item>
      <title>Causal Regularization: On the trade-off between in-sample risk and out-of-sample risk guarantees</title>
      <link>https://arxiv.org/abs/2205.01593</link>
      <description>arXiv:2205.01593v3 Announce Type: replace 
Abstract: Invariant prediction uses the prediction stability of causal relationships across different environments to identify causal variables. Conversely, using causal variables gives prediction guarantees even in out-of-sample data settings. In this paper, we investigate the identification of causal-like models from in-sample data that ensure out-of-sample risk guarantees when predicting a target variable from an arbitrary set of covariates.
  Ordinary least squares minimizes in-sample risk but offers limited out-of-sample guarantees, while causal models optimize out-of-sample guarantees at the expense of in-sample performance. We introduce a form of \textit{causal regularization} to balance these properties. In the population setting, higher regularization yields estimators with greater risk stability, albeit with increased in-sample risk. Empirically, however, there is a further trade-off to consider, as finite in-sample data reduced the ability to correctly identify models with high out-of-sample risk guarantees. We show how in such empirical settings the optimal causal regularizer can be found via cross-validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.01593v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Kania, Ernst Wit</dc:creator>
    </item>
    <item>
      <title>Model-based Clustering for Network Data via a Latent Shrinkage Position Cluster Model</title>
      <link>https://arxiv.org/abs/2310.03630</link>
      <description>arXiv:2310.03630v2 Announce Type: replace 
Abstract: Low-dimensional representation and clustering of network data are tasks of great interest across various fields. Latent position models are routinely used for this purpose by assuming that each node has a location in a low-dimensional latent space, and by enabling node clustering. However, these models fall short through their inability to simultaneously determine the latent space dimension and number of clusters. Here we introduce the latent shrinkage position cluster model (LSPCM), which addresses this limitation. The LSPCM posits an infinite dimensional latent space and assumes a Bayesian nonparametric shrinkage prior on the latent positions' variance parameters resulting in higher dimensions having increasingly smaller variances, aiding the identification of dimensions with non-negligible variance. Further, the LSPCM assumes the latent positions follow a sparse finite Gaussian mixture model, allowing for automatic inference on the number of clusters related to non-empty mixture components. As a result, the LSPCM simultaneously infers the effective dimension of the latent space and the number of clusters, eliminating the need to fit and compare multiple models. The performance of the LSPCM is assessed via simulation studies and demonstrated through application to two real Twitter network datasets from sporting and political contexts. Open source software is available to facilitate widespread use of the LSPCM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.03630v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xian Yao Gwee, Isobel Claire Gormley, Michael Fop</dc:creator>
    </item>
    <item>
      <title>The trivariate wrapped Cauchy copula</title>
      <link>https://arxiv.org/abs/2401.10824</link>
      <description>arXiv:2401.10824v3 Announce Type: replace 
Abstract: In this paper, we propose a new flexible distribution for data on the three-dimensional torus which we call a trivariate wrapped Cauchy copula. Our trivariate copula has several attractive properties. It has a simple form of density and desirable modality properties. Its parameters allow for an adjustable degree of dependence between every pair of variables and these can be easily estimated. The conditional distributions of the model are well studied bivariate wrapped Cauchy distributions. Furthermore, the distribution can be easily simulated. Parameter estimation via maximum likelihood for the distribution is given and we highlight the simple implementation procedure to obtain these estimates. We illustrate our trivariate wrapped Cauchy copula on data from protein bioinformatics of conformational angles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10824v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shogo Kato, Christophe Ley, Sophia Loizidou, Kanti V. Mardia</dc:creator>
    </item>
    <item>
      <title>Impact of Near-Positivity Violations on IPTW-Estimated Marginal Structural Survival Models With Time-Dependent Confounding</title>
      <link>https://arxiv.org/abs/2403.19606</link>
      <description>arXiv:2403.19606v3 Announce Type: replace 
Abstract: In longitudinal observational studies, marginal structural models (MSMs) are a class of causal models used to analyse the effect of an exposure on the (time-to-event) outcome of interest, while accounting for exposure-affected time-dependent confounding. In the applied literature, inverse probability of treatment weighting (IPTW) has been widely adopted to estimate MSMs. An essential assumption for IPTW-based MSMs is the positivity assumption, which ensures that, for any combination of measured confounders among individuals, there is a non-zero probability of receiving each possible treatment strategy. Positivity is crucial for valid causal inference through IPTW-based MSMs, but is often overlooked compared to confounding bias. Positivity violations may also arise due to randomness, in situations where the assignment to a specific treatment is theoretically possible but is either absent or rarely observed in the data, leading to near violations. These situations are common in practical applications, particularly when the sample size is small, and they pose significant challenges for causal inference. This study investigates the impact of near-positivity violations on estimates from IPTW-based MSMs in survival analysis. Two algorithms are proposed for simulating longitudinal data from hazard-MSMs, accommodating near-positivity violations, a time-varying binary exposure, and a time-to-event outcome. Cases of near-positivity violations, where remaining unexposed is rare within certain confounder levels, are analysed across various scenarios and weight truncation (WT) strategies. This work aims to serve as a critical warning against overlooking the positivity assumption or naively applying WT in causal studies using longitudinal observational data and IPTW.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19606v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/bimj.70093</arxiv:DOI>
      <arxiv:journal_reference>Biometrical Journal 2025, 67, no. 6: e70093</arxiv:journal_reference>
      <dc:creator>Marta Spreafico</dc:creator>
    </item>
    <item>
      <title>Bayesian Nonparametrics for Principal Stratification with Continuous Post-Treatment Variables</title>
      <link>https://arxiv.org/abs/2405.17669</link>
      <description>arXiv:2405.17669v3 Announce Type: replace 
Abstract: Principal stratification provides a causal inference framework for investigating treatment effects in the presence of a post-treatment variable. Principal strata play a key role in characterizing the treatment effect by identifying groups of units with the same or similar values for the potential post-treatment variable at all treatment levels. The literature has focused mainly on binary post-treatment variables. Few papers considered continuous post-treatment variables. In the presence of a continuous post-treatment, a challenge is how to identify and characterize meaningful coarsening of the latent principal strata that lead to interpretable principal causal effects. This paper introduces the Confounders-Aware SHared atoms BAyesian mixture (CASBAH), a novel approach for principal stratification with binary treatment and continuous post-treatment variables. CASBAH leverages Bayesian nonparametric priors with an innovative hierarchical structure for the potential post-treatment outcomes that overcomes some of the limitations of previous works. Specifically, the novel features of our method allow for (i) identifying coarsened principal strata through a data-adaptive approach and (ii) providing a comprehensive quantification of the uncertainty surrounding stratum membership. Through Monte Carlo simulations, we show that the proposed methodology performs better than existing methods in characterizing the principal strata and estimating principal effects of the treatment. Finally, CASBAH is applied to a case study in which we estimate the causal effects of US national air quality regulations on pollution levels and health outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17669v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dafne Zorzetto, Antonio Canale, Fabrizia Mealli, Francesca Dominici, Falco J. Bargagli-Stoffi</dc:creator>
    </item>
    <item>
      <title>bayesCureRateModel: Bayesian Cure Rate Modeling for Time to Event Data in R</title>
      <link>https://arxiv.org/abs/2409.10221</link>
      <description>arXiv:2409.10221v2 Announce Type: replace 
Abstract: The family of cure models provides a unique opportunity to simultaneously model both the proportion of cured subjects (those not facing the event of interest) and the distribution function of time-to-event for susceptibles (those facing the event). In practice, the application of cure models is mainly facilitated by the availability of various R packages. However, most of these packages primarily focus on the mixture or promotion time cure rate model. This article presents a fully Bayesian approach implemented in R to estimate a general family of cure rate models in the presence of covariates. It builds upon the work by Papastamoulis and Milienos (2024) by additionally considering various options for describing the promotion time, including the Weibull, exponential, Gompertz, log-logistic and finite mixtures of gamma distributions, among others. Moreover, the user can choose any proper distribution function for modeling the promotion time (provided that some specific conditions are met). Posterior inference is carried out by constructing a Metropolis-coupled Markov chain Monte Carlo (MCMC) sampler, which combines Gibbs sampling for the latent cure indicators and Metropolis-Hastings steps with Langevin diffusion dynamics for parameter updates. The main MCMC algorithm is embedded within a parallel tempering scheme by considering heated versions of the target posterior distribution. The package is illustrated on a real dataset analyzing the duration of the first marriage under the presence of various covariates such as the race, age and the presence of kids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10221v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Panagiotis Papastamoulis, Fotios Milienos</dc:creator>
    </item>
    <item>
      <title>A General Latent Embedding Approach for Modeling Non-uniform High-dimensional Sparse Hypergraphs with Multiplicity</title>
      <link>https://arxiv.org/abs/2410.12108</link>
      <description>arXiv:2410.12108v2 Announce Type: replace 
Abstract: Recent research has shown growing interest in modeling hypergraphs, which capture polyadic interactions among entities beyond traditional dyadic relations. However, most existing methodologies for hypergraphs face significant limitations, including their heavy reliance on uniformity restrictions for hyperlink orders and their inability to account for repeated observations of identical hyperlinks. In this work, we introduce a novel and general latent embedding approach that addresses these challenges through the integration of latent embeddings, vertex degree heterogeneity parameters, and an order-adjusting parameter. Theoretically, we investigate the identifiability conditions for the latent embeddings and associated parameters, and we establish the convergence rates of their estimators along with asymptotic distributions. Computationally, we employ a projected gradient ascent algorithm for parameter estimation. Comprehensive simulation studies demonstrate the effectiveness of the algorithm and validate the theoretical findings. Moreover, an application to a co-citation hypergraph illustrates the advantages of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12108v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shihao Wu, Gongjun Xu, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>Agnostic Characterization of Interference in Randomized Experiments</title>
      <link>https://arxiv.org/abs/2410.13142</link>
      <description>arXiv:2410.13142v4 Announce Type: replace 
Abstract: We give an approach for characterizing interference by lower bounding the number of units whose outcome depends on selected groups of treated individuals, such as depending on the treatment of others, or others who are at least a certain distance away. The approach is applicable to randomized experiments with binary-valued outcomes. Asymptotically conservative point estimates and one-sided confidence intervals may be constructed with no assumptions beyond the known randomization design, allowing the approach to be used when interference is poorly understood, or when an observed network might only be a crude proxy for the underlying social mechanisms. Point estimates are equal to H\'{a}jek-weighted comparisons of units with differing levels of treatment exposure. Empirically, we find that the width of our interval estimates is competitive with (and often smaller than) those of the EATE, an assumption-lean treatment effect, suggesting that the proposed estimands may be intrinsically easier to estimate than treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13142v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Choi</dc:creator>
    </item>
    <item>
      <title>Characterizing the Effects of Environmental Exposures on Social Mobility: Bayesian Semi-parametrics for Principal Stratification</title>
      <link>https://arxiv.org/abs/2412.00311</link>
      <description>arXiv:2412.00311v3 Announce Type: replace 
Abstract: Understanding the causal effects of air pollution exposures on social mobility is attracting increasing attention. At the same time, education is widely recognized as a key driver of social mobility. However, the causal pathways linking fine particulate matter (PM2.5) exposure, educational attainment, and social mobility remain largely unexplored. To address this, we adopt the principal stratification approach, which rigorously defines causal effects when a post-treatment variable--educational attainment--is affected by exposure--PM2.5--and may, in turn, affect the primary outcome--social mobility. To estimate the causal effects, we propose a Bayesian semi-parametric method leveraging infinite mixtures for modeling the primary outcome. The proposed method (i) allows flexible modeling of the distribution of the primary potential outcomes, (ii) improves the accuracy of counterfactual imputation--a fundamental problem in causal inference framework--, and (iii) enables the characterization of treatment effects across different values of the post-treatment variable. We evaluate the performance of the proposed methodology through a Monte Carlo simulation study, demonstrating its advantages over existing approaches. Finally, we apply our method to a national dataset of 3,009 counties in the United States to estimate the causal effect of PM2.5 on social mobility, taking into account educational attainment as a post-treatment variable. Our findings indicate that in counties where higher PM2.5 exposure significantly reduces educational attainment social mobility decreases by approximately 5% compared to counties with lower PM2.5 exposure. We also find that in counties where exposure to PM2.5 does not affect educational attainment, social mobility is reduced by approximately 2% hinting at the possibility of further, yet unexplored, pathways connecting air pollution and social mobility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00311v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dafne Zorzetto, Paolo Dalla Torre, Sonia Petrone, Francesca Dominici, Falco J. Bargagli-Stoffi</dc:creator>
    </item>
    <item>
      <title>Bayesian shrinkage priors for penalized synthetic control estimators in the presence of spillovers</title>
      <link>https://arxiv.org/abs/2501.08231</link>
      <description>arXiv:2501.08231v3 Announce Type: replace 
Abstract: Synthetic control (SC) methods are widely used to estimate the effects of policy interventions, especially those targeting specific geographic regions, referred to as units. These methods construct a weighted combination of untreated units, forming a "synthetic" control that approximates the counterfactual outcomes of the treated unit had the intervention not occurred. Although neighboring areas are often selected as controls due to their similarity in observed and unobserved characteristics, their proximity can lead to spillover effects, where the intervention indirectly impacts control units, potentially biasing causal estimates. To address this challenge, we introduce a Bayesian SC framework with utility-based shrinkage priors. Our approach extends traditional penalization techniques (i.e., horseshoe, spike-and-slab) by incorporating a utility function that combines covariate similarity and spatial distance. This provides a metric that guides the data-driven selection of control units based on their relevance and spillover risk, which is assumed to increase with spatial proximity. Rather than outright excluding neighboring units, the method balances bias and variance by reducing the importance of potentially contaminated controls by spillovers. We evaluate the proposed method through simulation studies at varying spillover levels and apply it to assess the impact of Philadelphia's 2017 beverage tax on the sales of sugar-sweetened and artificially sweetened beverages in mass merchandise stores.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08231v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Esteban Fern\'andez-Morales, Arman Oganisian, Youjin Lee</dc:creator>
    </item>
    <item>
      <title>bayesNMF: Fast Bayesian Poisson NMF with Automatically Learned Rank Applied to Mutational Signatures</title>
      <link>https://arxiv.org/abs/2502.18674</link>
      <description>arXiv:2502.18674v2 Announce Type: replace 
Abstract: Bayesian Non-Negative Matrix Factorization (NMF) is a method of interest in fields including genomics, neuroscience, and audio and image processing. Bayesian Poisson NMF is of particular importance for counts data, for example in cancer mutational signature analysis. However, MCMC methods for Bayesian Poisson NMF require a computationally intensive augmentation. Further, identifying latent rank is necessary, but commonly used heuristic approaches are slow and potentially subjective, while methods that learn rank automatically are unable to provide posterior uncertainties. In this paper, we introduce bayesNMF, a computationally efficient Gibbs sampler for Bayesian Poisson NMF. Metropolis-Hastings steps are used to avoid augmentation, where full conditionals from a Normal-likelihood NMF is used as geometry-informed, high-overlap proposals. We additionally define sparse Bayesian factor inclusion (SBFI) as a method to identify rank automatically while providing posterior uncertainty quantification. We provide an open-source R software package with all of the models and plotting capabilities demonstrated in this paper on GitHub at jennalandy/bayesNMF, and supplemental materials are available online. Although our applications focus on cancer mutational signatures, our software and results can be extended to any use of Bayesian Poisson NMF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18674v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jenna M. Landy, Nishanth Basava, Giovanni Parmigiani</dc:creator>
    </item>
    <item>
      <title>Geodesic Synthetic Control Methods for Random Objects and Functional Data</title>
      <link>https://arxiv.org/abs/2505.00331</link>
      <description>arXiv:2505.00331v2 Announce Type: replace 
Abstract: We introduce a geodesic synthetic control method for causal inference that extends existing synthetic control methods to scenarios where outcomes are elements in a geodesic metric space rather than scalars. Examples of such outcomes include distributions, compositions, networks, trees and functional data, among other data types that can be viewed as elements of a geodesic metric space given a suitable metric. We extend this further to geodesic synthetic difference-in-differences that builds on the established synthetic difference-in-differences for Euclidean outcomes. This estimator generalizes both the geodesic synthetic control method and a previously proposed geodesic difference-in-differences method and exhibits a double robustness property. The proposed geodesic synthetic control method is illustrated through comprehensive simulation studies and applications to the employment composition changes following the 2011 Great East Japan Earthquake, and the impact of abortion liberalization policy on fertility patterns in East Germany. We illustrate the proposed geodesic synthetic difference-in-differences by studying the consequences of the Soviet Union's collapse on age-at-death distributions for males and females.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00331v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daisuke Kurisu, Yidong Zhou, Taisuke Otsu, Hans-Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>Statistical Analysis of Conditional Group Distributionally Robust Optimization with Cross-Entropy Loss</title>
      <link>https://arxiv.org/abs/2507.09905</link>
      <description>arXiv:2507.09905v2 Announce Type: replace 
Abstract: In multi-source learning with discrete labels, distributional heterogeneity across domains poses a central challenge to developing predictive models that transfer reliably to unseen domains. We study multi-source unsupervised domain adaptation, where labeled data are available from multiple source domains and only unlabeled data are observed from the target domain. To address potential distribution shifts, we propose a novel Conditional Group Distributionally Robust Optimization (CG-DRO) framework that learns a classifier by minimizing the worst-case cross-entropy loss over the convex combinations of the conditional outcome distributions from sources domains. We develop an efficient Mirror Prox algorithm for solving the minimax problem and employ a double machine learning procedure to estimate the risk function, ensuring that errors in nuisance estimation contribute only at higher-order rates. We establish fast statistical convergence rates for the empirical CG-DRO estimator by constructing two surrogate minimax optimization problems that serve as theoretical bridges. A distinguishing challenge for CG-DRO is the emergence of nonstandard asymptotics: the empirical CG-DRO estimator may fail to converge to a standard limiting distribution due to boundary effects and system instability. To address this, we introduce a perturbation-based inference procedure that enables uniformly valid inference, including confidence interval construction and hypothesis testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09905v2</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zijian Guo, Zhenyu Wang, Yifan Hu, Francis Bach</dc:creator>
    </item>
    <item>
      <title>A Bayesian Dirichlet Auto-Regressive Conditional Heteroskedasticity Model for Forecasting Currency Shares</title>
      <link>https://arxiv.org/abs/2507.14132</link>
      <description>arXiv:2507.14132v2 Announce Type: replace 
Abstract: We analyze daily Airbnb service-fee shares across eleven settlement currencies, a compositional series that shows bursts of volatility after shocks such as the COVID-19 pandemic. Standard Dirichlet time series models assume constant precision and therefore miss these episodes. We introduce B-DARMA-DARCH, a Bayesian Dirichlet autoregressive moving average model with a Dirichlet ARCH component, which lets the precision parameter follow an ARMA recursion. The specification preserves the Dirichlet likelihood so forecasts remain valid compositions while capturing clustered volatility. Simulations and out-of-sample tests show that B-DARMA-DARCH lowers forecast error and improves interval calibration relative to Dirichlet ARMA and log-ratio VARMA benchmarks, providing a concise framework for settings where both the level and the volatility of proportions matter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14132v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison Katz, Robert E. Weiss</dc:creator>
    </item>
    <item>
      <title>Penalized Empirical Likelihood for Doubly Robust Causal Inference under Contamination in High Dimensions</title>
      <link>https://arxiv.org/abs/2507.17439</link>
      <description>arXiv:2507.17439v4 Announce Type: replace 
Abstract: We propose a doubly robust estimator for the average treatment effect in high dimensional
  low sample size observational studies, where contamination and model misspecification pose
  serious inferential challenges. The estimator combines bounded influence estimating equations
  for outcome modeling with covariate balancing propensity scores for treatment assignment,
  embedded within a penalized empirical likelihood framework using nonconvex regularization.
  It satisfies the oracle property by jointly achieving consistency under partial model correct ness, selection consistency, robustness to contamination, and asymptotic normality. For uncertainty quantification, we derive a finite sample confidence interval using cumulant generating
  functions and influence function corrections, avoiding reliance on asymptotic approximations.
  Simulation studies and applications to gene expression datasets (Golub and Khan) demonstrate superior performance in bias, error metrics, and interval calibration, highlighting the
  method robustness and inferential validity in HDLSS regimes. One notable aspect is that
  even in the absence of contamination, the proposed estimator and its confidence interval
  remain efficient compared to those of competing models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17439v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Byeonghee Lee, Sangwook Kang, Ju-Hyun Park, Saebom Jeon, Joonsung Kang</dc:creator>
    </item>
    <item>
      <title>Hypothesis testing for community structure in temporal networks using e-values</title>
      <link>https://arxiv.org/abs/2507.23034</link>
      <description>arXiv:2507.23034v2 Announce Type: replace 
Abstract: Community structure in networks naturally arises in various applications. But while the topic has received significant attention for static networks, the literature on community structure in temporally evolving networks is more scarce. In particular, there are currently no statistical methods available to test for the presence of community structure in a sequence of networks evolving over time. In this work, we propose a simple yet powerful test using e-values, an alternative to p-values that is more flexible in certain ways. Specifically, an e-value framework retains valid testing properties even after combining dependent information, a relevant feature in the context of testing temporal networks. We apply the proposed test to synthetic and real-world networks, demonstrating various features inherited from the e-value formulation and exposing some of the inherent difficulties of testing on temporal networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23034v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Yanchenko, Jonathan P. Williams, Ryan Martin</dc:creator>
    </item>
    <item>
      <title>Mendelian Randomization Methods for Causal Inference: Estimands, Identification and Inference</title>
      <link>https://arxiv.org/abs/2509.11519</link>
      <description>arXiv:2509.11519v2 Announce Type: replace 
Abstract: Mendelian randomization (MR) has become an essential tool for causal inference in biomedical and public health research. By using genetic variants as instrumental variables, MR helps address unmeasured confounding and reverse causation, offering a quasi-experimental framework to evaluate causal effects of modifiable exposures on health outcomes. Despite its promise, MR faces substantial methodological challenges, including invalid instruments, weak instrument bias, and design complexities across different data structures. In this tutorial review, we provide a comprehensive overview of MR methods for causal inference, emphasizing clarity of causal interpretation, study design comparisons, availability of software tools, and practical guidance for applied scientists. We organize the review around causal estimands, ensuring that analyses are anchored to well-defined causal questions. We discuss the problems of invalid and weak instruments, comparing available strategies for their detection and correction. We integrate discussions of population-based versus family-based MR designs, analyses based on individual-level versus summary-level data, and one-sample versus two-sample MR designs, highlighting their relative advantages and limitations. We also summarize recent methodological advances and software developments that extend MR to settings with many weak or invalid instruments and to modern high-dimensional omics data. Real-data applications, including UK Biobank and Alzheimer's disease proteomics studies, illustrate the use of these methods in practice. This review aims to serve as a tutorial-style reference for both methodologists and applied scientists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11519v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minhao Yao, Anqi Wang, Xihao Li, Zhonghua Liu</dc:creator>
    </item>
    <item>
      <title>Understanding and Using the Relative Importance Measures Based on Orthogonalization and Reallocation</title>
      <link>https://arxiv.org/abs/2510.13389</link>
      <description>arXiv:2510.13389v2 Announce Type: replace 
Abstract: A class of relative importance measures based on orthogonalization and reallocation, ORMs, has been found to effectively approximate the General Dominance index (GD). In particular, Johnson's Relative Weight (RW) has been deemed the most successful ORM in the literature. Nevertheless, the theoretical foundation of the ORMs remains unclear. To further understand the ORMs, we provide a generalized framework that breaks down the ORM into two functional steps: orthogonalization and reallocation. To assess the impact of each step on the performance of ORMs, we conduct extensive Monte Carlo simulations under various predictors' correlation structures and response variable distributions. Our findings reveal that Johnson's minimal transformation consistently outperforms other common orthogonalization methods. We also summarize the performance of reallocation methods under four scenarios of predictors' correlation structures in terms of the first principal component and the variance inflation factor (VIF). This analysis provides guidelines for selecting appropriate reallocation methods in different scenarios, illustrated with real-world dataset examples. Our research offers a deeper understanding of ORMs and provides valuable insights for practitioners seeking to accurately measure variable importance in various modeling contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13389v2</guid>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tien-En Chang, Argon Chen</dc:creator>
    </item>
    <item>
      <title>From Stochasticity to Signal: A Bayesian Latent State Model for Reliable Measurement with LLMs</title>
      <link>https://arxiv.org/abs/2510.23874</link>
      <description>arXiv:2510.23874v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly used to automate classification tasks in business, such as analyzing customer satisfaction from text. However, the inherent stochasticity of LLMs, in terms of their tendency to produce different outputs for the same input, creates a significant measurement error problem that is often neglected with a single round of output, or addressed with ad-hoc methods like majority voting. Such naive approaches fail to quantify uncertainty and can produce biased estimates of population-level metrics. In this paper, we propose a principled solution by reframing LLM variability as a statistical measurement error problem and introducing a Bayesian latent state model to address it. Our model treats the true classification (e.g., customer dissatisfaction) as an unobserved latent variable and the multiple LLM ratings as noisy measurements of this state. This framework allows for the simultaneous estimation of the LLM's false positive and false negative error rates, the underlying base rate of the phenomenon in the population, the posterior probability of the true state for each individual observation, and the causal impact of a business intervention, if any, on the latent state. Through simulation studies, we demonstrate that our model accurately recovers true parameters where naive methods fail. We conclude that this methodology provides a general and reliable framework for converting noisy, probabilistic outputs from LLMs into accurate and actionable insights for scientific and business applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23874v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yichi Zhang, Ignacio Martinez</dc:creator>
    </item>
    <item>
      <title>Automatic selection of hyper-parameters via the use of softened profile likelihood</title>
      <link>https://arxiv.org/abs/2510.25632</link>
      <description>arXiv:2510.25632v2 Announce Type: replace 
Abstract: We extend a heuristic method for automatic dimensionality selection, which maximizes a profile likelihood to identify "elbows" in scree plots. Our extension enables researchers to make automatic choices of multiple hyper-parameters simultaneously. To facilitate our extension to multi-dimensions, we propose a "softened" profile likelihood. We present two distinct parameterizations of our solution and demonstrate our approach on elastic nets, support vector machines, and neural networks. We also briefly discuss applications of our method to other data-analytic tasks than hyper-parameter selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25632v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gengyang Chen, Mu Zhu</dc:creator>
    </item>
    <item>
      <title>A KL-divergence based test for elliptical distribution</title>
      <link>https://arxiv.org/abs/2510.26775</link>
      <description>arXiv:2510.26775v2 Announce Type: replace 
Abstract: We conduct a KL-divergence based procedure for testing elliptical distributions. The procedure simultaneously takes into account the two defining properties of an elliptically distributed random vector: independence between length and direction, and uniform distribution of the direction. The test statistic is constructed based on the $k$ nearest neighbors ($k$NN) method, and two cases are considered where the mean vector and covariance matrix are known and unknown. First-order asymptotic properties of the test statistic are rigorously established by creatively utilizing sample splitting, truncation and transformation between Euclidean space and unit sphere, while avoiding assuming Fr\'echet differentiability of any functionals. Debiasing and variance inflation are further proposed to treat the degeneration of the influence function. Numerical implementations suggest better size and power performance than the state of the art procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26775v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yin Tang, Yanyuan Ma, Bing Li</dc:creator>
    </item>
    <item>
      <title>Change-in-velocity detection for multidimensional data</title>
      <link>https://arxiv.org/abs/2510.27150</link>
      <description>arXiv:2510.27150v2 Announce Type: replace 
Abstract: In this work, we introduce CPLASS (Continuous Piecewise-Linear Approximation via Stochastic Search), an algorithm for detecting changes in velocity within multidimensional data. The one-dimensional version of this problem is known as the change-in-slope problem (see Fearnhead &amp; Grose (2022), Baranowski et al. (2019)). Unlike traditional changepoint detection methods that focus on changes in mean, detecting changes in velocity requires a specialized approach due to continuity constraints and parameter dependencies, which frustrate popular algorithms like binary segmentation and dynamic programming. To overcome these difficulties, we introduce a specialized penalty function to balance improvements in likelihood due to model complexity, and a Markov Chain Monte Carlo (MCMC)-based approach with tailored proposal mechanisms for efficient parameter exploration. Our method is particularly suited for analyzing intracellular transport data, where the multidimensional trajectories of microscale cargo are driven by teams of molecular motors that undergo complex biophysical transitions. To ensure biophysical realism in the results, we introduce a speed penalty that discourages overfitted of short noisy segments while maintaining consistency in the large-sample limit. Additionally, we introduce a summary statistic called the Cumulative Speed Allocation, which is robust with respect to idiosyncracies of changepoint detection while maintaining the ability to discriminate between biophysically distinct populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27150v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linh Do, Dat Do, Keisha J. Cook, Scott A. McKinley</dc:creator>
    </item>
    <item>
      <title>You Are the Best Reviewer of Your Own Papers: The Isotonic Mechanism</title>
      <link>https://arxiv.org/abs/2206.08149</link>
      <description>arXiv:2206.08149v3 Announce Type: replace-cross 
Abstract: Machine learning (ML) and artificial intelligence (AI) conferences including NeurIPS and ICML have experienced a significant decline in peer review quality in recent years. To address this growing challenge, we introduce the Isotonic Mechanism, a computationally efficient approach to enhancing the accuracy of noisy review scores by incorporating authors' private assessments of their submissions. Under this mechanism, authors with multiple submissions are required to rank their papers in descending order of perceived quality. Subsequently, the raw review scores are calibrated based on this ranking to produce adjusted scores. We prove that authors are incentivized to truthfully report their rankings because doing so maximizes their expected utility, modeled as an additive convex function over the adjusted scores. Moreover, the adjusted scores are shown to be more accurate than the raw scores, with improvements being particularly significant when the noise level is high and the author has many submissions -- a scenario increasingly prevalent at large-scale ML/AI conferences.
  We further investigate whether submission quality information beyond a simple ranking can be truthfully elicited from authors. We establish that a necessary condition for truthful elicitation is that the mechanism be based on pairwise comparisons of the author's submissions. This result underscores the optimality of the Isotonic Mechanism, as it elicits the most fine-grained truthful information among all mechanisms we consider. We then present several extensions, including a demonstration that the mechanism maintains truthfulness even when authors have only partial rather than complete information about their submission quality. Finally, we discuss future research directions, focusing on the practical implementation of the mechanism and the further development of a theoretical framework inspired by our mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.08149v3</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Weijie Su</dc:creator>
    </item>
    <item>
      <title>On the breakdown point of transport-based quantiles</title>
      <link>https://arxiv.org/abs/2410.16554</link>
      <description>arXiv:2410.16554v2 Announce Type: replace-cross 
Abstract: Recent work has used optimal transport ideas to generalize the notion of (center-outward) quantiles to dimension $d\geq 2$. We study the robustness properties of these transport-based quantiles by deriving their breakdown point, roughly, the smallest amount of contamination required to make these quantiles take arbitrarily aberrant values. We prove that the transport median defined in Chernozhukov et al.~(2017) and Hallin et al.~(2021) has breakdown point of $1/2$. Moreover, a point in the transport depth contour of order $\tau\in [0,1/2]$ has breakdown point of $\tau$. This shows that the multivariate transport depth shares the same breakdown properties as its univariate counterpart. Our proof relies on a general argument connecting the breakdown point of transport maps evaluated at a point to the Tukey depth of that point in the reference measure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16554v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Avella-Medina, Alberto Gonz\'alez-Sanz</dc:creator>
    </item>
    <item>
      <title>Testing Random Effects for Binomial Data</title>
      <link>https://arxiv.org/abs/2504.13977</link>
      <description>arXiv:2504.13977v3 Announce Type: replace-cross 
Abstract: In modern scientific research, small-scale studies with limited participants are increasingly common. However, interpreting individual outcomes can be challenging, making it standard practice to combine data across studies using random effects to draw broader scientific conclusions. In this work, we introduce an optimal methodology for assessing the goodness of fit of a reference distribution for the random effects arising from binomial counts. For meta-analyses, we also derive optimal tests to evaluate whether multiple studies are in agreement before pooling the data. In all cases, we prove that the proposed tests optimally distinguish null and alternative hypotheses separated in the 1-Wasserstein distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13977v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Kania, Larry Wasserman, Sivaraman Balakrishnan</dc:creator>
    </item>
    <item>
      <title>Pluri-Gaussian rapid updating of geological domains</title>
      <link>https://arxiv.org/abs/2506.01575</link>
      <description>arXiv:2506.01575v2 Announce Type: replace-cross 
Abstract: Over the past decade, the rapid updating of resource knowledge and the integration of real-time sensor information have gained attention in both industry and academia. However, most studies on rapid resource model updating have focused on continuous variables, such as grade variables and coal quality parameters. Geological domain modelling is an essential component of resource estimation, which is why it is crucial to extend data assimilation techniques to enable the rapid updating of categorical variables. In this paper, a methodology inspired by pluri-Gaussian simulation is proposed for near-real-time updating of geological domains, followed by updating grade variables within these domain boundaries. The proposed algorithm consists of a Gibbs sampler for converting geological domains into Gaussian random fields, an ensemble Kalman filter with multiple data assimilations for rapid updating, and rotation based iterative Gaussianisation for multi-Gaussian transformation. We demonstrate the algorithm by using a synthetic case study with observations sampled from the ground truth, as well as a real case study that uses production drilling samples to jointly update geological domains and grade variables. Both case studies are based on real data from an iron oxide-copper-gold deposit in South Australia. This approach enhances resource knowledge by incorporating both categorical and continuous variables, leading to improved reproduction of domain geometries, closer matches between predictions and observations, and more geologically realistic resource models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01575v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sultan Abulkhair, Peter Dowd, Chaoshui Xu</dc:creator>
    </item>
    <item>
      <title>Sampling by averaging: A multiscale approach to score estimation</title>
      <link>https://arxiv.org/abs/2508.15069</link>
      <description>arXiv:2508.15069v2 Announce Type: replace-cross 
Abstract: We introduce a novel framework for efficient sampling from complex, unnormalised target distributions by exploiting multiscale dynamics. Traditional score-based sampling methods either rely on learned approximations of the score function or involve computationally expensive nested Markov chain Monte Carlo (MCMC) loops. In contrast, the proposed approach leverages stochastic averaging within a slow-fast system of stochastic differential equations (SDEs) to estimate intermediate scores along a diffusion path without training or inner-loop MCMC. Two algorithms are developed under this framework: MultALMC, which uses multiscale annealed Langevin dynamics, and MultCDiff, based on multiscale controlled diffusions for the reverse-time Ornstein-Uhlenbeck process. Both overdamped and underdamped variants are considered, with theoretical guarantees of convergence to the desired diffusion path. The framework is extended to handle heavy-tailed target distributions using Student's t-based noise models and tailored fast-process dynamics. Empirical results across synthetic and real-world benchmarks, including multimodal and high-dimensional distributions, demonstrate that the proposed methods are competitive with existing samplers in terms of accuracy and efficiency, without the need for learned models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15069v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Paula Cordero-Encinar, Andrew B. Duncan, Sebastian Reich, O. Deniz Akyildiz</dc:creator>
    </item>
    <item>
      <title>Topic Analysis with Side Information: A Neural-Augmented LDA Approach</title>
      <link>https://arxiv.org/abs/2510.24918</link>
      <description>arXiv:2510.24918v2 Announce Type: replace-cross 
Abstract: Traditional topic models such as Latent Dirichlet Allocation (LDA) have been widely used to uncover latent structures in text corpora, but they often struggle to integrate auxiliary information such as metadata, user attributes, or document labels. These limitations restrict their expressiveness, personalization, and interpretability. To address this, we propose nnLDA, a neural-augmented probabilistic topic model that dynamically incorporates side information through a neural prior mechanism. nnLDA models each document as a mixture of latent topics, where the prior over topic proportions is generated by a neural network conditioned on auxiliary features. This design allows the model to capture complex nonlinear interactions between side information and topic distributions that static Dirichlet priors cannot represent. We develop a stochastic variational Expectation-Maximization algorithm to jointly optimize the neural and probabilistic components. Across multiple benchmark datasets, nnLDA consistently outperforms LDA and Dirichlet-Multinomial Regression in topic coherence, perplexity, and downstream classification. These results highlight the benefits of combining neural representation learning with probabilistic topic modeling in settings where side information is available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24918v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Biyi Fang, Truong Vo, Kripa Rajshekhar, Diego Klabjan</dc:creator>
    </item>
  </channel>
</rss>
