<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.med-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.med-ph</link>
    <description>physics.med-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.med-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 25 Nov 2024 05:02:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Injection Bias Reduction Techniques in Quantitative Angiography Using Patient-Specific Phantoms of Intracranial Aneurysm</title>
      <link>https://arxiv.org/abs/2411.14475</link>
      <description>arXiv:2411.14475v1 Announce Type: new 
Abstract: In intracranial aneurysm (IA) treatment, digital subtraction angiography (DSA) monitors device-induced hemodynamic changes. Quantitative angiography (QA) provides more precise assessments but is limited by hand-injection variability. This study evaluates correction methods using in vitro phantoms that mimic diverse aneurysm morphologies and locations, addressing the 2D and temporal limitations of DSA. We used a patient-specific phantom to replicate three distinct IA morphologies at various Circle of Willis points: the middle cerebral artery (MCA), anterior communicating artery (ACA), and the internal carotid artery (ICA), each varying in size and shape. The diameters of the IA at MCA, ACA and ICA are 10.1, 10 and 7 millimeters, respectively. QA parameters for both non-stenosed and stenosed conditions were measured with 5ml and 10ml boluses over various injection durations to generate time density curves (TDCs). To address the variability in injection, several singular value decomposition (SVD) variants, standard SVD (sSVD) with Tikhonov regularization, block-circulant SVD (bSVD), and oscillation index SVD (oSVD) were applied. These methods enabled the extraction of IA impulse response function (IRF), peak height (PHIRF), area under the curve (AUCIRF), and mean transit time (MTT). We evaluated the robustness of bias-reducing methods by observing the invariance of these parameters with respect to the injection conditions, and the location and size of the aneurysm. The application of SVD variants, sSVD, bSVD, and oSVD, significantly reduced QA parameter variability due to injection techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14475v1</guid>
      <category>physics.med-ph</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parmita Mondal, Kyle A Williams, Parisa Naghdi, Ahmad Rahmatpour, Mohammad Mahdi Shiraz Bhurwani, Swetadri Vasan Setlur Nagesh, Ciprian N Ionita</dc:creator>
    </item>
    <item>
      <title>Calculations of the cell survival rate after irradiating with minibeams of protons and $^{12}$C</title>
      <link>https://arxiv.org/abs/2411.14589</link>
      <description>arXiv:2411.14589v1 Announce Type: new 
Abstract: The propagation of minibeams of protons and $^{12}$C in a water phantom was modelled with Geant4 v10.3, and the survival probabilities of human salivary gland cells representing healthy and tumour tissues of normal radiosensitivity were calculated with the modified microdosimetric kinetic model. The advantage of minibeams over homogeneous irradiation in sparing healthy tissues proximal to the tumour was demonstrated. Survival-volume histograms were proposed to quantify the difference in the survival probability of cells inside and outside the minibeam spots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14589v1</guid>
      <category>physics.med-ph</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. O. Svetlichnyi (Institute for Nuclear Research of the Russian Academy of Sciences), S. D. Savenkov (Institute for Nuclear Research of the Russian Academy of Sciences), I. A. Pshenichnov (Institute for Nuclear Research of the Russian Academy of Sciences)</dc:creator>
    </item>
    <item>
      <title>ACE-Net: AutofoCus-Enhanced Convolutional Network for Field Imperfection Estimation with application to high b-value spiral Diffusion MRI</title>
      <link>https://arxiv.org/abs/2411.14630</link>
      <description>arXiv:2411.14630v1 Announce Type: new 
Abstract: Spatiotemporal magnetic field variations from B0-inhomogeneity and diffusion-encoding-induced eddy-currents can be detrimental to rapid image-encoding schemes such as spiral, EPI and 3D-cones, resulting in undesirable image artifacts. In this work, a data driven approach for automatic estimation of these field imperfections is developed by combining autofocus metrics with deep learning, and by leveraging a compact basis representation of the expected field imperfections. The method was applied to single-shot spiral diffusion MRI at high b-values where accurate estimation of B0 and eddy were obtained, resulting in high quality image reconstruction without need for additional external calibrations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14630v1</guid>
      <category>physics.med-ph</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengze Gao, Zachary Shah, Xiaozhi Cao, Nan Wang, Daniel Abraham, Kawin Setsompop</dc:creator>
    </item>
    <item>
      <title>In vivo 4D x-ray dark-field lung imaging in mice</title>
      <link>https://arxiv.org/abs/2411.14669</link>
      <description>arXiv:2411.14669v1 Announce Type: new 
Abstract: X-ray dark-field imaging is well-suited to visualizing the health of the lungs because the alveoli create a strong dark-field signal. However, time-resolved and tomographic (i.e., 4D) dark-field imaging is challenging, since most x-ray dark-field techniques require multiple sample exposures, captured while scanning the position of crystals or gratings. Here, we present the first in vivo 4D x-ray dark-field lung imaging in mice. This was achieved by synchronizing the data acquisition process of a single-exposure grid-based imaging approach with the breath cycle. The short data acquisition time per dark-field projection made this approach feasible for 4D x-ray dark-field imaging by minimizing the motion-blurring effect, the total time required and the radiation dose imposed on the sample. Images were captured from a control mouse and from mouse models of muco-obstructive disease and lung cancer, where a change in the size of the alveoli was expected. This work demonstrates that the 4D dark-field signal provides complementary information that is inaccessible from conventional attenuation-based CT images, in particular, how the size of the alveoli from different parts of the lungs changes throughout a breath cycle, with examples shown across the different models. By quantifying the dark-field signal and relating it to other physical properties of the alveoli, this technique could be used to perform functional lung imaging that allows the assessment of both global and regional lung conditions where the size or expansion of the alveoli is affected.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14669v1</guid>
      <category>physics.med-ph</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ying Ying How, Nicole Reyne, Michelle K. Croughan, Patricia Cmielewski, Daniel Batey, Lucy F. Costello, Ronan Smith, Jannis N. Ahlers, Marian Cholewa, Magdalena Kolodziej, Julia Duerr, Marcus A. Mall, Marcus J. Kitchen, Marie-Liesse Asselin-Labat, David M. Paganin, Martin Donnelley, Kaye S. Morgan</dc:creator>
    </item>
    <item>
      <title>Considerations and Recommendations from the ISMRM Diffusion Study Group for preclinical diffusion MRI: Part 1 -- In vivo small-animal imaging</title>
      <link>https://arxiv.org/abs/2209.12994</link>
      <description>arXiv:2209.12994v5 Announce Type: replace 
Abstract: Small-animal diffusion MRI (dMRI) has been used for methodological development and validation, characterizing the biological basis of diffusion phenomena, and comparative anatomy. The steps from animal setup and monitoring, to acquisition, analysis, and interpretation are complex, with many decisions that may ultimately affect what questions can be answered using the resultant data. This work aims to present selected recommendations and guidelines from the diffusion community, on best practices for preclinical dMRI of in vivo animals. We describe the general considerations and foundational knowledge that must be considered when designing experiments. We briefly describe differences in animal species and disease models and discuss why some may be more or less appropriate for different studies. We then give guidelines for in vivo acquisition protocols, including decisions on hardware, animal preparation, and imaging sequences, followed by advice for data processing including pre-processing, model-fitting, and tractography. Finally, we provide an online resource which lists publicly available preclinical dMRI datasets and software packages, to promote responsible and reproducible research. In each section, we attempt to provide guides and recommendations, but also highlight areas for which no guidelines exist (and why), and where future work should focus. While we mainly cover the central nervous system (on which most preclinical dMRI studies are focused), we also provide, where possible and applicable, recommendations for other organs of interest. An overarching goal herein is to enhance the rigor and reproducibility of small animal dMRI acquisitions and analyses, and thereby advance biomedical knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.12994v5</guid>
      <category>physics.med-ph</category>
      <category>q-bio.TO</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ileana O Jelescu, Francesco Grussu, Andrada Ianus, Brian Hansen, Rachel L C Barrett, Manisha Aggarwal, Stijn Michielse, Fatima Nasrallah, Warda Syeda, Nian Wang, Jelle Veraart, Alard Roebroeck, Andrew F Bagdasarian, Cornelius Eichner, Farshid Sepehrband, Jan Zimmermann, Lucas Soustelle, Christien Bowman, Benjamin C Tendler, Andreea Hertanu, Ben Jeurissen, Marleen Verhoye, Lucio Frydman, Yohan van de Looij, David Hike, Jeff F Dunn, Karla Miller, Bennett A Landman, Noam Shemesh, Adam Anderson, Emilie McKinnon, Shawna Farquharson, Flavio Dell' Acqua, Carlo Pierpaoli, Ivana Drobnjak, Alexander Leemans, Kevin D Harkins, Maxime Descoteaux, Duan Xu, Hao Huang, Mathieu D Santin, Samuel C. Grant, Andre Obenaus, Gene S Kim, Dan Wu, Denis Le Bihan, Stephen J Blackband, Luisa Ciobanu, Els Fieremans, Ruiliang Bai, Trygve B Leergaard, Jiangyang Zhang, Tim B Dyrby, G Allan Johnson, Julien Cohen-Adad, Matthew D Budde, Kurt G Schilling</dc:creator>
    </item>
    <item>
      <title>Multi-Branch Generative Models for Multichannel Imaging with an Application to PET/CT Synergistic Reconstruction</title>
      <link>https://arxiv.org/abs/2404.08748</link>
      <description>arXiv:2404.08748v3 Announce Type: replace-cross 
Abstract: This paper presents a novel approach for learned synergistic reconstruction of medical images using multi-branch generative models. Leveraging variational autoencoders (VAEs), our model learns from pairs of images simultaneously, enabling effective denoising and reconstruction. Synergistic image reconstruction is achieved by incorporating the trained models in a regularizer that evaluates the distance between the images and the model. We demonstrate the efficacy of our approach on both Modified National Institute of Standards and Technology (MNIST) and positron emission tomography (PET)/computed tomography (CT) datasets, showcasing improved image quality for low-dose imaging. Despite challenges such as patch decomposition and model limitations, our results underscore the potential of generative models for enhancing medical imaging reconstruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08748v3</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>physics.med-ph</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noel Jeffrey Pinton, Alexandre Bousse, Catherine Cheze-Le-Rest, Dimitris Visvikis</dc:creator>
    </item>
    <item>
      <title>Single color digital H&amp;E staining with In-and-Out Net</title>
      <link>https://arxiv.org/abs/2405.13278</link>
      <description>arXiv:2405.13278v2 Announce Type: replace-cross 
Abstract: Virtual staining streamlines traditional staining procedures by digitally generating stained images from unstained or differently stained images. While conventional staining methods involve time-consuming chemical processes, virtual staining offers an efficient and low infrastructure alternative. Leveraging microscopy-based techniques, such as confocal microscopy, researchers can expedite tissue analysis without the need for physical sectioning. However, interpreting grayscale or pseudo-color microscopic images remains a challenge for pathologists and surgeons accustomed to traditional histologically stained images. To fill this gap, various studies explore digitally simulating staining to mimic targeted histological stains. This paper introduces a novel network, In-and-Out Net, specifically designed for virtual staining tasks. Based on Generative Adversarial Networks (GAN), our model efficiently transforms Reflectance Confocal Microscopy (RCM) images into Hematoxylin and Eosin (H&amp;E) stained images. We enhance nuclei contrast in RCM images using aluminum chloride preprocessing for skin tissues. Training the model with virtual H\&amp;E labels featuring two fluorescence channels eliminates the need for image registration and provides pixel-level ground truth. Our contributions include proposing an optimal training strategy, conducting a comparative analysis demonstrating state-of-the-art performance, validating the model through an ablation study, and collecting perfectly matched input and ground truth images without registration. In-and-Out Net showcases promising results, offering a valuable tool for virtual staining tasks and advancing the field of histological image analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13278v2</guid>
      <category>cs.CV</category>
      <category>physics.med-ph</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.compmedimag.2024.102468</arxiv:DOI>
      <arxiv:journal_reference>Computerized Medical Imaging and Graphics, volume = {118}, pages = {102468}, year = {2024}, issn = {0895-6111},</arxiv:journal_reference>
      <dc:creator>Mengkun Chen, Yen-Tung Liu, Fadeel Sher Khan, Matthew C. Fox, Jason S. Reichenberg, Fabiana C. P. S. Lopes, Katherine R. Sebastian, Mia K. Markey, James W. Tunnell</dc:creator>
    </item>
  </channel>
</rss>
