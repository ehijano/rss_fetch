<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 14 Jul 2025 04:02:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Simultaneous Estimation and Model Choice for Big Discrete Time-to-Event Data with Additive Predictors</title>
      <link>https://arxiv.org/abs/2507.08099</link>
      <description>arXiv:2507.08099v1 Announce Type: new 
Abstract: Discrete-time hazard models are widely used when event times are measured in intervals or are not precisely observed. While these models can be estimated using standard generalized linear model techniques, they rely on extensive data augmentation, making estimation computationally demanding in high-dimensional settings. In this paper, we demonstrate how the recently proposed Batchwise Backfitting algorithm, a general framework for scalable estimation and variable selection in distributional regression, can be effectively extended to discrete hazard models. Using both simulated data and a large-scale application on infant mortality in sub-Saharan Africa, we show that the algorithm delivers accurate estimates, automatically selects relevant predictors, and scales efficiently to large data sets. The findings underscore the algorithm's practical utility for analysing large-scale, complex survival data with high-dimensional covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08099v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin M\"uller, Nikolaus Umlauf, Johannes Seiler, Kenneth Harttgen, Stefan Lang</dc:creator>
    </item>
    <item>
      <title>Block Designs that Provide Optimal Power in the Cochran-Mantel-Haenszel Test</title>
      <link>https://arxiv.org/abs/2507.08125</link>
      <description>arXiv:2507.08125v1 Announce Type: new 
Abstract: We consider the asymptotic power performance under local alternatives of the Cochran-Mantel-Haenszel test. Our setting is non-traditional: we investigate randomized experiments that assign subjects via Fisher's blocking design. We show that blocking designs that satisfy a certain balance condition are asymptotically optimal. When the potential outcomes can be ordered, the balance condition is met for all blocking designs with number of blocks going to infinity. More generally, we prove that the pairwise matching design of Greevy et al. (2004) satisfies the balance condition under mild assumptions. In smaller sample sizes, we show a second order effect becomes operational thereby making blocking designs with a smaller number optimal. In practical settings with many covariates, we recommend pairwise matching for its ability to approximate the balance condition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08125v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Azriel, Adam Kapelner, Abba M. Krieger</dc:creator>
    </item>
    <item>
      <title>A proposal for homoscedastic modelling with conditional auto-regressive distributions</title>
      <link>https://arxiv.org/abs/2507.08376</link>
      <description>arXiv:2507.08376v1 Announce Type: new 
Abstract: Conditional auto-regressive (CAR) distributions are widely used to induce spatial dependence in the geographic analysis of areal data. These distributions establish multivariate dependence networks by defining conditional relationships between neighboring units, resulting in positive dependence among nearby observations. Despite their practical convenience, the conditional nature of CAR distributions can lead to undesirable marginal properties, such as inherent heterogeneity assumptions that may significantly impact the posterior distributions.
  In this paper, we highlight the variance issues associated with CAR distributions, particularly focusing on edge effects and artifacts related to the region's geometry. We show that edge effects may be more significant and widespread in the outcomes of disease mapping studies than previously anticipated. To address these homoscedasticity concerns, we introduce a new conditional autoregressive distribution designed to mitigate these problems. We demonstrate how this distribution effectively resolves the practical issues identified in earlier models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08376v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miguel A. Martinez-Beneito, Aritz Ad\'in, Tom\'as Goicoa, Lola Ugarte</dc:creator>
    </item>
    <item>
      <title>Nonparametric predictive inference for discrete data via Metropolis-adjusted Dirichlet sequences</title>
      <link>https://arxiv.org/abs/2507.08629</link>
      <description>arXiv:2507.08629v1 Announce Type: new 
Abstract: This article is motivated by challenges in conducting Bayesian inferences on unknown discrete distributions, with a particular focus on count data. To avoid the computational disadvantages of traditional mixture models, we develop a novel Bayesian predictive approach. In particular, our Metropolis-adjusted Dirichlet (MAD) sequence model characterizes the predictive measure as a mixture of a base measure and Metropolis-Hastings kernels centered on previous data points. The resulting MAD sequence is asymptotically exchangeable and the posterior on the data generator takes the form of a martingale posterior. This structure leads to straightforward algorithms for inference on count distributions, with easy extensions to multivariate, regression, and binary data cases. We obtain a useful asymptotic Gaussian approximation and illustrate the methodology on a variety of applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08629v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Davide Agnoletto, Tommaso Rigon, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Robust inference under Benford's law</title>
      <link>https://arxiv.org/abs/2507.08650</link>
      <description>arXiv:2507.08650v1 Announce Type: new 
Abstract: We address the task of identifying anomalous observations by analyzing digits under the lens of Benford's law. Motivated by the crucial objective of providing reliable statistical analysis of customs declarations, we answer one major and still open question: How can we detect the behavior of operators who are aware of the prevalence of the Benford's pattern in the digits of regular observations and try to manipulate their data in such a way that the same pattern also holds after data fabrication? This challenge arises from the ability of highly skilled and strategically minded manipulators in key organizational positions or criminal networks to exploit statistical knowledge and evade detection. For this purpose, we write a specific contamination model for digits, obtain new relevant distributional results and derive appropriate goodness-of-fit statistics for the considered adversarial testing problem. Along our path, we also unveil the peculiar relationship between two simple conformance tests based on the distribution of the first digit. We show the empirical properties of the proposed tests through a simulation exercise and application to data from international trade transactions. Although we cannot claim that our results are able to anticipate data fabrication with certainty, they surely point to situations where more substantial controls are needed. Furthermore, our work can reinforce trust in data integrity in many critical domains where mathematically informed misconduct is suspected.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08650v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucio Barabesi, Andrea Cerioli, Andrea Cerasa, Domenico Perrotta</dc:creator>
    </item>
    <item>
      <title>Total/dual correlation/coherence, redundancy/synergy, complexity, and O-information for real and complex valued multivariate data</title>
      <link>https://arxiv.org/abs/2507.08773</link>
      <description>arXiv:2507.08773v1 Announce Type: new 
Abstract: Firstly, assuming Gaussianity, equations for the following information theory measures are presented: total correlation/coherence (TC), dual total correlation/coherence (DTC), O-information, TSE complexity, and the redundancy-synergy index (RSI). Since these measures are functions of the covariance matrix "S" and its inverse "S^-1", the associated Wishart and inverse-Wishart distributions are of note. The DTC is shown here to be the Kullback-Leibler (KL) divergence for the inverse-Wishart pair "(S^-1)" and its diagonal matrix "diag(S^-1)", shedding light on its interpretation as a measure of "total partial correlation", -lndetP, with test hypothesis H0: P=I, where "P" is the standardized inverse covariance (i.e. P=(D^-1/2)(S^-1)(D^-1/2), with D=diag(S^-1)). The second aim of this paper introduces a generalization of all these measures for structured groups of variables. For instance, consider three or more groups, each consisting of three or more variables, with predominant redundancy within each group, but with synergistic interactions between groups. O-information will miss the between group synergy (since redundancy occurs more often in the system). In contrast, the structured O-information measure presented here will correctly report predominant synergy between groups. This is a relevant generalization towards structured multivariate information measures. A third aim is the presentation of a framework for quantifying the contribution of "connections" between variables, to the system's TC, DTC, O-information, and TSE complexity. A fourth aim is to present a generalization of the redundancy-synergy index for quantifying the contribution of a group of variables to the system's redundancy-synergy balance. Finally, it is shown that the expressions derived here directly apply to data from several other elliptical distributions. All program codes, data files, and executables are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08773v1</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Roberto D. Pascual-Marqui, Kieko Kochi, Toshihiko Kinoshita</dc:creator>
    </item>
    <item>
      <title>CLEAR: Calibrated Learning for Epistemic and Aleatoric Risk</title>
      <link>https://arxiv.org/abs/2507.08150</link>
      <description>arXiv:2507.08150v1 Announce Type: cross 
Abstract: Accurate uncertainty quantification is critical for reliable predictive modeling, especially in regression tasks. Existing methods typically address either aleatoric uncertainty from measurement noise or epistemic uncertainty from limited data, but not necessarily both in a balanced way. We propose CLEAR, a calibration method with two distinct parameters, $\gamma_1$ and $\gamma_2$, to combine the two uncertainty components for improved conditional coverage. CLEAR is compatible with any pair of aleatoric and epistemic estimators; we show how it can be used with (i) quantile regression for aleatoric uncertainty and (ii) ensembles drawn from the Predictability-Computability-Stability (PCS) framework for epistemic uncertainty. Across 17 diverse real-world datasets, CLEAR achieves an average improvement of 28.2% and 17.4% in the interval width compared to the two individually calibrated baselines while maintaining nominal coverage. This improvement can be particularly evident in scenarios dominated by either high epistemic or high aleatoric uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08150v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilia Azizi, Juraj Bodik, Jakob Heiss, Bin Yu</dc:creator>
    </item>
    <item>
      <title>Optimal Experimental Design for Microplastics Sampling Experiments</title>
      <link>https://arxiv.org/abs/2507.08170</link>
      <description>arXiv:2507.08170v1 Announce Type: cross 
Abstract: Microplastics contamination is one of the most rapidly growing research topics. However, monitoring microplastics contamination in the environment presents both logistical and statistical challenges, particularly when constrained resources limit the scale of sampling and laboratory analysis. In this paper, we propose a Bayesian framework for the optimal experimental design of microplastic sampling campaigns. Our approach integrates prior knowledge and uncertainty quantification to guide decisions on how many spatial Centrosamples to collect and how many particles to analyze for polymer composition. By modeling particle counts as a Poisson distribution and polymer types as a Multinomial distribution, we developed a conjugate Bayesian model that enables efficient posterior inference. We introduce variance-based loss functions to evaluate expected information gain for both abundance and composition, and we formulate a constrained optimization problem that incorporates realistic cost structures. Our results provide principled and interpretable recommendations for allocating limited resources across the sampling and analysis phases. Through simulated scenarios and real-world-inspired examples, we demonstrate how the proposed methodology adapts to prior assumptions and cost variations, ensuring robustness and flexibility. This work contributes to the broader field of Bayesian experimental design by offering a concrete, application-driven case study that underscores the value of formal design strategies in environmental monitoring contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08170v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco A. Aquino-L\'opez, Ana Carolina Ruiz-Fern\'andez, Joan-Albert Sanchez-Cabeza, J. Andr\'es Christen</dc:creator>
    </item>
    <item>
      <title>ALL-IN meta-analysis: breathing life into living systematic reviews and prospective meta-analyses</title>
      <link>https://arxiv.org/abs/2109.12141</link>
      <description>arXiv:2109.12141v2 Announce Type: replace 
Abstract: Science is justly admired as a cumulative process ("standing on the shoulders of giants"), yet scientific knowledge is typically built on a patchwork of research contributions without much coordination. This lack of efficiency has specifically been addressed in clinical research by recommendations against avoidable research waste and for living systematic reviews and prospective meta-analysis. We propose to further those recommendations with ALL-IN meta-analysis: Anytime Live and Leading INterim meta-analysis. ALL-IN provides meta-analysis based on e-values and anytime-valid confidence intervals that can be updated at any time - reanalyzing after each new observation while retaining type-I error and coverage guarantees, live - no need to prespecify the looks, and leading - in the decisions on whether individual studies should be initiated, stopped or expanded, the meta-analysis can be the leading source of information without losing validity to accumulation bias. The analysis design requires no information about the trial sample sizes or the number of trials eventually included. So ALL-IN meta-analysis can be applied retrospectively as well as prospectively, to evaluate the evidence once or sequentially. Because the intention of the analysis does not change the validity of the results, the results of the analysis can change the intentions ('optional stopping' and 'optional continuation' based on the results so far). On the one hand: any analysis can be turned into a living one, or even become prospective and real-time by updating with new trial data and including interim data from trials that are still ongoing - without any changes in the cut-offs for testing or the method for interval estimation. On the other hand: no stopping rule needs to be enforced for the analysis to remain valid, so a prospective meta-analysis can be a bottom-up collaboration [...]</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.12141v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.12688/f1000research.74223.2</arxiv:DOI>
      <arxiv:journal_reference>[version 2; peer review: 1 approved, 2 approved with reservations]. F1000Research 2025, 11:549</arxiv:journal_reference>
      <dc:creator>Judith ter Schure, Peter Gr\"unwald</dc:creator>
    </item>
    <item>
      <title>Estimation of conditional average treatment effects on distributed confidential data</title>
      <link>https://arxiv.org/abs/2402.02672</link>
      <description>arXiv:2402.02672v4 Announce Type: replace 
Abstract: The estimation of conditional average treatment effects (CATEs) is an important topic in many scientific fields. CATEs can be estimated with high accuracy if data distributed across multiple parties are centralized. However, it is difficult to aggregate such data owing to confidentiality or privacy concerns. To address this issue, we propose data collaboration double machine learning, a method for estimating CATE models using privacy-preserving fusion data constructed from distributed sources, and evaluate its performance through simulations. We make three main contributions. First, our method enables estimation and testing of semi-parametric CATE models without iterative communication on distributed data, providing robustness to model mis-specification compared to parametric approaches. Second, it enables collaborative estimation across different time points and parties by accumulating a knowledge base. Third, our method performs as well as or better than existing methods in simulations using synthetic, semi-synthetic, and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02672v4</guid>
      <category>stat.ME</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuji Kawamata, Ryoki Motai, Yukihiko Okada, Akira Imakura, Tetsuya Sakurai</dc:creator>
    </item>
    <item>
      <title>A step towards the integration of machine learning and classic model-based survey methods</title>
      <link>https://arxiv.org/abs/2402.07521</link>
      <description>arXiv:2402.07521v2 Announce Type: replace 
Abstract: The usage of machine learning methods in traditional surveys including official statistics, is still very limited. Therefore, we propose a predictor supported by these algorithms, which can be used to predict any population or subpopulation characteristics. Machine learning methods have already been shown to be very powerful in identifying and modelling complex and nonlinear relationships between the variables, which means they have very good properties in case of strong departures from the classic assumptions. Therefore, we analyse the performance of our proposal under a different set-up, which, in our opinion, is of greater importance in real-life surveys. We study only small departures from the assumed model to show that our proposal is a good alternative, even in comparison with optimal methods under the model. Moreover, we propose the method of the ex ante accuracy estimation of machine learning predictors, giving the possibility of the accuracy comparison with classic methods. The solution to this problem is indicated in the literature as one of the key issues in integrating these approaches. The simulation studies are based on a real, longitudinal dataset, where the prediction of subpopulation characteristics is considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07521v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s13042-025-02718-6</arxiv:DOI>
      <dc:creator>Tomasz \.Z\k{a}d{\l}o, Adam Chwila</dc:creator>
    </item>
    <item>
      <title>Asymmetric canonical correlation analysis of Riemannian and high-dimensional data</title>
      <link>https://arxiv.org/abs/2404.11781</link>
      <description>arXiv:2404.11781v2 Announce Type: replace 
Abstract: In this paper, we introduce a novel statistical model for the integrative analysis of Riemannian-valued functional data and high-dimensional data. We apply this model to explore the dependence structure between each subject's dynamic functional connectivity -- represented by a temporally indexed collection of positive definite covariance matrices -- and high-dimensional data representing lifestyle, demographic, and psychometric measures. Specifically, we employ a reformulation of canonical correlation analysis that enables efficient control of the complexity of the functional canonical directions using tangent space sieve approximations. Additionally, we enforce an interpretable group structure on the high-dimensional canonical directions via a sparsity-promoting penalty. The proposed method shows improved empirical performance over alternative approaches and comes with theoretical guarantees. Its application to data from the Human Connectome Project reveals a dominant mode of covariation between dynamic functional connectivity and lifestyle, demographic, and psychometric measures. This mode aligns with results from static connectivity studies but reveals a unique temporal non-stationary pattern that such studies fail to capture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11781v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Buenfil, Eardi Lila</dc:creator>
    </item>
    <item>
      <title>Bayesian Nonparametric Sensitivity Analysis of Multiple Test Procedures Under Dependence</title>
      <link>https://arxiv.org/abs/2410.08080</link>
      <description>arXiv:2410.08080v4 Announce Type: replace 
Abstract: This article introduces a sensitivity analysis method for Multiple Testing Procedures (MTPs) using marginal $p$-values. The method is based on the Dirichlet process (DP) prior distribution, specified to support the entire space of MTPs, where each MTP controls either the family-wise error rate (FWER) or the false discovery rate (FDR) under arbitrary dependence between $p$-values. This DP MTP sensitivity analysis method provides uncertainty quantification for MTPs, by accounting for uncertainty in the selection of such MTPs and their respective threshold decisions regarding which number of smallest $p$-values are significant discoveries, from a given set of null hypothesis tested, while measuring each $p$-value's probability of significance over the DP prior predictive distribution of this space of all MTPs, and reducing the possible conservativeness of using only one such MTP for multiple testing. The DP MTP sensitivity analysis method is illustrated through the analysis of over twenty-eight thousand $p$-values arising from hypothesis tests performed on a 2022 dataset of a representative sample of three million U.S. high school students observed on 239 variables. They include tests which, respectively, relate variables about the disruption caused by school closures during the COVID-19 pandemic, with various mathematical cognition, academic achievement, and student background variables. R software code for the DP MTP sensitivity analysis method is provided in the Code and Data Supplement of this article (available upon request).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08080v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Karabatsos</dc:creator>
    </item>
    <item>
      <title>Rational Expectations in Empirical Bayes</title>
      <link>https://arxiv.org/abs/2411.06129</link>
      <description>arXiv:2411.06129v3 Announce Type: replace 
Abstract: We propose a principled framework for nonparametric empirical Bayes (EB) estimation, based on the idea that the prior should be consistent with the observed posterior and that Bayesian updating should be stable. Focusing on discretized priors, we characterize EB estimators as fixed points of a posterior belief operator. We establish the uniqueness of such fixed points and illustrate how the approach improves transparency and interpretability in standard EB settings, including a recent model of discrimination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06129v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valentino Dardanoni, Stefano Demichelis</dc:creator>
    </item>
    <item>
      <title>Modeling temporal dependence in a sequence of spatial random partitions driven by spanning tree: an application to mosquito-borne diseases</title>
      <link>https://arxiv.org/abs/2501.04601</link>
      <description>arXiv:2501.04601v2 Announce Type: replace 
Abstract: Spatially constrained clustering is an important field of research, particularly when it involves changes over time. Partitioning a map is not simple since there is a vast number of possible partitions within the search space. In spatio-temporal clustering, this task becomes even more difficult, as we must consider sequences of partitions. Motivated by these challenges, we introduce a Bayesian model for time-dependent sequences of spatial random partitions by proposing a prior distribution based on product partition models that correlates partitions. Additionally, we employ random spanning trees to facilitate the exploration of the partition search space and to guarantee spatially constrained clustering. This work is motivated by a relevant applied problem: identifying spatial and temporal patterns of mosquito-borne diseases. Given the overdispersion present in this type of data, we introduce a spatio-temporal Poisson mixture model in which mean and dispersion parameters vary according to spatio-temporal covariates. The proposed model is applied to analyze the number of dengue cases reported weekly from 2018 to 2023 in the Southeast region of Brazil. We also evaluate model performance using simulated data. Overall, the proposed model has proven to be a competitive approach for analyzing the temporal evolution of spatial clustering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04601v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jessica Pavani, Rosangela Helena Loschi, Fernando Andres Quintana</dc:creator>
    </item>
    <item>
      <title>Change-point problem: Direct estimation using a geometry inspired identifiable reparameterization</title>
      <link>https://arxiv.org/abs/2502.11679</link>
      <description>arXiv:2502.11679v2 Announce Type: replace 
Abstract: Estimation of mean shift in a temporally ordered sequence of random variables with a possible existence of change-point is an important problem in many disciplines. In the available literature of more than fifty years the estimation methods of the mean shift is usually dealt as a two-step problem. A test for the existence of a change-point is followed by an estimation process of the mean shift, which is known as testimator. The problem suffers from over parametrization. When viewed as an estimation problem, we establish that the maximum likelihood estimator (MLE) always gives a false alarm indicting an existence of a change-point in the given sequence even though there is no change-point at all. After modelling the parameter space as a modified horn torus. We introduce a new method of estimation of the parameters. The newly introduced estimation method of the mean shift is assessed with a proper Riemannian metric on that conic manifold. It is seen that its performance is superior compared to that of the MLE. The proposed method is implemented on Bitcoin data and compared its performance with the performance of the MLE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11679v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Buddhananda Banerjee, Arnab Kumar Laha</dc:creator>
    </item>
    <item>
      <title>Eigengap Sparsity for Covariance Parsimony</title>
      <link>https://arxiv.org/abs/2504.10110</link>
      <description>arXiv:2504.10110v2 Announce Type: replace 
Abstract: Covariance estimation is a central problem in statistics. An important issue is that there are rarely enough samples $n$ to accurately estimate the $p (p+1) / 2$ coefficients in dimension $p$. Parsimonious covariance models are therefore preferred, but the discrete nature of model selection makes inference computationally challenging. In this paper, we propose a relaxation of covariance parsimony termed "eigengap sparsity" and motivated by the good accuracy-parsimony tradeoffs of eigenvalue-equalization in covariance matrices. This penalty can be included in a penalized-likelihood framework that we propose to solve with a projected gradient descent on a monotone cone. The algorithm turns out to resemble an isotonic regression of mutually-attracted sample eigenvalues, drawing an interesting link between covariance parsimony and shrinkage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10110v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom Szwagier, Guillaume Olikier, Xavier Pennec</dc:creator>
    </item>
    <item>
      <title>Modern causal inference approaches to improve power for subgroup analysis in randomized controlled trials</title>
      <link>https://arxiv.org/abs/2505.08960</link>
      <description>arXiv:2505.08960v2 Announce Type: replace 
Abstract: Randomized controlled trials (RCTs) often include subgroup analyses to assess whether treatment effects vary across pre-specified patient populations. However, these analyses frequently suffer from small sample sizes which limit the power to detect heterogeneous effects. Power can be improved by leveraging predictors of the outcome -- i.e., through covariate adjustment -- as well as by borrowing external data from similar RCTs or observational studies. The benefits of covariate adjustment may be limited when the trial sample is small. Borrowing external data can increase the effective sample size and improve power, but it introduces two key challenges: (i) integrating data across sources can lead to model misspecification, and (ii) practical violations of the positivity assumption -- where the probability of receiving the target treatment is near-zero for some covariate profiles in the external data -- can lead to extreme inverse-probability weights and unstable inferences, ultimately negating potential power gains. To account for these shortcomings, we present an approach to improving power in pre-planned subgroup analyses of small RCTs that leverages both baseline predictors and external data. We propose debiased estimators that accommodate parametric, machine learning, and nonparametric Bayesian methods. To address practical positivity violations, we introduce three estimators: a covariate-balancing approach, an automated debiased machine learning (DML) estimator, and a calibrated DML estimator. We show improved power in various simulations and offer practical recommendations for the application of the proposed methods. Finally, we apply them to evaluate the effectiveness of citalopram for negative symptoms in first-episode schizophrenia patients across subgroups defined by duration of untreated psychosis, using data from two small RCTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08960v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonio D'Alessandro, Jiyu Kim, Samrachana Adhikari, Donald Goff, Falco J. Bargagli Stoffi, Michele Santacatterina</dc:creator>
    </item>
    <item>
      <title>Rating competitors in games with strength-dependent tie probabilities</title>
      <link>https://arxiv.org/abs/2506.11354</link>
      <description>arXiv:2506.11354v2 Announce Type: replace 
Abstract: Competitor rating systems for head-to-head games are typically used to measure playing strength from game outcomes. Ratings computed from these systems are often used to select top competitors for elite events, for pairing players of similar strength in online gaming, and for players to track their own strength over time. Most implemented rating systems assume only win/loss outcomes, and treat occurrences of ties as the equivalent to half a win and half a loss. However, in games such as chess, the probability of a tie (draw) is demonstrably higher for stronger players than for weaker players, so that rating systems ignoring this aspect of game results may produce strength estimates that are unreliable. We develop a new rating system for head-to-head games based on a model by Glickman (2025) that explicitly acknowledges that a tie may depend on the strengths of the competitors. The approach uses a Bayesian dynamic modeling framework. Within each time period, posterior updates are computed in closed form using a single Newton-Raphson iteration evaluated at the prior mean. The approach is demonstrated on a large dataset of chess games played in International Correspondence Chess Federation tournaments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11354v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark E. Glickman</dc:creator>
    </item>
    <item>
      <title>A Bayesian framework for change-point detection with uncertainty quantification</title>
      <link>https://arxiv.org/abs/2507.01558</link>
      <description>arXiv:2507.01558v2 Announce Type: replace 
Abstract: We introduce a novel Bayesian method that can detect multiple structural breaks in the mean and variance of a length $T$ time-series. Our method quantifies uncertainty by returning $\alpha$-level credible sets around the estimated locations of the breaks. In the case of a single change in the mean and/or the variance of an independent sub-Gaussian sequence, we prove that our method attains a localization rate that is minimax optimal up to a $\log T$ factor. For an $\alpha$-mixing sequence with dependence, we prove this optimality holds up to $\log^2 T$ factor. For $d$-dimensional mean changes, we show that if $d \gg \log T$ and the mean signal is dense, then our method exactly recovers the location of the change. Our method detects multiple change-points by modularly ``stacking'' single change-point models and searching for a variational approximation to the posterior distribution. This approach is applicable to both continuous and count data. Extensive simulation studies demonstrate that our method is competitive with the state-of-the-art in terms of speed and performance, and we produce credible sets that are an order of magnitude smaller than our competitors without sacrificing nominal coverage guarantees. We apply our method to real data by detecting i) the gating of an ion channel in the outer membrane of a bacterial cell, and ii) changes in the lithological structure of an oil well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01558v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Davis Berlind, Lorenzo Cappello, Oscar Hernan Madrid Padilla</dc:creator>
    </item>
    <item>
      <title>DUST: A Duality-Based Pruning Method For Exact Multiple Change-Point Detection</title>
      <link>https://arxiv.org/abs/2507.02467</link>
      <description>arXiv:2507.02467v2 Announce Type: replace 
Abstract: We tackle the challenge of detecting multiple change points in large time series by optimising a penalised likelihood derived from exponential family models. Dynamic programming algorithms can solve this task exactly with at most quadratic time complexity. In recent years, the development of pruning strategies has drastically improved their computational efficiency. However, the two existing approaches have notable limitations: PELT struggles with pruning efficiency in sparse-change scenarios, while FPOP's structure is not adapted to multi-parametric settings. To address these issues, we introduce the DUal Simple Test (DUST) framework, which prunes candidate changes by evaluating a dual function against a threshold. This approach is highly flexible and broadly applicable to parametric models of any dimension. Under mild assumptions, we establish strong duality for the underlying non-convex pruning problem. We demonstrate DUST's effectiveness across various change-point regimes and models. In particular, for one-parametric models, DUST matches the simplicity of PELT with the efficiency of FPOP. Its use is especially advantageous for non-Gaussian models. Finally, we apply DUST to mouse monitoring time series under a change-in-variance model, illustrating its ability to recover the optimal change-point structure efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02467v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Runge, Charles Truong, Simon Quern\'e</dc:creator>
    </item>
    <item>
      <title>Nonparametric Treatment Effect Identification in School Choice</title>
      <link>https://arxiv.org/abs/2112.03872</link>
      <description>arXiv:2112.03872v4 Announce Type: replace-cross 
Abstract: This paper studies nonparametric identification and estimation of causal effects in centralized school assignment. In many centralized assignment algorithms, students are subjected to both lottery-driven variation and regression discontinuity (RD) driven variation. We characterize the full set of identified atomic treatment effects (aTEs), defined as the conditional average treatment effect between a pair of schools, given student characteristics. Atomic treatment effects are the building blocks of more aggregated notions of treatment contrasts, and common approaches to estimating aggregations of aTEs can mask important heterogeneity. In particular, many aggregations of aTEs put zero weight on aTEs driven by RD variation, and estimators of such aggregations put asymptotically vanishing weight on the RD-driven aTEs. We provide a diagnostic and recommend new aggregation schemes. Lastly, we provide estimators and accompanying asymptotic results for inference for those aggregations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.03872v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen</dc:creator>
    </item>
    <item>
      <title>Signed Diverse Multiplex Networks: Clustering and Inference</title>
      <link>https://arxiv.org/abs/2402.10242</link>
      <description>arXiv:2402.10242v3 Announce Type: replace-cross 
Abstract: The paper introduces a Signed Generalized Random Dot Product Graph (SGRDPG) model, which is a variant of the Generalized Random Dot Product Graph (GRDPG), where, in addition, edges can be positive or negative. The setting is extended to a multiplex version, where all layers have the same collection of nodes and follow the SGRDPG. The only common feature of the layers of the network is that they can be partitioned into groups with common subspace structures, while otherwise matrices of connection probabilities can be all different. The setting above is extremely flexible and includes a variety of existing multiplex network models, including GRDPG, as its particular cases.
  By employing novel methodologies, our paper ensures strongly consistent clustering of layers and highly accurate subspace estimation, which are significant improvements over the results of Pensky and Wang (2024). All algorithms and theoretical results in the paper remain true for both signed and binary networks. In addition, the paper shows that keeping signs of the edges in the process of network construction leads to a better precision of estimation and clustering and, hence, is beneficial for tackling real world problems such as, for example, analysis of brain networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10242v3</guid>
      <category>cs.SI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marianna Pensky</dc:creator>
    </item>
    <item>
      <title>Potential weights and implicit causal designs in linear regression</title>
      <link>https://arxiv.org/abs/2407.21119</link>
      <description>arXiv:2407.21119v3 Announce Type: replace-cross 
Abstract: When we interpret linear regression as estimating causal effects justified by quasi-experimental treatment variation, what do we mean? This paper characterizes the necessary implications when linear regressions are interpreted causally. A minimal requirement for causal interpretation is that the regression estimates some contrast of individual potential outcomes under the true treatment assignment process. This requirement implies linear restrictions on the true distribution of treatment. Solving these linear restrictions leads to a set of implicit designs. Implicit designs are plausible candidates for the true design if the regression were to be causal. The implicit designs serve as a framework that unifies and extends existing theoretical results across starkly distinct settings (including multiple treatment, panel, and instrumental variables). They lead to new theoretical insights for widely used but less understood specifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21119v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen</dc:creator>
    </item>
    <item>
      <title>Local transfer learning Gaussian process modeling, with applications to surrogate modeling of expensive computer simulators</title>
      <link>https://arxiv.org/abs/2410.12690</link>
      <description>arXiv:2410.12690v3 Announce Type: replace-cross 
Abstract: A critical bottleneck for scientific progress is the costly nature of computer simulations for complex systems. Surrogate models provide an appealing solution: such models are trained on simulator evaluations, then used to emulate and quantify uncertainty on the expensive simulator at unexplored inputs. In many applications, one often has available data on related systems. For example, in designing a new jet turbine, there may be existing studies on turbines with similar configurations. A key question is how information from such ``source'' systems can be transferred for effective surrogate training on the ``target'' system of interest. We thus propose a new LOcal transfer Learning Gaussian Process (LOL-GP) model, which leverages a carefully-designed Gaussian process to transfer such information for surrogate modeling. The key novelty of the LOL-GP is a latent regularization model, which identifies regions where transfer should be performed and regions where it should be avoided. Such a ``local transfer'' property is present in many scientific systems: at certain parameters, systems may behave similarly and thus transfer is beneficial; at other parameters, they may behave differently and thus transfer is detrimental. By accounting for local transfer, the LOL-GP can temper the risk of ``negative transfer'', i.e., the risk of worsening predictive performance from information transfer. We derive a Gibbs sampling algorithm for efficient posterior predictive sampling on the LOL-GP, for both the multi-source and multi-fidelity transfer settings. We then show, via a suite of numerical experiments and an application for jet turbine design, the improved surrogate performance of the LOL-GP over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12690v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinming Wang, Simon Mak, John Miller, Jianguo Wu</dc:creator>
    </item>
    <item>
      <title>Frequentist Statistics as Internalist Reliabilism</title>
      <link>https://arxiv.org/abs/2411.08547</link>
      <description>arXiv:2411.08547v4 Announce Type: replace-cross 
Abstract: There has long been an impression that reliabilism implies externalism and that frequentist statistics, due to its reliabilist nature, is inherently externalist. I argue, however, that frequentist statistics can plausibly be understood as a form of internalist reliabilism -- internalist in the conventional sense, yet reliabilist in certain unconventional and intriguing ways. Crucially, in developing the thesis that reliabilism does not imply externalism, my aim is not to stretch the meaning of `reliabilism' merely to sever the implication. Instead, it is to gain a deeper understanding of frequentist statistics, which stands as one of the most sustained attempts by scientists to develop an epistemology for their own use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08547v4</guid>
      <category>stat.OT</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hanti Lin</dc:creator>
    </item>
    <item>
      <title>Accounting for Missing Data in Public Health Research Using a Synthesis of Statistical and Mathematical Models</title>
      <link>https://arxiv.org/abs/2503.02789</link>
      <description>arXiv:2503.02789v2 Announce Type: replace-cross 
Abstract: Introduction: Missing data is a challenge to medical research. Accounting for missing data by imputing or weighting conditional on covariates relies on the variable with missingness being observed at least some of the time for all unique covariate values. This requirement is referred to as positivity, and violations can result in bias. Here, we review a novel approach to addressing positivity violations in the context of systolic blood pressure. Methods: To illustrate the proposed approach, we estimate the mean systolic blood pressure among children and adolescents aged 2-17 years old in the United States using data from 2017-2018 National Health and Nutrition Examination Survey (NHANES). As blood pressure was never measured for those aged 2-7, there exists a positivity violation by design. Using a recently proposed synthesis of statistical and mathematical models, we integrate external information with NHANES to address our motivating question. Results: With the synthesis model, the estimated mean systolic blood pressure was 100.5 (95% confidence interval: 99.9, 101.0), which is notably lower than either a complete-case analysis or extrapolation from a statistical model. The synthesis results were supported by a diagnostic comparing the performance of the mathematical model in the positive region. Conclusion: Positivity violations pose a threat to quantitative medical research, and standard approaches to addressing nonpositivity rely on restrictive untestable assumptions. Using a synthesis model, like the one detailed here, offers a viable alternative through integration of external information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02789v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul N Zivich, Bonnie E Shook-Sa, Stephen R Cole, Eric T Lofgren, Jessie K Edwards</dc:creator>
    </item>
    <item>
      <title>Reinterpreting demand estimation</title>
      <link>https://arxiv.org/abs/2503.23524</link>
      <description>arXiv:2503.23524v2 Announce Type: replace-cross 
Abstract: This paper bridges the demand estimation and causal inference literatures by interpreting nonparametric structural assumptions as restrictions on counterfactual outcomes. It offers nontrivial and equivalent restatements of key demand estimation assumptions in the Neyman-Rubin potential outcomes model, for both settings with market-level data (Berry and Haile, 2014) and settings with demographic-specific market shares (Berry and Haile, 2024). The reformulation highlights a latent homogeneity assumption underlying structural demand models: The relationship between counterfactual outcomes is assumed to be identical across markets. This assumption is strong, but necessary for identification of market-level counterfactuals. Viewing structural demand models as misspecified but approximately correct reveals a tradeoff between specification flexibility and robustness to latent homogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23524v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen</dc:creator>
    </item>
  </channel>
</rss>
